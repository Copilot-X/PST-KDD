text,label
"global-history based predictor derived from PPM. PPM was originally introduced for text compression <ref type=""bibr"" target=""#b0"">[1]</ref>, and it was used in <ref type=""bibr"" target=""#b1"">[2]</ref> nd of the PCF, we build S * and sort sequences in decreasing potentials. Then we compute m(T ) from <ref type=""bibr"" target=""#b0"">(1)</ref>. Because of memory limitations on our machines, when |S| exc rting at this step, the content of T is allowed to change dynamically. We can no longer use formula <ref type=""bibr"" target=""#b0"">(1)</ref>, and so now we generate results by performing a second pass",1
"ginally introduced for text compression <ref type=""bibr"" target=""#b0"">[1]</ref>, and it was used in <ref type=""bibr"" target=""#b1"">[2]</ref> for branch prediction. Figure <ref type=""figure"" target=""#fi",0
"re exists a longer matching sequence in T , i.e., s∈T f (s.0 u ). More explanations can be found in <ref type=""bibr"" target=""#b6"">[7]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=",0
"be viewed as a 4 th order approximation to PPM <ref type=""bibr"" target=""#b2"">[3]</ref>, while YAGS <ref type=""bibr"" target=""#b3"">[4]</ref>, which is a GPPM predictor too, can be viewed as a 1 st orde",0
"a synopsis of cbp1.5, which features 5 banks. It can be viewed as a 4 th order approximation to PPM <ref type=""bibr"" target=""#b2"">[3]</ref>, while YAGS <ref type=""bibr"" target=""#b3"">[4]</ref>, which i are not folding a random value, but a global history value derived from the previous history value <ref type=""bibr"" target=""#b2"">[3]</ref>. Figure <ref type=""figure"" target=""#fig_1"">2</ref> shows two",0
"including state of the art fetch architectures like the FTB proposed by Reinman, Austin and Calder <ref type=""bibr"" target=""#b29"">[30]</ref> and the trace cache architecture as proposed by Rotenberg, h prediction mechanism and the instruction cache access, as proposed by Reinman, Austin, and Calder <ref type=""bibr"" target=""#b29"">[30]</ref>. The branch prediction mechanism is a fully autonomous eng ion cache is then driven by the requests stored in the FTQ.</p><p>Another important contribution of <ref type=""bibr"" target=""#b29"">[30]</ref> is the Fetch Target Buffer (FTB). It extends the BTB by al ssibly containing multiple basic blocks.</p><p>The use of an FTQ is not novel, it was introduced in <ref type=""bibr"" target=""#b29"">[30]</ref>. It decouples the branch prediction from the memory access .0""><head n=""3.3."">Fetch target queue</head><p>Following the proposal of Reinman, Austin and Calder <ref type=""bibr"" target=""#b29"">[30]</ref> we have decoupled the branch prediction stage from the ins ream fetch architecture with three other state-of-the-art fetch architectures: the FTB architecture <ref type=""bibr"" target=""#b29"">[30]</ref> using a perceptron branch predictor <ref type=""bibr"" targe",1
"t=""#b29"">[30]</ref> and the trace cache architecture as proposed by Rotenberg, Bennett and Smith in <ref type=""bibr"" target=""#b31"">[32]</ref>.</p><p>In Section 3 we describe our proposed stream fetch f> shows a block diagram of the trace cache mechanism as proposed by Rotenberg, Benett and Smith in <ref type=""bibr"" target=""#b31"">[32]</ref>. The trace cache captures the dynamic instruction stream, <ref type=""bibr"" target=""#b33"">[34]</ref>, and the trace cache architecture using a trace predictor <ref type=""bibr"" target=""#b31"">[32]</ref> and selective trace storage <ref type=""bibr"" target=""#b28"" rget=""#b8"">[9,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b30"">31,</ref><ref type=""bibr"" target=""#b31"">32]</ref> is one such high fetch width mechanism, recently implemente",1
"scalar processor, fetching multiple basic blocks per cycle becomes necessary.</p><p>The trace cache <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" targ the Pentium4 <ref type=""bibr"" target=""#b13"">[14]</ref>, or enabling dynamic optimization of traces <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b21"">22]</ref>. Our instruction str mechanism. Techniques such as path associativity, partial matching, and inactive instruction issue <ref type=""bibr"" target=""#b8"">[9]</ref> target an improvement in the trace cache hit rate. Branch pr",0
"1"">32]</ref> is one such high fetch width mechanism, recently implemented in the Pentium4 processor <ref type=""bibr"" target=""#b13"">[14]</ref> <ref type=""foot"" target=""#foot_0"">1</ref> . Figure <ref ty ce cache mechanism, like its ability to store decoded instructions for processors like the Pentium4 <ref type=""bibr"" target=""#b13"">[14]</ref>, or enabling dynamic optimization of traces <ref type=""bib",0
"epresentative instruction segment we have analyzed the distribution of basic blocks as described in <ref type=""bibr"" target=""#b35"">[36]</ref>.</p><p>Previous has shown that code layout optimizations h",0
"cycle, as long as they reside sequentially in the same cache line.</p><p>The Alpha EV8 architecture <ref type=""bibr"" target=""#b33"">[34]</ref> uses an interleaved BTB and a multiple branch predictor to or <ref type=""bibr"" target=""#b17"">[18]</ref>, the Alpha EV8 architecture using a 2bcgskew predictor <ref type=""bibr"" target=""#b33"">[34]</ref>, and the trace cache architecture using a trace predictor",0
">11]</ref>, and to align branches to benefit the underlying fetch architecture and branch predictor <ref type=""bibr"" target=""#b3"">[4]</ref>. Our previous work presents a detailed analysis <ref type=""b",0
"ance. The branch address cache <ref type=""bibr"" target=""#b37"">[38]</ref>, and the collapsing buffer <ref type=""bibr"" target=""#b6"">[7]</ref> represent earlier attempts at a fetch architecture capable o -banked instruction cache, so that we can always guarantee a full width of instructions, as done in <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b30"">31]</ref>. Our solution requir",0
"to the decode stage.</p><p>Other multiple branch predictors like the multiple blockahead predictor <ref type=""bibr"" target=""#b34"">[35]</ref>, or the tree-like subgraph predictor <ref type=""bibr"" targ",0
"e=""bibr"" target=""#b8"">[9]</ref> target an improvement in the trace cache hit rate. Branch promotion <ref type=""bibr"" target=""#b20"">[21]</ref> targets an increase in the average trace length, allowing",0
")</ref>. In this paper we use Checkpoint Processing and Recovery (CPR) as the baseline architecture <ref type=""bibr"" target=""#b1"">[2]</ref> since it has been shown to outperform conventional ROB-based rview</head><p>CPR is a ROB-free proposal for building scalable large instruction window processors <ref type=""bibr"" target=""#b1"">[2]</ref>. CPR addresses the scalability and performance limitations o",1
"ow.</p><p>Proposals for resource efficient microarchitectures include Out-oforder commit processors <ref type=""bibr"" target=""#b7"">[8]</ref> and Cherry <ref type=""bibr"" target=""#b14"">[15]</ref>. Out-of #b7"">[8]</ref> and Cherry <ref type=""bibr"" target=""#b14"">[15]</ref>. Out-of-order commit processors <ref type=""bibr"" target=""#b7"">[8]</ref> combine a checkpoint proposal <ref type=""bibr"" target=""#b8"">",0
"ge instruction window processing models have been proposed <ref type=""bibr"" target=""#b19"">[20,</ref><ref type=""bibr"" target=""#b20"">21]</ref>. These processing models significantly change the underlyin",0
"non-blocking design allows for an effectively large scheduler. The Waiting Instruction Buffer (WIB) <ref type=""bibr"" target=""#b13"">[14]</ref> employs a small and fast scheduler backed by a larger buff tion.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""6.2"">CFP and WIB</head><p>The WIB <ref type=""bibr"" target=""#b13"">[14]</ref> also drains load-miss-dependent instructions into a specia =""#b7"">[8]</ref> combine a checkpoint proposal <ref type=""bibr"" target=""#b8"">[9]</ref> with the WIB <ref type=""bibr"" target=""#b13"">[14]</ref> to address scheduler limitations for a checkpoint processo ORK</head><p>Section 3.3 discusses non-blocking schedulers <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b13"">14]</ref>. Various register file organizations have been proposed and",0
"m (e.g., a backward slice of a cache miss) is pre-executed <ref type=""bibr"" target=""#b18"">[19,</ref><ref type=""bibr"" target=""#b22"">23]</ref> on idle contexts of a multithreaded processor prior to enco",0
"l correlation similar to prior studies of spatial footprints <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b16"">17]</ref>. We define a spatial region as a fixed-size portion of the with the code and/or data address that initiates the pattern <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b16"">17]</ref>. Whereas existing spatial pattern prefetching designs are e ow that the cache-coupled structures used in previous work ( <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b16"">17]</ref>) are suboptimal for observing spatial correlation. Accesses rate predictions when correlation table storage is unbounded <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b16"">17]</ref>. By combining both quantities, which we call PC+address ind pproximated by combining the PC with a spatial region offset <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b16"">17]</ref>. The spatial region offset of a data address is the distanc ""4.2."">Indexing</head><p>Prior studies of spatial predictors <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b16"">17]</ref> advocate predictor indices that include address information tries, consequently polluting the PHT.</p><p>Past predictors <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b16"">17]</ref> couple the predictor training structure to a sectored (i.e. y practical implementation evaluated on server workloads provides less than 20% miss rate reduction <ref type=""bibr"" target=""#b16"">[17]</ref>.</p><p>In this paper, we reconsider prediction and streami is work demonstrates: • Effective spatial correlation and prediction. Contrary to previous findings <ref type=""bibr"" target=""#b16"">[17]</ref>, address-based correlation is not needed to predict the ac region generation is defined can significantly impact the accuracy and coverage of spatial patterns <ref type=""bibr"" target=""#b16"">[17]</ref>. A generation must be defined to ensure that, when SMS str t distinguish among distinct access patterns to different data structures by the same code (e.g.,   <ref type=""bibr"" target=""#b16"">[17]</ref>, which indicated that PC+address provides superior coverag experience worse conflict behavior. To mitigate this disadvantage, the spatial footprint predictor <ref type=""bibr"" target=""#b16"">[17]</ref> employed a decoupled sectored cache <ref type=""bibr"" targe",1
"ad><p>We formalize our notion of spatial correlation similar to prior studies of spatial footprints <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b16"">17]</ref>. We define a spatial ed in hardware by correlating patterns with the code and/or data address that initiates the pattern <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b16"">17]</ref>. Whereas existing sp tracking of spatial correlation. We show that the cache-coupled structures used in previous work ( <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b16"">17]</ref>) are suboptimal for dex consistently provides the most accurate predictions when correlation table storage is unbounded <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b16"">17]</ref>. By combining both q lications, PC+address indexing can be approximated by combining the PC with a spatial region offset <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b16"">17]</ref>. The spatial region =""http://www.tei-c.org/ns/1.0""><head n=""4.2."">Indexing</head><p>Prior studies of spatial predictors <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b16"">17]</ref> advocate predictor i coverage and/or fragment prediction entries, consequently polluting the PHT.</p><p>Past predictors <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b16"">17]</ref> couple the predictor eas existing spatial pattern prefetching designs are effective for desktop/engineering applications <ref type=""bibr"" target=""#b3"">[4]</ref>, the only practical implementation evaluated on server workl e highest coverage.</p><p>For scientific applications, we corroborate the conclusions of prior work <ref type=""bibr"" target=""#b3"">[4]</ref> that indicate PC+offset indexing generally approaches the pe led sectored cache <ref type=""bibr"" target=""#b21"">[22]</ref>, whereas the spatial pattern predictor <ref type=""bibr"" target=""#b3"">[4]</ref> provided a logical sectored-cache tag array alongside a trad",1
"nfidence intervals that target ±5% error on change in performance, using pairedmeasurement sampling <ref type=""bibr"" target=""#b30"">[31]</ref>. We launch measurements from checkpoints with warmed cache",0
"r streaming (e.g., <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" tar atial relationships can be exploited through simple prefetching schemes, such as stride prefetching <ref type=""bibr"" target=""#b23"">[24]</ref>.</p><p>Commercial applications exhibit complex access patt",0
"), decision support system (DSS), and web server workloads is spent on memory system-related stalls <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target",0
"arget=""#b2"">3,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b29"">30]</ref>. These workloads ch",0
"20"">[21]</ref>. Software prefetching can accelerate certain database operations, such as hash joins <ref type=""bibr"" target=""#b4"">[5]</ref>. Temporal streaming reduces coherence stalls by streaming re",0
"et=""#b17"">[18,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b28"">29]</ref>), few studies have ss of prediction approaches exploit temporal rather than spatial correlation among addresses (e.g., <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b29"">30]</ref>). These approaches",0
"ardware approaches exploit variations in spatial locality at runtime, including the dual data cache <ref type=""bibr"" target=""#b10"">[11]</ref>, the spatial locality detection table <ref type=""bibr"" tar",0
". Our most significant departure from prior designs is that those designs target decoupled sectored <ref type=""bibr"" target=""#b21"">[22]</ref> or sub-blocked caches. Integrating spatial pattern predict l footprint predictor <ref type=""bibr"" target=""#b16"">[17]</ref> employed a decoupled sectored cache <ref type=""bibr"" target=""#b21"">[22]</ref>, whereas the spatial pattern predictor <ref type=""bibr"" ta",0
"structions a di cult task, even in the presence of aiding devices like a hardware trace cache (HTC) <ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b3"">4]</ref>. Indeed, a trace cach",1
"n code reordering techniques has largely focused on simply reducing the instruction cache miss rate <ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" targe ften ignored trying to reduce the instruction cache miss rate <ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b8"">9]</ref>. Finally, to our knowledge, code reordering techniques have n",0
"arget=""#b10"">11,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b7"">8]</ref>. This approach made sen",0
"ulers still difficult to implement.</p><p>Academic researchers have proposed dataflow prescheduling <ref type=""bibr"" target=""#b5"">[5,</ref><ref type=""bibr"" target=""#b8"">8,</ref><ref type=""bibr"" target so been academic work in attempting to reduce use of the picker. Select-free instruction scheduling <ref type=""bibr"" target=""#b5"">[5]</ref> and grandchild scheduling <ref type=""bibr"" target=""#b30"">[30",1
"-way partitioned entries. Multilevel partitioning techniques <ref type=""bibr"" target=""#b4"">[4,</ref><ref type=""bibr"" target=""#b18"">18]</ref> are a related approach which resemble cache hierarchies by",0
"different heuristics <ref type=""bibr"" target=""#b1"">[1,</ref><ref type=""bibr"" target=""#b22"">22,</ref><ref type=""bibr"" target=""#b23"">23,</ref><ref type=""bibr"" target=""#b26"">26,</ref><ref type=""bibr"" tar",0
"the latest Intel and AMD desktop/server cores only have integer scheduler sizes of 24 to 32 entries <ref type=""bibr"" target=""#b1"">[1,</ref><ref type=""bibr"" target=""#b17"">17]</ref> primarily for latenc d thereby divide and conquer the problem. Partitions can be created by several different heuristics <ref type=""bibr"" target=""#b1"">[1,</ref><ref type=""bibr"" target=""#b22"">22,</ref><ref type=""bibr"" targ",0
"ready instructions for execution, forms a tight loop which is well known as critical to performance <ref type=""bibr"" target=""#b2"">[2]</ref>.</p><p>There are two conventional methods to implement the m",0
"ve high fetch bandwidth, while maintaining the complexity under control, is the stream fetch engine <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b15"">16]</ref>.</p><p>This fetch div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.2"">Fetch Models</head><p>The stream fetch engine <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b15"">16]</ref> model is shown in for the stream fetch engine to provide high fetch bandwidth while requiring low implementation cost <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b15"">16]</ref>. However, having h ww.tei-c.org/ns/1.0""><head n=""5.1"">The Multiple Stream Predictor</head><p>The next stream predictor <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b15"">16]</ref>, which is shown in zed code layout. In addition, data are shown for the original single-stream predictor, described in <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b15"">16]</ref>, and a 2-stream mu p>To avoid this increase in the fetch engine complexity, we propose to use long instruction streams <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b15"">16]</ref> as basic predictio /p><p>Our instruction cache setup uses wide cache lines, that is, 4 times the processor fetch width <ref type=""bibr"" target=""#b10"">[11]</ref>, and 64KB total hardware budget. The trace fetch architect",1
"used by modern technologies, prevent branch prediction tables from being accessed in a single cycle <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b6"">7]</ref>. This fact limits fetc slower wires in modern technologies, cause branch prediction tables to require multi-cycle accesses <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b6"">7]</ref>.</p><p>The trace predi ies and larger wire delays cause branch prediction tables to require multiple cycles to be accessed <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b6"">7]</ref>, limiting the fetch en ggressive 8 fan-out-of-four delays clock period, that is, a 3.47 GHz clock frequency as reported in <ref type=""bibr"" target=""#b0"">[1]</ref>. It has been claimed in <ref type=""bibr"" target=""#b3"">[4]</r",0
"hat techniques for enlarging the streams finalizing in particular branch types achieve poor results <ref type=""bibr"" target=""#b17"">[18]</ref>. This is due to Amdahl's law: although these techniques en",0
"elining the branch predictor and interleaving prediction requests from different threads each cycle <ref type=""bibr"" target=""#b1"">[2]</ref>. Nevertheless, analyzing the accuracy and performance of pip",0
"instructions. The iBTB, the FTB, and the stream fetch architecture use a return address stack (RAS) <ref type=""bibr"" target=""#b8"">[9]</ref> to predict the target address of return instructions. There",0
"have a longer access time, and this may increase the critical path length and penalize performance <ref type=""bibr"" target=""#b0"">[1]</ref>.</p><p>In this paper we propose a novel register renaming ap",1
"buffer used just for renaming and avoids to store the result in the reorder buffer(e.g. PowerPC 604 <ref type=""bibr"" target=""#b11"">[12]</ref>). • A physical register file. In this case there is a phys first one is used by the Intel Pentium Pro <ref type=""bibr"" target=""#b1"">[2]</ref>, the PowerPC 604 <ref type=""bibr"" target=""#b11"">[12]</ref>, and the HAL SPARC64 <ref type=""bibr"" target=""#b2"">[3]</re",0
"chemes are being used in the latest microprocessors. The first one is used by the Intel Pentium Pro <ref type=""bibr"" target=""#b1"">[2]</ref>, the PowerPC 604 <ref type=""bibr"" target=""#b11"">[12]</ref>,",0
"1.0""><head>Functional Unit Count Latency</head><p>Simple The processor has a lookup-free data cache <ref type=""bibr"" target=""#b6"">[7]</ref> that allows up to 8 pending misses to different cache lines.",0
"on for a DEC AlphaStation 600 5/266 with a DEC 21164 processor, is instrumented using the Atom tool <ref type=""bibr"" target=""#b12"">[13]</ref>. The instrumented program is executed and the trace genera",0
"istance in terms of memory references between two accesses to the same memory location. Fang et al. <ref type=""bibr"" target=""#b9"">[9]</ref> define memory distance to include reuse distance, access dis he first store in a sequence of stores writing to the same address with the same value. Fang et al. <ref type=""bibr"" target=""#b9"">[9]</ref> propose a feedback-directed mechanism based upon access dist d method performs much better than the previous access distance based mem-ory disambiguation scheme <ref type=""bibr"" target=""#b9"">[9]</ref>, and yields performance very close to perfect memory disambi ace is small.</p><p>While the above schemes are based on memory dependence predictions, Fang et al. <ref type=""bibr"" target=""#b9"">[9]</ref> have proposed a feedback-directed memory scheme which use me e, the load/store queue size, and machine state. Taking all these factors into account, Fang et al. <ref type=""bibr"" target=""#b9"">[9]</ref> define the access dis-tance of a load as the number of memor filed one. Our method assumes that small store distances are independent of input size. Fang et al. <ref type=""bibr"" target=""#b9"">[9]</ref> have observed that over 80% of the memory instructions in SP n store distances is not negligible, the memory distance analysis framework proposed by Fang et al. <ref type=""bibr"" target=""#b9"">[9]</ref> can be applied to enhance our mechanism by predicting store listed in Table <ref type=""table"" target=""#tab_2"">1</ref>(b). For the access distance based scheme <ref type=""bibr"" target=""#b9"">[9]</ref> as described in Section 2.3.2, we use the test and train inp This last phenomenon can be overcome using the memory distance prediction developed by Fang et al. <ref type=""bibr"" target=""#b9"">[9]</ref>.</p><p>When comparing SD and SS16K, SD suffers from issues r ref type=""bibr"" target=""#b28"">28]</ref> and instructionbased <ref type=""bibr"" target=""#b8"">[8,</ref><ref type=""bibr"" target=""#b9"">9,</ref><ref type=""bibr"" target=""#b13"">13]</ref> reuse distances have ir performance models to calculate cache misses. Fang et al. <ref type=""bibr"" target=""#b8"">[8,</ref><ref type=""bibr"" target=""#b9"">9]</ref> introduce the notion of memory distance to encompass reuse di",1
"rget=""#b5"">[5,</ref><ref type=""bibr"" target=""#b10"">10,</ref><ref type=""bibr"" target=""#b11"">11,</ref><ref type=""bibr"" target=""#b15"">15,</ref><ref type=""bibr"" target=""#b16"">16,</ref><ref type=""bibr"" tar gly better results <ref type=""bibr"" target=""#b11"">[11,</ref><ref type=""bibr"" target=""#b16"">16,</ref><ref type=""bibr"" target=""#b15"">15,</ref><ref type=""bibr"" target=""#b5"">5]</ref>. The problem of memor y disambiguation and communication through memory has been studied extensively by Moshovos and Sohi <ref type=""bibr"" target=""#b15"">[15]</ref>. The dynamic memory disambiguators proposed mainly use ass",0
"get=""#b11"">11,</ref><ref type=""bibr"" target=""#b15"">15,</ref><ref type=""bibr"" target=""#b16"">16,</ref><ref type=""bibr"" target=""#b17"">17]</ref>. While many hardware techniques achieve good performance, m ave shown that memory order violation detection can be based on values, instead of addresses. ?nder <ref type=""bibr"" target=""#b17"">[17]</ref> has proposed a light-weight memory dependence predictor wh",0
"management. Others have developed efficient reuse distance analysis tools to estimate cache misses <ref type=""bibr"" target=""#b1"">[1,</ref><ref type=""bibr"" target=""#b4"">4,</ref><ref type=""bibr"" target /ref><ref type=""bibr"" target=""#b27"">27]</ref> and to evaluate the effect of program transformations <ref type=""bibr"" target=""#b1"">[1,</ref><ref type=""bibr"" target=""#b2"">2,</ref><ref type=""bibr"" target",0
"instructions through one profiling run to generate cache replacement hints for an Itanium processor <ref type=""bibr"" target=""#b3"">[3]</ref>. Marin and Mellor-Crummey <ref type=""bibr"" target=""#b13"">[13",0
"tion that is at the head of the instruction queue 6cycles after the one that triggered the overflow <ref type=""bibr"" target=""#b11"">[12]</ref>. On an Intel Core 2 machine, we observed a similar phenome the other hand, provide instructionbased sampling (IBS) which is similar to the ProfileMe approach <ref type=""bibr"" target=""#b11"">[12]</ref>. Unfortunately, this facility only allows sam-pling instru sed profiling. ProfileMe was proposed hardware support to allow accurate instruction-level sampling <ref type=""bibr"" target=""#b11"">[12]</ref> for Alpha processors. AMD adopts the ProfileMe approach in",1
"mpling to switch between instrumented and noninstrumented code in a controlled, fine-grained manner <ref type=""bibr"" target=""#b15"">[16]</ref>. Finally, stack sampling has been used, without the use of",0
"rry out a whole-program analysis, which is usually extremely expensive and not scalable.</p><p>LIPO <ref type=""bibr"" target=""#b17"">[18]</ref> is a technique aimed at using a lightweight approach to pe",0
"ters. Ammons, Ball, and Larus proposed instrumenting programs to read hardware performance counters <ref type=""bibr"" target=""#b1"">[2]</ref>. By selecting where to reset and sample the counters, the au",0
"have proposed using PMU-based sample profiles to drive other optimizations such as data prefetching <ref type=""bibr"" target=""#b0"">[1]</ref>, these techniques are mainly adopted in dynamic optimizers,",0
"global-history based predictor derived from PPM. PPM was originally introduced for text compression <ref type=""bibr"" target=""#b0"">[1]</ref>, and it was used in <ref type=""bibr"" target=""#b1"">[2]</ref> nd of the PCF, we build S * and sort sequences in decreasing potentials. Then we compute m(T ) from <ref type=""bibr"" target=""#b0"">(1)</ref>. Because of memory limitations on our machines, when |S| exc rting at this step, the content of T is allowed to change dynamically. We can no longer use formula <ref type=""bibr"" target=""#b0"">(1)</ref>, and so now we generate results by performing a second pass",1
"ginally introduced for text compression <ref type=""bibr"" target=""#b0"">[1]</ref>, and it was used in <ref type=""bibr"" target=""#b1"">[2]</ref> for branch prediction. Figure <ref type=""figure"" target=""#fi",0
"re exists a longer matching sequence in T , i.e., s∈T f (s.0 u ). More explanations can be found in <ref type=""bibr"" target=""#b6"">[7]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=",0
"be viewed as a 4 th order approximation to PPM <ref type=""bibr"" target=""#b2"">[3]</ref>, while YAGS <ref type=""bibr"" target=""#b3"">[4]</ref>, which is a GPPM predictor too, can be viewed as a 1 st orde",0
"a synopsis of cbp1.5, which features 5 banks. It can be viewed as a 4 th order approximation to PPM <ref type=""bibr"" target=""#b2"">[3]</ref>, while YAGS <ref type=""bibr"" target=""#b3"">[4]</ref>, which i are not folding a random value, but a global history value derived from the previous history value <ref type=""bibr"" target=""#b2"">[3]</ref>. Figure <ref type=""figure"" target=""#fig_1"">2</ref> shows two",0
""" target=""#b4"">[5]</ref>, Superthreaded <ref type=""bibr"" target=""#b19"">[20]</ref>, Trace Processors <ref type=""bibr"" target=""#b16"">[17]</ref> [21], Speculative Multithreaded <ref type=""bibr"" target=""#",1
"ith <ref type=""bibr"" target=""#b15"">[16]</ref> and also investigated by Sastry, Palacharla and Smith <ref type=""bibr"" target=""#b17"">[18]</ref>, which extends a conventional microarchitecture in order t DG with respect to a node v is defined as the set of nodes from which v can be reached, including v <ref type=""bibr"" target=""#b17"">[18]</ref>. Figure <ref type=""figure"">2</ref> shows the backward slic and, its hardware complexity is negligible.</p><p>The static partitioning proposed by Sastry et al. <ref type=""bibr"" target=""#b17"">[18]</ref> is based on sending to the integer cluster all instruction slice steering. The numbers for the static partitioning have been obtained from the original paper <ref type=""bibr"" target=""#b17"">[18]</ref> and the dynamic approach has been simulated using the same w.tei-c.org/ns/1.0""><head n=""4."">Related Work</head><p>The proposal of Sastry, Palacharla and Smith <ref type=""bibr"" target=""#b17"">[18]</ref> and that of Palacharla, Jouppi and Smith <ref type=""bibr""",0
"rscalar microarchitectures will face significant problems such as the growing impact of wire delays <ref type=""bibr"" target=""#b1"">[2]</ref>  <ref type=""bibr"" target=""#b12"">[13]</ref> and increasing co",0
"figures were obtained through a cyclebased timing simulator based on the SimpleScalar tool set v3.0 <ref type=""bibr"" target=""#b2"">[3]</ref>, which was extended to simulate the architecture described i",0
"he register file access time, which in turn is one of the critical delays of superscalar processors <ref type=""bibr"" target=""#b6"">[7]</ref>. This scheme performs at the same level when there is just o",0
"mory disambiguation for largewindow processors, in which thousands of instructions may be in flight <ref type=""bibr"" target=""#b12"">[12,</ref><ref type=""bibr"" target=""#b14"">14,</ref><ref type=""bibr"" ta",1
"store forwarding (in the case of multiple address matches). We modeled the ULB-LSQ in the sim-alpha <ref type=""bibr"" target=""#b15"">[15]</ref> simulator and simulated single Simpoint regions of 100M fo",0
"Cain and Lipasti's scheme, to reduce the number of loads that have to perform commit time checking <ref type=""bibr"" target=""#b20"">[20]</ref>. These mechanisms eliminate the CAM from the LQ but requir",0
"address-partitioned, forwarding buffer backed up by a centralized, fully associative ageindexed LSQ <ref type=""bibr"" target=""#b1"">[1]</ref>. Baugh and Zilles use a small centralized forwarding buffer, e forwarding buffers <ref type=""bibr"" target=""#b10"">[10,</ref><ref type=""bibr"" target=""#b8"">8,</ref><ref type=""bibr"" target=""#b1"">1]</ref> can also be explained in part by late allocation.</p></div> <",0
"artitioning the LSQ into its three functions, and distributing and address interleaving all of them <ref type=""bibr"" target=""#b3"">[3]</ref>. The three structures are a set associative cache for forwar",0
"arget=""#b7"">8,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b29"">30,</ref><ref type=""bibr"" target=""#b36"">37]</ref>, is that repetitive control flow graph traversals lead to r temporal streaming <ref type=""bibr"" target=""#b34"">[35,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" target=""#b36"">37]</ref> from prefetching approaches that only retrieve a constant n y counting successful prefetches. Hence, like past designs <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b36"">37]</ref>, TIFS uses the next-best option, the Recent heuristic, as i target data accesses <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b36"">37]</ref>. TIFS adds three logical structures to the chip: a set of S cts the anatomy of the SVB. Our SVB design is adapted from <ref type=""bibr"" target=""#b34"">[35,</ref><ref type=""bibr"" target=""#b36"">37]</ref>. The SVB contains a small fully-associative buffer for temp arget=""#b7"">8,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b29"">30,</ref><ref type=""bibr"" target=""#b36"">37]</ref>. These prefetchers target primarily off-chip data reference prefetcher to retrieve instruction-cache blocks ahead of the fetch unit for the rest of the stream <ref type=""bibr"" target=""#b36"">[37]</ref>.</p><p>Figure <ref type=""figure"">5</ref> shows the cumulat the L2 cache (see <ref type=""bibr"">Section 5)</ref>.</p><p>The term temporal stream, introduced in <ref type=""bibr"" target=""#b36"">[37]</ref>, refers to extended sequences of data references that recu",1
"-cache miss sequences directly. Our key observation, inspired by recent studies of data prefetching <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target prefetching approaches that only retrieve a constant number of blocks in response to a miss (e.g., <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" targ sign is based on recent proposals for addresscorrelated prefetch of recurring temporal data streams <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target",0
"L1-I cache; and a single Index Table <ref type=""table"">.</ref> As in prior stream buffer proposals <ref type=""bibr"" target=""#b13"">[14]</ref>, the SVB holds streamed blocks that have not yet been acce tp://www.tei-c.org/ns/1.0""><head n=""5.1.3"">End of stream detection</head><p>In prior stream buffers <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b27"">28]</ref>, in the stride pre",0
"f type=""bibr"" target=""#b5"">[6]</ref>, off-chip data misses <ref type=""bibr"" target=""#b34"">[35,</ref><ref type=""bibr"" target=""#b35"">36]</ref>, and program paths <ref type=""bibr"" target=""#b15"">[16]</ref ting additional addresses distinguishes temporal streaming <ref type=""bibr"" target=""#b34"">[35,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" target=""#b36"">37]</ref> from prefetching ap rior work reports that off-chip temporal datamiss streams exhibit a median length of 8 to 10 blocks <ref type=""bibr"" target=""#b35"">[36]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>",0
"efetch. Pre-execution and speculative threading mechanisms <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b33"">34,</ref><ref type=""bibr"" target=""#b39"">40]</ref> and Runahead Execut",0
"educes the effectiveness of prefetching.</p><p>Recent solutions use the Global History Buffer (GHB) <ref type=""bibr"" target=""#b27"">[28]</ref>, which organizes correlation information by storing recent uce the GHB as a general structure for prefetching streams of temporally correlated memory requests <ref type=""bibr"" target=""#b27"">[28]</ref>. However, when used to record address correlation <ref typ /DC prefetcher, which which learns the deltas, or differences, between consecutive memory addresses <ref type=""bibr"" target=""#b27"">[28]</ref>. Delta correlation allows PC/DC to store all meta-data on http://www.tei-c.org/ns/1.0"" place=""foot"" n=""2"" xml:id=""foot_1"">Using Nesbit and Smith's terminology<ref type=""bibr"" target=""#b27"">[28]</ref>, in which the name before the slash describes the referenc streams based on the PC of the loading instruction, which is known to improve coverage and accuracy <ref type=""bibr"" target=""#b27"">[28,</ref><ref type=""bibr"" target=""#b38"">39,</ref><ref type=""bibr"" ta fetchers, SMS <ref type=""bibr"" target=""#b38"">[39]</ref>, which exploits spatial locality, and PC/DC <ref type=""bibr"" target=""#b27"">[28,</ref><ref type=""bibr"" target=""#b12"">13]</ref>, which uses delta long streams. Rather than use address correlation, other GHBbased prefetchers use delta correlation <ref type=""bibr"" target=""#b27"">[28,</ref><ref type=""bibr"" target=""#b26"">27]</ref>, whose space requi -based prefetchers <ref type=""bibr"" target=""#b28"">[29,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b38"">39,</ref><ref type=""bibr"" tar ype=""bibr"" target=""#b42"">[43]</ref> or address correlation <ref type=""bibr"" target=""#b26"">[27,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b11"">12]</ref>, sacrificing signif",1
"prefetches need to be inserted far from their use.</p><p>Hardware solutions, such as pointer caches <ref type=""bibr"" target=""#b9"">[10]</ref> and hardware jump pointer tables <ref type=""bibr"" target=""#",0
"relation information <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b40"">41,</ref><ref type=""bibr"" target=""#b42"">43]</ref>. Access to this off-chip meta-data increases prediction lat ed prefetchers can amortize the cost of off-chip meta-data access by fetching long temporal streams <ref type=""bibr"" target=""#b42"">[43,</ref><ref type=""bibr"" target=""#b8"">9]</ref>. Unfortunately, temp d memory traffic optimizations for reading and updating the off-chip history buffer and index table <ref type=""bibr"" target=""#b42"">[43]</ref>. These techniques reduce the memory traffic from 3× <ref t /ref><ref type=""bibr"" target=""#b36"">37,</ref><ref type=""bibr"" target=""#b43"">44]</ref> to 1.05-1.75× <ref type=""bibr"" target=""#b42"">[43]</ref> for long streams. Rather than use address correlation, oth erences to traverse the entire chain. As a result, GHB-based designs forsake either PC localization <ref type=""bibr"" target=""#b42"">[43]</ref> or address correlation <ref type=""bibr"" target=""#b26"">[27,",0
"ltering mechanisms to inform a CDP prefetcher about the pointers that are most likely to be fetched <ref type=""bibr"" target=""#b13"">[14]</ref>.</p><p>There are two key differences between the ISB and p tcher whose accuracy is below 40% needs to be throttled down to preserve overall system performance <ref type=""bibr"" target=""#b13"">[14]</ref>.</p><p>Figure <ref type=""figure"" target=""#fig_17"">13</ref>",0
"""#b19"">[20]</ref>. Sair, et al., support irregular streams by introducing a stride length predictor <ref type=""bibr"" target=""#b33"">[34]</ref>.</p><p>Hur and Lin enhance stream prefetchers by adding a",0
"by m orthonormal matrices. We refer the readers to <ref type=""bibr"" target=""#b24"">(Wong, 1967;</ref><ref type=""bibr"" target=""#b0"">Absil et al., 2004)</ref> for details on the Riemannian geometry of th",1
"methods have been previously proposed <ref type=""bibr"" target=""#b25"">(Yamaguchi et al., 1998;</ref><ref type=""bibr"" target=""#b15"">Sakano, 2000;</ref><ref type=""bibr"" target=""#b5"">Fukui &amp; Yamaguch correlation was used in previous works <ref type=""bibr"" target=""#b25"">(Yamaguchi et al., 1998;</ref><ref type=""bibr"" target=""#b15"">Sakano, 2000;</ref><ref type=""bibr"" target=""#b5"">Fukui &amp; Yamaguch d can be extended to any distance described in the paper. There are attempts to use kernels for MSM <ref type=""bibr"" target=""#b15"">(Sakano, 2000)</ref>. However, the kernel is used only to represent d",0
"the Paper</head><p>Although the Projection metric and the Binet-Cauchy metric were previously used <ref type=""bibr"" target=""#b1"">(Chang et al., 2006;</ref><ref type=""bibr"" target=""#b23"">Wolf &amp; Sh",0
"r"" target=""#b15"">Sakano, 2000;</ref><ref type=""bibr"" target=""#b5"">Fukui &amp; Yamaguchi, 2003;</ref><ref type=""bibr"" target=""#b11"">Kim et al., 2007)</ref>. However, these methods adopt an inconsistent i-c.org/ns/1.0""><head n=""5.3.3."">Discriminant Analysis of Canonical Correlations (DCC)</head><p>DCC <ref type=""bibr"" target=""#b11"">(Kim et al., 2007)</ref> can be understood as a nonparametric version",0
"running program. Although previous work has quantified the theoretical benefits of high adaptivity <ref type=""bibr"" target=""#b0"">[1]</ref>, predicting and delivering this adaptation is still an open out-of-order superscalar processor and is similar to spaces that other researchers have considered <ref type=""bibr"" target=""#b0"">[1]</ref>. We vary fourteen different microarchitectural parameters ac org/ns/1.0""><head>IX. PRIOR WORK ON MICROARCHITECTURAL ADAPTIVITY</head><p>Recently, Lee and Brooks <ref type=""bibr"" target=""#b0"">[1]</ref> showed that it is possible to significantly increase process",1
"k et. al. ""Core Fusion"" <ref type=""bibr"" target=""#b8"">[9]</ref> and Tarjan et al. ""Core Federation"" <ref type=""bibr"" target=""#b9"">[10]</ref>. These last two approaches merge simple cores together in o",0
"example. Wavelet analysis has also gained some attention <ref type=""bibr"" target=""#b41"">[42]</ref>, <ref type=""bibr"" target=""#b42"">[43]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>",0
"eline <ref type=""bibr"" target=""#b15"">[16]</ref>, <ref type=""bibr"" target=""#b7"">[8]</ref> and caches <ref type=""bibr"" target=""#b13"">[14]</ref>, <ref type=""bibr"" target=""#b29"">[30]</ref> have been studi f>, instruction working sets <ref type=""bibr"" target=""#b30"">[31]</ref> or conditional branch counts <ref type=""bibr"" target=""#b13"">[14]</ref>, for example. Wavelet analysis has also gained some attent",0
"</ref> looked at multimedia applications characterised by repeated frame processing. Hsu and Kremer <ref type=""bibr"" target=""#b33"">[34]</ref> implemented a compiler algorithm that adapts the voltage a",0
"neration.</p><p>Unlike other trace-driven runtime optimizers for native binary code, such as Dynamo <ref type=""bibr"" target=""#b3"">[4]</ref>, we have both the rich V-ISA and a cooperating code generato amic optimization of programs. Transmeta's CMS <ref type=""bibr"" target=""#b10"">[11]</ref> and Dynamo <ref type=""bibr"" target=""#b3"">[4]</ref> identify and optimize hot traces at runtime, similar to our",1
"Data Structure Analysis to partition the heap into separate pools for each data structure instance <ref type=""bibr"" target=""#b24"">[25]</ref>. Finally, we have shown that the LLVA representation is ri",0
"chitectures we term Virtual Instruction Set Computers (VISC) <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" tar mental benefits could be exploited in potentially unlimited ways by processor designers. Prior work <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b31"">32,</ref><ref type=""bibr"" ta s been in the past. Furthermore, hardware mechanisms can be used to assist these tasks in many ways <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" ta eptions and selfmodifying code to minimize the difficulties faced by previous translators for DAISY <ref type=""bibr"" target=""#b13"">[14]</ref> and Crusoe <ref type=""bibr"" target=""#b10"">[11]</ref>.</p>< tem/38 and AS/400 family <ref type=""bibr"" target=""#b8"">[9]</ref>, the DAISY project at IBM Research <ref type=""bibr"" target=""#b13"">[14]</ref>, Smith et al.'s proposal for Codesigned Virtual Machines i ed to support modern static and dynamic optimization techniques for general-purpose software. DAISY <ref type=""bibr"" target=""#b13"">[14]</ref> developed a dynamic translation scheme for emulating multi as can be seen in DAISY's and Crusoe's translation schemes <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b13"">14]</ref>. Not only is the entire translator program located in ROM,",0
"ot"" methods (e.g., see <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b33"">34]</ref>). Finally, many har",0
"n we discussed in the Introduction).</p><p>Previous authors have developed Typed Assembly Languages <ref type=""bibr"" target=""#b27"">[28,</ref><ref type=""bibr"" target=""#b6"">7]</ref> with goals that gene",0
"iprocessors (SMPs) and distributed shared memory systems (DSMs) <ref type=""bibr"">[19][20]</ref>[21] <ref type=""bibr"" target=""#b21"">[22]</ref>[23] <ref type=""bibr"" target=""#b23"">[24]</ref>[25] <ref typ",0
"b11"">[12]</ref>[16], file system caching <ref type=""bibr"" target=""#b12"">[13]</ref>[15], or RamDisks <ref type=""bibr"" target=""#b13"">[14]</ref>, typically over conventional network interfaces (i.e., Eth",0
"nterfaces. Recent startups like Virident <ref type=""bibr"" target=""#b32"">[32]</ref> and Texas Memory <ref type=""bibr"" target=""#b33"">[33]</ref> propose the use of solid-state storage, such as NAND Flash",0
"0""><head>B. FIFO Queue Benchmark</head><p>A simple FIFO queue implementation was written based upon <ref type=""bibr"" target=""#b7"">[8]</ref>. The queue is based around a ring buffer, with read and writ re operations are needed (i.e. we do not need CAS). For MIPS64 this has been implemented exactly as <ref type=""bibr"" target=""#b7"">[8]</ref>, for Mamba a simple modification was made. The FIFO ring buf",1
"r cores.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>V. RELATED WORK</head><p>Godson-T <ref type=""bibr"" target=""#b8"">[9]</ref> is also a MIPS based multicore architecture that utilizes pr",0
"reads. Much effort has been made to optimize coherency protocols to reduce needless communication ( <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b2"">[3]</ref>). Furthermore, in o",0
"ht construct we call a 'notify chain'. These are similar in structure to MCS queue based spin-locks <ref type=""bibr"" target=""#b6"">[7]</ref> with one crucial difference. Rather than spinning on a parti p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>A. MCS Lock Benchmark</head><p>The MCS Lock <ref type=""bibr"" target=""#b6"">[7]</ref> is a standard way of implementing a scalable spin-lock. Any accesses.</p><p>We have implemented the MCS lock on MIPS64 using the CAS primitive as described in <ref type=""bibr"" target=""#b6"">[7]</ref>. In Mamba a notify chain as described above is used. When a",0
"pping to a software handler if this takes too long. Mamba builds on earlier concepts from Cambridge <ref type=""bibr"" target=""#b12"">[13]</ref>.</p><p>The Intel x86 ISA <ref type=""bibr"" target=""#b13"">[1",0
"as designed to be representative of next-generation sharedmemory programs for chip-multiprocessors"" <ref type=""bibr"" target=""#b2"">[3]</ref>. Our experiments show that for those programs, no matter how </p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2.1"">Benchmarks</head><p>We use PARSEC <ref type=""bibr"" target=""#b2"">[3]</ref> as the benchmark suite. It is a recently released suite desi Because there is no close-form expression for the equation, the program uses numerical computation <ref type=""bibr"" target=""#b2"">[3]</ref>.</p><p>The input data file of this benchmark includes an arr 6MB</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>* : see<ref type=""bibr"" target=""#b2"">[3]</ref> for detail.</note></figure> <figure xmlns=""http://www.tei-c. , as well as systems applications that mimic large-scale multithreaded commercial programs. Studies <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b1"">2]</ref> have shown that the su 0 {0 2 4 6},{1 3 5 7},{0 2 4 6},{1 3 5 7} 1 161.6 0 {0 2 1 3},{4 5 6 7},{0 2 1 3},{4 5 6 7} 4 161. <ref type=""bibr"" target=""#b2"">3</ref> No binding 165.7</p><p>In PARSEC, ferret and dedup are two suc urement are relevant to this current work. Bienia and others <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b2"">3]</ref> have shown a detailed exploration of the characterization of",1
"measure running times, and employ the Performance Application Programming Interface (PAPI) library <ref type=""bibr"" target=""#b3"">[4]</ref> to read memory-related hardware performance counters, includ",0
"ns (a typical interval granularity used in phase detection <ref type=""bibr"" target=""#b18"">[19,</ref><ref type=""bibr"" target=""#b17"">18]</ref>) when the program runs in different thread-core assignments",0
"mimic large-scale multithreaded commercial programs. Studies <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b1"">2]</ref> have shown that the suite covers a wide range of working set d characterization and performance measurement are relevant to this current work. Bienia and others <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b2"">3]</ref> have shown a detailed",0
"rformance considerably. The effectiveness of those techniques has shown on sets of independent jobs <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" targ odern CMP on the performance of contemporary multithreaded applications. Many previous explorations <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" targ for improving the performance of these programs, a contrast to previous results on independent jobs <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b19"">20]</ref> and server programs shared cache has been job co-scheduling including thread clustering. Many job co-scheduling studies <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" targ",0
"linear system is large it is typically solved using an iterative solver such as conjugate gradient <ref type=""bibr"" target=""#b22"">[Shewchuk 1994</ref>] whose inner loop performs the sparse matrixvect",0
"nly way to solve Poisson equations; Fourier transforms can be used to directly calculate a solution <ref type=""bibr"" target=""#b23"">[Simchony et al. 1990</ref>]. However, this approach requires O(n lg",0
"pute multi-viewpoint <ref type=""bibr"" target=""#b2"">[Agarwala et al. 2006</ref>] and video panoramas <ref type=""bibr"" target=""#b1"">[Agarwala et al. 2005]</ref>. Others have confirmed <ref type=""bibr"" t 2</ref>). Most of our results are panoramas whose seams were computed using hierarchical graph cuts <ref type=""bibr"" target=""#b1"">[Agarwala et al. 2005]</ref>, though the first result in Table <ref ty perform gradientdomain compositing for video <ref type=""bibr"" target=""#b29"">[Wang et al. 2004;</ref><ref type=""bibr"" target=""#b1"">Agarwala et al. 2005]</ref>, where scalability concerns are even great",0
"ods <ref type=""bibr"" target=""#b20"">[Saad 2003</ref>] are well-studied and can be adapted to the GPU <ref type=""bibr"" target=""#b5"">[Bolz et al. 2003]</ref>. <ref type=""bibr"" target=""#b27"">Szeliski [200",0
"on, called receiver class prediction optimization (RCPO) <ref type=""bibr"" target=""#b10"">[11]</ref>, <ref type=""bibr"" target=""#b24"">[24]</ref>, <ref type=""bibr"" target=""#b20"">[21]</ref>, <ref type=""bib ll with direct method calls in object-oriented languages <ref type=""bibr"" target=""#b10"">[11]</ref>, <ref type=""bibr"" target=""#b24"">[24]</ref>, <ref type=""bibr"" target=""#b20"">[21]</ref>, <ref type=""bib morphic inline caches <ref type=""bibr"" target=""#b23"">[23]</ref>, and type feedback/devirtualization <ref type=""bibr"" target=""#b24"">[24]</ref>, <ref type=""bibr"" target=""#b28"">[28]</ref>. As we show in",1
""">[11]</ref>, <ref type=""bibr"" target=""#b24"">[24]</ref>, <ref type=""bibr"" target=""#b20"">[21]</ref>, <ref type=""bibr"" target=""#b5"">[6]</ref> or devirtualization <ref type=""bibr"" target=""#b28"">[28]</ref "">[11]</ref>, <ref type=""bibr"" target=""#b24"">[24]</ref>, <ref type=""bibr"" target=""#b20"">[21]</ref>, <ref type=""bibr"" target=""#b5"">[6]</ref>, <ref type=""bibr"" target=""#b28"">[28]</ref>. Ishizaki et al. direct calls <ref type=""bibr"" target=""#b20"">[21]</ref>, <ref type=""bibr"" target=""#b17"">[18]</ref>, <ref type=""bibr"" target=""#b5"">[6]</ref>, as shown in Fig. <ref type=""figure"" target=""#fig_0"">11b</re tion call and its devirtualized form. usually has higher accuracy than an indirect branch predictor <ref type=""bibr"" target=""#b5"">[6]</ref>. However, not all indirect calls can be converted to multipl form RCPO, the following conditions need to be fulfilled <ref type=""bibr"" target=""#b17"">[18]</ref>, <ref type=""bibr"" target=""#b5"">[6]</ref>:</p><p>1. The number of frequent target addresses from a cal",1
"f type=""bibr"" target=""#b20"">[21]</ref>, <ref type=""bibr"" target=""#b5"">[6]</ref> or devirtualization <ref type=""bibr"" target=""#b28"">[28]</ref>. This optimization statically converts an indirect branch ly a subset of indirect branches with a limited number of targets that can be determined statically <ref type=""bibr"" target=""#b28"">[28]</ref>. Our proposed VPC prediction mechanism provides the benefi 24"">[24]</ref>, <ref type=""bibr"" target=""#b20"">[21]</ref>, <ref type=""bibr"" target=""#b5"">[6]</ref>, <ref type=""bibr"" target=""#b28"">[28]</ref>. Ishizaki et al. <ref type=""bibr"" target=""#b28"">[28]</ref> <ref type=""bibr"" target=""#b5"">[6]</ref>, <ref type=""bibr"" target=""#b28"">[28]</ref>. Ishizaki et al. <ref type=""bibr"" target=""#b28"">[28]</ref> classify the devirtualization techniques into guarded devi on can overcome this limitation, but it requires an expensive mechanism called on-stack replacement <ref type=""bibr"" target=""#b28"">[28]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head tion based on static analysis requires type analysis, which in turn requires whole program analysis <ref type=""bibr"" target=""#b28"">[28]</ref>, and unsafe languages like Cþþ also require pointer alias or large applications. Due to the limited applicability of static devirtualization, Ishizaki et al. <ref type=""bibr"" target=""#b28"">[28]</ref> report only an average 40 percent reduction in the number et=""#b23"">[23]</ref>, and type feedback/devirtualization <ref type=""bibr"" target=""#b24"">[24]</ref>, <ref type=""bibr"" target=""#b28"">[28]</ref>. As we show in Section 6, the benefit of devirtualization",1
"zation (RCPO) <ref type=""bibr"" target=""#b10"">[11]</ref>, <ref type=""bibr"" target=""#b24"">[24]</ref>, <ref type=""bibr"" target=""#b20"">[21]</ref>, <ref type=""bibr"" target=""#b5"">[6]</ref> or devirtualizati ted languages <ref type=""bibr"" target=""#b10"">[11]</ref>, <ref type=""bibr"" target=""#b24"">[24]</ref>, <ref type=""bibr"" target=""#b20"">[21]</ref>, <ref type=""bibr"" target=""#b5"">[6]</ref>, <ref type=""bibr"" ircle class at runtime, the compiler can convert the indirect call to multiple guarded direct calls <ref type=""bibr"" target=""#b20"">[21]</ref>, <ref type=""bibr"" target=""#b17"">[18]</ref>, <ref type=""bib",1
"orithm is inspired by a compiler optimization, called receiver class prediction optimization (RCPO) <ref type=""bibr"" target=""#b10"">[11]</ref>, <ref type=""bibr"" target=""#b24"">[24]</ref>, <ref type=""bib s the substitution of an indirect method call with direct method calls in object-oriented languages <ref type=""bibr"" target=""#b10"">[11]</ref>, <ref type=""bibr"" target=""#b24"">[24]</ref>, <ref type=""bib nce impact due to virtual function calls. These approaches include the method cache in Smalltalk-80 <ref type=""bibr"" target=""#b10"">[11]</ref>, polymorphic inline caches <ref type=""bibr"" target=""#b23"">",0
"ls into direct function calls (e.g., the Bartok compiler <ref type=""bibr"" target=""#b48"">[48]</ref>, <ref type=""bibr"" target=""#b42"">[42]</ref> or the .NET Runtime <ref type=""bibr"" target=""#b43"">[43]</r",0
""">[12]</ref>, <ref type=""bibr"" target=""#b33"">[33]</ref>, <ref type=""bibr"" target=""#b12"">[13]</ref>, <ref type=""bibr"" target=""#b13"">[14]</ref>, <ref type=""bibr"" target=""#b47"">[47]</ref> require large h le indirect branch predictors using a cascaded predictor <ref type=""bibr"" target=""#b12"">[13]</ref>, <ref type=""bibr"" target=""#b13"">[14]</ref>. The cascaded predictor is a hybrid of two or more target ex second-stage predictor is used to predict hard-topredict indirect branches. Driesen and Ho ¨lzle <ref type=""bibr"" target=""#b13"">[14]</ref> concluded that a three-stage cascaded predictor performed ed indirect branch predictors are themselves complicated <ref type=""bibr"" target=""#b12"">[13]</ref>, <ref type=""bibr"" target=""#b13"">[14]</ref>, <ref type=""bibr"" target=""#b33"">[33]</ref>, <ref type=""bib",0
"ediction using a set of modern object-oriented Java applications, the full set of DaCapo benchmarks <ref type=""bibr"" target=""#b3"">[4]</ref>. Our goal is to demonstrate the benefits of VPC prediction o",0
"or <ref type=""bibr"" target=""#b26"">[26]</ref> which includes a specialized indirect branch predictor <ref type=""bibr"" target=""#b19"">[20]</ref>. The data are collected with hardware performance counters tium M and AMD Barcelona implement specialized hardware to help the prediction of indirect branches <ref type=""bibr"" target=""#b19"">[20]</ref>, <ref type=""bibr"" target=""#b1"">[2]</ref>, demonstrating th",0
"duced if the processor already supports the prediction of multiple conditional branches in parallel <ref type=""bibr"" target=""#b51"">[51]</ref>. The prediction logic can perform the calculation of VPCA",0
"osed to reduce aliasing in conditional branch predictors <ref type=""bibr"" target=""#b41"">[41]</ref>, <ref type=""bibr"" target=""#b8"">[9]</ref> can also be used to reduce aliasing due to VPC prediction. H",0
"he branch target buffer (BTB) to predict indirect branches <ref type=""bibr"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b21"">[22]</ref>, <ref type=""bibr"" target=""#b34"">[34]</ref>. The BTB implic",0
"essor implementations with limited number of read/write ports in the BTB or the direction predictor <ref type=""bibr"" target=""#b40"">[40]</ref>.)</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head",0
"based on class hierarchy analysis in the ipa-branch experimental branch, but not in the main branch <ref type=""bibr"" target=""#b44"">[44]</ref>.</p><p>iDNA <ref type=""bibr"" target=""#b2"">[3]</ref> is a d",0
"xmlns=""http://www.tei-c.org/ns/1.0""><head n=""7.1"">Methodology</head><p>We have built an iDNA-based <ref type=""bibr"" target=""#b2"">[3]</ref> cycle-accurate x86 simulator to evaluate VPC prediction on J perimental branch, but not in the main branch <ref type=""bibr"" target=""#b44"">[44]</ref>.</p><p>iDNA <ref type=""bibr"" target=""#b2"">[3]</ref> is a dynamic binary instrumentation tool similar to Pin <ref",0
"nvert the indirect call to multiple guarded direct calls <ref type=""bibr"" target=""#b20"">[21]</ref>, <ref type=""bibr"" target=""#b17"">[18]</ref>, <ref type=""bibr"" target=""#b5"">[6]</ref>, as shown in Fig. imizations. The compiler could inline the direct function calls or perform interprocedural analysis <ref type=""bibr"" target=""#b17"">[18]</ref>. Removing function calls also reduces the register save/ r tiple conditional branches. In order to perform RCPO, the following conditions need to be fulfilled <ref type=""bibr"" target=""#b17"">[18]</ref>, <ref type=""bibr"" target=""#b5"">[6]</ref>:</p><p>1. The num",0
""">[30]</ref> recently proposed handling hard-to-predict indirect branches using dynamic predication <ref type=""bibr"" target=""#b36"">[36]</ref>. In this technique, if the target address of an indirect b ruction set architecture, and significant hardware support for dynamic predication (as described in <ref type=""bibr"" target=""#b36"">[36]</ref>). However, the two approaches can be combined and used tog",0
"with multiple conditional branch mispredictions, if the guard tests become hard-to-predict branches <ref type=""bibr"" target=""#b48"">[48]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head y provably monomorphic virtual function calls into direct function calls (e.g., the Bartok compiler <ref type=""bibr"" target=""#b48"">[48]</ref>, <ref type=""bibr"" target=""#b42"">[42]</ref> or the .NET Run",0
"table"">3</ref> provides a brief description of the other two Cþþ benchmarks.</p><p>We use Pinpoints <ref type=""bibr"" target=""#b45"">[45]</ref> to select a representative simulation region for each benc",0
"get=""#b12"">13,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" tar milar recent proposals <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b22"">23]</ref>, a sequence of succ target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" tar arate the storage of address sequences and correlation data <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b26"">27]</ref>. A history buffer r -proposed prefetch meta-data organization where misses are logged continuously in a circular buffer <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b26"">27]</ref>. By separating ind ddress-correlating prefetchers beyond our idealized implementation of the proposed prior techniques <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b26"">27]</ref>. However, improved et=""#b12"">[13,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b20"">21]</ref>. However, correlation table storage requirements are propor e=""bibr"" target=""#b26"">[27]</ref> and the predictor organization of the Global History Buffer (GHB) <ref type=""bibr"" target=""#b20"">[21]</ref>. After we describe the basic hardware operation, the follo ssor equipped with STMS. STMS comprises on-chip prefetch buffers and queues and offchip index table <ref type=""bibr"" target=""#b20"">[21]</ref> and history buffers <ref type=""bibr"" target=""#b20"">[21]</r rs and queues and offchip index table <ref type=""bibr"" target=""#b20"">[21]</ref> and history buffers <ref type=""bibr"" target=""#b20"">[21]</ref> 1 . The prefetch buffers and queues, located along side th associative table design limits maximum prefetch sequence length (referred to as the prefetch depth <ref type=""bibr"" target=""#b20"">[21]</ref>) based on the size of a table entry. Because storage cost,",1
"e, efforts are under way to restructure server workloads to increase on-chip data sharing and reuse <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b18"">19]</ref>. Although these ef",0
"rget=""#b9"">10,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" tar s entirely on-chip <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b20"">21]</ref>. However, correlati gabytes of storage <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b17"">18]</ref>.</p><p>To improve practicality, recent address-correlating /ref>, or trigger prefetchers earlier to improve lookahead <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b17"">18]</ref>.</p><p>The key limitation of pairwise-correlating prefetche",0
"key requirements to make offchip meta-data practical: (1) minimal off-chip metadata lookup latency, <ref type=""bibr"" target=""#b1"">(2)</ref> bandwidth-efficient meta-data updates, and (3) off-chip look her predictors that can make use of the on-chip cache hierarchy to provide ""virtual"" on-chip lookup <ref type=""bibr"" target=""#b1"">[2]</ref>, address correlation tables are substantially larger than th",0
"rage <ref type=""bibr"" target=""#b12"">[13]</ref>, or trigger prefetchers earlier to improve lookahead <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b17"">18]</ref>.</p><p>The key lim",0
"hat are not critical, as addressed in Equation <ref type=""formula"">2</ref>. Such voltage clustering <ref type=""bibr"" target=""#b9"">9</ref> makes additional demands on computeraided design tools. Future",0
"ronous techniques can play an important role in globally asynchronous, locally synchronous systems. <ref type=""bibr"" target=""#b1"">2</ref> Such systems reduce clock power and help with the growing prob",0
"em stores the program in compressed form and decompresses it on the fly, typically on a cache miss. <ref type=""bibr"" target=""#b6"">7</ref> Reducing memory size translates to power savings. It also redu",0
"blished work on MCTS, to provide the reader Fig. <ref type=""figure"">1</ref>. The basic MCTS process <ref type=""bibr"" target=""#b16"">[17]</ref>.</p><p>with the tools to solve new problems using MCTS and basic MCTS process is conceptually very simple, as shown in Figure <ref type=""figure"">1</ref> (from <ref type=""bibr"" target=""#b16"">[17]</ref>). A tree 1 is built in an incremental and asymmetric manne t two moves and uses LGR-1 if there is no LGR-2 entry for the last two moves.</p><p>Baier and Drake <ref type=""bibr"" target=""#b16"">[17]</ref> propose an extension to LGR-1 and LGR-2 called Last Good R ests using the Last Good Reply heuristic (6.1.8) to inform simulations, modified by Baier and Drake <ref type=""bibr"" target=""#b16"">[17]</ref> to include the forgetting of bad moves. Most programs use",1
"e already stronger than the best human players even before MCTS methods were applied.</p><p>Nijssen <ref type=""bibr"" target=""#b152"">[152]</ref> developed a UCT player for Othello called MONTHELLO and",0
"states, or of stateaction pairs. Under certain circumstances the algorithms may even be equivalent <ref type=""bibr"" target=""#b202"">[201]</ref>, but TDL algorithms do not usually build trees, and the message-passing parallelisation for efficient use of clusters <ref type=""bibr"">(6.3)</ref>. Silver <ref type=""bibr"" target=""#b202"">[201]</ref> uses temporal difference learning methods (4.3.1) to ext",0
"</p><p>Kloetzer has studied MCTS approaches to Amazons <ref type=""bibr"" target=""#b116"">[116]</ref>, <ref type=""bibr"" target=""#b114"">[114]</ref> culminating in a PhD thesis on the topic <ref type=""bibr",0
"ei-c.org/ns/1.0""><head n=""4.9.1"">Reflexive Monte Carlo Search</head><p>Reflexive Monte Carlo search <ref type=""bibr"" target=""#b38"">[39]</ref> works by conducting several recursive layers of Monte Carl , beating the existing human record of 68 moves and AI record of 74 moves using simulated annealing <ref type=""bibr"" target=""#b38"">[39]</ref>. Cazenave then applied nested Monte Carlo search (NMCS) (4",0
"that in their experiment, ignoring warm-up could result in an error as high as 15% in simulated CPI <ref type=""bibr"" target=""#b11"">[12]</ref>. Thus adequate warm-up is critical to the accuracy of samp hniques. For example, MRRL is claimed to have achieved 90% of the maximum possible simulation speed <ref type=""bibr"" target=""#b11"">[12]</ref>. However, careful analysis of the experiment reveals funct <ref type=""foot"" target=""#foot_0"">1</ref> . Fifty 1 million-instruction sampling units are used in <ref type=""bibr"" target=""#b11"">[12]</ref>. Suppose that a benchmark is 100 billion instructions long struction caches. In their second technique, they measure the Memory Reference Reuse Latency (MRRL) <ref type=""bibr"" target=""#b11"">[12]</ref>, which refers to the elapsed time measured in number of in oose a sampling unit size of 1 million instructions. This sampling unit size was used in MRRL paper <ref type=""bibr"" target=""#b11"">[12]</ref>, and Variance SimPoint <ref type=""bibr"" target=""#b14"">[15] redicts.</p><p>For MRRL, we choose the p-value to be 99.9%, which is the default value suggested in <ref type=""bibr"" target=""#b11"">[12]</ref>. For BLRL, we use the p-value of 90%. Both methods are als p://www.tei-c.org/ns/1.0"" place=""foot"" n=""2"" xml:id=""foot_1""><p>No warm up length number is given in<ref type=""bibr"" target=""#b11"">[12]</ref>. This number is based on our experiment with MRRL. See Sec",1
"pling units to warm up the pipeline. The profiler for MRRL was downloaded from its author's website <ref type=""bibr"" target=""#b17"">[18]</ref>.</p><p>Although not implemented in the current simulator,",1
"al proposed the Boundary Line Reuse Latency (BLRL<ref type=""foot"" target=""#foot_2"">3</ref> ) method <ref type=""bibr"" target=""#b9"">[10]</ref>, in which every memory reference in a sampling unit is dire tei-c.org/ns/1.0"" place=""foot"" n=""3"" xml:id=""foot_2""><p>The method was not given an official name in<ref type=""bibr"" target=""#b9"">[10]</ref>. It is called ""Boundary Line Reuse Latency"" because it is e",1
"tab_0"">1</ref>, are used in our experiment. The programs, downloaded from the SimpleScalar web site <ref type=""bibr"" target=""#b15"">[16]</ref>, are compiled for the Alpha ISA. All the experiments are d",0
"he sample size. Recently, Wunderlich et al. applied sampling theory to microarchitecture simulation <ref type=""bibr"" target=""#b1"">[2]</ref>. Under the assumption of no measuring error, they showed tha caches is to do cache simulation throughout the benchmark execution. This is how the SMARTS scheme <ref type=""bibr"" target=""#b1"">[2]</ref> does the warm-up. The simulator switches between functional path execution on the caches during functional warm-up. It has been shown that this error is small <ref type=""bibr"" target=""#b1"">[2]</ref>  <ref type=""bibr"" target=""#b2"">[3]</ref>. Although this warm processor configuration used in our experiment. This configuration is adapted from the SMARTS paper <ref type=""bibr"" target=""#b1"">[2]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n= imulator executes 4,000 instructions in cycle accurate mode to warm up the pipeline as suggested in <ref type=""bibr"" target=""#b1"">[2]</ref>, and then the CPI of 1 million instruction sampling unit is rm-up. In this simulation the caches are always simulated between every sampling units as in SMARTS <ref type=""bibr"" target=""#b1"">[2]</ref>. The sampling units and the cycleaccurate warm-up are the sa",0
"r sampling methods such as SimPoint <ref type=""bibr"" target=""#b13"">[14]</ref> and Variance SimPoint <ref type=""bibr"" target=""#b14"">[15]</ref>, where a small number of relatively large sampling units a g unit size was used in MRRL paper <ref type=""bibr"" target=""#b11"">[12]</ref>, and Variance SimPoint <ref type=""bibr"" target=""#b14"">[15]</ref>.</p><p>In this section, each benchmark execution is divide",0
"techniques to determine the warm-up length for a sampling unit. The Minimal Subset Evaluation (MSE) <ref type=""bibr"" target=""#b8"">[9]</ref> technique uses formulas derived from combinatorics and proba",0
"s not easy, but monitoring the warm-up process with Vengroff et al's deterministic finite automaton <ref type=""bibr"" target=""#b16"">[17]</ref> may be much simpler. Extending SMA to warm up other struct",0
"lways warming up the cache makes distributed simulation hard. For sampling methods such as SimPoint <ref type=""bibr"" target=""#b13"">[14]</ref> and Variance SimPoint <ref type=""bibr"" target=""#b14"">[15]<",0
"block will be a hit or a miss. Such references are referred to as coldstart references. Laha et al <ref type=""bibr"" target=""#b6"">[7]</ref> proposed not counting these cold-start references when calcu",0
"lled ""PRIME"" scheme <ref type=""bibr"" target=""#b5"">[6]</ref> following Crowley et. al.'s terminology <ref type=""bibr"" target=""#b3"">[4]</ref>.</p><p>At an extreme, zero instructions are used to warm up",0
"ef>, are compiled for the Alpha ISA. All the experiments are done on our modified SimpleScalar v3.0 <ref type=""bibr"" target=""#b0"">[1]</ref>. Table <ref type=""table"" target=""#tab_1"">2</ref> shows the m",0
"tional warm-up. It has been shown that this error is small <ref type=""bibr"" target=""#b1"">[2]</ref>  <ref type=""bibr"" target=""#b2"">[3]</ref>. Although this warm-up scheme is by far the most accurate, i",0
"end to sustain only a small fraction of the peak bandwidth <ref type=""bibr"" target=""#b27"">[27,</ref><ref type=""bibr"" target=""#b37"">37]</ref>. The end result is either a significant performance hit, or ed descriptions in <ref type=""bibr"" target=""#b11"">[11,</ref><ref type=""bibr"" target=""#b12"">12,</ref><ref type=""bibr"" target=""#b37"">37]</ref> on DRAM systems and in <ref type=""bibr"" target=""#b6"">[6,</r r, the FR-FCFS (first-ready first-come first-serve) policy <ref type=""bibr"" target=""#b36"">[36,</ref><ref type=""bibr"" target=""#b37"">37]</ref> provides the best average performance. Among all ready comm hich was shown to be the best-performing policy on average <ref type=""bibr"" target=""#b36"">[36,</ref><ref type=""bibr"" target=""#b37"">37]</ref>, (2) a conventional inorder memory controller <ref type=""bi average) in our more aggressive setup.</p><p>Rixner et al. <ref type=""bibr"" target=""#b36"">[36,</ref><ref type=""bibr"" target=""#b37"">37]</ref> examine various DRAM command scheduling policies and propos ]</ref>) with both a realistic, contemporary controller design (using the FR-FCFS scheduling policy <ref type=""bibr"" target=""#b37"">[37,</ref><ref type=""bibr"" target=""#b49"">49]</ref>), and an optimisti erings and interleavings of DRAM commands result in different levels of DRAM throughput and latency <ref type=""bibr"" target=""#b37"">[37]</ref>. Finding a good schedule is not an easy task as scheduling >Current memory controllers use relatively simple policies to schedule DRAM accesses. Rixner et al. <ref type=""bibr"" target=""#b37"">[37]</ref> show that none of the fixed policies studied provide the b 6"">[36,</ref><ref type=""bibr"" target=""#b37"">37]</ref>, (2) a conventional inorder memory controller <ref type=""bibr"" target=""#b37"">[37]</ref>, and (3) an optimistic (i.e., ideally efficient) scheduler significantly underperforms the baseline FR-FCFS controller, in line with previous research results <ref type=""bibr"" target=""#b37"">[37]</ref>.</p><p>Figure <ref type=""figure"" target=""#fig_13"">8</ref> ta bus utilization in our FR-FCFS baseline is consistent with what is reported in previous research <ref type=""bibr"" target=""#b37"">[37]</ref> and by DRAM manufacturers <ref type=""bibr"" target=""#b27"">[ arget=""#b37"">[37]</ref> and by DRAM manufacturers <ref type=""bibr"" target=""#b27"">[27]</ref>. Rixner <ref type=""bibr"" target=""#b37"">[37]</ref> reported an average utilization of approximately 35% for a get=""#b25"">25,</ref><ref type=""bibr"" target=""#b30"">30,</ref><ref type=""bibr"" target=""#b36"">36,</ref><ref type=""bibr"" target=""#b37"">37,</ref><ref type=""bibr"" target=""#b38"">38,</ref><ref type=""bibr"" tar",1
"Aware Schedulers</head><p>Most memory controller proposals <ref type=""bibr"" target=""#b17"">[17,</ref><ref type=""bibr"" target=""#b25"">25,</ref><ref type=""bibr"" target=""#b30"">30,</ref><ref type=""bibr"" tar propose the FR-FCFS policy. Hong et al. <ref type=""bibr"" target=""#b16"">[16]</ref> and McKee et al. <ref type=""bibr"" target=""#b25"">[25]</ref> describe policies that reorder accesses from different str",0
"ither a significant performance hit, or an over-provisioned (and therefore expensive) memory system <ref type=""bibr"" target=""#b13"">[13,</ref><ref type=""bibr"" target=""#b22"">22,</ref><ref type=""bibr"" ta",0
"are memory controller proposals have been published lately <ref type=""bibr"" target=""#b29"">[29,</ref><ref type=""bibr"" target=""#b31"">31,</ref><ref type=""bibr"" target=""#b34"">34]</ref>.</p><p>Providing Qo ssors. QoS-aware memory controllers were recently proposed <ref type=""bibr"" target=""#b29"">[29,</ref><ref type=""bibr"" target=""#b31"">31,</ref><ref type=""bibr"" target=""#b34"">34]</ref> to provide fair acc any potential fairness problems across threads, we evaluate Nesbit et al.'s Fair Queueing scheduler <ref type=""bibr"" target=""#b31"">[31]</ref>. Figure <ref type=""figure"" target=""#fig_21"">16</ref> shows",0
"thms have been used to find k-way partitions, spectral clusterings, and separators in planar graphs <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target",1
"5]</ref>. The analysis of the Nibble algorithm is based on a mixing result by Lov?sz and Simonovits <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b9"">10]</ref>, which shows that cut lity from Lemma 3 to relate apr(?, s, r) to itself. In contrast, the proof of Lov?sz and Simonovits <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b9"">10]</ref> relates the walk dist",0
"Vectors</head><p>PageRank was introduced by Brin and Page <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b2"">3]</ref>. For convenience, we introduce a lazy variation of PageRank,",0
"ed algorithm for computing approximate PageRank vectors. We use a technique introduced by Jeh-Widom <ref type=""bibr"" target=""#b6"">[7]</ref>, and further developed by Berkhin in his Bookmark Coloring A re some useful properties of PageRank vectors (also see <ref type=""bibr"" target=""#b5"">[6]</ref> and <ref type=""bibr"" target=""#b6"">[7]</ref>). The proofs are given in the Appendix.</p><p>Proposition 1. uch that vol(Supp(p)) ? 1 ? . We remark that this algorithm is based on the algorithms of Jeh-Widom <ref type=""bibr"" target=""#b6"">[7]</ref> and Berkhin <ref type=""bibr"" target=""#b0"">[1]</ref>, both of ntext-sensitive search <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b6"">7]</ref>. The preference vectors used in our algorithms have all proba",0
"ide personalized search ranking and context-sensitive search <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b6"">7]</ref>. The preference vectors .</formula><p>Applying this rearranged transformation to a starting distribution s yields equation <ref type=""bibr"" target=""#b4"">(5)</ref>.</p><formula xml:id=""formula_15"">pr(?, s) = sR ? = ?s + (1 - xml:id=""formula_36"">p = p + ?r(u)? u . r = r -r(u)? u + (1 -?)r(u)? u W.</formula><p>Using equation <ref type=""bibr"" target=""#b4"">(5)</ref>,</p><formula xml:id=""formula_37"">p + pr(?, r) = p + pr(?, r",0
"cted dependence violations -squashing those threads where parallelism was not found. Helper threads <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target",1
"erage helper thread length of 34 instructions. In the Transactional Coherency and Consistency paper <ref type=""bibr"" target=""#b5"">[6]</ref> that investigates the characteristics of common transactions t an architecture that supports transactional memory can do some amount of thread-level speculation <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" targ",0
"ediction algorithm <ref type=""bibr"" target=""#b39"">[40,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b31"">32]</ref>. If values are inco",0
"history).</p><p>Several papers examine branch prediction on simultaneous multithreading processors <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b28"">29]</ref>. In general, branc",0
"on paradigms. Clearly, a huge barrier to the use of execution models with data-flow characteristics <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b32"">33]</ref> on a conventional a",0
"t improvement of 3.2% compared to the two-bit TC-AGE baseline (analogue of SRRIP for exclusive LLC) <ref type=""bibr"" target=""#b5"">[5]</ref> while saving 66.6% data write transactions from the L2 cache ecisions and assign insertion ages to the non-bypassed blocks in the context of exclusive L3 caches <ref type=""bibr"" target=""#b5"">[5]</ref>. LLC insertion and replacement policies based on static and ache blocks presented in an earlier study in the context of a cache hierarchy with an exclusive LLC <ref type=""bibr"" target=""#b5"">[5]</ref>. According to the terminology used in that study, the set C0 st two extra state bits (S1, S0) per L2 cache block (as opposed to three bits per L2 cache block in <ref type=""bibr"" target=""#b5"">[5]</ref>). Figure <ref type=""figure"" target=""#fig_4"">4</ref>, through all L2 cache evictions and decides the insertion age of a block based on the two-bit TC-AGE policy <ref type=""bibr"" target=""#b5"">[5]</ref>. This policy is the analogue of SRRIP for exclusive LLCs. It in this discussion to conform to the terminology used in the prior work on exclusive LLC management<ref type=""bibr"" target=""#b5"">[5]</ref>. Age can be considered synonymous to RRPV in this discussion",1
"rget=""#b6"">[6,</ref><ref type=""bibr"" target=""#b12"">12,</ref><ref type=""bibr"" target=""#b13"">13,</ref><ref type=""bibr"" target=""#b14"">14,</ref><ref type=""bibr"" target=""#b16"">16,</ref><ref type=""bibr"" tar",0
"manage LLCs shared between CPU workloads and GPGPU workloads in a CPU-GPU heterogeneous environment <ref type=""bibr"" target=""#b17"">[17]</ref>. Further, the PACMan family of policies is shown to outper",0
"IP. The bar on the right in each group shows the number of LLC misses in Belady's optimal algorithm <ref type=""bibr"" target=""#b1"">[1,</ref><ref type=""bibr"" target=""#b20"">20]</ref> normalized to the ba",0
"e in cache bypassing by dynamically segmenting the LLC between referenced and not referenced blocks <ref type=""bibr"" target=""#b11"">[11]</ref>. PC-less light-weight dead block predictors exploiting the",0
"</p><p>Finally, a recent trend in computer graphics has been the use of rendered images as textures <ref type=""bibr"" target=""#b2"">[3]</ref>. As a result, it has become desirable to unify the framebuff accessed in parallel is possible if the texels are stored in a morton order within the cache lines <ref type=""bibr"" target=""#b2"">[3]</ref>. Morton order implies that the texels are stored in 2x2 bloc",1
"ed Graphics Port (AGP) <ref type=""bibr"" target=""#b0"">[1]</ref> from Intel Corporation, Magic Carpet <ref type=""bibr"" target=""#b11"">[12]</ref> from MIPS Technologies, and Talisman <ref type=""bibr"" targ",0
"e Visual Instruction Set (VIS) in UltraSPARC TM <ref type=""bibr"" target=""#b15"">[16]</ref> and FBRAM <ref type=""bibr"" target=""#b17"">[18]</ref> from Sun Microsystems, MMX TM Technology <ref type=""bibr""",0
"mising approach for rendering directly from compressed textures has been proposed in the literature <ref type=""bibr"" target=""#b1"">[2]</ref>. In future work, it would be interesting to study the intera",0
"ms, MMX TM Technology <ref type=""bibr"" target=""#b18"">[19]</ref> and Accelerated Graphics Port (AGP) <ref type=""bibr"" target=""#b0"">[1]</ref> from Intel Corporation, Magic Carpet <ref type=""bibr"" target",0
"ue of memory instructions. Chrysos and Emer proposed using Store Sets to predict memory dependences <ref type=""bibr"" target=""#b2"">[3]</ref>. They show that an out-of-order processor using store sets a rresponding store is added to the load's store set.</p><p>The store set implementation presented in <ref type=""bibr"" target=""#b2"">[3]</ref> uses a pair of tables to predict memory dependences. The fir ssive processor model that is comparable to the configuration used in the original store sets study <ref type=""bibr"" target=""#b2"">[3]</ref> and the 8-wide configuration from the Stack Value File study e sets based memory bypassing. In both cases, we also use the set merging optimization described in <ref type=""bibr"" target=""#b2"">[3]</ref>, which we found to improve performance slightly. We simulate . Chrysos and Emer proposed store sets for memory dependence prediction which we used in this study <ref type=""bibr"" target=""#b2"">[3]</ref>. The Alpha 21264 speculatively issues loads, but uses a simp",1
"several renaming-based techniques for move elimination, memory disambiguation and memory bypassing <ref type=""bibr"" target=""#b3"">[4]</ref>. Moshovos and Sohi propose memory cloaking and bypassing to",0
"ructions, and then we plotted the store-load dependence graphs with VCG, a graph visualization tool <ref type=""bibr"" target=""#b12"">[13]</ref>. The graph for an invocation of the node to heap function",0
"of aggressive load speculation techniques and examine the effects of combining different approaches <ref type=""bibr"" target=""#b1"">[2]</ref>. They show that memory dependence prediction and value predi",0
"squashes and reissues those instructions that are actually data-dependent on the misspeculated load <ref type=""bibr"" target=""#b11"">[12]</ref>. This results in higher amounts of instruction-level paral",0
"e normalized maximum load ? for Fennel, the previously bestknown heuristic (linear weighted degrees <ref type=""bibr"" target=""#b28"">[29]</ref>) and hash partitioning of vertices for the Twitter graph w balanced graph partitioning problem in the dynamic setting is known as streaming graph partitioning <ref type=""bibr"" target=""#b28"">[29]</ref>. Vertices (or edges) arrive and the decision of the placem ich places a vertex to the cluster with the fewest non-neighbors, and the degree-based heuristic of <ref type=""bibr"" target=""#b28"">[29]</ref>, which serves as the current state-of-the-art method with n the non-neighbors heuristic <ref type=""bibr"" target=""#b25"">[26]</ref> and the neighbors heuristic <ref type=""bibr"" target=""#b28"">[29]</ref>. This provides improved performance for the balanced parti formance of Fennel versus the best previously-known heuristic, which is the linear weighted degrees <ref type=""bibr"" target=""#b28"">[29]</ref>, and the baseline Hash Partition of vertices. We observe t in <ref type=""bibr"">[1]</ref>.</p><p>Online graph partitioning was introduced by Stanton and Kliot <ref type=""bibr"" target=""#b28"">[29]</ref>. The online setting is also well adapted to dynamic graphs stributed system. Currently the most advanced online partitioning algorithm is by Stanton and Kliot <ref type=""bibr"" target=""#b28"">[29]</ref>, against which we extensively compare our approach.</p></d ome order, each one with the set of its neighbors. We consider three different stream orders, as in <ref type=""bibr"" target=""#b28"">[29]</ref>.</p><p>? Random: Vertices arrive according to a random per rtex balanced partitions in Section 5, Fennel also works for edge balanced parititions as well, see <ref type=""bibr"" target=""#b28"">[29]</ref>.</p><p>Application to Classical Balanced Partitioning. Cla f neighbours in Si, i.e |N (v)?Si|. This is one of the greedy rules considered by Stanton and Kliot <ref type=""bibr"" target=""#b28"">[29]</ref>, and is a greedy rule that may result in highly imbalanced most ?? n k . This algorithm for 1 ? ? ? 2 amounts to interpolating between the basic heuristics of <ref type=""bibr"" target=""#b28"">[29]</ref> and <ref type=""bibr"" target=""#b25"">[26]</ref>. The overall te-of-the-art heuristics. Specifically, in our evaluation we consider the following heuristics from <ref type=""bibr"" target=""#b28"">[29]</ref>, which we briefly describe here for completeness. Let v be (v)(1 -exp |Si| -n/k)). ? Non-Neighbors (NN): minumum |Si \ N (v)|.</formula><p>In accordance with <ref type=""bibr"" target=""#b28"">[29]</ref>, we observed that LDG is the best performing heuristic. Ev e used in practice <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b28"">29]</ref>. Our work contributes towards bridging the gap between theo e provide a novel perspective on a recent line of research <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b28"">29]</ref> for the balanced graph partitioning problem, which results interpolation between the two state-of-the-art heuristics <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b28"">29]</ref> for streaming partitioning. Specifically, we evaluate our p",1
"#b19"">[20]</ref>, and heuristics that are used in practice <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b28"">29]</ref>. Our work contribut e most popular heuristics used for streaming balanced graph partitioning: the folklore heuristic of <ref type=""bibr"" target=""#b25"">[26]</ref> which places a vertex to the cluster with the fewest non-n llows us to define formally the notion of interpolation between between the non-neighbors heuristic <ref type=""bibr"" target=""#b25"">[26]</ref> and the neighbors heuristic <ref type=""bibr"" target=""#b28"" elds the following heuristic: place a vertex to the partition with the least number of nonneighbors <ref type=""bibr"" target=""#b25"">[26]</ref>. This assignment accounts for both the cost of cut edges a unts to interpolating between the basic heuristics of <ref type=""bibr"" target=""#b28"">[29]</ref> and <ref type=""bibr"" target=""#b25"">[26]</ref>. The overall complexity of our algorithm is O(n + m).</p>< o, it allows us to quantify the notion of interpolation between the two state-of-the-art heuristics <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b28"">29]</ref> for streaming part",1
"balanced graph partitioning problem appears in various guises in numerous other domains, e.g., see <ref type=""bibr"" target=""#b15"">[16]</ref>.</p><p>Another major challenge in the area of big graph da scribe here for completeness. Let v be the newly arrived vertex, then place v to a cluster Si with  <ref type=""bibr"" target=""#b15"">[16]</ref> averaged over 5 random graphs generated according to the H e graphs, e.g., <ref type=""bibr"" target=""#b19"">[20]</ref>, and heuristics that are used in practice <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" ta",0
"ithms and heuristics exist for this problem, see, e.g., <ref type=""bibr"" target=""#b8"">[9]</ref> and <ref type=""bibr"" target=""#b17"">[18]</ref> respectively. When ? = 1 + ? for any desired but fixed ? &",0
"p><p>created upon post-processing datasets such as Twitter posts are also dynamic, see for instance <ref type=""bibr"" target=""#b1"">[2]</ref>. It is crucial to have efficient graph partitioners of dynam",0
"k ? 1 clusters is equal to 1 -1/k. Given the fact that real-world graphs tend to have sparser cuts <ref type=""bibr"" target=""#b3"">[4]</ref>, it is important to discover methods that are computationall",0
"ts open source cousin Apache Giraph, PEGASUS <ref type=""bibr"" target=""#b12"">[13]</ref> and GraphLab <ref type=""bibr"" target=""#b21"">[22]</ref> use as a default partitioner Hash Partition of vertices, w",0
"formulation as a maximization problem which makes a connection with the concept of graph modularity <ref type=""bibr"" target=""#b23"">[24]</ref>. For a graph G = (V, E) and S ? V , we define the function power law graphs CL(20 000,2.5), since this value matches the typical slope of real-world networks <ref type=""bibr"" target=""#b23"">[24]</ref>. Figure <ref type=""figure"" target=""#fig_1"">1</ref> shows t",0
"learly outperforms Metis.</p><p>Power Law: It is well known that power law graphs have no good cuts <ref type=""bibr"" target=""#b11"">[12]</ref>, but power-law graphs are commonly observed in practice so",0
", in 2009, Twitter reported more than 40 million of users and about 1.5 billion of social relations <ref type=""bibr"" target=""#b20"">[21]</ref>. A standard approach for scalable computation on massive s",0
"f type=""bibr"" target=""#b15"">[16]</ref>, we use the SEQUITUR hierarchical data compression algorithm <ref type=""bibr"" target=""#b9"">[10]</ref> to identify repetitive sub-sequences within the miss traces",1
"arget=""#b8"">9,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" tar",0
"ed nearly 15 years ago, but still used in current Solaris releases-is per-processor dispatch queues <ref type=""bibr"" target=""#b17"">[18]</ref>. In the earliest versions of Solaris, and older UNIX imple tion names. For Solaris, several public resources document the kernel implementation details (e.g., <ref type=""bibr"" target=""#b17"">[18]</ref>). Furthermore, with the release of OpenSolaris in 2006, we described in Section 2.1. Extensive discussion of the operation of the Solaris scheduler appears in <ref type=""bibr"" target=""#b17"">[18]</ref>.</p><p>Kernel MMU and trap handlers Functions (other than",0
"rary multi-core. The L1s and shared L2 implement a MOSI coherence protocol based closely on Piranha <ref type=""bibr"" target=""#b1"">[2]</ref>. The hierarchy is non-inclusive. Other system details do not",0
"al server applications <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" targe ointerbased structures <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target= single system organization, focusing either on uniprocessors <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target= titive).</p><p>Discussion. Our results confirm prior studies <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b24"">25]</ref>: a substantial miss f plications are dominated by pointer-based data structures with complex, non-strided access patterns <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" targ usefulness of temporal streams. Long streams amortize prefetch costs (e.g., off-chip lookup latency <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target ong-in all cases the median stream length exceeds the fixed prefetch depths of many prior proposals <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" targ -level memory thread <ref type=""bibr"" target=""#b20"">[21]</ref>, epoch-based correlation prefetching <ref type=""bibr"" target=""#b7"">[8]</ref>, and last-touch correlated data streaming <ref type=""bibr"" t",0
"CAREER Award CCR -0133777.</p><p>technology, the larger the cache, the slower the cache will become <ref type=""bibr"" target=""#b1"">[2]</ref>, and larger caches increase the cost of manufacturing. Anoth he. An inclusive cache system implies that the contents of the L1 cache be a subset of the L2 cache <ref type=""bibr"" target=""#b1"">[2]</ref>. This decreases the effective cache capacity available for u",1
"arget=""#b1"">[2]</ref>. This decreases the effective cache capacity available for unique information <ref type=""bibr"" target=""#b2"">[3]</ref>. An exclusive cache hierarchy is a hierarchy in which the co f an on-die two-level exclusive cache architecture in the Athlon TM and Duron TM processors in 2000 <ref type=""bibr"" target=""#b2"">[3]</ref>. Figure <ref type=""figure"" target=""#fig_0"">1</ref> illustrat",0
"www.tei-c.org/ns/1.0""><head n=""3.3"">Methodology and Benchmarks</head><p>10 benchmarks from SPEC2000 <ref type=""bibr"" target=""#b6"">[7]</ref> were used in the experiments presented. These benchmarks wer",0
"exclusive cache has improved performance over inclusive cache due to the increased on-chip capacity <ref type=""bibr"" target=""#b4"">[5]</ref>. The concept of an exclusive cache hierarchy can be relevant -side and DRAM bus.</p><p>Exclusive caching has many advantages over conventional inclusive caching <ref type=""bibr"" target=""#b4"">[5]</ref>. Increased associativity is provided through the increased c",0
"work is carried out in the Forte verification framework, originally built on top of the Voss system <ref type=""bibr"" target=""#b7"">[8]</ref>. The interface language to Forte is reFLect, a lazy, strongl",1
"ructions. In the current paper we discuss further expansion of this work on Intel Core TM i7 design <ref type=""bibr"" target=""#b0"">[1]</ref>. For this project, we used formal verification as the primar he processor, e.g. the main memory or external bus.</p><p>In a multi-core design like Intel Core i7 <ref type=""bibr"" target=""#b0"">[1]</ref>, a single processor contains several cores and logic for com",0
"interface language to Forte is reFLect, a lazy, strongly-typed functional language in the ML family <ref type=""bibr"" target=""#b17"">[18]</ref>. Most of our verification code is written in reFLect: spec",0
"a formidable task -the informal specifications of IA-32 architectural instructions take two volumes <ref type=""bibr"" target=""#b1"">[2]</ref>. On the other, the existence of written specifications as a",0
"pical single-core IA-32 processor consists of the following major design components called clusters <ref type=""bibr"" target=""#b8"">[9]</ref>:</p><p>-The front end cluster FEC fetches and decodes archit",0
"information stored per BTB entry, the better the performance, provided the content reduces aliasing <ref type=""bibr"" target=""#b16"">[16]</ref>.</p><p>A large BTB tends to be more accurate, but the incr cache) and large amount of data is accessed slowly (L2, L3, main memory etc.). It was suggested by <ref type=""bibr"" target=""#b16"">[16]</ref> that multilevel BTBs may be able to strike a balance betwe s in different threads <ref type=""bibr"" target=""#b4"">[4,</ref><ref type=""bibr"" target=""#b5"">5,</ref><ref type=""bibr"" target=""#b16"">16]</ref>.</p><p>A balance must be made among the number of entries,",1
"from branch prediction, especially as lower cycle times have necessitated increased pipeline depths <ref type=""bibr"" target=""#b3"">[3]</ref>.</p><p>Significant advantages may be obtained when the BTB i",0
"latency, size and cost issues of traditional predictors. Predictor virtualization was suggested in <ref type=""bibr"" target=""#b17"">[17]</ref> as a means to reduce the resource a conventional predictio",0
"p><p>Recognizing the potential of virtualization, Emma et al. in a patent application filed in 2003 <ref type=""bibr"" target=""#b1"">[1]</ref> describe a system to preload branch prediction data from a l without dedicating large chip resources for a conventional BTB, expanding upon the work invented in <ref type=""bibr"" target=""#b1"">[1]</ref>. This work relies on temporal correlation within the first l",0
"nism for looking up addresses and redirecting the flow of instructions to prevent sequencing issues <ref type=""bibr"" target=""#b7"">[7]</ref>. Studies have shown that a branch prediction scheme with a B",0
"period under study, which was just over 14% and we were long 653 out of 1077 days.)</p><p>As Sharpe <ref type=""bibr"" target=""#b5"">(6)</ref> points out, instead of buying and selling short the DJIA, we",1
"r example, one Granville indicator is the mutual fund cash fund to asset ratio. This is analyzed in <ref type=""bibr"" target=""#b4"">(5)</ref>, which concludes this indicator has no predictive power. Ano",0
"uto-encoder. Most recently, a Deep Structured Semantic Models (DSSM) for Web search was proposed in <ref type=""bibr"" target=""#b5"">[6]</ref>, which is reported to outperform significantly semantic hash in an input word sequence into a feature vector using the technique called word hashing proposed in <ref type=""bibr"" target=""#b5"">[6]</ref>. For example, the word is represented by a count vector of i hastic gradient ascent. Learning of the C-DSSM is similar to that of learning the DSSM described in <ref type=""bibr"" target=""#b5"">[6]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=",1
"nd other conventional semantic models.</p><p>In this study, based on a convolutional neural network <ref type=""bibr"" target=""#b0"">[1]</ref>, we present a new Convolutional Deep Structured Semantic Mod",1
"and its extensions, are able to map a query to its relevant documents at the semantic level (e.g., <ref type=""bibr"" target=""#b1"">[2]</ref>). However, most latent semantic models still view a query (o ion in search queries and documents is a long-standing research topic in information retrieval (IR) <ref type=""bibr"" target=""#b1"">[2]</ref>[4] <ref type=""bibr"" target=""#b7"">[8]</ref>. Usually, the con",0
"document for ranking. The results are reported by mean Normalized Discounted Cumulative Gain (NDCG) <ref type=""bibr"" target=""#b6"">[7]</ref>. In our experiments, the clickthrough data used for model tr",0
"br"" target=""#b2"">[3]</ref>[6][9] <ref type=""bibr"" target=""#b9"">[10]</ref>. Salakhutdinov and Hinton <ref type=""bibr"" target=""#b8"">[9]</ref> demonstrated that the semantic structures can be extracted v",0
"rate line of research, deep learning based techniques have been proposed for semantic understanding <ref type=""bibr"" target=""#b2"">[3]</ref>[6][9] <ref type=""bibr"" target=""#b9"">[10]</ref>. Salakhutdino",0
"ng-standing research topic in information retrieval (IR) <ref type=""bibr"" target=""#b1"">[2]</ref>[4] <ref type=""bibr"" target=""#b7"">[8]</ref>. Usually, the contextual information captured by models such",0
"hniques have been proposed for semantic understanding <ref type=""bibr"" target=""#b2"">[3]</ref>[6][9] <ref type=""bibr"" target=""#b9"">[10]</ref>. Salakhutdinov and Hinton <ref type=""bibr"" target=""#b8"">[9]",0
"ffective. As an alternative, there are retrieval methods such as the phrase-based translation model <ref type=""bibr"" target=""#b4"">[5]</ref> that directly model phrases (or word n-grams), but they ofte mercial search engine. On average, each query is associated with 65 Web documents (URLs). Following <ref type=""bibr"" target=""#b4"">[5]</ref>, we only used the title field of a Web document for ranking.",0
"veral techniques for nonsequential instruction prefetching <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" targe",1
"poor cache performance <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" tar ed the performance impact of architectural features on DBMSs <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" tar increases in the instruction cache miss rates <ref type=""bibr"" target=""#b10"">[11]</ref>. Lo et al. <ref type=""bibr"" target=""#b11"">[12]</ref> also showed that in OLTP workloads, the instruction cache",0
"he sequence of basic blocks that lie on the most commonly occurring control flow path. Romer et al. <ref type=""bibr"" target=""#b17"">[18]</ref> implemented the Pettis and Hansen code layout algorithm us",0
"e will have large main memory configurations, and many working sets will be resident in main memory <ref type=""bibr"" target=""#b1"">[2]</ref>. Moreover techniques such as concurrent query execution, whe",0
"systems with their large code and data footprints suffer significantly from poor cache performance <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target y recently that researchers have examined the performance impact of architectural features on DBMSs <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" targ ds, the instruction cache miss rate is nearly three times the data cache miss rate. Ailamaki et al. <ref type=""bibr"" target=""#b0"">[1]</ref> analyzed three commercial DBMSs on a Xeon processor and show",0
"d effects are modeled correctly and bandwidth limitations are enforced in our model as described in <ref type=""bibr"" target=""#b15"">[16]</ref>. The memory bus has a bandwidth of 4.5 GB/s.</p><p>The bas",1
".tei-c.org/ns/1.0""><head n=""3.1.2."">Prefetch Lateness:</head><p>Miss Status Holding Register (MSHR) <ref type=""bibr"" target=""#b11"">[12]</ref> is a hardware structure that keeps track of all in-flight",0
"d Section 2.1.</p><p>3 Similar results were reported by <ref type=""bibr"" target=""#b7"">[8]</ref> and <ref type=""bibr"" target=""#b17"">[18]</ref>. All average IPC results in this paper are computed as geo",0
"t incurs a heavy overhead in terms of both hardware and complexity. We use the Bloom filter concept <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b19"">20]</ref> to provide a simple",0
"lta correlation prefetcher <ref type=""bibr"" target=""#b9"">[10]</ref> or a PC-based stride prefetcher <ref type=""bibr"" target=""#b0"">[1]</ref>. Compared to a conventional GHB-based delta correlation pref FDP on a PC-Based Stride Prefetcher</head><p>We also evaluated FDP on a PC-based stride prefetcher <ref type=""bibr"" target=""#b0"">[1]</ref> and found that the results are similar to those achieved on",0
"blem is apparent in Figure <ref type=""figure"" target=""#fig_0"">2</ref>. or ""Bezier patches,"" Gouraud <ref type=""bibr"" target=""#b10"">[11]</ref> developed an algorithm to shade curved surfaces. With his",1
"rned with the shading problem; the contour edge problem is discussed by the author and F.C. Crow in <ref type=""bibr"" target=""#b6"">[7]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>In tion do not yield a unit vector, and since eqs. ( <ref type=""formula"" target=""#formula_6"">6</ref>), <ref type=""bibr"" target=""#b6"">(7)</ref>, and (8) require a unit normal vector, some extra hardware i",0
"ighly utilized. While recent work has proposed speculation <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b12"">13]</ref> to cut down the router critical path delay by parallelizing speculation (SP) can be used to cut down the critical path <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b15"">16]</ref>. Fig. <ref type=""bi e crossbar from each input. Separable VC and switch allocators modeled closely after the designs in <ref type=""bibr"" target=""#b12"">[13]</ref> are assumed as they are fast and of low complexity, while",1
"<p>To remove the serialization delay due to routing, prior work has proposed lookahead routing (LA) <ref type=""bibr"" target=""#b14"">[15]</ref> where the route of a packet is determined one hop in advan",0
"2 <ref type=""bibr"" target=""#b13"">[14]</ref> traces were gathered by running the benchmarks on Bochs <ref type=""bibr"" target=""#b20"">[21]</ref>, a multiprocessor simulator with an embedded Linux 2.4 ker",0
"limitations to wire scaling and increasing bandwidth demands <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref>, packet-switched on-chip networks are fast replacing shared b",0
"ghput when the express channels are not highly utilized. While recent work has proposed speculation <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b12"">13]</ref> to cut down the ro sallowing pipeline bypassing, aggressive speculation (SP) can be used to cut down the critical path <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" ta veral speculation mechanisms that target Trouter, such as bypassing and speculation. Mullins et al. <ref type=""bibr"" target=""#b11"">[12]</ref> propose a doubly-speculative single-stage design in which",0
"as exponential service times, infinite buffers, and so on <ref type=""bibr"" target=""#b6"">[7]</ref>, <ref type=""bibr"" target=""#b8"">[9]</ref>, <ref type=""bibr"" target=""#b12"">[13]</ref>. Likewise, analys et=""#b6"">[7]</ref> and hypercubes <ref type=""bibr"" target=""#b22"">[24]</ref>. The study presented in <ref type=""bibr"" target=""#b8"">[9]</ref> is not restricted to a particular topology, but it assumes a where T Bus is the service time of the Bus architecture and C Bus is the contention matrix given in <ref type=""bibr"" target=""#b8"">(9)</ref>. Finally, we note that when each buffer shown in Fig. <ref t",1
"e utilization is 1 for a system with a single queue. The following example gives more intuition for <ref type=""bibr"" target=""#b4"">(5)</ref>.</p><p>Example 1: Consider the case P = 1 (i.e., single queu the P×P matrices. Finally, since all elements of R are greater than or equal to 0 ( R ≥ 0), we use <ref type=""bibr"" target=""#b4"">(5)</ref> to prove the theorem</p><formula xml:id=""formula_29"">(I − T .0""><head>D. Performance Comparisons Using Multimedia System</head><p>The router model described by <ref type=""bibr"" target=""#b4"">(5)</ref> provides an analytical approach to analyze the effect of var and determine arrival rates ( ) and the contention matrix C for the bottleneck router. Then, we use <ref type=""bibr"" target=""#b4"">(5)</ref> to analyze the impact of H S and B on buffer utilization. Fi , α &gt; 1 but still close to 1). Scaling the traffic arrival rates to the router inputs, αλ j , in <ref type=""bibr"" target=""#b4"">(5)</ref> allows us to write the following relation between the throug",0
"for high dimensional networks. A more general analytical model for wormhole routing is presented in <ref type=""bibr"" target=""#b13"">[14]</ref>. The model provides average packet latency estimates using ormance analysis <ref type=""bibr"" target=""#b6"">[7]</ref>, <ref type=""bibr"" target=""#b9"">[10]</ref>, <ref type=""bibr"" target=""#b13"">[14]</ref>, enables us to derive closed loop solutions and show that terative process similar to <ref type=""bibr"" target=""#b6"">[7]</ref> or through computation ordering <ref type=""bibr"" target=""#b13"">[14]</ref>. In order to compute the delay experienced at the intermed",0
"ion and platform models are usually developed separately <ref type=""bibr"" target=""#b14"">[16]</ref>, <ref type=""bibr"" target=""#b16"">[18]</ref>. After that, the application is mapped to the target platf re"">1</ref>. Proposed performance analysis methodology is illustrated here using the Y-chart scheme <ref type=""bibr"" target=""#b16"">[18]</ref>. Fig. <ref type=""figure"">2</ref>. Router model as a collec",0
"uffers, and so on <ref type=""bibr"" target=""#b6"">[7]</ref>, <ref type=""bibr"" target=""#b8"">[9]</ref>, <ref type=""bibr"" target=""#b12"">[13]</ref>. Likewise, analysis of power consumption of the NoC archit analysis techniques that can be used in optimization loops are extremely important. The authors in <ref type=""bibr"" target=""#b12"">[13]</ref> consider the buffer sizing problem and present a performan , network architecture synthesis <ref type=""bibr"" target=""#b21"">[23]</ref>, buffer space allocation <ref type=""bibr"" target=""#b12"">[13]</ref>) for fast and accurate performance estimations.</p></div>",0
"ns is determined by dynamic external customers. The Sharing Architecture is inspired by Core Fusion <ref type=""bibr"" target=""#b22"">[24]</ref>, WiD-GET <ref type=""bibr"" target=""#b62"">[64]</ref>, the Di e fetch stage of the pipeline, it stalls all of the Slices in a VCore.</p><p>Similar to Core Fusion <ref type=""bibr"" target=""#b22"">[24]</ref>, a distributed branch predictor is used, thus the effectiv issue processors. The Sharing Architecture  <ref type=""bibr"" target=""#b6"">[8]</ref> and Core Fusion <ref type=""bibr"" target=""#b22"">[24]</ref> require LSQ bank prediction because a memory operation's e distribute Reorder Buffer (ROB) entries across Slices. We leverage the approach used by Core Fusion <ref type=""bibr"" target=""#b22"">[24]</ref> which uses a pre-commit pointer to guarantee that all ROBs potential to exploit high ILP.</p><p>The Sharing Architecture leverages many ideas from Core Fusion <ref type=""bibr"" target=""#b22"">[24]</ref> on how to distribute resources across cores, but unlike Co",1
"get=""#b15"">17,</ref><ref type=""bibr"" target=""#b47"">49,</ref><ref type=""bibr"" target=""#b48"">50,</ref><ref type=""bibr"" target=""#b51"">53]</ref>, and how ILP is mapped across multiple cores in the Raw <re",0
"only large structures are shared while we aim at finer grain sharing.</p><p>Clustered Architectures <ref type=""bibr"" target=""#b43"">[45]</ref> and dynamically configurable clustered architectures <ref",0
"gh-level goal from the user. Such an auto-tuning system would likely require the use of a heartbeat <ref type=""bibr"" target=""#b20"">[22]</ref> or performance feedback to evaluate the success criteria.",0
"n this work, we focus on three of these proposed schemes -the first based on working set signatures <ref type=""bibr"" target=""#b9"">[10]</ref> <ref type=""bibr"" target=""#b10"">[11]</ref>, the second based n lead to unpredictable, non-optimal results. Consequently, algorithms such as the ones proposed in <ref type=""bibr"" target=""#b9"">[10]</ref> <ref type=""bibr"" target=""#b10"">[11]</ref> do not perform tu ets, BBVs, and conditional branch counters. In addition to instruction working set based techniques <ref type=""bibr"" target=""#b9"">[10]</ref>[11], we evaluate branch and procedure working set based tec are defined as the set of branches/procedures touched over the sampling interval. In previous work <ref type=""bibr"" target=""#b9"">[10]</ref>[11], we defined a similarity metric called the relative wor and working sets are too large to be efficiently stored and compared in hardware. In previous work <ref type=""bibr"" target=""#b9"">[10]</ref> <ref type=""bibr"" target=""#b10"">[11]</ref>, we proposed a ha e</head><p>A working set signature is a lossy-compressed representation of the complete working set <ref type=""bibr"" target=""#b9"">[10]</ref> <ref type=""bibr"" target=""#b10"">[11]</ref>. The signature is 512 bits) and accumulator tables (1024, 128, 32 entries) are similar to those used in previous work <ref type=""bibr"" target=""#b9"">[10]</ref>[11] <ref type=""bibr"" target=""#b16"">[19]</ref>. Procedure si be used in tuning algorithms to reuse previously found optimal configurations for recurring phases <ref type=""bibr"" target=""#b9"">[10]</ref>[11] <ref type=""bibr"">[12] [19]</ref>. This eliminates a sig",1
"order to achieve better power/performance characteristics <ref type=""bibr"" target=""#b0"">[1]</ref>- <ref type=""bibr"" target=""#b11"">[12]</ref>. Similarly, on the software side, dynamic code optimizatio phase changes <ref type=""bibr"" target=""#b1"">[2]</ref>[10] <ref type=""bibr"" target=""#b10"">[11]</ref> <ref type=""bibr"" target=""#b11"">[12]</ref> [19] <ref type=""bibr"" target=""#b17"">[20]</ref> <ref type="" icting phases <ref type=""bibr"" target=""#b1"">[2]</ref>[10] <ref type=""bibr"" target=""#b10"">[11]</ref> <ref type=""bibr"" target=""#b11"">[12]</ref> <ref type=""bibr"" target=""#b16"">[19]</ref>. In this work, w lts indicate that as few as 32 24-bit counters are sufficient to represent BBVs.</p><p>Huang et al. <ref type=""bibr"" target=""#b11"">[12]</ref> use subroutines to identify program phases. They propose t rvals at the beginning of a stable phase to complete the tuning process (the algorithm presented in <ref type=""bibr"" target=""#b11"">[12]</ref> is an exception). If phases are short (i.e. small number o",0
"</ref>[10] <ref type=""bibr"" target=""#b10"">[11]</ref> <ref type=""bibr"" target=""#b11"">[12]</ref> [19] <ref type=""bibr"" target=""#b17"">[20]</ref> <ref type=""bibr"" target=""#b18"">[21]</ref>. Recently, sever chmarks by identifying sections of code whose performance is representative of the entire benchmark <ref type=""bibr"" target=""#b17"">[20]</ref> <ref type=""bibr"" target=""#b18"">[21]</ref>. Thus, one of th is a normalized metric, the maximum possible working set difference is 100%.</p><p>Sherwood et al. <ref type=""bibr"" target=""#b17"">[20]</ref> define a BBV to be a set of counters, each of which counts",0
"to implicitly <ref type=""bibr"" target=""#b12"">[13]</ref> or explicitly detect program phase changes <ref type=""bibr"" target=""#b1"">[2]</ref>[10] <ref type=""bibr"" target=""#b10"">[11]</ref> <ref type=""bib techniques aimed specifically at detecting phase changes, identifying phases and predicting phases <ref type=""bibr"" target=""#b1"">[2]</ref>[10] <ref type=""bibr"" target=""#b10"">[11]</ref> <ref type=""bib tors <ref type=""bibr"" target=""#b16"">[19]</ref>, and the third based on a conditional branch counter <ref type=""bibr"" target=""#b1"">[2]</ref>. Because phases are not well-defined, a significant aspect o v> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2."">Background</head><p>Balasubramonian et al. <ref type=""bibr"" target=""#b1"">[2]</ref> use a conditional branch counter to detect program phase cha d><p>Program phase detection techniques have been used in power/performance optimization algorithms <ref type=""bibr"" target=""#b1"">[2]</ref>[10] <ref type=""bibr"" target=""#b10"">[11]</ref> [12] <ref type of counters can provide reasonable phase detection ability. In fact, the conditional branch counter <ref type=""bibr"" target=""#b1"">[2]</ref>, which is an extreme example with a single counter, works qu",0
"[11]</ref> <ref type=""bibr"" target=""#b11"">[12]</ref> [19] <ref type=""bibr"" target=""#b17"">[20]</ref> <ref type=""bibr"" target=""#b18"">[21]</ref>. Recently, several researchers have proposed hardware tech ose performance is representative of the entire benchmark <ref type=""bibr"" target=""#b17"">[20]</ref> <ref type=""bibr"" target=""#b18"">[21]</ref>. Thus, one of the desirable properties in a phase detectio",0
">[19]</ref> as a way to study brain function. We consider the simplest of many types of perceptrons <ref type=""bibr"" target=""#b1"">[2]</ref>, a single-layer perceptron consisting of one artificial neur",1
"et=""#b16"">[17,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b3"">4]</ref>, which occurs when two unrelated branches destructively inter et=""#b16"">[17,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b3"">4]</ref> but do not change the basic prediction mechanism. Given a gen information is typical of recent work in branch prediction <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b3"">4]</ref>.</p><p>Gathering traces. Our simulations use the instrumented n a generous hardware budget, many of these two-level schemes perform about the same as one another <ref type=""bibr"" target=""#b3"">[4]</ref>.</p><p>Most two-level predictors cannot consider long histor dicator of performance in extreme conditions, and it uses the same methodology as other recent work <ref type=""bibr"" target=""#b3"">[4]</ref>. Note that previous studies have used the 8 SPEC 95 integer",0
"network is making its decision. Techniques have been proposed to extract rules from neural networks <ref type=""bibr"" target=""#b20"">[21]</ref>, but these rules are not always accurate. Perceptrons do n",0
"predictor that uses a 2K byte choice table and the same choice mechanism as that of the Alpha 21264 <ref type=""bibr"" target=""#b13"">[14]</ref>. The goal of our hybrid predictor is to show that because and perceptron predictor components, along with a mechanism, similar to the one in the Alpha 21264 <ref type=""bibr"" target=""#b13"">[14]</ref>, that dynamically chooses between the two using a 2K byte . Per-branch and path information can yield greater accuracy <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b13"">14]</ref>, but our restriction to global information is typical of re",0
"etworks have been used for a variety of applications, including pattern recognition, classification <ref type=""bibr"" target=""#b7"">[8]</ref>, and image understanding <ref type=""bibr"" target=""#b14"">[15, e neural architectures, such as ADALINE <ref type=""bibr"" target=""#b24"">[25]</ref> and Hebb learning <ref type=""bibr"" target=""#b7"">[8]</ref>, but we found that these were less effective than perceptron >A limitation of perceptrons is that they are only capable of learning linearly separable functions <ref type=""bibr"" target=""#b7"">[8]</ref>. Imagine the set of all possible inputs to a perceptron as a for which the perceptron will respond false and the set for which the perceptron will respond true <ref type=""bibr"" target=""#b7"">[8]</ref>. A Boolean function over variables Ü½ Ò is linearly separabl",0
"generally performed at two levels: feature level or early fusion and decision level or late fusion <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b44"">45,</ref><ref type=""bibr"" targ inear kernel function.</p><p>Many existing literature use the SVM-based fusion scheme. Adams et al. <ref type=""bibr"" target=""#b2"">[3]</ref> adopted a late fusion approach in order to detect semantic c the NLF method is used for nonlinear combination of multimodal information. This method is based on <ref type=""bibr"" target=""#b2"">[3]</ref>, where SVM is first used as a classifier for the individual f> SVM based score space classification of combined information from multiple intermediate concepts <ref type=""bibr"" target=""#b2"">[3]</ref> The key idea was to utilize the synchrony measure between th their correlation over time. This approach is used for speech recognition. The work by Adams et al. <ref type=""bibr"" target=""#b2"">[3]</ref> also used a Bayesian network in addition to SVM and showed t ibr"" target=""#b24"">[25]</ref>, Xie et al. <ref type=""bibr"" target=""#b145"">[145]</ref>, Adams et al. <ref type=""bibr"" target=""#b2"">[3]</ref> Photo and video annotation Linear weighted fusion Iyenger et fusion Iyenger et al. <ref type=""bibr"" target=""#b57"">[58]</ref> Support vector machine Adams et al. <ref type=""bibr"" target=""#b2"">[3]</ref>, Iyenger et al. <ref type=""bibr"" target=""#b57"">[58]</ref>, W",1
"e of correlation, confidence, and the context), also related to how to fuse, are described in Sect. <ref type=""bibr"" target=""#b3"">4</ref>. This section further elaborates the issues when to fuse (the hich provided the decision about the identity of the talking face. On another front, Aguilar et al. <ref type=""bibr"" target=""#b3"">[4]</ref> provided a comparison between the rule-based fusion and lear",1
"><p>The particle methods have been widely used in multimedia analysis. For instance, Vermaak et al. <ref type=""bibr"" target=""#b132"">[132]</ref> used particle filters to estimate the predictions from a to fuse 2D object shapes and audio information for speaker tracking. However, unlike Vermaak et al. <ref type=""bibr"" target=""#b132"">[132]</ref>, the latter uses the concept of importance particle filt m parameters and the tracking position in the audio-visual state-space model. Unlike Vermaak et al. <ref type=""bibr"" target=""#b132"">[132]</ref> or Perez et al. <ref type=""bibr"" target=""#b98"">[99]</ref",0
"al and maximally compact in the projected space. The computation details of A and B can be found in <ref type=""bibr"" target=""#b117"">[117]</ref>. The first M vectors of A and B are used to compute the sidering CCA for audio-video based talking-face identity verification. Similarly, Slaney and Covell <ref type=""bibr"" target=""#b117"">[117]</ref> used CCA for talking-face detection.</p><p>Cross-modal f ese works include Hershey and Movellan <ref type=""bibr"" target=""#b46"">[47]</ref>, Slaney and Covell <ref type=""bibr"" target=""#b117"">[117]</ref>, Iyengar et al. <ref type=""bibr"" target=""#b56"">[57]</ref",0
"ad n=""3.3.1"">Kalman filter</head><p>The Kalman filter (KF) <ref type=""bibr"" target=""#b65"">[66,</ref><ref type=""bibr"" target=""#b112"">112]</ref> allows for real-time processing of dynamic low-level data",0
"""#b153"">[153]</ref>; multimodal human computer interaction <ref type=""bibr"" target=""#b59"">[60,</ref><ref type=""bibr"" target=""#b96"">97]</ref>; audio-visual biometric <ref type=""bibr"" target=""#b4"">[5]</",0
"een addressed by performing the multimedia analysis tasks (such as event detection) over a timeline <ref type=""bibr"" target=""#b28"">[29]</ref>. A timeline refers to a measurable span of time with infor",0
".</p><p>Among the video-based fusion research, the most popular are the well-known TRECVID datasets <ref type=""bibr"" target=""#b1"">[2]</ref> that are available in different versions since 2001. A quick",0
", Lucey et al. <ref type=""bibr"" target=""#b77"">[78]</ref> for spoken word recognition, Hua and Zhang <ref type=""bibr"" target=""#b54"">[55]</ref> for image retrieval, McDonald and Smeaton <ref type=""bibr"" is a simple but nonrealistic choice.</p><p>A decision level fusion scheme proposed by Hua and Zhang <ref type=""bibr"" target=""#b54"">[55]</ref> is based on the human's psychological observations which t strategy and is derived by adding the difference of two decisions to their average (please refer to <ref type=""bibr"" target=""#b54"">[55]</ref> for formalism). The authors have demonstrated the utility Image segmentation, classification, recognition, and retrieval Linear weighted fusion Hua and Zhang <ref type=""bibr"" target=""#b54"">[55]</ref> Support vector machine Zhu et al. <ref type=""bibr"" target= "" target=""#b51"">[52]</ref>. Precision and recall measure are also commonly used for image retrieval <ref type=""bibr"" target=""#b54"">[55]</ref>. Similarly, for the video shot retrieval some researchers <ref type=""bibr"" target=""#b152"">152]</ref> or have not elaborated the issue of weight determination <ref type=""bibr"" target=""#b54"">[55,</ref><ref type=""bibr"" target=""#b66"">67,</ref><ref type=""bibr"" ta",0
"not only a reduced set of features but it is also used for classification. The readers may refer to <ref type=""bibr"" target=""#b134"">[134]</ref> for further details about these feature dimensionality r",0
"tackle the challenges of multicore scaling in the dark silicon age.</p><p>Recently, Raghavan et al. <ref type=""bibr"" target=""#b16"">[17]</ref> proposed computational sprinting, in which a chip improves con.</p><p>Instead of shrinking the chip or sacrificing transistor density, computational sprinting <ref type=""bibr"" target=""#b16"">[17]</ref> embraces dark silicon by leveraging the extra transistors chip can sustain computational sprinting for one second in the worst case, which is consistent with <ref type=""bibr"" target=""#b16"">[17]</ref>. Later we will analyze how NoC-sprinting influences the sp",1
"target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b17"">18]</ref> to mitigate the latency overhead caused by frequent router",0
"n, especially for multi-threaded applications that have various scalability. Here we use PARSEC 2.1 <ref type=""bibr"" target=""#b1"">[2]</ref> as an example. We simulate CMP systems using gem5 <ref type= table"" target=""#tab_0"">1</ref>. We evaluate NoC-sprinting with multi-threaded workloads from PARSEC <ref type=""bibr"" target=""#b1"">[2]</ref> by assuming the chip can sustain computational sprinting for",0
"orplan of the on-chip resources (cores, caches, routers, etc.) for such sprinting-based multicores? <ref type=""bibr"" target=""#b2"">(3)</ref> what would be an appropriate NoC power-management scheme? To ARSEC 2.1 <ref type=""bibr"" target=""#b1"">[2]</ref> as an example. We simulate CMP systems using gem5 <ref type=""bibr"" target=""#b2"">[3]</ref> and observe the performance speedup when varying the core co v xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4"">Architectural Evaluation</head><p>We use the gem5 <ref type=""bibr"" target=""#b2"">[3]</ref> full system simulator to setup a sprinting-based multicore a",0
"topology region contains all the line segments connecting any pair of nodes inside it. Flich et al. <ref type=""bibr"" target=""#b6"">[7]</ref> proposed a distributed routing algorithm for irregular NoC t",0
">[11]</ref> although it disregards many architectural details. Babka and T?ma present their work in <ref type=""bibr"" target=""#b4"">[4]</ref>, focusing mainly on translation lookaside buffers and cache",1
"therwise a copy can be kept in the L3. Both processors use extended versions of the well-known MESI <ref type=""bibr"" target=""#b7"">[7]</ref> protocol to ensure cache coherency. AMD Opteron processors i",0
"ntee a cache line's presence in a higher level cache.</p><p>AMD's last level cache is non-inclusive <ref type=""bibr"" target=""#b6"">[6]</ref>, i.e neither exclusive nor inclusive. If a cache line is tra",0
"factor is the architecture of the memory subsystem in conjunction with the cache coherency protocol <ref type=""bibr"" target=""#b12"">[12]</ref>.</p><p>While the basic memory hierarchy structure is simil processors including a ping-pong implementation to analyze the latency of cache-to-cache transfers <ref type=""bibr"" target=""#b12"">[12]</ref>. However, they do not cover inter-core bandwidths. To the",0
"mple task which can be easily extracted from who is 'standing with whom' (as suggested by Yu et al. <ref type=""bibr"" target=""#b19"">[20]</ref>), but one quickly finds that in more crowded situations wh ours can be identified. Second, we make improvements over the baseline method proposed by Yu et al. <ref type=""bibr"" target=""#b19"">[20]</ref> by formulating the problem as one of identifying dominant of F-formations was not explicitly addressed.</p><p>The closest work to ours was done by Yu et al. <ref type=""bibr"" target=""#b19"">[20]</ref>, who proposed a system that could track and discover group test data set was rather artificial as 23 people were asked to ""mingle in a 3-group configuration"" <ref type=""bibr"" target=""#b19"">[20]</ref> (p.1468). Three groups were indeed identified but given th mlns=""http://www.tei-c.org/ns/1.0""><head n=""3.1"">F-formations as High Modularity</head><p>Yu et al. <ref type=""bibr"" target=""#b19"">[20]</ref> defined the task of identifying groups of people in terms measuring the affinity between people is to use their relative proximity, as proposed by Yu et al. <ref type=""bibr"" target=""#b19"">[20]</ref>. The symmetric distance function between person i and j is ed dominant set S (GC), as described in Sec. 6. We compare our results with the method of Yu et al. <ref type=""bibr"" target=""#b19"">[20]</ref>, that used modularity cut and proximity to create the affi ity cut and Kernighan-Lin refinement (MC+KL), though the performance in all cases was improved over <ref type=""bibr"" target=""#b19"">[20]</ref>. In addition to the performance of different methods, we a =""table"" target=""#tab_1"">2</ref> shows that all our proposed methods out-performs that the baseline <ref type=""bibr"" target=""#b19"">[20]</ref>. Both fully manual methods with our proposed dominant set ne to all other methods, we see that using the basic modularity cut algorithm proposed by Yu et al. <ref type=""bibr"" target=""#b19"">[20]</ref> only just out performs it, indicating how poor modularity as presented by formulating the problem as one of finding dominant sets. Compared to modularity cut <ref type=""bibr"" target=""#b19"">[20]</ref>, significant and more stable performance improvements were to everyone else in the network. For the task of identifying spatially separated groups, Yu et al. <ref type=""bibr"" target=""#b19"">[20,</ref><ref type=""bibr"" target=""#b2"">3]</ref> have shown the effec",1
"studies have started to consider scenarios where more people <ref type=""bibr"" target=""#b6"">(7)</ref><ref type=""bibr"" target=""#b7"">(8)</ref><ref type=""bibr"" target=""#b8"">(9)</ref><ref type=""bibr"" targe the clique sizes are not affected by the number of positive links and the cardinality of the graph <ref type=""bibr"" target=""#b7"">[8]</ref>. Pavan and Pelillo <ref type=""bibr"" target=""#b16"">[17]</ref>",0
"target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b0"">1,</ref><ref type=""bibr"" target=""#b17"">18]</ref> during walking, which are readily measurable from the extra",0
"of participants is relatively small (2 people: <ref type=""bibr"" target=""#b18"">[19]</ref>, 4 people: <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b13"">14]</ref>), though some studie the affinity between detected people. Other work which is close to ours is that of Brdiczka et al. <ref type=""bibr"" target=""#b1"">[2]</ref> who analysed speech activity to identify who was interacting",0
"ios where more people <ref type=""bibr"" target=""#b6"">(7)</ref><ref type=""bibr"" target=""#b7"">(8)</ref><ref type=""bibr"" target=""#b8"">(9)</ref><ref type=""bibr"" target=""#b9"">(10)</ref><ref type=""bibr"" targ are more likely to be filled by mostly unacquainted groups <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b0"">1]</ref>. Unacquainted groups wi nding people who are 'together' based on the persistence of their proximity and direction of motion <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" targ",0
"sic idea is very similar to instruction fetch in Multiscalar <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b20"">21]</ref>: Multiscalar divides the sequential instruction stream into i o n i s s i m i l a r t o t h a t u s e d b y Multiscalar <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b20"">21]</ref>. When a fragment is renamed, the hardware determines which fragments. This is similar to the idea of traces <ref type=""bibr"" target=""#b19"">[20]</ref> or tasks <ref type=""bibr"" target=""#b20"">[21]</ref>, except that fragments are completely general, whereas the",1
"done statically, for example at link time <ref type=""bibr"" target=""#b11"">[12]</ref>, or dynamically <ref type=""bibr"" target=""#b0"">[1]</ref>. Since this approach is not always successful, especially as instructions. However, it is important to realize that there are two problems in instruction fetch: <ref type=""bibr"" target=""#b0"">(1)</ref> what to fetch, and (2) how to fetch. Parallel fetch is aimed",0
"in the static program <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b17"">18]</ref>. This rearrangement may be done statically, for example at",0
"that are likely to be consecutive in the dynamic program are also consecutive in the static program <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target",0
"ons that are contiguous in dynamic program order but not in the static program. A collapsing buffer <ref type=""bibr"" target=""#b6"">[7]</ref> is an example of this approach. Studies have shown that this",0
"ref>  Register traffic characteristics. We collect a number of characteristics concerning registers <ref type=""bibr"" target=""#b5"">[6]</ref>. Our first characteristic is the average number of input ope",1
"Data stream strides. The data stream is characterized with respect to local and global data strides <ref type=""bibr"" target=""#b9"">[10]</ref>. A global stride is defined as the difference in the data m",1
"based on microarchitecture-independent characteristics such as the stack distance, see for example <ref type=""bibr"" target=""#b17"">[18]</ref>. However, we are unaware of any work that proposes a super",0
"dependent manner we used the Prediction by Partial Matching (PPM) predictor proposed by Chen et al. <ref type=""bibr"" target=""#b1"">[2]</ref>, which is a universal compression/prediction technique.</p><",0
"uring the microarchitecture-independent characteristics discussed in section 2.1 is done using ATOM <ref type=""bibr"" target=""#b14"">[15]</ref>. ATOM is a binary instrumentation tool that allows for ins",0
"tecture-dependent characteristics such as cache miss rates, branch mispredict rates, etc. Yi et al. <ref type=""bibr"" target=""#b16"">[17]</ref> use a Plackett-Burman design for classifying benchmarks ba",0
"en microarchitecture-independent program characteristics and processor performance, see for example <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target",0
"composition techniques <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b11"">12]</ref>. These techniques first measure a number of program charact",0
"mance, see for example <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b13"">14]</ref>. However, these techniques do not predict performance for a",0
"y. Several researchers have proposed methods for quantifying program similarity. Saavedra and Smith <ref type=""bibr"" target=""#b12"">[13]</ref> use the squared Euclidean distance computed in a benchmark",0
"ch precludes fast prediction.</p><p>A recent proposal, the Dead-Block Correlating Prefetcher (DBCP) <ref type=""bibr"" target=""#b11"">[12]</ref>, achieves maximal prefetch lookahead through correlation t correlating GHB prefetcher achieves only 31% performance improvement. DBCP with 2MB on-chip storage <ref type=""bibr"" target=""#b11"">[12]</ref> achieves only 17% performance improvement. Finally, increa d n=""2"">Background: DBCP Prefetching</head><p>Lai et al. proposed Dead-Block Correlated Prefetching <ref type=""bibr"" target=""#b11"">[12]</ref> to predict the last touch to a cache block prior to that b .1"">Why is DBCP Impractical?</head><p>Figure <ref type=""figure"">3</ref> depicts the anatomy of DBCP <ref type=""bibr"" target=""#b11"">[12]</ref>. On every memory access, DBCP computes a last-touch histor /1.0""><head n=""3.3"">Lookahead for Sequence Retrieval</head><p>Cache misses are frequently clustered <ref type=""bibr"" target=""#b11"">[12]</ref>, corresponding to bursts of last touches. During such burs lns=""http://www.tei-c.org/ns/1.0""><head n=""4.1"">Recording Last-Touch Signatures</head><p>As in DBCP <ref type=""bibr"" target=""#b11"">[12]</ref>, LT-cords constructs signatures using the history table. E rget=""#b14"">15]</ref>. The realistic DBCP is implemented with a 2MB on-chip correlation table as in <ref type=""bibr"" target=""#b11"">[12]</ref>. For comparison with a larger L2, we quadruple the size of etween a last touch to a block until its eventual eviction. 1 The figure corroborates prior results <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" ta",1
"an approach called delta correlation. The delta-correlating Global History Buffer (GHB) prefetcher <ref type=""bibr"" target=""#b14"">[15]</ref> was recently shown to outperform a variety of other hardwa target=""#b3"">4,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b19"">20]</ref>, which promises wid dex and history tables, as recommended for SPEC applications <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b14"">15]</ref>. The realistic DBCP is implemented with a 2MB on-chip corre latency as the base 1MB cache. We corroborate prior results <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b14"">15]</ref> showing that GHB, an advanced stride/delta correlating pred",0
"s must store correlation data across long (e.g., billions of instructions) recurring program phases <ref type=""bibr"" target=""#b17"">[18]</ref> with little temporal reuse. Because last-touch signatures",0
"perl, for which the ""splitmail"" input is used. We include results for three pointer-intensive Olden <ref type=""bibr"" target=""#b1"">[2]</ref> benchmarks (indicated by * in Table <ref type=""table"" target",0
"ref type=""bibr"" target=""#b18"">19]</ref>, pointer dereference <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b7"">8]</ref>, or accesses to linked data structures <ref type=""bibr"" targe",0
"cache set or in the SRAM in order to reduce the average write frequency of the STT-RAM cache lines <ref type=""bibr"" target=""#b4"">[4]</ref>.</p><p>However, there exist intrinsic limitations in both ap cache architecture with 4-way SRAM and 12-way STT-RAM, which is similar to the setting described in <ref type=""bibr"" target=""#b4"">[4]</ref> <ref type=""bibr"" target=""#b7"">[7]</ref>. The block-level ini quently. Note that life time of the STT-RAM mainly depends on the peak write count of all the cells <ref type=""bibr"" target=""#b4"">[4]</ref> <ref type=""bibr"" target=""#b5"">[5]</ref>. Even static optimiz 8]</ref> for both of the pure static <ref type=""bibr"" target=""#b10"">[10]</ref> and the pure dynamic <ref type=""bibr"" target=""#b4"">[4]</ref> scheme. The peak write count of static optimization is signi ctually write-intensive STT-RAM data blocks to SRAM. We use the dynamic migration scheme similar to <ref type=""bibr"" target=""#b4"">[4]</ref>, which is briefly described as follows. Each L2 cache block <head>Pure dynamic optimization (dynamic):</head><p>We use the dynamic migration scheme proposed in <ref type=""bibr"" target=""#b4"">[4]</ref>. Our dynamic migration scheme in Section 4.3 uses this schem he first STT-RAM line becomes defective, which is similar to the estimation methodology proposed in <ref type=""bibr"" target=""#b4"">[4]</ref> and <ref type=""bibr"" target=""#b5"">[5]</ref>.</p><p>Figure <r",1
"ion 4.3 is required to provide better optimization. We extend the full-system cycle-accurate Simics <ref type=""bibr"" target=""#b17"">[17]</ref> and GEMS <ref type=""bibr"" target=""#b18"">[18]</ref> simulat",0
"T-RAM has significantly higher endurance (10 9 versus 10 12 write cycles) and shorter write latency <ref type=""bibr"" target=""#b1"">[1]</ref> and is much more promising in the last-level cache design <r e last-level cache design. Although ITRS predicts the write cycles of STT-RAM will be 10 15 at 2024 <ref type=""bibr"" target=""#b1"">[1]</ref>, the best available write cycles of STT-RAM are 4 x 10 12 at",0
"are 4 x 10 12 at present <ref type=""bibr"" target=""#b5"">[5]</ref>. Supposed we execute segmentation <ref type=""bibr"" target=""#b8"">[8]</ref>, a medical imaging application, on a 4GHz CPU with 32 KB L1 et=""#tab_0"">1</ref> shows the STT-RAM cell write count distribution of the segmentation application <ref type=""bibr"" target=""#b8"">[8]</ref> for both of the pure static <ref type=""bibr"" target=""#b10"">[ ll the applications have such characteristics, especially in the three medical imaging applications <ref type=""bibr"" target=""#b8"">[8]</ref> used in our study. As shown in Figure <ref type=""figure"" tar target=""#b21"">[21]</ref> (bzip2, mcf and lbm) and five applications from the medical imaging domain <ref type=""bibr"" target=""#b8"">[8]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=",0
"ardware mechanism that predicts ""when"" a block in a data cache becomes evictable. In a recent paper <ref type=""bibr"" target=""#b6"">[7]</ref>, we proposed trace-based predictors that record a trace of s placement and a subsequent address to prefetch for the corresponding block frame. In a recent paper <ref type=""bibr"" target=""#b6"">[7]</ref>, we proposed Last-Touch Predictors (LTPs) to predict memory g array and stores a trace encoding associated with every tag. We use truncated addition (as before <ref type=""bibr"" target=""#b6"">[7]</ref>) to maintain a fixed-size encoding for every instruction tra ave dead-block signatures that are proper subsequences of each other resulting in subtrace aliasing <ref type=""bibr"" target=""#b6"">[7]</ref>. To prevent aliasing, DBP maintains deadblock signatures per yond just predicting memory invalidation and sharing for scientific applications in multiprocessors <ref type=""bibr"" target=""#b6"">[7]</ref>. The results corroborate the intuition that because memory i is that while data structures are often referenced in multiple distinct program contexts or phases <ref type=""bibr"" target=""#b6"">[7]</ref> (e.g., a given sequence procedure invocations), they are not",1
"tunately, because working set sizes in a cache may largely vary both within and across applications <ref type=""bibr"" target=""#b17"">[ 18]</ref>, LRU stacks fail to predict dead blocks accurately. The g",0
"target=""#b2"">3,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b5"">6]</ref> or software <ref type=""bibr"" target=""#b8"">[9,</ref><ref type= as strided accesses <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b5"">6]</ref> and accesses to linked data structures <ref type=""bibr"" targe ry instruction PC in a small table and prefetch based on the stride. Jouppi proposed stream buffers <ref type=""bibr"" target=""#b5"">[6]</ref> to detect unit stride cache miss address sequences and corre",0
"arget=""#b16"">[17,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" targe he literature that target generalized memory access patterns <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b2"">3]</ref> -including strided accesses, and indirect accesses to linked efetcher implementation</head><p>Unlike previous MCP studies <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b2"">3]</ref> that evaluated the prefetcher's effectiveness for single-issu he related work in Section 5. In this section, we will focus on Miss Correlating Prefetchers (MCPs) <ref type=""bibr"" target=""#b2"">[3]</ref>, the most effective of current hardware prefetchers applicab data structures and associate them with instruction PCs to initiate a prefetch. Charney and Reeves <ref type=""bibr"" target=""#b2"">[3]</ref> were first to use address correlation in hardware on the L1",0
"in that represents the DUT. The Markov Chain is then used to generate test-cases for the design. In <ref type=""bibr"" target=""#b10"">[11]</ref>, the coverage analysis results trigger a set of generation",1
"the coverage analysis to automatically modify the directives to the test generator. For example, in <ref type=""bibr"" target=""#b1"">[2]</ref>, a genetic algorithm is used to select and modify test-cases",1
"based schemes. Bayesian networks also play a crucial role in diagnosis and decision support systems <ref type=""bibr"" target=""#b9"">[10]</ref>.</p><p>Obviously, there's a computational problem in dealin",0
"ing interest in using data to learn both the structure and probabilities of a Bayesian network (cf. <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target ion of time, thus modeling temporal dependencies in a stochastic process. The second extension (see <ref type=""bibr"" target=""#b2"">[3]</ref>) enriches the Bayesian network paradigm with decision making s used and the resulting coverage tasks. We then use one of the many known learning algorithms (cf. <ref type=""bibr"" target=""#b2"">[3]</ref>) to estimate the Bayesian network's parameters (i.e. the set",0
"amily of coverage events that share common properties are grouped together to form a coverage model <ref type=""bibr"" target=""#b6"">[7]</ref>. Members of the coverage model are called coverage tasks and l are called coverage tasks and are considered part of the test plan. Cross-product coverage models <ref type=""bibr"" target=""#b6"">[7]</ref> are of special interest. These models are defined by a basic",0
"e test generators and to better hit areas or specific tasks in the design that are not covered well <ref type=""bibr"" target=""#b4"">[5]</ref>.</p><p>The analysis of coverage reports, and their translati",0
"b1"">[2]</ref>, a genetic algorithm is used to select and modify test-cases to increase coverage. In <ref type=""bibr"" target=""#b12"">[13]</ref>, coverage analysis data is used to modify the parameters o",0
"ensionality""). The main breakthrough emerged in the late 1980s and can be attributed to Judea Pearl <ref type=""bibr"" target=""#b11"">[12]</ref>, who introduced 'modularity', thus enabling large and comp o components represent a unique joint probability distribution over the complete set of variables X <ref type=""bibr"" target=""#b11"">[12]</ref>. The joint probability distribution is given by the follow Bayesian network (cf. <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b11"">12]</ref>).</p><p>Typical types of queries that can be efficiently an",0
"ed via different directions, the chances to discover hidden bugs related to this task are increased <ref type=""bibr"" target=""#b7"">[8]</ref>.</p><p>In the past, two general approaches for CDG have been",0
"is used to generate test directives designed to accurately hit the coverage tasks. For example, in <ref type=""bibr"" target=""#b13"">[14]</ref> an FSM model of pipelines is used to generate tests that c",0
"addition and subtraction algorithms.</p><p>Other recent algorithms for modular arithmetic appear in <ref type=""bibr"" target=""#b2"">[3]</ref>, <ref type=""bibr"" target=""#b5"">[6]</ref>.</p><p>Fix N &gt; 1",1
"//www.tei-c.org/ns/1.0""><p>1. Description. Some algorithms <ref type=""bibr"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b3"">[4]</ref>, <ref type=""bibr"" t",0
"xml:lang=""en""> 		<body> <div xmlns=""http://www.tei-c.org/ns/1.0""><p>1. Description. Some algorithms <ref type=""bibr"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" t",0
"=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b3"">[4]</ref>, <ref type=""bibr"" target=""#b4"">[5]</ref> require extensive modular arithmetic. We propose a represent",0
"p>Other recent algorithms for modular arithmetic appear in <ref type=""bibr"" target=""#b2"">[3]</ref>, <ref type=""bibr"" target=""#b5"">[6]</ref>.</p><p>Fix N &gt; 1. Define an N-residue to be a residue cla",0
"d Error-Driven Learning is an effective learning algorithm which was proposed by Eric Brill in 1992 <ref type=""bibr"" target=""#b13"">[14]</ref> . It could obtain transformation rules automatically durin",1
"ype=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b2"">3]</ref> , Hidden Markov Model (HMM) <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b4"">5]</ref> , Condition Random Fie",0
"ation of existing model and the specifications of the new corpus to obtain the transformation rules <ref type=""bibr"" target=""#b12"">[13]</ref> . If only adjust the templates properly and learn again, t optimum conversion list of rules from tens of thousands of candidate rules is a NP-complete problem <ref type=""bibr"" target=""#b12"">[13]</ref> . So, we adopt an approximate algorithm to optimize the se",0
".</p><p>The statistical models which were used in Japanese NER, such as Maximum Entropy Model (MEM) <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b2"">3]</ref> , Hidden Markov Model",0
"ithreaded SPL fabric shared by four single issue out-of-order processor cores. In our previous work <ref type=""bibr"" target=""#b44"">[43]</ref>, we evaluated the use of SPL with a range of in-order and re possible, but this consideration is beyond the scope of this paper.</p><p>Each SPL, adopted from <ref type=""bibr"" target=""#b44"">[43]</ref> and shown in more detail in Figures <ref type=""figure"" tar n 2, we provide an overview of the SPL microarchitecture. A more complete treatment is available in <ref type=""bibr"" target=""#b44"">[43]</ref>. </p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head d n=""2.1"">SPL Hardware Microarchitecture</head><p>We adopt the row-based fabric of our earlier work <ref type=""bibr"" target=""#b44"">[43]</ref> in which the space of shared SPL configurations was explor ately available after the initial configuration overhead is paid.</p><p>Using our analytical models <ref type=""bibr"" target=""#b44"">[43]</ref>, we arrive at the area and power results for eight single ficient as well. While one might consider shrinking the private SPL even further, our previous work <ref type=""bibr"" target=""#b44"">[43]</ref> has shown that that this yields poor performance.</p></div e four processor cores, as well as spatial partitioning to permit private or semi-private operation <ref type=""bibr"" target=""#b44"">[43]</ref>. Spatial partitioning is enabled by inserting additional m arger speedups than the product of the two techniques applied in isolation.</p><p>Our previous work <ref type=""bibr"" target=""#b44"">[43]</ref> identifies a number of characteristics of past reconfigura e impact of incorporating the fabric with processors of different complexity. While the emphasis of <ref type=""bibr"" target=""#b44"">[43]</ref> is on the fabric design, this paper proposes a complete ha he four processor cores, as well as spatial partitioning to permit private or semi-private operation<ref type=""bibr"" target=""#b44"">[43]</ref>. Spatial partitioning is enabled by inserting additional m",1
"alysis, and stability control. Prior work in optimizing resource allocation during different phases <ref type=""bibr"" target=""#b15"">[14,</ref><ref type=""bibr"" target=""#b34"">33]</ref> only address singl",0
"ve the performance and power efficiency of microprocessors <ref type=""bibr"" target=""#b24"">[23,</ref><ref type=""bibr"" target=""#b42"">41]</ref>. Researchers have proposed specialized fabrics that are spe urable Processors</head><p>Several excellent survey papers <ref type=""bibr"" target=""#b24"">[23,</ref><ref type=""bibr"" target=""#b42"">41]</ref> provide an overview of the contributions of prior reconfigu",0
"reconfigurable coprocessor and allows different instruction sets to be loaded into the coprocessor <ref type=""bibr"" target=""#b13"">[12]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head",0
"ong multiple threads <ref type=""bibr"" target=""#b9"">[9,</ref><ref type=""bibr"" target=""#b26"">25,</ref><ref type=""bibr"" target=""#b39"">38</ref>]. Sun's UltraSPARC T1 <ref type=""bibr"" target=""#b40"">[39]</r",0
"<ref type=""bibr"" target=""#b3"">[2,</ref><ref type=""bibr"" target=""#b5"">4]</ref> and the predictor in <ref type=""bibr"" target=""#b13"">[12]</ref> separate the stream of misses according to the PC of the m we show that it can be easily implemented on a simple extension to the Global History Buffer (GHB) <ref type=""bibr"" target=""#b13"">[12]</ref>, which is a convenient structure to implement various pref Bench suites and compare it against both a state-of-the-art PC based localization prefetcher -PC/DC <ref type=""bibr"" target=""#b13"">[12]</ref> -and its non-localized counterpart -G/DC <ref type=""bibr"" ages of localization and correlation, a taxonomy to classify prefetching algorithms was proposed in <ref type=""bibr"" target=""#b13"">[12]</ref>. In this taxonomy, each algorithm is denoted as a pair X/Y alization and constant stride correlation would be referred to as PC/CS, and the best prefetcher in <ref type=""bibr"" target=""#b13"">[12]</ref> uses PC based localization and address delta correlation a d amortize long memory access times, these prefetchers often resort to large degrees of prefetching <ref type=""bibr"" target=""#b13"">[12]</ref>. The final result is that this leads to two unfortunate be d><p>Many (hardware) data structures can be used to implement different prefetching algorithms, but <ref type=""bibr"" target=""#b13"">[12]</ref> has proposed a common data structure that is flexible enou alization schemes. Figure <ref type=""figure"">2a</ref> shows an example GHB for the PC/DC prefetcher <ref type=""bibr"" target=""#b13"">[12]</ref>. A detailed description of a hardware implementation of de y, such as tables: reduction of stale data, a more complete history, and lower storage requirements <ref type=""bibr"" target=""#b13"">[12]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head ely the Miss Graph prefetcher. To accommodate the new level of operation, we extend the taxonomy of <ref type=""bibr"" target=""#b13"">[12]</ref> with a third term, so that prefetching schemes are denoted ntries of the IT. As misses occur, the IT and the GHB are populated as described in Section 2.3 and <ref type=""bibr"" target=""#b13"">[12]</ref>. The new PreviousIT pointer is left pointing to the last s re-quires are the NextIT and Ctr for each IT entry and a single PreviousIT register. As observed in <ref type=""bibr"" target=""#b13"">[12]</ref> and in our own experience, both an IT and a GHB with 512 e <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.1"">Prefetching Algorithms</head><p>We use PC/DC <ref type=""bibr"" target=""#b13"">[12]</ref> as a representative of a modern highperformance prefetcher >[9]</ref> for TLB prefetching. The adaptation of delta correlation to data prefetching was done in <ref type=""bibr"" target=""#b13"">[12]</ref>, where the PC/DC prefetcher was proposed. That work also p red it against a state-of-the-art memory access instruction PC based localization prefetcher -PC/DC <ref type=""bibr"" target=""#b13"">[12]</ref>. Experimental results showed that the proposed prefetcher =""#b13"">[12]</ref> -and its non-localized counterpart -G/DC <ref type=""bibr"" target=""#b10"">[9,</ref><ref type=""bibr"" target=""#b13"">12]</ref>. Experimental results show that the proposed prefetcher con on that generated them <ref type=""bibr"" target=""#b3"">[2,</ref><ref type=""bibr"" target=""#b5"">4,</ref><ref type=""bibr"" target=""#b13"">12]</ref>, or to the region of memory they reference <ref type=""bibr"" ntially lead to higher performance gains than those that treat all misses as a single global stream <ref type=""bibr"" target=""#b13"">[12,</ref><ref type=""bibr"" target=""#b16"">15]</ref>, although this var for each stream depending on the ""strength"" of the links followed.</p><p>As with other prefetchers <ref type=""bibr"" target=""#b13"">[12,</ref><ref type=""bibr"" target=""#b14"">13]</ref>, in order to avoid localization and was shown to consistently outperform other localized and non-localized prefetchers <ref type=""bibr"" target=""#b13"">[12,</ref><ref type=""bibr"" target=""#b16"">15]</ref>. We also use a G/D",1
"but only indirectly in as much as there is or not temporal locality in the use of data. The work in <ref type=""bibr"" target=""#b11"">[10]</ref> proposed localizing streams based on a history of the last spatial stream. Mostly orthogonal to the issues of how to localize streams and correlate addresses, <ref type=""bibr"" target=""#b11"">[10]</ref> and <ref type=""bibr"" target=""#b6"">[5]</ref> proposed using",0
"of execution context localization. The predictors in <ref type=""bibr"" target=""#b14"">[13]</ref> and <ref type=""bibr"" target=""#b20"">[19]</ref> separate the stream of misses according to memory address ut with delta correlations and with dynamic tuning of region sizes and prefetch degree. The work in <ref type=""bibr"" target=""#b20"">[19]</ref> proposed a similar address localization using memory regio eaming <ref type=""bibr"" target=""#b23"">[22]</ref> to recurring spatially correlated blocks of misses <ref type=""bibr"" target=""#b20"">[19]</ref>, instead of individual misses, and a mechanism to reconstr ory they reference <ref type=""bibr"" target=""#b14"">[13,</ref><ref type=""bibr"" target=""#b15"">14,</ref><ref type=""bibr"" target=""#b20"">19]</ref>, or to some period of time in which the misses occur <ref t",0
"c.org/ns/1.0""><head n=""4.3"">Benchmarks</head><p>We use programs from the SPEC CPU 2006 and BioBench <ref type=""bibr"" target=""#b1"">[1]</ref> benchmark suites running the Reference data set, except clus",0
"comes unfeasible. A recent relaxation in language modeling <ref type=""bibr"" target=""#b28"">[27,</ref><ref type=""bibr"" target=""#b29"">28]</ref> turns the prediction problem on its head. First, instead of",1
"y learning clusters <ref type=""bibr"" target=""#b34"">[33]</ref>, by adding edges between nearby nodes <ref type=""bibr"" target=""#b15"">[14]</ref>, by using PageRank <ref type=""bibr"" target=""#b25"">[24]</re",0
"o use relational data as part of the classification process, but are quite slow unless approximated <ref type=""bibr"" target=""#b21"">[20]</ref>. Our approach is complementary; instead of encoding the st",0
". We learn our latent social representations, instead of computing statistics related to centrality <ref type=""bibr"" target=""#b13"">[13]</ref> or partitioning <ref type=""bibr"" target=""#b42"">[41]</ref>. ive ones).</p><p>A number of techniques for generating features from graphs have also been proposed <ref type=""bibr"" target=""#b13"">[13,</ref><ref type=""bibr"" target=""#b18"">17,</ref><ref type=""bibr"" ta",0
"in order to survive.</p><p>In this paper we introduce deep learning (unsupervised feature learning) <ref type=""bibr"" target=""#b3"">[3]</ref> techniques, which have proven successful in natural language",0
"be more efficient <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b27"">29,</ref><ref type=""bibr"" target=""#b36"">38,</ref><ref type=""bibr"" target=""#b37"">39]</ref>.</p><p>In this work ing sure that the search index fits in memory, eliminating page faults and minimizing disk activity <ref type=""bibr"" target=""#b36"">[38]</ref>. We simulate clients using the Faban driver. The clients a ng in inefficiency through waste of resources. At the same time, our results corroborate prior work <ref type=""bibr"" target=""#b36"">[38]</ref>, indicating that niche processors offer excessively simple Microsoft and showed the implications of such workloads on data-center server design. Reddi et al. <ref type=""bibr"" target=""#b36"">[38]</ref> characterized the Bing search engine, showing that the com nt research activity on characterizing particular scale-out workloads, either micro-architecturally <ref type=""bibr"" target=""#b36"">[38,</ref><ref type=""bibr"" target=""#b40"">42]</ref>, or at the system",1
"ation environments <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b32"">34,</ref><ref type=""bibr"" target=""#b34"">36]</ref>. We include traditi",0
"reds of disks and flash devices, as it is traditionally done with largescale database installations <ref type=""bibr"" target=""#b42"">[44]</ref>, we construct a network-attached iSCSI storage array by cr",0
"scalar out-of-order processors cannot be performed precisely due to overlapped work in the pipeline <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b26"">28]</ref>. We present execut",0
"get=""#b28"">30,</ref><ref type=""bibr"" target=""#b29"">31,</ref><ref type=""bibr"" target=""#b30"">32,</ref><ref type=""bibr"" target=""#b39"">41]</ref>. To the best of our knowledge, our study is the first work",0
"UMON-global is still substantial. To reduce the overhead of UMON, we use Dynamic Set Sampling (DSS) <ref type=""bibr"" target=""#b11"">[12]</ref>. The key idea behind DSS is that the behavior of the cache e use 32 sets for UMON-DSS. The sampled sets for UMON-DSS are chosen using the simple static policy <ref type=""bibr"" target=""#b11"">[12]</ref>, which means set 0 and every 33rd set is selected. For the m (MLP) now fits in the cache which reduces the average MLP and increases the average mlp-based cost<ref type=""bibr"" target=""#b11"">[12]</ref> of each miss.</p></note> 			<note xmlns=""http://www.tei-c. > 			<note xmlns=""http://www.tei-c.org/ns/1.0"" place=""foot"" n=""4"" xml:id=""foot_2""><p>DSS was used in<ref type=""bibr"" target=""#b11"">[12]</ref> to choose between two replacement policies. Thus, it was u allocate) by using the hit counter information of the sampled sets. Therefore the bounds derived in<ref type=""bibr"" target=""#b11"">[12]</ref> are not applicable to our mechanism.</p></note> 			<note x",1
"ven the other application is zero). To address this shortcoming of the greedy algorithm, Suh et. al <ref type=""bibr"" target=""#b17"">[18]</ref> propose to also invoke the greedy algorithm for each combi on-convex points of all the competing applications can be very large. To avoid the time complexity, <ref type=""bibr"" target=""#b17"">[18]</ref> suggests that the greedy algorithm be invoked only for som partitioning of shared cache was first investigated by Suh et al. <ref type=""bibr"">[17][18]</ref>. <ref type=""bibr"" target=""#b17"">[18]</ref> describes a low-overhead scheme that uses recency position",0
"made by the partitioning algorithm, the baseline LRU policy is augmented to enable way partitioning <ref type=""bibr"" target=""#b1"">[2]</ref>[18] <ref type=""bibr"" target=""#b5"">[6]</ref>. To implement wa ity information can be obtained for a given application is also dependent on the other application. <ref type=""bibr"" target=""#b1"">(2)</ref> The recency position at which the application gets a hit is",0
"ency-hiding ability of multithreaded architectures. Unlike conventional multithreaded architectures <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target",1
"f type=""bibr"" target=""#b2"">[3]</ref>, Gunther <ref type=""bibr"" target=""#b11"">[12]</ref>, Li and Chu <ref type=""bibr"" target=""#b15"">[16]</ref>, and Govindarajan, et al., <ref type=""bibr"" target=""#b9"">[",0
"x 2-bit PHT is accessed by the XOR of the lower bits of the address and the global history register <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b29"">30]</ref>. Return destinatio",0
"chitectures. Unlike conventional multithreaded architectures <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" targe struction queues. This achieves three purposes: (1) it prevents any one thread from filling the IQ, <ref type=""bibr"" target=""#b1"">(2)</ref>   priority to threads that are moving instructions through t ltithreading, but assign work onto processors at a coarser level than individual instructions. Tera <ref type=""bibr"" target=""#b1"">[2]</ref> combines LIW with fine-grain multithreading.</p></div> <div",0
"iction is provided by a decoupled branch target buffer (BTB) and pattern history table (PHT) scheme <ref type=""bibr"" target=""#b3"">[4]</ref>. We use a 256entry BTB, organized as four-way set associativ",0
"cycle-bycycle schedule repeatedly. Caching these schedules is the motivation for Execution Caching <ref type=""bibr"" target=""#b46"">[47]</ref>, to allow the scheduler to be shut down (which they argue dynamic schedules should be generated. The major motivation in this regard has been energy savings, <ref type=""bibr"" target=""#b46"">[47,</ref><ref type=""bibr"" target=""#b48"">49]</ref>, generally at the via the dynamic scheduler (upwards of 70%, 45% and 20% for <ref type=""bibr"" target=""#b33"">[34,</ref><ref type=""bibr"" target=""#b46"">47,</ref><ref type=""bibr"" target=""#b48"">49]</ref> respectively) rathe",1
"get=""#b29"">30,</ref><ref type=""bibr"" target=""#b37"">38,</ref><ref type=""bibr"" target=""#b43"">44,</ref><ref type=""bibr"" target=""#b49"">50]</ref>. While insightful and prescient in many respects especially",0
"stently provide higher single-thread performance relative to comparably provisioned inorder designs <ref type=""bibr"" target=""#b21"">[22]</ref>.</p><p>The common explanation for the IO/OOO performance d P relative to the OOO including run-ahead execution (Power6) and Subordinate scout threading (Rock) <ref type=""bibr"" target=""#b21"">[22]</ref>. Yet, a recent in-order design that combined a sophisticat tore-buffer with a good compiler was still some 42% slower than a comparably provisioned OOO design <ref type=""bibr"" target=""#b21"">[22]</ref>. Why? Figures 1 and 2 highlight a major challenge for in-o at which point they must stall (""stall on use"").</p><p>In iCFP (in-order Continual Flow Pipelining) <ref type=""bibr"" target=""#b21"">[22]</ref>, this sliceout mechanism requires the much the same specul o achieve, on average, 47% of the performance of the OOO, which is consistent with previous results <ref type=""bibr"" target=""#b21"">[22]</ref>. This same work suggests that coupling a comparably provis nced our simulator to implement a simple implementation of in-order Continual Flow Pipelining (CFP) <ref type=""bibr"" target=""#b21"">[22]</ref>, as described in Section 3.5. The dynamism that iCFP provi ground between these extremes is provided by <ref type=""bibr"" target=""#b2"">[3]</ref>. We chose iCFP <ref type=""bibr"" target=""#b21"">[22]</ref> due to its recentness, its focus on in-order processors an line blocking problem inherent to cache misses in IO designs <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b21"">22]</ref>. In contrast, the enduring trend over several generations o",0
"the load is ready to execute as soon as it enters the window. In the parlance of Fields criticality <ref type=""bibr"" target=""#b13"">[14]</ref>, the consumer of the load issued prior to the mispredicted",0
"cilities (very large ROB, renamer, larger Load/Store Queue, dynamic/adaptive memory disambiguation) <ref type=""bibr"" target=""#b48"">[49]</ref> and the generally lower cost of misspeculation; the OOO ca hoisting instructions is difficult to match both in theory <ref type=""bibr"" target=""#b44"">[45,</ref><ref type=""bibr"" target=""#b48"">49]</ref> and practice <ref type=""bibr"" target=""#b16"">[17]</ref>. Mat e major motivation in this regard has been energy savings, <ref type=""bibr"" target=""#b46"">[47,</ref><ref type=""bibr"" target=""#b48"">49]</ref>, generally at the cost of performance degradation though of %, 45% and 20% for <ref type=""bibr"" target=""#b33"">[34,</ref><ref type=""bibr"" target=""#b46"">47,</ref><ref type=""bibr"" target=""#b48"">49]</ref> respectively) rather than executing the captured OOO traces",0
"SMARTS <ref type=""bibr"" target=""#b28"">[29]</ref>) and representative sampling (as done in SimPoint <ref type=""bibr"" target=""#b21"">[22]</ref>). Our experimental results using the SPEC CPU2000 benchmar st well known representative sampling approach is the SimPoint approach proposed by Sherwood et al. <ref type=""bibr"" target=""#b21"">[22]</ref>. SimPoint picks a small number of sampling units that accu onsiders multiple randomly chosen cluster centers and uses the Bayesian Information Criterion (BIC) <ref type=""bibr"" target=""#b21"">[22]</ref> to assess the quality of the clustering: the clustering wi ifferent ways for doing so, such as code working sets <ref type=""bibr"" target=""#b5"">[6]</ref>, BBVs <ref type=""bibr"" target=""#b21"">[22]</ref>, procedure calls <ref type=""bibr"" target=""#b12"">[13]</ref>",1
"havior, which is a related but different problem.</p><p>In their follow-on work, Isci and Martonosi <ref type=""bibr"" target=""#b14"">[15]</ref> compare clustering based on BBVs versus processor componen",0
"EnergyBench benchmark suite, which reports energy consumption while running performance benchmarks <ref type=""bibr"" target=""#b17"">[18]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head",0
"ide superscalar microprocessor such as the Alpha EV7 (21364). Power is estimated using Wattch v1.02 <ref type=""bibr"" target=""#b1"">[2]</ref> and HotLeakage <ref type=""bibr"" target=""#b22"">[23]</ref> ass",0
"arge gap between maximum and typical power consumption. Dynamic thermal management (DTM) techniques <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b22"">23]</ref> seek to exploit this",0
"hing that often occurs in direct-mapped buffers. A 3-bit performance counter based on Heil's design <ref type=""bibr"" target=""#b16"">[17]</ref> tracks the effectiveness of each entry and is used to sele pendence chain. If the generating values are present then ARVI's predictions are near perfect. Heil <ref type=""bibr"" target=""#b16"">[17]</ref> proposed another approach that correlates on the differenc",1
"rget=""#b8"">[9,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b30"">31]</ref>, only small increme",0
"nce the L1 hybrid is used to filter easily predicted highly biased branches, a confidence estimator <ref type=""bibr"" target=""#b13"">[14]</ref> indicates whether the branch is more difficult to predict ch prediction process involve correlating the actual branch register values with the branch outcome <ref type=""bibr"" target=""#b13"">[14]</ref> using a conventional value predictor. The authors of the s",0
"ontinue to increase the number of stages in future designs <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b27"">28]</ref>. We have extended the base simulator to support two levels",0
"#b5"">[6]</ref>, criticality measures and their application <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b28"">29,</ref><ref type=""bibr"" target=""#b29"">30]</ref>, and decoupled arch riticality measures:</head><p>Load criticality was originally investigated by Srinivasan and Lebeck <ref type=""bibr"" target=""#b28"">[29,</ref><ref type=""bibr"" target=""#b29"">30]</ref> in order to improv",0
"accepted method of determining the critical path of program execution was proposed by Fields et al. <ref type=""bibr"" target=""#b5"">[5]</ref>. A graph of dynamic instructions is constructed, modeling ea sing directed graphs, and proposed a token-based hardware mechanism to approximate this in hardware <ref type=""bibr"" target=""#b5"">[5]</ref>. Runahead <ref type=""bibr"" target=""#b3"">[3,</ref><ref type=""",1
"nnels used so far. We also cut the number of L2 MSHR entries in half.</p><p>We use weighted speedup <ref type=""bibr"" target=""#b24"">[24]</ref> to quantify the schedulers' effects on throughput. To calc",0
"g the L2 misses marked critical. Based on Runahead and CLEAR <ref type=""bibr"" target=""#b3"">[3,</ref><ref type=""bibr"" target=""#b13"">13,</ref><ref type=""bibr"" target=""#b18"">18]</ref>-Recall that in outo t load when the prediction is correct, or to still warm up processor and cache structures otherwise <ref type=""bibr"" target=""#b13"">[13]</ref>. Checkpoint support is still needed.</p><p>Targeting these Runahead <ref type=""bibr"" target=""#b3"">[3,</ref><ref type=""bibr"" target=""#b18"">18]</ref> and CLEAR <ref type=""bibr"" target=""#b13"">[13]</ref> both propose mechanisms to alleviate the effect of blockin",0
"""#b8"">[8]</ref>, MORSE-P <ref type=""bibr"">[9,</ref><ref type=""bibr"" target=""#b16"">16]</ref>, PAR-BS <ref type=""bibr"" target=""#b17"">[17]</ref>, and TCM <ref type=""bibr"" target=""#b12"">[12]</ref>.</p></d scheduler on multiprogrammed workloads. In this section, we provide our results relative to PAR-BS <ref type=""bibr"" target=""#b17"">[17]</ref>. We also show results for the more recent TCM proposal <re se the time needed for all threads to reach the barrier. Fairness-oriented schedulers (e.g., PAR-BS <ref type=""bibr"" target=""#b17"">[17]</ref>, ATLAS <ref type=""bibr"" target=""#b11"">[11]</ref>) target r kloads from adding ranked criticality. CBP table is 64 entries, and PAR-BS's marking cap is set to 5<ref type=""bibr"" target=""#b17"">[17]</ref>.</figDesc><table><row><cell></cell><cell>Scheduler</cell><",0
"w that guaranteeing that these loads hit in the L1 cache increases performance by an average of 40% <ref type=""bibr"" target=""#b27"">[27]</ref>. For an online implementation, they mark loads as critical",0
"be accessed multiple times within a short time interval. However, as identi ed by prior work (e.g., <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" ta to mitigate the negative e ects of pollution and thrashing <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" tar llution or thrashing <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" tar ompiler-based techniques.</p><p>Much prior research (e.g., <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" tar e. When multiple threads share the cache, prior approaches <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b16"">17]</ref> learn the thrashing behavior of individual threads using a her prior approaches <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b16"">17]</ref> to address cache thrashing or pollution individually (see T target=""#b15"">[16]</ref> (TA-DIP), 2) Thread-aware dynamic re-reference interval prediction policy <ref type=""bibr"" target=""#b16"">[17]</ref> (TA-DRRIP), 3) Signature-based Hit Prediction using Instru ave di erent reuse behavior.</p><p>Thread-Aware Dynamic Re-Reference Interval Prediction (TA-DRRIP) <ref type=""bibr"" target=""#b16"">[17]</ref> improves upon TA-DIP by using a better replacement policy line LRU and DIP, the last-level cache uses the re-reference interval prediction replacement policy <ref type=""bibr"" target=""#b16"">[17]</ref>. For our EAF proposals, we assume that the operations on t mproves weighted speedup by 17% compared to the LRU policy. These workloads are known to have scans <ref type=""bibr"" target=""#b16"">[17]</ref> (accesses to a large number of blocks with no reuse) which EAF mechanism to 1) a cache following the LRU replacement policy, and 2) a cache following the RRIP <ref type=""bibr"" target=""#b16"">[17]</ref> policy for all 4-core workloads. As the gure shows, EAF-ca",1
"or research (e.g., <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" tar",0
"s with shared caches <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b30"">31]</ref>. The mechanisms to",0
"ntly running applications share the on-chip last-level cache <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target= ed processor-stalls. All systems use a 3-level cache hierarchy similar to some modern architectures <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b21"">22]</ref>. The L1 and L2 cache",0
"sed to modify the cache insertion policy to mitigate the negative e ects of pollution and thrashing <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" ta working set is larger than the cache size. When multiple threads share the cache, prior approaches <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b16"">17]</ref> learn the thrashin ent approaches that aim to prevent pollution or thrashing: 1) Thread-aware dynamic insertion policy <ref type=""bibr"" target=""#b15"">[16]</ref> (TA-DIP), 2) Thread-aware dynamic re-reference interval pr . We now describe each mechanism individually.</p><p>Thread-Aware Dynamic Insertion Policy (TA-DIP) <ref type=""bibr"" target=""#b15"">[16]</ref> addresses the thrashing problem by determining thrashing a storage overhead compared to certain other prior approaches <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b16"">17]</ref> to address cache th st closely related work on addressing pollution or thrashing <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" tar",0
"systems. A more detailed description of the protocol and a proof of its security were presented in <ref type=""bibr"" target=""#b12"">[14]</ref>, though the published version of that paper did not presen s modifications to the group in the PVL.</p><p>A proof of the consistency protocol was presented in <ref type=""bibr"" target=""#b12"">[14]</ref>, but is beyond the scope of this paper. At a high level, h",1
"haring to a CFS-like file system, but trusts the server for the integrity of read-shared data. SNAD <ref type=""bibr"" target=""#b14"">[16]</ref> can use digital signatures for integrity, but does not gua",0
"erver. Unlike previous Byzantine-fault-tolerant file systems <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b20"">22]</ref> that distribute trust but assume that a threshold fraction the file system state if users have no other evidence of each other's on-line activity.</p><p>Pond <ref type=""bibr"" target=""#b20"">[22]</ref> is a prototype of the OceanStore system, a large-scale dat",0
"growing interest in peer-to-peer storage systems comprised of potentially untrusted nodes. Farsite <ref type=""bibr"" target=""#b1"">[2]</ref> investigated the possibility of spreading such a file system",0
"sign we are using, a flaw was recently found in its proof of security and a better variant proposed <ref type=""bibr"" target=""#b9"">[11]</ref> with a correct proof. We intend to switch to the newer vari key. We also provide benchmarks for 6,000 bit keys, which can be broken with a work factor of 2 128 <ref type=""bibr"" target=""#b9"">[11]</ref>.</p><p>Figure <ref type=""figure"">7</ref> provides a represe",0
"d Error-Driven Learning is an effective learning algorithm which was proposed by Eric Brill in 1992 <ref type=""bibr"" target=""#b11"">[14]</ref> . It could obtain transformation rules automatically durin",1
".</p><p>The statistical models which were used in Japanese NER, such as Maximum Entropy Model (MEM) <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b2"">3]</ref> , Hidden Markov Model",0
"b4"">5]</ref> , Condition Random Field (CRF) <ref type=""bibr"" target=""#b5"">[6]</ref> , Decision Tree <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b7"">8]</ref> , Support Vector Machi",0
"the precision and the recall of the NEs which occur in the training corpus rarely would be degraded <ref type=""bibr"" target=""#b12"">[15]</ref> . Second, if the training corpus is limited, it is feasibl",0
"to improve performance. A bypass and insertion algorithm for exclusive LLCs was presented recently <ref type=""bibr"" target=""#b7"">[7]</ref>. It classifies blocks based on their access number in the L2",1
"reuse blocks, these studies can be classified into PC based <ref type=""bibr"" target=""#b5"">[5,</ref><ref type=""bibr"" target=""#b8"">8,</ref><ref type=""bibr"" target=""#b33"">33]</ref> and address based <re",0
"ef><ref type=""bibr"" target=""#b8"">8,</ref><ref type=""bibr"" target=""#b33"">33]</ref> and address based <ref type=""bibr"" target=""#b13"">[13,</ref><ref type=""bibr"" target=""#b15"">15,</ref><ref type=""bibr"" ta",0
"be used to index the BDCT, such as memory address, instruction program counter (PC), or many others <ref type=""bibr"" target=""#b35"">[35]</ref>. Previous studies have shown that methods using PC can be ks and retain them longer in the cache. Using signatures based on PC or instruction sequences, SHiP <ref type=""bibr"" target=""#b35"">[35]</ref> proposes a re-reference interval predictor to improve perf get=""#b20"">20,</ref><ref type=""bibr"" target=""#b21"">21,</ref><ref type=""bibr"" target=""#b23"">23,</ref><ref type=""bibr"" target=""#b35"">35,</ref><ref type=""bibr"" target=""#b37"">37]</ref>, the shortened PC i r"" target=""#b6"">[6]</ref>, or similar to Assertion 1 and 3 <ref type=""bibr"" target=""#b19"">[19,</ref><ref type=""bibr"" target=""#b35"">35,</ref><ref type=""bibr"" target=""#b37"">37]</ref>. However, since suc",0
"assified into PC based <ref type=""bibr"" target=""#b5"">[5,</ref><ref type=""bibr"" target=""#b8"">8,</ref><ref type=""bibr"" target=""#b33"">33]</ref> and address based <ref type=""bibr"" target=""#b13"">[13,</ref>",0
"rovement can be achieved.</p><p>We use a stream prefetcher <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b11"">12]</ref> similar to the one in IBM's POWER 4 <ref type=""bibr"" target rget=""#b6"">[7,</ref><ref type=""bibr"" target=""#b28"">29,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b11"">12]</ref> to indicate whether or not a cache line (or request) was br get=""#b27"">28,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b11"">12]</ref> have been proposed. Although these proposals have the simil we need to measure the run-time accuracy of the prefetcher <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b4"">5]</ref>. Therefore, we add an handling techniques <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b4"">5]</ref> aim to reduce the int .5"">Comparison with Prefetch-Aware DRAM Controllers</head><p>Prefetch-Aware DRAM Controllers (PADC) <ref type=""bibr"" target=""#b11"">[12]</ref> was proposed to maximize DRAM row buffer hits for useful r . As we discuss in Section 5.5, our mechanisms are complementary to prefetch-aware DRAM controllers <ref type=""bibr"" target=""#b11"">[12]</ref> which employ an adaptive prefetch handling technique that",1
"prefetching, out-of-order execution, and runahead execution <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b15"">16]</ref> are designed to amortize the cost of long memory latencies sals have explored increasing Memory-Level Parallelism (MLP) <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" tar",0
"wo different types of prefetchers: GHB (Global History Buffer)-based CZone Delta Correlation (C/DC) <ref type=""bibr"" target=""#b17"">[18]</ref> and PC-based stride <ref type=""bibr"" target=""#b0"">[1]</ref",0
"rget=""#b7"">[8,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b25"">26]</ref>. These works define M",0
"-based CZone Delta Correlation (C/DC) <ref type=""bibr"" target=""#b17"">[18]</ref> and PC-based stride <ref type=""bibr"" target=""#b0"">[1]</ref>. Both the C/DC and stride prefetchers accurately capture a s",0
"tical performance bottleneck in traditional server workloads <ref type=""bibr"" target=""#b9"">[9,</ref><ref type=""bibr"" target=""#b8"">8,</ref><ref type=""bibr"" target=""#b11"">11,</ref><ref type=""bibr"" targe he observation that instruction cache miss or instruction execution sequences are highly repetitive <ref type=""bibr"" target=""#b8"">[8,</ref><ref type=""bibr"" target=""#b9"">9]</ref>. These designs log the past sequences on subsequent triggers. The most recent proposal, Proactive Instruction Fetch (PIF) <ref type=""bibr"" target=""#b8"">[8]</ref>, can eliminate nearly all L1 instruction misses, but require ler indexing; we find that coverage saturates with only 64kB of storage (relative to over 200kB for <ref type=""bibr"" target=""#b8"">[8]</ref>). Using the gem5 <ref type=""bibr"" target=""#b4"">[4]</ref> ful control flow speculation to explore ahead of the instruction fetch unit. As shown by Ferdman et al. <ref type=""bibr"" target=""#b8"">[8]</ref>, these prefetchers suffer from interference caused by wrong ed leading to more accurate prefetching.</p><p>TIFS <ref type=""bibr"" target=""#b9"">[9]</ref> and PIF <ref type=""bibr"" target=""#b8"">[8]</ref> address the limitations of branchpredictor-directed prefetch entries, we first compress the sequence of misses using a scheme similar to that pro-posed for PIF <ref type=""bibr"" target=""#b8"">[8]</ref>. The key observation is that the misses associated with a pa che blocks in each Miss Table entry remains prohibitive. Fortunately, as observed by Ferdman et al. <ref type=""bibr"" target=""#b8"">[8]</ref>, misses tend to be closely clustered with only a few discont m best.</p><p>? PIF is our implementation of Proactive Instruction Fetch proposed by Ferdman et al. <ref type=""bibr"" target=""#b8"">[8]</ref>, which is the most effective instruction prefetching design",1
"TI-GET requests. Facebook has disclosed that MULTI-GETs play a central role in its use of memcached <ref type=""bibr"" target=""#b6"">[6]</ref>. This workload seeks to emulate the requests required to con",0
"eddedto server-class systems, include some form of next-line or stream-based instruction prefetcher <ref type=""bibr"" target=""#b13"">[13,</ref><ref type=""bibr"" target=""#b14"">14,</ref><ref type=""bibr"" ta their simplicity, next-line and stream prefetchers are widely deployed in industrial designs (e.g., <ref type=""bibr"" target=""#b13"">[13]</ref>). To our knowledge, more sophisticated hardware prefetcher",0
"ccess time estimates for RDIP's miss table and structures used in other prefetchers using CACTI 5.3 <ref type=""bibr"" target=""#b35"">[35]</ref>. </p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head e=""table"" target=""#tab_6"">2</ref>, we present static power requirements, which we obtain from CACTI <ref type=""bibr"" target=""#b35"">[35]</ref>. Based on Table <ref type=""table"" target=""#tab_6"">2</ref>",0
"get=""#b24"">24,</ref><ref type=""bibr"" target=""#b27"">27,</ref><ref type=""bibr"" target=""#b28"">28,</ref><ref type=""bibr"" target=""#b33"">33,</ref><ref type=""bibr"" target=""#b36"">36]</ref>. However, poor bran rget=""#b5"">[5,</ref><ref type=""bibr"" target=""#b27"">27,</ref><ref type=""bibr"" target=""#b28"">28,</ref><ref type=""bibr"" target=""#b33"">33]</ref>. Run-ahead execution <ref type=""bibr"" target=""#b22"">[22]</r t run-away growth in the number of prefetch candidates. The branch history guided prefetcher (BHGP) <ref type=""bibr"" target=""#b33"">[33]</ref> keeps track of the branch instructions executed and near f",0
"ads rely on temporal streaming to record, and subsequently replay, entire sequences of instructions <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b14"">15]</ref>. As a class, these r's inability to predict instruction cache misses that are not contiguous, stream-based prefetchers <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b14"">15]</ref> exploit the recurr to record instruction fetch streams in a similar vein to prior stream-based instruction prefetchers <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b14"">15]</ref>. We augment each c ory record and replay mechanisms from previously proposed per-core data and instruction prefetchers <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" ta ge of 90% of instruction cache misses, but necessitates over 200KB per core for its history storage <ref type=""bibr"" target=""#b13"">[14]</ref>.</p><p>Stream-based instruction prefetchers were proposed core area by 16% and 59% for two lean-core designs over the state-of-the-art instruction prefetcher <ref type=""bibr"" target=""#b13"">[14]</ref>. The absolute performance improvement on a suite of divers /p><p>The state-of-the-art stream-based instruction prefetcher is Proactive Instruction Fetch (PIF) <ref type=""bibr"" target=""#b13"">[14]</ref>, which extends earlier work on temporal streaming <ref typ l noise in streams introduced by the instruction cache replacement policy and branch mispredictions <ref type=""bibr"" target=""#b13"">[14]</ref>. To mitigate increased history storage requirements result re it to the state-of-the-art instruction prefetcher with per-core private instruction history, PIF <ref type=""bibr"" target=""#b13"">[14]</ref>, using trace-based and cycle-accurate simulations of a 16- effectiveness and history storage requirements with the state-ofthe-art instruction prefetcher, PIF <ref type=""bibr"" target=""#b13"">[14]</ref>. Like other stream-based instruction and data prefetchers F requires 32K spatial region records in the history buffer targeting 90% instruction miss coverage <ref type=""bibr"" target=""#b13"">[14]</ref>, also validated with our experiments. As a result, the his effectiveness, we first compare the fraction of instruction cache misses predicted by SHIFT to PIF <ref type=""bibr"" target=""#b13"">[14]</ref>. For the purposes of this study, we only track the predict eliminates 92% of the instruction cache misses with 13% overprediction, corroborating prior results <ref type=""bibr"" target=""#b13"">[14]</ref>. However, PIF_2K, which has the same aggregate storage ove treams of discontinuities in its history, enhancing the lookahead of discontinuity prefetching. PIF <ref type=""bibr"" target=""#b13"">[14]</ref>, records the complete retire-order instruction cache acces",1
"isses by exploiting code commonality across multiple threads <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b10"">11]</ref>. These approaches distribute the code footprint across priv",0
"et=""#b11"">[12,</ref><ref type=""bibr"" target=""#b30"">30,</ref><ref type=""bibr"" target=""#b36"">36,</ref><ref type=""bibr"" target=""#b42"">42]</ref>. To explore future code paths, an idle thread <ref type=""bi",0
"by discontinuities <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b30"">30,</ref><ref type=""bibr"" target=""#b36"">36,</ref><ref type=""bibr"" target=""#b42"">42]</ref>. To explore future",0
"LLC is under 1%.</p><p>The second source of overhead is due to the extra LLC traffic, generated by <ref type=""bibr"" target=""#b0"">(1)</ref> read and write requests to the history buffer; (2) useless L xplore future code paths, an idle thread <ref type=""bibr"" target=""#b20"">[20]</ref>, a helper thread <ref type=""bibr"" target=""#b0"">[1]</ref>, speculative threading mechanisms <ref type=""bibr"" target=""#",0
"ased on re-reference interval prediction and updates the predicted age on a hit in an inclusive LLC <ref type=""bibr"" target=""#b15"">[15]</ref>. Although such an option of age update is nonexistent in a he TC-AGE policy is similar to SRRIP-style static algorithms originally proposed for inclusive LLCs <ref type=""bibr"" target=""#b15"">[15]</ref>. In our age assignment setting where a lower age correspon licy shows one way to implement DRRIP-style dynamic policies originally proposed for inclusive LLCs <ref type=""bibr"" target=""#b15"">[15]</ref>.</p><p>The TC-AGE policy improves performance by more than",1
"last level (L3 in this study) of an inclusive design can end up making wrong replacement decisions <ref type=""bibr"" target=""#b14"">[14]</ref>. The risk of premature evictions from the L1 and L2 caches situation can further improve with access hints from the inner levels or other proactive mechanisms <ref type=""bibr"" target=""#b14"">[14]</ref>. This is not possible in an exclusive design. A block resi rid of premature evictions from inner level caches in an inclusive hierarchy have been proposed in <ref type=""bibr"" target=""#b14"">[14]</ref>. The importance of treating shared blocks specially in an",0
"all data caches.</p><p>These proposals require a profile pass that gathers the locality information <ref type=""bibr"" target=""#b17"">[17]</ref>, or build a PC and data address-based locality predictor <",0
"ion has already motivated commercial processor designers to adopt fully or partially exclusive LLCs <ref type=""bibr"" target=""#b1"">[1]</ref>. The performance gains in an exclusive design over an identi",0
".</p><p>Recently, van Renesse and Schneider presented a chain replication method for object storage <ref type=""bibr"" target=""#b46"">[47]</ref> over fail-stop servers, designed to provide strong consist bject storage system that, while maintaining the strong consistency properties of chain replication <ref type=""bibr"" target=""#b46"">[47]</ref>, provides lower latency and higher throughput for read ope ain, with transmission costs equally spread over all nodes. The simulation results of previous work <ref type=""bibr"" target=""#b46"">[47]</ref> showed competitive or superior throughput for CR compared cular, CRAQ's choice of allowing a node to join anywhere in a chain (as opposed only to at its tail <ref type=""bibr"" target=""#b46"">[47]</ref>), as well as properly handling failures during recovery, r An alternative approach, taken by GFS <ref type=""bibr"" target=""#b21"">[22]</ref> and promoted in CR <ref type=""bibr"" target=""#b46"">[47]</ref>, is to use the membership management service as a director lti-chain multi-object updates was heavily influenced by Sinfonia.</p><p>CRAQ and Chain Replication <ref type=""bibr"" target=""#b46"">[47]</ref> are both examples of object-based storage systems that exp",1
"e explores how to improve reliability while minimizing replica maintenance under transient failures <ref type=""bibr"" target=""#b10"">[11]</ref>. Strongly-consistent mutable data is considered by OceanSt",0
"ercial systems to sacrifice strong consistency semantics due to their perceived costs (as at Google <ref type=""bibr"" target=""#b21"">[22]</ref>, Amazon <ref type=""bibr"" target=""#b14"">[15]</ref>, eBay <r nsistent hashing <ref type=""bibr"" target=""#b28"">[29]</ref> or a more centralized directory approach <ref type=""bibr"" target=""#b21"">[22]</ref>-but these algorithms might still find load imbalances if p , when workloads are read mostly-an assumption used in other systems such as the Google File System <ref type=""bibr"" target=""#b21"">[22]</ref> and Memcached <ref type=""bibr"" target=""#b17"">[18]</ref>-th et=""#b14"">[15,</ref><ref type=""bibr"" target=""#b15"">16]</ref>. An alternative approach, taken by GFS <ref type=""bibr"" target=""#b21"">[22]</ref> and promoted in CR <ref type=""bibr"" target=""#b46"">[47]</re have traded consistency for scalability or operation under partitions. The Google File System (GFS) <ref type=""bibr"" target=""#b21"">[22]</ref> is a cluster-based object store, similar in setting to CRA",0
"/ref>.</p><p>Focusing on peer-to-peer settings, CFS provides a read-only filesystem on top of a DHT <ref type=""bibr"" target=""#b13"">[14]</ref>; Carbonite explores how to improve reliability while minim",0
"the figure by the dotted blue arrow-which returns its known version number for the requested object <ref type=""bibr"" target=""#b0"">(1)</ref>. The dirty node then returns the old object value (V 1 ) ass <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b33"">34]</ref> and quorum systems <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b36"">37]</ref>. These protocols pro",0
"mentioning branch predictor warmup is by Haskins and Conte <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b19"">20]</ref> in which they propose memory reference reuse latency (MRRL) mup length per sampling unit. (Note that the MRRL approach <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b19"">20]</ref> corresponds to a zero BHM history length.) We further obser",1
"ibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b8"">9]</ref> and (iii) compressed branch traces <ref type=""bibr"" target=""#b9"">[10]</ref> that are reduced through BHM. In other words, instead of ha gh BHM. In other words, instead of having branch traces of full benchmark executions as proposed in <ref type=""bibr"" target=""#b9"">[10]</ref>, BHM limits the length of the compressed branch traces. Thi not contained in the microarchitecture warmup.</p><p>For addressing this problem, Barr and Asanovic <ref type=""bibr"" target=""#b9"">[10]</ref> propose to employ branch trace compression. They store a co ranch predictor during sampled simulation. The branch trace compression scheme by Barr and Asanovic <ref type=""bibr"" target=""#b9"">[10]</ref> however does not address the issue of how far one needs to",0
"target=""#b16"">[17]</ref>, boundary line reuse latency (BLRL) <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b17"">18]</ref>, self-monitored adaptive cache warmup (SMA) <ref type=""bibr",0
"ces. Therefore, researchers have proposed sampled simulation <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target= r"" target=""#b2"">[3]</ref> and targeted sampling based on program phase behavior as done in SimPoint <ref type=""bibr"" target=""#b1"">[2]</ref>.</p><p>The second issue is how to get to those sampling unit ,</ref><ref type=""bibr"" target=""#b8"">9]</ref> (sampling unit size of 10K instructions) and SimPoint <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target",0
"""#b7"">[8,</ref><ref type=""bibr"" target=""#b17"">18]</ref>, self-monitored adaptive cache warmup (SMA) <ref type=""bibr"" target=""#b18"">[19]</ref>, memory hierarchy state (MHS) <ref type=""bibr"" target=""#b4",0
"r et al. for making replacement decisions in a software-managed UCA called the Indirect Index Cache <ref type=""bibr"" target=""#b8"">[9]</ref>. In our scheme, when a hit occurs to a cache line, it is swa hes built with discrete components <ref type=""bibr"" target=""#b16"">[17]</ref>. Hallnor and Reinhardt <ref type=""bibr"" target=""#b8"">[9]</ref> studied a fully associative software-managed design for larg",1
"ectmapped on-chip caches by virtually binding regions of the address space to portions of the cache <ref type=""bibr"" target=""#b4"">[5]</ref>. They also studied, as did Johnson and Hwu <ref type=""bibr""",0
"ted for modeling these long transmission channels, since it uses the Rubenstein RC wire delay model <ref type=""bibr"" target=""#b13"">[14]</ref> and assumes bit-line capacitative loading on each wire. We",0
"ge cache designs. Kessler examined designs for multi-megabyte caches built with discrete components <ref type=""bibr"" target=""#b16"">[17]</ref>. Hallnor and Reinhardt <ref type=""bibr"" target=""#b8"">[9]</",0
"as the bandwidth demands on the package grow, and as smaller technologies permit more bits per mm 2 <ref type=""bibr"" target=""#b12"">[13]</ref>.</p><p>Current multi-level cache hierarchies are organized",0
"ost will be increased.</p><p>This paper surveys the-state-of-the-art FTL algorithms. Gal and Toledo <ref type=""bibr"" target=""#b6"">[7]</ref> also provided algorithms and data structures for flash memor",1
"hat the file system layer in Fig. <ref type=""figure"" target=""#fig_0"">1</ref> is the FAT file system <ref type=""bibr"" target=""#b3"">[4]</ref>, which is widely used in many embedded systems. Fig. <ref ty sector, one or more file allocation tables, a root directory, and the volume files. A recent study <ref type=""bibr"" target=""#b3"">[4]</ref> contains a more detailed description of the FAT file system.",0
"r"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b13"">14]</ref>. At the core an FTL g-based FTL algorithms <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b12"">13]</ref> are proposed. Some detailed algorithms will be presented in div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.1."">Mitsubishi</head><p>The Mitsubishi algorithm <ref type=""bibr"" target=""#b12"">[13]</ref> is based on block mapping in Section 3.1.2. Its goal was t",0
"n be written to it. The memory portion for erasing differs in size from that for reading or writing <ref type=""bibr"" target=""#b1"">[2]</ref>, resulting in the major performance degradation of the overa are termed FTL (Flash Translation Layer) has been introduced <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target= is hardly feasible for small embedded systems. For this reason, block mapping-based FTL algorithms <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target",0
"to their work, the present study focuses on FTL algorithms and does not discuss file system issues <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" ta",0
")</ref> proposed FloWorM system that includes tracker, analyzer and reporter based on NetFlow data. <ref type=""bibr"" target=""#b5"">Abdulla et al. (2011)</ref> presented a support vector machine (SVM) m nti and Rossi, 2011)</ref>, or data fusion with other log files such as Snort, DNS related requests <ref type=""bibr"" target=""#b5"">(Abdulla et al., 2011)</ref> (number of DNS requests, response, normal",1
"lf-similarity of flow interval, entropy, kernel function, mutual information and Hellinger distance <ref type=""bibr"" target=""#b132"">(Valenti and Rossi, 2011)</ref>, or data fusion with other log files",0
", usage of transitional technologies of IPv6 to IPv4, interface identifier assignment schemes, etc. <ref type=""bibr"" target=""#b117"">(Shen et al., 2009;</ref><ref type=""bibr"" target=""#b152"">Zhang and M",0
"a coordinating mechanism for flow sampling, hash-based packet selection, and workload distributed. <ref type=""bibr"" target=""#b96"">Morariu and Stiller (2011)</ref> proposed a distributed IP traffic an",0
"et=""#b2"">[3]</ref><ref type=""bibr"" target=""#b3"">[4]</ref><ref type=""bibr"" target=""#b4"">[5]</ref>[8] <ref type=""bibr"" target=""#b9"">[10]</ref><ref type=""bibr"" target=""#b10"">[11]</ref><ref type=""bibr"" ta d 100003FA, which is often observed and utilized for prediction in the Markov prefetching algorithm <ref type=""bibr"" target=""#b9"">[10]</ref> . The following section discusses data prefetching methodol strides are recognizable. To capture repetitiveness in data reference addresses, Markov prefetching <ref type=""bibr"" target=""#b9"">[10]</ref> was proposed. This strategy assumes the history might repea",1
"""#b4"">[5]</ref>[8] <ref type=""bibr"" target=""#b9"">[10]</ref><ref type=""bibr"" target=""#b10"">[11]</ref><ref type=""bibr"" target=""#b11"">[12]</ref><ref type=""bibr"" target=""#b12"">[13]</ref><ref type=""bibr"" t",0
"re-based speculative precomputation <ref type=""bibr"">[12] [17]</ref> and data-driven multithreading <ref type=""bibr"" target=""#b17"">[18]</ref> are such examples. The techniques include simple prefetchi",0
"m. Numerous studies have been conducted and many strategies have been proposed for data prefetching <ref type=""bibr"" target=""#b1"">[2]</ref><ref type=""bibr"" target=""#b2"">[3]</ref><ref type=""bibr"" targe",0
"ither by a programmer or by a complier during the optimization phase. Recent work in helper threads <ref type=""bibr"" target=""#b18"">[19]</ref> , software-based speculative precomputation <ref type=""bib",0
"pe=""bibr"" target=""#b20"">[21]</ref> and the introduction of the TAGE predictor by Seznec and Michaud <ref type=""bibr"" target=""#b25"">[26]</ref>. In practice, on the traces distributed for the first two e predictors targeting special categories of branches with a state-of-the-art main predictor (TAGE, <ref type=""bibr"" target=""#b25"">[26]</ref>, OGEHL <ref type=""bibr"" target=""#b20"">[21]</ref>, Piecewis L <ref type=""bibr"" target=""#b7"">[8]</ref>), e.g. a loop predictor with the TAGE predictor in L-TAGE <ref type=""bibr"" target=""#b25"">[26]</ref> or the address-branch correlator in <ref type=""bibr"" targe ns/1.0""><head n=""3."">Background on the TAGE Predictor</head><p>The TAGE predictor was introduced in <ref type=""bibr"" target=""#b25"">[26]</ref> and is the core predictor of the L-TAGE predictor that won h a single 4-bit counter USE_ALT_ON_NA was found to allow to (slightly) improve prediction accuracy <ref type=""bibr"" target=""#b25"">[26]</ref>. The prediction computation algorithm is as follows:</p><p ts showed that one can use wider tag for long histories for a better tradeoff. Previous experiments <ref type=""bibr"" target=""#b25"">[26]</ref> have shown that the TAGE predictor performs efficiently on",1
"arget=""#b20"">21,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" targe",0
"target=""#b7"">8,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b17"">18]</ref>. However, tremendous progress were made during this period:",0
"ive. This technique was already mentioned by Baniasadi and Moshovos in the context of energy saving <ref type=""bibr"" target=""#b1"">[2]</ref>.</p><p>We confirmed this assumption by simulations of 3 very",0
"new solutions combining local history and global history were proposed for neural based predictors <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b7"">8]</ref>. Up to now, there ha",0
"l.</ref> were the first to propose the concept of dead-block prediction and a trace-based predictor <ref type=""bibr"" target=""#b15"">[16]</ref>, which predicts a block dead once it has been accessed by #b0"">[1]</ref>, <ref type=""bibr"" target=""#b6"">[7]</ref>, <ref type=""bibr"" target=""#b13"">[14]</ref>, <ref type=""bibr"" target=""#b15"">[16]</ref>. Software solutions pass hints about dead-block informatio f> and PC based <ref type=""bibr"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b13"">[14]</ref>, <ref type=""bibr"" target=""#b15"">[16]</ref>. Compared to data-address based approaches, PC-based appro ed, and time-based.</p><p>Lai et al. were the first to propose the concept of dead-block prediction <ref type=""bibr"" target=""#b15"">[16]</ref> and a trace-based dead-block predictor for the L1 cache, c nt, bypassing, power reduction, and coherence protocol optimizations.</p><p>Prefetching: Lai et al. <ref type=""bibr"" target=""#b15"">[16]</ref> and Hu et al. <ref type=""bibr"" target=""#b6"">[7]</ref> used g compared to triggering prefetches on cache misses. Ferdman and Falsafi later extended the work in <ref type=""bibr"" target=""#b15"">[16]</ref> to store correlation patterns off-chip and stream them on- div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>By references By bursts Trace</head><p>RefTrace <ref type=""bibr"" target=""#b15"">[16]</ref> BurstTrace Prediction Counting RefCount <ref type=""bibr"" t rg/ns/1.0"" place=""foot"" n=""1"" xml:id=""foot_0""><p>RefTrace was evaluated on directly mapped caches in<ref type=""bibr"" target=""#b15"">[16]</ref>. This paper uses it on set-associative caches. In contrast",1
"dating some of the shared cache blocks early. Lai and Falsafi later proposed a last-touch predictor <ref type=""bibr"" target=""#b14"">[15]</ref> that uses PC traces to predict when shared cache blocks sh",0
"t=""#b20"">[21]</ref> (corner turn and vpenta), a speech recognition application (sphinx), and stream <ref type=""bibr"" target=""#b17"">[18]</ref>. For each benchmark, we simulate up to 2 billion instructi",0
"0""><head n=""1."">Introduction</head><p>Prior studies have shown that data caches have low efficiency <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b25"">[26]</ref>; only a small fra ."">Cache Efficiency</head><p>The concept of cache efficiency was first proposed by Burger et al. in <ref type=""bibr"" target=""#b1"">[2]</ref>, where cache efficiency is defined as the average fraction o",0
"<ref type=""bibr"" target=""#b22"">[23]</ref>, <ref type=""bibr"" target=""#b27"">[28]</ref> or in hardware <ref type=""bibr"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b6"">[7]</ref>, <ref type=""bibr"" t ssified into two categories: dataaddress based <ref type=""bibr"" target=""#b6"">[7]</ref> and PC based <ref type=""bibr"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b13"">[14]</ref>, <ref type=""bibr"" not accessed in more than twice this number of cycles, it is predicted dead. Abella et al. proposed <ref type=""bibr"" target=""#b0"">[1]</ref> another timebased predictor to turn off dead blocks dynamica br"" target=""#b12"">[13]</ref>. Abella et al. proposed to turn off blocks in the L2 cache dynamically <ref type=""bibr"" target=""#b0"">[1]</ref>. Both schemes predict how many cycles have to pass before a ""#b13"">[14]</ref> BurstCount metric Time TimeKeeping <ref type=""bibr"" target=""#b6"">[7]</ref>, IATAC <ref type=""bibr"" target=""#b0"">[1]</ref> Future work</p></div> <div xmlns=""http://www.tei-c.org/ns/1.",0
"gment every physical register with a Superseded bit and a Pending count. This support is similar to <ref type=""bibr"" target=""#b16"">[17]</ref>. The Superseded bit marks whether the instruction that sup techniques.</p><p>The second category includes work related to register recycling. Moudgill et al. <ref type=""bibr"" target=""#b16"">[17]</ref> discuss performing early register recycling in out-of-orde er processors that support precise exceptions. However, the implementation of precise exceptions in <ref type=""bibr"" target=""#b16"">[17]</ref> relies on either checkpoint/rollback for every replay even",1
"es early to improve utilization, opts to build larger structures for these resources. Lebeck et al. <ref type=""bibr"" target=""#b9"">[10]</ref> propose a two-level hierarchical instruction window to keep",0
"ern out-of-order processors typically employ a reorder buffer (ROB) to retire instructions in order <ref type=""bibr"" target=""#b17"">[18]</ref>. In-order retirement enables precise bookkeeping of the ar ssor execu- of No Return (PNR). We assume a circular ROB implementation with Head and Tail pointers <ref type=""bibr"" target=""#b17"">[18]</ref>. tion seamlessly falls back to non-Cherry mode (Section 2. ckpoint, and then executes code in order until the excepting instruction is met. Smith and Pleszkun <ref type=""bibr"" target=""#b17"">[18]</ref> discuss several methods to support precise exceptions. The",0
"teed that all older loads that need to fetch their data from memory have already done so. Condition <ref type=""bibr"" target=""#b1"">(2)</ref> implies that all older loads are older than oldest´Í Ë µ (ty ld have an additive effect.</p><p>Finally, we notice that, concurrently to our work, Cristal et al. <ref type=""bibr"" target=""#b1"">[2]</ref> propose the use of checkpointing to allow early release of u",0
"ss how to combine Cherry and Speculative Multithreading (SM) <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" targe rom a sequential code and executed speculatively in parallel <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" targe",0
"arget=""#b8"">9,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" tar ing-based scheduling <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b32"">33]</ref>. Sampling-based sch pling-based strategy <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b32"">33]</ref> that assumes runnin gs. They evaluate both static and dynamic scheduling policies. In their followon work, Kumar et al. <ref type=""bibr"" target=""#b18"">[19]</ref> study scheduling on heterogeneous multi-cores while runnin",1
"target=""#b11"">[12]</ref>, AMD's Fusion <ref type=""bibr"" target=""#b0"">[1]</ref>, and NVidia's Tegra <ref type=""bibr"" target=""#b24"">[25]</ref>; or CPU plus accelerators, e.g., IBM's * A large part of t",0
"ce=""foot"" n=""1"" xml:id=""foot_0""><p>If the cache hierarchy is different, then techniques described in<ref type=""bibr"" target=""#b13"">[14]</ref> can be used to estimate misses for a different cache size.",0
"target=""#b2"">3,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" tar terogeneous multi-core is to apply sampling-based scheduling <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" tar ment. We compare PIE scheduling to a sampling-based strategy <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" tar ingle-ISA heterogeneous multicores, we only discuss this class of heterogeneity.</p><p>Kumar et al. <ref type=""bibr"" target=""#b17"">[18]</ref> made the case for heterogeneous single-ISA multi-core proc",0
"ng on heterogeneous multicores, and we propose the PIE method for doing so.</p><p>Patsilaras et al. <ref type=""bibr"" target=""#b27"">[28,</ref><ref type=""bibr"" target=""#b28"">29]</ref> study how to best",0
"aswell.</p><p>? We evaluate the performance of a state-of-the-art indirect branch predictor, ITTAGE <ref type=""bibr"" target=""#b30"">[31]</ref>, proposed in the literature on the same interpreters, and cades, and presents the state-of-the-art branch predictors TAGE for conditional branches and ITTAGE <ref type=""bibr"" target=""#b30"">[31]</ref> for indirect branches. Section 4 presents experimental set er) with a tagged (PC+global branch history) indexed table.</p><p>More recently, Seznec and Michaud <ref type=""bibr"" target=""#b30"">[31]</ref> derived IT-TAGE from their TAGE predictor. Instead of simp >We also experimented with a state-of-the-art branch predictor from the literature: TAGE and ITTAGE <ref type=""bibr"" target=""#b30"">[31]</ref>. The performance is provided through simulation of traces =""#b14"">[15]</ref> and geometric history length predictors <ref type=""bibr"" target=""#b27"">[28,</ref><ref type=""bibr"" target=""#b30"">31]</ref>. All these propositions have influenced the design of the p loop predictor and maybe a local history predictor), TAGE <ref type=""bibr"" target=""#b28"">[29,</ref><ref type=""bibr"" target=""#b30"">31]</ref> is generally considered as the state-of-the-art in global h",1
"ef type=""bibr"" target=""#b35"">[36]</ref>. Scientists at CERN also developed an interpreter for C/C++ <ref type=""bibr"" target=""#b6"">[7]</ref>.</p><p>Although they are designed for portability, interpret",0
"s hardware counters for retired indirect jumps, but for ""speculative and retired indirect branches"" <ref type=""bibr"" target=""#b13"">[14]</ref>. It turns out that non-retired indirect branches are rare.",0
"gshare-like indexed table is used to store the indirect branch targets. However Driesen and Holzle <ref type=""bibr"" target=""#b7"">[8]</ref> point out that many indirect branches are correctly predicte",0
"ns motivating the choice of Jython for the data analysis software of the Herschel Space Observatory <ref type=""bibr"" target=""#b35"">[36]</ref>. Scientists at CERN also developed an interpreter for C/C+",0
"ge, or involve a language-specific encoder applied to each sentence whose outputs are then compared <ref type=""bibr"" target=""#b14"">(Hermann and Blunsom, 2014</ref>). An encoder neural network reads an",0
"eural machine translation is a newly emerging approach to machine translation, recently proposed by <ref type=""bibr"" target=""#b17"">Kalchbrenner and Blunsom (2013)</ref>, <ref type=""bibr"" target=""#b27"" as an additional feature in the phrase-based statistical machine translation system. More recently, <ref type=""bibr"" target=""#b17"">Kalchbrenner and Blunsom (2013)</ref> and <ref type=""bibr"" target=""#b ave proposed the use of neural networks to directly learn this conditional distribution (see, e.g., <ref type=""bibr"" target=""#b17"">Kalchbrenner and Blunsom, 2013;</ref><ref type=""bibr"" target=""#b6"">Ch at other architectures such as a hybrid of an RNN and a de-convolutional neural network can be used <ref type=""bibr"" target=""#b17"">(Kalchbrenner and Blunsom, 2013)</ref>.</p></div> <div xmlns=""http://",0
"d like to thank the developers of Theano <ref type=""bibr"" target=""#b4"">(Bergstra et al., 2010;</ref><ref type=""bibr"" target=""#b1"">Bastien et al., 2012)</ref>. We acknowledge the support of the followi",0
"m search to find a translation that approximately maximizes the conditional probability (see, e.g., <ref type=""bibr"" target=""#b11"">Graves, 2012;</ref><ref type=""bibr"" target=""#b5"">Boulanger-Lewandowsk",0
"kloads with a more pragmatic experiment approach in comparison with that of CloudSuite described in <ref type=""bibr"" target=""#b16"">[17]</ref>. We adopt larger input data sets varying from 147 to 187 G both the memory and disk systems instead of completely storing data (only 4.5GB for Naive Bayes in <ref type=""bibr"" target=""#b16"">[17]</ref>) in the memory system. And for each workload, we collect t performance data of the whole run time after the warm-up instead of a short period (180 seconds in <ref type=""bibr"" target=""#b16"">[17]</ref>).</p><p>We find that big data analytics applications share chip multiprocessors (PARSEC), and scale-out service (four among six benchmarks in ClousSuite paper <ref type=""bibr"" target=""#b16"">[17]</ref>) workloads. Meanwhile the service workloads in data center s, e.g., HPC-HPL, HPC-DGEMM and chip multiprocessors workloads.</p><p>• Corroborating previous work <ref type=""bibr"" target=""#b16"">[17]</ref>, both the big data analytics workloads and service workloa the big data analytics workloads and the service workloads (four among six benchmarks in ClousSuite <ref type=""bibr"" target=""#b16"">[17]</ref>, SPECweb and TPC-W) in terms of processor pipeline stall b vel cache), respectively. For the service workloads, our observations corroborate the previous work <ref type=""bibr"" target=""#b16"">[17]</ref>: the L2 cache is ineffective.</p><p>• For the big data ana rk of characterizing scale-out (data center) workloads on a micro-architecture level is Cloud-Suite <ref type=""bibr"" target=""#b16"">[17]</ref>. However, CloudSuite paper is biased towards online servic . The input data size varies from 147 to 187 GB. In comparison with that of CloudSuite described in <ref type=""bibr"" target=""#b16"">[17]</ref>, our approach are more pragmatic. We adopt a larger data i in both memory and disk systems instead of completely storing data (only 4.5 GB for Naive Bayes in <ref type=""bibr"" target=""#b16"">[17]</ref>) in memory. The number of instructions retired of the big , HPCC, PARSEC, TPC-W, SPECweb 2005, and CloudSuite-a scale-out benchmark suite for cloud computing <ref type=""bibr"" target=""#b16"">[17]</ref>, and compared them with big data analytics workloads.</p>< as one of the representative big data analytics workloads with a larger data input set (147 GB). In <ref type=""bibr"" target=""#b16"">[17]</ref>, the data input size is only 4.5 GB.</p><p>We set up the o n fetch stalls indicates the front end inefficiency. Our observation corroborates the previous work <ref type=""bibr"" target=""#b16"">[17]</ref>. The front end inefficiency may caused by high-level langu Figure <ref type=""figure"" target=""#fig_3"">5</ref>.</p><p>Implications: Corroborating previous work <ref type=""bibr"" target=""#b16"">[17]</ref>, both the big data analytics workloads and the service wor ns, which may be caused by two factors: deep memory hierarchy with long latency in modern processor <ref type=""bibr"" target=""#b16"">[17]</ref>, and large binary size complicated by high-level language, sor and save the die area. For the service workloads, our observation corroborate the previous work <ref type=""bibr"" target=""#b16"">[17]</ref>: the L2 cache is ineffective.</p><formula xml:id=""formula_ s in modern superscalar processors as previous work found e.g., on-chip bandwidth, die area and etc <ref type=""bibr"" target=""#b16"">[17]</ref>. According to our correlation analysis in this section, ar ten last level cache hit latency and reduce L2 cache miss penalty, which corroborates previous work <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b30"">31]</ref>. Moreover, for mod s prevent us from breaking down the execution time precisely due to overlapped work in the pipeline <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" ta get=""#b18"">19,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b16"">17]</ref> and etc. Narayanan et al. <ref type=""bibr"" target=""#b32"">[3",1
"mining algorithms or evaluate clusters using data analytics workloads in different aspects, such as <ref type=""bibr"" target=""#b32"">[33,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" ta pe=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b16"">17]</ref> and etc. Narayanan et al. <ref type=""bibr"" target=""#b32"">[33]</ref> characterize traditional data analytics workloads on singl",0
"et=""#b44"">[45,</ref><ref type=""bibr"" target=""#b40"">41,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" target=""#b21"">22]</ref>. For example, the recommendation system is a typical exampl",0
"=""3"" xml:id=""foot_2"">HMM invokes ICTCLASS<ref type=""bibr"" target=""#b3"">[4]</ref>; SVM invokes LIBSVM<ref type=""bibr"" target=""#b13"">[14]</ref> and Hive-bench invokes Hive.</note> 			<note xmlns=""http:/",0
"<note xmlns=""http://www.tei-c.org/ns/1.0"" place=""foot"" n=""3"" xml:id=""foot_2"">HMM invokes ICTCLASS<ref type=""bibr"" target=""#b3"">[4]</ref>; SVM invokes LIBSVM<ref type=""bibr"" target=""#b13"">[14]</ref>",0
"pe=""bibr"" target=""#b2"">(Caruana, 1995;</ref><ref type=""bibr"" target=""#b1"">Bengio et al., 2011;</ref><ref type=""bibr"" target=""#b0"">Bengio, 2011)</ref>. In transfer learning, we first train a base netwo",1
"ransfer results on a well-known architecture. We use the reference implementation provided by Caffe <ref type=""bibr"" target=""#b8"">(Jia et al., 2014</ref>) so that our results will be comparable, exten",0
"e will be able to use them for transfer learning <ref type=""bibr"" target=""#b2"">(Caruana, 1995;</ref><ref type=""bibr"" target=""#b1"">Bengio et al., 2011;</ref><ref type=""bibr"" target=""#b0"">Bengio, 2011)<",0
"><p>Although examples of successful feature transfer have been reported elsewhere in the literature <ref type=""bibr"" target=""#b6"">(Girshick et al., 2013;</ref><ref type=""bibr"" target=""#b5"">Donahue et",0
"get=""#b4"">(Donahue et al., 2013a;</ref><ref type=""bibr"" target=""#b13"">Zeiler and Fergus, 2013;</ref><ref type=""bibr"" target=""#b12"">Sermanet et al., 2014)</ref>, collectively suggesting that these laye",0
"r setting, all our ConvNet layer configurations are designed using the same principles, inspired by <ref type=""bibr"" target=""#b3"">Ciresan et al. (2011)</ref>; <ref type=""bibr"" target=""#b21"">Krizhevsky t=""#b23"">Lin et al. (2014)</ref>.</p><p>Small-size convolution filters have been previously used by <ref type=""bibr"" target=""#b3"">Ciresan et al. (2011)</ref>, but their nets are significantly less dee",1
"on ILSVRC, as feature Published as a conference paper at ICLR 2015 served in semantic segmentation <ref type=""bibr"" target=""#b24"">(Long et al., 2014)</ref>, image caption generation <ref type=""bibr""",0
"ep features on Caltech-101<ref type=""bibr"" target=""#b8"">(Fei-Fei et al., 2004)</ref> and Caltech-256<ref type=""bibr"" target=""#b13"">(Griffin et al., 2007)</ref> image classification benchmarks. Caltech",0
"TATION DETAILS</head><p>Our implementation is derived from the publicly available C++ Caffe toolbox <ref type=""bibr"" target=""#b17"">(Jia, 2013)</ref> (branched out in December 2013), but contains a num",0
"et=""#b33"">(Zeiler &amp; Fergus, 2013;</ref><ref type=""bibr"" target=""#b6"">Donahue et al., 2013;</ref><ref type=""bibr"" target=""#b27"">Razavian et al., 2014;</ref><ref type=""bibr"" target=""#b1"">Chatfield e",0
"=""#b2"">2,</ref><ref type=""bibr"" target=""#b3"">3]</ref>. Parallel research exists in vocal expression <ref type=""bibr"" target=""#b4"">[4,</ref><ref type=""bibr"" target=""#b5"">5]</ref>. Many prosodic feature ees with an earlier study examining gender differences in emotion detection using the voice channel <ref type=""bibr"" target=""#b4"">[4]</ref>. More detailed studies can be performed in the future.</p><p",1
"include ratings when both the audio and visual information is present.</p><p>Similarly, the HUMAINE <ref type=""bibr"" target=""#b40"">[39]</ref> and RECOLA <ref type=""bibr"" target=""#b41"">[40]</ref> datab",0
"et=""#b20"">[20,</ref><ref type=""bibr"" target=""#b21"">21,</ref><ref type=""bibr"" target=""#b22"">22,</ref><ref type=""bibr"" target=""#b23"">23,</ref><ref type=""bibr"" target=""#b24"">24]</ref> and audio-visual cl",0
"lly recognized by different groups of people based on the activation of specific facial expressions <ref type=""bibr"" target=""#b1"">[1,</ref><ref type=""bibr"" target=""#b2"">2,</ref><ref type=""bibr"" target",0
"type=""bibr"" target=""#b23"">23,</ref><ref type=""bibr"" target=""#b24"">24]</ref> and audio-visual clips <ref type=""bibr"" target=""#b25"">[25,</ref><ref type=""bibr"" target=""#b26"">26,</ref><ref type=""bibr"" ta",0
"mal preprocessing scheme.</p><p>The spectrograms are processed by a deep bidirectional LSTM network <ref type=""bibr"" target=""#b10"">(Graves et al., 2013)</ref> with a Connectionist Temporal Classificat M is used for the hidden layers the complete architecture is referred to as deep bidirectional LSTM <ref type=""bibr"" target=""#b10"">(Graves et al., 2013)</ref>.</p></div> <div xmlns=""http://www.tei-c.o",1
"bination of bidirectional LSTM and CTC has been applied to characterlevel speech recognition before <ref type=""bibr"" target=""#b5"">(Eyben et al., 2009)</ref>, however the relatively shallow architectur",1
"=""#b10"">(Graves et al., 2013)</ref> with a Connectionist Temporal Classification (CTC) output layer <ref type=""bibr"" target=""#b9"">(Graves et al., 2006;</ref><ref type=""bibr"">Graves, 2012, Chapter 7)</ _8"">15</ref>) can be efficiently evaluated and differentiated using a dynamic programming algorithm <ref type=""bibr"" target=""#b9"">(Graves et al., 2006)</ref>. Given a target transcription y * , the ne",1
"rks are now able to directly classify raw pixels into high-level concepts such as object categories <ref type=""bibr"" target=""#b17"">(Krizhevsky et al., 2012)</ref> and messages on traffic signs <ref ty",0
"directly train HMM-neural network hybrids to maximise the probability of the correct transcription <ref type=""bibr"" target=""#b0"">(Bahl et al., 1986;</ref><ref type=""bibr"" target=""#b16"">Jaitly et al.,",0
"he introduction of neural networks <ref type=""bibr"" target=""#b2"">(Bourlard &amp; Morgan, 1993;</ref><ref type=""bibr"" target=""#b13"">Hinton et al., 2012)</ref>, the networks are at present only a single",0
"a novel adaptive multi-compositionality layer in recursive neural network, which is named as AdaRNN <ref type=""bibr"" target=""#b1"">(Dong et al., 2014)</ref>. It consists of more than one composition fu",1
"cts as targets, and sentiments for them are heuristically determined by the dominant opinion words. <ref type=""bibr"" target=""#b8"">Jiang et al. (2011)</ref> combine the target-independent features (con s. These features are all target-independent.</p><p>SVM-dep: We re-implement the method proposed by <ref type=""bibr"" target=""#b8"">Jiang et al. (2011)</ref>. It combines both 1 http://goo.gl/5Enpu7 the wever, the accuracy and F1-score do not gain significantly. This is caused by mismatch of the rules <ref type=""bibr"" target=""#b8"">(Jiang et al., 2011)</ref> used to extract the target-dependent featur",0
"The neural models use distributed representation <ref type=""bibr"" target=""#b6"">(Hinton, 1986;</ref><ref type=""bibr"" target=""#b12"">Rumelhart et al., 1986;</ref><ref type=""bibr"" target=""#b0"">Bengio et ularization penalty is used.</p><p>Based on the converted tree, we employ backpropagation algorithm <ref type=""bibr"" target=""#b12"">(Rumelhart et al., 1986)</ref> to propagate the errors from root node",0
"preprocess the tweets by replacing the targets with $T$ and setting their POS tags to NN. Liblinear <ref type=""bibr"" target=""#b3"">(Fan et al., 2008)</ref> is used for baselines. A tweet-specific token",0
") to leverage the ability of deep learning models. The neural models use distributed representation <ref type=""bibr"" target=""#b6"">(Hinton, 1986;</ref><ref type=""bibr"" target=""#b12"">Rumelhart et al., 1",0
"n technique for achieving truthfulness in online auctions is based on the concept of a supply curve <ref type=""bibr"" target=""#b21"">[22]</ref>, as applied by Zhang et al. <ref type=""bibr"" target=""#b3"">",1
"ing <ref type=""bibr"" target=""#b2"">[3]</ref>, <ref type=""bibr"" target=""#b3"">[4]</ref>. Spot Instance <ref type=""bibr"" target=""#b4"">[5]</ref> is a first-step attempt to apply the auction mechanism on Am ining budget is small, and larger otherwise. We characterize this property using a utility function <ref type=""bibr"" target=""#b4"">(5)</ref> This utility function is consistent with the social welfare",0
"wireless spectrum allocation <ref type=""bibr"" target=""#b11"">[12]</ref>, and wireless crowdsourcing <ref type=""bibr"" target=""#b13"">[14]</ref>.</p><p>The celebrated VCG mechanism <ref type=""bibr"" targe",0
"tions of auctions are found in a wide range of research areas, such as network bandwidth allocation <ref type=""bibr"" target=""#b12"">[13]</ref>, wireless spectrum allocation <ref type=""bibr"" target=""#b1",0
"ctions focus on a single-round auction only and ignore such temporal correlation in decision making <ref type=""bibr"" target=""#b7"">[8]</ref>.</p><p>Cloud Auctions Should Be Combinatorial: A cloud compu ype=""bibr"" target=""#b19"">[20]</ref> introduce the concept of bundles in VM allocation. Zhang et al. <ref type=""bibr"" target=""#b7"">[8]</ref> are among the first to study dynamic VM provisioning and des et=""#b2"">(3)</ref>. In order to analyze the property about constraint (1b), we prove the following: <ref type=""bibr"" target=""#b7"">(8)</ref> We prove (8) by induction. Equation (8) holds for apparently is to our target (8), obviously we only need to show: . We utilize the inequality: Since , we prove <ref type=""bibr"" target=""#b7"">(8)</ref>. Now we utilize the inequality (8) to prove claim <ref type= claim <ref type=""bibr"" target=""#b2"">(3)</ref>. For some user , suppose is the first time . Then by <ref type=""bibr"" target=""#b7"">(8)</ref>,</p><p>. The algorithm never gives user any new bundles once",0
"ef><ref type=""bibr"" target=""#b15"">16]</ref> further extend the deep models for multimodal learning. <ref type=""bibr"" target=""#b16"">[17]</ref> design a cross-media learning method based on DNN, and lev network (CNN) with cross autoencoders to learn the latent high-level attributes on crossmodal units <ref type=""bibr"" target=""#b16"">[17]</ref> <ref type=""bibr"" target=""#b17"">[18]</ref>. Finally, we pro very tweets by utilizing a recently proposed cross-media model, namely the Cross Autoencoders (CAE) <ref type=""bibr"" target=""#b16"">[17]</ref>.</p><p>An auto encoder is a basic unit in deep neural netw s for comparison with previous work, due to the different goal, our results are not comparable with <ref type=""bibr"" target=""#b16"">[17]</ref>. Actually, the most related user-level prediction work is",1
"#b11"">[12]</ref><ref type=""bibr"" target=""#b12"">[13]</ref><ref type=""bibr"" target=""#b13"">[14]</ref>. <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b15"">16]</ref> further extend the arned. Otherwise the network may tend to learn trivial attributes for each modality as mentioned in <ref type=""bibr"" target=""#b14"">[15]</ref>.</p><p>Micro-blog data is typical cross-media data. It con",0
"earn the latent high-level attributes on crossmodal units <ref type=""bibr"" target=""#b16"">[17]</ref> <ref type=""bibr"" target=""#b17"">[18]</ref>. Finally, we propose a deep neural network (DNN) model to further model a user as a subject of series of tweets, we apply Convolutional Neural Networks (CNN) <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b24"">25]</ref>. CNNs have large l",0
"ies have revealed a link between stress and mental diseases like anxiety disorders, depression etc. <ref type=""bibr"" target=""#b1"">[2]</ref>. Stress has been a threat to human health for a long time. T",0
"superior ability of deep neural networks (DNN) in learning features from large scale unlabeled data <ref type=""bibr"" target=""#b11"">[12]</ref><ref type=""bibr"" target=""#b12"">[13]</ref><ref type=""bibr"" t auto encoder is a basic unit in deep neural networks for learning distinctive attributes from data <ref type=""bibr"" target=""#b11"">[12]</ref><ref type=""bibr"" target=""#b12"">[13]</ref><ref type=""bibr"" t",0
"o-Encoder (CAE) to learn the joint representation using Denoising Auto-Encoder (DAE) style learning <ref type=""bibr"" target=""#b14"">[15]</ref>. Fig. <ref type=""figure"" target=""#fig_4"">5</ref> shows a s",1
"egory lexicon is built based on the ""Simplified Chinese Language Inquiry and Word Count Dictionary"" <ref type=""bibr"" target=""#b8"">[9]</ref>. It contains 1307 words, which are categorized into four typ",0
"en devoted to detecting psychological stress more efficiently and timely recent years. Most of them <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b2"">3]</ref> focus on detecting str The equipment is used to gather physiological parameters and daily activity data. Andrew Raij .etc. <ref type=""bibr"" target=""#b1"">[2]</ref> designed a mobile phone based system, named mStress, using 6",0
"ations is based on psychological principles and art theories <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b6"">7]</ref>. Finally, a deep neural network is implemented to learn the s sed on previous psychological principles and art theories in <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b6"">7]</ref>. The definitions are as follows:</p></div> <div xmlns=""http:/",0
"that deep neural networks can be used to learn cross-media or share representations in speech video <ref type=""bibr"" target=""#b12"">[13]</ref> or images with text tag <ref type=""bibr"" target=""#b13"">[14 arn feature within each single modality with few correlation connection between multiple modalities <ref type=""bibr"" target=""#b12"">[13]</ref>. We train the CAE with a cropped set of data that input fr",0
"(FDP) <ref type=""bibr"" target=""#b20"">[21]</ref> and Address Map Pattern Matching Prefetching (AMPM) <ref type=""bibr"" target=""#b11"">[12]</ref>. We now describe both of these techniques in some detail.<",1
"p><p>Finally, In our sensitivity analysis, Section 6.5, we evaluate 6 workloads from CloudSuite 1.0 <ref type=""bibr"" target=""#b7"">[8]</ref>. For each Cloud-Suite application, we began simulation at th",0
"arget=""#b19"">20,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" targ",0
"target=""#b5"">6,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b0"">1,</ref><ref type=""bibr"" target= rameters</head><p>We evaluate Sandbox Prefetching using the Wind River Simics full system simulator <ref type=""bibr"" target=""#b1"">[2]</ref>, which has been augmented to precisely model a DRAM main mem",0
"arget=""#b10"">11,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" targe lications can benefit from sequential and stride prefetching <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b8"">9]</ref>. However, applications that lack spatial locality receive ver",0
"xt layer. This greedy layer-wise training process forms the model of the Stacked Sparse Autoencoder <ref type=""bibr"" target=""#b23"">(Vincent et al. 2010)</ref>. When all the layers are trained in this",1
"s an unweighted protein-protein interaction (PPI) network from the Database of Interacting Proteins <ref type=""bibr"" target=""#b17"">(Salwinski et al. 2004</ref>). The average degree of nodes is about 6",0
"many applications such as image classification, speech recognition, and natural language processing <ref type=""bibr"" target=""#b1"">(Bengio 2009</ref>) <ref type=""bibr"" target=""#b8"">(Dahl et al. 2012</r",0
"bibr"" target=""#b13"">(Krizhevsky, Sutskever, and Hinton 2012)</ref>, and natural language processing <ref type=""bibr"" target=""#b7"">(Collobert et al. 2011)</ref>. However, to our knowledge, the adoption type=""bibr"" target=""#b1"">(Bengio 2009</ref>) <ref type=""bibr"" target=""#b8"">(Dahl et al. 2012</ref>) <ref type=""bibr"" target=""#b7"">(Collobert et al. 2011)</ref>  <ref type=""bibr"" target=""#b13"">(Krizhev",0
"olutional kernels such as a fixed window <ref type=""bibr"" target=""#b5"">(Collobert et al. 2011;</ref><ref type=""bibr"" target=""#b12"">Kalchbrenner and Blunsom 2013)</ref>. When using such kernels, it is e=""bibr"" target=""#b17"">Mikolov (2012)</ref> uses recurrent neural network to build language models. <ref type=""bibr"" target=""#b12"">Kalchbrenner and Blunsom (2013)</ref> proposed a novel recurrent netw",1
"a text word by word and stores the semantics of all the previous text in a fixed-sized hidden layer <ref type=""bibr"" target=""#b7"">(Elman 1990</ref>). The advantage of RecurrentNN is the ability to bet",1
"utual information <ref type=""bibr"" target=""#b6"">(Cover and Thomas 2012)</ref>, or L1 regularization <ref type=""bibr"" target=""#b20"">(Ng 2004)</ref> to select useful features. Machine learning algorithm st two major methods for comparison: the context-free grammar (CFG) produced by the Berkeley parser <ref type=""bibr"" target=""#b20"">(Petrov et al. 2006)</ref>  </p></div> <div xmlns=""http://www.tei-c.o",0
"ip-gram model to pre-train the word embedding. this model is the state-of-the-art in many NLP tasks <ref type=""bibr"" target=""#b1"">(Baroni, Dinu, and Kruszewski 2014)</ref>. The Skipgram model trains t",0
"ing word representations have been proposed <ref type=""bibr"" target=""#b2"">(Bengio et al. 2003;</ref><ref type=""bibr"" target=""#b18"">Mnih and Hinton 2007;</ref><ref type=""bibr"" target=""#b17"">Mikolov 201",0
"uniform distribution. The magnitude of the maximum or minimum equals the square root of the ""fanin"" <ref type=""bibr"" target=""#b21"">(Plaut and Hinton 1987</ref>). The number is the network node of the",0
"ks <ref type=""bibr"" target=""#b10"">(Hinton and Salakhutdinov 2006)</ref> and representation learning <ref type=""bibr"" target=""#b3"">(Bengio, Courville, and Vincent 2013)</ref> have led to new ideas for",0
"atly alleviates the data sparsity problem <ref type=""bibr"" target=""#b2"">(Bengio et al. 2003)</ref>. <ref type=""bibr"" target=""#b16"">Mikolov, Yih, and Zweig (2013)</ref> shows that pre-trained word embe",0
"(Cover and Thomas 2012)</ref>, pLSA <ref type=""bibr"" target=""#b5"">(Cai and Hofmann 2003)</ref>, LDA <ref type=""bibr"" target=""#b9"">(Hingmire et al. 2013)</ref>, are applied to select more discriminativ n several classification tasks. We select two methods as the methods for comparison: ClassifyLDA-EM <ref type=""bibr"" target=""#b9"">(Hingmire et al. 2013</ref>) and Labeled-LDA (Li, Sun, and Zhang 2008) the bydate version and select four major categories (comp, politics, rec, and religion) followed by<ref type=""bibr"" target=""#b9"">Hingmire et al. (2013)</ref>.ACL Anthology Network 3 This dataset cont",0
"s.</p><p>The idea of extracting features for NLP using convolutional DNN was previously explored by <ref type=""bibr"" target=""#b3"">Collobert et al. (2011)</ref>, in the context of POS tagging, chunking Recognition (NER) and Semantic Role Labeling (SRL). Our work shares similar intuition with that of <ref type=""bibr"" target=""#b3"">Collobert et al. (2011)</ref>. In <ref type=""bibr"" target=""#b3"">(Collo tation component, each input word token is transformed into a vector by looking up word embeddings. <ref type=""bibr"" target=""#b3"">Collobert et al. (2011)</ref> reported that word embeddings learned fr ural network, the convolution approach is a natural method to merge all of the features. Similar to <ref type=""bibr"" target=""#b3"">Collobert et al. (2011)</ref>, we first process the output of Window P , we heuristically choose d e = 5. Finally, the word dimension and learning rate are the same as in <ref type=""bibr"" target=""#b3"">Collobert et al. (2011)</ref>. Table <ref type=""table"">2 reports</ref> ons.org/licenses/by/4.0/ 1 A word embedding is a distributed representation for a word. For example,<ref type=""bibr"" target=""#b3"">Collobert et al. (2011)</ref> use a 50-dimensional vector to represent -words model</note> 			<note xmlns=""http://www.tei-c.org/ns/1.0"" place=""foot"" n=""3"" xml:id=""foot_1""><ref type=""bibr"" target=""#b3"">Collobert et al. (2011)</ref> proposed a pairwise ranking approach to ares similar intuition with that of <ref type=""bibr"" target=""#b3"">Collobert et al. (2011)</ref>. In <ref type=""bibr"" target=""#b3"">(Collobert et al., 2011)</ref>, all of the tasks are considered as the",1
"ased on learning a distributed representation for each word, which is also called a word embeddings <ref type=""bibr"" target=""#b17"">(Turian et al., 2010)</ref>. <ref type=""bibr"" target=""#b14"">Socher et arget=""#foot_1"">3</ref> . However, there are many trained word embeddings that are freely available <ref type=""bibr"" target=""#b17"">(Turian et al., 2010)</ref>. A comparison of the available word embed beyond the scope of this paper. Our experiments directly utilize the trained embeddings provided by <ref type=""bibr"" target=""#b17"">Turian et al.(2010)</ref>.</p></div> <div xmlns=""http://www.tei-c.org",0
"ly selected the most frequent words in the contexts to represent the relation between the nominals. <ref type=""bibr"" target=""#b2"">Chen et al. (2005)</ref> proposed a novel unsupervised method based on",0
"ors in the syntactic tree path that connects two nominals to determine their semantic relationship. <ref type=""bibr"" target=""#b6"">Hashimoto et al. (2013)</ref> also use an RNN for relation classificat",0
"et=""#b12"">[13,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" targe >19,</ref><ref type=""bibr"" target=""#b19"">20]</ref> for GP. Very recently, the authors of PowerGraph <ref type=""bibr"" target=""#b5"">[6]</ref> find that the vertex-cut methods can achieve better performa rtex-cut has attracted more and more attention from DGC research community. For example, PowerGraph <ref type=""bibr"" target=""#b5"">[6]</ref> adopts a random vertex-cut method and two greedy variants fo sly guarantee good workload balance. • DBH can be implemented as an execution engine for PowerGraph <ref type=""bibr"" target=""#b5"">[6]</ref>, and hence all PowerGraph applications can be seamlessly sup hines. Hence, |A(v)| is the number of replicas of v among different machines. Similar to PowerGraph <ref type=""bibr"" target=""#b5"">[6]</ref>, one of the replicas of a vertex is chosen as the master and the α is, the more skewed a graph will be. This power-law degree distribution makes GP challenging <ref type=""bibr"" target=""#b5"">[6]</ref>. Although vertex-cut methods can achieve better performance though vertex-cut methods can achieve better performance than edge-cut methods for power-law graphs <ref type=""bibr"" target=""#b5"">[6]</ref>, existing vertex-cut methods, such as random method in Power ysis for our DBH method. For comparison, the random vertex-cut method (called Random) of PowerGraph <ref type=""bibr"" target=""#b5"">[6]</ref> and the grid-based constrained solution (called Grid) of Gra ly to the p machines via a randomized hash function. The result can be directly got from PowerGraph <ref type=""bibr"" target=""#b5"">[6]</ref>. Lemma 1. Assume that we have a sequence of n vertices {v i theorem says that our DBH method has smaller expected replication factor than Random of PowerGraph <ref type=""bibr"" target=""#b5"">[6]</ref>.</p><p>Next we turn to the analysis of the balance constrain ""5.2"">Baselines and Evaluation Metric</head><p>In our experiment, we adopt the Random of PowerGraph <ref type=""bibr"" target=""#b5"">[6]</ref> and the Grid of GraphBuilder [8]<ref type=""foot"" target=""#fo",1
"#b5"">[6]</ref> adopts a random vertex-cut method and two greedy variants for GP.</p><p>GraphBuilder <ref type=""bibr"" target=""#b7"">[8]</ref> provides some heuristics, such as the grid-based constrained sting vertex-cut methods, such as random method in PowerGraph and grid-based method in GraphBuilder <ref type=""bibr"" target=""#b7"">[8]</ref>, cannot make effective use of the powerlaw distribution to a ""bibr"" target=""#b5"">[6]</ref> and the grid-based constrained solution (called Grid) of GraphBuilder <ref type=""bibr"" target=""#b7"">[8]</ref> are adopted as baselines. Our analysis is based on randomiza",1
"f type=""bibr"" target=""#b14"">[15]</ref>, use edge-cut methods <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" tar",0
"balance factors. We de- The degrees of natural graphs usually follow skewed power-law distributions <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b0"">1]</ref>:</p><formula xml:id=""f",0
"natural graphs usually follow skewed power-law distributions <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b0"">1]</ref>:</p><formula xml:id=""formula_2"">fine 1 n n i=1 |A(v i )| as r",0
"n for GP. Hence, they cannot achieve satisfactory performance in natural powerlaw graphs. PowerLyra <ref type=""bibr"" target=""#b3"">[4]</ref> tries to combine both edge-cut and vertex-cut together by us oot"" target=""#foot_0"">1</ref> as baselines for empirical comparison. The method Hybrid of PowerLyra <ref type=""bibr"" target=""#b3"">[4]</ref> is not adopted for comparison because it combines both edge-",0
"ices. Furthermore, many machine learning and data mining algorithms can also be modeled with graphs <ref type=""bibr"" target=""#b12"">[13]</ref>. Hence, machine learning based on distributed graph-comput ermined by the number of mirrors of the vertices. Most traditional DGC frameworks, such as GraphLab <ref type=""bibr"" target=""#b12"">[13]</ref> and Pregel <ref type=""bibr"" target=""#b14"">[15]</ref>, use ph-computing (DGC) frameworks has attracted much attention from big data machine learning community <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" ta",0
"arget=""#b14"">15,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b6"">7]</ref>. To perform distribut",0
"learning community <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" targe",0
"oc bottom-up fashion, where PMU designers attempted to cover key issues via ""dedicated miss events"" <ref type=""bibr"" target=""#b0"">[1]</ref>. Yet, how does one pin-point performance issues that were no rchy's top level. This accurate classification distinguishes our method from previous approaches in <ref type=""bibr"" target=""#b0"">[1]</ref>[5][6].</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><h same symptom. Such scenarios of L1 hits and near caches' misses, are not handled by some approaches <ref type=""bibr"" target=""#b0"">[1]</ref> <ref type=""bibr"" target=""#b4"">[5]</ref>.</p><p>Note performa t=""#b5"">[6]</ref>, nor complex structures with latency counters as in Accurate CPI Stacks proposals <ref type=""bibr"" target=""#b0"">[1]</ref>[8] <ref type=""bibr"" target=""#b8"">[9]</ref>.</p></div> <div x tempted to accurately classify performance impacts on out-of-order architectures. Eyerman et al. in <ref type=""bibr"" target=""#b0"">[1]</ref>[9] use a simulation-based interval analysis model in order t reference model) is that it restricts all stalls to a fixed set of eight predefined miss events. In <ref type=""bibr"" target=""#b0"">[1]</ref>[4] <ref type=""bibr"" target=""#b4"">[5]</ref> there is no consi",1
"/p><p>Table <ref type=""table"">6</ref>: Results of tuning Software Prefetch case Software Prefetches <ref type=""bibr"" target=""#b9"">[10]</ref> are planted in the algorithm's critical loop to prefetch da hniques might tolerate this; such as Sandy Bridge's support of up to eight general-purpose counters <ref type=""bibr"" target=""#b9"">[10]</ref>, or eventmultiplexing in the tools <ref type=""bibr"" target= s exceed Machine Issue Width (MIW). This capability is called Counter Mask ever available in x86 PMU<ref type=""bibr"" target=""#b9"">[10]</ref>.</note></figure> <figure xmlns=""http://www.tei-c.org/ns/1.0",0
"ation challenges (detailed in next section). Multiple tools have adopted our method including VTune <ref type=""bibr"" target=""#b1"">[2]</ref> and an add-on package to the standard Linux perf utility <re ogram scope first, may be applied to our method as already done in VTune's General Exploration view <ref type=""bibr"" target=""#b1"">[2]</ref>. While <ref type=""bibr"" target=""#b11"">[12]</ref> estimates s eneral-purpose counters <ref type=""bibr"" target=""#b9"">[10]</ref>, or eventmultiplexing in the tools <ref type=""bibr"" target=""#b1"">[2]</ref> <ref type=""bibr"" target=""#b2"">[3]</ref>. Still a better hard",0
"SB, the Decoded-uop Stream Buffer introduced in Sandy Bridge) are a couple examples from Intel Core <ref type=""bibr"" target=""#b6"">[7]</ref>.</p><p>Top-Down further distinguishes between latency and ba re the pipeline is flushed. For example, incorrect data speculation generated Memory Ordering Nukes <ref type=""bibr"" target=""#b6"">[7]</ref> -a subset of Machine Clears. We make this distinction as the crocode sequences such as Floating Point (FP) assists typically hurt performance and can be avoided <ref type=""bibr"" target=""#b6"">[7]</ref>. They are isolated under Micro Sequencer metric in order to per flowchart in Figure <ref type=""figure"" target=""#fig_0"">4</ref>. Load blocks due to 4K Aliasing <ref type=""bibr"" target=""#b6"">[7]</ref> is another scenario with same symptom. Such scenarios of L1 e iTLB and i-cache accesses are supported with better timing to improve the benefits of prefetching <ref type=""bibr"" target=""#b6"">[7]</ref>. This can be clearly noticed for the benefiting benchmarks w",0
"tional counters/logic. We have pointed to more drawbacks in previous sections.</p><p>More recently, <ref type=""bibr"" target=""#b12"">[13]</ref> and <ref type=""bibr"" target=""#b11"">[12]</ref> proposed ins d instrumentationbased tools to analyze data-locality and scalability bottlenecks, respectively. In <ref type=""bibr"" target=""#b12"">[13]</ref>, average memory latency is sampled with a PMU and coupled better metrics based on our MemStalls.L3Miss event e.g. can be used instead of raw latency value in <ref type=""bibr"" target=""#b12"">[13]</ref> to quantify when speedup may apply. Examining metrics at h",0
"ing instructions out of order. We believe that a solution similar to those proposed by Stark et al. <ref type=""bibr"" target=""#b18"">[19]</ref> and Cher et al. <ref type=""bibr"" target=""#b2"">[3]</ref> ca .</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4"">Related Work</head><p>Stark et al. <ref type=""bibr"" target=""#b18"">[19]</ref> proposed a limited form of out-of-order instruction fetch",1
"from the primary table. The trace predictor is based on the Multiscalar task predictor named ""DOLC"" <ref type=""bibr"" target=""#b0"">[1]</ref>.</p><p>Each entry in the predictor contains the starting add </p><p>S p e c u l a t ive m u l t i t h r e a d i n g a r c h i t e c t u r e s l i ke Multiscalar <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b17"">18]</ref> come closest to this",0
"h r e a d i n g a r c h i t e c t u r e s l i ke Multiscalar <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b17"">18]</ref> come closest to this technique as far as the nature of inst",0
"><head n=""3"">Experimental Evaluation</head><p>We used a simulator based on the SimpleScalar toolset <ref type=""bibr"" target=""#b1"">[2]</ref> to model a multiple sequencer based fetch mechanism. Paramet",0
"edictors can achieve equivalent or higher prediction accuracies than conventional branch predictors <ref type=""bibr"" target=""#b6"">[7]</ref>.</p><p>Once future control flow can be predicted at trace gr "">Trace Prediction.</head><p>We u s e t h e t r a c e p r e d i c t o r proposed by Jacobson et al. <ref type=""bibr"" target=""#b6"">[7]</ref>. Each trace is assigned a trace identifier obtained by combi",0
"ore, the left side of expression 3 becomes equal to AE ? ?. Thus AE ? ? AE ?</formula><p>? ? ? ? ?? <ref type=""bibr"" target=""#b4"">(5)</ref> Consider the sum in this expression. The number of terms wit <ref type=""bibr"" target=""#b7"">[8]</ref> and set sampling <ref type=""bibr"" target=""#b10"">[11]</ref>  <ref type=""bibr"" target=""#b4"">[5]</ref>.</p><p>Hardware monitoring tools collect statistics from har",1
"tistics from hardware and present the information in aggregated form to the user. Examples are DCPI <ref type=""bibr"" target=""#b0"">[1]</ref>, which uses an advanced hardware support to collect detailed",0
"<ref type=""bibr"" target=""#b5"">[6]</ref>, CPROF <ref type=""bibr"" target=""#b13"">[14]</ref> and MemSpy <ref type=""bibr"" target=""#b17"">[18]</ref> <ref type=""bibr"" target=""#b18"">[19]</ref>. These tools are",0
"tion in computer architecture. An extended version of this paper is available as a technical report <ref type=""bibr"" target=""#b1"">[2]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=",0
"on 2.3.</p><p>The proof-of-concept implementation is based on code instrumentation. We use the SAIT <ref type=""bibr"" target=""#b8"">[9]</ref> SPARC assembly code instrumentation tool to insert a small p",0
"routine <ref type=""bibr"">[8,</ref><ref type=""bibr"" target=""#b14"">17,</ref><ref type=""bibr"">25,</ref><ref type=""bibr"" target=""#b16"">191</ref> to minimize the number of conflict misses. Reducing the num",1
"ce, and is the main cause of decreased prediction accuracy <ref type=""bibr"" target=""#b25"">[28,</ref><ref type=""bibr"" target=""#b17"">20]</ref>.</p><p>Dealiased branch predictors reduce negative PHT inte",1
"is is called prediction table interference, and is the main cause for decreased prediction accuracy <ref type=""bibr"" target=""#b25"">[28]</ref>.</p><p>Dynamic prediction tables can be organized in a cle ppens more often than positive interference, and is the main cause of decreased prediction accuracy <ref type=""bibr"" target=""#b25"">[28,</ref><ref type=""bibr"" target=""#b17"">20]</ref>.</p><p>Dealiased b",1
"e same outcome for a given branch. This prediction was obtained either using very simple heuristics <ref type=""bibr"" target=""#b20"">[23]</ref>, static analysis [l], or profile information <ref type=""bi ken (or usually not taken), or aligning branches so that only a forward branch is usually not taken <ref type=""bibr"" target=""#b20"">[23]</ref>. In this work we examine how code layout optimizations tar adaptive predictors</head><p>The more simple dynamic branch predictor (the bimodal branch predictor <ref type=""bibr"" target=""#b20"">[23]</ref>) simply keeps a saturating two-bit counter for each branch ranch behavior, and lookup the data each time the branch executes to produce a direction prediction <ref type=""bibr"" target=""#b20"">[23,</ref><ref type=""bibr"" target=""#b23"">26]</ref>.</p><p>But the siz",0
"the results in the paper were obtained using a simulator derived from the SimpleScalar 3.0 tool set <ref type=""bibr"" target=""#b1"">[2]</ref>. We run most of the SPECint95 benchmarks plus the PostgreSQL",0
"sually implying code replication <ref type=""bibr"" target=""#b26"">[14,</ref><ref type=""bibr"">27,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"">13,</ref><ref type=""bibr"">161</ref>. These co",0
"tables is due to conflict aliasing, not capacity problems. Derived from the skew-associative caches <ref type=""bibr"" target=""#b18"">[21]</ref>, the gskew predictor stores r; branches in three separate",0
"formations proposed to improve static branch prediction accuracy, usually implying code replication <ref type=""bibr"" target=""#b26"">[14,</ref><ref type=""bibr"">27,</ref><ref type=""bibr"" target=""#b8"">9,<",0
"tion tables. Figure <ref type=""figure"" target=""#fig_0"">2</ref>.a shows the agree prediction scheme <ref type=""bibr"" target=""#b21"">[24]</ref>. The agree predictor adds an extra bit of information asso h modem branch predictors that are already organized to min- imize such interference like the agree <ref type=""bibr"" target=""#b21"">[24]</ref>, bimode <ref type=""bibr"">[lo]</ref>,</p><p>and gskew [12,",0
"ccuracies, limitations and pitfalls of the related technique known as negotiated-congestion routing <ref type=""bibr"" target=""#b27"">[28]</ref>. In Copyright (c) 2008 IEEE. Personal use of this material 5"">[6]</ref>, <ref type=""bibr"" target=""#b12"">[13]</ref>, <ref type=""bibr"" target=""#b14"">[15]</ref>, <ref type=""bibr"" target=""#b27"">[28]</ref>, <ref type=""bibr"" target=""#b31"">[32]</ref>, <ref type=""bib ested regions, often at the cost of increased wirelength.</p><p>Negotiated-congestion Routing (NCR) <ref type=""bibr"" target=""#b27"">[28]</ref> was introduced in the mid-1990s for global routing in FPGA (b e ), added cost reflecting congestion history (h e ), and penalty for current congestion (p e ) <ref type=""bibr"" target=""#b27"">[28]</ref>. NCR seeks to minimize e c e .</p><p>To begin negotiated-c re-route is the same for each iteration, but can be chosen arbitrarily, according to the authors of <ref type=""bibr"" target=""#b27"">[28]</ref>, because the gradual cost increase in congested areas remo =""formula_7"">c e = b e + h e • p e<label>(8)</label></formula><p>which is different than Equation 1 <ref type=""bibr"" target=""#b27"">[28]</ref>, but also is more intuitive since it preserves the base co",1
"he literature generally have smaller runtimes than flat techniques and show higher completion rates <ref type=""bibr"" target=""#b9"">[10]</ref>, <ref type=""bibr"" target=""#b10"">[11]</ref>.</p></div> <div =""bibr"" target=""#b10"">[11]</ref> is a multi-level router based on the techniques first presented in <ref type=""bibr"" target=""#b9"">[10]</ref> with several important enhancements. The first is that MARS",0
"chnique is surprisingly useful in ASIC routing and justified by via minimization. Empirical studies <ref type=""bibr"" target=""#b42"">[43]</ref> show that in a fullyrouted design a majority of all 2-pin",0
"to boost research in physical design, while also leading to better commercial place-and-route tools <ref type=""bibr"" target=""#b13"">[14]</ref>.</p><p>The superficial similarity between negotiated-conge",0
"sharer representation. We leverage recent prior work on efficient highly-associative caches (ZCache <ref type=""bibr"" target=""#b24"">[25]</ref> and Cuckoo Directory <ref type=""bibr"" target=""#b9"">[10]</r ractice as if replacement candidates were selected randomly, independently of the addresses tracked <ref type=""bibr"" target=""#b24"">[25]</ref>. We exploit this property to design and analyze SCD:</p><p array designs, such as skew-associative caches <ref type=""bibr"" target=""#b26"">[27]</ref> or zcaches <ref type=""bibr"" target=""#b24"">[25]</ref>. SPACE <ref type=""bibr"" target=""#b35"">[36]</ref> observes has proposed cache designs that provide high associativity with a small number of ways. Both ZCache <ref type=""bibr"" target=""#b24"">[25]</ref> and Cuckoo Directory <ref type=""bibr"" target=""#b9"">[10]</r se models to show that associativity depends only on the number of replacement candidates, not ways <ref type=""bibr"" target=""#b24"">[25]</ref>, and to implement scalable and efficient cache partitionin ized, and processed in FCFS order, to preserve atomicity and ensure fairness. Second, as in zcaches <ref type=""bibr"" target=""#b24"">[25]</ref>, the array is pipelined, and we allow concurrent non-confl having 16 cores, a directory and L3 bank, and a memory controller. Both L2 and L3 are 4-way zcaches <ref type=""bibr"" target=""#b24"">[25]</ref> with 16 and 52 replacement candidates, respectively. Cache ts, but replacements incur similar energy costs as a set-associative cache of similar associativity <ref type=""bibr"" target=""#b24"">[25]</ref>. In directories, the cost of a replacement is also much sm on over the cache array. We have shown that in practice, this is an accurate assumption for zcaches <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b25"">26]</ref>. We leverage this ons, which are simple to implement and work well in practice <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b24"">25]</ref>. Tiles are connected with an 8?8 mesh network-on-chip (NoC)",1
"hes and directories use H 3 hash functions, which are simple to implement and work well in practice <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b24"">25]</ref>. Tiles are connected",0
"based, writeinvalidate protocols, as alternative protocols scale poorly beyond a few private caches <ref type=""bibr"" target=""#b13"">[14]</ref>. These protocols use a coherence directory to track which",0
"ociative lookups make them very energy-inefficient with a large number of cores. Sparse directories <ref type=""bibr"" target=""#b12"">[13]</ref> are associative, address-indexed arrays, where each entry y techniques have been explored to represent sharer sets inexactly through coarse-grain bit-vectors <ref type=""bibr"" target=""#b12"">[13]</ref>, limited pointers <ref type=""bibr"" target=""#b0"">[1,</ref>< br"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b28"">29]</ref>. In contrast, sparse directories <ref type=""bibr"" target=""#b12"">[13]</ref> are organized as an associative array indexed by line addr ce in sparse directories. Full-map sparse directories encode the sharer set exactly in a bit-vector <ref type=""bibr"" target=""#b12"">[13]</ref>. They support all sharing patterns, but require storage pr nexact encoding of the sharer set. Traditional alternatives include coarse-grain sharer bit-vectors <ref type=""bibr"" target=""#b12"">[13]</ref>, and limited pointer schemes, in which each entry can hold",0
"n single-level protocols, and significantly harder to verify <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b7"">8]</ref>.</p><p>Motivated by the shortcomings of traditional approache",0
"to handle the variety of models of parallelism that appear in HPC programs. Toward this end, PEBIL <ref type=""bibr"" target=""#b0"">[1]</ref> has recently added support for handling multithreaded x86 64 afety</head><p>PEBIL generates and inserts code into the program which has two principle functions: <ref type=""bibr"" target=""#b0"">(1)</ref> to add functionality to a program, functionality which is us",1
"eral sampling scenarios for the OpenMP-multithreaded implementations of the NAS Parallel Benchmarks <ref type=""bibr"" target=""#b3"">[4]</ref>. Collecting a memory address trace is a useful way of stress ools to collect memory address traces for the OpenMP implementations of the NAS Parallel Benchmarks <ref type=""bibr"" target=""#b3"">[4]</ref> A list of all benchmarks along with breif descriptions of th",0
"r and program development. Models of performance <ref type=""bibr"" target=""#b5"">[6]</ref> and energy <ref type=""bibr"" target=""#b6"">[7]</ref> can also depend on the ability to understand how programs ut",0
"type=""bibr"" target=""#b1"">[2]</ref>, DyninstAPI <ref type=""bibr"" target=""#b2"">[3]</ref> and Valgrind <ref type=""bibr"" target=""#b7"">[8]</ref> being the most popular. This section focuses on Pin and Dyni",0
"/Linux binary instrumentation platforms -Pin <ref type=""bibr"" target=""#b1"">[2]</ref> and DyninstAPI <ref type=""bibr"" target=""#b2"">[3]</ref>. We then go on to compare PEBIL to Pin experimentally in the x86/Linux binary instrumentation projects -Pin <ref type=""bibr"" target=""#b1"">[2]</ref>, DyninstAPI <ref type=""bibr"" target=""#b2"">[3]</ref> and Valgrind <ref type=""bibr"" target=""#b7"">[8]</ref> being t",0
"tly. On one hand, prior non-uniform cache access (NUCA) work <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target= NUCA by adaptively placing data close to the requesting core <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target= istance. However, these best-effort techniques often result in hotspots and additional interference <ref type=""bibr"" target=""#b2"">[3]</ref>. On the other hand, prior work has proposed a variety of par at D-NUCA often causes significant bank contention and uneven distribution of accesses across banks <ref type=""bibr"" target=""#b2"">[3]</ref>. We also see this effect in Sec. VI -R-NUCA has the highest",1
"only provide few partitions and often degrade performance, D-NUCA schemes seldom use them. ASP-NUCA <ref type=""bibr"" target=""#b11"">[12]</ref>, ESP-NUCA <ref type=""bibr"" target=""#b30"">[31]</ref>, and E tency of private caches. These schemes often size partitions using hill-climbing (e.g., shadow tags <ref type=""bibr"" target=""#b11"">[12]</ref> or LRU way hit counters <ref type=""bibr"" target=""#b15"">[16",1
"head first finds all POIs in O(S) for each partition. This is inspired by the three coins algorithm <ref type=""bibr"" target=""#b29"">[30]</ref>. For example, we construct the convex hull ADHI in Fig. <r",0
"n local optima, whereas Jigsaw captures full miss curves to make global decisions.</p><p>CloudCache <ref type=""bibr"" target=""#b21"">[22]</ref> implements virtual private caches that can span multiple b",0
"target=""#b7"">8,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" tar target=""#b7"">8,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" tar ntage partitioning <ref type=""bibr"" target=""#b36"">[37]</ref>, up to 2.05× (11.4% gmean) over R-NUCA <ref type=""bibr"" target=""#b13"">[14]</ref>, and up to 24% (6.3% gmean) over an idealized shared-priva ef> use page coloring and a NUCA-aware allocator to map pages to specific banks. Hardavellas et al. <ref type=""bibr"" target=""#b13"">[14]</ref> find that most applications have a few distinct classes of shares also enables specializing shares to different types of data (e.g., shared vs thread-private <ref type=""bibr"" target=""#b13"">[14]</ref>). For these reasons, we choose to map data to shares.</p>< B uses three entries, but there are a large number of shares in the system.</p><p>Similar to R-NUCA <ref type=""bibr"" target=""#b13"">[14]</ref>, page classification is done incrementally and lazily, at ache partitioning (UCP) <ref type=""bibr"" target=""#b33"">[34]</ref>. R-NUCA is configured as proposed <ref type=""bibr"" target=""#b13"">[14]</ref> with 4-way rotational interleaving and page-based reclassi er, instead of mapping pages to locations as in prior work <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b13"">14]</ref>, we map pages to shares or logical caches, and decide the p",0
"firms. We provide a detailed analysis of Peekahead's run-time and correctness in a technical report <ref type=""bibr"" target=""#b3"">[4]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>C. t alternative cache organizations and then present a focused analysis of Jigsaw. A technical report <ref type=""bibr"" target=""#b3"">[4]</ref> includes additional results and experiments.</p></div> <div",0
"w.tei-c.org/ns/1.0""><head>B. Non-Uniform Cache Access (NUCA) Architectures</head><p>NUCA techniques <ref type=""bibr"" target=""#b19"">[20]</ref> reduce the access latency of large distributed caches, and of large distributed caches, and have been the subject of extensive research. Static NUCA (S-NUCA) <ref type=""bibr"" target=""#b19"">[20]</ref> simply spreads the data across all banks with a fixed line",0
"get=""#b17"">18,</ref><ref type=""bibr"" target=""#b30"">31,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" target=""#b45"">46]</ref> has proposed a variety of placement, migration, and replica get=""#b17"">18,</ref><ref type=""bibr"" target=""#b30"">31,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" target=""#b45"">46]</ref>. They involve a combination of placement, migration, and re",0
"me schemes can partition the cache by sets instead of ways <ref type=""bibr"" target=""#b34"">[35,</ref><ref type=""bibr"" target=""#b42"">43]</ref>, but they require significant changes to cache arrays. Alte",0
"rely on heuristics <ref type=""bibr"" target=""#b27"">[28,</ref><ref type=""bibr"" target=""#b43"">44,</ref><ref type=""bibr"" target=""#b44"">45]</ref>, which provide no guarantees and often require many more wa k with way-partitioning, it can be used with other schemes <ref type=""bibr"" target=""#b36"">[37,</ref><ref type=""bibr"" target=""#b44"">45]</ref>. Instead of capturing miss curves, some propose to estimate",0
"xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2."">Background and Related Work</head><p>Previous work <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b37"">38]</ref> describes support me ware support for thread activation and deactivation, as found in prior studies of thread scheduling <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b37"">38]</ref>. While those works u",1
"for Transactional Coherency and Consistency) showed average transaction lengths in the low hundreds <ref type=""bibr"" target=""#b8"">[9]</ref>.</p><p>We also see that copying entire caches proactively -t",0
"also demand frequent migration. Speculative multithreading <ref type=""bibr"" target=""#b31"">[32,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" tar",0
"e Multithreading (SpMT), loss of cache state impedes performance as execution migrates across cores <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" ta",0
"read. Helper threads <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b39"">40]</ref> also utilize parallel hardware for speedup, without actuall e similar structures in the capture stage of our migration system.</p><p>Speculative Precomputation <ref type=""bibr"" target=""#b39"">[40,</ref><ref type=""bibr"" target=""#b10"">11]</ref> targets memory ins",0
"type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"">10,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"">28,</ref><ref type=""bibr"">33,</ref><ref typ type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"">10,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"">28,</ref><ref type=""bibr"">33,</ref><ref typ f type=""bibr"">[37]</ref>, or a core with variable parameters <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b22"">23]</ref> to adapt to the running application at a coarse granularity",0
"ns=""http://www.tei-c.org/ns/1.0""><head>E. Core Area</head><p>Like past heterogeneous designs (e.g., <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"">33,</ref><ref type=""bibr"">62]</ref>), HBA nfrequent (cold) and frequent (hot) code traces respectively <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"">51]</ref>, and use various mechanisms to fo",0
"rithms and for any application with serialized code sections <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"">15,</ref><ref type=""bibr"">28,</ref><ref type= core-level heterogeneity. These heterogeneous designs either combine multiple separate cores (e.g., <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target Coarse-grained Heterogeneous Cores: Several works propose the use of statically heterogeneous cores <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target",0
"process with Synopsys tools. We provide all numbers and formulas of our model in a technical report <ref type=""bibr"" target=""#b15"">[16]</ref>.</p><p>One parameter of our model is the sensitivity of de nts at multiple representative regions as provided by PinPoints [4S]. All checkpoints are listed in <ref type=""bibr"" target=""#b15"">[16]</ref>, along with their individual performance and energy consum extensively but can report only some analyses below due to space constraints. Our technical report <ref type=""bibr"" target=""#b15"">[16]</ref> provides additional results, including sensitivity studies",0
"s <ref type=""bibr"" target=""#b14"">[15]</ref>, dynamic predication based on frequently executed paths <ref type=""bibr"" target=""#b13"">[14]</ref>, and predicate prediction <ref type=""bibr"" target=""#b25"">[",1
"irst, to get the branch's register index, and then accessing the predicate register file.</p><p>NSR <ref type=""bibr"" target=""#b4"">[5]</ref> does not predict branches at all, rather, a branch waits in",0
"n may be marginally accelerated by a dedicated execution backend for the slice.</p><p>Mahlke et al. <ref type=""bibr"" target=""#b20"">[21]</ref> implemented a predicate register file in the fetch stage,",0
"ery policies, including checkpoint policies (inorder vs. OoO reclamation, with confidence estimator <ref type=""bibr"" target=""#b12"">[13]</ref> versus without) and number of checkpoints (from 0 to 64).",0
"et=""#b10"">[11,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b8"">9]</ref>. The key difference is egister mappings is simpler, moreover, it is an optional enhancement for CFD.</p><p>Chappell et al. <ref type=""bibr"" target=""#b7"">[8]</ref> proposed Simultaneous Subordinate Microthreading (SSMT) as a",0
"></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>C. Encoding Hints in the PC</head><p>Jim?nez <ref type=""bibr"" target=""#b12"">[14]</ref> proposes to encode branch hints in the program counter. By ich are placed as much as possible on cold code paths. Overall, placement is not exact. We refer to <ref type=""bibr"" target=""#b12"">[14]</ref> for details of the algorithm.</p><p>Figure <ref type=""figu",1
">, early register release <ref type=""bibr"" target=""#b6"">[8]</ref> and criticallity-aware processors <ref type=""bibr"" target=""#b7"">[9]</ref>. Unfortunately, there are is no generic way to actually prov",0
"ctions. Many processors already contain mechanisms to convey hints on, e.g., branch directions [2], <ref type=""bibr"" target=""#b1"">[3]</ref>, branch targets <ref type=""bibr"" target=""#b2"">[4]</ref> and",0
"type=""bibr"" target=""#b4"">[6]</ref>, <ref type=""bibr"" target=""#b5"">[7]</ref>, early register release <ref type=""bibr"" target=""#b6"">[8]</ref> and criticallity-aware processors <ref type=""bibr"" target=""#",0
"dicated that hint bits are an enabling mechanism for hardware improvements such as value prediction <ref type=""bibr"" target=""#b4"">[6]</ref>, <ref type=""bibr"" target=""#b5"">[7]</ref>, early register rel",0
"e of the key design choices for a multilevel cache hierarchy is whether or not to enforce inclusion <ref type=""bibr"" target=""#b4"">[6,</ref><ref type=""bibr"" target=""#b25"">27,</ref><ref type=""bibr"" targ le inclusion greatly simplifies the cache coherence protocol <ref type=""bibr"" target=""#b7"">[9,</ref><ref type=""bibr"" target=""#b4"">6]</ref>, it limits performance when the size of the largest cache is f all the smaller caches of a multi-level cache hierarchy be a subset of the last-level cache (LLC) <ref type=""bibr"" target=""#b4"">[6]</ref>. When a line is evicted from the LLC, inclusion is enforced fit that an inclusive LLC provides, thus breaking the coherence benefits that come with inclusivity <ref type=""bibr"" target=""#b4"">[6]</ref>. While snoop filters <ref type=""bibr"" target=""#b3"">[5,</ref>",1
"OGY</head></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>A. Simulator</head><p>We use CMP$im <ref type=""bibr"" target=""#b14"">[16]</ref>, a Pin <ref type=""bibr"" target=""#b17"">[19]</ref> based tra",0
"/ns/1.0""><head>A. Simulator</head><p>We use CMP$im <ref type=""bibr"" target=""#b14"">[16]</ref>, a Pin <ref type=""bibr"" target=""#b17"">[19]</ref> based trace-driven x86 simulator for our performance studi",0
"use the LRU replacement policy while the LLC uses the Not Recently Used (NRU) replacement policy 4 <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b12"">14]</ref>. We model a stream p",0
"oads, however, our results indicate that global temporal locality indeed benefits CMPs. Garde et al <ref type=""bibr"" target=""#b11"">[13]</ref> followed up on Zahran's work and deconstructed global repl",0
"he time of its last reference. From the last reference until the block is evicted the block is dead <ref type=""bibr"" target=""#b12"">[13]</ref>. Cache blocks are dead on average 86.2% of the time over a everal dead block predictors and applied them to problems such as prefetching and block replacement <ref type=""bibr"" target=""#b12"">[13]</ref>, <ref type=""bibr"" target=""#b14"">[15]</ref>, <ref type=""bib ""#b0"">[1]</ref>.</p><p>1) Trace Based Predictor: Dead block prediction was introduced by Lai et al. <ref type=""bibr"" target=""#b12"">[13]</ref>. The Lai et al. predictor is used to prefetch data into de . In keeping with the methodology of recent cache papers <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" target=""#b12"">[13]</ref>, <ref type=""bibr"" target=""#b19"">[20]</ref>, <ref type=""bib",1
"dex a prediction table. A trace based predictor is also used to optimize a cache coherence protocol <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" target=""#b22"">[23]</ref> and perform dyn ments model a 16-way set-associative last-level cache to remain consistent with other previous work <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" target=""#b14"">[15]</ref>, <ref type=""bib e a 2MB LLC for the single-thread workloads. In keeping with the methodology of recent cache papers <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" target=""#b12"">[13]</ref>, <ref type=""bib",0
"of dead block prediction involves predicting in software <ref type=""bibr"" target=""#b24"">[25]</ref>, <ref type=""bibr"" target=""#b20"">[21]</ref>. In this approach the compiler collects dead block informa",0
"rget=""#b5"">[6]</ref>. The version we used was provided with the JILP Cache Replacement Championship <ref type=""bibr"" target=""#b1"">[2]</ref>. It models an out-of-order 4-wide 8-stage pipeline with a 12",0
"14"">[15]</ref>, <ref type=""bibr"" target=""#b10"">[11]</ref>, <ref type=""bibr"" target=""#b4"">[5]</ref>, <ref type=""bibr"" target=""#b0"">[1]</ref>.</p><p>1) Trace Based Predictor: Dead block prediction was i to prefetch into the L1 cache and filter a victim cache. Abella et al. propose a similar predictor <ref type=""bibr"" target=""#b0"">[1]</ref> based on number of references rather than cycles for reducin",0
"ures of the common covariates. These factors limit the benefit of applying stratification routinely <ref type=""bibr"" target=""#b8"">(8)</ref>.</p><p>Our new method (i) accomplishes the goals of stratifi",1
"ods suffer from well-recognized limitations that result from sampling variance and reporting biases <ref type=""bibr"" target=""#b4"">(4,</ref><ref type=""bibr"" target=""#b6"">6)</ref>.</p><p>Signal detectio systems for identifying ADEs faces challenges as a result of sampling variance and reporting biases <ref type=""bibr"" target=""#b4"">(4,</ref><ref type=""bibr"" target=""#b6"">6)</ref>. Modern signal detecti a fixed set of covariates reduces power by dividing up the available data across unimportant strata <ref type=""bibr"" target=""#b4"">(4,</ref><ref type=""bibr"" target=""#b15"">15)</ref>. Our approach does n thetically associated with an event that is more appropriately attributed to the underlying disease <ref type=""bibr"" target=""#b4"">(4)</ref>. For example, it is common for diabetes drugs to be reported",0
"by showing improved performance (compared to SIDER) at predicting drug targets and drug indications <ref type=""bibr"" target=""#b13"">(13,</ref><ref type=""bibr"" target=""#b14"">14)</ref>. Furthermore, we u argets and drug indications</head><p>Using a simplified version of the analysis by Campillos et al. <ref type=""bibr"" target=""#b13"">(13)</ref>, we computed pairwise similarity metrics between all drugs ed by Campillos and colleagues for use in correlating side-effect similarity to shared drug targets <ref type=""bibr"" target=""#b13"">(13)</ref>; and (iv) an independent database of the adverse event rep re similar in the side effects they elicit, then they are more likely to share a common drug target <ref type=""bibr"" target=""#b13"">(13)</ref>. As validation of the biological relevance of the methods ext-mined database of drug package inserts) to good effect <ref type=""bibr"" target=""#b12"">(12,</ref><ref type=""bibr"" target=""#b13"">13,</ref><ref type=""bibr"" target=""#b20"">20)</ref>. Our OFFSIDES datab",0
"de effects by comparing the structural similarity of drugs <ref type=""bibr"" target=""#b16"">(16,</ref><ref type=""bibr"" target=""#b17"">17)</ref>. In an analogous manner, protein structural similarity can",0
"blishing safety and efficacy of singleagent therapeutics. A wide range of methods, from text mining <ref type=""bibr"" target=""#b27"">(27,</ref><ref type=""bibr"" target=""#b28"">28)</ref> to network modelin",0
"over the past 20 years as new drug approvals lag behind, ballooning research and development costs <ref type=""bibr"" target=""#b0"">(1)</ref>. An increasing proportion of experimental drugs are failing",1
"tems biology approach revealed new molecular players in the development of coronary atherosclerosis <ref type=""bibr"" target=""#b21"">(22)</ref> and identified new targets in the treatment of in-stent re",0
"drug treatments have been used for over 30 years as a way to increase efficacy and reduce toxicity <ref type=""bibr"" target=""#b13"">(14)</ref>. Successful development of combinatorial treatments-what w",0
"together. This is evident by the relatively high rate of unexpected adverse drug-drug interactions <ref type=""bibr"" target=""#b48"">(49)</ref>. The human system is a complex network of interaction path",0
"r training neural networks in the reinforcement learning setting, such as neural fitted Q-iteration <ref type=""bibr"" target=""#b18"">24</ref> , these methods involve the repeated training of networks de ters on each game, privy only to the inputs a human player would have. In contrast to previous work <ref type=""bibr"" target=""#b18"">24,</ref><ref type=""bibr"" target=""#b20"">26</ref> , our approach incor e history and the action have been used as inputs to the neural network by some previous approaches <ref type=""bibr"" target=""#b18"">24,</ref><ref type=""bibr"" target=""#b20"">26</ref> . The main drawback",1
"uts a human player would have. In contrast to previous work <ref type=""bibr"" target=""#b18"">24,</ref><ref type=""bibr"" target=""#b20"">26</ref> , our approach incorporates 'end-to-end' reinforcement learn as inputs to the neural network by some previous approaches <ref type=""bibr"" target=""#b18"">24,</ref><ref type=""bibr"" target=""#b20"">26</ref> . The main drawback of this type of architecture is that a s",1
"bserved, low-dimensional state spaces. Here we use recent advances in training deep neural networks <ref type=""bibr"" target=""#b3"">[9]</ref><ref type=""bibr"" target=""#b4"">[10]</ref><ref type=""bibr"" targ get=""#b10"">16</ref> known as deep neural networks. Notably, recent advances in deep neural networks <ref type=""bibr"" target=""#b3"">[9]</ref><ref type=""bibr"" target=""#b4"">[10]</ref><ref type=""bibr"" targ",0
"experience replay <ref type=""bibr"" target=""#b15"">[21]</ref><ref type=""bibr"" target=""#b16"">[22]</ref><ref type=""bibr"" target=""#b17"">[23]</ref> that randomizes over the data, thereby removing correlatio replay algorithm <ref type=""bibr"" target=""#b15"">[21]</ref><ref type=""bibr"" target=""#b16"">[22]</ref><ref type=""bibr"" target=""#b17"">[23]</ref> involving the storage and representation of recently exper large neural networks without diverging.</p><p>First, we use a technique known as experience replay <ref type=""bibr"" target=""#b17"">23</ref> in which we store the agent's experiences at each time-step,",0
"agents have achieved some successes in a variety of domains <ref type=""bibr"" target=""#b0"">[6]</ref><ref type=""bibr"" target=""#b1"">[7]</ref><ref type=""bibr"" target=""#b2"">[8]</ref> , their applicability",0
"""bibr"" target=""#b7"">13</ref> that has eluded previous efforts <ref type=""bibr"" target=""#b2"">8,</ref><ref type=""bibr"" target=""#b8"">14,</ref><ref type=""bibr"" target=""#b9"">15</ref> . To achieve this, we",0
"rceptual learning may influence the characteristics of representations within primate visual cortex <ref type=""bibr"" target=""#b21"">27,</ref><ref type=""bibr"" target=""#b22"">28</ref> . Notably, the succe",0
"rithms 3 . While reinforcement learning agents have achieved some successes in a variety of domains <ref type=""bibr"" target=""#b0"">[6]</ref><ref type=""bibr"" target=""#b1"">[7]</ref><ref type=""bibr"" targe",0
"a biologically inspired mechanism termed experience replay <ref type=""bibr"" target=""#b15"">[21]</ref><ref type=""bibr"" target=""#b16"">[22]</ref><ref type=""bibr"" target=""#b17"">[23]</ref> that randomizes o cally dependent on our incorporation of a replay algorithm <ref type=""bibr"" target=""#b15"">[21]</ref><ref type=""bibr"" target=""#b16"">[22]</ref><ref type=""bibr"" target=""#b17"">[23]</ref> involving the sto of recently experienced trajectories during offline periods <ref type=""bibr"" target=""#b15"">21,</ref><ref type=""bibr"" target=""#b16"">22</ref> (for example, waking rest) providing a putative mechanism by ism by which value functions may be efficiently updated through interactions with the basal ganglia <ref type=""bibr"" target=""#b16"">22</ref> . In the future, it will be important to explore the potenti",0
"mpetencies on a varied range of challenging tasks-a central goal of general artificial intelligence <ref type=""bibr"" target=""#b7"">13</ref> that has eluded previous efforts <ref type=""bibr"" target=""#b2",0
"#b17"">17</ref> , face recognition <ref type=""bibr"" target=""#b18"">18</ref> , and playing Atari games <ref type=""bibr"" target=""#b19"">19</ref> . They use many layers of neurons, each arranged in overlapp",1
"r"" target=""#b13"">13</ref> , dynamic komi <ref type=""bibr"" target=""#b59"">59</ref> or an opening book <ref type=""bibr"" target=""#b60"">60</ref> . The parameters used by AlphaGo in the Fan Hui match are li",0
"t=""#b13"">13,</ref><ref type=""bibr"" target=""#b21"">[21]</ref><ref type=""bibr"" target=""#b22"">[22]</ref><ref type=""bibr"" target=""#b23"">[23]</ref><ref type=""bibr"" target=""#b24"">[24]</ref> . The SL policy n ger networks, as it prevents the intermediate filters from identifying specific asymmetric patterns <ref type=""bibr"" target=""#b23"">23</ref> . Instead, we exploit symmetries at run-time by dynamically",0
"ng expert moves in the game of Go using supervised learning <ref type=""bibr"" target=""#b13"">13,</ref><ref type=""bibr"" target=""#b21"">[21]</ref><ref type=""bibr"" target=""#b22"">[22]</ref><ref type=""bibr"" t tern is a binary feature matching the colour and liberty count in a 12-point diamond-shaped pattern <ref type=""bibr"" target=""#b21"">21</ref> centred around the previous move. Additionally, a small numb",0
"Go programs are based on MCTS, enhanced by policies that are trained to predict human expert moves <ref type=""bibr"" target=""#b13"">13</ref> . These policies are used to narrow the search to a beam of s of AlphaGo and several other Go programs, including the strongest commercial programs Crazy Stone <ref type=""bibr"" target=""#b13"">13</ref> and Zen, and the strongest open source programs Pachi 14 and combined with a policy that is used to narrow the beam of the search tree to high-probability moves <ref type=""bibr"" target=""#b13"">13</ref> ; or to bias the bonus term towards high-probability moves < atterns <ref type=""bibr"" target=""#b50"">50</ref> or learning rollout policies by supervised learning <ref type=""bibr"" target=""#b13"">13</ref> , reinforcement learning <ref type=""bibr"" target=""#b16"">16</ do not appear to give any additional benefit. In addition AlphaGo does not use progressive widening <ref type=""bibr"" target=""#b13"">13</ref> , dynamic komi <ref type=""bibr"" target=""#b59"">59</ref> or an p>Our rollout policy p π (a|s) contains less handcrafted knowledge than stateof-the-art Go programs <ref type=""bibr"" target=""#b13"">13</ref> . Instead, we exploit the higher-quality action selection wi lity actions, and to sample actions during rollouts. This approach has achieved strong amateur play <ref type=""bibr"" target=""#b13"">[13]</ref><ref type=""bibr"" target=""#b14"">[14]</ref><ref type=""bibr"" t ref type=""bibr"" target=""#b15"">[15]</ref> . However, prior work has been limited to shallow policies <ref type=""bibr"" target=""#b13"">[13]</ref><ref type=""bibr"" target=""#b14"">[14]</ref><ref type=""bibr"" t →∞ V s v s v s lim lim n n n P n</formula><p>. The strongest current Go programs are based on MCTS <ref type=""bibr"" target=""#b13"">[13]</ref><ref type=""bibr"" target=""#b14"">[14]</ref><ref type=""bibr"" t fficient learning updates with immediate feedback and high-quality gradients. Similar to prior work <ref type=""bibr"" target=""#b13"">13,</ref><ref type=""bibr"" target=""#b15"">15</ref> , we also train a fa line, we build on prior work on predicting expert moves in the game of Go using supervised learning <ref type=""bibr"" target=""#b13"">13,</ref><ref type=""bibr"" target=""#b21"">[21]</ref><ref type=""bibr"" ta",0
"r"" target=""#b34"">[35]</ref>, which was then used to develop a multimodal deception detection system <ref type=""bibr"" target=""#b1"">[2]</ref>. An extensive review of approaches for evaluating human cred sent useful clues for deception, their performance is often similar to that of the n-grams features <ref type=""bibr"" target=""#b1"">[2]</ref>. Since in our current work we are not focusing on the insigh",1
"While there is research work that has used court trial transcripts to identify deceptive statements <ref type=""bibr"" target=""#b13"">[14]</ref>, we are not aware of any previous work that took into cons cusing on real-life high-stake data. The work closest to ours is presented by Fornaciari and Poesio <ref type=""bibr"" target=""#b13"">[14]</ref>, which targets the identification of deception in statemen",1
"pe=""bibr"" target=""#b8"">[9]</ref>. The gesture annotation is performed using the MUMIN coding scheme <ref type=""bibr"" target=""#b2"">[3]</ref>, which is a standard multimodal annotation scheme for interp",1
"ies previous findings where human ability to spot liars was found to be slightly better than chance <ref type=""bibr"" target=""#b0"">[1]</ref>. Moreover, the performance of the human annotators appears t",0
"relying solely on such physiological measurements can be biased and misleading. Chittaranjan et al. <ref type=""bibr"" target=""#b6"">[7]</ref> created an audio-visual recordings of the ""Are you a Werewol",0
"<ref type=""bibr"" target=""#b41"">[42,</ref><ref type=""bibr"" target=""#b22"">23]</ref>, social networks <ref type=""bibr"" target=""#b20"">[21]</ref>, and consumer report websites <ref type=""bibr"" target=""#b3",0
"ere analyzed using smoothness and asymmetry measurements to further relate them to an act of deceit <ref type=""bibr"" target=""#b11"">[12]</ref>. Tian et al. <ref type=""bibr"" target=""#b37"">[38]</ref> con",0
"deception process. Research work has relied on the Linguistic Inquiry and Word Count (LIWC) lexicon <ref type=""bibr"" target=""#b33"">[34]</ref> to build deception models using machine learning approache nt set.</p><p>Previous work has also considered features derived from semantic lexicons, e.g., LIWC <ref type=""bibr"" target=""#b33"">[34]</ref>. However, while these are great features to consider in or",0
"o detect deceit by tracking the hand movements of subjects <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b39"">40]</ref>, or using geometric features related to the hand and head m",0
"r hand gestures, blob analysis was used to detect deceit by tracking the hand movements of subjects <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b39"">40]</ref>, or using geometri",0
"isplays and hand movements, as they have been previously found to correlate with deceptive behavior <ref type=""bibr"" target=""#b8"">[9]</ref>. The gesture annotation is performed using the MUMIN coding",0
"s derived from syntactic CFG trees and part of speech tags <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b42"">43]</ref>. Some studies have also incorporated the analysis of psycho",0
"ype=""bibr"" target=""#b39"">40]</ref>, or using geometric features related to the hand and head motion <ref type=""bibr"" target=""#b26"">[27]</ref>. Caso et al. <ref type=""bibr"" target=""#b5"">[6]</ref> ident",0
"e. Several studies <ref type=""bibr"" target=""#b40"">[41,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b9"">10]</ref> indicated that relying solely on such physiological measurem",0
"order to improve lie detection in criminalsuspect interrogations, Sumriddetchkajorn and Somboonkaew <ref type=""bibr"" target=""#b36"">[37]</ref> developed an infrared system to detect lies by using therm",0
"to a conventional design due to an increase in the access time on a filter cache miss. The L-Cache <ref type=""bibr"" target=""#b5"">[6]</ref> similarly reduces switching activity by holding loop-nested",1
"frequently disabled. In addition, frequent cache flushing can significantly impact cache miss rate <ref type=""bibr"" target=""#b3"">[4]</ref>, and considerable energy may be expended.</p><p>An alternati",0
"proaches have been proposed for reducing the switching activity in on-chip caches. The filter cache <ref type=""bibr"" target=""#b17"">[18]</ref> attempts to reduce energy dissipation by placing a small (",0
"acity, and conflict types. More recent tools, such as the profiling tool developed by Ammons et al. <ref type=""bibr"" target=""#b1"">[2]</ref> and Compaq's ProfileMe <ref type=""bibr"" target=""#b9"">[10]</r",0
"onventional caches and caches employing selective cache ways.</p><p>We use the SimpleScalar toolset <ref type=""bibr"" target=""#b7"">[8]</ref> to model an out-oforder speculative processor with a two-lev",0
"hes in drug discovery <ref type=""bibr"" target=""#b0"">[1]</ref><ref type=""bibr"" target=""#b1"">[2]</ref><ref type=""bibr"" target=""#b2"">[3]</ref>. We have shown previously that a large semantic network of d f>.</p><p>In this work, we apply a random walk-based link prediction algorithm based on Chen et al. <ref type=""bibr"" target=""#b2"">[3]</ref> to a more extensive drug-target network and evaluated its pe rug transition matrix. The calculation of each of the transition matrix in discussed in Chen et al. <ref type=""bibr"" target=""#b2"">[3]</ref>. The random walk is implemented on the heterogeneous network #b14"">[16]</ref>. Because of these evidences, we here simply adopt the previously used value of 0.3 <ref type=""bibr"" target=""#b2"">[3]</ref>. Second, the robustness of ? (jumping probability) has alrea >. It has been shown that the weight parameters w d and w t are robust among the prediction results <ref type=""bibr"" target=""#b2"">[3]</ref>.</p><p>In our drug target network 684 (94%) drugs have at le arget network into account without separating protein categories, in contrast to the previous study <ref type=""bibr"" target=""#b2"">[3]</ref>. The following estimation corroborates our approach. Our dru network and (S ab equals 1 if node a and b are connected, 0 otherwise) K a denotes the degree of a. <ref type=""bibr"" target=""#b2"">(3)</ref> With the probability c, the walker goes back to a. (4) After is the first time that the random walk-based method is evaluated using a binding assay dataset (cf. <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b4"">5]</ref>).</p></div> <div xmlns",1
"protein targets. We searched three databases ChEMBL <ref type=""bibr"" target=""#b16"">[19]</ref>, PDSP <ref type=""bibr"" target=""#b17"">[20]</ref>, and Pubchem <ref type=""bibr"" target=""#b18"">[21]</ref> usi",0
"rated the power of networkbased approaches in drug discovery <ref type=""bibr"" target=""#b0"">[1]</ref><ref type=""bibr"" target=""#b1"">[2]</ref><ref type=""bibr"" target=""#b2"">[3]</ref>. We have shown previo y using the R Biostrings package and the normalization procedure proposed by Bleakley and Yamanishi <ref type=""bibr"" target=""#b1"">[2]</ref>:</p><p>(1) where SW (?, ?) means the original Smith-Waterman g the following procedure: <ref type=""bibr"" target=""#b0"">(1)</ref> we start a random walker from a. <ref type=""bibr"" target=""#b1"">(2)</ref> At each time step, with the probability 1c, the walker walks",0
"examining the shared drugs. Finally, the links between drugs and targets are obtained from DrugBank <ref type=""bibr"" target=""#b5"">[6]</ref>.</p><p>Random walk is a useful mathematical framework that p ead>Datasets Drugs</head><p>We compile a set of approved drugs from DrugBank database (Version 3.0) <ref type=""bibr"" target=""#b5"">[6]</ref>, consisting of 727 compounds and 3519 protein targets (Addit",0
"ground</head><p>Recent work has demonstrated the power of networkbased approaches in drug discovery <ref type=""bibr"" target=""#b0"">[1]</ref><ref type=""bibr"" target=""#b1"">[2]</ref><ref type=""bibr"" targe r each pair of nodes a, b ? V we can assign a proximity score by executing the following procedure: <ref type=""bibr"" target=""#b0"">(1)</ref> we start a random walker from a. <ref type=""bibr"" target=""#b",0
"se performances than a single convolution due to overfitting. To overcome overfitting, Liang and Hu <ref type=""bibr"" target=""#b16"">[17]</ref> uses a recurrent layer that takes feed-forward inputs into is in accordance with the limited success of previous methods using at most three recursions so far <ref type=""bibr"" target=""#b16"">[17]</ref>. Among many reasons, two severe problems are vanishing and",1
"sion reduction occurs in the recurrent convolutional neural networks used for semantic segmentation <ref type=""bibr"" target=""#b21"">[22]</ref>. As SR methods predict full-sized images, dimension reduct",1
"ding <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b18"">19]</ref>, sparse coding <ref type=""bibr"" target=""#b30"">[31,</ref><ref type=""bibr"" target=""#b31"">32,</ref><ref type=""bibr"" ta org/ns/1.0""><head n=""4.1."">Datasets</head><p>For training, we use 91 images proposed in Yang et al. <ref type=""bibr"" target=""#b30"">[31]</ref> for all experiments. For testing, we use four datasets. Da",0
"eceptive fields (224x224 common in ImageNet classification <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b23"">24]</ref>). Among many approaches to widen the receptive field, incre",0
"et=""#b27"">[28,</ref><ref type=""bibr"" target=""#b28"">29,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b1"">2]</ref> predict image details only. Similarly, we find that this doma",0
"e set all weights to zero except self-connections (connection to the same neuron in the next layer) <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b13"">14]</ref>. Biases are set to",0
"target=""#b4"">5]</ref>. Dataset B100 consists of natural images in the Berkeley Segmentation Dataset <ref type=""bibr"" target=""#b19"">[20]</ref>. Finally, dataset Urban100, urban images recently provided",0
"med. Our network has the receptive field of 41 by 41 and this is relatively large compared to SRCNN <ref type=""bibr"" target=""#b4"">[5]</ref>  <ref type=""bibr"">(13 by 13)</ref>. While DRCN has good prop target=""#b27"">28,</ref><ref type=""bibr"" target=""#b28"">29]</ref>, convolutional neural network (CNN) <ref type=""bibr"" target=""#b4"">[5]</ref> and random forest <ref type=""bibr"" target=""#b22"">[23]</ref>. 3]</ref>.</p><p>Among several recent learning-based successes, convolutional neural network (SRCNN) <ref type=""bibr"" target=""#b4"">[5]</ref> demonstrated the feasibility of an end-to-end approach to SR terpolated input image (to the desired size) as input x and predicts the target image y as in SRCNN <ref type=""bibr"" target=""#b4"">[5]</ref>. Our goal is to learn a model f that predicts values ŷ = f ( ts. We first describe datasets used Ground Truth A+ <ref type=""bibr"" target=""#b28"">[29]</ref> SRCNN <ref type=""bibr"" target=""#b4"">[5]</ref> RFL <ref type=""bibr"" target=""#b22"">[23]</ref> SelfEx   for t parisons. For benchmark, we use public code for A+ <ref type=""bibr"" target=""#b28"">[29]</ref>, SRCNN <ref type=""bibr"" target=""#b4"">[5]</ref>, RFL <ref type=""bibr"" target=""#b22"">[23]</ref> and SelfEx <r used for benchmark <ref type=""bibr"" target=""#b28"">[29,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b4"">5]</ref>. Dataset B100 consists of natural images in the Berkeley Segm",0
"n=""2.1."">Single-Image Super-Resolution</head><p>We apply DRCN to single-image super-resolution (SR) <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" targ",0
"t.</p><p>For initializing weights in non-recursive layers, we use the method described in He et al. <ref type=""bibr"" target=""#b8"">[9]</ref>. For recursive convolutions, we set all weights to zero exce",0
"ut <ref type=""bibr"" target=""#b16"">[17]</ref>.</p><p>When the graph structure of the input is known, <ref type=""bibr"" target=""#b1"">[2]</ref> introduced a model to generalize ConvNets using low learning r parameters. Our main contributions can be summarized as follows:</p><p>• We extend the ideas from <ref type=""bibr"" target=""#b1"">[2]</ref> to large-scale classification problems, specifically Imagene vised fashion. However, it does not attempt to exploit any weight-sharing strategy.</p><p>Recently, <ref type=""bibr"" target=""#b1"">[2]</ref> proposed a generalization of convolutions to graphs via the v xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.1"">Spectral Networks</head><p>Our work builds upon <ref type=""bibr"" target=""#b1"">[2]</ref> which introduced spectral networks. We recall the definition mula_4"">∂ k x(ξ) ∂ξ k ≤ C |u| k |x(u)|du ,</formula><p>where x(ξ) is the Fourier transform of x. In <ref type=""bibr"" target=""#b1"">[2]</ref> it was suggested to use the same principle in a general grap ction</head><p>Whereas some recognition tasks in non-Euclidean domains, such as those considered in <ref type=""bibr"" target=""#b1"">[2]</ref> or <ref type=""bibr"" target=""#b11"">[12]</ref>, might have a p",1
"label></formula><p>In our experiments, we also consider the variant of self-tuning diffusion kernel <ref type=""bibr"" target=""#b20"">[21]</ref> ω(i, j) = exp</p><formula xml:id=""formula_8"">− d(i,j) σ i",0
"local receptive fields <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b13"">14]</ref>, mostly with applications to image recognition. In particul",0
"rough the identification of a certain graphical model using 1 -penalized logistic regression. Also, <ref type=""bibr"" target=""#b2"">[3]</ref> considers the problem of learning a deep architecture throug",0
"eech. These properties are exploited efficiently by ConvNets <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b6"">7]</ref>, which are designed to extract local features that are shared",0
"they have recently been shown to be particularly unstable to adversarial perturbations of the data <ref type=""bibr"" target=""#b17"">[18]</ref>. In fact, very small and often imperceptible perturbations e classifier, it seems that the adversarial perturbations are generalizable across different models <ref type=""bibr"" target=""#b17"">[18]</ref>. This can actually become a real concern from a security p of the relevant work. The phenomenon of adversarial instability was first introduced and studied in <ref type=""bibr"" target=""#b17"">[18]</ref>. The authors estimated adversarial examples by solving pen explaining the presence of adversarial examples. Unfortunately, the optimization method employed in <ref type=""bibr"" target=""#b17"">[18]</ref> is time-consuming and therefore does not scale to large da he training procedure that allows to boost the robustness of the classifier. Notably, the method in <ref type=""bibr"" target=""#b17"">[18]</ref> was applied in order to generate adversarial perturbations he proposed DeepFool approach to stateof-the-art techniques to compute adversarial perturbations in <ref type=""bibr"" target=""#b17"">[18]</ref> and <ref type=""bibr"" target=""#b3"">[4]</ref>. The method in ref type=""bibr"" target=""#b17"">[18]</ref> and <ref type=""bibr"" target=""#b3"">[4]</ref>. The method in <ref type=""bibr"" target=""#b17"">[18]</ref> solves a series of penalized optimization problems to find ver that the proposed approach also yields slightly smaller perturbation vectors than the method in <ref type=""bibr"" target=""#b17"">[18]</ref>. The proposed approach is hence more accurate in detecting mplexity aspect, the proposed approach is substantially faster than the standard method proposed in <ref type=""bibr"" target=""#b17"">[18]</ref>. In fact, while the approach <ref type=""bibr"" target=""#b17 standard method proposed in <ref type=""bibr"" target=""#b17"">[18]</ref>. In fact, while the approach <ref type=""bibr"" target=""#b17"">[18]</ref> involves a costly minimization of a series of objective fu",1
"al flow method. The convergence analysis of this optimization technique can be found for example in <ref type=""bibr"" target=""#b20"">[21]</ref>. Our algorithm in the binary case can alternatively be see",0
"ould finally mention that the phenomenon of adversarial instability also led to theoretical work in <ref type=""bibr"" target=""#b1"">[2]</ref> that studied the problem of adversarial perturbations on som",0
"enerated synthetic unrecognizable images, which are classified with high confidence. The authors of <ref type=""bibr"" target=""#b2"">[3]</ref> also studied a related problem of finding the minimal geomet",0
"(NIN) architecture <ref type=""bibr"" target=""#b10"">[11]</ref>.</p><p>• ILSVRC 2012: We used CaffeNet <ref type=""bibr"" target=""#b6"">[7]</ref> and GoogLeNet <ref type=""bibr"" target=""#b16"">[17]</ref> pre-",0
"org/ns/1.0""><head n=""2."">Related Work</head><p>Residual Representations. In image recognition, VLAD <ref type=""bibr"" target=""#b17"">[18]</ref> is a representation that encodes by the residual vectors w isher Vector <ref type=""bibr"" target=""#b29"">[30]</ref> can be formulated as a probabilistic version <ref type=""bibr"" target=""#b17"">[18]</ref> of VLAD. Both of them are powerful shallow representations",1
"d by the number of stacked layers (depth). Recent evidence <ref type=""bibr"" target=""#b39"">[40,</ref><ref type=""bibr"" target=""#b42"">43]</ref> reveals that network depth is of crucial importance, and th rk depth is of crucial importance, and the leading results <ref type=""bibr"" target=""#b39"">[40,</ref><ref type=""bibr"" target=""#b42"">43,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" tar to the output <ref type=""bibr"" target=""#b32"">[33,</ref><ref type=""bibr"" target=""#b47"">48]</ref>. In <ref type=""bibr"" target=""#b42"">[43,</ref><ref type=""bibr"" target=""#b23"">24]</ref>, a few intermediat entering layer responses, gradients, and propagated errors, implemented by shortcut connections. In <ref type=""bibr"" target=""#b42"">[43]</ref>, an ""inception"" layer is composed of a shortcut branch and",1
"et=""#b39"">[40,</ref><ref type=""bibr"" target=""#b42"">43,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b15"">16]</ref> on the challenging ImageNet dataset <ref type=""bibr"" target b39"">[40]</ref> models, with a depth of sixteen <ref type=""bibr"" target=""#b39"">[40]</ref> to thirty <ref type=""bibr"" target=""#b15"">[16]</ref>. Many other nontrivial visual recognition tasks <ref type= rget=""#b35"">36,</ref><ref type=""bibr"" target=""#b11"">12]</ref> and intermediate normalization layers <ref type=""bibr"" target=""#b15"">[16]</ref>, which enable networks with tens of layers to start conver ugmentation in <ref type=""bibr"" target=""#b20"">[21]</ref> is used. We adopt batch normalization (BN) <ref type=""bibr"" target=""#b15"">[16]</ref> right after each convolution and before activation, follow type=""bibr"" target=""#b15"">[16]</ref> right after each convolution and before activation, following <ref type=""bibr"" target=""#b15"">[16]</ref>. We initialize the weights as in <ref type=""bibr"" target="" of 0.9. We do not use dropout <ref type=""bibr"" target=""#b12"">[13]</ref>, following the practice in <ref type=""bibr"" target=""#b15"">[16]</ref>.</p><p>In testing, for comparison studies we adopt the sta ifficulty is unlikely to be caused by vanishing gradients. These plain networks are trained with BN <ref type=""bibr"" target=""#b15"">[16]</ref>, which ensures forward propagated signals to have non-zero et=""#b39"">[40]</ref> (v5) 6.8 PReLU-net <ref type=""bibr"" target=""#b11"">[12]</ref> 4.94 BN-inception <ref type=""bibr"" target=""#b15"">[16]</ref> 4.82 ResNet (ILSVRC <ref type=""bibr"">'15)</ref> 3.57</p><p tum of 0.9, and adopt the weight initialization in <ref type=""bibr"" target=""#b11"">[12]</ref> and BN <ref type=""bibr"" target=""#b15"">[16]</ref> but with no dropout. These models are trained with a minib",0
"""http://www.tei-c.org/ns/1.0""><head n=""1."">Introduction</head><p>Deep convolutional neural networks <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b20"">21]</ref> have led to a seri with tens of layers to start converging for stochastic gradient descent (SGD) with backpropagation <ref type=""bibr"" target=""#b21"">[22]</ref>.</p><p>When deeper networks are able to start converging,",0
"? An obstacle to answering this question was the notorious problem of vanishing/exploding gradients <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b0"">1,</ref><ref type=""bibr"" targ",0
"> of VLAD. Both of them are powerful shallow representations for image retrieval and classification <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b46"">47]</ref>. For vector quantiza",0
"tantially better than previous networks.</p><p>Similar phenomena are also shown on the CIFAR-10 set <ref type=""bibr"" target=""#b19"">[20]</ref>, suggesting that the optimization difficulties and the eff .0""><head n=""4.2."">CIFAR-10 and Analysis</head><p>We conducted more studies on the CIFAR-10 dataset <ref type=""bibr"" target=""#b19"">[20]</ref>, which consists of 50k training images and 10k testing ima",0
"et=""#b37"">[38,</ref><ref type=""bibr"" target=""#b36"">37,</ref><ref type=""bibr"" target=""#b30"">31,</ref><ref type=""bibr"" target=""#b45"">46]</ref> propose methods for centering layer responses, gradients, a",0
"). On this dataset we use identity shortcuts in all cases (i.e., option A), method error (%) Maxout <ref type=""bibr"" target=""#b8"">[9]</ref> 9.38 NIN <ref type=""bibr"" target=""#b24"">[25]</ref> 8.81 DSN ork may be unnecessarily large (19.4M) for this small dataset. Strong regularization such as maxout <ref type=""bibr"" target=""#b8"">[9]</ref> or dropout <ref type=""bibr"" target=""#b12"">[13]</ref> is appl",0
"rnative to Multigrid is hierarchical basis preconditioning <ref type=""bibr"" target=""#b43"">[44,</ref><ref type=""bibr"" target=""#b44"">45]</ref>, which relies on variables that represent residual vectors s. It has been shown <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b43"">44,</ref><ref type=""bibr"" target=""#b44"">45</ref>] that these solvers converge much faster than standard solve",0
"ng boxes, eliminate duplicate detections, and rescore the boxes based on other objects in the scene <ref type=""bibr"" target=""#b12"">[13]</ref>. These complex pipelines are slow and hard to optimize bec Then, classifiers <ref type=""bibr"" target=""#b34"">[35,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b9"">10]</ref> or localizers <ref t",1
"ontextual information about classes as well as their appearance. Fast R-CNN, a top detection method <ref type=""bibr"" target=""#b13"">[14]</ref>, mistakes background patches in an image for objects becau pendently and the resulting system is very slow, taking more than 40 seconds per image at test time <ref type=""bibr"" target=""#b13"">[14]</ref>.</p><p>YOLO shares some similarities with R-CNN. Each grid ork by sharing computation and using neural networks to propose regions instead of Selective Search <ref type=""bibr"" target=""#b13"">[14]</ref>  <ref type=""bibr"" target=""#b26"">[27]</ref>. While they off the errors on VOC 2007 made by YOLO and Fast R-CNN, one of the highest performing versions of R-CNN <ref type=""bibr"" target=""#b13"">[14]</ref>. Based on the different error profiles we show that YOLO c",0
"aining</head><p>We pretrain our convolutional layers on the ImageNet 1000-class competition dataset <ref type=""bibr"" target=""#b28"">[29]</ref>. For pretraining we use the first 20 convolutional layers",0
"pe=""bibr"" target=""#b2"">[3]</ref>. We compare YOLO to other detection systems on the Picasso Dataset <ref type=""bibr"" target=""#b11"">[12]</ref> and the People-Art Dataset <ref type=""bibr"" target=""#b2"">[",0
"run either in sliding window fashion over the whole image or on some subset of regions in the image <ref type=""bibr"" target=""#b33"">[34,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" ta s use region proposals instead of sliding windows to find objects in images. Selective</p><p>Search <ref type=""bibr"" target=""#b33"">[34]</ref> generates potential bounding boxes, a convolutional networ",0
"of 88% on the ImageNet 2012 validation set, comparable to the GoogLeNet models in Caffe's Model Zoo <ref type=""bibr"" target=""#b23"">[24]</ref>.</p><p>We then convert the model to perform detection. Ren",0
"networks to propose regions instead of Selective Search <ref type=""bibr"" target=""#b13"">[14]</ref>  <ref type=""bibr"" target=""#b26"">[27]</ref>. While they offer speed and accuracy improvements over R-C",0
"ref type=""bibr"" target=""#b5"">[6]</ref>). Then, classifiers <ref type=""bibr"" target=""#b34"">[35,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" tar",0
"cedure. We learn residuals only and use extremely high learning rates (10 4 times higher than SRCNN <ref type=""bibr"" target=""#b8"">[6]</ref>) enabled by adjustable gradient clipping. Our proposed metho ely, random forest <ref type=""bibr"" target=""#b20"">[18]</ref> and convolutional neural network (CNN) <ref type=""bibr"" target=""#b8"">[6]</ref> have also been used with large improvements in accuracy.</p> 8"">[6]</ref> have also been used with large improvements in accuracy.</p><p>Among them, Dong et al. <ref type=""bibr"" target=""#b8"">[6]</ref> has demonstrated that a CNN can be used to learn a mapping f are highly correlated. Moreover, our initial learning rate is 10 4 times higher than that of SRCNN <ref type=""bibr"" target=""#b8"">[6]</ref>. This is enabled by residual-learning and gradient clipping. d reconstruction. Filters of spatial sizes 9 × 9, 1 × 1, and 5 × 5 were used respectively.</p><p>In <ref type=""bibr"" target=""#b8"">[6]</ref>, Dong et al. attempted to prepare deeper models, but failed ce. We successfully use 20 weight layers (3 × 3 for each layer). Our network is very deep (20 vs. 3 <ref type=""bibr"" target=""#b8"">[6]</ref>) and information used for reconstruction (receptive field) i for Very Deep Networks Training deep models can fail to converge in realistic limit of time. SRCNN <ref type=""bibr"" target=""#b8"">[6]</ref> fails to show superior performance with more than three weig a network to converge within a week on a common GPU. Looking at Fig. <ref type=""figure"">9</ref> of <ref type=""bibr"" target=""#b8"">[6]</ref>, it is not easy to say their deeper networks have converged n of 200 images from Berkeley Segmentation Dataset <ref type=""bibr"" target=""#b18"">[16]</ref>. SRCNN <ref type=""bibr"" target=""#b8"">[6]</ref> uses a very large ImageNet dataset.</p><p>We use 291 images The public code of SRCNN based on a CPU implementation is slower than the code used by Dong et. al <ref type=""bibr"" target=""#b8"">[6]</ref> in their paper based on a GPU implementation.</p><p>In Figur",1
"image given a low-resolution (LR) image, commonly referred as single image super-resolution (SISR) <ref type=""bibr"" target=""#b14"">[12]</ref>, <ref type=""bibr"" target=""#b10"">[8]</ref>, <ref type=""bibr",0
"to model a mapping from LR to HR patches. Neighbor embedding <ref type=""bibr"" target=""#b6"">[4,</ref><ref type=""bibr"" target=""#b17"">15]</ref> methods interpolate the patch subspace. Sparse coding <ref resolution methods <ref type=""bibr"" target=""#b23"">[21,</ref><ref type=""bibr"" target=""#b24"">22,</ref><ref type=""bibr"" target=""#b17"">15,</ref><ref type=""bibr"" target=""#b5"">3]</ref> and we find that CNN- n SR comparisons are considered. Possible pairs (s train ,s test ) are tried for the dataset 'Set5' <ref type=""bibr"" target=""#b17"">[15]</ref>. Experimental results are summarized in Table <ref type=""t can be slightly different.</p><p>Test dataset For benchmark, we use four datasets. Datasets 'Set5' <ref type=""bibr"" target=""#b17"">[15]</ref> and 'Set14' <ref type=""bibr"" target=""#b28"">[26]</ref> are",0
"olution (SISR) <ref type=""bibr"" target=""#b14"">[12]</ref>, <ref type=""bibr"" target=""#b10"">[8]</ref>, <ref type=""bibr"" target=""#b11"">[9]</ref>. SISR is widely used in computer vision applications rangin bibr"" target=""#b22"">[20,</ref><ref type=""bibr"" target=""#b15"">13]</ref> or internal patch recurrence <ref type=""bibr"" target=""#b11"">[9]</ref>.</p><p>Currently, learning methods are widely used to model ticated models to luminance components as in other methods <ref type=""bibr"" target=""#b6"">[4]</ref>, <ref type=""bibr"" target=""#b11"">[9]</ref>, <ref type=""bibr"" target=""#b28"">[26]</ref>. This is because",0
"operties that could be used to craft adversarial samples <ref type=""bibr"" target=""#b17"">[18]</ref>, <ref type=""bibr"" target=""#b28"">[30]</ref>, <ref type=""bibr"" target=""#b34"">[36]</ref>. Simply put, th ages that are unrecognizable to humans, but are nonetheless labeled as recognizable objects by DNNs <ref type=""bibr"" target=""#b28"">[30]</ref>. For instance, they demonstrated how a DNN will classify a e backpropagation procedure used during network training <ref type=""bibr"" target=""#b17"">[18]</ref>, <ref type=""bibr"" target=""#b28"">[30]</ref>, <ref type=""bibr"" target=""#b34"">[36]</ref>. This approach",1
"such attacks occur today on non-DL classification systems <ref type=""bibr"" target=""#b5"">[6]</ref>, <ref type=""bibr"" target=""#b6"">[7]</ref>, <ref type=""bibr"" target=""#b21"">[22]</ref>. In the physical",0
"financial fraud detection <ref type=""bibr"" target=""#b22"">[23]</ref>, and recently malware detection <ref type=""bibr"" target=""#b13"">[14]</ref>.</p><p>This increasing use of deep learning is creating in",0
"ned in two categories, depending on whether DNNs are trained in a supervised or unsupervised manner <ref type=""bibr"" target=""#b27"">[29]</ref>. Supervised training leads to models that map unseen sampl <div xmlns=""http://www.tei-c.org/ns/1.0""><head>Increasing complexity</head><p>Decreasing knowledge <ref type=""bibr"" target=""#b27"">[29]</ref> ADVERSARIAL GOALS ADVERSARIAL CAPABILITIES Fig. <ref type=",0
"rocessing of complex data in many domains such as vision <ref type=""bibr"" target=""#b23"">[24]</ref>, <ref type=""bibr"" target=""#b35"">[37]</ref>, speech recognition <ref type=""bibr"" target=""#b14"">[15]</r",0
"echniques for graph-structured inputs. Our starting point is previous work on Graph Neural Networks <ref type=""bibr"" target=""#b24"">(Scarselli et al., 2009)</ref>, which we modify to use gated recurren ucing a sequence of outputs. Here, (1) is mostly achieved by previous work on Graph Neural Networks <ref type=""bibr"" target=""#b24"">(Scarselli et al., 2009)</ref>; we make several minor adaptations of on graphs, including Graph Neural Networks <ref type=""bibr"" target=""#b10"">(Gori et al., 2005;</ref><ref type=""bibr"" target=""#b24"">Scarselli et al., 2009)</ref>, spectral networks <ref type=""bibr"" tar ion, we review Graph Neural Networks (GNNs) <ref type=""bibr"" target=""#b10"">(Gori et al., 2005;</ref><ref type=""bibr"" target=""#b24"">Scarselli et al., 2009)</ref> and introduce notation and concepts tha irected edge v → v , but we note that the framework can easily be adapted to undirected graphs; see <ref type=""bibr"" target=""#b24"">Scarselli et al. (2009)</ref>. The node vector (or node representatio v = f * (l v , l CO(v) , l NBR(v) , h (t−1) NBR(v)</formula><p>). Several variants are discussed in <ref type=""bibr"" target=""#b24"">Scarselli et al. (2009)</ref> including positional graph forms, node- l graph forms, node-specific updates, and alternative representations of neighborhoods. Concretely, <ref type=""bibr"" target=""#b24"">Scarselli et al. (2009)</ref> suggest decomposing f * (•) to be a sum unction g(h v , l v ) that maps to an output. This is generally a linear or neural network mapping. <ref type=""bibr"" target=""#b24"">Scarselli et al. (2009)</ref> focus on outputs that are independent p r"" target=""#b10"">(Gori et al., 2005;</ref><ref type=""bibr"" target=""#b6"">Di Massa et al., 2006;</ref><ref type=""bibr"" target=""#b24"">Scarselli et al., 2009;</ref><ref type=""bibr"">Uwents et al., 2011)</r",1
"outputs in the next section. The biggest modification of GNNs is that we use Gated Recurrent Units <ref type=""bibr"" target=""#b5"">(Cho et al., 2014)</ref> and unroll the recurrence for a fixed number",1
"re problem is to find mathematical descriptions of the data structures used in a program. Following <ref type=""bibr"" target=""#b3"">Brockschmidt et al. (2015)</ref>, we have phrased this as a machine le ate of memory, to a logical description of the data structures that have been instantiated. Whereas <ref type=""bibr"" target=""#b3"">Brockschmidt et al. (2015)</ref> relied on a large amount of hand-engi >A more complex scenario allowing for nested data structures (e.g., list of lists) was discussed in <ref type=""bibr"" target=""#b3"">Brockschmidt et al. (2015)</ref>. We have also successfully extended t ing for exact equality.</p><p>We compared our GGS-NN-based model with a method we developed earlier <ref type=""bibr"" target=""#b3"">(Brockschmidt et al., 2015)</ref>. The earlier approach treats each pr",0
"e operations on graphs, building a learnable, differentiable variant of a successful graph feature. <ref type=""bibr"" target=""#b17"">Lusci et al. (2013)</ref> converts an arbitrary undirected graph to a",0
"ng a different dimension.</p><p>GGS-NNs are related to soft alignment and attentional models (e.g., <ref type=""bibr"" target=""#b1"">Bahdanau et al. (2014)</ref>; <ref type=""bibr"" target=""#b16"">Kumar et",0
"earning is done via the Almeida-Pineda algorithm <ref type=""bibr"" target=""#b0"">(Almeida, 1990;</ref><ref type=""bibr"" target=""#b21"">Pineda, 1987)</ref>, which works by running the propagation to conver",0
"ww.tei-c.org/ns/1.0""><head>Test results appear in</head><p>The results for this task are given in   <ref type=""bibr"" target=""#b23"">Reynolds, 2002)</ref>, which uses inductive predicates to describe ab",0
"as node-level regression or classification.</p><p>Learning is done via the Almeida-Pineda algorithm <ref type=""bibr"" target=""#b0"">(Almeida, 1990;</ref><ref type=""bibr"" target=""#b21"">Pineda, 1987)</ref",0
"idze et al., 2011)</ref>, and methods that define graph features in terms of random walks on graphs <ref type=""bibr"" target=""#b20"">(Perozzi et al., 2014)</ref>. More closely related to our goal in thi raph-structured inputs, but we are not aware of work that learns the kernels and outputs sequences. <ref type=""bibr"" target=""#b20"">Perozzi et al. (2014)</ref> convert graphs into sequences by followin",0
"k variant.</p><p>An analogy can be drawn between our adaptation from GNNs to GG-NNs, to the work of <ref type=""bibr"" target=""#b7"">Domke (2011) and</ref><ref type=""bibr"">Stoyanov et al. (2011)</ref> in",0
"amounts of supervision. However, even systems that have relied extensively on unsupervised features <ref type=""bibr"" target=""#b7"">(Collobert et al., 2011;</ref><ref type=""bibr"" target=""#b36"">Turian et s, and present a hybrid tagging architecture. This architecture is similar to the ones presented by <ref type=""bibr"" target=""#b7"">Collobert et al. (2011)</ref> and <ref type=""bibr"" target=""#b19"">Huang ></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.2"">Pretrained embeddings</head><p>As in <ref type=""bibr"" target=""#b7"">Collobert et al. (2011)</ref>, we use pretrained word embeddings to in g of our    Several other neural architectures have previously been proposed for NER. For instance, <ref type=""bibr"" target=""#b7"">Collobert et al. (2011)</ref> uses a CNN over a sequence of word embed",1
"is similar to the ones presented by <ref type=""bibr"" target=""#b7"">Collobert et al. (2011)</ref> and <ref type=""bibr"" target=""#b19"">Huang et al. (2015)</ref>.</p></div> <div xmlns=""http://www.tei-c.org character-level embeddings and with the bidirectional LSTM being replaced by a CNN. More recently, <ref type=""bibr"" target=""#b19"">Huang et al. (2015)</ref> presented a model similar to our LSTM-CRF,",1
"nd gazetteers.</p><p>Language independent NER models like ours have also been proposed in the past. <ref type=""bibr"" target=""#b8"">Cucerzan and Yarowsky (1999;</ref><ref type=""bibr"">2002)</ref> present",0
"002</ref><ref type=""bibr"">and CoNLL-2003</ref><ref type=""bibr"">datasets (Tjong Kim Sang, 2002;</ref><ref type=""bibr"" target=""#b35"">Tjong Kim Sang and De Meulder, 2003)</ref> that contain independent n",0
"me is only capable of determining that the subsequent word cannot be the interior of another label. <ref type=""bibr"" target=""#b33"">Ratinov and Roth (2009)</ref> and <ref type=""bibr"" target=""#b10"">Dai ayesian nonparametrics to construct a database of named entities in an almost unsupervised setting. <ref type=""bibr"" target=""#b33"">Ratinov and Roth (2009)</ref> quantitatively compare several approach",0
"e convolutional networks-have been proposed to learn representations of words from their characters <ref type=""bibr"" target=""#b39"">(Zhang et al., 2015;</ref><ref type=""bibr"" target=""#b20"">Kim et al.,",0
"orms all previous models that do not incorporate external features, apart from the one presented by <ref type=""bibr"" target=""#b6"">Chiu and Nichols (2015)</ref>.</p><p>Tables <ref type=""table"" target="" sequence learning problem and incorporate character-based representations into their encoder model. <ref type=""bibr"" target=""#b6"">Chiu and Nichols (2015)</ref> employ an architecture similar to ours,",0
"he performance of SGD, such as Adadelta <ref type=""bibr"" target=""#b37"">(Zeiler, 2012)</ref> or Adam <ref type=""bibr"" target=""#b21"">(Kingma and Ba, 2014)</ref>. Although we observe faster convergence u",0
"target=""#b7"">(Collobert et al., 2011;</ref><ref type=""bibr"" target=""#b36"">Turian et al., 2010;</ref><ref type=""bibr"" target=""#b23"">Lin and Wu, 2009;</ref><ref type=""bibr"">Ando and Zhang, 2005b, inter ou and Xu (2015)</ref> also used a similar model and adapted it to the semantic role labeling task. <ref type=""bibr"" target=""#b23"">Lin and Wu (2009)</ref> used a linear chain CRF with L 2 regularizati",0
"group consisting of time-and vocal tract length shifts for an application to speech recognition by <ref type=""bibr"" target=""#b42"">Zhang et al. (2015)</ref>.</p></div> <div xmlns=""http://www.tei-c.org",0
"with rectified linear activation units, and nothing else. Our second baseline is a residual network <ref type=""bibr"" target=""#b13"">(He et al., 2016)</ref>, which consists of an initial convolution lay",0
"ef type=""bibr"" target=""#b2"">Baydin &amp; Pearlmutter (2014)</ref>, and applied to small problems by <ref type=""bibr"" target=""#b12"">Domke (2012)</ref>. However, the naïve approach fails for real-sized s=""http://www.tei-c.org/ns/1.0""><head n=""5."">Related work</head><p>The most closely-related work is <ref type=""bibr"" target=""#b12"">Domke (2012)</ref>, who derived algorithms to compute reverse-mode de",1
"eep learning entirely infeasible.</p><p>Applying RMD to hyperparameter optimization was proposed by <ref type=""bibr"" target=""#b3"">Bengio (2000)</ref> and <ref type=""bibr"" target=""#b2"">Baydin &amp; Pea , Eigenmann &amp; Nossek (1999)</ref>, <ref type=""bibr"" target=""#b9"">Chen &amp; Hagan (1999)</ref>, <ref type=""bibr"" target=""#b3"">Bengio (2000)</ref>, <ref type=""bibr"" target=""#b0"">Abdel-Gawad &amp; R",1
"e can compute exact gradients of any type of hyperparameter, whether or not learning has converged. <ref type=""bibr"" target=""#b8"">Chapelle et al. (2002)</ref> introduced a differentiable bound on the",0
"ckage for Python, available at github.com/HIPS/autograd. This package differentiates standard Numpy <ref type=""bibr"" target=""#b28"">(Oliphant, 2007)</ref> code, and can differentiate code containing wh",0
"learning rate schedules, or set their shape using one or two hyperparameters set by crossvalidation <ref type=""bibr"" target=""#b11"">(Dahl et al., 2014;</ref><ref type=""bibr"" target=""#b36"">Sutskever et",0
"dients with respect to training parameters.</p><p>Gradients with respect to Markov chain parameters <ref type=""bibr"" target=""#b33"">Salimans et al. (2014)</ref> tune the step-size and mass-matrix param",0
"is gradient-free model-based optimization <ref type=""bibr"" target=""#b34"">(Snoek et al., 2012;</ref><ref type=""bibr"" target=""#b6"">Bergstra et al., 2011;</ref><ref type=""bibr"">2013;</ref><ref type=""bib",0
"., 2012;</ref><ref type=""bibr"" target=""#b6"">Bergstra et al., 2011;</ref><ref type=""bibr"">2013;</ref><ref type=""bibr"" target=""#b20"">Hutter et al., 2011)</ref>. Hyperparameters are chosen to optimize th",0
"When the input distribution to a learning system changes, it is said to experience covariate shift <ref type=""bibr"" target=""#b17"">(Shimodaira, 2000)</ref>. This is typically handled via domain adapta",1
"by changing the parameters of the optimization algorithm to depend on the network activation values <ref type=""bibr"" target=""#b22"">(Wiesler et al., 2014;</ref><ref type=""bibr"" target=""#b14"">Raiko et a",1
"type=""bibr"" target=""#b17"">(Shimodaira, 2000)</ref>. This is typically handled via domain adaptation <ref type=""bibr"" target=""#b7"">(Jiang, 2008)</ref>. However, the notion of covariate shift can be ext",0
"et classification</head><p>We applied Batch Normalization to a new variant of the Inception network <ref type=""bibr"" target=""#b20"">(Szegedy et al., 2014)</ref>, trained on the Im-ageNet classification Convolutional layers use ReLU as the nonlinearity. The main difference to the network described in <ref type=""bibr"" target=""#b20"">(Szegedy et al., 2014)</ref> is that the 5 × 5 convolutional layers a edicted by the constituent networks. The details of ensemble and multicrop inference are similar to <ref type=""bibr"" target=""#b20"">(Szegedy et al., 2014)</ref>.</p><p>We demonstrate in Fig. <ref type=",0
"tion in the rest of the text. The training was performed on a large-scale, distributed architecture <ref type=""bibr"" target=""#b1"">(Dean et al., 2012)</ref>, using 5 concurrent steps on each of 10 mode",0
"lization to combat it, we considered the problem of predicting the digit class on the MNIST dataset <ref type=""bibr"" target=""#b8"">(LeCun et al., 1998a)</ref>. We used a very simple network, with a 28x",0
"divergence. Furthermore, batch normalization regularizes the model and reduces the need for Dropout <ref type=""bibr"" target=""#b18"">(Srivastava et al., 2014)</ref>. Finally, Batch Normalization makes i g rate 6 times faster.</p><p>Remove Local Response Normalization While Inception and other networks <ref type=""bibr"" target=""#b18"">(Srivastava et al., 2014)</ref> benefit from it, we found that with B",0
"variants such as momentum <ref type=""bibr"" target=""#b19"">(Sutskever et al., 2013)</ref> and Adagrad <ref type=""bibr"" target=""#b3"">(Duchi et al., 2011)</ref> have been used to achieve state of the art astic Gradient Descent with a mini-batch size m &gt; 1, or with any of its variants such as Adagrad <ref type=""bibr"" target=""#b3"">(Duchi et al., 2011)</ref>. The normalization of activations that depe",0
"= max(x, 0), careful initialization <ref type=""bibr"" target=""#b0"">(Bengio &amp; Glorot, 2010;</ref><ref type=""bibr"" target=""#b16"">Saxe et al., 2013)</ref>, and small learning rates. If, however, we c he layer Jacobians to have singular values close to 1, which is known to be beneficial for training <ref type=""bibr"" target=""#b16"">(Saxe et al., 2013)</ref>. Consider two consecutive layers with norma",0
"traditional models <ref type=""bibr"" target=""#b23"">(Wu et al., 2015)</ref> and the ensemble model of <ref type=""bibr"" target=""#b5"">(He et al., 2015)</ref>. The latter reports the top-5 error of 4.94%,",0
"cantly improved image classification <ref type=""bibr"" target=""#b13"">[14]</ref> and object detection <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b18"">19]</ref> accuracy. Compared t task that requires more complex methods to solve. Due to this complexity, current approaches (e.g., <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" targ n this paper, we streamline the training process for stateof-the-art ConvNet-based object detectors <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b10"">11]</ref>. We propose a single egressors, rather than training a softmax classifier, SVMs, and regressors in three separate stages <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b10"">11]</ref>. The components of t very deep detection network (VGG16 <ref type=""bibr"" target=""#b19"">[20]</ref>) 9× faster than R-CNN <ref type=""bibr"" target=""#b8"">[9]</ref> and 3× faster than SPPnet <ref type=""bibr"" target=""#b10"">[11 1.0""><head n=""1.1."">R-CNN and SPPnet</head><p>The Region-based Convolutional Network method (R-CNN) <ref type=""bibr"" target=""#b8"">[9]</ref> achieves excellent object detection accuracy by using a deep k h , for each of the K object classes, indexed by k. We use the parameterization for t k given in <ref type=""bibr"" target=""#b8"">[9]</ref>, in which t k specifies a scale-invariant translation and lo eparates localization and classification. OverFeat <ref type=""bibr"" target=""#b18"">[19]</ref>, R-CNN <ref type=""bibr"" target=""#b8"">[9]</ref>, and SPPnet <ref type=""bibr"" target=""#b10"">[11]</ref> also t tions of the dataset). We use mini-batches of size R = 128, sampling 64 RoIs from each image. As in <ref type=""bibr"" target=""#b8"">[9]</ref>, we take 25% of the RoIs from object proposals that have int rm non-maximum suppression independently for each class using the algorithm and settings from R-CNN <ref type=""bibr"" target=""#b8"">[9]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n= he first is the CaffeNet (essentially AlexNet <ref type=""bibr"" target=""#b13"">[14]</ref>) from R-CNN <ref type=""bibr"" target=""#b8"">[9]</ref>. We alternatively refer to this CaffeNet as model S, for ""sm",1
"g the experiment. The state-of-the-art for measuring object proposal quality is Average Recall (AR) <ref type=""bibr"" target=""#b11"">[12]</ref>. AR correlates well with mAP for several proposal methods",0
".6."">Preliminary MS COCO results</head><p>We applied Fast R-CNN (with VGG16) to the MS COCO dataset <ref type=""bibr"" target=""#b17"">[18]</ref> to establish a preliminary baseline. We trained on the 80k",0
"mprove results because the tasks influence each other through a shared representation (the ConvNet) <ref type=""bibr"" target=""#b1"">[2]</ref>. Does multi-task training improve object detection accuracy",0
"ti-task loss L on each labeled RoI to jointly train for classification and bounding-box regression: <ref type=""bibr"" target=""#b0"">(1)</ref> in which L cls (p, u) = − log p u is log loss for true class",0
"benchmarks are an important tool to analyze specific aspects of computer systems. In previous work <ref type=""bibr"" target=""#b6"">[7]</ref>, <ref type=""bibr"" target=""#b7"">[8]</ref> we presented micro- t L3 slice or obtains a copy from another core's L1 or L2 cache as indicated by the core valid bits <ref type=""bibr"" target=""#b6"">[7]</ref>. In case of an L3 miss, the caching agent forwards the reque obenchmark Design</head><p>We use an extended version of the synthetic microbenchmarks presented in <ref type=""bibr"" target=""#b6"">[7]</ref>. They include data placement and coherence state control mec",1
"vers use point-to-point connections between the processors <ref type=""bibr"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b1"">[2]</ref>. The available memory bandwidth scales with the number of pr he distributed caches in Haswell-EP based systems are kept coherent using a snooping based protocol <ref type=""bibr"" target=""#b1"">[2]</ref>. Two snooping modes-source snooping and home snooping-are av > describe the directory assisted snoop broadcast protocol (DAS)-an extension of the MESIF protocol <ref type=""bibr"" target=""#b1"">[2]</ref>. The DAS protocol uses an in-memory directory to accelerate /1.0""><head>A. MESIF Implementation</head><p>Cache coherence is maintained using the MESIF protocol <ref type=""bibr"" target=""#b1"">[2]</ref>. It inherits the modified, exclusive, shared, and invalid st ei-c.org/ns/1.0""><head>B. Source Snoop Mode</head><p>In the source snoop mode of the MESIF protocol <ref type=""bibr"" target=""#b1"">[2]</ref>, snoops are sent by the caching agents. In case of an L3 mis /div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>C. Home Snoop Mode</head><p>The MESIF protocol <ref type=""bibr"" target=""#b1"">[2]</ref> also supports a home snoop mode in which snoops are sent by rwarded by the other node 8 as expected according to the state specifications in the MESIF protocol <ref type=""bibr"" target=""#b1"">[2]</ref>. For 2.5 MiB 5 The behavior doesn't change if hardware prefe",0
"roadcast protocol"" stores 2-bit of directory information for each cache line in the memory ECC bits <ref type=""bibr"" target=""#b16"">[17</ref>  that a potentially modified copy exists in another node. T",0
"inimal group miss ratio) and required the assumption that the individual miss ratio curve be convex <ref type=""bibr"" target=""#b8"">[9]</ref>. Our algorithm uses dynamic programming to examine the entir he cache between instructions and data, and for multiprogramming by giving each process a partition <ref type=""bibr"" target=""#b8"">[9]</ref>.</p><p>In the way of optimizing their algorithm, they proved ss-rate derivative. The allocation is optimal if the miss-rate derivatives are as equal as possible <ref type=""bibr"" target=""#b8"">[9]</ref>. The optimality depends on several assumptions. One is that",1
"r for multiple objectives, e.g., both throughput and fairness in elastic cache utility optimization <ref type=""bibr"" target=""#b17"">[18]</ref>.</p><p>Concurrent Reuse Distance: Simulation of shared cac",0
"lement (inclusive) of the trace and continuing for k elements is denoted window (i, k). The windows <ref type=""bibr"" target=""#b1"">(2,</ref><ref type=""bibr"" target=""#b5"">6)</ref> and (4, 2) are boxed i s in shared cache <ref type=""bibr"" target=""#b11"">[12]</ref>, optimal memory allocation in Memcached <ref type=""bibr"" target=""#b1"">[2]</ref>, and a study from the OS community on server cache performan cache <ref type=""bibr"" target=""#b11"">[12]</ref>, Hu et al. on keyvalue access traces for Memcached <ref type=""bibr"" target=""#b1"">[2]</ref>, and Wires et al.</p><p>on disk access traces for server cac state workload, and found that optimal partition converges 4 times faster than free-for-all sharing <ref type=""bibr"" target=""#b1"">[2]</ref>. Finally, Wang et al. showed strong correlation (coefficient",0
"aluated and validated for solo-use cache, including the two initial studies of the footprint theory <ref type=""bibr"" target=""#b14"">[15]</ref>, <ref type=""bibr"" target=""#b15"">[16]</ref>. Independent va",0
"h, Natural is the same as the capitalist cache sharing, and Equal the same as the socialist sharing <ref type=""bibr"" target=""#b16"">[17]</ref>. The capitalist miss ratio depends on peers (market condit e space in equal partitions than in natural partitions.</p><p>To use the terminology of Xie and Loh <ref type=""bibr"" target=""#b16"">[17]</ref>, there is more resource under utilization in the socialist",0
"e proposed method naturally extends our previous work of unsupervised information network embedding <ref type=""bibr"" target=""#b26"">[27]</ref> and first learns a low dimensional embedding for words thr e network is embedded into a low dimensional vector space that preserves the second-order proximity <ref type=""bibr"" target=""#b26"">[27]</ref> between the vertices in the network. The representation of is suitable for arbitrary types of information networks: undirected or directed, binary or weighted <ref type=""bibr"" target=""#b26"">[27]</ref>. The LINE model optimizes an objective function which aims vious work, we introduced the LINE model to learn the embedding of large-scale information networks <ref type=""bibr"" target=""#b26"">[27]</ref>. LINE is mainly designed for homogeneous networks, i.e., n l for embedding bipartite networks. The essential idea is to make use of the second-order proximity <ref type=""bibr"" target=""#b26"">[27]</ref> between vertices, which assumes vertices with similar neig jective (3) can be optimized with stochastic gradient descent using the techniques of edge sampling <ref type=""bibr"" target=""#b26"">[27]</ref> and negative sampling <ref type=""bibr"" target=""#b17"">[18]< descent in learning network embeddings. For the detailed optimization process, readers can refer to <ref type=""bibr"" target=""#b26"">[27]</ref>.</p><p>The embeddings of the word-word, word-document, and (4) is to merge the all the edges in the three sets Eww, E wd , E wl and then deploy edge sampling <ref type=""bibr"" target=""#b26"">[27]</ref>, which samples an edge for model updating in each step, wi 0]</ref>.</p><p>• LINE: the large-scale information network embedding model proposed by Tang et al. <ref type=""bibr"" target=""#b26"">[27]</ref>. We use the LINE model to learn unsupervised embeddings wi",1
"data set, our results are different from those reported in <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b15"">16]</ref>. This is because their embeddings are trained on the mixtur",0
"ween different words are commonly ignored.</p><p>Distributed representations of words and documents <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b9"">10]</ref> effectively address "">[7]</ref>. Mikilov et al. proposed a simple and elegant word embedding model called the Skip-gram <ref type=""bibr"" target=""#b17"">[18]</ref>, which uses the embedding of the target word to predict th hes have been proved to be quite efficient, scaling up to millions of documents on a single machine <ref type=""bibr"" target=""#b17"">[18]</ref>.</p><p>Because of the unsupervised learning process, the r gs of words and/or documents by utilizing word co-occurrences in the local context (e.g., Skip-gram <ref type=""bibr"" target=""#b17"">[18]</ref>) or at document level (e.g., paragraph vectors <ref type="" ing the techniques of edge sampling <ref type=""bibr"" target=""#b26"">[27]</ref> and negative sampling <ref type=""bibr"" target=""#b17"">[18]</ref>. In each step, a binary edge e = (i, j) is sampled with th [21]</ref>.</p><p>• Skip-gram: the state-of-the-art word embedding model proposed by Mikolov et al. <ref type=""bibr"" target=""#b17"">[18]</ref>. For the document embedding, we simply take the average of",0
"rk. This is inspired by the idea of pre-training and fine-tuning in the literature of deep learning <ref type=""bibr"" target=""#b1"">[2]</ref>.</p><p>In joint training, all three types of networks are us",0
"ssical text representations such as statistical topic models, e.g., the latent Dirichlet allocation <ref type=""bibr"" target=""#b3"">[4]</ref>. To capture the document-level word cooccurrences, we introd",0
""">Rosas et al., 2014)</ref>, which was then used to develop a multimodal deception detection system <ref type=""bibr"" target=""#b1"">(Abouelenien et al., 2014)</ref>. An extensive review of approaches fo",1
"isplays and hand movements, as they have been previously found to correlate with deceptive behavior <ref type=""bibr"" target=""#b8"">(Depaulo et al., 2003)</ref>. The gesture annotation is performed usin es is motivated by previous research that has suggested that deceivers' speech has lower complexity <ref type=""bibr"" target=""#b8"">(Depaulo et al., 2003)</ref>. We use the tool described in <ref type="" or gaze) and nod (Side-Turn-R) more frequently than truth-tellers. This agrees with the findings in <ref type=""bibr"" target=""#b8"">(Depaulo et al., 2003)</ref> that liars who are more motivated to get",1
"b8"">(Depaulo et al., 2003)</ref>. The gesture annotation is performed using the MUMIN coding scheme <ref type=""bibr"" target=""#b2"">(Allwood et al., 2007)</ref>.</p><p>In the MUMIN scheme, facial displa",1
"ies previous findings where human ability to spot liars was found to be slightly better than chance <ref type=""bibr"" target=""#b0"">(Aamodt and Custer, 2006)</ref>. Moreover, the performance of the huma",0
"12)</ref> indicated that relying solely on physiological measurements can be biased and misleading. <ref type=""bibr"" target=""#b6"">Chittaranjan et al. (Chittaranjan and Hung, 2010</ref>) created an aud",0
"ents to further relate them to an act of deceit <ref type=""bibr"" target=""#b11"">(Ekman, 2003)</ref>. <ref type=""bibr"" target=""#b36"">Tian et al. (Tian et al., 2005)</ref> considered features such as fac",0
"ere analyzed using smoothness and asymmetry measurements to further relate them to an act of deceit <ref type=""bibr"" target=""#b11"">(Ekman, 2003)</ref>. <ref type=""bibr"" target=""#b36"">Tian et al. (Tian",0
"r large-scale applications. Moreover, polygraph tests were shown to be misleading in multiple cases <ref type=""bibr"" target=""#b39"">(Vrij, 2001;</ref><ref type=""bibr"" target=""#b13"">Gannon et al., 2009) such physiological features such as heart rate, respiration rate, skin temperature. Several studies <ref type=""bibr"" target=""#b39"">(Vrij, 2001;</ref><ref type=""bibr"" target=""#b13"">Gannon et al., 2009;",0
""">Tsechpenakis et al., 2005)</ref>, or using geometric features related to the hand and head motion <ref type=""bibr"" target=""#b24"">(Meservy et al., 2005)</ref>. Caso et al. <ref type=""bibr"" target=""#b",0
"syntactic CFG trees and part of speech tags <ref type=""bibr"" target=""#b12"">(Feng et al., 2012;</ref><ref type=""bibr"" target=""#b42"">Xu and Zhao, 2012)</ref>. Research work has also relied on the LIWC l",0
"nd gestures, blob analysis was used to detect deceit by tracking the hand movements of the subjects <ref type=""bibr"" target=""#b21"">(Lu et al., 2005;</ref><ref type=""bibr"" target=""#b38"">Tsechpenakis et",0
"e also analyze the contribution of the linguistic features. Using the linguistic ethnography method <ref type=""bibr"" target=""#b26"">(Mihalcea and Pulman, 2009)</ref>, we obtain the most dominant LIWC w",0
"ype=""bibr"" target=""#b39"">(Vrij, 2001;</ref><ref type=""bibr"" target=""#b13"">Gannon et al., 2009;</ref><ref type=""bibr"" target=""#b9"">Derksen, 2012)</ref> indicated that relying solely on physiological me",0
"ests were shown to be misleading in multiple cases <ref type=""bibr"" target=""#b39"">(Vrij, 2001;</ref><ref type=""bibr"" target=""#b13"">Gannon et al., 2009)</ref>, as human judgment is often biased.</p><p> espiration rate, skin temperature. Several studies <ref type=""bibr"" target=""#b39"">(Vrij, 2001;</ref><ref type=""bibr"" target=""#b13"">Gannon et al., 2009;</ref><ref type=""bibr"" target=""#b9"">Derksen, 2012",0
"p://www.tei-c.org/ns/1.0""><head n=""1"">Introduction</head><p>Knowledge bases (KBs), such as Freebase <ref type=""bibr"" target=""#b1"">(Bollacker et al., 2008)</ref>, NELL <ref type=""bibr"" target=""#b19"">(M",1
"relations in the tensor) has high rank, learning good embeddings can be challenging. The ARE model <ref type=""bibr"" target=""#b22"">(Nickel et al., 2014)</ref> attempted to address this by only making",0
"tions to infer missing relationships. Some of earliest work along these lines were the RESCAL model <ref type=""bibr"" target=""#b21"">(Nickel et al., 2011)</ref> and Structured Embeddings <ref type=""bibr",0
"s/1.0""><head n=""2"">The Path Ranking Algorithm</head><p>The path ranking algorithm was introduced by <ref type=""bibr"" target=""#b13"">Lao and Cohen (2010)</ref>. It is a two-step process for generating a",0
"s of facts about common or popular entities <ref type=""bibr"" target=""#b30"">(West et al., 2014;</ref><ref type=""bibr"" target=""#b5"">Choi et al., 2015)</ref>. The task of knowledge base completion-fillin",0
"icit partitioning of the cache among co-running applications for throughput or fairness improvement <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" ta",1
"task-core assignments <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b8"">9]</ref>. Tasks have much shorter lifetimes than threads, and there is",0
"the memory hierarchy that uses the Simics full-system simulator as its functional simulation engine <ref type=""bibr"" target=""#b13"">[14]</ref>. We model a multicore chip with a highlyassociative shared",0
"get=""#b18"">19,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" target=""#b31"">32,</ref><ref type=""bibr"" target=""#b35"">36]</ref>.</p><p>Shared LLC management techniques for multicore proce get=""#b17"">18,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b31"">32,</ref><ref type=""bibr"" target=""#b35"">36]</ref>. Proposals for replacement policy modification have concent load has been the focus of several partitioning techniques <ref type=""bibr"" target=""#b31"">[32,</ref><ref type=""bibr"" target=""#b35"">36]</ref>. A partitioning scheme that always allocates space to a thr eve that utility. The winning thread is then allocated the minimum number of ways.</p><p>Suh et al. <ref type=""bibr"" target=""#b35"">[36]</ref> compute the marginal utility of each thread from the actua",0
"arget=""#b7"">[8,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" target=""#b33"">34,</ref><ref type=""bibr"" target=""#b37"">38,</ref><ref type=""bibr"" tar ts of identifying and eliminating non-temporal accesses in a multicore multiprogramming environment <ref type=""bibr"" target=""#b33"">[34]</ref>. The authors develop a profile-based approach to identify",0
"ee ways. 1. Multipage mappings use one TLB entry to map multiple pages (e.g., 8-16 pages per entry) <ref type=""bibr"" target=""#b36"">[38,</ref><ref type=""bibr"" target=""#b37"">39,</ref><ref type=""bibr"" ta es, and thus the OS can only allocate them when the available memory is size-aligned and contiguous <ref type=""bibr"" target=""#b36"">[38,</ref><ref type=""bibr"" target=""#b37"">39]</ref>. In addition, many anslations for a variety of block sizes and exploits the clustering behavior of the buddy allocator <ref type=""bibr"" target=""#b36"">[38,</ref><ref type=""bibr"" target=""#b37"">39]</ref>.</p><p>Finally, ea =""bibr"" target=""#b45"">[47]</ref>, CoLT <ref type=""bibr"" target=""#b37"">[39]</ref> and Clustered TLBs <ref type=""bibr"" target=""#b36"">[38]</ref>, pack multiple Page  multiple of translations (e.g., 8-16) counters. (ii) We emulate multipage mappings in BadgerTrap. We implement the Clustered TLB approach <ref type=""bibr"" target=""#b36"">[38]</ref> of Pham et al., configured with 512 fullyassociative entri 2 fullyassociative entries. Each entry indexes up to an 8-page cluster, shown best by Clustered TLB <ref type=""bibr"" target=""#b36"">[38]</ref>. We use eager paging to increase the opportunities to form lable on x86-64 processors. All other configurations are emulated. The CTLB bars show Clustered TLB <ref type=""bibr"" target=""#b36"">[38]</ref> results. The DS bars show direct segments <ref type=""bibr"" ype=""bibr"" target=""#b45"">[47]</ref>, CoLT <ref type=""bibr"" target=""#b37"">[39]</ref>, Clustered TLBs <ref type=""bibr"" target=""#b36"">[38]</ref>), huge pages <ref type=""bibr"" target=""#b0"">[1,</ref><ref t",1
"ecution-time overheads of up to 50% due to page table walks <ref type=""bibr"" target=""#b8"">[10,</ref><ref type=""bibr"" target=""#b10"">12,</ref><ref type=""bibr"" target=""#b29"">31]</ref>. This overhead is l b20"">[22]</ref>the same methodology as in prior TLB studies <ref type=""bibr"" target=""#b8"">[10,</ref><ref type=""bibr"" target=""#b10"">12,</ref><ref type=""bibr"" target=""#b21"">23]</ref>. We compare RMM to al memory system performance studies use this same approach <ref type=""bibr"" target=""#b8"">[10,</ref><ref type=""bibr"" target=""#b10"">12,</ref><ref type=""bibr"" target=""#b21"">23]</ref>.</p><p>BadgerTrap i skipping one or more memory references during the page-walk <ref type=""bibr"" target=""#b6"">[8,</ref><ref type=""bibr"" target=""#b10"">12,</ref><ref type=""bibr"" target=""#b25"">27]</ref>. RMM is orthogonal",0
"support more sizes <ref type=""bibr"" target=""#b33"">[35,</ref><ref type=""bibr"" target=""#b39"">41,</ref><ref type=""bibr"" target=""#b42"">44]</ref>. The effectiveness of huge pages is limited by the size-ali",0
"<ref type=""bibr"" target=""#b23"">[25]</ref>, BioBench <ref type=""bibr"" target=""#b5"">[7]</ref>, Parsec <ref type=""bibr"" target=""#b13"">[15]</ref> and big-memory workloads <ref type=""bibr"" target=""#b8"">[10",0
"6-64 architectures. Use of huge pages (THP <ref type=""bibr"" target=""#b4"">[6]</ref> and libhugetlbfs <ref type=""bibr"" target=""#b0"">[1]</ref>) increase TLB reach substantially, but also suffer from size e Pages using Transparent Huge Pages (THP) <ref type=""bibr"" target=""#b4"">[6]</ref> and libhugetlbfs <ref type=""bibr"" target=""#b0"">[1]</ref> increase the TLB reach by mapping very large regions with a iques. The 4 KB, 2 MB Transparent Huge Pages (THP) <ref type=""bibr"" target=""#b4"">[6]</ref> and 1 GB <ref type=""bibr"" target=""#b0"">[1]</ref> configurations show the measured overhead for the three diff br"" target=""#b37"">[39]</ref>, Clustered TLBs <ref type=""bibr"" target=""#b36"">[38]</ref>), huge pages <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b4"">6,</ref><ref type=""bibr"" target",0
""" target=""#b27"">[1]</ref>) or lack a clear objective function tailored for network embedding (e.g., <ref type=""bibr"" target=""#b42"">[16]</ref>). We anticipate that a new model with a carefully designed e for both undirected and directed graphs.</p><p>The most recent work related with ours is DeepWalk <ref type=""bibr"" target=""#b42"">[16]</ref>, which deploys a truncated random walk for social network /ref> . The Flickr network is denser than the Youtube network (the same network as used in DeepWalk <ref type=""bibr"" target=""#b42"">[16]</ref>). (3) Citation Networks. Two types of citation networks ar rk can be represented as an affinity matrix, and is able to represent each vertex with a • DeepWalk <ref type=""bibr"" target=""#b42"">[16]</ref>. DeepWalk is an approach recently proposed for social netw r"" target=""#b39"">[13]</ref>. For other networks, the dimension is set as 128 by default, as used in <ref type=""bibr"" target=""#b42"">[16]</ref>. Other default settings include: the number of negative sa",1
"]</ref>; and ""You shall know a word by the company it keeps"" (Firth, J. R. 1957:11) in text corpora <ref type=""bibr"" target=""#b31"">[5]</ref>. Indeed, people who share many common friends are likely to",0
"get=""#foot_0"">3</ref>  <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b12"">13]</ref>. They analyzed the relationship between internal properties bibr"" target=""#b4"">[5]</ref>, narcissism <ref type=""bibr"" target=""#b6"">[7]</ref>, self-presentation <ref type=""bibr"" target=""#b12"">[13]</ref> and the corresponding user profiles on Facebook.</p><p>We",1
"e real world but represent information sharing relationships <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b7"">8]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""",0
"has attracted considerable research interest in recent years <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b9"">10]</ref>, which are known as us heir introductory information or the content of their tweets <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref>, or based on the name, location, and sex of the user <ref typ",0
"ucted for Facebook<ref type=""foot"" target=""#foot_0"">3</ref>  <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b12"">13]</ref>. They analyzed the re n internal properties such as self-construction <ref type=""bibr"" target=""#b4"">[5]</ref>, narcissism <ref type=""bibr"" target=""#b6"">[7]</ref>, self-presentation <ref type=""bibr"" target=""#b12"">[13]</ref>",0
"based on profile images have been conducted for Facebook<ref type=""foot"" target=""#foot_0"">3</ref>  <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target 12"">13]</ref>. They analyzed the relationship between internal properties such as self-construction <ref type=""bibr"" target=""#b4"">[5]</ref>, narcissism <ref type=""bibr"" target=""#b6"">[7]</ref>, self-pr",0
"s by studying the two-month history made available by Amazon <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b13"">15]</ref>. Though these statistics alone are sufficient for the user are limited to statistical studies of historical spot prices <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b13"">15]</ref>.</p><p>Game theoretic pricing. Spot pricing is a distribute tradeoff is the fact that user jobs can have long runtimes spanning many changes in the spot price <ref type=""bibr"" target=""#b13"">[15]</ref>. Users then face two key challenges: 1) Users must predict int ensures that the job is sufficiently interruptible. We use p to denote the optimal bid price to <ref type=""bibr"" target=""#b13"">(15)</ref>.</p><p>We now observe that the expected running time in <r of the spot price monotonically decreases, i.e., F π (p) is concave, the optimal bid price solving <ref type=""bibr"" target=""#b13"">(15)</ref> </p><formula xml:id=""formula_29"">is p = ψ −1 t k t r − 1 , . Comparing <ref type=""bibr"" target=""#b17"">(19)</ref> to bidding for a single persistent request in <ref type=""bibr"" target=""#b13"">(15)</ref>, we see that <ref type=""bibr"" target=""#b17"">(19)</ref> can ""#b13"">(15)</ref>, we see that <ref type=""bibr"" target=""#b17"">(19)</ref> can be solved similarly to <ref type=""bibr"" target=""#b13"">(15)</ref> in Proposition 5.</p><p>By comparing the costs for multipl t k ts .</p><p>Proof of Proposition 5.</p><p>Proof. By taking the first-order derivative of Φ(p) in <ref type=""bibr"" target=""#b13"">(15)</ref>  <ref type=""figure"" target=""#fig_3"">3</ref>), F π (p) is c",1
"Usage-based pricing can affect overall demand levels, but does not even out short-term fluctuations <ref type=""bibr"" target=""#b11"">[13]</ref>. To manage these fluctuations in demand for a fixed amount re have a shorter expected running time. Job interruptibility. We can use the expected running time <ref type=""bibr"" target=""#b11"">(13)</ref> to observe the effect of the recovery time parameter, t r s feasible at any price.</p><p>The optimal bid price. We can now multiply the expected running time <ref type=""bibr"" target=""#b11"">(13)</ref> with the expected spot price <ref type=""bibr"" target=""#b7"" o <ref type=""bibr"" target=""#b13"">(15)</ref>.</p><p>We now observe that the expected running time in <ref type=""bibr"" target=""#b11"">(13)</ref> decreases with the bid price, while the expected spot pric very, execution, and overhead times. Hence, we can extend the result for a single persistent bid in <ref type=""bibr"" target=""#b11"">(13)</ref> as</p><formula xml:id=""formula_32"">M i=1 T i F π (p) = t s",1
"><p>While many works have considered the operational problem of scheduling jobs within a datacenter <ref type=""bibr"" target=""#b6"">[8,</ref><ref type=""bibr"" target=""#b12"">14,</ref><ref type=""bibr"" targ . Many works have considered resource allocation in the cloud from a purely operational perspective <ref type=""bibr"" target=""#b6"">[8,</ref><ref type=""bibr"" target=""#b12"">14,</ref><ref type=""bibr"" targ",0
"get=""#b8"">[10,</ref><ref type=""bibr"" target=""#b15"">17,</ref><ref type=""bibr"" target=""#b32"">34,</ref><ref type=""bibr"" target=""#b36"">38]</ref>, we aim to develop both a reasonable model for how the prov <ref type=""bibr"" target=""#b32"">34]</ref> or social welfare <ref type=""bibr"" target=""#b23"">[25,</ref><ref type=""bibr"" target=""#b36"">38]</ref>. We construct a similar model but relate it to empirical bi the social welfare to be the provider's objective function <ref type=""bibr"" target=""#b23"">[25,</ref><ref type=""bibr"" target=""#b36"">38]</ref>. While our current formulation matches well with the observ objectives, such as clearing the market, are also possible <ref type=""bibr"" target=""#b8"">[10,</ref><ref type=""bibr"" target=""#b36"">38,</ref><ref type=""bibr"" target=""#b37"">39]</ref>; in fact, some stud",0
"k from other auctions for computing or utility resources, e.g., auctions for smart grid electricity <ref type=""bibr"" target=""#b7"">[9]</ref>, secondary spectrum access <ref type=""bibr"" target=""#b14"">[1 ly the expected running time <ref type=""bibr"" target=""#b11"">(13)</ref> with the expected spot price <ref type=""bibr"" target=""#b7"">(9)</ref> to find that the cost of a job with a persistent request is",0
"their costs by using spot rather than on-demand instances <ref type=""bibr"" target=""#b26"">[28,</ref><ref type=""bibr"" target=""#b35"">37]</ref>, they only consider heuristic bidding strategies for single",0
">27,</ref><ref type=""bibr"" target=""#b27"">29]</ref>, including improvements to Amazon's spot pricing <ref type=""bibr"" target=""#b33"">[35]</ref>. However, in cloud scenarios, user valuations for the inst",0
"bidding strategies for users. Joint userprovider interactions for cloud services are considered in <ref type=""bibr"" target=""#b24"">[26]</ref>, but auction-specific works on both provider and user acti",0
"me works have shown that users can reduce their costs by using spot rather than on-demand instances <ref type=""bibr"" target=""#b26"">[28,</ref><ref type=""bibr"" target=""#b35"">37]</ref>, they only conside",0
"duce more flexible pricing plans in which resources are priced according to real-time market demand <ref type=""bibr"" target=""#b38"">[40]</ref>. Amazon's Elastic Compute Cloud (EC2) spot pricing <ref ty",0
"procedures such as generating forced alignments and decision trees. Meanwhile, another line of work <ref type=""bibr"" target=""#b6"">[6,</ref><ref type=""bibr"" target=""#b7"">7,</ref><ref type=""bibr"" target ction to infer speech-label alignments automatically. This CTC technique is further investigated in <ref type=""bibr"" target=""#b6"">[6,</ref><ref type=""bibr"" target=""#b7"">7,</ref><ref type=""bibr"" target incorporate lexicons and language models into decoding. When decoding CTC-trained models, past work <ref type=""bibr"" target=""#b6"">[6,</ref><ref type=""bibr"" target=""#b8"">8,</ref><ref type=""bibr"" target enchmark show that Eesen results in superior performance than the existing end-to-end ASR pipelines <ref type=""bibr"" target=""#b6"">[6,</ref><ref type=""bibr"" target=""#b8"">8]</ref>. The WERs of Eesen are ained using frame-level labels with respect to the cross-entropy (CE) criterion. Instead, following <ref type=""bibr"" target=""#b6"">[6,</ref><ref type=""bibr"" target=""#b8"">8,</ref><ref type=""bibr"" target /1.0""><head n=""3.1."">Decoding with WFSTs</head><p>Previous work has introduced a variety of methods <ref type=""bibr"" target=""#b6"">[6,</ref><ref type=""bibr"" target=""#b8"">8,</ref><ref type=""bibr"" target ARPA format (which we will consistently refer to as standard). To be consistent with previous work <ref type=""bibr"" target=""#b6"">[6,</ref><ref type=""bibr"" target=""#b8"">8]</ref>, we report our results e"">3</ref> lists the results of end-to-end ASR systems that have been reported in the previous work <ref type=""bibr"" target=""#b6"">[6,</ref><ref type=""bibr"" target=""#b8"">8]</ref> and on the same datase ature differ not only in their model architectures but also in their decoding methods. For example, <ref type=""bibr"" target=""#b6"">[6]</ref> and <ref type=""bibr"" target=""#b8"">[8]</ref> adopt two distin "">[10]</ref> or achieve the integration under constrained conditions (e.g., nbest list rescoring in <ref type=""bibr"" target=""#b6"">[6]</ref>). In this work, we propose a generalized decoding approach b ed in decoding. When only the lexicon is used, our decoding behaves similarly as the beam search in <ref type=""bibr"" target=""#b6"">[6]</ref>. In this case, the WER rises quickly to 26.92%. This obvious dard language model, the character-based system gets the WER of 9.07%. CTC experiments in past work <ref type=""bibr"" target=""#b6"">[6]</ref> have adopted an expanded vocabulary, and re-trained the lang ref type=""bibr"" target=""#b8"">8]</ref> and on the same dataset. Our Eesen framework outperforms both <ref type=""bibr"" target=""#b6"">[6]</ref> and <ref type=""bibr"" target=""#b8"">[8]</ref> in terms of WERs ]</ref> in terms of WERs on the testing set. It is worth pointing out that the 8.7% WER reported in <ref type=""bibr"" target=""#b6"">[6]</ref> is obtained not in a purely end-to-end manner. Instead, the bibr"" target=""#b6"">[6]</ref> is obtained not in a purely end-to-end manner. Instead, the authors of <ref type=""bibr"" target=""#b6"">[6]</ref> generate a nbest list of hypotheses from a hybrid DNN model, e WERs of Eesen systems via more advanced learning techniques (e.g., expected transcription loss in <ref type=""bibr"" target=""#b6"">[6]</ref>) and alternative decoding approach (e.g., dynamic decoders <",1
""" target=""#b6"">[6,</ref><ref type=""bibr"" target=""#b7"">7,</ref><ref type=""bibr"" target=""#b8"">8,</ref><ref type=""bibr"" target=""#b9"">9,</ref><ref type=""bibr"" target=""#b10"">10,</ref><ref type=""bibr"" targe",1
"viewed as a sequence-to-sequence learning problem. We exploit deep recurrent neural networks (RNNs) <ref type=""bibr"" target=""#b16"">[15,</ref><ref type=""bibr"" target=""#b17"">16]</ref> as the acoustic mo",1
", another line of work <ref type=""bibr"" target=""#b6"">[6,</ref><ref type=""bibr"" target=""#b7"">7,</ref><ref type=""bibr"" target=""#b8"">8,</ref><ref type=""bibr"" target=""#b9"">9,</ref><ref type=""bibr"" target= urther investigated in <ref type=""bibr"" target=""#b6"">[6,</ref><ref type=""bibr"" target=""#b7"">7,</ref><ref type=""bibr"" target=""#b8"">8,</ref><ref type=""bibr"" target=""#b14"">14]</ref> on large-scale acoust s into decoding. When decoding CTC-trained models, past work <ref type=""bibr"" target=""#b6"">[6,</ref><ref type=""bibr"" target=""#b8"">8,</ref><ref type=""bibr"" target=""#b10"">10]</ref> has successfully cons pect to the cross-entropy (CE) criterion. Instead, following <ref type=""bibr"" target=""#b6"">[6,</ref><ref type=""bibr"" target=""#b8"">8,</ref><ref type=""bibr"" target=""#b10"">10]</ref>, we adopt the CTC obj s</head><p>Previous work has introduced a variety of methods <ref type=""bibr"" target=""#b6"">[6,</ref><ref type=""bibr"" target=""#b8"">8,</ref><ref type=""bibr"" target=""#b10"">10]</ref> to decode CTC-trained ctures but also in their decoding methods. For example, <ref type=""bibr"" target=""#b6"">[6]</ref> and <ref type=""bibr"" target=""#b8"">[8]</ref> adopt two distinct versions of beam search for decoding CTC the same dataset. Our Eesen framework outperforms both <ref type=""bibr"" target=""#b6"">[6]</ref> and <ref type=""bibr"" target=""#b8"">[8]</ref> in terms of WERs on the testing set. It is worth pointing ou erior performance than the existing end-to-end ASR pipelines <ref type=""bibr"" target=""#b6"">[6,</ref><ref type=""bibr"" target=""#b8"">8]</ref>. The WERs of Eesen are on a par with strong hybrid HMM/DNN ba y refer to as standard). To be consistent with previous work <ref type=""bibr"" target=""#b6"">[6,</ref><ref type=""bibr"" target=""#b8"">8]</ref>, we report our results on the eval92 set. Our experimental se end ASR systems that have been reported in the previous work <ref type=""bibr"" target=""#b6"">[6,</ref><ref type=""bibr"" target=""#b8"">8]</ref> and on the same dataset. Our Eesen framework outperforms both",0
"mes, etc.) directly without any intermediate components (e.g., GMMs). On this aspect, Graves et al. <ref type=""bibr"" target=""#b13"">[13]</ref> introduce the connectionist temporal classification (CTC) ead><p>Acoustic models in Eesen are deep bidirectional RNNs trained with the CTC objective function <ref type=""bibr"" target=""#b13"">[13]</ref>. We describe the model structure in Section 2.1, and resta pe=""bibr"" target=""#b8"">8,</ref><ref type=""bibr"" target=""#b10"">10]</ref>, we adopt the CTC objective <ref type=""bibr"" target=""#b13"">[13]</ref> to automatically learn the alignments between speech frame he RNN outputs with label sequences, an intermediate representation, the CTC path, is introduced in <ref type=""bibr"" target=""#b13"">[13]</ref>. A CTC path p = (p 1 , ..., p T ) is a sequence of labels ason is that the softmax-layer outputs from a CTC-trained model display a highly peaky distribution <ref type=""bibr"" target=""#b13"">[13,</ref><ref type=""bibr"" target=""#b14"">14]</ref>. That is, a majori",0
"various languages <ref type=""bibr"" target=""#b33"">[32,</ref><ref type=""bibr"" target=""#b34"">33,</ref><ref type=""bibr"" target=""#b35"">34]</ref> and different types (e.g., noisy, far-field) of speech, and",0
"type=""bibr"" target=""#b32"">[31]</ref>). Also, we are interested to apply Eesen to various languages <ref type=""bibr"" target=""#b33"">[32,</ref><ref type=""bibr"" target=""#b34"">33,</ref><ref type=""bibr"" ta",0
"type=""bibr"" target=""#b37"">36]</ref> and adaptive training <ref type=""bibr"" target=""#b38"">[37,</ref><ref type=""bibr"" target=""#b39"">38]</ref> techniques for the CTC models.</p></div> <div xmlns=""http:/",0
"cture as the acoustic model. However, combing LSTMs with other network structures, e.g., time-delay <ref type=""bibr"" target=""#b24"">[23,</ref><ref type=""bibr"" target=""#b25"">24]</ref> or convolutional n",0
"lso, we are interested to apply Eesen to various languages <ref type=""bibr"" target=""#b33"">[32,</ref><ref type=""bibr"" target=""#b34"">33,</ref><ref type=""bibr"" target=""#b35"">34]</ref> and different types",0
"emory (LSTM) units <ref type=""bibr"" target=""#b18"">[17,</ref><ref type=""bibr"" target=""#b19"">18,</ref><ref type=""bibr"" target=""#b20"">19,</ref><ref type=""bibr"" target=""#b21"">20]</ref> as the RNN building "" target=""#b25"">24]</ref> or convolutional neural networks <ref type=""bibr"" target=""#b26"">[25,</ref><ref type=""bibr"" target=""#b20"">19]</ref>, is straightforward to achieve.</p></div> <div xmlns=""http:",0
"arget=""#b9"">9,</ref><ref type=""bibr"" target=""#b10"">10,</ref><ref type=""bibr"" target=""#b11"">11,</ref><ref type=""bibr"" target=""#b12"">12]</ref> has focused on end-to-end ASR, i.e., modeling the mapping b",0
"bing LSTMs with other network structures, e.g., time-delay <ref type=""bibr"" target=""#b24"">[23,</ref><ref type=""bibr"" target=""#b25"">24]</ref> or convolutional neural networks <ref type=""bibr"" target=""#",0
"we propose a generalized decoding approach based on WFSTs <ref type=""bibr"" target=""#b28"">[27,</ref><ref type=""bibr"" target=""#b29"">28]</ref>. A WFST is a finite-state acceptor (FSA) in which each tran erage any language models that can be converted into WFSTs. Following conventions in the literature <ref type=""bibr"" target=""#b29"">[28]</ref>, the language model WFST is denoted as G.</p><p>Lexicon. A hybrid HMM/DNN system. The hybrid system is constructed by following the standard Kaldi recipe ""s5"" <ref type=""bibr"" target=""#b29"">[28]</ref>. Inputs of the DNN model are 11 neighboring frames of filt",0
"modeling in Eesen cannot leverage speaker adapted front-ends. We will study new speaker adaptation <ref type=""bibr"" target=""#b36"">[35,</ref><ref type=""bibr"" target=""#b37"">36]</ref> and adaptive train",0
"ef>.</p><p>To address this disparity, researchers have designed heterogeneous multi-core processors <ref type=""bibr"" target=""#b2"">[2]</ref> in which an application is mapped to the most efficient core gher switching costs of coarse-grained heterogeneous systems <ref type=""bibr"" target=""#b1"">[1,</ref><ref type=""bibr"" target=""#b2"">2]</ref> enforce switching granularities of the order of milli-seconds s big</p></note> 			<note xmlns=""http://www.tei-c.org/ns/1.0"" place=""foot"" n=""2"" xml:id=""foot_1""><p><ref type=""bibr"" target=""#b2"">2</ref> OinO stands for an InO core appearing to be OoO</p></note>",1
"e for each trace and perfectly predictable control-flow. These results corroborate with recent work <ref type=""bibr"" target=""#b8"">[8]</ref> that credit the OoO's ability to create good static schedule",1
"hereby reducing the overall energy consumption. Prior work has proposed heterogeneous architectures <ref type=""bibr"" target=""#b4"">[4,</ref><ref type=""bibr"" target=""#b5"">5,</ref><ref type=""bibr"" target seconds. Novel architectures minimize migration costs by either sharing of structures between cores <ref type=""bibr"" target=""#b4"">[4,</ref><ref type=""bibr"" target=""#b7"">7]</ref> or reducing the distan performance loss as compared to execution on big. Our core architecture, based on a Composite Core <ref type=""bibr"" target=""#b4"">[4]</ref>, executes a single application thread, and includes both a b /www.tei-c.org/ns/1.0""><head n=""3.5"">Switching overheads</head><p>DynaMOS adopts the Composite Core <ref type=""bibr"" target=""#b4"">[4]</ref> architecture, which tightly couples the big and little cores ducing the distance between them using 3D technology <ref type=""bibr"" target=""#b6"">[6]</ref>. Cores <ref type=""bibr"" target=""#b4"">[4]</ref> shares access to L1 caches, TLBs, fetch unit and branch pred",0
"phase behavior in a subset of h64ref 's execution, a compute-intensive benchmark from SPECInt 2006 <ref type=""bibr"" target=""#b13"">[13]</ref> exhibiting predictable control and data-flow. Each point r k, there can be many traces that spawn from a common header PC. Its high branch misprediction rates <ref type=""bibr"" target=""#b13"">[13]</ref> indicate that these traces are not predictably repeatable.",0
"et=""#b10"">10]</ref> or on different, specialized pipelines <ref type=""bibr"" target=""#b11"">[11,</ref><ref type=""bibr"" target=""#b12"">12]</ref>.</p><p>We overcome several design challenges en route to en propose using two pipelines: one for trace generation and another for trace execution. Turboscalar <ref type=""bibr"" target=""#b12"">[12]</ref> has a thin cold pipeline that discovers ILP over a long in",0
"frequency of the core (DVFS) to improve the core's energy efficiency at the expense of performance <ref type=""bibr"" target=""#b29"">[29,</ref><ref type=""bibr"" target=""#b30"">30]</ref>. Recent work <ref",0
"mensions of heterogeneity. These include cores with different sets of microarchitectural parameters <ref type=""bibr"" target=""#b21"">[21,</ref><ref type=""bibr"" target=""#b22"">22]</ref>, specialized hardw",0
"cores with different sets of microarchitectural parameters <ref type=""bibr"" target=""#b21"">[21,</ref><ref type=""bibr"" target=""#b22"">22]</ref>, specialized hardware for specific codes <ref type=""bibr"" t",0
"=""#b21"">[21,</ref><ref type=""bibr"" target=""#b22"">22]</ref>, specialized hardware for specific codes <ref type=""bibr"" target=""#b23"">[23,</ref><ref type=""bibr"" target=""#b24"">24]</ref>, and heterogeneous",0
"primarily focused on intra-line compression to minimize decompression latency. Even the recent work <ref type=""bibr"" target=""#b24"">[24]</ref> which does compress across cache lines, is optimized for s er metric to measure area overhead is dictionary size. C-Pack in Adaptive and Decoupled requires    <ref type=""bibr"" target=""#b24"">[24]</ref>. As MORC allocates 512-bytes for each compression and deco type=""bibr"" target=""#b18"">[18]</ref>, Decoupled <ref type=""bibr"" target=""#b19"">[19]</ref>, and SC2 <ref type=""bibr"" target=""#b24"">[24]</ref> compressed caches. These schemes are evaluated with perfec cient than larger uncompressed caches thanks to significantly lower static and dynamic energy power <ref type=""bibr"" target=""#b24"">[24]</ref>. Figure <ref type=""figure"" target=""#fig_15"">9a</ref> compa ype=""bibr"" target=""#b20"">20,</ref><ref type=""bibr"" target=""#b22"">22]</ref>. While one recent scheme <ref type=""bibr"" target=""#b24"">[24]</ref> compresses inter-line, it still prioritizes single-stream type=""bibr"" target=""#b18"">[18]</ref>, Decoupled <ref type=""bibr"" target=""#b19"">[19]</ref>, and SC2 <ref type=""bibr"" target=""#b24"">[24]</ref>.</p><p>Adaptive <ref type=""bibr"" target=""#b18"">[18]</ref> , but is designed to be easier to implement <ref type=""bibr"" target=""#b42"">[42]</ref>. Finally, SC2 <ref type=""bibr"" target=""#b24"">[24]</ref> is most similar to MORC because it maintains a system-wide",1
"ed as deltas to their immediate predecessor, using a scheme similar to DE-FLATE's distance encoding <ref type=""bibr"" target=""#b32"">[32]</ref>, replicated in Table <ref type=""table"" target=""#tab_2"">2</",0
"et=""#b18"">[18,</ref><ref type=""bibr"" target=""#b19"">19,</ref><ref type=""bibr"" target=""#b20"">20,</ref><ref type=""bibr"" target=""#b21"">21,</ref><ref type=""bibr"" target=""#b22"">22,</ref><ref type=""bibr"" tar us compression algorithms, including LZ-based algorithms, and found that base-delta encoding (e.g., <ref type=""bibr"" target=""#b21"">[21]</ref>) compresses tags best. In our implementation, tags are enc lel. As base-delta compression is fast, and has been implemented to decompress 64Bytes in one cycle <ref type=""bibr"" target=""#b21"">[21]</ref>, we assume that the implemented tag compression can also d",0
"64b comparison (65nm) <ref type=""bibr"" target=""#b12"">[12]</ref> 2pJ 1x 64b access 128KB SRAM (32nm) <ref type=""bibr"" target=""#b13"">[13]</ref> 4pJ 2x 64b floating point op (45nm) <ref type=""bibr"" targe ly increase the size eightfold to .08mm 2 . For reference, a 16-way 256KB cache in 32nm is 2.12mm 2 <ref type=""bibr"" target=""#b13"">[13]</ref>.</p><p>Another metric to measure area overhead is dictiona ss; access energy numbers are per cache line. For SRAM and caches we use the 32nm models from CACTI <ref type=""bibr"" target=""#b13"">[13]</ref> L1  <ref type=""bibr"" target=""#b17"">[17]</ref>, assuming a",0
"nt for FP workloads which have huge working sets. For example, a characterization study of SPEC2006 <ref type=""bibr"" target=""#b39"">[39]</ref> shows that the LLC miss-rate of cactu-sADM is the same for",0
"f an LIP cache. We classify instruction cache misses into three categories as originally defined in <ref type=""bibr"" target=""#b48"">[51]</ref>. Non-repetitive misses do not belong to any recurring patt onding patterns.</p><p>We propose to use the Temporal Instruction Fetch Streaming (TIFS) prefetcher <ref type=""bibr"" target=""#b48"">[51]</ref> to prefetch recurring missing instructions. TIFS predicts nal hardware cost, we choose LIP instead of BIP. TIFS is implemented as described by Ferdman et al. <ref type=""bibr"" target=""#b48"">[51]</ref>. We find that it is sufficient for the IML to keep track o",1
"cy can drastically improve the front-end efficiency, even without the prefetching. Hempstead et al. <ref type=""bibr"" target=""#b60"">[64]</ref> designed a specialized event-driven architecture for embed",0
"its have been made.</p><p>Messaging Messengers are amongst the most popular applications used today <ref type=""bibr"" target=""#b23"">[25]</ref>. Each user sends and receives messages by communicating wi",0
"Languages Richards et al. explore languagelevel characteristics of client-side JavaScript programs <ref type=""bibr"" target=""#b62"">[66]</ref>. Our work studies server-side JavaScript and focuses on th",0
"ted for many years for highly concurrent server architecture <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b55"">59,</ref><ref type=""bibr"" target=""#b56"">60]</ref>, large-scale simula",0
"their maximum achievable parallelism exceeds 100?. It may seem that thread-level speculation (TLS) <ref type=""bibr"" target=""#b27"">[28,</ref><ref type=""bibr"" target=""#b59"">60,</ref><ref type=""bibr"" ta alization: Prior TLS schemes enforce inorder commits by passing a token among ready-to-commit tasks <ref type=""bibr"" target=""#b27"">[28,</ref><ref type=""bibr"" target=""#b60"">61,</ref><ref type=""bibr"" ta eculation (TLS) schemes to parallelize sequential programs <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b59"">60,</ref><ref type=""bibr"" tar that caused the violation and all later speculative tasks <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b60"">61,</ref><ref type=""bibr"" tar ve multiversioning <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b28"">29,</ref><ref type=""bibr"" tar",1
"e (e.g., 256), each of which stores a 64-bit timestamp. With Panigrahy and Sharma's PIDR OPT method <ref type=""bibr"" target=""#b53"">[54]</ref>, finding the next task to dispatch requires a single looku",0
". However, building high-performance allocators is complex <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b64"">65]</ref>. Instead, the simulator allocates and frees memory in a tas",0
"ints, such as geo-replicated databases where transactions must appear to execute in timestamp order <ref type=""bibr"" target=""#b13"">[14]</ref>, or deterministic architectures <ref type=""bibr"" target=""#",0
"get=""#b23"">24,</ref><ref type=""bibr"" target=""#b41"">42,</ref><ref type=""bibr"" target=""#b48"">49,</ref><ref type=""bibr"" target=""#b56"">57,</ref><ref type=""bibr"" target=""#b73"">74]</ref>. Similar to our lim on windows are needed to exploit it (&gt;100K instructions <ref type=""bibr"" target=""#b41"">[42,</ref><ref type=""bibr"" target=""#b56"">57]</ref>). Our oracle tool focuses on task-level parallelism, so it",0
"refetcher, offset prefetchers, and the sandbox method for selecting the prefetch offset dynamically <ref type=""bibr"" target=""#b25"">[26]</ref>. Offset prefetching is a generalization of next-line prefe </ref> (this list is not exhaustive).</p><p>Recently, Pugsley et al. introduced Sandbox prefetching <ref type=""bibr"" target=""#b25"">[26]</ref>. The Sandbox prefetcher prefetches line X + D when line X dge, the first published full-fledged offset prefetcher is the Sandbox prefetcher by Pugsley et al. <ref type=""bibr"" target=""#b25"">[26]</ref>. However, the offset selection mechanism in the Sandbox pr owledge, the SBP prefetcher of Pugsley et al. is the first published full-fledged offset prefetcher <ref type=""bibr"" target=""#b25"">[26]</ref>. The SBP prefetcher is cost-effective and was shown to out with actual prefetches.</p><p>We implemented the SBP prefetcher as described in the original paper <ref type=""bibr"" target=""#b25"">[26]</ref>, but with a few modifications to make the comparison with",1
"tch timeliness, which depend on applications characteristics <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b36"">37]</ref>. Hur and Lin have pro </ref>. Hur and Lin have proposed a method for making stream prefetching efficient on short streams <ref type=""bibr"" target=""#b8"">[9]</ref>.</p><p>The prefetchers mentioned above exploit simple memory",0
"able some history about past memory accesses and use that history to predict future memory accesses <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" ta",0
"get=""#b15"">16,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" targe",0
"[33]</ref>. So and Rechtschaffen proposed to use cache replacement status instead of a prefetch bit <ref type=""bibr"" target=""#b33"">[34]</ref>.</p><p>Stride prefetchers try to identify, among load and IP3: MRU insertion if demand miss, otherwise LRU insertion <ref type=""bibr"" target=""#b31"">[32,</ref><ref type=""bibr"" target=""#b33"">34,</ref><ref type=""bibr"" target=""#b37"">38]</ref>).</p><p>• IP4: MRU",0
"<head n=""2."">RELATED WORK</head><p>Simultaneous multithreading (SMT) was proposed by Tullsen et al. <ref type=""bibr"" target=""#b22"">[22]</ref> as a way to improve the utilization and throughput of a si",1
"cations, and some combinations may even degrade total throughput, for example due to cache trashing <ref type=""bibr"" target=""#b9"">[9]</ref>. Snavely and Tullsen <ref type=""bibr"" target=""#b19"">[19]</re",0
"lication when enabling SMT, based on performance counter events and machine learning. Settle et al. <ref type=""bibr"" target=""#b17"">[17]</ref> predict job symbiosis using offline profiled cache activit",0
"roup have finished but completion is still pending. The events behind this counter are not clear in <ref type=""bibr"" target=""#b1"">[1]</ref>, but it is non-negligible for some applications.</p><p>Table s, and multiple lower levels where each component is split up into several more detailed components <ref type=""bibr"" target=""#b1"">[1]</ref>. For example, the completion stall event of the first level,",0
"high amount of sharing in an SMT core.</p><p>This paper presents a new scheduler for the IBM POWER8 <ref type=""bibr"" target=""#b18"">[18]</ref> architecture, which is a multicore processor on which ever with a constant 1.7 factor. To explain this, we need to look at how the POWER8 commits instructions <ref type=""bibr"" target=""#b18"">[18]</ref>. Instructions are not committed individually, but in group",0
""">[2]</ref>, phase-based sampling <ref type=""bibr"" target=""#b2"">[3]</ref>, and statistical sampling <ref type=""bibr"" target=""#b3"">[4]</ref>. Of these techniques, the sampling based approaches typicall 8"">[9]</ref> extended SimPoint to provide statistical confidence measures.</p><p>Wunderlich, et al. <ref type=""bibr"" target=""#b3"">[4]</ref> developed the SMARTS framework, which applies statistical sa wn techniques for inferring statistics about a population given a sample of that population. SMARTS <ref type=""bibr"" target=""#b3"">[4]</ref> demonstrated that systematic sampling can be used to approxi mpared LiveSim with no sampling simulation and with a sampling mode that was very similar to SMARTS <ref type=""bibr"" target=""#b3"">[4]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n= f type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b8"">9]</ref> and statistical sampling <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" targe",1
"they proposed ways to determine when to begin warmup prior to simulating a sample. Eeckhout et al. <ref type=""bibr"" target=""#b24"">[25]</ref> proposed a similar technique that further reduced the amou",0
"can then be used to quickly rebuild the cache state prior to detailed simulation of a sampling unit <ref type=""bibr"" target=""#b13"">[14]</ref>.</p><p>LiveSim builds on existing work that uses sampling cache warmup problem we adapted the memory timestamp record (MTR) technique proposed by Barr et al. <ref type=""bibr"" target=""#b13"">[14]</ref>, and we call our adapted cache warmup technique LiveCache. are executed, the real simulation starts. This is similar to the technique proposed by Barr et al. <ref type=""bibr"" target=""#b13"">[14]</ref> with some slight changes to simplify the integration with elerate warmup. For LiveSim we developed LiveCache by adapting a technique developed by Barr et al. <ref type=""bibr"" target=""#b13"">[14]</ref> which keeps track of the sequence of memory operations dur",0
"SC <ref type=""bibr"" target=""#b16"">[17]</ref> as the timing simulator and a modified version of QEMU <ref type=""bibr"" target=""#b17"">[18]</ref> as the emulation engine.</p><p>We compared 3 different sim",0
"rchers have looked for ways to speed up thermal simulation <ref type=""bibr"" target=""#b26"">[27,</ref><ref type=""bibr"" target=""#b27"">28]</ref>, multithreaded simulation <ref type=""bibr"" target=""#b16"">[1",0
"tative CNN architectures, which we chose because of their nice tradeoffs between speed and accuracy <ref type=""bibr"" target=""#b2"">[2]</ref>:</p><p>1. The ILSVRC-2012 <ref type=""bibr"" target=""#b19"">[19 ng entry of <ref type=""bibr"" target=""#b12"">[12]</ref> (AlexNet).</p><p>2. The CNN-M-2048 model from <ref type=""bibr"" target=""#b2"">[2]</ref> (VGG-CNN-M-2048), which is a variant of the model introduced nt of training data without overfitting. Our approach to tackling this problem follows recent works <ref type=""bibr"" target=""#b2"">[2,</ref><ref type=""bibr"" target=""#b8"">8,</ref><ref type=""bibr"" target ch smaller datasets, we set an initial learning rate of 0.001, which is lower than the typical 0.01 <ref type=""bibr"" target=""#b2"">[2,</ref><ref type=""bibr"" target=""#b12"">12]</ref>, so as not to drasti",1
"type=""bibr"" target=""#b2"">[2]</ref> (VGG-CNN-M-2048), which is a variant of the model introduced in <ref type=""bibr"" target=""#b28"">[28]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head",1
"were learned for LPQ-TOP, audio, gist and SIFT features, and were combined in an SVM classifier. In <ref type=""bibr"" target=""#b14"">[14]</ref>, the optimal fusion of classifiers for HOG, dense SIFT, an >[10]</ref>, particularly previous years' EmotiW challenge <ref type=""bibr"" target=""#b11"">[11,</ref><ref type=""bibr"" target=""#b14"">14]</ref>.</p><p>However, due to the small dataset size for the Emoti",0
"ing the target dataset <ref type=""bibr"" target=""#b8"">[8,</ref><ref type=""bibr"" target=""#b9"">9,</ref><ref type=""bibr"" target=""#b17"">17,</ref><ref type=""bibr"" target=""#b26"">26]</ref>. This approach has",0
"fully between different expressions, even though they rely only on low frequency visual information <ref type=""bibr"" target=""#b1"">[1]</ref>. In our case, the lower frequency (but larger) FER-2013 data",0
"space <ref type=""bibr"" target=""#b29"">[29]</ref>. A very detailed and recent review can be found in <ref type=""bibr"" target=""#b21"">[21]</ref>.</p><p>The Emotion Recognition in the Wild (EmotiW) contes",0
"for HOG, dense SIFT, and deep convolutional features was learned based on a Riemannian manifold. In <ref type=""bibr"" target=""#b23"">[23]</ref> audio, LPQ-TOP, LBP-TOP, PHOG and SIFT features were used",0
"P, LBP-TOP, PHOG and SIFT features were used along with a hierarchical classifier fusion method. In <ref type=""bibr"" target=""#b3"">[3]</ref> HOG-TOP and audio features were fused using multiple kernel",0
"essions can be very nuanced, making it difficult even for humans to agree on their correct labeling <ref type=""bibr"" target=""#b20"">[20]</ref>.</p><p>We suspect that the inherent difficulty in assignin",0
"t k-step relational information in distinct subspaces.</p><p>Another recently proposed work is LINE <ref type=""bibr"" target=""#b24"">[25]</ref>, which has a loss function to capture both 1-step and 2-st el. This is considered as an equally weighted linear combination of k-step information. Tang et al. <ref type=""bibr"" target=""#b24"">[25]</ref> later proposed a large-scale information network embedding /head><p>We use the following methods of graph representation as baseline algorithms.</p><p>1. LINE <ref type=""bibr"" target=""#b24"">[25]</ref>. LINE is a recently proposed method for learning graph rep <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""6.3"">Parameter Settings</head><p>As suggested in <ref type=""bibr"" target=""#b24"">[25]</ref>, for LINE, we set the mini-batch size of stochastic gradie pWalk and E-SGNS, we set window size as 10, walk length as 40, walks per vertex as 80. According to <ref type=""bibr"" target=""#b24"">[25]</ref>, LINE yielded better results when the learned graph repres he dimension d of representations is set as 128 for Blogcatalog network and DBLP network as used in <ref type=""bibr"" target=""#b24"">[25]</ref> and is set as 64 for 20-NewsGroup network as used in <ref and use these samples for training, and use the remaining vertices for evaluation. As suggested in <ref type=""bibr"" target=""#b24"">[25]</ref>, we set k-max as 0, 200, 500 and 1000, respectively, and w",1
"ere has been a surge of interest in learning graph representations from data. For example, DeepWalk <ref type=""bibr"" target=""#b19"">[20]</ref>, one recent model, transforms a graph structure into a sam thod, which used stochastic gradient descent to optimize matrices from large graphs. Perozzi et al. <ref type=""bibr"" target=""#b19"">[20]</ref> presented an approach, which transformed graph structure i ep relational information and tuning the threshold of maximum number of vertices.</p><p>2. DeepWalk <ref type=""bibr"" target=""#b19"">[20]</ref>. DeepWalk is a method that learns the representation of so uction strategy for vertices with small degrees to achieve the optimal performance. As mentioned in <ref type=""bibr"" target=""#b19"">[20]</ref>, for DeepWalk and E-SGNS, we set window size as 10, walk l lti-label classification task by regarding the learned representations as features.</p><p>Following <ref type=""bibr"" target=""#b19"">[20,</ref><ref type=""bibr"" target=""#b26"">27]</ref>, we use the LibLin",1
"stics <ref type=""bibr"" target=""#b4"">[5]</ref>. Previous work include Latent Semantic Analysis (LSA) <ref type=""bibr"" target=""#b14"">[15]</ref>, which decomposes termdocument matrix and yields latent se",0
"del of Perozzi et al. [20]  as well as the skip-gram model with negative sampling of Mikolov et al. <ref type=""bibr"" target=""#b17"">[18]</ref> We conduct experiments on a language network, a social net f vertices using uniform sampling (which is also called truncated random walk). The skip-gram model <ref type=""bibr"" target=""#b17"">[18]</ref>, originally designed for learning word representations fro g methods employ a fixed slide window capturing context words of current word. Models like skipgram <ref type=""bibr"" target=""#b17"">[18]</ref> are proposed, which provide an efficient approach to learn ll other pairs do not come from the graph.</p><p>Motivated by the skip-gram model by Mikolov et al. <ref type=""bibr"" target=""#b17"">[18]</ref>, we employ noise contrastive estimation (NCE), which is pr E-SGNS. Skip-gram is an efficient model that learns the representation of each word in large corpus <ref type=""bibr"" target=""#b17"">[18]</ref>. For this enhanced version, we first utilize uniform sampl",0
"e network through a clustering task by employing the learned representations in a k-means algorithm <ref type=""bibr"" target=""#b1"">[2]</ref>.</p><p>To assess the quality of the results, we report the a",0
"onal vectors of the graphs which can then be applied to social network classification. Ahmed et al. <ref type=""bibr"" target=""#b0"">[1]</ref> proposed a graph factorization method, which used stochastic",0
"ative approaches other than the popular SVD can also be exploited. Examples include incremental SVD <ref type=""bibr"" target=""#b21"">[22]</ref>, independent component analysis (ICA) <ref type=""bibr"" tar",0
"assess the quality of the results, we report the averaged Normalized Mutual Information (NMI) score <ref type=""bibr"" target=""#b23"">[24]</ref> over 10 different runs for each system. To understand the",0
"ibr"" target=""#b19"">[20,</ref><ref type=""bibr"" target=""#b26"">27]</ref>, we use the LibLinear package <ref type=""bibr"" target=""#b9"">[10]</ref> to train one-vs-rest logistic regression classifiers, and w",0
"MDS) <ref type=""bibr"" target=""#b7"">[8]</ref>, IsoMap <ref type=""bibr"" target=""#b27"">[28]</ref>, LLE <ref type=""bibr"" target=""#b20"">[21]</ref>, and Laplacian Eigenmaps <ref type=""bibr"" target=""#b2"">[3]",0
"unknown true labels and some behavior assumptions, with examples of the Dawid-Skene (DS) estimator <ref type=""bibr"" target=""#b4"">[5]</ref>, the minimax entropy (Entropy) estimator<ref type=""foot"" tar ://www.tei-c.org/ns/1.0""><head n=""2.2"">Dawid-Skene Estimator</head><p>The method of Dawid and Skene <ref type=""bibr"" target=""#b4"">[5]</ref> is a generative approach by considering worker confusability ed majority voting (IWMV) <ref type=""bibr"" target=""#b10"">[11]</ref>, the Dawid-Skene (DS) estimator <ref type=""bibr"" target=""#b4"">[5]</ref>, and the minimax entropy (Entropy) estimator <ref type=""bibr m c = 2ˆ[−8 : 0] and = <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b4"">5]</ref> by the method in Sec. 6.2. As for Gibbs-CrowdSVM, we generate",1
"n matrix, various variants of the DS estimator have been studied, including the homogenous DS model <ref type=""bibr"" target=""#b14"">[15]</ref> and the class-conditional DS model <ref type=""bibr"" target",0
"s among all the images, and each image is labeled by all 39 workers. 4,214 labels in total. Flowers <ref type=""bibr"" target=""#b17"">[18]</ref>: It contains 2,366 binary labels for a dataset with 200 fl",0
"r"" target=""#b3"">[4]</ref>. So it can be efficiently solved by welldeveloped SVM solvers like LIBSVM <ref type=""bibr"" target=""#b1"">[2]</ref>. For updating y, we define (x) + := max(0, x), and then it i",0
"e majority voting and the weighted majority voting that takes worker reliability into consideration <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b10"">11]</ref>.</p><p>In this pape eighted majority voting (WMV) by putting different weights on workers to measure worker reliability <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b10"">11]</ref>.</p></div> <div xml e it is intractable to directly solve problem ( <ref type=""formula"" target=""#formula_9"">9</ref>) or <ref type=""bibr"" target=""#b9"">(10)</ref>, we introduce the structured mean-field assumption on the p <p>Infer y: Fixing the distributions of Φ and η at their optimum q * , we find y by solving problem <ref type=""bibr"" target=""#b9"">(10)</ref>. To make the prediction more efficient, we approximate the",0
"researchers have found convolutional networks (ConvNets) <ref type=""bibr"" target=""#b16"">[17]</ref>  <ref type=""bibr"" target=""#b17"">[18]</ref> are useful in extracting information from raw signals, ran",1
"<p>Bag-of-means on word embedding. We also have an experimental model that uses k-means on word2vec <ref type=""bibr"" target=""#b22"">[23]</ref> learnt from the training subset of each dataset, and then d-to-end learned word representations. We offer comparisons with both using the pretrained word2vec <ref type=""bibr"" target=""#b22"">[23]</ref> embedding <ref type=""bibr"" target=""#b15"">[16]</ref> and us",0
"on h(x) = max{0, x}, which makes our convolutional layers similar to rectified linear units (ReLUs) <ref type=""bibr"" target=""#b23"">[24]</ref>. The algorithm used is stochastic gradient descent (SGD) w",0
"ls is temporal max-pooling. It is the 1-D version of the max-pooling module used in computer vision <ref type=""bibr"" target=""#b1"">[2]</ref>. Given a discrete input function g(x) ∈ [1, l] → R, the max-",0
"was explored in literature. It has been shown that ConvNets can be directly applied to distributed <ref type=""bibr"" target=""#b5"">[6]</ref>  <ref type=""bibr"" target=""#b15"">[16]</ref> or discrete <ref",0
"<ref type=""bibr"" target=""#b21"">[22]</ref>) and region-based convolutional neural networks (R-CNNs) <ref type=""bibr"" target=""#b5"">[6]</ref>. Although region-based CNNs were computationally expensive a b5"">[6]</ref>. Although region-based CNNs were computationally expensive as originally developed in <ref type=""bibr"" target=""#b5"">[6]</ref>, their cost has been drastically reduced thanks to sharing c rk whose last fc layer simultaneously predicts multiple (e.g., 800) boxes, which are used for R-CNN <ref type=""bibr"" target=""#b5"">[6]</ref> object detection. Their proposal network is applied on a sin rget=""#foot_3"">3</ref>For regression, we adopt the parameterizations of the 4 coordinates following <ref type=""bibr"" target=""#b5"">[6]</ref>:</p><formula xml:id=""formula_2"">t x = (x − x a )/w a , t y = odel for ImageNet classification <ref type=""bibr"" target=""#b16"">[17]</ref>, as is standard practice <ref type=""bibr"" target=""#b5"">[6]</ref>. We tune all layers of the ZF net, and conv3 1 and up for th",1
"d a weight decay of 0.0005 <ref type=""bibr"" target=""#b10"">[11]</ref>. Our implementation uses Caffe <ref type=""bibr"" target=""#b9"">[10]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>S",0
"or locating class-specific or classagnostic bounding boxes <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" targe f type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b19"">20]</ref>. In the OverFeat method <ref type=""bibr"" target=""#b17"">[18]</ref>, a fully-connected (fc) layer is trained to predict the bo attracting increasing attention for efficient, yet accurate, visual recognition. The OverFeat paper <ref type=""bibr"" target=""#b17"">[18]</ref> computes conv features from an image pyramid for classific oposals are fewer.</p><p>One-Stage Detection vs. Two-Stage Proposal + Detection. The OverFeat paper <ref type=""bibr"" target=""#b17"">[18]</ref> proposes a detection method that uses regressors and class eat and MultiBox in more depth later in context with our method. Shared computation of convolutions <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" targ",0
"tion of convolutions <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b4"">5]</ref> has been attracting inc ype=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b15"">16]</ref> and semantic segmentation <ref type=""bibr"" target=""#b1"">[2]</ref>. Fast R-CNN <ref type=""bibr"" target=""#b4"">[5]</ref> enables",0
"CNN <ref type=""bibr"" target=""#b4"">[5]</ref>, achieves near real-time rates using very deep networks <ref type=""bibr"" target=""#b18"">[19]</ref>, when ignoring the time spent on region proposals. Now, pr fective running time for proposals is just 10 milliseconds. Using the expensive very deep models of <ref type=""bibr"" target=""#b18"">[19]</ref>, our detection method still has a frame rate of 5fps (incl rget=""#b22"">[23]</ref> (ZF), which has 5 shareable conv layers and the Simonyan and Zisserman model <ref type=""bibr"" target=""#b18"">[19]</ref> (VGG), which has 13 shareable conv layers.</p><p>To genera conv layers and 3 fc layers, and the public VGG-16 model<ref type=""foot"" target=""#foot_5"">5</ref>  <ref type=""bibr"" target=""#b18"">[19]</ref> that has 13 conv layers and 3 fc layers. We primarily eval",0
"dversarial samples <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b18"">19]</ref>; adversaries subtly alter legitimate inputs (call input per g on previous work <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b18"">19]</ref> describing how adversaries can efficiently select perturbat f previous attacks <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b18"">19]</ref> for knowledge of the target architecture and parameters. We ike neural networks<ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b18"">19]</ref>. In addition, we introduce new techniques to craft adversar ep neural network (DNN) using crafted inputs and output labels generated by the target ""victim"" DNN <ref type=""bibr"" target=""#b18"">[19]</ref>. Thereafter, the local network was used to generate advers d in <ref type=""bibr"" target=""#b11"">[12]</ref> or the Jacobian-based iterative approach proposed in <ref type=""bibr"" target=""#b18"">[19]</ref>. We only provide here a brief description of the fast grad",1
"neralization knowledge learned by a model into another model <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b9"">10]</ref>.</p><p>This paper demonstrates that adversaries can reliably",0
"ssion is the generalization of logistic regression to classification problems with N &gt; 2 classes <ref type=""bibr"" target=""#b17"">[18]</ref>. Logistic regression seeks to find the hypothesis best mat ghbors</head><p>The k nearest neighbor (kNN) algorithm is a lazy-learning non-parametric classifier <ref type=""bibr"" target=""#b17"">[18]</ref>: it does not require a training phase. Predictions are mad .5"">Decision Trees</head><p>Decision trees are defined by recursively partitioning the input domain <ref type=""bibr"" target=""#b17"">[18]</ref>. Partitioning is performed by selecting a feature and a co s of known input-label pairs ( x, y), classifiers f make label predictions f (x) on unseen inputs x <ref type=""bibr"" target=""#b17"">[18]</ref>. Models extrapolate from knowledge extracted by processing",0
"To train DNN, LR, and kNN models, we use Theano <ref type=""bibr"" target=""#b2"">[3]</ref> and Lasagne <ref type=""bibr"" target=""#b1"">[2]</ref>. The DNN is made up of a hierarchy of 2 convolutional layers ants of the procedure detailed in Section 4.1 with λ = 0.1: (1) vanilla Jacobian-based augmentation,<ref type=""bibr"" target=""#b1"">(2)</ref> </note></figure> <figure xmlns=""http://www.tei-c.org/ns/1.0""",0
"a set of techniques to transfer the generalization knowledge learned by a model into another model <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b9"">10]</ref>.</p><p>This paper dem",0
"esigner's productivity in developing hardware accelerators <ref type=""bibr"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b2"">[2]</ref>. While such hardware accelerators can often deliver signific UKU <ref type=""bibr"" target=""#b8"">[8]</ref>, or QuickDough <ref type=""bibr"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b2"">[2]</ref> have demonstrated the benefits of overlay by improving desig hard processors <ref type=""bibr"" target=""#b9"">[9]</ref>, <ref type=""bibr"" target=""#b10"">[10]</ref>, <ref type=""bibr"" target=""#b2"">[2]</ref> are used and the integration between the processor and accel",1
"structures of FPGA is comprehensively studied so as to maximize the performance of soft processors <ref type=""bibr"" target=""#b14"">[14]</ref>, <ref type=""bibr"" target=""#b15"">[15]</ref>.</p><p>In addit",0
"5"">[15]</ref>.</p><p>In addition, soft vector processors <ref type=""bibr"" target=""#b16"">[16]</ref>, <ref type=""bibr"" target=""#b17"">[17]</ref>, <ref type=""bibr"" target=""#b18"">[18]</ref>, soft VLIW proc",0
"omise of using FPGA overlays to improve designer's productivity in developing hardware accelerators <ref type=""bibr"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b2"">[2]</ref>. While such hardwar ref type=""bibr"" target=""#b7"">[7]</ref>, QUKU <ref type=""bibr"" target=""#b8"">[8]</ref>, or QuickDough <ref type=""bibr"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b2"">[2]</ref> have demonstrated t",0
"to a nurse. The primary embedding studied in this paper is the popular publicly-available word2vec <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b24"">25]</ref> embedding trained g we refer to in this paper is the aforementioned w2vNEWS embedding, a d = 300-dimensional word2vec <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b24"">25]</ref> embedding, which h",1
"recent study found that algorithms used to predict repeat offenders exhibit indirect racial biases <ref type=""bibr"" target=""#b0"">[1]</ref>. Different demographic and geographic groups also use differ",0
"airness have been described in a number of works, see, e.g., <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b9"">10]</ref> and a recent survey <r",0
"al matter of counting pairs of words that occur together. However, such counts are often misleading <ref type=""bibr"" target=""#b13"">[14]</ref>. For instance, the term male nurse is several times more f d nurse is more male than quarterback. More generally, Gordon and Van Durme show how reporting bias <ref type=""bibr"" target=""#b13"">[14]</ref>, including the fact that common assumptions are often left",0
"exhibit various biases, such as racial discrimination and gender bias in the ads presented to users <ref type=""bibr"" target=""#b35"">[36,</ref><ref type=""bibr"" target=""#b5"">6]</ref>. A recent study foun",0
"motivated the development and release of high performance stacked DRAM from several leading vendors <ref type=""bibr"" target=""#b12"">[13]</ref><ref type=""bibr"" target=""#b13"">[14]</ref><ref type=""bibr"" t tom layer of the chip has many-core compute engine and the top layer has wide I/O DRAMs (similar to <ref type=""bibr"" target=""#b12"">[13]</ref>) which are used as LLC shared by all cores. To optimize th",1
""" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b60"">62]</ref> Refresh overhead management <ref type=""bibr"" target=""#b57""> the new bottleneck. In other words, the benefits of using DRAM cache are bounded.</p><p>Pan et al. <ref type=""bibr"" target=""#b60"">[62]</ref> study the use of die-stacked DRAM cache in VLIW (very larg "" target=""#b3"">[4]</ref>, Hardavellas et al. <ref type=""bibr"" target=""#b4"">[5]</ref> and Pan et al. <ref type=""bibr"" target=""#b60"">[62]</ref> use different models or evaluation platforms, they all pro",0
"ie-stacking technology <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b23"">25]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n bandwidth benefits compared to conventional off-chip memory <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b23"">25,</ref><ref type=""bibr"" target=""#b31"">33]</ref>. In terms of energy ores results in only a small temperature increase (less than 10 ? for 8 additional layers of memory <ref type=""bibr"" target=""#b23"">[25]</ref>).</p><p>The bandwidth provided by off-chip main memory is y large number of layers presents challenges of power delivery and cooling, yield, testability etc. <ref type=""bibr"" target=""#b23"">[25]</ref>.</p><p>Challenges in hybrid caches: Some researchers have",0
"DRAM architecture <ref type=""bibr"" target=""#b19"">[21]</ref><ref type=""bibr"" target=""#b20"">[22]</ref><ref type=""bibr"" target=""#b21"">[23]</ref><ref type=""bibr"" target=""#b22"">[24]</ref> and die-stacking r and one capacitor) structure with a cell size of 6-10F 2 <ref type=""bibr"" target=""#b18"">[20,</ref><ref type=""bibr"" target=""#b21"">23]</ref>. Clearly, DRAM provides much higher density than SRAM. Henc",0
"high performance stacked DRAM from several leading vendors <ref type=""bibr"" target=""#b12"">[13]</ref><ref type=""bibr"" target=""#b13"">[14]</ref><ref type=""bibr"" target=""#b14"">[15]</ref><ref type=""bibr"" t",0
"pogation on just single hidden-layer feedforward neural networks. Recent attempts in this direction <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b27"">28]</ref> propose efficient r the search space through tunable parameters, in contrast to rigid search procedures in prior work <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b27"">28]</ref>. Consequently, our of nodes. We contrast the performance of node2vec with state-of-the-art feature learning algorithms <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b27"">28]</ref>. We experiment wit odel, recent research established an analogy for networks by representing a network as a ""document"" <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b27"">28]</ref>. The same way as a r shortcoming of prior work which fail to offer any flexibility in sampling of nodes from a network <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b27"">28]</ref>. Our algorithm nod roceed by extending the Skip-gram architecture to networks <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b23"">24]</ref>. We seek to optimize the following objective function, whic normalized Laplacian matrix of graph G as the feature vector representations for nodes. • DeepWalk <ref type=""bibr"" target=""#b23"">[24]</ref>: This approach learns d-dimensional feature representation lude other matrix factorization approaches which have already been shown to be inferior to DeepWalk <ref type=""bibr"" target=""#b23"">[24]</ref>. We also exclude a recent approach, GraRep <ref type=""bibr the sampling procedure computationally efficient. We showed how random walks, also used in DeepWalk <ref type=""bibr"" target=""#b23"">[24]</ref>, allow the sampled nodes to be reused as neighborhoods for SION</head><p>Both DeepWalk and LINE can be seen as rigid search strategies over networks. DeepWalk <ref type=""bibr"" target=""#b23"">[24]</ref> proposes search using uniform random walks. The obvious li",1
"ation could be based on the structural roles of nodes in the network (i.e., structural equivalence) <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" targe nd structural equivalence <ref type=""bibr"" target=""#b11"">[12]</ref>. Under the homophily hypothesis <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b35"">36]</ref> nodes that are highl",0
"tp://www.tei-c.org/ns/1.0"" type=""table"" xml:id=""tab_3""><head></head><label></label><figDesc>Facebook<ref type=""bibr"" target=""#b13"">[14]</ref>: In the Facebook network, nodes represent users, and edges",0
"al clustering makes a strong homophily assumption that graph cuts will be useful for classification <ref type=""bibr"" target=""#b28"">[29]</ref>. Such assumptions are reasonable in many scenarios, but un he performance of node2vec against the following feature learning algorithms: • Spectral clustering <ref type=""bibr"" target=""#b28"">[29]</ref>: This is a matrix factorization approach in which we take",0
"s in networks often shuttle between two kinds of similarities: homophily and structural equivalence <ref type=""bibr"" target=""#b11"">[12]</ref>. Under the homophily hypothesis <ref type=""bibr"" target=""#",0
"ei-c.org/ns/1.0""><head n=""4.1"">Stressor Event and Subject Dictionaries</head><p>The word embeddings <ref type=""bibr"" target=""#b8"">[Mikolov et al., 2013]</ref> have been found effective in estimating t",1
"#b3"">[Chen et al., 2009]</ref>: It is a convex relaxation of the alternating structure optimization <ref type=""bibr"" target=""#b0"">[Ando and Zhang, 2005]</ref>, which decomposes the predictive model of",0
"decades. Most of these measurements <ref type=""bibr"" target=""#b9"">[Rowlison and Felner, 1988;</ref><ref type=""bibr"" target=""#b1"">Brantley and Jones, 1993]</ref> are based on questionnaires and interv fier that is found to effective in several classification problems.</p><p>• Softmax Regression (SR) <ref type=""bibr"" target=""#b1"">[Böhning, 1992]</ref>: It is a model that is used to predict the proba",0
"ressor Event Dictionary. Based on the event categories in the professional life events stress scale <ref type=""bibr"" target=""#b6"">[Holmes and Rahe, 1967]</ref>, we first selected the top 12 categories are based on questionnaires and interviews. Among them, the Social Readjustment Rating Scale (SRRS) <ref type=""bibr"" target=""#b6"">[Holmes and Rahe, 1967]</ref> is one of the widely-accepted metrics. I tegorized the stressor events into 43 categories based on the professional life events stress scale <ref type=""bibr"" target=""#b6"">[Holmes and Rahe, 1967]</ref>.</p><p>We then manually defined a set of rds of different social relations. However, stressor subjects expressed in  Inspired by the work in <ref type=""bibr"" target=""#b6"">[Haslam, 1994]</ref>, we categorized the subjects into six categories.",0
"n recent years, there exist some studies on leveraging social media data for mental health care. De <ref type=""bibr"" target=""#b4"">Choudhury et al. [2013]</ref> were the first to explore social media d",0
"ble to DT-ORS-Net applications. To this end, the two-player Markov stopping game (MSG) developed in <ref type=""bibr"" target=""#b16"">[17]</ref> can serve as a good starting point, which extends the clas e theoretic setting so as to handle the potential conflicts between the two players.</p><p>However, <ref type=""bibr"" target=""#b16"">[17]</ref> does not provide a systematic method to deal with a genera the optimal strategy for each player in such situations, the two-player MSG framework developed in <ref type=""bibr"" target=""#b16"">[17]</ref> may serve as a basis. Particularly, in the two-player MSG, yer. However, a systematic method for handling a general number of players in a MSG is missing from <ref type=""bibr"" target=""#b16"">[17]</ref>. Considering this, a general M-MSG is proposed in the next needed. Particularly, when two players coexist in the MSG, the randomized stopping time is used in <ref type=""bibr"" target=""#b16"">[17]</ref> to deal with the potential competition from the other play ynamism in the number of players as the game evolving, the concept of selection time is proposed in <ref type=""bibr"" target=""#b16"">[17]</ref>. However, constructing the selection time essentially requ",1
", an optimal relay selection strategy is developed in <ref type=""bibr"" target=""#b10"">[11]</ref>. In <ref type=""bibr"" target=""#b11"">[12]</ref>, the dynamism of electricity price and the deferrability o onding Nash value V i k N (XN , RN ) using ( <ref type=""formula"" target=""#formula_15"">10</ref>) and <ref type=""bibr"" target=""#b11"">(12)</ref>.</p><formula xml:id=""formula_21"">End For 1 ≤ n ≤ N − 1</fo sponding Nash value V i k n (Xn, Rn) using ( <ref type=""formula"" target=""#formula_15"">10</ref>) and <ref type=""bibr"" target=""#b11"">(12)</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>",1
"<label>(9)</label></formula><p>with the right-hand side given by ( <ref type=""formula"">7</ref>) and <ref type=""bibr"" target=""#b7"">(8)</ref>. Denote the mixed-strategy NE of this auxiliary game by P i h possible realization of XN and RN</p><p>• Compute the payoff function of the auxiliary game using <ref type=""bibr"" target=""#b7"">(8)</ref> and ( <ref type=""formula"" target=""#formula_14"">9</ref>). • C ce allocation problems<ref type=""bibr"" target=""#b5"">[6]</ref><ref type=""bibr"" target=""#b6"">[7]</ref><ref type=""bibr"" target=""#b7"">[8]</ref><ref type=""bibr"" target=""#b8"">[9]</ref> in which the luxury o",0
"sources: the regular resource and the opportunistic resource <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b17"">18]</ref>. Particularly, the opportunistic resources can be a set of",0
"stic spectrum resource exploited in cognitive radio networks <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref>, idle virtual machines may be offered by a cloud provider as cloud resource according to the corresponding multi-player randomized stopping time T ( P) given by <ref type=""bibr"" target=""#b1"">(2)</ref>.</p><p>Similarly, the participant recruitment problem descri /ref>, and so are X n 's. Similarly to the previous problem, a recruiter can invoke Algorithm 1 and <ref type=""bibr"" target=""#b1"">(2)</ref> to determine optimally when to hire a mobile user.</p><p>Thr",0
"nd P i k n is an NE of player i k in the auxiliary game at timeslot n with payoff function given by <ref type=""bibr"" target=""#b8"">(9)</ref>. Consequently,</p><formula xml:id=""formula_19"">V i k n (X n target=""#b5"">[6]</ref><ref type=""bibr"" target=""#b6"">[7]</ref><ref type=""bibr"" target=""#b7"">[8]</ref><ref type=""bibr"" target=""#b8"">[9]</ref> in which the luxury of resource selection over time is usual",0
"users. For example, in a vehicle based crowdsourcing network <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b18"">19]</ref>, companies can set up access points near a highway and ask",0
"build their results on the classic optimal stopping theory <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b14"">15]</ref> that can provide the optimal online strategy for accessing er MSG, some basics of the classic Markov stopping problem <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b14"">15]</ref>  </p><formula xml:id=""formula_0"">T * = inf{1 ≤ n ≤ N |f (X",0
"recursively compute an NE strategy for each player. For ease of presentation, only symmetric games <ref type=""bibr"" target=""#b20"">[21]</ref> will be considered in this work, in which the reward funct",0
"y be offered by a cloud provider as an opportunistic computing resource in cloud-computing networks <ref type=""bibr"" target=""#b2"">[3]</ref>; lowprice harvested energy (e.g., wind or solar energy) may sidering that the front-end workload dynamism in a cloud can usually be modeled as a Markov process <ref type=""bibr"" target=""#b2"">[3]</ref>, it is reasonable to assume that the sequence of opportunist der can offer two types of computing resources: the regular resource and the opportunistic resource <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b17"">18]</ref>. Particularly, the o",0
"=""#b30"">31,</ref><ref type=""bibr"" target=""#b42"">43]</ref>, and fine-grain reconfigurable processors <ref type=""bibr"" target=""#b64"">[64]</ref> are all examples.</p><p>Highly configurable processors hav ile maintaining a homogeneous fabric. The CASH architecture is inspired by the Sharing Architecture <ref type=""bibr"" target=""#b64"">[64]</ref>, but improves on it with fast reconfiguration, a well defi type=""bibr"" target=""#b4"">[5]</ref> on the CASH Architecture (which extends the Sharing Architecture <ref type=""bibr"" target=""#b64"">[64]</ref>). The video encoder is an excellent example of our target 1.0""><head>A. Architecture Overview</head><p>The CASH architecture extends the Sharing Architecture <ref type=""bibr"" target=""#b64"">[64]</ref> -a prior configurable core architecture -by <ref type=""bib et=""#b14"">[15,</ref><ref type=""bibr"" target=""#b40"">41,</ref><ref type=""bibr"" target=""#b57"">57,</ref><ref type=""bibr"" target=""#b64"">64]</ref> allow fine grain control over resource scheduling. As fine- able architectures <ref type=""bibr"" target=""#b40"">[41,</ref><ref type=""bibr"" target=""#b57"">57,</ref><ref type=""bibr"" target=""#b64"">64]</ref>. The one drawback of these approaches is that fine-grain co",1
"s without the need of a compiler.</p><p>The CASH architecture leverages many ideas from Core Fusion <ref type=""bibr"" target=""#b26"">[27]</ref> to distribute resources across cores, but unlike Core Fusi",0
"geometries and require different techniques to solve efficiently. Machine learning based techniques <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b25"">26]</ref> have been proposed f",0
"mic assignment of resources, dynamic transport of operands, and dynamic instruction ordering. TRIPS <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b45"">46]</ref> has an array of ALUs",0
"e. Scheduling algorithms for heterogeneous processors have been proposed for MS Bing's index search <ref type=""bibr"" target=""#b44"">[45]</ref>. While heterogeneous processors improve interactive data c",0
"e=""bibr"" target=""#b3"">[3]</ref>, on which the OS kernel (Linux 4.2) and the embedded network GARNET <ref type=""bibr"" target=""#b1"">[1]</ref> are enhanced according to our technique. The details of the",1
"ection are used to evaluate the impact of critical sections on multi-threaded applications. Work in <ref type=""bibr"" target=""#b9"">[9]</ref> proposes speedup stack, which quantifies the impact of vario",0
"s wait. A memory-interference induced application slowdown estimation (MISE) model is introduced in <ref type=""bibr"" target=""#b20"">[20]</ref>, which can be utilized to achieve both QoS guarantee and s",0
"n previous studies <ref type=""bibr"" target=""#b21"">[21,</ref><ref type=""bibr"" target=""#b12"">12,</ref><ref type=""bibr"" target=""#b13"">13,</ref><ref type=""bibr"" target=""#b17"">17,</ref><ref type=""bibr"" tar n asymmetric CMP ( <ref type=""bibr"" target=""#b21"">[21,</ref><ref type=""bibr"" target=""#b12"">12,</ref><ref type=""bibr"" target=""#b13"">13,</ref><ref type=""bibr"" target=""#b17"">17,</ref><ref type=""bibr"" tar the above studies <ref type=""bibr"" target=""#b21"">[21,</ref><ref type=""bibr"" target=""#b12"">12,</ref><ref type=""bibr"" target=""#b13"">13,</ref><ref type=""bibr"" target=""#b17"">17,</ref><ref type=""bibr"" tar bottleneck, and the one that keeps CPUs waiting for the longest time is the critical bottleneck. In <ref type=""bibr"" target=""#b13"">[13]</ref>, utility-based acceleration of multi-threaded applications",0
"sentiment information in the text, we introduce an external sentiment knowledge base, Senti-WordNet <ref type=""bibr"" target=""#b17"">[10]</ref>, which forms the sentiment view. Then, using a framework o ment aspect of the associate text. For this, we use an external knowledge base, called SentiWordNet <ref type=""bibr"" target=""#b17"">[10]</ref>. It is based on the well-known English lexical dictionary ures with the mid-level features (denoted as Low&amp;SentiBank), and a textual feature-based method <ref type=""bibr"" target=""#b17"">[10]</ref> (denoted as SentiStrength<ref type=""foot"" target=""#foot_3""",1
"nship between features, we introduce explicit feature maps <ref type=""bibr"" target=""#b19"">[12,</ref><ref type=""bibr"" target=""#b20"">13]</ref> to CCA. Finally, using the features that are projected to t use. In contrast, recent advances of explicit feature maps <ref type=""bibr"" target=""#b19"">[12,</ref><ref type=""bibr"" target=""#b20"">13]</ref> can convert nonlinear problems to linear problems, which ca computation complexity, one can use explicit feature maps <ref type=""bibr"" target=""#b19"">[12,</ref><ref type=""bibr"" target=""#b20"">13]</ref>. Let φ(x) denote an explicit feature mapping such that K i rnel. All other histogram-based features were mapped using the exact Bhattacharyya kernel map- ping <ref type=""bibr"" target=""#b20"">[13]</ref>. Finally, similar to <ref type=""bibr"" target=""#b16"">[9]</r",1
"orms the sentiment view. Then, using a framework of multi-view canonical correlation analysis (CCA) <ref type=""bibr"" target=""#b18"">[11]</ref>, we calculate a latent embedding space in which correlatio s the linear relationship between random variables. Several nonlinear extensions such as kernel CCA <ref type=""bibr"" target=""#b18"">[11]</ref> and Deep CCA <ref type=""bibr"" target=""#b29"">[22]</ref> hav ions among multiple views using a framework of the generalization of canonical correlation analysis <ref type=""bibr"" target=""#b18"">[11]</ref>. Let X i (i ∈ {v, t, s}) denote the feature matrix of the at the distances in the resulting space between each pair of views for the same image are minimized <ref type=""bibr"" target=""#b18"">[11]</ref>. The objective function to learn the latent space is as fo ϕ j (X j ), and w ik represents the k-th column of the matrix W i . In the conventional kernel CCA <ref type=""bibr"" target=""#b18"">[11]</ref>, kernel trick is used in Eq. (1). To reduce the computatio",1
"fically, to capture the nonlinear relationship between features, we introduce explicit feature maps <ref type=""bibr"" target=""#b19"">[12,</ref><ref type=""bibr"" target=""#b20"">13]</ref> to CCA. Finally, u high computational complexity and memory use. In contrast, recent advances of explicit feature maps <ref type=""bibr"" target=""#b19"">[12,</ref><ref type=""bibr"" target=""#b20"">13]</ref> can convert nonlin l trick is used in Eq. (1). To reduce the computation complexity, one can use explicit feature maps <ref type=""bibr"" target=""#b19"">[12,</ref><ref type=""bibr"" target=""#b20"">13]</ref>. Let φ(x) denote a IST features, attribute features, and SentiBank features, we use the random Fourier feature mapping <ref type=""bibr"" target=""#b19"">[12]</ref> to approximate the Gaussian kernel. All other histogram-ba",1
"ion in recent years <ref type=""bibr"" target=""#b11"">[4]</ref><ref type=""bibr"" target=""#b12"">[5]</ref><ref type=""bibr"" target=""#b13"">[6]</ref><ref type=""bibr"" target=""#b14"">[7]</ref>.</p><p>Conventional olarity classifiers <ref type=""bibr"" target=""#b11"">[4]</ref><ref type=""bibr"" target=""#b12"">[5]</ref><ref type=""bibr"" target=""#b13"">[6]</ref>. However, due to the affective gap between lowlevel visual ges on social media <ref type=""bibr"" target=""#b11"">[4]</ref><ref type=""bibr"" target=""#b12"">[5]</ref><ref type=""bibr"" target=""#b13"">[6]</ref><ref type=""bibr"" target=""#b14"">[7]</ref>. Typically, the goa image. Similarly, attribute features including facial expression were used as mid-level features in <ref type=""bibr"" target=""#b13"">[6]</ref>. These conventional methods focus on how to design visual r performance improvement. We will introduce additional views or features such as facial expressions <ref type=""bibr"" target=""#b13"">[6]</ref>. In addition, we will introduce the deep learning-based fea",0
"as follows.</p><p>• Flickr dataset. From Flickr, we first downloaded a set of image IDs provided by <ref type=""bibr"" target=""#b35"">[28]</ref>. Some images were unavailable, and limiting the number of",0
"odal retrieval tasks <ref type=""bibr"" target=""#b15"">[8,</ref><ref type=""bibr"" target=""#b16"">9,</ref><ref type=""bibr"" target=""#b23"">[16]</ref><ref type=""bibr"" target=""#b24"">[17]</ref><ref type=""bibr"" t",0
"art theory using relatively small and controlled datasets <ref type=""bibr"" target=""#b21"">[14,</ref><ref type=""bibr"" target=""#b22"">15]</ref>, while recent works have started to analyze the sentiments",0
"dition, we will introduce the deep learning-based features <ref type=""bibr"" target=""#b37"">[30,</ref><ref type=""bibr"" target=""#b38"">31]</ref>, which have significantly improved many computer vision tas",0
"and descriptions) can improve the image content recognition <ref type=""bibr"" target=""#b15"">[8,</ref><ref type=""bibr"" target=""#b16"">9]</ref>. Inspired from these studies, to bridge images and sentiment ny times a word appears in text around the image. Following <ref type=""bibr"" target=""#b15"">[8,</ref><ref type=""bibr"" target=""#b16"">9]</ref>, we use the linear kernel for the textual features, which co ved several image annotation and crossmodal retrieval tasks <ref type=""bibr"" target=""#b15"">[8,</ref><ref type=""bibr"" target=""#b16"">9,</ref><ref type=""bibr"" target=""#b23"">[16]</ref><ref type=""bibr"" tar r problems to linear problems, which can be solved by linear frameworks with a low computation cost <ref type=""bibr"" target=""#b16"">[9,</ref><ref type=""bibr"" target=""#b30"">23]</ref>. Following these st w.</p><p>Visual features: Following the feature design used in recent visual classification methods <ref type=""bibr"" target=""#b16"">[9,</ref><ref type=""bibr"" target=""#b25"">18,</ref><ref type=""bibr"" tar view formulation has recently proven to be effective for cross-modal retrieval and image annotation <ref type=""bibr"" target=""#b16"">[9,</ref><ref type=""bibr"" target=""#b26"">19]</ref>. In the following s alues of each dimension in the embedding space. p is a weighting parameter, which is set to 4 as in <ref type=""bibr"" target=""#b16"">[9,</ref><ref type=""bibr"" target=""#b26"">19]</ref>. Using Eq. ( <ref t exact Bhattacharyya kernel map- ping <ref type=""bibr"" target=""#b20"">[13]</ref>. Finally, similar to <ref type=""bibr"" target=""#b16"">[9]</ref>, we reduce each kernelmapped feature to 500 dimensions usin φ(x). Instead of using the kernel trick, the mapping φ(x) can substituted to the objective function <ref type=""bibr"" target=""#b16"">[9]</ref>. Solving the following generalized eigenvalue problem provi",0
"ding opinion mining about social events, product marketing, and affective human-machine interaction <ref type=""bibr"" target=""#b10"">[3]</ref>. Thus, automatic inference of the sentiment implied in the",0
"=""#b8"">[1]</ref>, and Instagram, which has grown to have more than 400 million monthly active users <ref type=""bibr"" target=""#b9"">[2]</ref>. These images uploaded by Internet users can be considered t",0
""">[5]</ref> 70.01 ± 0.63% 67.26 ± 1.12% Low&amp;SentiBank 70.54 ± 1.00% 68.03 ± 1.36% SentiStrength <ref type=""bibr"" target=""#b36"">[29]</ref> 59.30 ± 0.87% 62.78 ± 0.91% USEA <ref type=""bibr"" target=""",0
"=""#b24"">[17]</ref><ref type=""bibr"" target=""#b25"">[18]</ref><ref type=""bibr"" target=""#b26"">[19]</ref><ref type=""bibr"" target=""#b27"">[20]</ref>, but its effectiveness has not been fully demonstrated in",0
"e=""bibr"" target=""#b17"">[10]</ref>. It is based on the well-known English lexical dictionary WordNet <ref type=""bibr"" target=""#b33"">[26]</ref>, and has been utilized in text-based opinion mining tasks",0
"max pooling. We also extract the following mid-level features: 2,000-dimensional attribute features <ref type=""bibr"" target=""#b31"">[24]</ref> and 1,200-dimensional SentiBank outputs <ref type=""bibr"" t",0
". Centralized traffic engineering (TE) techniques have been proposed to improve network utilization <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b19"">20]</ref> without affecting ineering with pricing. Pricing can guard existing TE techniques that improve WAN utilization (e.g., <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b19"">20]</ref>) against strategic ble portion of inter-datacenter transfers have deadlines, and can be modeled using this abstraction <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b21"">22]</ref>. For example, peri ase of our solution ( ?4.4). Other portions of the WAN traffic may not be governed by any TE scheme <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" ta rther, by using prices, SAM is mostly protected from strategic users ( ?4.2). Similar to prior work <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b19"">20]</ref>, we execute SAM on that the bandwidth required for such requests is known a priori (e.g., from historical usage, as in <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b21"">22]</ref>), and is appropria fic engineering for datacenter WANs has been drawn recent attention from both industry and academia <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" ta for ad hoc high priority traffic; the volume to be set aside is estimated based on historical usage <ref type=""bibr"" target=""#b17"">[18]</ref>. When unexpected congestion occurs, perhaps because of mor 7"">[18,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" target=""#b23"">24]</ref>. SWAN <ref type=""bibr"" target=""#b17"">[18]</ref> and B4 <ref type=""bibr"" target=""#b19"">[20]</ref> aim to im",1
"side, where individual routing nodes may act strategically <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b14"">15]</ref>. We instead consider consumer incentives and assume a singl",1
"oose 10% of the timesteps in a window). We address this issue by using sorting-network inequalities <ref type=""bibr"" target=""#b24"">[25]</ref>, which reduces the number of constraints to polynomial wit for the construction and proof. We note that our solution improves upon the techniques proposed in <ref type=""bibr"" target=""#b24"">[25]</ref> by requiring 40% fewer ""sorting"" constraints per link (det details in appendix). Furthermore, we provide a rigorous proof of correctness, which was missing in <ref type=""bibr"" target=""#b24"">[25]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head </p><p>x + y = m + M, m ? x, m ? y. Note this implies M ? max{x, y} and m ? min{x, y}. We note that <ref type=""bibr"" target=""#b24"">[25]</ref> uses five sorting constraints in their solution; hence, ou",0
"<ref type=""bibr"" target=""#b20"">[21]</ref>, oblivious routing <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b13"">14]</ref>, and finding suitable routing parameters for given protocol r"" target=""#b13"">14]</ref>, and finding suitable routing parameters for given protocols (e.g., OSPF <ref type=""bibr"" target=""#b13"">[14]</ref>). Recent papers consider the objective of imposing fairnes",0
"g and machine learning. The existence of optimal ""market"" prices is a classical result in economics <ref type=""bibr"" target=""#b2"">[3]</ref>, and given sufficient data it is possible to learn those pri",0
"ref> point out that such prices can be used to guide online allocation. An alternative line of work <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target larger than the sum of T -k-smallest numbers from f 0 1 , f 0 2 , . . . f 0 T . This together with <ref type=""bibr"" target=""#b4"">(5)</ref> guarantees that</p><formula xml:id=""formula_14"">F k T -k + F",0
"each byte is priced accordingly -then along the path Price Menu: Transfer from S to T time interval <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref> time interval <ref typ nterval <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref> time interval <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b0"">1]</ref> Network (all capacitie 1,</ref><ref type=""bibr"" target=""#b1"">2]</ref> time interval <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b0"">1]</ref> Network (all capacities = 1)</p><p>Figure <ref type=""figure""> less load. Solver. We built our modules as linear programs and execute them using the Gurobi solver <ref type=""bibr"" target=""#b0"">[1]</ref>. Metrics. We evaluate a few different metrics. First, we use",0
"pricing methods (all prices per unit transferred), and total welfare of the schedule.</p><p>welfare <ref type=""bibr"" target=""#b32"">[33]</ref>, and the optimization in ( <ref type=""formula"" target=""#fo fficiency of its WAN resources, which may be motivated by competitive pressure from other providers <ref type=""bibr"" target=""#b32"">[33]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head ighly competitive, and hence a profit-seeking provider will anyway be driven to optimize for welfare<ref type=""bibr"" target=""#b32"">[33]</ref>.</p></note> 			<note xmlns=""http://www.tei-c.org/ns/1.0"" p",0
"rget=""#b19"">20]</ref> without affecting low latency traffic and with explicit support for deadlines <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b33"">34]</ref>. Such techniques c he promised service guarantees. SAM is related to recent work on multi-timestep traffic engineering <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b33"">34]</ref> but extends it in have deadlines, and can be modeled using this abstraction <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b21"">22]</ref>. For example, periodic index refreshes are expected to be f d by any TE scheme <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" target=""#b21"">22]</ref>. For example, there could be some latency-sensitive request ests is known a priori (e.g., from historical usage, as in <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b21"">22]</ref>), and is appropriately reserved on all links of the network or every request) minus the cost incurred. Practical online versions of this scheme (such as Tempus <ref type=""bibr"" target=""#b21"">[22]</ref>) would obviously perform worse and hence we do not conside and do not offer time-guarantees. From the TE perspective, the most relevant work to ours is Tempus <ref type=""bibr"" target=""#b21"">[22]</ref> which introduces a TE framework which incorporates request",0
"t=""#b13"">[14]</ref>). Recent papers consider the objective of imposing fairness in a shared network <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b10"">11]</ref>.</p><p>Traffic engi",0
"of localized spectral filters on graphs <ref type=""bibr"" target=""#b11"">(Hammond et al., 2011;</ref><ref type=""bibr"" target=""#b5"">Defferrard et al., 2016)</ref>.</p></div> <div xmlns=""http://www.tei-c er neighborhood). The complexity of evaluating Eq. 5 is O(|E|), i.e. linear in the number of edges. <ref type=""bibr"" target=""#b5"">Defferrard et al. (2016)</ref> use this K-localized convolution to def pectral graph convolutional neural networks, introduced in Bruna et al. (2014) and later extended by<ref type=""bibr"" target=""#b5"">Defferrard et al. (2016)</ref> with fast localized convolutions. In co introduced to the original frameworks of<ref type=""bibr"" target=""#b3"">Bruna et al. (2014)</ref> and<ref type=""bibr"" target=""#b5"">Defferrard et al. (2016)</ref> that improve scalability and classifica",1
"ions until node representations reach a stable fixed point. This restriction was later alleviated in<ref type=""bibr"" target=""#b15"">Li et al. (2016)</ref> by introducing modern practices for recurrent",0
"""bibr"" target=""#b30"">(Zhu et al., 2003;</ref><ref type=""bibr"" target=""#b29"">Zhou et al., 2004;</ref><ref type=""bibr"" target=""#b1"">Belkin et al., 2006;</ref><ref type=""bibr"" target=""#b26"">Weston et al. hods based on graph-Laplacian regularization <ref type=""bibr"" target=""#b30"">(Zhu et al., 2003;</ref><ref type=""bibr"" target=""#b1"">Belkin et al., 2006;</ref><ref type=""bibr"" target=""#b26"">Weston et al. label propagation <ref type=""bibr"" target=""#b30"">(Zhu et al., 2003)</ref>, manifold regularization <ref type=""bibr"" target=""#b1"">(Belkin et al., 2006)</ref> and deep semi-supervised embedding <ref ty iEmb) <ref type=""bibr"" target=""#b26"">(Weston et al., 2012)</ref>, manifold regularization (ManiReg) <ref type=""bibr"" target=""#b1"">(Belkin et al., 2006)</ref> and skip-gram based graph embeddings (Deep",0
"tting, a number of simplifications (see Section 2.2) can be introduced to the original frameworks of<ref type=""bibr"" target=""#b3"">Bruna et al. (2014)</ref> and<ref type=""bibr"" target=""#b5"">Defferrard",0
", the widespread use of such tools raises legitimate privacy concerns. For instance, Mislove et al. <ref type=""bibr"" target=""#b22"">[24]</ref> demonstrated how, by analysing Facebook's social network s",1
"ithin C † to b − d nodes from outside of C † .</formula><p>This heuristic is inspired by modularity <ref type=""bibr"" target=""#b24"">[26]</ref>-a widely used index for measuring the quality of any given anguage (version 1.0.1), namely: Eigenvector <ref type=""bibr"" target=""#b23"">[25]</ref>, Betweenness <ref type=""bibr"" target=""#b24"">[26]</ref>, Walktrap <ref type=""bibr"" target=""#b27"">[29]</ref>, Louva",0
"g from strict legal controls <ref type=""bibr"" target=""#b0"">[1]</ref>, through algorithmic solutions <ref type=""bibr"" target=""#b13"">[15]</ref>, to market-like mechanisms that allow participants to mone",0
"istic twice on the 9/11 terrorist network to hide Mohamed Atta-one of the ringleaders of the attack <ref type=""bibr"" target=""#b17"">[19]</ref>. The red link is the one to be to removed by the algorithm a). Covert organizations: we consider three terrorist network, responsible for the WTC 9/11 attacks <ref type=""bibr"" target=""#b17"">[19]</ref>; the 2002 Bali attack <ref type=""bibr"" target=""#b11"">[13]<",0
"pe=""bibr"" target=""#b23"">[25]</ref>, Betweenness <ref type=""bibr"" target=""#b24"">[26]</ref>, Walktrap <ref type=""bibr"" target=""#b27"">[29]</ref>, Louvain <ref type=""bibr"" target=""#b4"">[6]</ref>, Greedy <",0
"vides tolerance against incorrect labels.</p><p>The recently introduced transform/stability loss of <ref type=""bibr"" target=""#b20"">Sajjadi et al. (2016b)</ref> is based on the same principle as our wo rements is ∼0.5 percentage points better than independent flips.</p><p>A principled comparison with <ref type=""bibr"" target=""#b20"">Sajjadi et al. (2016b)</ref> is difficult due to several reasons. The paths, and comparing the outputs of the network instead of pre-activation data of the final layer. <ref type=""bibr"" target=""#b20"">Sajjadi et al. (2016b)</ref> recently introduced a new loss function",1
"ed results should indicate the upper bound of obtainable accuracy. 36.02 ± 0.10 Virtual Adversarial <ref type=""bibr"" target=""#b14"">(Miyato et al., 2016)</ref> 24.63 ADGM <ref type=""bibr"" target=""#b12""",0
"ase of it. The Π-model can also be seen as a simplification of the Γ-model of the ladder network by <ref type=""bibr"" target=""#b17"">Rasmus et al. (2015)</ref>, a previously presented network architectu he data is obtained.</p><p>Our approach is somewhat similar to the Γ-model of the ladder network by <ref type=""bibr"" target=""#b17"">Rasmus et al. (2015)</ref>, but conceptually simpler. In the Π-model, ed the issue by shuffling the input sequences in such a way that stratification is guaranteed, e.g. <ref type=""bibr"" target=""#b17"">Rasmus et al. (2015)</ref> (confirmed from the authors). This kind of he ones that are most directly connected to our work.</p><p>Γ-model is a subset of a ladder network <ref type=""bibr"" target=""#b17"">(Rasmus et al., 2015)</ref> that introduces lateral connections into",0
"regating, or bagging, multiple networks are trained independently based on subsets of training data <ref type=""bibr"" target=""#b1"">(Breiman, 1996)</ref>. This results in an ensemble that is more stable",0
"get=""#b40"">41]</ref> and modelling short-text similarities <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b28"">29,</ref><ref type=""bibr"" tar e=""bibr"" target=""#b30"">31]</ref> or representation-focused <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b34"">[35]</ref><ref type=""bibr"" ta the distributed model improves drastically in the presence of more data. Unlike some previous work <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" ta nt. Our n-graph based input encoding is motivated by the trigraph encoding proposed by Huang et al. <ref type=""bibr"" target=""#b15"">[16]</ref>, but unlike their approach we don't limit our input repres on the retrieval task, as a baseline in this paper. Both the deep structured semantic model (DSSM) <ref type=""bibr"" target=""#b15"">[16]</ref> and its convolutional variant CDSSM <ref type=""bibr"" targe lated papers that use short text such as title, for document ranking or related tasks. Huang et al. <ref type=""bibr"" target=""#b15"">[16]</ref> learn a distributed representation of query and title, for",1
"nteraction-focused <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b30"">31]</ref> or representation-focused <ref type=""bibr"" target=""#b14"">[1 h positions. It is also similar to the indicator matching matrix proposed previously by Pang et al. <ref type=""bibr"" target=""#b30"">[31]</ref>. While the interaction matrix X perfectly captures every q <ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b33"">34]</ref>.</p><p>Pang et al. <ref type=""bibr"" target=""#b30"">[31]</ref> propose the use of matching matrices to represent the simi",1
"type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b37"">38]</ref>, and entity extraction <ref type=""bibr"" target=""#b8"">[9]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=",0
"alysis (LSA) <ref type=""bibr"" target=""#b4"">[5]</ref>, probabilistic latent semantic analysis (PLSA) <ref type=""bibr"" target=""#b13"">[14]</ref> and latent Dirichlet allocation (LDA) <ref type=""bibr"" tar",0
"pace, then it is their distributed representations that are compared. Along these lines, Guo et al. <ref type=""bibr"" target=""#b11"">[12]</ref> classify recent DNN models for short-text matching as eith T was constrained by memory requirements, and we pick 499 for our experiments.</p><p>The DRMM model <ref type=""bibr"" target=""#b11"">[12]</ref> uses a DNN to perform term matching, with few hundred para y using the match matrix to generate summary statistics it is possible to make the method work well <ref type=""bibr"" target=""#b11"">[12]</ref>, which is our DRMM baseline.</p><p>These term embeddings a ve results have been reported for title-based DSSM and CDSSM on the ad hoc document retrieval tasks <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b29"">30]</ref>, we included docum",0
"set to 0.20 and 0.01, respectively, based on a validation set. We implemented our model using CNTK <ref type=""bibr"" target=""#b39"">[40]</ref> and trained the model with stochastic gradient descent bas",0
"or document ranking is an important challenge in IR. However, as was noted during a recent workshop <ref type=""bibr"" target=""#b3"">[4]</ref>, despite the recent surge in interests towards applying deep",0
"term matches between the query and the document are fundamental to all information retrieval models <ref type=""bibr"" target=""#b6"">[7]</ref>. Traditional IR models, such as BM25 <ref type=""bibr"" target",0
"dels for short-text matching as either interaction-focused <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b30"">31]</ref> or representation-f",0
", it is possible to do document ranking using a distributed representation of terms. Mikolov et al. <ref type=""bibr"" target=""#b23"">[24]</ref> developed the popular word2vec embedding approach that has",0
"compared to DNNs <ref type=""bibr"" target=""#b12"">[13]</ref>. Recently, very deep CNNs architectures <ref type=""bibr"" target=""#b13"">[14]</ref> have also been shown to be successful in ASR <ref type=""bi R, recently there have been several advancements in the computer vision community on very deep CNNs <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b17"">18]</ref> that have not been TMs.</p><p>We are driven by same motivation that led to the success of very deep networks in vision <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" ta",1
"cements in the computer vision community on very deep CNNs <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b17"">18]</ref> that have not been * Work done as Google Brain interns. exp build such deeper models. NiN has seen great success in computer vision, building very deep models <ref type=""bibr"" target=""#b17"">[18]</ref>. We show how to apply NiN principles in hierarchical Recur on that led to the success of very deep networks in vision <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" tar",1
"oped for classical ASR <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target= nd decoder models using recurrent models with LSTMs <ref type=""bibr"" target=""#b5"">[6]</ref> or GRUs <ref type=""bibr"" target=""#b3"">[4]</ref>. However, their use of hierarchy in the encoders demonstrate eep CNN techniques to significantly improve over previous shallow seq2seq speech recognition models <ref type=""bibr"" target=""#b3"">[4]</ref>. Our best model achieves a WER of 10.53% where our baseline oder depth of the baseline model without using any convolutional layers. Our baseline model follows <ref type=""bibr"" target=""#b3"">[4]</ref> using the skip connection technique in its time reduction. T btained 10.5% WER without a language model, an 8.5% absolute improvement over published best result <ref type=""bibr"" target=""#b3"">[4]</ref>. While we demonstrated our results only on the seq2seq task,",1
"esteps the complicated machinery developed for classical ASR <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target= w.tei-c.org/ns/1.0""><head n=""2.1."">Listen, Attend and Spell</head><p>Listen, Attend and Spell (LAS) <ref type=""bibr"" target=""#b2"">[3]</ref> is an attention-based seq2seq model which learns to transcri TM) <ref type=""bibr"" target=""#b24"">[25]</ref> network with hierarchical subsampling as described in <ref type=""bibr"" target=""#b2"">[3]</ref>. In our work, we replace Listen with a network of very deep lower dimension and apply BN and ReLU non-linearity to replace the skip subsampling connections in <ref type=""bibr"" target=""#b2"">[3]</ref>. Moreover, we further increase the depth of the network by a",0
"BN speeds up training and acts as an regularizer. BN has also seen success in endto-end CTC models <ref type=""bibr"" target=""#b21"">[22]</ref>. The seq2seq attention mechanism <ref type=""bibr"" target="" e we construct a minibatch containing multiple utterances, we follow the sequencewise normalization <ref type=""bibr"" target=""#b21"">[22]</ref>. For each output channel, we compute the mean and variance",0
"new direction for ASR that entirely sidesteps the complicated machinery developed for classical ASR <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target",0
"ied to many ASR tasks <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b10"">11]</ref>. Unlike Deep Neural Networks (DNNs) <ref type=""bibr"" target nal fully-connected deep neural networks on many ASR tasks <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b10"">11]</ref>, we investigate the effect of convolutional layers in seq2s",0
"7"">[18]</ref>. We show how to apply NiN principles in hierarchical Recurrent Neural Networks (RNNs) <ref type=""bibr"" target=""#b19"">[20]</ref>.</p><p>2. Batch Normalization (BN) <ref type=""bibr"" target",0
""" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b5"">6]</ref>. It is able to do this ted WER on WSJ without an LM was the seq2seq model with Task Loss Estimation achieving 18.0% WER in <ref type=""bibr"" target=""#b4"">[5]</ref>. Our baseline, also a seq2seq model, achieved 14.76% WER. Ou /ref>. Our baseline, also a seq2seq model, achieved 14.76% WER. Our model is different from that of <ref type=""bibr"" target=""#b4"">[5]</ref> in that we did not use location-based priors on the attentio",0
"3"">[14]</ref> have also been shown to be successful in ASR <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b16"">17]</ref>, using more non-lin addition of context window for each frame, or a way to treat the full utterance as a single sample <ref type=""bibr"" target=""#b15"">[16]</ref>. One advantage of the seq2seq model is that the encoder ca",0
"rks (CNNs) <ref type=""bibr"" target=""#b7"">[8]</ref> have been successfully applied to many ASR tasks <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" targe",0
"1e−4 after it converged. We used 10 GPU workers for asynchronous SGD under the TensorFlow framework <ref type=""bibr"" target=""#b31"">[32]</ref>. We monitor the dev93 Word Error Rate (WER) until converge",0
"nd apply some of these techniques in our end-to-end speech model:</p><p>1. Network-in-Network (NiN) <ref type=""bibr"" target=""#b18"">[19]</ref> increases network depth through the use of 1x1 convolution",0
"re these skip connections to build deeper acoustic encoders.</p><p>4. Convolutional LSTM (ConvLSTM) <ref type=""bibr"" target=""#b23"">[24]</ref> use convolutions to replace the inner products within the duct of the LSTM with convolutions.</p><p>The Convolutional LSTM (ConvLSTM) was first introduced in <ref type=""bibr"" target=""#b23"">[24]</ref>. Although the fully connected LSTM layer has proven powerf",0
"show how BN can be applied to seq2seq acoustic model encoders.</p><p>3. Residual Networks (ResNets) <ref type=""bibr"" target=""#b22"">[23]</ref> learns a residual function of the input through the usage ient problem. In this study, we use a residual CNN/LSTM, to train deeper networks. Residual network <ref type=""bibr"" target=""#b22"">[23]</ref> contains direct links between the lower layer outputs and et=""#b13"">[14,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b22"">23]</ref> -add depth of processing using more non-linearities and exp proposed recently to enable training of very deep networks <ref type=""bibr"" target=""#b26"">[27,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" tar",0
"), the Super-Resolution Convolutional Neural Network (SRCNN) <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref> has demonstrated superior performance to the previous hand-cr m, the Super-Resolution Convolutional Neural Network (SRCNN) <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref> has drawn considerable attention due to its simple network st Convolutional Neural Network (SRCNN) proposed by Dong et al. <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref>. Motivated by SRCNN, some problems such as face hallucination ><p>We first briefly describe the network structure of SRCNN <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref>, and then we detail how we reformulate the network layer by l Different Upscaling Factors</head><p>Unlike existing methods <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref> that need to train a network from scratch for a different sca lgorithms are mostly learning-based (or patch-based) methods <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target= space, then followed by a complex mapping to another high-dimensional HR feature space. Dong et al. <ref type=""bibr"" target=""#b1"">[2]</ref> show that the mapping accuracy can be substantially improved a wider mapping layer, but at the cost of the running time. For example, the large SRCNN (SRCNN-Ex) <ref type=""bibr"" target=""#b1"">[2]</ref> has 57,184 parameters, which are six times larger than that with no pre-processing. 2) The proposed model achieves a speed up of at least 40× than the SRCNN-Ex <ref type=""bibr"" target=""#b1"">[2]</ref> while still keeping its exceptional performance. One of its ters in a layer) and depth (i.e., the number of layers) of the mapping layer. As indicated in SRCNN <ref type=""bibr"" target=""#b1"">[2]</ref>, a 5 × 5 layer achieves much better results than a 1 × 1 lay s an average PSNR of 32.87 dB, which is already higher than that of SRCNN-Ex (32.75 dB) reported in <ref type=""bibr"" target=""#b1"">[2]</ref>. The FSRCNN (48,12,2) contains only 8,832 parameters, then t F) <ref type=""bibr"" target=""#b6"">[7]</ref>, SRCNN <ref type=""bibr"" target=""#b0"">[1]</ref>, SRCNN-Ex <ref type=""bibr"" target=""#b1"">[2]</ref> and the sparse coding based network (SCN) <ref type=""bibr"" t",1
"16"">[17]</ref> have achieved state-of-the-art results. Deeper structures have also been explored in <ref type=""bibr"" target=""#b17"">[18]</ref> and <ref type=""bibr"" target=""#b18"">[19]</ref>. Different f no loss of mapping accuracy. Furthermore, all these networks <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b18"">19]</ref> need to process the",1
"/ref>. The results of PSNR (dB), SSIM and IFC <ref type=""bibr"" target=""#b28"">[29]</ref> on the Set5 <ref type=""bibr"" target=""#b29"">[30]</ref>, Set14 <ref type=""bibr"" target=""#b8"">[9]</ref> and BSD200",0
"such as face hallucination <ref type=""bibr"" target=""#b15"">[16]</ref> and depth map super-resolution <ref type=""bibr"" target=""#b16"">[17]</ref> have achieved state-of-the-art results. Deeper structures",0
"ation kernel like in FCN <ref type=""bibr"" target=""#b12"">[13]</ref>, or 'unpooling+convolution' like <ref type=""bibr"" target=""#b13"">[14]</ref>. Instead, it consists of diverse automatically learned ups or bilinear) in-network <ref type=""bibr"" target=""#b12"">[13]</ref> or having 'unpooling+convolution' <ref type=""bibr"" target=""#b13"">[14]</ref>, the deconvolution layer learns a set of upsampling kernel",0
", the FSRCNN-s can run in real-time (&gt; 24 fps) on a generic CPU. The chart is based on the Set14 <ref type=""bibr"" target=""#b8"">[9]</ref> results summarized in Tables <ref type=""table"" target=""#tab_ dataset. Following SRCNN and SCN, we use the Set5 <ref type=""bibr"" target=""#b14"">[15]</ref>, Set14 <ref type=""bibr"" target=""#b8"">[9]</ref> and BSD200 <ref type=""bibr"" target=""#b24"">[25]</ref> dataset f type=""bibr"" target=""#b28"">[29]</ref> on the Set5 <ref type=""bibr"" target=""#b29"">[30]</ref>, Set14 <ref type=""bibr"" target=""#b8"">[9]</ref> and BSD200 <ref type=""bibr"" target=""#b24"">[25]</ref>   </p><",0
"results. Deeper structures have also been explored in <ref type=""bibr"" target=""#b17"">[18]</ref> and <ref type=""bibr"" target=""#b18"">[19]</ref>. Different from the conventional learning-based methods, S , all these networks <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b18"">19]</ref> need to process the bicubic-upscaled LR images. The propose",0
"l applied in image super-resolution (SR), the Super-Resolution Convolutional Neural Network (SRCNN) <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref> has demonstrated super en low-resolution (LR) one. Recent SR algorithms are mostly learning-based (or patch-based) methods <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target n the LR and HR image spaces. Among them, the Super-Resolution Convolutional Neural Network (SRCNN) <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref> has drawn considerable work is termed as the Super-Resolution Convolutional Neural Network (SRCNN) proposed by Dong et al. <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref>. Motivated by SRCNN, s =""3"">Fast Super-Resolution by CNN</head><p>We first briefly describe the network structure of SRCNN <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref>, and then we detail ho /ns/1.0""><head n=""4.4"">Experiments for Different Upscaling Factors</head><p>Unlike existing methods <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref> that need to train a n ory. For example, to upsample an 240 × 240 image by a factor of 3, the speed of the original SR-CNN <ref type=""bibr"" target=""#b0"">[1]</ref> is about 1.32 fps, which is far from real-time (24 fps). To rsion can run in real-time (&gt;24 fps) on a generic CPU with better restoration quality than SRCNN <ref type=""bibr"" target=""#b0"">[1]</ref>. 3) We transfer the convolution layers of the proposed netwo tisfying the real-time requirement. Furthermore, the FSRCNN (32,5,1) even outperforms SRCNN (9-1-5) <ref type=""bibr"" target=""#b0"">[1]</ref> (see Table <ref type=""table"" target=""#tab_4"">3 and 4</ref>). databases, namely the super-resolution forest (SRF) <ref type=""bibr"" target=""#b6"">[7]</ref>, SRCNN <ref type=""bibr"" target=""#b0"">[1]</ref>, SRCNN-Ex <ref type=""bibr"" target=""#b1"">[2]</ref> and the sp rningbased SR methods <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b0"">1]</ref>. As deep models generally benefit from big data, studies have",0
"ecting the sub-image size is the deconvolution layer. As we train our models with the Caffe package <ref type=""bibr"" target=""#b26"">[27]</ref>, its deconvolution filters will generate the output with s",0
"chers' noisy votes; for this purpose, we use the state-of-the-art moments accountant technique from <ref type=""bibr"" target=""#b0"">Abadi et al. (2016)</ref>, which tightens the privacy bound when the t effect, limiting applicability to logistic regression with convex loss. Also, unlike the methods of <ref type=""bibr"" target=""#b0"">Abadi et al. (2016)</ref>, which represent the state-of-the-art in dif e a privacy/utility tradeoff that equals or improves upon bespoke learning methods such as those of <ref type=""bibr"" target=""#b0"">Abadi et al. (2016)</ref>.</p><p>Section 5 further discusses the relat g the need for supervision. • We present a new application of the moments accountant technique from <ref type=""bibr"" target=""#b0"">Abadi et al. (2016)</ref> for improving the differential-privacy analy (8.19, 10 −6 ) for SVHN, respectively with accuracy of 98.00% and 90.66%. In comparison, for MNIST, <ref type=""bibr"" target=""#b0"">Abadi et al. (2016)</ref> obtain a looser (8, 10 −5 ) privacy bound an y cost, we use recent advances in privacy cost accounting. The moments accountant was introduced by <ref type=""bibr"" target=""#b0"">Abadi et al. (2016)</ref>, building on previous work <ref type=""bibr"" rivacy loss random variable.</p><p>The following properties of the moments accountant are proved in <ref type=""bibr"" target=""#b0"">Abadi et al. (2016)</ref>.</p><p>Theorem 1. 1. [Composability] Suppose 00% and 90.66%. These results improve the differential privacy state-of-the-art for these datasets. <ref type=""bibr"" target=""#b0"">Abadi et al. (2016)</ref> previously obtained 97% accuracy with a (8, he large number of parameters prevents the technique from providing a meaningful privacy guarantee. <ref type=""bibr"" target=""#b0"">Abadi et al. (2016)</ref> provided stricter bounds on the privacy loss we keep track of the privacy budget throughout the student's training using the moments accountant <ref type=""bibr"" target=""#b0"">(Abadi et al., 2016)</ref>. When teachers reach a strong quorum, this e cost of assuming that nonprivate unlabeled data is available, an assumption that is not shared by <ref type=""bibr"" target=""#b0"">(Abadi et al., 2016;</ref><ref type=""bibr"" target=""#b33"">Shokri &amp;",1
"n.</p><p>Siamese networks: Our first attempt was to train a pair of siamese networks, introduced by <ref type=""bibr"" target=""#b6"">Bromley et al. (1993)</ref> in the context of one-shot learning and la",0
"ggregation and transfer <ref type=""bibr"" target=""#b5"">(Breiman, 1994)</ref>, previously explored by <ref type=""bibr"" target=""#b28"">Nissim et al. (2007)</ref>, <ref type=""bibr"" target=""#b29"">Pathak et self data-dependent and should not be revealed. To get around this, we bound the smooth sensitivity <ref type=""bibr"" target=""#b28"">(Nissim et al., 2007)</ref> of the moments and add noise proportional",0
"larly <ref type=""bibr"" target=""#b20"">Hamm et al. (2016)</ref>. In this strategy, first, an ensemble <ref type=""bibr"" target=""#b11"">(Dietterich, 2000)</ref> of teacher models is trained on disjoint sub",0
"tion problem.</p><p>Learning upscaling filters was briefly suggested in the footnote of Dong et.al. <ref type=""bibr"" target=""#b5"">[6]</ref>. However, the importance of integrating it into the CNN as p eration was not fully recognised and the option not explored. Additionally, as noted by Dong et al. <ref type=""bibr"" target=""#b5"">[6]</ref>, there are no efficient implementations of a convolution lay uate the power of the sub-pixel convolution layer by comparing against SRCNN's standard 9-1-5 model <ref type=""bibr"" target=""#b5"">[6]</ref>. Here, we follow the approach in <ref type=""bibr"" target=""#b CNN's standard 9-1-5 model <ref type=""bibr"" target=""#b5"">[6]</ref>. Here, we follow the approach in <ref type=""bibr"" target=""#b5"">[6]</ref>, using relu as the activation function for our models in thi ard comparison with results from previous published results<ref type=""bibr"" target=""#b30"">[31,</ref><ref type=""bibr"" target=""#b5"">6]</ref>.</note> 		</body> 		<back> 			<div type=""references"">  				<l",1
"ty of images. The network structure is not limited to neural networks, for example, a random forest <ref type=""bibr"" target=""#b30"">[31]</ref> has also been successfully used for SISR. With the develop hannel in YCbCr colour space in this section because humans are more sensitive to luminance changes <ref type=""bibr"" target=""#b30"">[31]</ref>. For each upscaling factor, we train a specific network.</ t=""#b2"">[3]</ref>. For the interested reader, the results of other previous methods can be found in <ref type=""bibr"" target=""#b30"">[31]</ref>. We choose to compare against the best SRCNN 9-5-5 ImageNe et=""#b38"">39]</ref> from the Matlab codes provided by <ref type=""bibr"" target=""#b39"">[40]</ref> and <ref type=""bibr"" target=""#b30"">[31]</ref>. For methods which use convolutions including our own, a p rget=""#b26"">[27,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b43"">44,</ref><ref type=""bibr"" target=""#b30"">31,</ref><ref type=""bibr"" target=""#b2"">3]</ref>. To super-resolve a L evious works including <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b30"">31]</ref>. We show that the proposed model achieves state-of-art perf ed here in order to allow a straight-forward comparison with results from previous published results<ref type=""bibr"" target=""#b30"">[31,</ref><ref type=""bibr"" target=""#b5"">6]</ref>.</note> 		</body>",0
"ef> have recently also shown promise for SISR. These methods, employ the back-propagation algorithm <ref type=""bibr"" target=""#b21"">[22]</ref> to train on large image databases such as ImageNet <ref ty",0
"elated Work</head><p>The goal of SISR methods is to recover a HR image from a single LR input image <ref type=""bibr"" target=""#b13"">[14]</ref>. Recent popular SISR methods can be classified into edge-b",0
"orithm <ref type=""bibr"" target=""#b21"">[22]</ref> to train on large image databases such as ImageNet <ref type=""bibr"" target=""#b29"">[30]</ref> in order to learn nonlinear mappings of LR and HR image pa ides 136 texture images. For our final models, we use 50,000 randomly selected images from ImageNet <ref type=""bibr"" target=""#b29"">[30]</ref> for the training. Following previous works, we only consid training takes roughly three hours on a K2 GPU on 91 images, and seven days on images from ImageNet <ref type=""bibr"" target=""#b29"">[30]</ref> for upscaling factor of 3. We use the PSNR as the performa",0
"oupled Compressed Cache (DCC) [Sardashti and Wood 2013a,  2013b]  and Skewed Compressed Cache (SCC) <ref type=""bibr"" target=""#b26"">[Sardashti et al. 2014</ref>]-with a more practical and simpler desig br"">[Sardashti and</ref><ref type=""bibr"">Wood 2013a, 2013b]</ref> and Skewed Compressed Cache (SCC) <ref type=""bibr"" target=""#b26"">[Sardashti et al. 2014</ref>]-try to achieve some of these goals, alt he additional area and complexity of backward pointers to maintain the decoupled mapping.</p><p>SCC <ref type=""bibr"" target=""#b26"">[Sardashti et al. 2014</ref>] also uses super-block tags but eliminat (i.e., backward pointers) and the need to separately manage block and super-block replacements. SCC <ref type=""bibr"" target=""#b26"">[Sardashti et al. 2014</ref>] eliminates DCC's backward pointers and =""http://www.tei-c.org/ns/1.0""><head n=""2.3."">SCC: A State-of-the-Art Compressed Cache</head><p>SCC <ref type=""bibr"" target=""#b26"">[Sardashti et al. 2014</ref>] is a state-of-the-art compressed cache >Kim et al. 2002;</ref><ref type=""bibr"">Sardashti and</ref><ref type=""bibr"">Wood 2013a, 2013b;</ref><ref type=""bibr"" target=""#b26"">Sardashti et al. 2014]</ref>.</p><p>A compressed cache organization m ACC design in more detail.</p><p>3.1.1 YACC Tag Format. Like SCC, YACC uses sparse super-block tags <ref type=""bibr"" target=""#b26"">[Sardashti et al. 2014]</ref> to map the blocks that are compressed i ef type=""table"">II</ref> shows the main parameters. We use 64-byte cache block sizes. For YACC, SCC <ref type=""bibr"" target=""#b26"">[Sardashti et al. 2014]</ref> and DCC <ref type=""bibr"">[Sardashti and pe=""bibr"">Wood 2013a, 2013b]</ref>. We consider this extra overhead in our simulation. ? SCC models <ref type=""bibr"" target=""#b26"">[Sardashti et al. 2014]</ref> with four-block super-blocks and 16-byt xtra hardware complexity induced for storing and retrieving compressed data blocks. Compared to SCC <ref type=""bibr"" target=""#b26"">[Sardashti et al. 2014]</ref>, we addressed most of the issues of the",1
"-byte sub-blocks (i.e., each block compresses from zero to four sub-blocks).</p><p>We use CACTI 6.5 <ref type=""bibr"" target=""#b8"">[CACTI 2008]</ref> to model power at 32nm. We also use a detailed DRAM",0
"variable-size compressed cache blocks <ref type=""bibr"" target=""#b3"">[Alameldeen and Wood 2004;</ref><ref type=""bibr"" target=""#b13"">Hallnor and Reinhardt 2005;</ref><ref type=""bibr"" target=""#b17"">Kim e rget=""#b3"">[Alameldeen and Wood 2004]</ref>), and in others, they may be noncontiguous (e.g., IIC-C <ref type=""bibr"" target=""#b13"">[Hallnor and Reinhardt 2005]</ref> and DCC <ref type=""bibr"">[Sardasht",0
"r"" target=""#b3"">[Alameldeen and Wood 2004;</ref><ref type=""bibr"" target=""#b38"">Ziv et al. 1977</ref><ref type=""bibr"" target=""#b39"">Ziv et al. , 1978;;</ref><ref type=""bibr"" target=""#b14"">Huffman 1952;",0
"tences contribute to the classification decision which can be of value in applications and analysis <ref type=""bibr"" target=""#b21"">(Shen et al., 2014;</ref><ref type=""bibr"">Gao et al., 2014)</ref>.</p",1
"ludes two levels of attention mechanisms <ref type=""bibr"" target=""#b0"">(Bahdanau et al., 2014;</ref><ref type=""bibr"" target=""#b29"">Xu et al., 2015)</ref> -one at the word level and one at the sentence o select the reference words in original language for words in foreign language before translation. <ref type=""bibr"" target=""#b29"">Xu et al. (2015)</ref> uses the attention mechanism in image caption",1
"( §3.5). To include sensitivity to this fact, our model includes two levels of attention mechanisms <ref type=""bibr"" target=""#b0"">(Bahdanau et al., 2014;</ref><ref type=""bibr"" target=""#b29"">Xu et al., <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2.1"">GRU-based sequence encoder</head><p>The GRU <ref type=""bibr"" target=""#b0"">(Bahdanau et al., 2014)</ref> uses a gating mechanism to track the sta rds to vectors through an embedding matrix W e , x ij = W e w ij .</p><p>We use a bidirectional GRU <ref type=""bibr"" target=""#b0"">(Bahdanau et al., 2014)</ref> to get annotations of words by summarizi f type=""bibr"" target=""#b14"">(Lin et al., 2015)</ref>.</p><p>The attention mechanism was proposed by <ref type=""bibr"" target=""#b0"">(Bahdanau et al., 2014)</ref> in machine translation. The encoder deco",1
"2015;</ref><ref type=""bibr"" target=""#b4"">Hermann et al., 2015)</ref>, and image question answering <ref type=""bibr"" target=""#b30"">(Yang et al., 2015)</ref>. Unlike these works, we explore a hierarchi",0
"on of a fixed query ""what is the informative word"" over the words like that used in memory networks <ref type=""bibr"" target=""#b23"">(Sukhbaatar et al., 2015;</ref><ref type=""bibr"" target=""#b10"">Kumar e ng <ref type=""bibr"" target=""#b27"">(Vinyals et al., 2014)</ref>, natural language question answering <ref type=""bibr"" target=""#b23"">(Sukhbaatar et al., 2015;</ref><ref type=""bibr"" target=""#b10"">Kumar e",0
"hims, 1998)</ref>. More recent approaches used deep learning, such as convolutional neural networks <ref type=""bibr"" target=""#b1"">(Blunsom et al., 2014)</ref> and recurrent neural networks based on lo",0
"ord embeddings of each document are used. SSWE uses sentiment specific word embeddings according to <ref type=""bibr"" target=""#b25"">(Tang et al., 2014)</ref>.</p></div> <div xmlns=""http://www.tei-c.org",0
"ning, 2012)</ref>, sentiment classification <ref type=""bibr"" target=""#b16"">(Maas et al., 2011;</ref><ref type=""bibr"" target=""#b19"">Pang and Lee, 2008)</ref>, and spam detection <ref type=""bibr"" target",0
"arget=""#b23"">(Sukhbaatar et al., 2015;</ref><ref type=""bibr"" target=""#b10"">Kumar et al., 2015;</ref><ref type=""bibr"" target=""#b4"">Hermann et al., 2015)</ref>, and image question answering <ref type=""b",0
"a more complete model we could use a GRU to get word vectors directly from characters, similarly to <ref type=""bibr"" target=""#b15"">(Ling et al., 2015)</ref>. We omitted this for simplicity.</p><p>Word",0
"ecommend. Although neural-network-based approaches to text classification have been quite effective <ref type=""bibr"" target=""#b8"">(Kim, 2014;</ref><ref type=""bibr"" target=""#b31"">Zhang et al., 2015;</r =""bibr"" target=""#b31"">(Zhang et al., 2015)</ref>.</p><p>CNN-word Word based CNN models like that of <ref type=""bibr"" target=""#b8"">(Kim, 2014)</ref> are used. CNN-char Character level CNN models are re a multiclass setting, that is, detection happens before the selection of the topic! 4 Related Work <ref type=""bibr"" target=""#b8"">Kim (2014)</ref> use neural networks for text classification. The arch",0
"igrams and bagof-bigrams as features respectively.</p><p>Text Features are constructed according to <ref type=""bibr"" target=""#b9"">(Kiritchenko et al., 2014)</ref>, including word and character n-grams",0
"target=""#b31"">Zhang et al., 2015;</ref><ref type=""bibr"" target=""#b7"">Johnson and Zhang, 2014;</ref><ref type=""bibr"" target=""#b26"">Tang et al., 2015)</ref>, in this paper we test the hypothesis that b seline methods and results are reported in <ref type=""bibr"" target=""#b31"">(Zhang et al., 2015;</ref><ref type=""bibr"" target=""#b26"">Tang et al., 2015)</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ d otherwise.</p><p>Yelp reviews are obtained from the Yelp Dataset Challenge in 2013, 2014 and 2015 <ref type=""bibr"" target=""#b26"">(Tang et al., 2015)</ref>. There are five levels of ratings from 1 to mlns=""http://www.tei-c.org/ns/1.0""><head n=""3.2.2"">SVMs</head><p>SVMs-based methods are reported in <ref type=""bibr"" target=""#b26"">(Tang et al., 2015)</ref>, including SVM+Unigrams, Bigrams, Text Feat 0""><head n=""3.2.3"">Neural Network methods</head><p>The neural network based methods are reported in <ref type=""bibr"" target=""#b26"">(Tang et al., 2015)</ref> and <ref type=""bibr"" target=""#b31"">(Zhang e states of all words is used as feature for classification. Conv-GRNN and LSTM-GRNN were proposed by <ref type=""bibr"" target=""#b26"">(Tang et al., 2015)</ref>. They also explore the hierarchical structu ibr"" target=""#b11"">(Lai et al., 2015;</ref><ref type=""bibr"" target=""#b32"">Zhou et al., 2015)</ref>. <ref type=""bibr"" target=""#b26"">Tang et al. (2015)</ref> use hierarchical structure in sentiment clas",0
"aas et al., 2011;</ref><ref type=""bibr"" target=""#b19"">Pang and Lee, 2008)</ref>, and spam detection <ref type=""bibr"" target=""#b20"">(Sahami et al., 1998)</ref>. Traditional approaches of text classific",0
"ions when generating words in the captions. Further uses of the attention mechanism include parsing <ref type=""bibr"" target=""#b27"">(Vinyals et al., 2014)</ref>, natural language question answering <re",0
"document vectors. There are some other works that use hierarchical structure in sequence generation <ref type=""bibr"" target=""#b13"">(Li et al., 2015)</ref> and language modeling <ref type=""bibr"" target",0
"CNN structure to for sentence classification <ref type=""bibr"" target=""#b11"">(Lai et al., 2015;</ref><ref type=""bibr"" target=""#b32"">Zhou et al., 2015)</ref>. <ref type=""bibr"" target=""#b26"">Tang et al.",0
"ic labeling <ref type=""bibr"" target=""#b28"">(Wang and Manning, 2012)</ref>, sentiment classification <ref type=""bibr"" target=""#b16"">(Maas et al., 2011;</ref><ref type=""bibr"" target=""#b19"">Pang and Lee,",0
"hes to text classification have been quite effective <ref type=""bibr"" target=""#b8"">(Kim, 2014;</ref><ref type=""bibr"" target=""#b31"">Zhang et al., 2015;</ref><ref type=""bibr"" target=""#b7"">Johnson and Zh et=""#b2"">(Diao et al., 2014)</ref>. The ratings range from 1 to 10. Yahoo answers are obtained from <ref type=""bibr"" target=""#b31"">(Zhang et al., 2015)</ref>. This is a topic We randomly select 10% of opic We randomly select 10% of the training samples as validation. Amazon reviews are obtained from <ref type=""bibr"" target=""#b31"">(Zhang et al., 2015)</ref>. The ratings are from 1 to 5. 3,000,000 re iv> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.2.1"">Linear methods</head><p>Linear methods <ref type=""bibr"" target=""#b31"">(Zhang et al., 2015)</ref> use the constructed statistics as features network based methods are reported in <ref type=""bibr"" target=""#b26"">(Tang et al., 2015)</ref> and <ref type=""bibr"" target=""#b31"">(Zhang et al., 2015)</ref>.</p><p>CNN-word Word based CNN models like ""bibr"" target=""#b8"">(Kim, 2014)</ref> are used. CNN-char Character level CNN models are reported in <ref type=""bibr"" target=""#b31"">(Zhang et al., 2015)</ref>. LSTM takes the whole document as a single , character-based CNN, and Conv-GRNN, LSTM-GRNN. These baseline methods and results are reported in <ref type=""bibr"" target=""#b31"">(Zhang et al., 2015;</ref><ref type=""bibr"" target=""#b26"">Tang et al., -dimensional one hot vector as input. They find that it performs well. Unlike word level modelings, <ref type=""bibr"" target=""#b31"">Zhang et al. (2015)</ref> apply a character-level CNN for text classi",0
"an knowledge about the extraction. For a general introduction of first-order logic, please refer to <ref type=""bibr"" target=""#b11"">[12]</ref>.</p><p>Complete consistency describes the fact that the va",1
"f profile extraction, the problem remains largely unsolved <ref type=""bibr"" target=""#b6"">[7]</ref>, <ref type=""bibr"" target=""#b7"">[8]</ref>, <ref type=""bibr"" target=""#b4"">[5]</ref>. To generate the pr",0
"l.</ref> propose a method to extract user attributes from the pictures posted in social media feeds <ref type=""bibr"" target=""#b17"">[18]</ref>, especially gender information. <ref type=""bibr"" target=""#",0
"eb mining and natural language processing. Related studies <ref type=""bibr"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b2"">[3]</ref>, <ref type=""bibr"" t /head><p>Previously considerable efforts have been made for obtaining user profiles. Back in 1900s, <ref type=""bibr"" target=""#b1"">[2]</ref> discussed algorithms for learning and revising user profiles",0
"For example, F1 score is reported as 92% for the task of homepage finding conducted by Tang et al. <ref type=""bibr"" target=""#b8"">[9]</ref>, and when 87% for extracting profile attributes from the hom erformances achieved by traditional methods are around 90% <ref type=""bibr"" target=""#b4"">[5]</ref>, <ref type=""bibr"" target=""#b8"">[9]</ref>. However, the overall accuracy by combining the two steps in und-truth dataset for quantitative evaluation, we randomly choose 2,000 researchers from AMiner.org <ref type=""bibr"" target=""#b8"">[9]</ref>. Specifically, for extracting Email of each researcher, we s",0
"tures in recent papers <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b18"">19]</ref>. Such deep networks reach top competitive results in the hi riginally used for a face verification task. We also evaluate DCN network architecture described in <ref type=""bibr"" target=""#b18"">[19]</ref> without any pre-training. The faces are resized to a fixed",1
"atures with a linear classifier can achieve good performance on different video analysis benchmarks <ref type=""bibr"" target=""#b5"">[6]</ref>.</p><p>As for task of video-based emotion recognition, few w 3D ConvNets, the operations are performed spatio-temporally by adding an additional time dimension <ref type=""bibr"" target=""#b5"">[6]</ref>. Hence such C3D networks preserve the temporal information o , and 2 fully connected layers, followed by a softmax output layer. Other parameters are similar to <ref type=""bibr"" target=""#b5"">[6]</ref>. The specific C3D structure used in our implementation is sh del alone can achieve good performance in action recognition <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b7"">8]</ref>. And we found that the",1
"rget=""#b2"">[3,</ref><ref type=""bibr"" target=""#b22"">23]</ref> or designing specific CNN-RNN networks <ref type=""bibr"" target=""#b4"">[5]</ref>. Such classifier that takes video sequences as input is beco erm Memory (LSTM) recurrent neural network is widely adopted <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b7"">8]</ref>. LSTM has memory abilit ion recognition using CNN or RNN structures in recent papers <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b18"">19]</ref>. Such deep networks r 18"">19]</ref>. Such deep networks reach top competitive results in the history of EmotiW challenges <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b26"">27]</ref>. Therefore we take s ork shows that either CNN-RNN or C3D model alone can achieve good performance in action recognition <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target",1
"limitation that they just handle spatial information. For example, in the EmotiW 2014 winner's work <ref type=""bibr"" target=""#b3"">[4]</ref>, all video frames are extracted from videos and regarded as rrent neural networks, the Long Short-Term Memory (LSTM) recurrent neural network is widely adopted <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target ew works are given for video-based emotion recognition using CNN or RNN structures in recent papers <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target",1
"bibr"" target=""#b10"">[11]</ref>, VGG <ref type=""bibr"" target=""#b12"">[13]</ref>, and Residual Network <ref type=""bibr"" target=""#b11"">[12]</ref>, which perform well for general objects recognition. Deep-",0
"sks such as handwriting recognition <ref type=""bibr"" target=""#b16"">[17]</ref> or speech recognition <ref type=""bibr"" target=""#b17"">[18]</ref>, they have difficulties in learning long-term dependencies",0
"<ref type=""bibr"" target=""#b0"">[1]</ref>; (2) group level emotion recognition on the HAPPEI database <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b26"">27]</ref>.</p><p>We only parti",0
"e=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b24"">25]</ref>, video action recognition <ref type=""bibr"" target=""#b21"">[22]</ref>, and so on.</p><p>Unlike C3D networks, a few works are giv rm late fusion akin to the two-stream hypothesis following <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b21"">22]</ref>. The performance of the system is around 25% for emotion cl",0
"<ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b28"">29]</ref>, video captioning <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b24"">25]</ref>, video action reco",0
"m is applied to preprocess video frames. Faces of video frames are detected by Viola-Jones cascades <ref type=""bibr"" target=""#b27"">[28]</ref> and falsely detected non-face frames are filtered out by a",0
"l models on optical flow images and perform late fusion akin to the two-stream hypothesis following <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b21"">22]</ref>. The performance o ligned faces as inputs, give a competitive result. Also, optical flow of faces are tested following <ref type=""bibr"" target=""#b20"">[21]</ref>, but no improvements are obtained for emotion classificati",0
"work is widely adopted <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b7"">8]</ref>. LSTM has memory ability and suits for processing sequences w in action recognition <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b7"">8]</ref>. And we found that the CNN-RNN and C3D hybrid network can fur ember or forget the value, and when the unit should output the value. We use LSTM unit described in <ref type=""bibr"" target=""#b7"">[8]</ref> in our method (Figure <ref type=""figure"" target=""#fig_3"">2</ nd  is the hyperbolic tangent function.</p><p>We use a similar framework with the one described in <ref type=""bibr"" target=""#b7"">[8]</ref>, which combines LSTMs with deep convolutional networks to tr",0
"he faces are taken from fc6 layer of VGG16-Face model fine-tuned with FER2013 face emotion database <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b15"">16]</ref>. For each iteratio <ref type=""bibr"" target=""#b9"">[10]</ref>, GoogLeNet <ref type=""bibr"" target=""#b10"">[11]</ref>, VGG <ref type=""bibr"" target=""#b12"">[13]</ref>, and Residual Network <ref type=""bibr"" target=""#b11"">[12]<",0
"<ref type=""bibr"" target=""#b28"">29]</ref>, video captioning <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b24"">25]</ref>, video action recognition <ref type=""bibr"" target=""#b21"">[2",0
"equences versus sequences problems, such as audio analysis <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b28"">29]</ref>, video captioning <ref type=""bibr"" target=""#b23"">[24,</ref>",0
".4.1 (Face-ours) and the face provided in AFEW6.0 (Face-AFEW). We fine-tuned the pretrained sport1m <ref type=""bibr"" target=""#b19"">[20]</ref> C3D model with these two kinds of face images, respectivel",0
"b11"">[12]</ref>, which perform well for general objects recognition. Deep-face VGG model (VGG-FACE) <ref type=""bibr"" target=""#b13"">[14]</ref> is tested for comparisons with VGG model trained on ImageN",0
"6-Face model fine-tuned with FER2013 face emotion database <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b15"">16]</ref>. For each iteration of RNN training, 16 face features are r",0
"also trained a SVM with the linear kernel using audio features extracted with the OpenSmile toolkit <ref type=""bibr"" target=""#b6"">[7]</ref>.</p><p>The CNN-RNN, C3D and audio SVM model were trained sep",0
"models, several mainstream CNN architectures trained with ImageNet are examined, including CaffeNet <ref type=""bibr"" target=""#b9"">[10]</ref>, GoogLeNet <ref type=""bibr"" target=""#b10"">[11]</ref>, VGG <",0
"., few hundreds/thousands of nodes/edges) and uses such counts as features for graph classification <ref type=""bibr"" target=""#b37"">(Vishwanathan, Schraudolph, Kondor and Borgwardt, 2010)</ref>. Previo role discovery <ref type=""bibr"" target=""#b30"">(Rossi and Ahmed, 2015b)</ref>, graph classification <ref type=""bibr"" target=""#b37"">(Vishwanathan et al., 2010)</ref>, and relational learning <ref type= et kernel) is that they scale poorly to large graphs with more than few hundreds/thousands of nodes <ref type=""bibr"" target=""#b37"">(Vishwanathan et al., 2010)</ref>. Thus, our fast algorithms would sp 178 protein graphs) and chemical compound graphs (MUTAG collection of 188 chemical compound graphs) <ref type=""bibr"" target=""#b37"">(Vishwanathan et al., 2010)</ref>. We extract the graphlet features u h kernels have been proposed in machine learning (e.g., graphlet, subtree, and random walk kernels) <ref type=""bibr"" target=""#b37"">(Vishwanathan et al., 2010;</ref><ref type=""bibr"" target=""#b3"">Costa",1
"ble and Cook, 2003)</ref>, as well as using graphlets as features for improving community detection <ref type=""bibr"" target=""#b31"">(Schaeffer, 2007)</ref>, role discovery <ref type=""bibr"" target=""#b30",0
"type=""figure"" target=""#fig_20"">8</ref> shows how we can find large cliques in the terroristRel data <ref type=""bibr"" target=""#b42"">(Zhao, Sen and Getoor, 2006)</ref>.</p></div> <div xmlns=""http://www. e largest clique in the network. Figure8shows how we can find large cliques in the terroristRel data<ref type=""bibr"" target=""#b42"">(Zhao, Sen and Getoor, 2006)</ref>.</figDesc></figure> <figure xmlns=",0
"""#b27"">(Pr?ulj et al., 2004;</ref><ref type=""bibr"" target=""#b23"">Milenkoviae and Pr?ulj, 2008;</ref><ref type=""bibr"" target=""#b13"">Hayes, Sun and Pr?ulj, 2013)</ref>. In this paper, we introduce an ef",0
", V k ? V )</formula><p>together with all the edges whose endpoints are both in this subset (i.e.,  <ref type=""bibr"" target=""#b11"">(Gross, Yellen and Zhang, 2013)</ref>.</p><formula xml:id=""formula_3"" ern ?ki , such that two vertices are connected in ?ki if and only if they are not connected in g ki <ref type=""bibr"" target=""#b11"">(Gross et al., 2013)</ref>. For example, cliques and independent sets s complementary graphlet ?ki in the complement graph ?. In other words, f (g ki , G) = f (? ki , ?) <ref type=""bibr"" target=""#b11"">(Gross et al., 2013)</ref>.</p></div> <div xmlns=""http://www.tei-c.or "">Relationship to Graph/Matrix Reconstruction Theorems</head><p>The graph reconstruction conjecture <ref type=""bibr"" target=""#b11"">(Gross et al., 2013)</ref>, states that an undirected graph G can be Our central principle is that any 4-node graphlet g 4i can be decomposed into four 3-node graphlets <ref type=""bibr"" target=""#b11"">(Gross et al., 2013)</ref>, obtained by deleting one node from g 4i e rge size. Recall that removing a node from a k-star or k-clique forms a star or clique of size k -1 <ref type=""bibr"" target=""#b11"">(Gross et al., 2013)</ref>. Accordingly, edges with large weights are hese problems are NP-hard, e.g., finding the clique of maximum size is a well-known NP-hard problem <ref type=""bibr"" target=""#b11"">(Gross et al., 2013</ref>). To answer these Fig. <ref type=""figure"" t",0
"t harm their representational capacity for object recognition.</p><p>The Single Shot Detector (SSD) <ref type=""bibr"" target=""#b21"">[22]</ref> is one of the first attempts at using a ConvNet's pyramida tiple layers before computing predictions, which is equivalent to summing transformed features. SSD <ref type=""bibr"" target=""#b21"">[22]</ref> and MS-CNN <ref type=""bibr"" target=""#b2"">[3]</ref> predict >[35]</ref>, context modeling <ref type=""bibr"" target=""#b15"">[16]</ref>, stronger data augmentation <ref type=""bibr"" target=""#b21"">[22]</ref>, etc. These improvements are complementary to FPNs and sho",1
"/ref><ref type=""bibr"" target=""#b24"">25]</ref>. They were so critical that object detectors like DPM <ref type=""bibr"" target=""#b6"">[7]</ref> required dense scale sampling to achieve good results (e.g., pyramidal shapes, they are unlike featurized image pyramids <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b33"">34]</ref> where predictions are",0
"s of the COCO competition, including detection, instance segmentation, and keypoint estimation. See <ref type=""bibr"" target=""#b13"">[14]</ref> for details.</p></div> <div xmlns=""http://www.tei-c.org/ns",0
"yers and instead builds the pyramid starting from high up in the network (e.g., conv4 3 of VGG nets <ref type=""bibr"" target=""#b35"">[36]</ref>) and then by adding several new layers. Thus it misses the pendent of the backbone convolutional architectures (e.g., <ref type=""bibr"" target=""#b18"">[19,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" target=""#b15"">16]</ref>), and in this paper",0
"of modern deep ConvNets <ref type=""bibr"" target=""#b18"">[19]</ref>, object detectors like Over-Feat <ref type=""bibr"" target=""#b33"">[34]</ref> and R-CNN <ref type=""bibr"" target=""#b11"">[12]</ref> showed turized image pyramids <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b33"">34]</ref> where predictions are made independently at all levels, see",0
"motifs or graphlets are defined as small induced subgraphs occurring in a bigger network structure <ref type=""bibr"" target=""#b4"">[4,</ref><ref type=""bibr"" target=""#b19"">19,</ref><ref type=""bibr"" targ tanding the higher-order organizational patterns in networks <ref type=""bibr"" target=""#b3"">[3,</ref><ref type=""bibr"" target=""#b4"">4]</ref>. On the algorithmic side, a large amount of research has been",1
"y of social networks <ref type=""bibr"" target=""#b9"">[9,</ref><ref type=""bibr"" target=""#b12"">12,</ref><ref type=""bibr"" target=""#b15"">15]</ref>. In contrast, in the temporal graphs we study here, three n",0
"ay connected forever <ref type=""bibr"" target=""#b2"">[2,</ref><ref type=""bibr"" target=""#b10"">10,</ref><ref type=""bibr"" target=""#b17"">17]</ref> or aggregate temporal information into a sequence of snapsh EMAIL-EU. This dataset is a collection of emails between members of a European research institution <ref type=""bibr"" target=""#b17"">[17]</ref>. An edge (u, v, t) signifies that person u sent person v a",0
"type=""bibr"" target=""#b17"">17]</ref> or aggregate temporal information into a sequence of snapshots <ref type=""bibr"" target=""#b1"">[1,</ref><ref type=""bibr"" target=""#b6"">6,</ref><ref type=""bibr"" target",0
"hermore, motifs are critical for understanding the higher-order organizational patterns in networks <ref type=""bibr"" target=""#b3"">[3,</ref><ref type=""bibr"" target=""#b4"">4]</ref>. On the algorithmic si",0
"ion system to utilize the entire MEDLINE data set. By using state-of-the-art tools, such as ToPMine <ref type=""bibr"" target=""#b12"">[16]</ref> and FastText <ref type=""bibr"" target=""#b6"">[9]</ref>, we a Latent Dirichlet Allocation (LDA) <ref type=""bibr"" target=""#b5"">[8]</ref> and topical phrase mining <ref type=""bibr"" target=""#b12"">[16]</ref>, along with other data mining techniques to conceptually l word phrases from that corpus such as ""asthma a ack,"" allowing us to treat phrases as single tokens <ref type=""bibr"" target=""#b12"">[16]</ref>. Next, we send the corpus through FastText, the most recen urned to the UMLS SPECIALIST NLP toolset <ref type=""bibr"" target=""#b0"">[1]</ref> as well as ToPMine <ref type=""bibr"" target=""#b12"">[16]</ref> and FastText <ref type=""bibr"" target=""#b6"">[9,</ref><ref t o . It is also important to note that we modify the version of ToP-Mine distributed by El-Kishky in <ref type=""bibr"" target=""#b12"">[16]</ref> to allow phrases containing numbers, such as gene names li over weigh a specialized language. However, phrase mining approaches that recover n-grams, such as <ref type=""bibr"" target=""#b12"">[16]</ref>, produce accurate methods without limiting the dictionary.",1
"ch contain knowledge representative of the path as a whole. PLDA+, a scalable implementation of LDA <ref type=""bibr"" target=""#b24"">[28]</ref>, allows us to quickly nd topic models in these clouds. Unl truction process, all share common topics. We perform topic modeling on these documents using PLDA+ <ref type=""bibr"" target=""#b24"">[28]</ref>. e result is a set of plain text topics which represent di s and PLDA+ is a scalable implementation of this algorithm <ref type=""bibr"" target=""#b16"">[20,</ref><ref type=""bibr"" target=""#b24"">28]</ref>. Developed by Zhiyuan Liu et al., PLDA+ quickly identi es g",1
"data sources. We use natural language processing methods, such as Latent Dirichlet Allocation (LDA) <ref type=""bibr"" target=""#b5"">[8]</ref> and topical phrase mining <ref type=""bibr"" target=""#b12"">[16 as their topical similarity using a process they call Phrase LDA.</p><p>Latent Dirichlet Allocation <ref type=""bibr"" target=""#b5"">[8]</ref> is the most common topic modeling process and PLDA+ is a sca",1
"jectives as the minimum logarithmic or linear arrangements <ref type=""bibr"" target=""#b31"">[35,</ref><ref type=""bibr"" target=""#b32"">36]</ref>. On a mixture of K − K, A − K, and A − A edges we anticipat",0
"esis. Initially, DDX3 was a target for the development of anti-viral therapy for the AIDS treatment <ref type=""bibr"" target=""#b21"">[25,</ref><ref type=""bibr"" target=""#b25"">29]</ref>.</p><p>More recent",0
"athering methods struggle to keep up with the growing wealth of forgo en and hard to nd information <ref type=""bibr"" target=""#b13"">[17]</ref>. eir work in the eld of hypothesis generation has included",0
"pplement this subset with concepts from the OMIM <ref type=""bibr"" target=""#b15"">[19]</ref> and GWAS <ref type=""bibr"" target=""#b3"">[6]</ref> databases, two genome speci c data sets. Still, their networ",0
"most recent implementation of word2vec from Milkolov et al. <ref type=""bibr"" target=""#b6"">[9,</ref><ref type=""bibr"" target=""#b19"">23,</ref><ref type=""bibr"" target=""#b26"">30,</ref><ref type=""bibr"" tar PMine <ref type=""bibr"" target=""#b12"">[16]</ref> and FastText <ref type=""bibr"" target=""#b6"">[9,</ref><ref type=""bibr"" target=""#b19"">23]</ref>. Our process for constructing A is summarized in Figure <re ort words, we accept that any n-gram seen by FastText is important. Following examples presented in <ref type=""bibr"" target=""#b19"">[23,</ref><ref type=""bibr"" target=""#b26"">30,</ref><ref type=""bibr"" ta",0
"o verify the ability of MOLIERE to make predictions similar to previous systems with restricted LDA <ref type=""bibr"" target=""#b45"">[49]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head .</p><p>Bio-LDA is a modi cation of LDA which limits the set of keywords to the set present in UMLS <ref type=""bibr"" target=""#b45"">[49]</ref>. is reduction improves the meaning and readability of topi e choices with methods described in Section 6.</p><p>We repeat an experiment done by Wang et al. in <ref type=""bibr"" target=""#b45"">[49]</ref> wherein we discover the implicit connections between the d <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.2"">Venlafaxine to HTR1A</head><p>Wang et al. in <ref type=""bibr"" target=""#b45"">[49]</ref> use a similar topic modeling approach, and nd during one o es, Venlafaxine to HTR1A, and Venlafaxine to HTR2A, we can corroborate the ndings of Wang et al. in <ref type=""bibr"" target=""#b45"">[49]</ref>. We nd that neither pair of keywords is directly connected ion space or the size of their dictionary. For example, specialized versions of LDA such as Bio-LDA <ref type=""bibr"" target=""#b45"">[49]</ref> uncover latent topics using a dictionary that gives a prio generation systems <ref type=""bibr"" target=""#b37"">[41,</ref><ref type=""bibr"" target=""#b40"">44,</ref><ref type=""bibr"" target=""#b45"">49]</ref>. However, most of these improve their performance by restri",0
"e MEDLINE data set <ref type=""bibr"" target=""#b34"">[38,</ref><ref type=""bibr"" target=""#b40"">44,</ref><ref type=""bibr"" target=""#b42"">46]</ref>. To use Arrowsmith, researchers supply two UMLS keywords wh ionally expensive, and queries were typically run on a subset of the MEDLINE data set (according to <ref type=""bibr"" target=""#b42"">[46]</ref> around 1,000 documents).</p><p>Spangler has also been a dr",0
"get=""#b9"">[12,</ref><ref type=""bibr"" target=""#b20"">24,</ref><ref type=""bibr"" target=""#b39"">43,</ref><ref type=""bibr"" target=""#b44"">48]</ref>. Currently, DDX3 is an established target for antitumor dru",0
"We are currently evaluating whether a ground-truth network, like the drug-side-e ect network SIDER <ref type=""bibr"" target=""#b22"">[26]</ref>, can be a good source of such hypotheses. For example, if",0
"to query for concepts intersecting two keywords, is a notable contribution to hypothesis generation <ref type=""bibr"" target=""#b23"">[27]</ref>. is system, proposed by Liu et al., is similar to both our",0
"ibr"" target=""#b4"">[7]</ref>. Our preliminary experiments with scalable time-dependent clustered LDA <ref type=""bibr"" target=""#b17"">[21]</ref> that signi cantly accelerates DTM demonstrate a potential",0
"omedical models <ref type=""bibr"" target=""#b11"">[15]</ref> and digital representations of hypothesis <ref type=""bibr"" target=""#b36"">[40]</ref>.</p><p>Divoli et al. analyze the assumptions made in medic n system must be able to produce reliable results from a set of contradicting information.</p><p>In <ref type=""bibr"" target=""#b36"">[40]</ref>, Soldatova and Rzhetsky describe a standardized way to rep",0
"tput mappings, potentially stochastic, with learnable parameters using directed acyclic graphs (see <ref type=""bibr"" target=""#b38"">Schulman et al. (2015)</ref> for a review). The state of each non-inp",1
"rget=""#b1"">Aitchison, 1985;</ref><ref type=""bibr"" target=""#b34"">Rayens &amp; Srinivasan, 1994;</ref><ref type=""bibr"" target=""#b6"">Favaro et al., 2011)</ref>. Of note is the Logistic Normal distributio rmalized infinitely divisible distributions (S. Favaro, personal communication), and the results of <ref type=""bibr"" target=""#b6"">Favaro et al. (2011)</ref> apply.</p><p>The idea of using a softmax of",0
"have been developed in machine learning <ref type=""bibr"" target=""#b30"">(Paisley et al., 2012;</ref><ref type=""bibr"" target=""#b13"">Gregor et al., 2013;</ref><ref type=""bibr"" target=""#b33"">Ranganath et",0
"target=""#b30"">(Paisley et al., 2012;</ref><ref type=""bibr"" target=""#b13"">Gregor et al., 2013;</ref><ref type=""bibr"" target=""#b33"">Ranganath et al., 2014;</ref><ref type=""bibr"">Mnih &amp; Gregor, 2014",0
"://www.tei-c.org/ns/1.0""><p>We present a variational approximation to the information bottleneck of <ref type=""bibr"" target=""#b33"">Tishby et al. (1999)</ref>. This variational approach allows us to pa oot_2"">3</ref> This approach is known as the information bottleneck (IB), and was first proposed in <ref type=""bibr"" target=""#b33"">Tishby et al. (1999)</ref>. Intuitively, the first term in R IB encou challenging. There are two notable exceptions: the first is when X, Y and Z are all discrete, as in <ref type=""bibr"" target=""#b33"">Tishby et al. (1999)</ref>; this can be used to cluster discrete data",1
"ed somewhat implicit until residual networks <ref type=""bibr"" target=""#b5"">(He et al. (2015)</ref>; <ref type=""bibr"" target=""#b6"">He et al. (2016)</ref>) explicitly introduced a reparameterization of ead><p>Since the advent of residual networks <ref type=""bibr"" target=""#b5"">(He et al. (2015)</ref>; <ref type=""bibr"" target=""#b6"">He et al. (2016)</ref>), most state-of-the-art networks for image clas d of a sequence of such residual blocks. In comparison with the full pre-activation architecture in <ref type=""bibr"" target=""#b6"">He et al. (2016)</ref>, we remove two batch normalization layers and o",1
"er remains elusive. Experiments in <ref type=""bibr"" target=""#b4"">Goodfellow et al. (2014)</ref> and <ref type=""bibr"" target=""#b3"">Dauphin et al. (2014)</ref> suggest that the training objectives have",0
"get=""#b17"">15,</ref><ref type=""bibr"" target=""#b19"">17,</ref><ref type=""bibr"" target=""#b28"">26,</ref><ref type=""bibr"" target=""#b9"">7]</ref>. Unlike traditional supervised learning, dense vectorized rep arget=""#b28"">26,</ref><ref type=""bibr"" target=""#b27"">25,</ref><ref type=""bibr"" target=""#b8"">6,</ref><ref type=""bibr"" target=""#b9"">7]</ref>. Many of these methods are technically inspired by word embed",1
"a><formula xml:id=""formula_8"">)</formula><p>where ? is a positive number usually referred as margin <ref type=""bibr"" target=""#b6"">[4]</ref>. A loss penalty will incur if the score of positive pair f (",0
"e more specific, we consider the problem of author identification under double-blind review setting <ref type=""bibr"" target=""#b13"">[11]</ref>, on which many peer review conferences/journals are based. selection of meta paths.</p><p>The problem of author identification has been briefly studied before <ref type=""bibr"" target=""#b13"">[11]</ref>. And we also notice KDD Cup 2013 has similar author identi",0
"candidate authors.</p><p>To learn the parameters U and w, we use stochastic gradient descent (SGD) <ref type=""bibr"" target=""#b7"">[5]</ref> based on a hinge loss ranking objective. For each triple (p,",0
"rget=""#b15"">13,</ref><ref type=""bibr"" target=""#b36"">34,</ref><ref type=""bibr"" target=""#b11"">9,</ref><ref type=""bibr"" target=""#b35"">33]</ref>.</p><p>Although feature engineering can incorporate prior k rget=""#b15"">13,</ref><ref type=""bibr"" target=""#b36"">34,</ref><ref type=""bibr"" target=""#b11"">9,</ref><ref type=""bibr"" target=""#b35"">33]</ref>, this thread of methods first extract features for each pai rget=""#b15"">13,</ref><ref type=""bibr"" target=""#b36"">34,</ref><ref type=""bibr"" target=""#b11"">9,</ref><ref type=""bibr"" target=""#b35"">33]</ref>, where participants are asked to predict which paper is tru",0
"k i=1 π i /y τ +1 i<label>(3)</label></formula><p>This distribution was independently discovered by <ref type=""bibr"" target=""#b13"">Maddison et al. (2016)</ref>, where it is referred to as the concrete",1
"h as the element-wise mean E p [z] = [π 1 , ..., π k ] of these vectors.</p><p>The Gumbel-Max trick <ref type=""bibr"" target=""#b8"">(Gumbel, 1954;</ref><ref type=""bibr"" target=""#b12"">Maddison et al., 20",1
"o as REINFORCE <ref type=""bibr"" target=""#b26"">(Williams, 1992)</ref> and likelihood ratio estimator <ref type=""bibr"" target=""#b3"">(Glynn, 1990</ref>)) uses the identity ∇ θ p θ (z) = p θ (z)∇ θ log p",0
"n particular, the variance of SF scales linearly with the number of dimensions of the sample vector <ref type=""bibr"" target=""#b20"">(Rezende et al., 2014a)</ref>, making it especially challenging to us",0
"ibr"" target=""#b4"">Graves et al., 2016)</ref>. Discrete representations are often more interpretable <ref type=""bibr"" target=""#b1"">(Chen et al., 2016)</ref> and more computationally efficient <ref type",0
"pretable <ref type=""bibr"" target=""#b1"">(Chen et al., 2016)</ref> and more computationally efficient <ref type=""bibr"" target=""#b18"">(Rae et al., 2016)</ref> than their continuous analogues.</p><p>Howev",0
"ent variables using backpropagation <ref type=""bibr"" target=""#b9"">(Kingma &amp; Welling, 2013;</ref><ref type=""bibr"" target=""#b21"">Rezende et al., 2014b)</ref>. As shown in Figure <ref type=""figure"" t",0
"rget=""#b6"">Gregor et al., 2013)</ref>, or biased path derivative estimators for Bernoulli variables <ref type=""bibr"" target=""#b0"">(Bengio et al., 2013)</ref>. However, no existing gradient estimator h ic sample.</p><p>For Bernoulli variables with mean parameter θ, the Straight-Through (ST) estimator <ref type=""bibr"" target=""#b0"">(Bengio et al., 2013)</ref> approximates m = µ θ (z), implying ∇ θ m = gh (ST) Gumbel Estimator, as it is reminiscent of the biased path derivative estimator described in <ref type=""bibr"" target=""#b0"">Bengio et al. (2013)</ref>. ST Gumbel-Softmax allows samples to be spa",0
"ocused on either score function estimators augmented with Monte Carlo variance reduction techniques <ref type=""bibr"" target=""#b16"">(Paisley et al., 2012;</ref><ref type=""bibr"" target=""#b14"">Mnih &amp;",0
"get=""#b16"">(Paisley et al., 2012;</ref><ref type=""bibr"" target=""#b14"">Mnih &amp; Gregor, 2014;</ref><ref type=""bibr"" target=""#b7"">Gu et al., 2016;</ref><ref type=""bibr"" target=""#b6"">Gregor et al., 201 training stochastic binary networks (SBN) <ref type=""bibr"" target=""#b19"">(Raiko et al., 2014;</ref><ref type=""bibr"" target=""#b7"">Gu et al., 2016;</ref><ref type=""bibr"" target=""#b15"">Mnih &amp; Rezend marize recent stochastic gradient estimators that utilize control variates. We direct the reader to <ref type=""bibr"" target=""#b7"">Gu et al. (2016)</ref> for further detail on these techniques.</p><p>• or non-quadratic f , since it ignores the correction term µ b in the estimator expression. • MuProp <ref type=""bibr"" target=""#b7"">(Gu et al., 2016</ref>) also models the baseline as a first-order Tayl",0
"loss from the image-space to a higher-level feature space of an object recognition system like VGG <ref type=""bibr"" target=""#b48"">[49]</ref>, resulting in sharper results despite lower PSNR values.</ etwork once to get the result. The exclusive use of 3×3 filters is inspired by the VGG architecture <ref type=""bibr"" target=""#b48"">[49]</ref> and allows for deeper models at a low number of parameters map φ, we use a pre-trained implementation of the popular VGG-19 network <ref type=""bibr"">[1,</ref><ref type=""bibr"" target=""#b48"">49]</ref>. It consists of stacked convolutions coupled with pooling l",1
"get=""#b39"">40,</ref><ref type=""bibr"" target=""#b51"">52,</ref><ref type=""bibr"" target=""#b58"">59,</ref><ref type=""bibr"" target=""#b60"">61,</ref><ref type=""bibr"" target=""#b63"">64]</ref> that learn a sparse",0
""" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" tar ef type=""bibr"" target=""#b14"">[15]</ref> and style transfer <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b52"">53]</ref>, however these meth . Alternative perceptual losses have been proposed for CNNs <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b23"">24]</ref> where the idea is to shift the loss from the image-space to target=""#b28"">[29]</ref> developed an approach that is similar to ours: inspired by Johnson et al. <ref type=""bibr"" target=""#b23"">[24]</ref>, they train feed-forward CNNs using a perceptual loss in c beit not pixel-perfect reproductions.  <ref type=""bibr"" target=""#b31"">[32]</ref> and Johnson et al. <ref type=""bibr"" target=""#b23"">[24]</ref> since feed-forward fully convolutional neural networks exh ce</head><p>Dosovitskiy and Brox <ref type=""bibr"" target=""#b9"">[10]</ref> as well as Johnson et al. <ref type=""bibr"" target=""#b23"">[24]</ref> propose a perceptual similarity measure. Rather than compu ts exhibit the same characteristics as previous approaches. The perceptual loss from Johnson et al. <ref type=""bibr"" target=""#b23"">[24]</ref> produces only a slightly sharper image than ENet-E. On the tly sharper images with realistic textures. Comparisons with further works including Johnson et al. <ref type=""bibr"" target=""#b23"">[24]</ref>, Bruna et al. <ref type=""bibr"" target=""#b3"">[4]</ref> and d with VGG, but likely a result of sharper images. Best results shown in bold.</p><p>Johnson et al. <ref type=""bibr"" target=""#b23"">[24]</ref> ENet-PAT IHR Figure <ref type=""figure"">6</ref>. Comparing et-PAT IHR Figure <ref type=""figure"">6</ref>. Comparing our model with a result from Johnson et al. <ref type=""bibr"" target=""#b23"">[24]</ref> on an image from BSD100 at 4x super-resolution. ENet-PAT's etwork that is able to synthesize a global texture (e.g., a given painting style) onto other images <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b52"">53]</ref>, however a single",0
"get=""#b26"">27,</ref><ref type=""bibr"" target=""#b50"">51,</ref><ref type=""bibr"" target=""#b57"">58,</ref><ref type=""bibr"" target=""#b62"">63]</ref>. They further include dictionary-based methods <ref type=""b",0
"et=""#b12"">[13,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b55"">56]</ref> or learn mappings between low and high resolution pairs of",0
"that the model manifold and the true distribution's support have a non-negligible intersection (see <ref type=""bibr"" target=""#b0"">[1]</ref>), and this means that the KL distance is not defined (or sim ining GANs is well known for being delicate and unstable, for reasons theoretically investigated in <ref type=""bibr"" target=""#b0"">[1]</ref>.</p><p>In this paper, we direct our attention on the various zero. This happens to be the case when two low dimensional manifolds intersect in general position <ref type=""bibr"" target=""#b0"">[1]</ref>.</p><p>Since the Wasserstein distance is much weaker than th ing gradients, as can be seen in Figure <ref type=""figure"">1</ref> of this paper and Theorem 2.4 of <ref type=""bibr"" target=""#b0"">[1]</ref>. In Figure <ref type=""figure"" target=""#fig_1"">2</ref> we sho e <ref type=""bibr"" target=""#b3"">[4]</ref>. This last phenomenon has been theoretically explained in <ref type=""bibr"" target=""#b0"">[1]</ref> and highlighted in <ref type=""bibr"" target=""#b10"">[11]</ref> nerated image, when the pixels were already normalized to be in the range <ref type=""bibr"">[0,</ref><ref type=""bibr"" target=""#b0"">1]</ref>. This is a very high amount of noise, so much that when paper",1
"-Encoders (VAEs) <ref type=""bibr"" target=""#b8"">[9]</ref> and Generative Adversarial Networks (GANs) <ref type=""bibr"" target=""#b3"">[4]</ref> are well known examples of this approach. Because VAEs focus s offer much more flexibility in the definition of the objective function, including Jensen-Shannon <ref type=""bibr"" target=""#b3"">[4]</ref>, and all f -divergences <ref type=""bibr"" target=""#b16"">[17]< nator is a sum of deltas on the points the discriminator assigns the highest values, as observed by <ref type=""bibr"" target=""#b3"">[4]</ref> and highlighted in <ref type=""bibr"" target=""#b10"">[11]</ref> N with a convolutional architecture trained with the standard GAN procedure using the − log D trick <ref type=""bibr"" target=""#b3"">[4]</ref>. The generated samples are 3-channel images of 64x64 pixels ningful (DCGAN generator, top right plot) and in other cases collapse to a single nonsensical image <ref type=""bibr"" target=""#b3"">[4]</ref>. This last phenomenon has been theoretically explained in <r 1]</ref> and highlighted in <ref type=""bibr"" target=""#b10"">[11]</ref>. When using the − log D trick <ref type=""bibr"" target=""#b3"">[4]</ref>, the discriminator loss and the generator loss are different",0
"23"">[24]</ref> -a collection of natural images of indoor bedrooms. Our baseline comparison is DCGAN <ref type=""bibr"" target=""#b17"">[18]</ref>, a GAN with a convolutional architecture trained with the tion and constant number of filters at every layer (as opposed to duplicating them every time as in <ref type=""bibr"" target=""#b17"">[18]</ref>). Aside from taking out batch normalization, the number of",0
"p>Now comes the question of finding the function f that solves the maximization problem in equation <ref type=""bibr"" target=""#b1"">(2)</ref>. To roughly approximate this, something that we can do is tr lexity.</p><p>• Generative Moment Matching Networks (GMMNs) <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b1"">2]</ref> are the generative counterpart of MMD. By backproping through",0
"that WGAN training becomes unstable at times when one uses a momentum based optimizer such as Adam <ref type=""bibr"" target=""#b7"">[8]</ref> (with β 1 &gt; 0) on the critic, or when one uses high learn",0
"irectly. Two popular neural network sequence models are Connectionist Temporal Classification (CTC) <ref type=""bibr"" target=""#b10"">[10]</ref> and recurrent models for sequence generation <ref type=""bi",1
"rks (CNNs) <ref type=""bibr"" target=""#b0"">[1]</ref> have achieved great success in acoustic modeling <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target "" target=""#b6"">6]</ref>, like regular Deep Neural Networks (DNNs), which results in a hybrid system <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target the required non-linear modeling capabilities.</p><p>Unlike the time windows applied in DNN systems <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target ://www.tei-c.org/ns/1.0""><head n=""2."">Convolutional Neural Networks</head><p>Most of the CNN models <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target n be very slow due to the iterative multiplications over time when the input sequence is very long; <ref type=""bibr"" target=""#b1"">(2)</ref> The training process is sometimes tricky due to the well-kno",1
"to improve the results for the task of speech recognition <ref type=""bibr"" target=""#b16"">[16,</ref><ref type=""bibr"" target=""#b24"">24,</ref><ref type=""bibr"" target=""#b25"">25,</ref><ref type=""bibr"" tar",0
"experiments were conducted using Theano <ref type=""bibr"" target=""#b35"">[35]</ref>, Blocks and Fuel <ref type=""bibr"" target=""#b36"">[36]</ref>. The authors would like to acknowledge the funding support",0
"type=""bibr"" target=""#b25"">25,</ref><ref type=""bibr"" target=""#b26"">26]</ref> is the maxout function <ref type=""bibr"" target=""#b27"">[27]</ref>. Following the same computational process as in <ref type= function <ref type=""bibr"" target=""#b27"">[27]</ref>. Following the same computational process as in <ref type=""bibr"" target=""#b27"">[27]</ref>, we take the number of piece-wise linear functions as 2 fo",0
"since it helps to reduce spectral variations within the same speaker and between different speakers <ref type=""bibr"" target=""#b28"">[28]</ref>, while pooling in time has been shown to be less helpful <",0
"tei-c.org/ns/1.0""><head n=""1."">Introduction</head><p>Recently, Convolutional Neural Networks (CNNs) <ref type=""bibr"" target=""#b0"">[1]</ref> have achieved great success in acoustic modeling <ref type="" RNN-based end-to-end systems are impressive, there are two important downsides to using RNNs/LSTMs: <ref type=""bibr"" target=""#b0"">(1)</ref> The training speed can be very slow due to the iterative mul",0
"ith multiple layers of LSTMs. The only previous attempt to combine CNNs with CTC that we know about <ref type=""bibr"" target=""#b21"">[21]</ref>, led to results that were far from the state-of-the-art. I",0
"to the well-known problem of gradient vanishing/exploding <ref type=""bibr"" target=""#b17"">[17,</ref><ref type=""bibr"" target=""#b18"">18]</ref>. Although various approaches have been proposed to address",0
"d n=""2.2.2."">Parametric Rectifier Linear Unit</head><p>The Parametric Rectifier Linear Unit (PReLU) <ref type=""bibr"" target=""#b23"">[23]</ref> is an extension of the ReLU in which the output of the mod",0
"=""#b5"">[6]</ref>, <ref type=""bibr"" target=""#b6"">[7]</ref>, <ref type=""bibr"" target=""#b7"">[8]</ref>, <ref type=""bibr"" target=""#b8"">[9]</ref>, <ref type=""bibr"" target=""#b9"">[10]</ref>. While these metho mance degradation as the prefetching depth grows. Since lbm has a variety of memory access patterns <ref type=""bibr"" target=""#b8"">[9]</ref>, deeper prefetching with a simple delta predictor wastes ban p><p>To address both prefetching coverage and accuracy, prior work has adopted lookahead mechanisms <ref type=""bibr"" target=""#b8"">[9]</ref>, <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" b11"">[12]</ref>, <ref type=""bibr"" target=""#b12"">[13]</ref>, or do not implement adaptive throttling <ref type=""bibr"" target=""#b8"">[9]</ref>. Moreover, most hardware prefetchers work in the physical ad st hardware prefetchers work in the physical address space <ref type=""bibr"" target=""#b3"">[4]</ref>, <ref type=""bibr"" target=""#b8"">[9]</ref>, <ref type=""bibr"" target=""#b9"">[10]</ref>, <ref type=""bibr"" nt, best of class, lookahead and non-lookahead prefetchers <ref type=""bibr"" target=""#b7"">[8]</ref>, <ref type=""bibr"" target=""#b8"">[9]</ref>, <ref type=""bibr"" target=""#b9"">[10]</ref>, including the win offset prefetchers take longer to train or fail to select the optimal offset. Lookahead prefetchers <ref type=""bibr"" target=""#b8"">[9]</ref>, <ref type=""bibr"" target=""#b11"">[12]</ref> efficiently encod nto the pattern table and generate further predictions. This recursion allows lookahead prefetchers <ref type=""bibr"" target=""#b8"">[9]</ref>, <ref type=""bibr"" target=""#b11"">[12]</ref> to prefetch far a o avoid over-prefetching, existing lookahead prefetchers <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" target=""#b8"">[9]</ref> globally and statically limit the depth to which lookahead i e transition. Previously proposed history-based techniques <ref type=""bibr"" target=""#b3"">[4]</ref>, <ref type=""bibr"" target=""#b8"">[9]</ref> address this by making predictions using the first offset in op performing recently proposed prefetching algorithms: the Variable Length Delta Prefetcher (VLDP) <ref type=""bibr"" target=""#b8"">[9]</ref>, the Best Offset Prefetcher (BOP) <ref type=""bibr"" target=""#",1
"=""#b6"">[7]</ref>, <ref type=""bibr"" target=""#b7"">[8]</ref>, <ref type=""bibr"" target=""#b8"">[9]</ref>, <ref type=""bibr"" target=""#b9"">[10]</ref>. While these methods show significant benefit, because they cal address space <ref type=""bibr"" target=""#b3"">[4]</ref>, <ref type=""bibr"" target=""#b8"">[9]</ref>, <ref type=""bibr"" target=""#b9"">[10]</ref>, <ref type=""bibr"" target=""#b10"">[11]</ref>, where the mappi ahead prefetchers <ref type=""bibr"" target=""#b7"">[8]</ref>, <ref type=""bibr"" target=""#b8"">[9]</ref>, <ref type=""bibr"" target=""#b9"">[10]</ref>, including the winner of the most recent data prefetching c s the most commonly seen delta pattern. Offset based prefetchers such as the Best Offset prefetcher <ref type=""bibr"" target=""#b9"">[10]</ref> and the Sandbox prefetcher <ref type=""bibr"" target=""#b14"">[ effect of either ignoring complex patterns altogether, as in the case of the Best Offset prefetcher <ref type=""bibr"" target=""#b9"">[10]</ref>, or requiring long, per-page, warmup times, as in Access Ma h Delta Prefetcher (VLDP) <ref type=""bibr"" target=""#b8"">[9]</ref>, the Best Offset Prefetcher (BOP) <ref type=""bibr"" target=""#b9"">[10]</ref>, and the DRAM-Aware Access Map Pattern Matching (DA-AMPM) < ) that cannot be easily captured in the physical address space. Therefore, offset-based prefetchers <ref type=""bibr"" target=""#b9"">[10]</ref>, <ref type=""bibr"" target=""#b14"">[15]</ref> work better than s a longer time to find the best offset value when more cache lines are in its recent request table <ref type=""bibr"" target=""#b9"">[10]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>E",0
"ated version of the simulation infrastructure used in the 2nd Data Prefetching Championship (DPC-2) <ref type=""bibr"" target=""#b17"">[18]</ref>. We model 1-4 out-of-order cores, whose parameters can be",0
"che miss occurs, and then prefetching either a set of lines sequentially following the current miss <ref type=""bibr"" target=""#b1"">[2]</ref>, a set of lines following a strided pattern with respect to",0
"type=""table"" target=""#tab_2"">I</ref>. ChampSim is a trace-based simulator, and we collect SimPoint <ref type=""bibr"" target=""#b18"">[19]</ref> traces from 18 memory intensive SPEC CPU2006 <ref type=""bi",0
"arget=""#b6"">7]</ref>. This important finding attracts great interests in edge partitioning recently <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target xisting partitioners, METIS gives the lowest replication factor which is consistent with literature <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b12"">13]</ref>. However, METIS runs titioning algorithm METIS <ref type=""bibr"" target=""#b7"">[8]</ref> is extended for edge partitioning <ref type=""bibr"" target=""#b2"">[3]</ref>, which makes full access to the graph structure by partition alanced if max i ∈[p] {|E i |} ≤ ⌈ α |E | /p⌉.<label>(1)</label></formula><p>The replication factor <ref type=""bibr"" target=""#b2"">[3]</ref> of a partitioning is defined as</p><formula xml:id=""formula_ <head n=""2.3"">NP-Hardness</head><p>The p-edge partitioning problem has been proved to be NP-hard in <ref type=""bibr"" target=""#b2"">[3]</ref> when p grows with n = |V |. To our best knowledge, it has no weight. One can turn a vertex-partitioner into an edge-partitioner while preserving its performance <ref type=""bibr"" target=""#b2"">[3]</ref>. To transform METIS to an edge-partitioner, we first call ME </table></figure> 			<note xmlns=""http://www.tei-c.org/ns/1.0"" place=""foot"" n=""1"" xml:id=""foot_0"">In<ref type=""bibr"" target=""#b2"">[3]</ref>, the NP-hardness is proved by a reduction from 3-partition p",1
"partitioning recently <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" tar and Oblivious rely only on historical data, and thus use the graph structure only partially. Sheep <ref type=""bibr"" target=""#b12"">[13]</ref> partitions the graph in a divide and conquer manner, which y in memory. This does lead to the state-of-the-art replication factors on a great number of graphs <ref type=""bibr"" target=""#b12"">[13]</ref>, but it is not applicable to large graphs.</p><p>In this p <ref type=""bibr"" target=""#b5"">[6]</ref>, HDRF <ref type=""bibr"" target=""#b14"">[15]</ref>, and Sheep <ref type=""bibr"" target=""#b12"">[13]</ref>. METIS is the state-of-the-art method for vertex partition endster and UK-union graphs on our 125 GB RAM machine. This result also echoes the fact reported by <ref type=""bibr"" target=""#b12"">[13]</ref>.</p><p>Running time. Fig. <ref type=""figure"" target=""#fig_ ce sequentially <ref type=""bibr"" target=""#b16"">[17]</ref>. Using only a single thread, NE and Sheep <ref type=""bibr"" target=""#b12"">[13]</ref> have similar running time. But Sheep can be speeded up sig forms other state-of-the-art ones including METIS <ref type=""bibr"" target=""#b7"">[8]</ref> and Sheep <ref type=""bibr"" target=""#b12"">[13]</ref> in terms of replication factors. Applying the NE algorithm owest replication factor which is consistent with literature <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b12"">13]</ref>. However, METIS runs out of memory when partitioning the Tw",1
"uted graph processing systems like PowerGraph <ref type=""bibr"" target=""#b5"">[6]</ref> and PowerLyra <ref type=""bibr"" target=""#b3"">[4]</ref> in Section 5.2.</p><p>Testbed. We evaluate all partitioners",0
"ation of |E | /n and σ /n. Here we apply the elegant treatment of random power-law graphs by Newman <ref type=""bibr"" target=""#b13"">[14]</ref> using generating functions, and derive the expected number e term E[ σ /n]. When τ &gt; 2,</formula><p>it is almost sure that the graph is not fully connected <ref type=""bibr"" target=""#b13"">[14]</ref>; when 2 &lt; τ &lt; 3.4785 and κ → ∞, besides small connec probability that this edge leads to a small component of size k. Here we follow Newman's assumption <ref type=""bibr"" target=""#b13"">[14]</ref> that there is no loop in the small components.</p><p>Hence",0
"tore vertex set. For instance, our implementation of NE takes about 90 GB RAM to partition uk-union <ref type=""bibr"" target=""#b1"">[2]</ref>. We have made some preliminary attempts to extend the NE alg streaming algorithm via sampling methods (Appendix B), which is able to partition the clueweb graph <ref type=""bibr"" target=""#b1"">[2]</ref> (|V | = 978M, |E| = 42.5B) whose edge set exceeds the volume "" target=""#tab_7"">4</ref>. To evaluate the scalability of the streaming we add a new graph clue-web <ref type=""bibr"" target=""#b1"">[2]</ref> of 978, 408, 098 vertices and 42, 574, 107, 469 edges. Compa",0
"<p>To handle large-scale graphs, distributed graph engines <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b15"">16]</ref> partition the input",0
"d to be more effective in advanced distributed graph engines <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b15"">16]</ref>, which evenly assigns partitioning performs better on many large real-world graphs <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b6"">7]</ref>. This important finding attracts great interests in edge part in recent graph systems including PowerGraph <ref type=""bibr"" target=""#b5"">[6]</ref>, Spark GraphX <ref type=""bibr"" target=""#b6"">[7]</ref>, and Chaos <ref type=""bibr"" target=""#b15"">[16]</ref>, to div ) = |W | + 3 k 2 .</formula><p>Thus e (H ) − 3 k 2 &gt; βe (H ). However, if this is the case, then <ref type=""bibr"" target=""#b6"">(7)</ref> </p><formula xml:id=""formula_45"">tells us that |E (V 1 ) ∪ E",0
"target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b16"">17]</ref>. Edge partitioning ncoming edge -an edge is more likely to be assigned to the partition with more adjacent edges. HDRF <ref type=""bibr"" target=""#b14"">[15]</ref> is another streaming algorithm that extends Oblivious by f ower-law graphs</head><p>Prior to this work, DBH <ref type=""bibr"" target=""#b16"">[17]</ref> and HDRF <ref type=""bibr"" target=""#b14"">[15]</ref> give upper bounds of expected replication factor of their with the ones in literature. Prior to ours, DBH <ref type=""bibr"" target=""#b16"">[17]</ref> and HDRF <ref type=""bibr"" target=""#b14"">[15]</ref> give upper bounds of expected replication factor for rando 4"">[15]</ref> give upper bounds of expected replication factor for random power-law graphs. In HDRF <ref type=""bibr"" target=""#b14"">[15]</ref>, an average-case analysis is applied to their streaming me <ref type=""bibr"" target=""#b16"">[17]</ref>, Oblivious <ref type=""bibr"" target=""#b5"">[6]</ref>, HDRF <ref type=""bibr"" target=""#b14"">[15]</ref>, and Sheep <ref type=""bibr"" target=""#b12"">[13]</ref>. METI is randomly assigned to one of its adjacent vertices' partition. For Oblivious and HDRF, following <ref type=""bibr"" target=""#b14"">[15]</ref>, we feed the edges in a random order, to balance the resul",0
"arget=""#b5"">6,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b16"">17]</ref>. Edge partitioning has been widely adopted in recent graph ph structure and incurs high vertex replication (Section 5). The density-based hashing method (DBH) <ref type=""bibr"" target=""#b16"">[17]</ref> first partitions the vertices randomly, and then assigns e .org/ns/1.0""><head n=""4.2"">Upper bound for random power-law graphs</head><p>Prior to this work, DBH <ref type=""bibr"" target=""#b16"">[17]</ref> and HDRF <ref type=""bibr"" target=""#b14"">[15]</ref> give up sting Bounds. Let us compare our theoretical bounds with the ones in literature. Prior to ours, DBH <ref type=""bibr"" target=""#b16"">[17]</ref> and HDRF <ref type=""bibr"" target=""#b14"">[15]</ref> give up rage-case analysis is applied to their streaming method to give a bound for powerlaw graphs. In DBH <ref type=""bibr"" target=""#b16"">[17]</ref>, an upper bound on expected replication factor is derived ng METIS <ref type=""bibr"" target=""#b7"">[8]</ref>, RAND <ref type=""bibr"" target=""#b5"">[6]</ref>, DBH <ref type=""bibr"" target=""#b16"">[17]</ref>, Oblivious <ref type=""bibr"" target=""#b5"">[6]</ref>, HDRF < ut their performance should be similar to DBH since they all scan the input graph once sequentially <ref type=""bibr"" target=""#b16"">[17]</ref>. Using only a single thread, NE and Sheep <ref type=""bibr""",0
"ug B, and drug C has a similar structure to drug A, there is likely a DDI between drug C and drug B <ref type=""bibr"" target=""#b5"">[8]</ref>. Vilar et al. predicted DDIs with a matrix transformation ap a matrix transformation approach using structural similarities of drugs with molecular fingerprints <ref type=""bibr"" target=""#b5"">[8]</ref>. In subsequent studies, the authors reported prediction meth ports in literature and databases. Comparing to the prediction results in the study by Vilar et al. <ref type=""bibr"" target=""#b5"">[8]</ref>, which also used drug structural similarities, two of the to rednisone, and lovastatin) were predicted to have DDI with simvastatin in the study by Vilar et al. <ref type=""bibr"" target=""#b5"">[8]</ref>.</p><p>These two case studies suggested that our approach co",1
"re data was demonstrated, which captured the characteristics that were missed by using only 2D data <ref type=""bibr"" target=""#b8"">[11]</ref>. Luo et al. developed a web server for DDI prediction throu",0
"sing protein-protein interaction network, which demonstrated an accuracy of 0.82 and recall of 0.62 <ref type=""bibr"" target=""#b13"">[16]</ref>. Cami et al. <ref type=""bibr"" target=""#b14"">[17]</ref> pre",0
"and 3A4 by using two types of chemical descriptors and the balanced accuracy ranged from 72 to 79% <ref type=""bibr"" target=""#b11"">[14]</ref>.</p><p>There are also knowledge-based studies for DDI pred",0
"by using the glm method implemented in caret <ref type=""bibr"" target=""#b38"">[42]</ref>, a popular R <ref type=""bibr"" target=""#b39"">[43]</ref> package. The scores were preprocessed using the preProcess",0
"ent works have focused on learning deep embeddings that can be used as universal object descriptors <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" targ effect of incorporating the CF into the fully-convolutional Siamese framework of Bertinetto et al. <ref type=""bibr"" target=""#b2"">[3]</ref>. We find that the CF does not improve results for networks t e performance. For our method, we prefer to build upon the fully-convolutional Siamese architecture <ref type=""bibr"" target=""#b2"">[3]</ref>, as it enforces the prior that the appearance similarity fun ."">Fully-convolutional Siamese networks</head><p>Our starting point is a network similar to that of <ref type=""bibr"" target=""#b2"">[3]</ref>, which we later modify in order to allow the model to be int t is necessary to combine this with a procedure that describes the logic of the tracker. Similar to <ref type=""bibr"" target=""#b2"">[3]</ref>, we employ a simplistic tracking algorithm to assess the uti r during training. We first compare against the symmetric Siamese architecture of Bertinetto et al. <ref type=""bibr"" target=""#b2"">[3]</ref>. We then compare the endto-end trained CFNet to a variant wh random seeds, this would require significantly more resources. Our baseline diverges slightly from <ref type=""bibr"" target=""#b2"">[3]</ref> in two ways. Firstly, we reduce the total stride of the netw ). We compare our methods against state-of-the-art trackers that can operate in realtime: SiamFC-3s <ref type=""bibr"" target=""#b2"">[3]</ref>, Staple <ref type=""bibr"" target=""#b1"">[2]</ref> and LCT <ref s=""http://www.tei-c.org/ns/1.0""><head>A. Implementation details</head><p>We follow the procedure of <ref type=""bibr"" target=""#b2"">[3]</ref> to minimize the loss (equation 2) through SGD, with the Xavi the following ReLU but not the following pooling layer (if any).Our baseline diverges slightly from<ref type=""bibr"" target=""#b2"">[3]</ref> in two ways. Firstly, we reduce the total stride of the netw e xmlns=""http://www.tei-c.org/ns/1.0"" place=""foot"" n=""1"" xml:id=""foot_0"">Note that this differs from<ref type=""bibr"" target=""#b2"">[3]</ref>, in which the target object and search area were instead den ve been introduced <ref type=""bibr"" target=""#b27"">[28,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b2"">3]</ref>, raising interest in the tracking community for their simplic rget=""#b12"">[13,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b2"">3]</ref> with CNN features, as proposed in previous work <ref type=""bi",1
"atch from the surrounding patches by solving a large ridge regression problem extremely efficiently <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b12"">13]</ref>. It has proved to be r to improve the tracking accuracy compared to the conventional choice of a fixed Gaussian response <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b12"">13]</ref>.</p></div> <div xmln p://www.tei-c.org/ns/1.0""><head n=""2."">Related work</head><p>Since the seminal work of Bolme et al. <ref type=""bibr"" target=""#b3"">[4]</ref>, the Correlation Filter has enjoyed great popularity within To reduce the effect of circular boundaries, the feature map x is pre-multiplied by a cosine window <ref type=""bibr"" target=""#b3"">[4]</ref> and the final template is cropped <ref type=""bibr"" target=""#",1
"descent (SGD), the workhorse of deep network optimization <ref type=""bibr"" target=""#b30"">[31,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b34"">35]</ref>. The extremely limi target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b30"">31,</ref><ref type=""bibr"" tar Furthermore, SGD is quite expensive for online adaptation <ref type=""bibr"" target=""#b30"">[31,</ref><ref type=""bibr"" target=""#b24"">25]</ref>.</p><p>A possible answer to these shortcomings is to have n onal improvements which can often be found in the tracking literature (e.g. bounding box regression <ref type=""bibr"" target=""#b24"">[25]</ref>, ensembling of multiple cues <ref type=""bibr"" target=""#b21",0
"olution to an optimization problem during training has been previously investigated. Ionescu et al. <ref type=""bibr"" target=""#b13"">[14]</ref> and Murray <ref type=""bibr"" target=""#b23"">[24]</ref> have s the linear map which is the adjoint of the differential. This property was used by Ionescu et al. <ref type=""bibr"" target=""#b13"">[14]</ref> to compute backpropagation maps using matrix differential",0
"lving a large ridge regression problem extremely efficiently <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b12"">13]</ref>. It has proved to be highly successful in object tracking ( ared to the conventional choice of a fixed Gaussian response <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b12"">13]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n the training feature map x = f ρ (x ′ ) by solving a ridge regression problem in the Fourier domain <ref type=""bibr"" target=""#b12"">[13]</ref>. Its effect can be understood as crafting a discriminative t with each circular shift of the image x * δ −u is as close as possible to a desired response y[u] <ref type=""bibr"" target=""#b12"">[13]</ref>, minimizing u∈U</p><formula xml:id=""formula_3"">( x * δ −u hat is equivalent to eq. 5. The solution to eq. 6 can be computed efficiently in the Fourier domain <ref type=""bibr"" target=""#b12"">[13]</ref>,</p><formula xml:id=""formula_9"">     k = 1 n ( x * • in Figure <ref type=""figure"">1</ref> corresponds exactly to the operation of a standard CF tracker <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" targ",0
"orithm invites a comparison to meta-learning. Recent works <ref type=""bibr"" target=""#b29"">[30,</ref><ref type=""bibr"" target=""#b0"">1]</ref> have proposed feed-forward architectures that can be interpre",0
"p>Back-propagating through a learning algorithm invites a comparison to meta-learning. Recent works <ref type=""bibr"" target=""#b29"">[30,</ref><ref type=""bibr"" target=""#b0"">1]</ref> have proposed feed-f",0
"bject tracking (e.g. <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b1"">2]</ref>), where its efficienc standard CF tracker <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b2"">3]</ref> with CNN features, as -3s <ref type=""bibr"" target=""#b2"">[3]</ref>, Staple <ref type=""bibr"" target=""#b1"">[2]</ref> and LCT <ref type=""bibr"" target=""#b21"">[22]</ref>. We also include the recent SAMF <ref type=""bibr"" target="" e.g. bounding box regression <ref type=""bibr"" target=""#b24"">[25]</ref>, ensembling of multiple cues <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b1"">2]</ref>, optical flow <ref t",0
"tly unroll a fixed number of iterations <ref type=""bibr"" target=""#b35"">[36]</ref>. Maclaurin et al. <ref type=""bibr"" target=""#b22"">[23]</ref> go further and back-propagate gradients through an entire",0
"s been previously investigated. Ionescu et al. <ref type=""bibr"" target=""#b13"">[14]</ref> and Murray <ref type=""bibr"" target=""#b23"">[24]</ref> have presented back-propagation forms for the SVD and Chol",0
"ly being developed to minimize this problem <ref type=""bibr"" target=""#b14"">(Rusu et al., 2016;</ref><ref type=""bibr"" target=""#b6"">Kirkpatrick et al., 2017;</ref><ref type=""bibr"" target=""#b2"">Fernando method to prevent catastrophic forgetting in discriminative models is elastic weight consolidation <ref type=""bibr"" target=""#b6"">(Kirkpatrick et al., 2017)</ref>. However, the method requires computi",1
"target=""#b14"">(Rusu et al., 2016;</ref><ref type=""bibr"" target=""#b6"">Kirkpatrick et al., 2017;</ref><ref type=""bibr"" target=""#b2"">Fernando et al., 2017)</ref>. Generative models, on the other hand, ar",1
"e class at a time, and specialized techniques are actively being developed to minimize this problem <ref type=""bibr"" target=""#b14"">(Rusu et al., 2016;</ref><ref type=""bibr"" target=""#b6"">Kirkpatrick et",1
"ttp://www.tei-c.org/ns/1.0""><head n=""3.1"">Datasets</head><p>We use publicly available datasets from <ref type=""bibr"" target=""#b18"">Zhang et al. (2015)</ref> to evaluate our models (http: //goo.gl/JyCn criminative LSTM model is competitive with other discriminative models based on logistic regression <ref type=""bibr"" target=""#b18"">(Zhang et al., 2015;</ref><ref type=""bibr"" target=""#b5"">Joulin et al. 015;</ref><ref type=""bibr"" target=""#b5"">Joulin et al., 2016)</ref> or convolutional neural networks <ref type=""bibr"" target=""#b18"">(Zhang et al., 2015;</ref><ref type=""bibr"" target=""#b16"">Xiao &amp; C",0
"ve model, the dimension of the class embedding is also set to 100. We train our model using AdaGrad <ref type=""bibr"" target=""#b1"">(Duchi et al., 2012)</ref> and tune the learning rate on development s",0
"contrastive estimation <ref type=""bibr"" target=""#b7"">(Mnih &amp; Teh, 2012)</ref>, sampled softmax <ref type=""bibr"" target=""#b4"">(Jean et al., 2015)</ref>, or one-vs-each approximation <ref type=""bib",0
"ult, with findings from convex problems failing to account for empirical facts about generalization <ref type=""bibr"" target=""#b17"">Zhang et al. (2017)</ref>. As such, this result is remarkable for bei",0
"dding techniques for all y ∈ Y. In order to do this, we use pretrained GloVe word embedding vectors <ref type=""bibr"" target=""#b12"">(Pennington et al., 2014)</ref>. <ref type=""foot"" target=""#foot_4"">5<",0
"native models based on logistic regression <ref type=""bibr"" target=""#b18"">(Zhang et al., 2015;</ref><ref type=""bibr"" target=""#b5"">Joulin et al., 2016)</ref> or convolutional neural networks <ref type=",0
"ing data distributions.</p><p>While neural networks are traditionally used as discriminative models <ref type=""bibr"" target=""#b9"">(Ney, 1995;</ref><ref type=""bibr"" target=""#b13"">Rubinstein &amp; Hasti",0
""" target=""#b18"">(Zhang et al., 2015;</ref><ref type=""bibr"" target=""#b16"">Xiao &amp; Cho, 2016;</ref><ref type=""bibr"" target=""#b0"">Conneau et al., 2016)</ref>. All of the generative models have lower c",0
"mpled softmax <ref type=""bibr"" target=""#b4"">(Jean et al., 2015)</ref>, or one-vs-each approximation <ref type=""bibr"" target=""#b15"">(Titsias, 2017)</ref>. However, even with these approximations, discr",0
"negative links in each mini-batch can lower the expected stochastic gradient variance. As shown in <ref type=""bibr"" target=""#b34"">[35,</ref><ref type=""bibr"" target=""#b35"">36]</ref>, the reduction of buting the training over multiple workers. Several sampling techniques, such as stratified sampling <ref type=""bibr"" target=""#b34"">[35]</ref> and importance sampling <ref type=""bibr"" target=""#b35"">[36",1
"wer the expected stochastic gradient variance. As shown in <ref type=""bibr"" target=""#b34"">[35,</ref><ref type=""bibr"" target=""#b35"">36]</ref>, the reduction of variance can lead to faster convergence. ques, such as stratified sampling <ref type=""bibr"" target=""#b34"">[35]</ref> and importance sampling <ref type=""bibr"" target=""#b35"">[36]</ref> are proposed to achieve the variance reduction. Different",1
"en increasing, the number of data points in the mini-batch. Sampling techniques are also studied in <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b38"">39]</ref> to distribute the co",0
"v m ]</formula><p>. Since matrix multiplication is higher in BLAS level than vector multiplication <ref type=""bibr"" target=""#b13"">[14]</ref>, even we increase the number of interactions, with medium find that the idea of sharing negative examples is exploited to speed up word embedding training in <ref type=""bibr"" target=""#b13"">[14]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head",0
"based loss functions <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b28"">29]</ref> adopt the ""Negative Sampling"" strategy, in which k negative ther embedding methods <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b28"">29]</ref> can automatically extract better features that produce high esc>Negative Sampling<ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b28"">29]</ref> Require: number of positive links in a mini-batch b, number edding (e.g., Word2Vec <ref type=""bibr"" target=""#b21"">[22]</ref>) and network embedding (e.g., LINE <ref type=""bibr"" target=""#b28"">[29]</ref>) tasks, negative sampling is utilized. Recent efforts have",0
"iltering for recommendation tasks, as seen in recent studies <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b31"">32,</ref><ref type=""bibr"" targe ate-of-the-art neural networkbased recommendation algorithms <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b29"">30]</ref>, and propose a more g m</head><p>In this work, we use the text recommendation task <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b30"">31,</ref><ref type=""bibr"" targe is worth noting that several existing stateof-the-art models <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b29"">30]</ref> are special cases of ade in combining collaborative filtering and neural networks <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b29"">30,</ref><ref type=""bibr"" targe al embedding framework in this work subsumes existing models <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b29"">30]</ref>.</p><p>Stochastic Gra "" target=""#b37"">[38]</ref> and some specific neural networks <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b3"">4]</ref>. Generalizing the existing work, we propose to replace the or . As for Yahoo! News data, there are 10,000 users, 58,579 items and 515,503 interactions. Following <ref type=""bibr"" target=""#b3"">[4]</ref>, we select a portion (20%) of items to form the pool of test bibr"" target=""#b29"">30]</ref> can be improved by others such as SG-loss and pairwise loss functions <ref type=""bibr"" target=""#b3"">[4]</ref>.</p><p>To further investigate the superior performance broug tion, <ref type=""bibr"" target=""#b0"">[1]</ref> adopts RNN/GRU to better understand the text content. <ref type=""bibr"" target=""#b3"">[4]</ref> proposes to use CNN and pairwise loss functions, and also in br"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b37"">38]</ref>, as well as content information <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" targ",0
"commendation. To overcome this problem, hybrid methods are proposed to incorporate side information <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" targ hybrid methods are proposed to incorporate side information <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" targe",0
"trix factorization <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b26"">27]</ref> are widely adopted. However, one of its limitation is the d raditional CF methods, such as sparse matrix factorization <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b26"">27]</ref>, are usually fast to train, while the deep neural networks ization techniques <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b26"">27]</ref>, each user/item ID is associated with a latent vector u or recommender systems, and methods like matrix factorization <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b26"">27]</ref> are widely adopted. While many papers focus on the explicit",0
">Algorithm 2</head><label>2</label><figDesc>Negative Sampling<ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b28"">29]</ref> Require: number of",0
"puter vision methods <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" tar xperiments, we follow the protocol used in previous papers <ref type=""bibr"" target=""#b19"">[20,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b14"">15]</ref>, which uses unseen #foot_0"">1</ref> , 10 2 ]), where the implementation of traditional classifiers becomes challenging <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b14"">15]</ref>.</p><p>Arguably, t rix of pairwise distances of a subset of the training set (i.e., the mini-batch) allows Song et al. <ref type=""bibr"" target=""#b20"">[21]</ref> to design of a new loss function that integrates all posit > (with and without FANNG <ref type=""bibr"" target=""#b4"">[5]</ref>), (2) lifted structured embedding <ref type=""bibr"" target=""#b20"">[21]</ref>, (3) N-pairs metric loss <ref type=""bibr"" target=""#b19"">[2 hod, and ( <ref type=""formula"">5</ref>) ), we use the same training and test set split described in <ref type=""bibr"" target=""#b20"">[21]</ref> across all datasets. Specifically, the means CUB200-2011 < target=""#b22"">[23]</ref> weights and randomly initialize the final fully connected layer similar to <ref type=""bibr"" target=""#b20"">[21]</ref>. We set the embedding size to 64 <ref type=""bibr"" target="" nnected layer similar to <ref type=""bibr"" target=""#b20"">[21]</ref>. We set the embedding size to 64 <ref type=""bibr"" target=""#b20"">[21]</ref> and the learning rate for the randomly initialized fully c omly initialized fully connected layer is multiplied by 10 to achieve faster convergence similar to <ref type=""bibr"" target=""#b20"">[21]</ref>.</p><p>For the experiments using triplet combined with glo =""bibr"" target=""#b22"">[23]</ref> and randomly initialize the final fully connected layer similar to <ref type=""bibr"" target=""#b20"">[21]</ref> . The learning rate for the randomly initialized fully con omly initialized fully connected layer is multiplied by 10 to achieve faster convergence similar to <ref type=""bibr"" target=""#b20"">[21]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head",1
"ead n=""4."">Experiments</head><p>For the experiments, we follow the protocol used in previous papers <ref type=""bibr"" target=""#b19"">[20,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" ta (2) lifted structured embedding <ref type=""bibr"" target=""#b20"">[21]</ref>, (3) N-pairs metric loss <ref type=""bibr"" target=""#b19"">[20]</ref>, (4) clustering <ref type=""bibr"" target=""#b14"">[15]</ref>,",1
"get=""#b15"">16,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b29"">30]</ref>. The main advantage of such models lies in their ability to",0
"rget=""#b9"">10,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" tar ad to the implementation of importance sampling techniques <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b25"">26]</ref> that stochastically uch as the siamese network that gradually introduces hardest possible positive and negative samples <ref type=""bibr"" target=""#b17"">[18]</ref>. This is achieved by randomly sampling the training set fo omparitive purposes, we note that larger minibatches (i.e. smaller M) tend to reduce training error <ref type=""bibr"" target=""#b17"">[18]</ref> up until performance begins to be limited by the naive use d mining is instead commonly performed over the stochastic subset of samples used in each minibatch <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b15"">16]</ref>. This method has t",0
"<p>The development of deep metric learning models for the estimation of effective feature embedding <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target gative is. Positive samples are then chosen to guarantee a non-zero response from the loss function <ref type=""bibr"" target=""#b1"">(2)</ref>.</p><p>More formally, we define a smart negative as any nega",0
"target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" tar",0
"get=""#b11"">12,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" tar",0
"b16"">[17]</ref>, hard negative and positive mining is a relabeling of the problem of bootstrappping <ref type=""bibr"" target=""#b21"">[22]</ref>, where the idea is to start the training of the embedding",0
"6"">[7,</ref><ref type=""bibr"" target=""#b25"">26]</ref>, which are an extension of the siamese network <ref type=""bibr"" target=""#b0"">[1]</ref>. Triplet networks are composed of three identical convolutio es a non-linear activation function (e.g., ReLU <ref type=""bibr"" target=""#b13"">[14]</ref>). Also in <ref type=""bibr"" target=""#b0"">(1)</ref>, note that f l = [f l,1 , ..., f l,n l ] represents an array",0
"target=""#b3"">4,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" tar neighbour search that forms the basis of our proposed method. As pointed out by Shrivastava et al. <ref type=""bibr"" target=""#b16"">[17]</ref>, hard negative and positive mining is a relabeling of the",0
"by powerful baseline systems, such as the Fast/Faster R-CNN <ref type=""bibr"" target=""#b14"">[9,</ref><ref type=""bibr"" target=""#b34"">29]</ref> and Fully Convolutional Network (FCN) <ref type=""bibr"" targ target=""#b14"">[9]</ref>. N is 64 for the C4 backbone (as in <ref type=""bibr"" target=""#b14"">[9,</ref><ref type=""bibr"" target=""#b34"">29]</ref>) and 512 for FPN (as in <ref type=""bibr"" target=""#b27"">[22] state-of-the-art instance segmentation results. Our method, called Mask R-CNN, extends Faster R-CNN <ref type=""bibr"" target=""#b34"">[29]</ref> by adding a branch for predicting segmentation masks on ea ding to RoIs on feature maps using RoIPool, leading to fast speed and better accuracy. Faster R-CNN <ref type=""bibr"" target=""#b34"">[29]</ref> advanced this stream by learning the attention mechanism w e of Fast/Faster R-CNN.</p><p>Faster R-CNN: We begin by briefly reviewing the Faster R-CNN detector <ref type=""bibr"" target=""#b34"">[29]</ref>. Faster R-CNN consists of two stages. The first stage, cal are shareable.</p><p>Inference: At test time, the proposal number is 300 for the C4 backbone (as in <ref type=""bibr"" target=""#b34"">[29]</ref>) and 1000 for FPN (as in <ref type=""bibr"" target=""#b27"">[2 hares features between the RPN and Mask R-CNN stages, following the 4-step training of Faster R-CNN <ref type=""bibr"" target=""#b34"">[29]</ref>. This model runs at 195ms per image on an Nvidia Tesla M40 hyper-parameters following existing Fast/Faster R-CNN work <ref type=""bibr"" target=""#b14"">[9,</ref><ref type=""bibr"" target=""#b34"">29,</ref><ref type=""bibr"" target=""#b27"">22]</ref>. Although these dec decisions were made for object detection in original papers <ref type=""bibr"" target=""#b14"">[9,</ref><ref type=""bibr"" target=""#b34"">29,</ref><ref type=""bibr"" target=""#b27"">22]</ref>, we found our insta",1
"ref>. All entries are single-model results.</p><p>is evaluating using mask IoU. As in previous work <ref type=""bibr"" target=""#b8"">[3,</ref><ref type=""bibr"" target=""#b27"">22]</ref>, we train using the",0
"segment proposal system in <ref type=""bibr"" target=""#b10"">[5]</ref> and object detection system in <ref type=""bibr"" target=""#b13"">[8]</ref> for ""fully convolutional instance segmentation"" (FCIS). The olutional instance segmentation"" (FCIS). The common idea in <ref type=""bibr"" target=""#b10"">[5,</ref><ref type=""bibr"" target=""#b13"">8,</ref><ref type=""bibr"" target=""#b26"">21]</ref> is to predict a set",0
"et=""#b15"">[10,</ref><ref type=""bibr"" target=""#b17"">12,</ref><ref type=""bibr"" target=""#b18"">13,</ref><ref type=""bibr"" target=""#b11"">6]</ref> resorted to bottom-up segments <ref type=""bibr"" target=""#b38",0
"er, recent research showed that an attacker could generate adversarial examples to fool classifiers <ref type=""bibr"" target=""#b33"">[34,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" targ b0"">(1)</ref> Training the target classifier with adversarial examples, called adversarial training <ref type=""bibr"" target=""#b33"">[34,</ref><ref type=""bibr"" target=""#b4"">5]</ref>; <ref type=""bibr"" ta cally, researchers showed that it was possible to generate adversarial examples to fool classifiers <ref type=""bibr"" target=""#b33"">[34,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" targ one may use a mixture of normal and adversarial examples in the training set for data augmentation <ref type=""bibr"" target=""#b33"">[34,</ref><ref type=""bibr"" target=""#b21"">22]</ref>, or mix the advers =""2.3"">Existing attacks</head><p>Since the discovery of adversarial examples for neural networks in <ref type=""bibr"" target=""#b33"">[34]</ref>, researchers have found adversarial examples on various ne h leveraged gradient based optimization from normal examples <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b33"">34,</ref><ref type=""bibr"" target=""#b4"">5]</ref>. Moosavi et al. showe",1
"er could generate adversarial examples to fool classifiers <ref type=""bibr"" target=""#b33"">[34,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" targe sible to generate adversarial examples to fool classifiers <ref type=""bibr"" target=""#b33"">[34,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" targe ier with adversarial examples, called adversarial training <ref type=""bibr"" target=""#b33"">[34,</ref><ref type=""bibr"" target=""#b4"">5]</ref>; <ref type=""bibr"" target=""#b1"">(2)</ref> Training a classifie from normal examples <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b33"">34,</ref><ref type=""bibr"" target=""#b4"">5]</ref>. Moosavi et al. showed that it was even possible to find one ""2.3.1"">Fast gradient sign method(FGSM). Given a normal image</head><p>x, fast gradient sign method <ref type=""bibr"" target=""#b4"">[5]</ref> looks for a similar image x ′ in the L ∞ neighborhood of x t ""#b21"">22]</ref>, or mix the adversarial objective with the classification objective as regularizer <ref type=""bibr"" target=""#b4"">[5]</ref>. Though this idea is promising, it is hard to reason about w",1
"<p>Another idea of defense is to detect adversarial examples with hand-crafted statistical features <ref type=""bibr"" target=""#b6"">[7]</ref> or separate classification networks <ref type=""bibr"" target=",0
"convolutional classification networks <ref type=""bibr"" target=""#b1"">[2]</ref>, generative networks <ref type=""bibr"" target=""#b13"">[14]</ref>, and recurrent networks <ref type=""bibr"" target=""#b26"">[27",0
"r"" target=""#b28"">29]</ref>, and human-computer interaction <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b12"">13]</ref>. These security-critical domains require better understandi",0
"r real-world systems <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b7"">8]</ref>. More specifically, researchers showed that it was possible t",0
"ks target classifiers.</p><p>Current defenses against adversarial examples follow three approaches: <ref type=""bibr"" target=""#b0"">(1)</ref> Training the target classifier with adversarial examples, ca asingly important role in modern world. They are used in autonomous control for robots and vehicles <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target",0
"to fool classifiers <ref type=""bibr"" target=""#b33"">[34,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b18"">19]</ref>. Their algorithms p to fool classifiers <ref type=""bibr"" target=""#b33"">[34,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b18"">19]</ref>. Their algorithms p",0
"s, their relevant data lie on a manifold that is of much lower dimension than the full sample space <ref type=""bibr"" target=""#b22"">[23]</ref>. This suggests that the normal examples for a classificati egligible. Researchers believe that N t constitute a manifold that is of much lower dimension than S<ref type=""bibr"" target=""#b22"">[23]</ref>. Since we do not know the data generation process, we appr",0
"one effective universal adversarial perturbation that, when applied, turned many images adversarial <ref type=""bibr"" target=""#b20"">[21]</ref>.</p><p>To simplify the discussion, we only focus on attack",0
">(Simonyan &amp; Zisserman, 2015;</ref><ref type=""bibr"" target=""#b44"">Srivastava et al., 2015;</ref><ref type=""bibr"" target=""#b18"">He et al., 2016;</ref><ref type=""bibr"" target=""#b21"">Huang et al., 20 et (left) <ref type=""bibr"" target=""#b34"">(LeCun et al., 1998)</ref> with a 110-layer ResNet (right) <ref type=""bibr"" target=""#b18"">(He et al., 2016)</ref> on the CIFAR-100 dataset. The top row shows t e normalization techniques have enabled the development of very deep architectures, such as ResNets <ref type=""bibr"" target=""#b18"">(He et al., 2016)</ref> and DenseNets <ref type=""bibr"" target=""#b22""> he past few years.</p><p>It is now common to see networks with hundreds, if not thousands of layers <ref type=""bibr"" target=""#b18"">(He et al., 2016;</ref><ref type=""bibr"" target=""#b21"">Huang et al., 2 ageNet models of 2015 all use an order of magnitude less weight decay than models of previous years <ref type=""bibr"" target=""#b18"">(He et al., 2016;</ref><ref type=""bibr"" target=""#b41"">Simonyan &amp;",1
"r"" target=""#b44"">Srivastava et al., 2015;</ref><ref type=""bibr"" target=""#b18"">He et al., 2016;</ref><ref type=""bibr"" target=""#b21"">Huang et al., 2016;</ref><ref type=""bibr"" target=""#b4"">2017)</ref>. A rks with hundreds, if not thousands of layers <ref type=""bibr"" target=""#b18"">(He et al., 2016;</ref><ref type=""bibr"" target=""#b21"">Huang et al., 2016)</ref> and hundreds of convolutional filters per l",0
"o generalize better than smaller ones, while exhibiting the capacity to easily fit the training set <ref type=""bibr"" target=""#b52"">(Zhang et al., 2017)</ref>.</p><p>Although increasing depth and width p>We can connect this finding to recent work examining the generalization of large neural networks. <ref type=""bibr"" target=""#b52"">Zhang et al. (2017)</ref> observe that deep neural networks seemingly",0
"<p>Our hybrid pointer-generator network facilitates copying words from the source text via pointing <ref type=""bibr"" target=""#b26"">(Vinyals et al., 2015)</ref>, which improves accuracy and handling of twork</head><p>Our pointer-generator network is a hybrid between our baseline and a pointer network <ref type=""bibr"" target=""#b26"">(Vinyals et al., 2015)</ref>, as it allows both copying words via poi plore sentence fusion using dependency trees.</p><p>Pointer-generator networks. The pointer network <ref type=""bibr"" target=""#b26"">(Vinyals et al., 2015)</ref> is a sequence-tosequence model that uses",1
"has made abstractive summarization viable <ref type=""bibr"" target=""#b3"">(Chopra et al., 2016;</ref><ref type=""bibr"" target=""#b17"">Nallapati et al., 2016;</ref><ref type=""bibr"" target=""#b20"">Rush et a arget=""#b7"">Gulcehre et al., 2016;</ref><ref type=""bibr"" target=""#b15"">Miao and Blunsom, 2016;</ref><ref type=""bibr"" target=""#b17"">Nallapati et al., 2016;</ref><ref type=""bibr"" target=""#b28"">Zeng et a cently-introduced CNN/ Daily Mail dataset <ref type=""bibr"" target=""#b8"">(Hermann et al., 2015;</ref><ref type=""bibr"" target=""#b17"">Nallapati et al., 2016)</ref>, which contains news articles (39 sente head><p>We use the CNN/Daily Mail dataset <ref type=""bibr"" target=""#b8"">(Hermann et al., 2015;</ref><ref type=""bibr"" target=""#b17"">Nallapati et al., 2016)</ref>, which contains online news articles (7 ad n=""2.1"">Sequence-to-sequence attentional model</head><p>Our baseline model is similar to that of <ref type=""bibr"" target=""#b17"">Nallapati et al. (2016)</ref>, and is depicted in Figure <ref type=""f e on those datasets.</p><p>However, large-scale datasets for summarization of longer text are rare. <ref type=""bibr"" target=""#b17"">Nallapati et al. (2016)</ref> adapted the DeepMind question-answering considerably different from that of <ref type=""bibr"" target=""#b7"">Gulcehre et al. (2016)</ref> and <ref type=""bibr"" target=""#b17"">Nallapati et al. (2016)</ref>. Those works train their pointer compon with multi-sentence summaries (3.75 sentences or 56 tokens on average). We used scripts supplied by <ref type=""bibr"" target=""#b17"">Nallapati et al. (2016)</ref> to obtain the same version of the the d and b ptr in equation 8), and coverage adds 512 extra parameters (w c in equation 11).</p><p>Unlike <ref type=""bibr"" target=""#b17"">Nallapati et al. (2016)</ref>, we do not pretrain the word embeddings is (+1.1 ROUGE-1, +2.0 ROUGE-2, +1.1 ROUGE-L) points respectively, and our best model scores exceed <ref type=""bibr"" target=""#b17"">Nallapati et al. (2016)</ref> by (+4.07 ROUGE-1, +3.98 ROUGE-2, +3.73 scene. (...) Summary: more questions than answers emerge in controversial s.c. police shooting. of <ref type=""bibr"" target=""#b17"">Nallapati et al. (2016)</ref> by several ROUGE points. Despite the br g Representations <ref type=""bibr"" target=""#b24"">(Takase et al., 2016)</ref>, hierarchical networks <ref type=""bibr"" target=""#b17"">(Nallapati et al., 2016)</ref>, variational autoencoders <ref type=""b nique that has been applied to <ref type=""bibr"">NMT (Sankaran et al., 2016)</ref> and summarization <ref type=""bibr"" target=""#b17"">(Nallapati et al., 2016)</ref>. In this approach, each attention dist rget=""#tab_2"">1</ref>), compared to the smaller boost given by temporal attention for the same task <ref type=""bibr"" target=""#b17"">(Nallapati et al., 2016)</ref>.</p></div> <div xmlns=""http://www.tei- he first three sentences of the article as a summary), and compare to the only existing abstractive <ref type=""bibr"" target=""#b17"">(Nallapati et al., 2016)</ref> and extractive <ref type=""bibr"" target training pairs, 13,368 validation pairs and 11,490 test pairs. Both the dataset's published results <ref type=""bibr"" target=""#b17"">(Nallapati et al., 2016</ref><ref type=""bibr"" target=""#b16"">(Nallapat le online. <ref type=""foot"" target=""#foot_5"">6</ref>Given that we generate plain-text summaries but <ref type=""bibr"" target=""#b17"">Nallapati et al. (2016;</ref><ref type=""bibr"" target=""#b16"">2017)</re",1
"n, that were applied to short-text summarization. We propose a novel variant of the coverage vector <ref type=""bibr"" target=""#b25"">(Tu et al., 2016)</ref> from Neural Machine Translation, which we use d n=""2.3"">Coverage mechanism</head><p>Repetition is a common problem for sequenceto-sequence models <ref type=""bibr"" target=""#b25"">(Tu et al., 2016;</ref><ref type=""bibr"" target=""#b14"">Mi et al., 2016 erating multi-sentence text (see Figure <ref type=""figure"">1</ref>). We adapt the coverage model of <ref type=""bibr"" target=""#b25"">Tu et al. (2016)</ref> to solve the problem. In our coverage model, w ine Translation <ref type=""bibr"" target=""#b10"">(Koehn, 2009)</ref>, coverage was adapted for NMT by <ref type=""bibr"" target=""#b25"">Tu et al. (2016)</ref> and <ref type=""bibr"" target=""#b14"">Mi et al. (",1
"ted by the decoder), and has decoder state s t . The attention distribution a t is calculated as in <ref type=""bibr"" target=""#b0"">Bahdanau et al. (2015)</ref>:</p><formula xml:id=""formula_0"">e t i = v als et al., 2015)</ref> is a sequence-tosequence model that uses the soft attention distribution of <ref type=""bibr"" target=""#b0"">Bahdanau et al. (2015)</ref> to produce an output sequence consisting",0
"target=""#b3"">(Chopra et al., 2016;</ref><ref type=""bibr"" target=""#b17"">Nallapati et al., 2016;</ref><ref type=""bibr"" target=""#b20"">Rush et al., 2015;</ref><ref type=""bibr"" target=""#b28"">Zeng et al., 2",0
"r"" target=""#b27"">Xu et al. (2015)</ref>, who apply a coverage-like method to image cap-tioning, and <ref type=""bibr"" target=""#b1"">Chen et al. (2016)</ref>, who also incorporate a coverage mechanism (w",0
"mately more useful. Therefore we apply our model to the recently-introduced CNN/ Daily Mail dataset <ref type=""bibr"" target=""#b8"">(Hermann et al., 2015;</ref><ref type=""bibr"" target=""#b17"">Nallapati e xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4"">Dataset</head><p>We use the CNN/Daily Mail dataset <ref type=""bibr"" target=""#b8"">(Hermann et al., 2015;</ref><ref type=""bibr"" target=""#b17"">Nallapati e =""bibr"" target=""#b17"">Nallapati et al. (2016)</ref> adapted the DeepMind question-answering dataset <ref type=""bibr"" target=""#b8"">(Hermann et al., 2015)</ref> for summarization, resulting in the CNN/D",0
"majority of past work has been extractive <ref type=""bibr"" target=""#b11"">(Kupiec et al., 1995;</ref><ref type=""bibr"" target=""#b18"">Paice, 1990;</ref><ref type=""bibr"" target=""#b21"">Saggion and Poibeau,",0
"to the difficulty of abstractive summarization, the great majority of past work has been extractive <ref type=""bibr"" target=""#b11"">(Kupiec et al., 1995;</ref><ref type=""bibr"" target=""#b18"">Paice, 1990",0
"vided the first abstractive baselines. The same authors then published a neural extractive approach <ref type=""bibr"" target=""#b16"">(Nallapati et al., 2017)</ref>, which uses hierarchical RNNs to selec y existing abstractive <ref type=""bibr"" target=""#b17"">(Nallapati et al., 2016)</ref> and extractive <ref type=""bibr"" target=""#b16"">(Nallapati et al., 2017)</ref> models on the full dataset. The output es not quite surpass the ROUGE scores of the lead-3 baseline, nor the current best extractive model <ref type=""bibr"" target=""#b16"">(Nallapati et al., 2017)</ref>. We discuss this issue in section 7.1. s. Both the dataset's published results <ref type=""bibr"" target=""#b17"">(Nallapati et al., 2016</ref><ref type=""bibr"" target=""#b16"">(Nallapati et al., , 2017) )</ref> use the anonymized version of the at we generate plain-text summaries but <ref type=""bibr"" target=""#b17"">Nallapati et al. (2016;</ref><ref type=""bibr"" target=""#b16"">2017)</ref> generate anonymized summaries (see Section 4), our ROUGE",0
"ents from the input sequence. The pointer network has been used to create hybrid approaches for NMT <ref type=""bibr"" target=""#b7"">(Gulcehre et al., 2016)</ref>, language modeling <ref type=""bibr"" targ (Merity et al., 2016)</ref>, and summarization <ref type=""bibr"" target=""#b6"">(Gu et al., 2016;</ref><ref type=""bibr"" target=""#b7"">Gulcehre et al., 2016;</ref><ref type=""bibr"" target=""#b15"">Miao and Bl ple occurrences of it in the source text.</p><p>Our approach is considerably different from that of <ref type=""bibr"" target=""#b7"">Gulcehre et al. (2016)</ref> and <ref type=""bibr"" target=""#b17"">Nallap",0
"ive and abstractive approaches, is similar to <ref type=""bibr"">Gu et al.'s (2016)</ref> CopyNet and <ref type=""bibr"" target=""#b15"">Miao and Blunsom's (2016)</ref> Forced-Attention Sentence Compression al networks <ref type=""bibr"" target=""#b17"">(Nallapati et al., 2016)</ref>, variational autoencoders <ref type=""bibr"" target=""#b15"">(Miao and Blunsom, 2016)</ref>, and direct optimization of the perfor bibr"" target=""#b6"">(Gu et al., 2016;</ref><ref type=""bibr"" target=""#b7"">Gulcehre et al., 2016;</ref><ref type=""bibr"" target=""#b15"">Miao and Blunsom, 2016;</ref><ref type=""bibr"" target=""#b17"">Nallapati al., 2016)</ref>.</p><p>Our approach is close to the Forced-Attention Sentence Compression model of <ref type=""bibr"" target=""#b15"">Miao and Blunsom (2016)</ref> and the CopyNet model of <ref type=""bib",0
"le <ref type=""table"" target=""#tab_2"">1</ref>. We evaluate our models with the standard ROUGE metric <ref type=""bibr"" target=""#b12"">(Lin, 2004b)</ref>, reporting the F 1 scores for ROUGE-1, ROUGE-2 and summary, which has been shown to lower ROUGE's reliability compared to multiple reference summaries <ref type=""bibr"" target=""#b12"">(Lin, 2004a)</ref>.</p><p>Due to the subjectivity of the task and thu",0
"pretrain the word embeddings -they are learned from scratch during training. We train using Adagrad <ref type=""bibr"" target=""#b5"">(Duchi et al., 2011)</ref> with learning rate 0.15 and an initial accu",0
""" target=""#b17"">Nallapati et al., 2016;</ref><ref type=""bibr"" target=""#b20"">Rush et al., 2015;</ref><ref type=""bibr"" target=""#b28"">Zeng et al., 2016)</ref>. Though these systems are promising, they ex get=""#b15"">Miao and Blunsom, 2016;</ref><ref type=""bibr"" target=""#b17"">Nallapati et al., 2016;</ref><ref type=""bibr"" target=""#b28"">Zeng et al., 2016)</ref>.</p><p>Our approach is close to the Forced-A",0
"he relational graph and apply it to both tasks.</p><p>Our entity classification model, similarly to <ref type=""bibr"" target=""#b16"">Kipf and Welling (2017)</ref>, uses softmax classifiers at each node nction or simply a linear transformation g m (h i , h j ) = W h j with a weight matrix W such as in <ref type=""bibr"" target=""#b16"">Kipf and Welling (2017)</ref>. This type of transformation has been s tion. While we only consider such a featureless approach in this work, we note that it was shown in <ref type=""bibr"" target=""#b16"">Kipf and Welling (2017)</ref> that it is possible for this class of m that operate on local graph neighborhoods <ref type=""bibr"" target=""#b8"">(Duvenaud et al. 2015;</ref><ref type=""bibr"" target=""#b16"">Kipf and Welling 2017)</ref> to large-scale relational data. These an d et al. 2015;</ref><ref type=""bibr"" target=""#b6"">Defferrard, Bresson, and Vandergheynst 2016;</ref><ref type=""bibr"" target=""#b16"">Kipf and Welling 2017)</ref> for large-scale and highly multi-relatio <ref type=""bibr"" target=""#b8"">(Duvenaud et al. 2015)</ref> and graph-based semi-supervised learning <ref type=""bibr"" target=""#b16"">(Kipf and Welling 2017)</ref>.</p><p>Motivated by these architectures",1
"s et al. 2015;</ref><ref type=""bibr"" target=""#b7"">Dong et al. 2015)</ref> and information retrieval <ref type=""bibr"" target=""#b17"">(Kotov and Zhai 2012;</ref><ref type=""bibr"" target=""#b4"">Dalton, Diet",0
"ya, and Berberich 2015;</ref><ref type=""bibr"" target=""#b13"">Hixon, Clark, and Hajishirzi 2015;</ref><ref type=""bibr"" target=""#b1"">Bordes et al. 2015;</ref><ref type=""bibr"" target=""#b7"">Dong et al. 201",0
"<ref type=""bibr"">(Bordes et al. 2013;</ref><ref type=""bibr"" target=""#b25"">Socher et al. 2013;</ref><ref type=""bibr"" target=""#b3"">Chang et al. 2014;</ref><ref type=""bibr"" target=""#b21"">Nickel, Rosasco",0
"genera-tive model based on a Laplacian pyramid framework (LAP-GAN) to generate realistic images in <ref type=""bibr"" target=""#b8"">[6]</ref>, which is the most related to our work. However, the propose",1
"yramid. The Laplacian pyramid has been used in a wide range of applications, such as image blending <ref type=""bibr"" target=""#b6"">[4]</ref>, texture synthesis <ref type=""bibr"" target=""#b16"">[14]</ref>",0
"14]</ref>, edge-aware filtering <ref type=""bibr"" target=""#b26"">[24]</ref> and semantic segmentation <ref type=""bibr"" target=""#b13"">[11,</ref><ref type=""bibr"" target=""#b27"">25]</ref>. <ref type=""bibr"">",0
"learn a nonlinear LR-to-HR mapping. The network is extended to embed a sparse coding-based network <ref type=""bibr"" target=""#b35"">[33]</ref> or use a deeper structure <ref type=""bibr"" target=""#b19"">[ several CNN based super-resolution models, e.g., SRCNN <ref type=""bibr"" target=""#b9"">[7]</ref>, SCN <ref type=""bibr"" target=""#b35"">[33]</ref>, VDSR <ref type=""bibr"" target=""#b19"">[17]</ref>, and DRCN SRCNN <ref type=""bibr"" target=""#b9"">[7]</ref>, FSRCNN <ref type=""bibr"" target=""#b10"">[8]</ref>, SCN <ref type=""bibr"" target=""#b35"">[33]</ref>, ESPCN <ref type=""bibr"" target=""#b30"">[28]</ref>, VDSR <re fExSR <ref type=""bibr"" target=""#b17"">[15]</ref>, RFL <ref type=""bibr"" target=""#b28"">[26]</ref>, SCN <ref type=""bibr"" target=""#b35"">[33]</ref>, VDSR <ref type=""bibr"" target=""#b19"">[17]</ref> and DRCN < get=""#b19"">17,</ref><ref type=""bibr"" target=""#b28"">26,</ref><ref type=""bibr"" target=""#b32"">30,</ref><ref type=""bibr"" target=""#b35"">33]</ref>. In contrast, our approach effectively suppresses such arti",0
"ut regard to their distance in the input or output sequences <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b15"">16]</ref>. In all but a few cases <ref type=""bibr"" target=""#b21"">[22]",1
"forms the foundation of the Extended Neural GPU <ref type=""bibr"" target=""#b19"">[20]</ref>, ByteNet <ref type=""bibr"" target=""#b14"">[15]</ref> and ConvS2S <ref type=""bibr"" target=""#b7"">[8]</ref>, all o ional layers in the case of contiguous kernels, or O(log k (n)) in the case of dilated convolutions <ref type=""bibr"" target=""#b14"">[15]</ref>, increasing the length of the longest paths between any tw f-attention and discuss its advantages over models such as <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b14"">15]</ref> and <ref type=""bibr"" target=""#b7"">[8]</ref>.</p></div> <div",1
"ence representations <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b18"">19]</ref>.</p><p>End-to-end m",0
"eight matrix between the two embedding layers and the pre-softmax linear transformation, similar to <ref type=""bibr"" target=""#b23"">[24]</ref>. In the embedding layers, we multiply those weights by √ d",0
"problems such as language modeling and machine translation <ref type=""bibr"" target=""#b28"">[29,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b4"">5]</ref>. Numerous efforts have quence transduction models have an encoder-decoder structure <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b28"">29]</ref>. Here, the encoder ma ttention mechanisms in sequence-to-sequence models such as <ref type=""bibr"" target=""#b30"">[31,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b7"">8</ref>]. • The encoder contains allowing modeling of dependencies without regard to their distance in the input or output sequences <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b15"">16]</ref>. In all but a few ca label>(1)</label></formula><p>The two most commonly used attention functions are additive attention <ref type=""bibr"" target=""#b1"">[2]</ref>, and dot-product (multiplicative) attention. Dot-product att",0
"layers are generally more expensive than recurrent layers, by a factor of k. Separable convolutions <ref type=""bibr"" target=""#b5"">[6]</ref>, however, decrease the complexity considerably, to O(k</p><f",0
"machine translation <ref type=""bibr"" target=""#b28"">[29,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b4"">5]</ref>. Numerous efforts have since continued to push the boundaries ure</head><p>Most competitive neural sequence transduction models have an encoder-decoder structure <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target",0
"in machine translations, such as word-piece <ref type=""bibr"" target=""#b30"">[31]</ref> and byte-pair <ref type=""bibr"" target=""#b24"">[25]</ref> representations. To improve computational performance for",0
"through factorization tricks <ref type=""bibr"" target=""#b17"">[18]</ref> and conditional computation <ref type=""bibr"" target=""#b25"">[26]</ref>, while also improving model performance in case of the lat",0
"type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b15"">16]</ref>. In all but a few cases <ref type=""bibr"" target=""#b21"">[22]</ref>, however, such attention mechanisms are used in conjunctio lment and learning task-independent sentence representations <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" tar",0
"ng over graph structures using convolution operators that offer promise as an embedding methodology <ref type=""bibr"" target=""#b16"">[17]</ref>. So far, graph convolutional networks (GCNs) have only bee nificant gains (7.4% on average) compared to an aggregator inspired by graph convolutional networks <ref type=""bibr"" target=""#b16"">[17]</ref>. Lastly, we probe the expressive capability of our approac ""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b17"">18]</ref>. The original GCN algorithm <ref type=""bibr"" target=""#b16"">[17]</ref> is designed for semi-supervised learning in a transductive r is nearly equivalent to the convolutional propagation rule used in the transductive GCN framework <ref type=""bibr"" target=""#b16"">[17]</ref>. In particular, we can derive an inductive variant of the regator convolutional since it is a rough, linear approximation of a localized spectral convolution <ref type=""bibr"" target=""#b16"">[17]</ref>. An important distinction between this convolutional aggre utional"" variant of GraphSAGE is an extended, inductive version of Kipf et al's semi-supervised GCN <ref type=""bibr"" target=""#b16"">[17]</ref>, we term this variant GraphSAGE-GCN. We test unsupervised ctive setting, where it can be extensively trained on a single, fixed graph. (That said, Kipf et al <ref type=""bibr"" target=""#b16"">[17]</ref> <ref type=""bibr"" target=""#b17"">[18]</ref> found that GCN-b d=""foot_2"">Note that this differs from Kipf et al's exact equation by a minor normalization constant<ref type=""bibr"" target=""#b16"">[17]</ref>.</note> 			<note xmlns=""http://www.tei-c.org/ns/1.0"" place convolutional networks (GCNs) have only been applied in the transductive setting with fixed graphs <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b17"">18]</ref>. In this work we b our approach is closely related to the graph convolutional network (GCN), introduced by Kipf et al. <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b17"">18]</ref>. The original GCN less"" GCN approach has parameter dimension O(|V|), so this requirement is not entirely unreasonable <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b17"">18]</ref>.</p><p>Following T "" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b23"">24]</ref>). The majority of t",1
"been applied in the transductive setting with fixed graphs <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b17"">18]</ref>. In this work we both extend GCNs to the task of inductive aph convolutional network (GCN), introduced by Kipf et al. <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b17"">18]</ref>. The original GCN algorithm <ref type=""bibr"" target=""#b16""> n O(|V|), so this requirement is not entirely unreasonable <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b17"">18]</ref>.</p><p>Following Theorem 1, we let x v ∈ U, ∀v ∈ V denote t trained on a single, fixed graph. (That said, Kipf et al <ref type=""bibr"" target=""#b16"">[17]</ref> <ref type=""bibr"" target=""#b17"">[18]</ref> found that GCN-based approach consistently outperformed De",1
"itectures for learning over graphs have been proposed (e.g., <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target= phs or are designed for whole-graph classification (or both) <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=",0
"4).</p><p>LSTM aggregator. We also examined a more complex aggregator based on an LSTM architecture <ref type=""bibr"" target=""#b13"">[14]</ref>. Compared to the mean aggregator, LSTMs have the advantage",0
"e an alignment procedure to share information between the graphs, such as the procedure proposed by <ref type=""bibr"" target=""#b11"">[12]</ref> for aligning the output of word embedding algorithms. Inve",0
"raining. One notable exception to this trend is the Planetoid-I algorithm introduced by Yang et al. <ref type=""bibr"" target=""#b39"">[40]</ref>, which is an inductive, embeddingbased approach to semi-su",0
"veral convolutional neural network architectures for learning over graphs have been proposed (e.g., <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target these methods do not scale to large graphs or are designed for whole-graph classification (or both) <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target",0
"are also a number of recent neural network approaches to supervised learning over graph structures <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" targe",0
"ode degrees using a smoothing parameter of 0.75, following <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b27"">28]</ref>. Initial experiment",0
"target=""#b8"">9,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b23"">24]</ref>). The majority of these methods do not scale to large graph "" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b23"">24]</ref>. However, our approach is closely related to the graph conv",0
"ith kernel machines, one typically observes an increase in performance when using soft-DTW over DTW <ref type=""bibr"" target=""#b6"">(Cuturi, 2011)</ref> for classification.</p><p>Our contributions. We e",1
"under the DTW geometry, a task for which our proposal significantly outperforms existing baselines <ref type=""bibr"" target=""#b17"">(Petitjean et al., 2011)</ref>. Next, we propose to tune the paramete ct to propose an alternative approach to the DBA (DTW Barycenter Averaging) clustering algorithm of <ref type=""bibr"" target=""#b17"">(Petitjean et al., 2011)</ref>, and observe that our smoothed approac the original dtw 0 discrepancy, than subgradient or alternating minimization approaches such as DBA <ref type=""bibr"" target=""#b17"">(Petitjean et al., 2011)</ref>, which can, on the contrary, get more ents</head><p>In this section, we compare the soft-DTW barycenter approach presented in §3.1 to DBA <ref type=""bibr"" target=""#b17"">(Petitjean et al., 2011)</ref> and a simple batch subgradient method. run faster in that context <ref type=""bibr"" target=""#b27"">(Yi et al., 1998)</ref>. Recent works by <ref type=""bibr"" target=""#b17"">Petitjean et al. (2011)</ref>; <ref type=""bibr"" target=""#b16"">Petitje } 6:</formula><p>end for 7: end for 8: Output: (r n,m , R) average time series under the DTW metric <ref type=""bibr"" target=""#b17"">(Petitjean et al., 2011;</ref><ref type=""bibr"" target=""#b25"">Schultz",0
"noticeably more difficult when the output objects have a structure, i.e. when they are not vectors <ref type=""bibr"" target=""#b1"">(Bakir et al., 2007)</ref>. We study here the case where each output o",0
"<ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b17"">18]</ref> and machine learning <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b3"">4]</ref> communities exposed bstitute. We expect the target DNN to misclassify them due to transferability between architectures <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b3"">4]</ref> To understand the di "" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b13"">14]</ref>, or (b) an independently collected training set to fit an a fit an auxiliary model <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b13"">14]</ref>. This limited their applicability to strong adversaries cap t the case in practice <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b13"">14]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n st adversarial attacks <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b13"">14]</ref> is to approximate its solution using gradient-based optimiz the greatest empirical success so far: adversarial training <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b13"">14]</ref>, and defensive distillation for DNNs <ref type=""bibr"" targe adversarial samples transfer between different architectures <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b13"">14]</ref>. Here, we build an intuition behind transferability based o to simultaneously provide all of these three key properties <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" tar robustness of significantly descriptive models, such as DNNs <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b16"">17]</ref>. We implemented an bution than the oracle could train a model with a different architecture and use it as a substitute <ref type=""bibr"" target=""#b13"">[14]</ref>: adversarial examples designed to manipulate the substitut",1
"models, such as DNNs <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b16"">17]</ref>. We implemented an approximation of this defense using the",0
"them to the benign or malware class. Efforts in the security <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target= ft adversarial examples using either: (a) detailed knowledge of the DNN architecture and parameters <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target "" target=""#b13"">14]</ref>, or (b) an independently collected training set to fit an auxiliary model <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target",0
"rget=""#b3"">[4,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b17"">18]</ref>. Our contributions ey demonstrated how an adversary may recover parameters from classifiers hosted by BigML and Amazon <ref type=""bibr"" target=""#b14"">[15]</ref>. However, it would be difficult to scale up the approach t",0
"fforts in the security <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b17"">18]</ref> and machine learning tecture and parameters <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b13"">14]</ref>, or (b) an independen adversarial examples showed this is not the case in practice <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b13"">14]</ref>.</p></div> <div xmlns L model: e.g., a DNN. The basis for most adversarial attacks <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b13"">14]</ref> is to approximate its 0""><head n=""3."">THREAT MODEL</head><p>A taxonomy of adversaries against DNN classifiers is found in <ref type=""bibr"" target=""#b8"">[9]</ref>. In our work, the adversary seeks to force a classifier to m where adversaries seek to take samples from any legitimate source class to any chosen target class <ref type=""bibr"" target=""#b8"">[9]</ref>. Misclassification attacks are a special case of source-targ our running set of architectures. Performances are comparable with some DNNs performing better 5 In <ref type=""bibr"" target=""#b8"">[9]</ref>, the algorithm stopped perturbing when the input reached the he adversary can use one of the previously described attacks <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b8"">9]</ref> to craft adversarial samples misclassified by F . As long as plementing two previously introduced approaches described in <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b8"">9]</ref>. We provide an overview of the two approaches, namely the Goo ial samples produced by each algorithm introduced previously <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b8"">9]</ref>, to elect the strongest technique under our threat model.</p>",0
"se access patterns <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" tar e requesting PC, and then adapt their policy to each class <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" tar [35]</ref> computes a statistical cost function from sampled reuse distance histograms. And Hawkeye <ref type=""bibr"" target=""#b15"">[16]</ref> emulates MIN's past decisions. Without a theoretical found ef type=""bibr"" target=""#b20"">[21]</ref>, PRP <ref type=""bibr"" target=""#b10"">[11]</ref>, and Hawkeye <ref type=""bibr"" target=""#b15"">[16]</ref> learn the behavior of different PCs. And PDP <ref type=""bi",1
"s common behavior.</p><p>More recently, a large body of work <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" targe replacement MDP using a reference model from our recent work <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b8"">9]</ref>. in the limit. The distance between these two lines in the li s a versatile mechanism that can implement many policies, specifically any ranking function of ages <ref type=""bibr"" target=""#b8"">[9]</ref>, such as random, LRU, PDP <ref type=""bibr"" target=""#b13"">[14",0
"target=""#b38"">[39]</ref>, 41% for DRRIP <ref type=""bibr"" target=""#b16"">[17]</ref>, and 42% for PDP <ref type=""bibr"" target=""#b13"">[14]</ref>.</p><p>Fewer misses translate into large area savings-EVA RDP <ref type=""bibr"" target=""#b26"">[27]</ref> try to predict candidates' times until reference. PDP <ref type=""bibr"" target=""#b13"">[14]</ref> protects lines from eviction for a fixed number of accesse ies that do not assume recency still base their policy on when a candidate was last referenced: PDP <ref type=""bibr"" target=""#b13"">[14]</ref> protects candidates until a certain age; and IRGD <ref typ and Hawkeye <ref type=""bibr"" target=""#b15"">[16]</ref> learn the behavior of different PCs. And PDP <ref type=""bibr"" target=""#b13"">[14]</ref> and IRGD <ref type=""bibr"" target=""#b34"">[35]</ref> use aux ance, maximizing upon the available information.</p><p>In contrast, no ""protecting distance"" in PDP <ref type=""bibr"" target=""#b13"">[14]</ref> can do so. Protecting the small array gives a hit rate of bandwidth.</p><p>A. Hardware operations Aging: Aging: Aging: Aging: We use per-set, coarsened ages <ref type=""bibr"" target=""#b13"">[14]</ref>. Each cache line has a k-bit age, and each set has a j-bit ally any ranking function of ages <ref type=""bibr"" target=""#b8"">[9]</ref>, such as random, LRU, PDP <ref type=""bibr"" target=""#b13"">[14]</ref>, IRGD <ref type=""bibr"" target=""#b34"">[35]</ref>, etc. We s stics based on observations of common-case access patterns <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" tar onitor properties independent of the replacement policy (cf. <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b34"">35]</ref>). As a result, our",0
"=""bibr"" target=""#b37"">[38]</ref>), or on systems with dedicated OS cores (e.g., the Kalray MPPA-256 <ref type=""bibr"" target=""#b11"">[12]</ref> or Fujitsu Sparc64 XIfx <ref type=""bibr"" target=""#b39"">[40",0
"cores (e.g., the Kalray MPPA-256 <ref type=""bibr"" target=""#b11"">[12]</ref> or Fujitsu Sparc64 XIfx <ref type=""bibr"" target=""#b39"">[40]</ref>).</p><p>However, if software updates are undesirable, we h",0
"rarchy.</p><p>To separate the metadata and data hierarchies, we build upon the Direct-to-Data (D2D) <ref type=""bibr"" target=""#b0"">[1]</ref> cache hierarchy, which replaces TLB lookups with access to l proposed cache optimizations under one common framework. These include:</p><p>? Direct data access <ref type=""bibr"" target=""#b0"">[1]</ref> to lower latency by using the metadata information to skip l ations. As a first step, we extend the energy-efficient, but single-core only, Direct-to-Data (D2D) <ref type=""bibr"" target=""#b0"">[1]</ref> cache framework to support multiple cores.</p></div> <div xm type=""bibr"" target=""#b38"">[39]</ref> and Sembrant et al. <ref type=""bibr"" target=""#b39"">[40]</ref>, <ref type=""bibr"" target=""#b0"">[1]</ref> extend the TLB with cacheline way information to reduce L1 c s handled by adding tracking pointers to each cacheline (e, f). More details on this can be found in<ref type=""bibr"" target=""#b0"">[1]</ref>.</p></note> 			<note xmlns=""http://www.tei-c.org/ns/1.0"" pla",1
"e browser with Telemetry <ref type=""bibr"" target=""#b31"">[32]</ref>), Server (mixes of SPEC CPU-2006 <ref type=""bibr"" target=""#b32"">[33]</ref>) and Database (TPC-C <ref type=""bibr"" target=""#b33"">[34]</",0
"ssed many time in many contexts (e.g., cooperative caching <ref type=""bibr"" target=""#b5"">[6]</ref>, <ref type=""bibr"" target=""#b21"">[22]</ref>, victim replication <ref type=""bibr"" target=""#b22"">[23]</r",0
"to see reuse <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" target=""#b12"">[13]</ref>, <ref type=""bibr"" target=""#b13"">[14]</ref>, <ref type=""bibr"" target=""#b14"">[15]</ref>, <ref type=""bib",0
"ss multicore issues. Boettcher et al. <ref type=""bibr"" target=""#b38"">[39]</ref> and Sembrant et al. <ref type=""bibr"" target=""#b39"">[40]</ref>, <ref type=""bibr"" target=""#b0"">[1]</ref> extend the TLB wi et=""#b38"">[39]</ref> keep the tag array and treat the TLB way information as hints. Sembrant et al. <ref type=""bibr"" target=""#b39"">[40]</ref> provide precise tracking and can therefore eliminate the t",0
"><ref type=""bibr"" target=""#b5"">5]</ref>,and trusted measurement is a key problem of this technology <ref type=""bibr"" target=""#b6"">[6,</ref><ref type=""bibr"" target=""#b7"">7]</ref>. Trusted computing tre",1
"ntal requirement and necessary consequence in order to ensure trust in the computing infrastructure <ref type=""bibr"" target=""#b3"">[3]</ref>.</p><p>Trusted Computing as proposed by the Trusted Computin",0
"ches or memories, or glitches in combinational logics that can propagate and be captured in latches <ref type=""bibr"" target=""#b8"">[8]</ref>. If not handled properly, such errors can cause illegal acce",0
">, as well as independent component analysis <ref type=""bibr"" target=""#b3"">[Cao et al. 2003]</ref>. <ref type=""bibr"" target=""#b4"">Cao et al. [2005]</ref> extract emotions using support vector machines lso present a user study rating the level of realism in emotion synthesis, covering several methods <ref type=""bibr"" target=""#b4"">[Cao et al. 2005;</ref><ref type=""bibr"" target=""#b27"">Liu and Osterman ing samples based on the apparent emotion <ref type=""bibr"" target=""#b0"">[Anderson et al. 2013;</ref><ref type=""bibr"" target=""#b4"">Cao et al. 2005;</ref><ref type=""bibr"" target=""#b11"">Deng et al. 2006;",1
"ect by deducing additional motion, such as head movements, eye saccades or blinks from audio (e.g., <ref type=""bibr"" target=""#b10"">[Deng et al. 2004;</ref><ref type=""bibr"" target=""#b29"">Marsella et al",0
"et=""#b27"">Liu and Ostermann 2011;</ref><ref type=""bibr"" target=""#b33"">Melenchon et al. 2009</ref>]. <ref type=""bibr"" target=""#b21"">Jia et al. [2014]</ref> use neural networks to learn a mapping from P",0
"ding phoneme coarticulation, lexical stress, and interaction between facial muscles and skin tissue <ref type=""bibr"" target=""#b13"">[Edwards et al. 2016]</ref>. Hence, we focus on the entire face, not del of <ref type=""bibr"" target=""#b42"">Taylor et al. [2012]</ref> and in the recent work dubbed JALI <ref type=""bibr"" target=""#b13"">[Edwards et al. 2016]</ref>. JALI factors the facial animation to lip anguage-specific rules, need for a nearperfect transcript for good quality (typically done manually <ref type=""bibr"" target=""#b13"">[Edwards et al. 2016]</ref>), inability to react convincingly to non- original video and audio footage from <ref type=""bibr"" target=""#b42"">Taylor et al. [2012]</ref> and <ref type=""bibr"" target=""#b13"">Edwards et al. [2016]</ref>. Note that in these comparisons we drive",0
"imization that considers the entire utterance. Subsequent work has improved the trajectory sampling <ref type=""bibr"" target=""#b0"">[Anderson et al. 2013;</ref><ref type=""bibr"" target=""#b48"">Wang and So ng captured video frames with concatenation, blending, and warping. Such image-based methods (e.g., <ref type=""bibr"" target=""#b0"">[Anderson et al. 2013;</ref><ref type=""bibr"" target=""#b9"">Deena et al. states would be to manually label or categorize the training samples based on the apparent emotion <ref type=""bibr"" target=""#b0"">[Anderson et al. 2013;</ref><ref type=""bibr"" target=""#b4"">Cao et al. 2 type=""bibr"" target=""#b47"">Wampler et al. [2007]</ref> also allow a user-specified emotional state. <ref type=""bibr"" target=""#b0"">Anderson et al. [2013]</ref> use cluster adaptive training to derive a",0
"eature interactions from raw data automatically. A popular approach is factorization machines (FMs) <ref type=""bibr"" target=""#b26"">[27]</ref>, which embeds features into a latent space and models the underlying structure, FM may not be expressive enough. Although higher-order FMs have been proposed <ref type=""bibr"" target=""#b26"">[27]</ref>, they still belong to the family of linear models and are ing the second-order factorized interactions between features. By specifying input features, Rendle <ref type=""bibr"" target=""#b26"">[27]</ref> showed that FM can mimic many speci c factorization models value, rather than simply an embedding table lookup, so as to account for the real valued features <ref type=""bibr"" target=""#b26"">[27]</ref>.</p><p>Bi-Interaction Layer. We then feed the embedding se nsorFlow implementation<ref type=""foot"" target=""#foot_7"">7</ref> of higherorder FM, as described in <ref type=""bibr"" target=""#b26"">[27]</ref>. We experimented with order size 3, since the MovieLens da n Machines</head><p>Factorization machines are originally proposed for collaborative recommendation <ref type=""bibr"" target=""#b26"">[27,</ref><ref type=""bibr"" target=""#b29"">30]</ref>. Given a real valu",1
"]</ref>, they still belong to the family of linear models and are claimed to be di cult to estimate <ref type=""bibr"" target=""#b27"">[28]</ref>. Moreover, they are known to have only marginal improvemen ag recommendation. With one hidden layer only, our NFM signi cantly outperforms FM (the ocial LibFM <ref type=""bibr"" target=""#b27"">[28]</ref> implementation) with a 7.3% improvement. Compared to the s te) linear models. In other words, the predicted target ˆ (x) is linear w.r.t. each model parameter <ref type=""bibr"" target=""#b27"">[28]</ref>. Formally, for each model parameter θ ∈ {w 0 , {w i }, { i itive embedding-based models that are speci cally designed for sparse data prediction:</p><p>-LibFM <ref type=""bibr"" target=""#b27"">[28]</ref>. is is the o cial implementation <ref type=""foot"" target=""",1
"ality prediction. In future, we will improve the e ciency of NFM by resorting to hashing techniques <ref type=""bibr"" target=""#b31"">[32,</ref><ref type=""bibr"" target=""#b40"">41]</ref> to make it more su",0
"br"" target=""#b28"">[29,</ref><ref type=""bibr"" target=""#b36"">37]</ref> or contrastive max-margin loss <ref type=""bibr"" target=""#b42"">[43]</ref>. In this work, we focus on the regression task and optimiz",0
"categorical variables. Although DNNs have exhibited strong ability to learn pa erns from dense data <ref type=""bibr"" target=""#b13"">[14]</ref>, the use of DNNs on sparse data has received less scrutiny imilar framework with Wide&amp;Deep by replacing the MLP with the state-of-the-art residual network <ref type=""bibr"" target=""#b13"">[14]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head due to the notorious problems of vanishing/exploding gradients, over ing, degradation, among others <ref type=""bibr"" target=""#b13"">[14]</ref>.</p><p>To demonstrate optimization di culties of DNNs empi amp;Deep, the training error is relatively high, which is likely because of the degradation problem <ref type=""bibr"" target=""#b13"">[14]</ref>. For DeepCross, the very low training error but high test ce the Bi-Interaction pooling with concatenation and apply a tower-structure MLP (or residual units <ref type=""bibr"" target=""#b13"">[14]</ref>) to hidden layers, we can recover the Wide&amp;Deep (or De , it is not technically di cult to build very deep models with hundreds or even thousands of layers <ref type=""bibr"" target=""#b13"">[14]</ref>. However, deeper models do not necessarily lead to be er r en shown that BN leads to faster convergence and be er performance in several computer vision tasks <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b41"">42]</ref>.</p></div> <div xm preventing over ing and as such, be er generalization can be achieved. enabled with a ratio of 0.  <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b19"">20]</ref> showed that by add",0
"ective function with regularizers like the graph Laplacian <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b33"">34]</ref>. Lastly, we are interested in exploring the Bi-Interaction",0
"ive ML models with such sparse data, it is crucial to account for the interactions between features <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" targ ime FM has been expressed under the neural network framework. While a recent work by Blondel et al. <ref type=""bibr"" target=""#b3"">[4]</ref> has uni ed FM and Polynomial network via kernelization, thei",0
"ating nonlinearities is to extend the objective function with regularizers like the graph Laplacian <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b33"">34]</ref>. Lastly, we are in",0
"binary features (a.k.a. feature vector) via one-hot encoding <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" targe actions under sparse se ings. Until very recently, some work <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" targe ful solutions in both industry and academia largely rely on manually cra ing combinatorial features <ref type=""bibr"" target=""#b8"">[9]</ref>, i.e., constructing new features by combining multiple predi 7.3% improvement. Compared to the state-of-the-art deep learning methods -the 3-layer Wide&amp;Deep <ref type=""bibr"" target=""#b8"">[9]</ref> and 10-layer DeepCross <ref type=""bibr"" target=""#b30"">[31]</ ral Network (FNN), which uses the feature embeddings learned by FM to initialize DNNs. Cheng et al. <ref type=""bibr"" target=""#b8"">[9]</ref> proposed Wide&amp;Deep for App recommendation, where the dee vieLens data concerns the ternary relationship between users, movies and tags.</p><p>-Wide&amp;Deep <ref type=""bibr"" target=""#b8"">[9]</ref>. As introduced in Section 2.2, the deep part rst concatenate prove FM's expressiveness. In contrast to traditional deep learning methods that simply concatenate <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b30"">31,</ref><ref type=""bibr"" targ ng-based methods become increasingly popular, which try to learn feature interactions from raw data <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" targ on. While it is claimed that multiple non-linear layers are able to learn feature interactions well <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b30"">31]</ref>, such a deep archite for the structure of fully connected layers (i.e., size of each layer), one can freely choose tower <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b15"">16]</ref>, constant <ref type= . NFM has a similar multi-layered neural architecture with several existing deep learning solutions <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" targ",0
"imic many speci c factorization models such as the standard MF, parallel factor analysis, and SVD++ <ref type=""bibr"" target=""#b21"">[22]</ref>, Owing to such genericity, FM has been recognized as one o",0
"Release Consistency (RC) are often mixed with the concept of ""SC for data-race-free (DRF) programs"" <ref type=""bibr"" target=""#b27"">[28]</ref>. It should be noted that ""SC for DRF"" is inadequate for an",1
"e two models is regarding store atomicity, which is often classified into the following three types <ref type=""bibr"" target=""#b18"">[19]</ref>:</p><p>? Single-copy atomic: a store becomes visible to al proposed a hardware scheme to dynamically convert programs across memory models described in MOSTs <ref type=""bibr"" target=""#b18"">[19]</ref>. MOST specifies the ordering strength (e.g., locally order",0
"uniprocessors, including even the load-value speculation <ref type=""bibr"" target=""#b19"">[20]</ref>- <ref type=""bibr"" target=""#b23"">[24]</ref>, without additional checks or logic. We give operational d he result of I 4 . A processor with loadvalue prediction <ref type=""bibr"" target=""#b19"">[20]</ref>- <ref type=""bibr"" target=""#b23"">[24]</ref> may guess the result of I 4 before executing it, and issue",0
"operational definitions: memory models of x86, ARM and POWER have all been formalized operationally <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b6"">[7]</ref>, <ref type=""bibr"" t the style used in the operational definitions of SC <ref type=""bibr"" target=""#b0"">[1]</ref> and TSO <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b24"">[25]</ref>. An I 2 E abstrac ty. Instead the manufactures and researchers have chosen to present weaker memory models, e.g., TSO <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b2"">[3]</ref>, <ref type=""bibr"" t mlns=""http://www.tei-c.org/ns/1.0""><head>A. TSO Model</head><p>The TSO abstract machine proposed in <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b24"">[25]</ref> (Figure <ref type",0
"anguages: C++ <ref type=""bibr"" target=""#b25"">[26]</ref>, <ref type=""bibr"" target=""#b46"">[47]</ref>- <ref type=""bibr"" target=""#b49"">[50]</ref>, Java <ref type=""bibr"" target=""#b50"">[51]</ref>- <ref type",0
"sociations. Moreover, DPCNN can be regarded as a deep extension of ShallowCNN, which we proposed in <ref type=""bibr"" target=""#b6"">(Johnson and Zhang, 2015b</ref>) and later tested with large datasets gion embedding enhanced with unsupervised embeddings (embeddings trained in an unsupervised manner) <ref type=""bibr"" target=""#b6"">(Johnson and Zhang, 2015b)</ref> for improving accuracy.</p></div> <di ing each word in the text to a word vector (word embedding). We take a more general viewpoint as in <ref type=""bibr"" target=""#b6"">(Johnson and Zhang, 2015b)</ref> and consider text region embedding -e w-1 as input, serves as an unsupervised embedding function in the model for text categorization. In <ref type=""bibr"" target=""#b6"">(Johnson and Zhang, 2015b)</ref> unsupervised embeddings obtained this ings. Note that ShallowCNN enhanced with unsupervised embeddings (row 2) was originally proposed in <ref type=""bibr"" target=""#b6"">(Johnson and Zhang, 2015b)</ref> as a semi-supervised extension of <re",1
"ayer character-level CNNs of <ref type=""bibr"" target=""#b14"">(Zhang et al., 2015)</ref>. However, in <ref type=""bibr"" target=""#b7"">(Johnson and Zhang, 2016)</ref>, very shallow 1-layer word-level CNNs f type=""bibr"" target=""#b6"">(Johnson and Zhang, 2015b</ref>) and later tested with large datasets in <ref type=""bibr"" target=""#b7"">(Johnson and Zhang, 2016)</ref>.</p><p>We show that DPCNN with 15 weig others are in English. Classes are balanced on all the datasets. Data preprocessing was done as in <ref type=""bibr"" target=""#b7"">(Johnson and Zhang, 2016)</ref>. That is, upper-case letters were conv r padding; however, the vocabulary size was limited to 30K words. For example, as also mentioned in <ref type=""bibr"" target=""#b7"">(Johnson and Zhang, 2016)</ref>, the complete vocabulary of the Ama.p tate comparison with ShallowCNN, we matched our unsupervised embedding setting exactly with that of <ref type=""bibr"" target=""#b7"">(Johnson and Zhang, 2016)</ref>. That is, we trained the same four typ ibr"" target=""#b5"">(Johnson and Zhang, 2015a)</ref>, and then it was tested on the large datasets in <ref type=""bibr"" target=""#b7"">(Johnson and Zhang, 2016)</ref>. The performance improvements of DPCNN layers (blue circles from left to right). For comparison, the Shal-lowCNN results (green 'x') from <ref type=""bibr"" target=""#b7"">(Johnson and Zhang, 2016)</ref> are also shown. The x-axis represents respective papers.</figDesc><table><row><cell>-dim bow or 200K-dim</cell></row></table><note>[JZ16]:<ref type=""bibr"" target=""#b7"">Johnson and Zhang (2016)</ref>, [YYDHSH16]: Yang et al. (2016), [CSBL1",1
"ime is bounded by a constant.</p><p>• Shortcut connections with pre-activation and identity mapping <ref type=""bibr"" target=""#b2"">(He et al., 2016)</ref> for enabling training of deep networks.</p><p> ions with identity mapping, which can be written as z + f (z) where f represents the skipped layers <ref type=""bibr"" target=""#b2"">(He et al., 2016)</ref>. In DPCNN, the skipped layers f (z) are two co d that preactivation indeed outperformed 'post-activation', which is in line with the image results <ref type=""bibr"" target=""#b2"">(He et al., 2016)</ref>.</p><p>No need for dimension matching Although imension matching Although the shortcut with pre-activation was adopted from the improved ResNet of <ref type=""bibr"" target=""#b2"">(He et al., 2016)</ref>, our model is simpler than ResNet (Figure <ref arget=""#b11"">(Simonyan and Zisserman, 2015;</ref><ref type=""bibr"" target=""#b1"">He et al., 2015</ref><ref type=""bibr"" target=""#b2"">He et al., , 2016;;</ref><ref type=""bibr"" target=""#b0"">Conneau et al.,",0
"y half.</p><p>A number of models <ref type=""bibr"" target=""#b11"">(Simonyan and Zisserman, 2015;</ref><ref type=""bibr"" target=""#b1"">He et al., 2015</ref><ref type=""bibr"" target=""#b2"">He et al., , 2016;; be trained. The number of W's rows is the number of feature maps (also called the number of filters <ref type=""bibr"" target=""#b1"">(He et al., 2015)</ref>) of this layer. We set activation σ(•) to the 6). It was meant to reduce learning rate when error plateaus, as is often done on image tasks, e.g.,<ref type=""bibr"" target=""#b1"">(He et al., 2015)</ref>, though for simplicity, the timing of reductio",0
"linearity' eases training of deep networks, similar to the role of constant error carousels in LSTM <ref type=""bibr"" target=""#b4"">(Hochreiter and Schmidhuder, 1997)</ref>. We empirically observed that",0
"b0"">Conneau et al., 2016)</ref>, a complex combination of CNNs and recurrent neural networks (RNNs) <ref type=""bibr"" target=""#b12"">(Tang et al., 2015)</ref>, and RNNs in a wordsentence hierarchy <ref ex than DPCNN due to the use of RNNs and linguistic knowledge for sentence segmentation. Similarly, <ref type=""bibr"" target=""#b12"">Tang et al. (2015)</ref> proposed to use CNN or LSTM to represent eac",0
"d to 100. Regularization was done by weight decay with the parameter 0.0001 and by optional dropout <ref type=""bibr"" target=""#b3"">(Hinton et al., 2012)</ref> with 0.5 applied to the input to the top l",0
"be effective for text categorization. While simple and shallow convolutional neural networks (CNNs) <ref type=""bibr"" target=""#b9"">(Kim, 2014;</ref><ref type=""bibr"">John-son and Zhang, 2015a)</ref> wer on and Zhang, 2016)</ref>. That is, upper-case letters were converted to lower-case letters. Unlike <ref type=""bibr"" target=""#b9"">(Kim, 2014;</ref><ref type=""bibr"" target=""#b14"">Zhang et al., 2015;</r",0
"NNs) <ref type=""bibr"" target=""#b12"">(Tang et al., 2015)</ref>, and RNNs in a wordsentence hierarchy <ref type=""bibr"" target=""#b13"">(Yang et al., 2016)</ref>.</p><p>A CNN is a feedforward network with over ShallowCNN indicates that the added depth is indeed useful, capturing more global information. <ref type=""bibr"" target=""#b13"">Yang et al. (2016)</ref>'s hierarchical attention network (row 3) con",0
"=""bibr"" target=""#b35"">(Zhu et al., 2015;</ref><ref type=""bibr"" target=""#b30"">Tai et al., 2015;</ref><ref type=""bibr"" target=""#b15"">Le and Zuidema, 2015)</ref>, which extends the chain LSTM to a recurs",1
"o this end, we will also encode syntactic parse trees of a premise and hypothesis through tree-LSTM <ref type=""bibr"" target=""#b35"">(Zhu et al., 2015;</ref><ref type=""bibr"" target=""#b30"">Tai et al., 20 ""#b20"">(Munkhdalai and Yu, 2016b)</ref>.</p><p>We ensemble our ESIM model with syntactic tree-LSTMs <ref type=""bibr"" target=""#b35"">(Zhu et al., 2015)</ref> based on syntactic parse trees and achieve s here are no enough leaves to form a full tree. Each tree node is implemented with a tree-LSTM block <ref type=""bibr"" target=""#b35"">(Zhu et al., 2015)</ref> same as in model ( <ref type=""formula"" targe",1
"f a premise and hypothesis through tree-LSTM <ref type=""bibr"" target=""#b35"">(Zhu et al., 2015;</ref><ref type=""bibr"" target=""#b30"">Tai et al., 2015;</ref><ref type=""bibr"" target=""#b15"">Le and Zuidema,",1
"f 0.5, which is applied to all feedforward connections. We use pre-trained 300-D Glove 840B vectors <ref type=""bibr"" target=""#b24"">(Pennington et al., 2014)</ref> to initialize our word embeddings. Ou",0
"t=""#b2"">(Bowman et al., , 2016;;</ref><ref type=""bibr"" target=""#b20"">Munkhdalai and Yu, 2016b;</ref><ref type=""bibr"" target=""#b22"">Parikh et al., 2016;</ref><ref type=""bibr"" target=""#b27"">Sha et al., br"" target=""#b32"">Wang and Jiang, 2016;</ref><ref type=""bibr"" target=""#b6"">Cheng et al., 2016;</ref><ref type=""bibr"" target=""#b22"">Parikh et al., 2016;</ref><ref type=""bibr"" target=""#b20"">Munkhdalai a =""#b21"">Paria et al., 2016)</ref>. Among them, more relevant to ours are the approaches proposed by <ref type=""bibr"" target=""#b22"">Parikh et al. (2016)</ref> and <ref type=""bibr"" target=""#b20"">Munkhda e=""bibr"" target=""#b20"">Munkhdalai and Yu (2016b)</ref>, which are among the best performing models. <ref type=""bibr"" target=""#b22"">Parikh et al. (2016)</ref> propose a relatively simple but very effec icated combinations of attention models, which provide about 0.5% gain over the results reported by <ref type=""bibr"" target=""#b22"">Parikh et al. (2016)</ref>.</p><p>It is, however, not very clear if t r"">(Mac-Cartney, 2009)</ref>. In neural network models, this is often achieved with soft attention. <ref type=""bibr"" target=""#b22"">Parikh et al. (2016)</ref> decomposed this process: the word sequence -trained word embedding by itself does not automatically consider the context around a word in NLI. <ref type=""bibr"" target=""#b22"">Parikh et al. (2016)</ref> did take into account the word order and c his plays an important role in achieving our best results, and the intra-sentence attention used by <ref type=""bibr"" target=""#b22"">Parikh et al. (2016)</ref> actually does not further improve over our TM to encode the premise and hypothesis, respectively. In our sequential inference model, unlike in <ref type=""bibr"" target=""#b22"">Parikh et al. (2016)</ref> which proposed to use a function F (ā i ), "" target=""#b6"">(Cheng et al., 2016)</ref> link the current word to previous words stored in memory. <ref type=""bibr"" target=""#b22"">Parikh et al. (2016)</ref> proposed a decomposable attention model wi layers. If we remove the pooling layer in inference composition and replace it with summation as in <ref type=""bibr"" target=""#b22"">Parikh et al. (2016)</ref>, the accuracy drops to 87.1%. If we remove inference enhancement layer, the accuracy drops to 87.0%. To provide some detailed comparison with <ref type=""bibr"" target=""#b22"">Parikh et al. (2016)</ref>, replacing bidirectional LSTMs in inferenc to the final classifier to determine the overall inference relationship. We consider that summation <ref type=""bibr"" target=""#b22"">(Parikh et al., 2016</ref>) could be sensitive to the sequence length n <ref type=""bibr"">(Cheng et al., 2016) 3.4M 88.5 86.3 (11)</ref> 200D decomposable attention model <ref type=""bibr"" target=""#b22"">(Parikh et al., 2016)</ref> 380K 89.5 86.3 (12) Intra-sentence attent bibr"" target=""#b22"">(Parikh et al., 2016)</ref> 380K 89.5 86.3 (12) Intra-sentence attention + (11) <ref type=""bibr"" target=""#b22"">(Parikh et al., 2016)</ref> 580K 90.5 86.8 (13) 300D NTI-SLSTM-LSTM <",0
"et al., 2015)</ref>, and text summarization <ref type=""bibr"" target=""#b26"">(Rush et al., 2015;</ref><ref type=""bibr"" target=""#b4"">Chen et al., 2016)</ref>, among others. For NLI, the idea allows neura",0
"ased models have been shown to be effective in a wide range of tasks, including machine translation <ref type=""bibr"" target=""#b0"">(Bahdanau et al., 2014)</ref>, speech recognition <ref type=""bibr"" tar",0
"""#b20"">Munkhdalai and Yu, 2016a;</ref><ref type=""bibr"" target=""#b25"">Rocktäschel et al., 2015;</ref><ref type=""bibr"" target=""#b32"">Wang and Jiang, 2016;</ref><ref type=""bibr"" target=""#b6"">Cheng et al. ntion <ref type=""bibr"" target=""#b25"">(Rocktäschel et al., 2015)</ref> 250K 85.3 83.5 (9) 300D mLSTM <ref type=""bibr"" target=""#b32"">(Wang and Jiang, 2016)</ref> 1.9M 92.0 86.1 (10) 450D LSTMN with deep cktäschel et al. (2015)</ref> is LSTMs enforcing the so called word-by-word attention. The model of <ref type=""bibr"" target=""#b32"">Wang and Jiang (2016)</ref> extends this idea to explicitly enforce w he model of <ref type=""bibr"" target=""#b20"">Munkhdalai and Yu (2016b)</ref> extends the framework of <ref type=""bibr"" target=""#b32"">Wang and Jiang (2016)</ref> to a full n-ary tree model and achieves f",0
"bibr"" target=""#b22"">Parikh et al., 2016;</ref><ref type=""bibr"" target=""#b27"">Sha et al., 2016;</ref><ref type=""bibr"" target=""#b21"">Paria et al., 2016)</ref>.</p><p>While some previous top-performing m target=""#b20"">Munkhdalai and Yu, 2016b;</ref><ref type=""bibr"" target=""#b27"">Sha et al., 2016;</ref><ref type=""bibr"" target=""#b21"">Paria et al., 2016)</ref>. Among them, more relevant to ours are the ref type=""bibr"" target=""#b27"">(Sha et al., 2016)</ref> 2.0M 90.7 87.5 (15) 300D btree-LSTM encoders <ref type=""bibr"" target=""#b21"">(Paria et al., 2016)</ref> 2.0M 88.6 87.6</p><p>(16) 600D ESIM 4.3M 9",0
"literature survey), which includes a large bulk of work on recognizing textual entailment, such as <ref type=""bibr"" target=""#b9"">(Dagan et al., 2005;</ref><ref type=""bibr"" target=""#b12"">Iftene and Ba",0
"e to us. In many problems, syntax and semantics interact closely, including in semantic composition <ref type=""bibr"" target=""#b23"">(Partee, 1995)</ref>, among others. Complicated tasks such as natural",0
"/ref>, image caption <ref type=""bibr"" target=""#b33"">(Xu et al., 2015)</ref>, and text summarization <ref type=""bibr"" target=""#b26"">(Rush et al., 2015;</ref><ref type=""bibr"" target=""#b4"">Chen et al., 2",0
"on recognizing textual entailment, such as <ref type=""bibr"" target=""#b9"">(Dagan et al., 2005;</ref><ref type=""bibr"" target=""#b12"">Iftene and Balahur-Dobrescu, 2007)</ref>, among others. More recently",0
"target=""#b1"">(Bowman et al., 2015</ref><ref type=""bibr"" target=""#b2"">(Bowman et al., , 2016;;</ref><ref type=""bibr"" target=""#b20"">Munkhdalai and Yu, 2016b;</ref><ref type=""bibr"" target=""#b22"">Parikh ibr"" target=""#b6"">Cheng et al., 2016;</ref><ref type=""bibr"" target=""#b22"">Parikh et al., 2016;</ref><ref type=""bibr"" target=""#b20"">Munkhdalai and Yu, 2016b;</ref><ref type=""bibr"" target=""#b27"">Sha et forming models use rather complicated network architectures to achieve the state-of-the-art results <ref type=""bibr"" target=""#b20"">(Munkhdalai and Yu, 2016b)</ref>, we demonstrate in this paper that e <ref type=""bibr"" target=""#b22"">(Parikh et al., 2016)</ref> 580K 90.5 86.8 (13) 300D NTI-SLSTM-LSTM <ref type=""bibr"" target=""#b20"">(Munkhdalai and Yu, 2016b)</ref> 3.2M 88.5 87.3 (14) 300D re-read LST erformed all the previous models, including those using much more complicated network architectures <ref type=""bibr"" target=""#b20"">(Munkhdalai and Yu, 2016b)</ref>.</p><p>We ensemble our ESIM model wi e=""bibr"" target=""#b19"">Mou et al., 2016;</ref><ref type=""bibr"" target=""#b16"">Liu et al., 2016;</ref><ref type=""bibr"" target=""#b20"">Munkhdalai and Yu, 2016a;</ref><ref type=""bibr"" target=""#b25"">Rocktäs o ours are the approaches proposed by <ref type=""bibr"" target=""#b22"">Parikh et al. (2016)</ref> and <ref type=""bibr"" target=""#b20"">Munkhdalai and Yu (2016b)</ref>, which are among the best performing model decomposes the NLI problem into subproblems that can be solved separately. On the other hand, <ref type=""bibr"" target=""#b20"">Munkhdalai and Yu (2016b)</ref> propose much more complicated network prising as it could help align the relevant text spans between premise and hypothesis. The model of <ref type=""bibr"" target=""#b20"">Munkhdalai and Yu (2016b)</ref> extends the framework of <ref type=""b e representations, and then replaces average pooling with intra-attention. The approach proposed by <ref type=""bibr"" target=""#b20"">Munkhdalai and Yu (2016a)</ref>  (2) 300D LSTM encoders <ref type=""bi coders <ref type=""bibr"" target=""#b16"">(Liu et al., 2016)</ref> 2.8M 84.5 84.2 (7) 300D NSE encoders <ref type=""bibr"" target=""#b20"">(Munkhdalai and Yu, 2016a)</ref> 3.0M 86.2 84.6</p><p>(8) 100D LSTM w",0
"s are heterogeneous in nature, involving diversity of node types and/or relationships between nodes <ref type=""bibr"" target=""#b24"">[25]</ref>. ese heterogeneous networks present unique challenges that </ref><ref type=""bibr"" target=""#b35"">35]</ref>. In contrast to conventional meta-path-based methods <ref type=""bibr"" target=""#b24"">[25]</ref>, the advantage of latent-space representation learning lie tructures and semantics of a given heterogeneous network. In metapath2vec, we rst propose meta-path <ref type=""bibr"" target=""#b24"">[25]</ref> based random walks in heterogeneous networks to generate h 1 • R 2 • • • • • R l −1 de</formula><p>nes the composite relations between node types V 1 and V l <ref type=""bibr"" target=""#b24"">[25]</ref>. Take Figure <ref type=""figure"">2</ref>(a) as an example, s in heterogeneous academic networks are ""APA"" and ""APVPA"" <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b24"">[25]</ref><ref type=""bibr"" target=""#b25"">[26]</ref><ref type=""bibr"" t ""bibr"" target=""#b20"">[21]</ref>. In speci c, we leverage the de nition of heterogeneous networks in <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b26"">27]</ref> and present the le commonly used in a symmetric way, that is, its rst node type V 1 is the same with the last one V l <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" ta ormation networks can bene t from the modeling of meta-paths <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b26"">27]</ref>.   </p><formula xml",1
"ng, most notably the group of NLP models known as word2vec <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b17"">18]</ref>. A number of recent research publications have proposed wor learn the distributed representations of words in a corpus <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b17"">18]</ref>. Inspired by it, DeepWalk <ref type=""bibr"" target=""#b21"">[2 lelized by using the same mechanism as word2vec and node2vec <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b17"">18]</ref>. All codes are implemented in C and C++ and our experiments e distributed representations of words in natural language <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b17"">18]</ref>. Building on word2vec, Perozzi et al. suggested that the ""c ghborhoods with network semantics for various types of nodes. Second, we extend the skip-gram model <ref type=""bibr"" target=""#b17"">[18]</ref> to facilitate the modeling of geographically and semantica p 2 &amp; p 3 ).</p><p>To achieve e cient optimization, Mikolov et al. introduced negative sampling <ref type=""bibr"" target=""#b17"">[18]</ref>, in which a relatively small set of words (nodes) are samp aximize the network probability in terms of local structures <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b21"">22]</ref>, that is:</p><formu d as a so max function <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b23"">24]</ref>, that is:</p><formu",1
"have been proposed for learning representations in networks <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" targe",0
"target=""#b34"">34]</ref>, such as the application of factorization models for recommendation systems <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b15"">16]</ref>, node classi catio",0
"twork embeddings can be further applied to various network mining tasks, such as node classi cation <ref type=""bibr"" target=""#b12"">[13]</ref>, clustering <ref type=""bibr"" target=""#b26"">[27,</ref><ref s over three classical heterogeneous network mining tasks, including multi-class node classi cation <ref type=""bibr"" target=""#b12"">[13]</ref>, node clustering <ref type=""bibr"" target=""#b26"">[27]</ref>",0
"ype=""bibr"" target=""#b25"">[26]</ref>. In addition, we also use the embedding projector in TensorFlow <ref type=""bibr"" target=""#b0"">[1]</ref> to visualize the node embeddings learned from the heterogene",0
"epresentation learning frameworks, such as DeepWalk <ref type=""bibr"" target=""#b21"">[22]</ref>, LINE <ref type=""bibr"" target=""#b29"">[30]</ref>, and node2vec <ref type=""bibr"" target=""#b7"">[8]</ref>. Ins 2vec <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b21"">22]</ref> LINE (1st+2nd) <ref type=""bibr"" target=""#b29"">[30]</ref> PTE <ref type=""bibr"" target=""#b28"">[29]</ref> metapath2vec ore we use p=1 and q=1 <ref type=""bibr"" target=""#b7"">[8]</ref> in node2vec for comparison. (2) LINE <ref type=""bibr"" target=""#b29"">[30]</ref>: We use the advanced version of LINE by considering both t decomposed a node's context into rst-order (friends) and second-order (friends' friends) proximity <ref type=""bibr"" target=""#b29"">[30]</ref>, which was further developed into a semi-supervised model homogeneous networks <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b29"">30]</ref>. Speci cally, conventional models su er from the identical ighborhood (context) <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b29"">30]</ref>. In a heterogeneous environment, how do we de ne and model",0
"lications have proposed word2vec-based network representation learning frameworks, such as DeepWalk <ref type=""bibr"" target=""#b21"">[22]</ref>, LINE <ref type=""bibr"" target=""#b29"">[30]</ref>, and node2 e=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b17"">18]</ref>. Inspired by it, DeepWalk <ref type=""bibr"" target=""#b21"">[22]</ref> and node2vec <ref type=""bibr"" target=""#b7"">[8]</ref> aim t ed Random Walks. How to e ectively transform the structure of a network into skip-gram? In DeepWalk <ref type=""bibr"" target=""#b21"">[22]</ref> and node2vec <ref type=""bibr"" target=""#b7"">[8]</ref>, this and metapath2vec++ with several recent network representation learning methods:</p><p>(1) DeepWalk <ref type=""bibr"" target=""#b21"">[22]</ref> / node2vec <ref type=""bibr"" target=""#b7"">[8]</ref>: With t suggested that the ""context"" of a node can be denoted by their co-occurrence in a random walk path <ref type=""bibr"" target=""#b21"">[22]</ref>. Formally, they put random walkers over networks to record ref type=""bibr"" target=""#b25"">[26]</ref> DeepWalk / node2vec <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b21"">22]</ref> LINE (1st+2nd) <ref type=""bibr"" target=""#b29"">[30]</ref> PT of local structures <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b21"">22]</ref>, that is:</p><formula xml:id=""formula_2"">arg max θ ∈V c ∈N etwork embedding models, which focus on homogeneous networks <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b29"">30]</ref>. Speci cally, conve the proximity between a node and its neighborhood (context) <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b29"">30]</ref>. In a heterogeneous",0
"target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b23"">24]</ref>, that is:</p><formula xml:id=""formula_4"">p(c t | ; θ ) = e",0
"ng from position s. This neural network combines the roles of both policy network and value network <ref type=""bibr"" target=""#b11"">12</ref> into a single architecture. The neural network consists of m perfect information. We follow the formalism of alternating Markov games described in previous work <ref type=""bibr"" target=""#b11"">12</ref> , noting that algorithms based on value or policy iteration ompare three distinct versions of AlphaGo:</p><p>1. AlphaGo Fan is the previously published program <ref type=""bibr"" target=""#b11"">12</ref> that played against Fan Hui in October 2015. This program wa described in this paper. However, it uses the same handcrafted features and rollouts as AlphaGo Lee <ref type=""bibr"" target=""#b11"">12</ref> and training was initialised by supervised learning from hum ue component, it was possible to avoid overfitting to the values (a problem described in prior work <ref type=""bibr"" target=""#b11"">12</ref> ). After 72 hours the move prediction accuracy exceeded the the KGS test set; the value prediction error was also substantially better than previously reported <ref type=""bibr"" target=""#b11"">12</ref> . The validation set was composed of professional games from of AlphaGo Fan, Crazy Stone, Pachi and GnuGo were anchored to the tournament values from prior work <ref type=""bibr"" target=""#b11"">12</ref> , and correspond to the players reported in that work. The r lso performed against baseline players with Elo ratings anchored to the previously published values <ref type=""bibr"" target=""#b11"">12</ref> .</p><p>We measured the head-to-head performance of AlphaGo hat maximise an upper confidence bound Q(s, a) + U (s, a), where U (s, a) ∝ P (s, a)/(1 + N (s, a)) <ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b23"">24</ref> , until a leaf node After 72 hours the move prediction accuracy exceeded the state of the art reported in previous work <ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b29"">[30]</ref><ref type=""bibr"" ta",1
"ed -even those filling in the player's own eyes (a standard heuristic used in all previous programs <ref type=""bibr"" target=""#b66"">67</ref> ).</p><p>The algorithm was started with random initial param",0
"(s, a) + U (s, a), where U (s, a) ∝ P (s, a)/(1 + N (s, a)) <ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b23"">24</ref> , until a leaf node s is encountered. This leaf position is in the search tree, a t = argmax a Q(s t , a) + U (s t , a) , using a variant of the PUCT algorithm <ref type=""bibr"" target=""#b23"">24</ref> ,</p><formula xml:id=""formula_2"">U (s, a) = c puct P (s, a)",0
"ndustrial control <ref type=""bibr"" target=""#b60"">[61]</ref><ref type=""bibr"" target=""#b61"">[62]</ref><ref type=""bibr"" target=""#b62"">[63]</ref> , and online recommendation systems <ref type=""bibr"" targe",0
""" target=""#b53"">54</ref> , Scrabble <ref type=""bibr"" target=""#b54"">55</ref> and most recently poker <ref type=""bibr"" target=""#b55"">56</ref> . In all of these examples, a value function was trained by get=""#b54"">55,</ref><ref type=""bibr"" target=""#b56"">57</ref> , or counterfactual regret minimisation <ref type=""bibr"" target=""#b55"">56</ref> . However, these methods utilised handcrafted input features 50"">51,</ref><ref type=""bibr"" target=""#b51"">52</ref> , handcrafted restrictions on the action space <ref type=""bibr"" target=""#b55"">56</ref> , or used pre-existing computer programs as training opponen ned by regression <ref type=""bibr"" target=""#b53"">[54]</ref><ref type=""bibr"" target=""#b54"">[55]</ref><ref type=""bibr"" target=""#b55"">[56]</ref> or temporaldifference learning <ref type=""bibr"" target=""#b",0
""">31]</ref>. As the pioneer CNN model for SR, Super-Resolution Convolutional Neural Network (SRCNN) <ref type=""bibr"" target=""#b1"">[2]</ref> predicts the nonlinear LR-HR mapping via a fully convolution monality among the above CNN models is that their networks contain fewer than 5 layers, e.g., SRCNN <ref type=""bibr"" target=""#b1"">[2]</ref> uses 3 convolutional layers. Their deeper structures with 4 ween the input ILR image and the output HR image. There are three notes for VDSR: (1) Un-like SRCNN <ref type=""bibr"" target=""#b1"">[2]</ref> that only uses 3 layers, VDSR stacks 20 weight layers (3 × 3 arget=""#b32"">[34]</ref>. The results are presented in Tab. 3. Note that Dataset Scale Bicubic SRCNN <ref type=""bibr"" target=""#b1"">[2]</ref> SelfEx <ref type=""bibr"" target=""#b9"">[10]</ref> RFL <ref typ e as <ref type=""bibr"" target=""#b12"">[13]</ref> reported in Tab. Qualitative comparisons among SRCNN <ref type=""bibr"" target=""#b1"">[2]</ref>, SelfEx <ref type=""bibr"" target=""#b9"">[10]</ref>, VDSR <ref PSyCo [20] and IA <ref type=""bibr"" target=""#b28"">[30]</ref>. The deep models (d ≤ 8) include SRCNN <ref type=""bibr"" target=""#b1"">[2]</ref>, DJSR <ref type=""bibr"" target=""#b31"">[33]</ref>, CSCN <ref t rsity of images. Shi et al. <ref type=""bibr"" target=""#b23"">[25]</ref> observe that the prior models <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b30"">32]</ref> increase LR image's 1</ref> shows the Peak Signal-to-Noise Ratio (PSNR) performance of several recent CNN models for SR <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" targ , k = 297K) structure, which has the same depth as VDSR and DRCN, but fewer parameters. Both the DL <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" targ only. Therefore, the input and output images are of the same size. For fair comparison, similar to <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" targ type=""bibr"" target=""#b9"">[10]</ref> RFL <ref type=""bibr"" target=""#b21"">[23]</ref>   the results of <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"">20,</",1
"hieves better performance than the state-of-theart methods <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b13"">14]</ref>. After increasing the depth without adding any parameters, eNet <ref type=""bibr"" target=""#b19"">[21]</ref>, Kim et al. <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b13"">14]</ref> propose two very deep convolutional networks for SR, both s ameters. Both the DL <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b13"">14]</ref> and non-DL <ref type=""bibr"" target=""#b9"">[10,</ref><ref typ testing sets, by citing the results of prior methods from <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b13"">14]</ref>. The two DRRN models outperforms all existing methods in al chieves better performance than the state-of-theart methods<ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b13"">14]</ref>. After increasing the depth without adding any parameters, the performance and significantly outperforms VDSR <ref type=""bibr"" target=""#b12"">[13]</ref>, DRCN <ref type=""bibr"" target=""#b13"">[14]</ref> and RED30 <ref type=""bibr"" target=""#b16"">[17]</ref> by 0.3 the other hand, to control the model parameters, the Deeply-Recursive Convolutional Network (DRCN) <ref type=""bibr"" target=""#b13"">[14]</ref> introduces a very deep recursive layer via a chain structu ><p>(2) Recursive learning of residual units is proposed in DRRN to keep our model compact. In DRCN <ref type=""bibr"" target=""#b13"">[14]</ref>, a deep recursive layer (up to 16 convolutional recursions et <ref type=""bibr"" target=""#b7"">[8]</ref>, VDSR <ref type=""bibr"" target=""#b12"">[13]</ref> and DRCN <ref type=""bibr"" target=""#b13"">[14]</ref>. Fig. <ref type=""figure"" target=""#fig_1"">2</ref> illustrat type=""bibr"" target=""#b12"">[13]</ref>. The purple line refers to a global identity mapping. (c) DRCN <ref type=""bibr"" target=""#b13"">[14]</ref>. The blue dashed box refers to a recursive layer, among wh esNet <ref type=""bibr"" target=""#b7"">[8]</ref>, VDSR <ref type=""bibr"" target=""#b12"">[13]</ref>, DRCN <ref type=""bibr"" target=""#b13"">[14]</ref> and DRRN. U, d, T, and B are the numbers of residual units cursive block.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2.3."">DRCN</head><p>DRCN <ref type=""bibr"" target=""#b13"">[14]</ref> is motivated by the observation that adding more weight la s a given image x as feature maps H 0 . The inference net f 2 (H 0 ) stacks T recursions (T = 16 in <ref type=""bibr"" target=""#b13"">[14]</ref>) in a recursive layer, with shared weights among these rec "">[3]</ref>. Very deep models (d ≥ 20) include VDSR <ref type=""bibr"" target=""#b12"">[13]</ref>, DRCN <ref type=""bibr"" target=""#b13"">[14]</ref>, RED <ref type=""bibr"" target=""#b16"">[17]</ref> and DRRN wi es the performance and significantly outperforms VDSR<ref type=""bibr"" target=""#b12"">[13]</ref>, DRCN<ref type=""bibr"" target=""#b13"">[14]</ref> and RED30<ref type=""bibr"" target=""#b16"">[17]</ref> by 0.37 type=""bibr"" target=""#b12"">[13]</ref>. The purple line refers to a global identity mapping. (c) DRCN<ref type=""bibr"" target=""#b13"">[14]</ref>. The blue dashed box refers to a recursive layer, among wh nt CNN models for SR <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" tar mparison, similar to <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b21"">23]</ref>, we crop pixels nea",1
"th but far fewer parameters, DRRN B1U9 achieves better performance than the state-of-theart methods <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b13"">14]</ref>. After increasing pe=""bibr"" target=""#b26"">28]</ref> on ImageNet <ref type=""bibr"" target=""#b19"">[21]</ref>, Kim et al. <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b13"">14]</ref> propose two very d /head></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.1."">Datasets</head><p>By following <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b21"">23]</ref>, we use a training mmarizes quantitative results on the four testing sets, by citing the results of prior methods from <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b13"">14]</ref>. The two DRRN mode pth but far fewer parameters, DRRN B1U9 achieves better performance than the state-of-theart methods<ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b13"">14]</ref>. After increasing meters, the 52-layer DRRN B1U25 further improves the performance and significantly outperforms VDSR <ref type=""bibr"" target=""#b12"">[13]</ref>, DRCN <ref type=""bibr"" target=""#b13"">[14]</ref> and RED30 respectively. On the one hand, to accelerate the convergence speed of very deep networks, the VDSR <ref type=""bibr"" target=""#b12"">[13]</ref> is trained with a very high learning rate (10 −1 , instead on focuses on three most related work to ours: ResNet <ref type=""bibr"" target=""#b7"">[8]</ref>, VDSR <ref type=""bibr"" target=""#b12"">[13]</ref> and DRCN <ref type=""bibr"" target=""#b13"">[14]</ref>. Fig. < on. Denoting the input as x and the underlying mapping as H(x), the residual mapping is defined as  <ref type=""bibr"" target=""#b12"">[13]</ref>. The purple line refers to a global identity mapping. (c) <ref type=""table"">1</ref>. Strategies used in ResNet <ref type=""bibr"" target=""#b7"">[8]</ref>, VDSR <ref type=""bibr"" target=""#b12"">[13]</ref>, DRCN <ref type=""bibr"" target=""#b13"">[14]</ref> and DRRN. "">VDSR</head><p>Differing from ResNet that uses residual learning in every few stacked layers, VDSR <ref type=""bibr"" target=""#b12"">[13]</ref> introduces GRL, i.e., residual learning between the input structure of DRRN is illustrated in Fig. <ref type=""figure"" target=""#fig_4"">5</ref>. Actually, VDSR <ref type=""bibr"" target=""#b12"">[13]</ref> can be viewed as a special case of DRRN, i.e., when U = 0, r that, for each original image, we have 7 additional augmented versions. Besides, inspired by VDSR <ref type=""bibr"" target=""#b12"">[13]</ref>, we also use scale augmentation to train our model, and im epochs. Since a large learning rate is used in our work, we adopt the adjustable gradient clipping <ref type=""bibr"" target=""#b12"">[13]</ref> to boost the convergence rate while suppressing exploding VDSR re-implementation also uses BN and ReLU as the activation functions, unlike the original VDSR <ref type=""bibr"" target=""#b12"">[13]</ref> that does not use BN. These results are faithful since our ese results are faithful since our VDSR re-implementation achieves similar benchmark performance as <ref type=""bibr"" target=""#b12"">[13]</ref> reported in Tab. Qualitative comparisons among SRCNN <ref RCNN <ref type=""bibr"" target=""#b1"">[2]</ref>, SelfEx <ref type=""bibr"" target=""#b9"">[10]</ref>, VDSR <ref type=""bibr"" target=""#b12"">[13]</ref> and DRRN are illustrated in Fig. <ref type=""figure"" target /1.0""><head n=""4.5."">Discussions</head><p>Since global residual learning has been well discussed in <ref type=""bibr"" target=""#b12"">[13]</ref>, in this section, we mainly focus on local residual learni ucture. Local Residual Learning To demonstrate the effectiveness of LRL, DRRN is compared with VDSR <ref type=""bibr"" target=""#b12"">[13]</ref>, which has no LRL. For fair comparison, the depth and numb 5]</ref> and FSRCNN <ref type=""bibr"" target=""#b2"">[3]</ref>. Very deep models (d ≥ 20) include VDSR <ref type=""bibr"" target=""#b12"">[13]</ref>, DRCN <ref type=""bibr"" target=""#b13"">[14]</ref>, RED <ref ameters, the 52-layer DRRN B1U25 further improves the performance and significantly outperforms VDSR<ref type=""bibr"" target=""#b12"">[13]</ref>, DRCN<ref type=""bibr"" target=""#b13"">[14]</ref> and RED30<r ResNet<ref type=""bibr"" target=""#b7"">[8]</ref>. The green dashed box means a residual unit. (b) VDSR<ref type=""bibr"" target=""#b12"">[13]</ref>. The purple line refers to a global identity mapping. (c) Ratio (PSNR) performance of several recent CNN models for SR <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" tar me depth as VDSR and DRCN, but fewer parameters. Both the DL <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b13"">14]</ref> and non-DL <ref typ images are of the same size. For fair comparison, similar to <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" tar",1
"""the deeper the better"" might not be the case in SR. Inspired by the success of very deep networks <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b25"">27,</ref><ref type=""bibr"" targ p networks could suffer from the performance degradation problem, as observed in visual recognition <ref type=""bibr"" target=""#b7"">[8]</ref> and image restoration <ref type=""bibr"" target=""#b16"">[17]</r nce Sec. 1 overviews DL-based SISR, this section focuses on three most related work to ours: ResNet <ref type=""bibr"" target=""#b7"">[8]</ref>, VDSR <ref type=""bibr"" target=""#b12"">[13]</ref> and DRCN <re iv> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2.1."">ResNet</head><p>The main idea of ResNet <ref type=""bibr"" target=""#b7"">[8]</ref> is to use a residual learning framework to ease the training iple weight layers in the residual unit) Table <ref type=""table"">1</ref>. Strategies used in ResNet <ref type=""bibr"" target=""#b7"">[8]</ref>, VDSR <ref type=""bibr"" target=""#b12"">[13]</ref>, DRCN <ref t bel>(1)</label></formula><p>where x is the output of the residual unit, h(x) is an identity mapping <ref type=""bibr"" target=""#b7"">[8]</ref> : h(x) = x, W is a set of weights (the biases are omitted to ng the recursive block structure, in which several residual units are stacked. Noted that in ResNet <ref type=""bibr"" target=""#b7"">[8]</ref>, different residual units use different inputs for the ident </p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.1."">Residual Unit</head><p>In ResNet <ref type=""bibr"" target=""#b7"">[8]</ref>, the basic residual unit is formulated as Eq. 1 and the acti fig_1""><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Simplified structures of (a) ResNet<ref type=""bibr"" target=""#b7"">[8]</ref>. The green dashed box means a residual unit. (b) VDSR<ref ty",1
",</ref><ref type=""bibr"" target=""#b25"">27,</ref><ref type=""bibr"" target=""#b26"">28]</ref> on ImageNet <ref type=""bibr"" target=""#b19"">[21]</ref>, Kim et al. <ref type=""bibr"" target=""#b12"">[13,</ref><ref",0
"ref><ref type=""bibr"" target=""#b33"">35]</ref> or other learning paradigms <ref type=""bibr"">[20,</ref><ref type=""bibr"" target=""#b20"">22,</ref><ref type=""bibr"" target=""#b21"">23,</ref><ref type=""bibr"" tar lfEx <ref type=""bibr"" target=""#b9"">[10]</ref>, RFL <ref type=""bibr"" target=""#b21"">[23]</ref>, NBSRF <ref type=""bibr"" target=""#b20"">[22]</ref>, PSyCo [20] and IA <ref type=""bibr"" target=""#b28"">[30]</re",0
"he compact models, large models demand more storage space and are less applicable to mobile systems <ref type=""bibr"" target=""#b5"">[6]</ref>. To address this issue, we propose a novel Deep Recursive Re",0
"ive function is optimized via the mini-batch stochastic gradient descent (SGD) with backpropagation <ref type=""bibr"" target=""#b14"">[15]</ref>. We implement DRRN via Caffe <ref type=""bibr"" target=""#b11",0
"(CNN), are widely used to address the ill- . PSNR of recent CNN models for scale factor ×3 on Set5 <ref type=""bibr"" target=""#b0"">[1]</ref>. Red points are our models. △, ✩, and • are models with less =""bibr"" target=""#b17"">[18]</ref>. For testing, we utilize four widely used benchmark datasets, Set5 <ref type=""bibr"" target=""#b0"">[1]</ref>, Set14 <ref type=""bibr"" target=""#b34"">[36]</ref>, BSD100 <re gure 1</head><label>1</label><figDesc>Figure1. PSNR of recent CNN models for scale factor ×3 on Set5<ref type=""bibr"" target=""#b0"">[1]</ref>. Red points are our models. △, ✩, and • are models with less",0
"r comparison, which claims to have the highest correlation with perceptual scores for SR evaluation <ref type=""bibr"" target=""#b32"">[34]</ref>. The results are presented in Tab. 3. Note that Dataset Sc",0
"128 filters of the size 3 × 3.</p><p>For weight initialization, we use the same method as He et al. <ref type=""bibr"" target=""#b6"">[7]</ref>, which is shown to be suitable for networks utilizing ReLU.",0
"type=""bibr"" target=""#b34"">[36]</ref>, BSD100 <ref type=""bibr"" target=""#b17"">[18]</ref> and Urban100 <ref type=""bibr"" target=""#b9"">[10]</ref>, which have 5, 14, 100 and 100 images respectively.</p></di M) <ref type=""foot"" target=""#foot_0"">1</ref> . Especially on the recent difficult Ur-ban100 dataset <ref type=""bibr"" target=""#b9"">[10]</ref>, DRRN significantly advances the state of the art, with the ted in Tab. 3. Note that Dataset Scale Bicubic SRCNN <ref type=""bibr"" target=""#b1"">[2]</ref> SelfEx <ref type=""bibr"" target=""#b9"">[10]</ref> RFL <ref type=""bibr"" target=""#b21"">[23]</ref>   the results eported in Tab. Qualitative comparisons among SRCNN <ref type=""bibr"" target=""#b1"">[2]</ref>, SelfEx <ref type=""bibr"" target=""#b9"">[10]</ref>, VDSR <ref type=""bibr"" target=""#b12"">[13]</ref> and DRRN ar et5 and Set14. Shallow (non-DL) models include A+ <ref type=""bibr"" target=""#b29"">[31]</ref>, SelfEx <ref type=""bibr"" target=""#b9"">[10]</ref>, RFL <ref type=""bibr"" target=""#b21"">[23]</ref>, NBSRF <ref 2,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b13"">14]</ref> and non-DL <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"">20,</ref><ref type=""bibr"" target=""#b21"">23] L <ref type=""bibr"" target=""#b21"">[23]</ref>   the results of <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"">20,</ref><ref type=""bibr"" target=""#b21"">23]<",0
"are performed after the weight layers. In contrast to such a ""post-activation"" structure, He et al. <ref type=""bibr"" target=""#b8"">[9]</ref> propose a ""preactivation"" structure, which performs the acti",0
"resholding algorithm <ref type=""bibr"" target=""#b4"">[5]</ref>, Cascaded Sparse Coding Network (CSCN) <ref type=""bibr"" target=""#b30"">[32]</ref> is trained end-to-end to fully exploit the natural sparsit SRCNN <ref type=""bibr"" target=""#b1"">[2]</ref>, DJSR <ref type=""bibr"" target=""#b31"">[33]</ref>, CSCN <ref type=""bibr"" target=""#b30"">[32]</ref>, ESPCN <ref type=""bibr"" target=""#b23"">[25]</ref> and FSRCN bibr"" target=""#b23"">[25]</ref> observe that the prior models <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b30"">32]</ref> increase LR image's resolution via bicubic interpolation be get=""#b13"">14,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b23"">25,</ref><ref type=""bibr"" target=""#b30"">32]</ref> versus the number of parameters, denoted as k. Compared to",0
"<ref type=""bibr"" target=""#b12"">[13]</ref>, DRCN <ref type=""bibr"" target=""#b13"">[14]</ref> and RED30 <ref type=""bibr"" target=""#b16"">[17]</ref> by 0.37, 0.21 and 0.21 dB respectively.</p><p>posed invers ction, and adopt an ensemble strategy to further improve the performance. Very recently, Mao et al. <ref type=""bibr"" target=""#b16"">[17]</ref> propose a 30-layer convolutional auto-encoder network name em, as observed in visual recognition <ref type=""bibr"" target=""#b7"">[8]</ref> and image restoration <ref type=""bibr"" target=""#b16"">[17]</ref>. The reason may be a significant amount of image details a VDSR <ref type=""bibr"" target=""#b12"">[13]</ref>, DRCN <ref type=""bibr"" target=""#b13"">[14]</ref>, RED <ref type=""bibr"" target=""#b16"">[17]</ref> and DRRN with d = 20 and 52. Fig. <ref type=""figure"" targe SR<ref type=""bibr"" target=""#b12"">[13]</ref>, DRCN<ref type=""bibr"" target=""#b13"">[14]</ref> and RED30<ref type=""bibr"" target=""#b16"">[17]</ref> by 0.37, 0.21 and 0.21 dB respectively.</figDesc></figure> rget=""#b1"">[2,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b23"">25,</ref><ref type=""bibr"" tar",0
"ref> or other learning paradigms <ref type=""bibr"">[20,</ref><ref type=""bibr"" target=""#b20"">22,</ref><ref type=""bibr"" target=""#b21"">23,</ref><ref type=""bibr"" target=""#b29"">31]</ref>. As the pioneer CNN .org/ns/1.0""><head n=""4.1."">Datasets</head><p>By following <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b21"">23]</ref>, we use a training dataset of 291 images, where 91 images a arget=""#b13"">14]</ref> and non-DL <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"">20,</ref><ref type=""bibr"" target=""#b21"">23]</ref> methods in recent years are used for benchmark. Experimenta rget=""#b1"">[2,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b21"">23]</ref>, we crop pixels near image boundary before evaluation, alth type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"">20,</ref><ref type=""bibr"" target=""#b21"">23]</ref> are cited from [20]<ref type=""foot"" target=""#foot_1"">2</ref c SRCNN <ref type=""bibr"" target=""#b1"">[2]</ref> SelfEx <ref type=""bibr"" target=""#b9"">[10]</ref> RFL <ref type=""bibr"" target=""#b21"">[23]</ref>   the results of <ref type=""bibr"" target=""#b1"">[2,</ref><r A+ <ref type=""bibr"" target=""#b29"">[31]</ref>, SelfEx <ref type=""bibr"" target=""#b9"">[10]</ref>, RFL <ref type=""bibr"" target=""#b21"">[23]</ref>, NBSRF <ref type=""bibr"" target=""#b20"">[22]</ref>, PSyCo [2",0
"f very deep networks <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b25"">27,</ref><ref type=""bibr"" target=""#b26"">28]</ref> on ImageNet <ref type=""bibr"" target=""#b19"">[21]</ref>, Kim",0
"tion Details</head><p>Data augmentation is performed on the 291-image training dataset. Inspired by <ref type=""bibr"" target=""#b28"">[30]</ref>, the flipped and rotated versions of the training images a ""bibr"" target=""#b21"">[23]</ref>, NBSRF <ref type=""bibr"" target=""#b20"">[22]</ref>, PSyCo [20] and IA <ref type=""bibr"" target=""#b28"">[30]</ref>. The deep models (d ≤ 8) include SRCNN <ref type=""bibr"" ta",0
"""#b30"">[32]</ref> is trained end-to-end to fully exploit the natural sparsity of images. Shi et al. <ref type=""bibr"" target=""#b23"">[25]</ref> observe that the prior models <ref type=""bibr"" target=""#b1 SR <ref type=""bibr"" target=""#b31"">[33]</ref>, CSCN <ref type=""bibr"" target=""#b30"">[32]</ref>, ESPCN <ref type=""bibr"" target=""#b23"">[25]</ref> and FSRCNN <ref type=""bibr"" target=""#b2"">[3]</ref>. Very d get=""#b12"">13,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b23"">25,</ref><ref type=""bibr"" target=""#b30"">32]</ref> versus the number o",0
"28"">[30]</ref>. The deep models (d ≤ 8) include SRCNN <ref type=""bibr"" target=""#b1"">[2]</ref>, DJSR <ref type=""bibr"" target=""#b31"">[33]</ref>, CSCN <ref type=""bibr"" target=""#b30"">[32]</ref>, ESPCN <re",0
"pting the machine learning approach.</p><p>One is based on the COMT (Co-Training Model Tree) method <ref type=""bibr"" target=""#b16"">[17]</ref>, which assumes a linear relation between parameters and th",1
"configuration errors <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" target=""#b32"">34,</ref><ref type=""bibr"" target=""#b33"">35]</ref>. It can also result",0
"stems under tune, we embed widely adopted benchmark tools in the workload generator. We use HiBench <ref type=""bibr"" target=""#b21"">[22]</ref> for Hive+Hadoop and Spark, YCSB <ref type=""bibr"" target=""#",0
"machine learning models are proposed for distributed systems <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b13"">14]</ref>, but these models are not applicable to the complicated cas",0
"type=""bibr"" target=""#b41"">43]</ref>, databases <ref type=""bibr"" target=""#b10"">[11]</ref> and Hadoop <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b18"">19]</ref> respectively, these rmance tuning. Thus, the method used in Starfish cannot be directly applied to other systems. Aloja <ref type=""bibr"" target=""#b3"">[4]</ref> adopts the common machine learning methods, exploiting a lar",0
"o transfer knowledge from data-rich head classes to data-poor tail classes. While transfer learning <ref type=""bibr"" target=""#b16"">[16,</ref><ref type=""bibr"" target=""#b17"">17,</ref><ref type=""bibr"" ta ref><ref type=""bibr"" target=""#b18"">18]</ref> from a source to target task is a well studied problem <ref type=""bibr"" target=""#b16"">[16,</ref><ref type=""bibr"" target=""#b19"">19]</ref>, by far the most c",1
"transfer learning <ref type=""bibr"" target=""#b16"">[16,</ref><ref type=""bibr"" target=""#b17"">17,</ref><ref type=""bibr"" target=""#b18"">18]</ref> from a source to target task is a well studied problem <ref ]</ref>, which describes a method for learning from small datasets (the ""few-shot"" learning problem <ref type=""bibr"" target=""#b18"">[18,</ref><ref type=""bibr"" target=""#b36"">36,</ref><ref type=""bibr"" ta",0
"k on meta-learning <ref type=""bibr"" target=""#b21"">[21,</ref><ref type=""bibr"" target=""#b22"">22,</ref><ref type=""bibr"" target=""#b23"">23,</ref><ref type=""bibr"" target=""#b24"">24,</ref><ref type=""bibr"" tar",0
"sual recognition, through the ability to learn ""big models"" with hundreds of millions of parameters <ref type=""bibr"" target=""#b1"">[1,</ref><ref type=""bibr"" target=""#b2"">2,</ref><ref type=""bibr"" target rk structure defined above, we now describe an efficient method for training based on two insights. <ref type=""bibr"" target=""#b1"">(1)</ref> The recursive definition of MetaModelNet suggests a recursiv",0
"get=""#b39"">39,</ref><ref type=""bibr"" target=""#b47"">47,</ref><ref type=""bibr"" target=""#b48"">48,</ref><ref type=""bibr"" target=""#b49"">49,</ref><ref type=""bibr"" target=""#b50"">50,</ref><ref type=""bibr"" tar",0
"through which they occur, are important and challenging problems that have attracted much attention <ref type=""bibr"" target=""#b9"">[10]</ref>. This paper focuses on predicting protein interfaces. Despi ppears to be saturated. This calls for new methodologies or sources of information to be exploited"" <ref type=""bibr"" target=""#b9"">[10]</ref>. Most machine learning methods for interface prediction use",1
"2]</ref> to machine translation <ref type=""bibr"" target=""#b23"">[24]</ref> and computational biology <ref type=""bibr"" target=""#b3"">[4]</ref>, has resulted in a resurgence of interest in this area. This",0
"locally describing the shape of an object, and various spectral representations of shape (see e.g. <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b16"">17]</ref>).</p><p>In this wo",0
"ate that this approach provides state-of-theart accuracy, outperforming a recent SVM-based approach <ref type=""bibr"" target=""#b1"">[2]</ref>. The proposed convolution operators are not specific to inte can be addressed using pairwise kernels, building the invariance into the representation (see e.g. <ref type=""bibr"" target=""#b1"">[2]</ref>). To create an order-invariant model in a setting which requ ding, and the labels are derived from the structure of the proteins in complex. As in previous work <ref type=""bibr"" target=""#b1"">[2]</ref>, two residues from different proteins are considered part of tein's sequence and structure. For the node features we used the same features used in earlier work <ref type=""bibr"" target=""#b1"">[2]</ref>, as summarized next. Protein sequence alone can be a good in dow of size 11 in sequence centered around the residue of interest and concatenating their features <ref type=""bibr"" target=""#b1"">[2]</ref>. Since we are explicitly representing the structure of a pro -Convolution), equivalent to Equation (1) with no summation over neighbors. The PAIRpred SVM method <ref type=""bibr"" target=""#b1"">[2]</ref> was trained by performing five fold cross validation on the",0
"elded the best performance in our validation experiments. We implemented our networks in TensorFlow <ref type=""bibr"" target=""#b0"">[1]</ref> v1.0.1 to make use of rapid training on GPUs. Training times",0
"represent the text comprehensively. Some ensemble approaches <ref type=""bibr"" target=""#b7"">[8]</ref><ref type=""bibr"" target=""#b8"">[9]</ref><ref type=""bibr"" target=""#b9"">[10]</ref> fused different text which can guarantee a linear increase of the words.</p><p>Assuming it follows the Markov assumption <ref type=""bibr"" target=""#b8"">[9]</ref> that the selection probability of any word only depends on t",1
"e ensemble approaches <ref type=""bibr"" target=""#b7"">[8]</ref><ref type=""bibr"" target=""#b8"">[9]</ref><ref type=""bibr"" target=""#b9"">[10]</ref> fused different text features and achieved promising result",1
"tional machine learning models are simple but have yield strong baselines. For example, Pang et al. <ref type=""bibr"" target=""#b1"">[2]</ref> proposed a SVM categorization model based on n-gram approach",0
"gories with the same number of documents. The 20Newsgroup<ref type=""foot"" target=""#foot_4"">3</ref>  <ref type=""bibr"" target=""#b14"">[15]</ref> data set is a standard database for machine learning evalu",0
"e"" target=""#tab_1"">2</ref>. Noted that the word vector input to GRU is initialized with the Glove 6 <ref type=""bibr"" target=""#b16"">[17]</ref> word vector matrix, others are all initialized with unifor",0
"xt features more effectively and optimize the algorithm for higher accuracy are the main challenges <ref type=""bibr"" target=""#b0"">[1]</ref>. Some conventional machine learning models are simple but ha s decision-level fusion to concatenate the complementary information of different kinds of features <ref type=""bibr"" target=""#b0"">[1]</ref>. The decision vector [x 1 , ⋯ , x k−1 , x k ] represents the",0
"eural network which can be used in time series analysis and avoid the problem of gradient vanishing <ref type=""bibr"" target=""#b11"">[12]</ref>.</p><p>In our model, GRU extract long-term dependency feat",0
"t is commonly used for emotional categorization. The ELEC<ref type=""foot"" target=""#foot_3"">2</ref>  <ref type=""bibr"" target=""#b13"">[14]</ref> data set is part of Amazon's electronic product review dat",0
"pe=""table"" target=""#tab_0"">1</ref> is a summary. The IMDB<ref type=""foot"" target=""#foot_2"">1</ref>  <ref type=""bibr"" target=""#b12"">[13]</ref> data set consists of numerous film movie reviews. It is co",0
". Value prediction was proposed to address this limitation <ref type=""bibr"" target=""#b12"">[12,</ref><ref type=""bibr"" target=""#b18"">20]</ref>. By predicting the value(s) produced by an instruction (pro iction</head><p>Since the introduction of value prediction <ref type=""bibr"" target=""#b12"">[12,</ref><ref type=""bibr"" target=""#b18"">20]</ref>, there has been a plethora of work on this subject. In gene that belong to the first sequence, a conventional value predictor (e.g., Last-Value-Predictor, LVP <ref type=""bibr"" target=""#b18"">[20]</ref>) might mispredict the second load's value because the valu",1
"r entries are impacted by the committing stores. We refer the readers to the EXACT branch predictor <ref type=""bibr"" target=""#b0"">[1]</ref> in which a similar machinery is employed to pro-actively upd",0
"/ref>, browser benchmark <ref type=""bibr"" target=""#b3"">[4]</ref>, and various Javascript benchmarks <ref type=""bibr"" target=""#b8"">[8,</ref><ref type=""bibr"">14,</ref><ref type=""bibr"">15,</ref><ref type",0
">Similar to early work <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b9"">9,</ref><ref type=""bibr"" target=""#b13"">13]</ref>, we observed that load memory addresses exhibit similar tem 4]</ref>, and to prefetch the data to be accessed by the next invocation in strided memory accesses <ref type=""bibr"" target=""#b13"">[13]</ref>. Similar to other prediction schemes, address predictors e",0
"cle for reading the data cache (facilitated by way prediction.) In a pipeline similar to Cortex-A72 <ref type=""bibr"" target=""#b11"">[11]</ref>, the Fetch and Decode stages take 5 and 3 cycles, respecti",0
"for word embedding has been shown to be an implicit factorization of a certain word-context matrix <ref type=""bibr"" target=""#b25"">[24]</ref>, and there is recent effort to theoretically explaining th ol(G) log д −x ⊤ i y j .</formula><p>Let us define z i, j = x ⊤ i y j . Following Levy and Goldberg <ref type=""bibr"" target=""#b25"">[24]</ref>, where the authors suggested that for a sufficient large e w i−T , • • • , w i−1 , w i+1 , • • • , w i+T .</formula><p>Following the work by Levy and Goldberg <ref type=""bibr"" target=""#b25"">[24]</ref>, SGNS is implicitly factorizing</p><formula xml:id=""formul x is not only ill-defined (since log 0 = −∞), but also dense. Inspired by the Shifted PPMI approach <ref type=""bibr"" target=""#b25"">[24]</ref>, we define M ′ such that M ′ i, j = max(M i, j , 1) (Line </ref>. Recently, there has been effort in understanding this model. For example, Levy and Goldberg <ref type=""bibr"" target=""#b25"">[24]</ref> prove that SGNS is actually conducting an implicit matrix target=""#b19"">[18]</ref> frame word embedding as a metric learning problem. Built upon the work in <ref type=""bibr"" target=""#b25"">[24]</ref>, we theoretically analyze popular skip-gram based network",1
"w P u,v,w X v,w = X u,v , and the existence of such X is guaranteed by the Perron-Frobenius theorem <ref type=""bibr"" target=""#b5"">[4]</ref>. Additionally, the higher-order transition probability tenso s difficult. However, we have noticed some recent progresses <ref type=""bibr"" target=""#b4"">[3,</ref><ref type=""bibr"" target=""#b5"">4,</ref><ref type=""bibr"" target=""#b16"">15]</ref> that try to understan",0
"us network embedding <ref type=""bibr"" target=""#b9"">[8,</ref><ref type=""bibr"" target=""#b13"">12,</ref><ref type=""bibr"" target=""#b21"">20,</ref><ref type=""bibr"" target=""#b37"">36]</ref>, semi-supervised ne",0
"ous of perspectives, such as heterogeneous network embedding <ref type=""bibr"" target=""#b9"">[8,</ref><ref type=""bibr"" target=""#b13"">12,</ref><ref type=""bibr"" target=""#b21"">20,</ref><ref type=""bibr"" tar ibr"" target=""#b32"">[31]</ref>, node2vec <ref type=""bibr"" target=""#b17"">[16]</ref>, and metapath2vec <ref type=""bibr"" target=""#b13"">[12]</ref> define vertices' context by the 1st-, 2nd-order, and meta-",0
"etwork structure. Moreover, there was an early attempt to theoretically analyze DeepWalk's behavior <ref type=""bibr"" target=""#b48"">[47]</ref>. However, their main theoretical results are not fully con "">48]</ref>, network embedding with rich vertex attributes <ref type=""bibr"" target=""#b44"">[43,</ref><ref type=""bibr"" target=""#b48"">47,</ref><ref type=""bibr"" target=""#b50"">49]</ref>, network embedding",0
"manifold learning algorithms <ref type=""bibr"" target=""#b37"">[38]</ref>, and geometric deep learning <ref type=""bibr"" target=""#b6"">[7]</ref>-all of which involve representation learning with graph-stru 3]</ref>, <ref type=""bibr"" target=""#b42"">[43]</ref>, <ref type=""bibr"" target=""#b37"">[38]</ref>, and <ref type=""bibr"" target=""#b6"">[7]</ref> for comprehensive overviews of these areas.</p></div> <div x lly similar to Algorithm 1, with some minor variations, and we refer the reader to Bronstein et al. <ref type=""bibr"" target=""#b6"">[7]</ref> for a thorough discussion of these techniques.</p></div> <di",1
"levant work, which we do not review in detail here-including latent space models of social networks <ref type=""bibr"" target=""#b32"">[33]</ref>, embedding methods for statistical relational learning <re ef>-all of which involve representation learning with graph-structured data. We refer the reader to <ref type=""bibr"" target=""#b32"">[33]</ref>, <ref type=""bibr"" target=""#b42"">[43]</ref>, <ref type=""bib etric relations in this latent space correspond to interactions (e.g., edges) in the original graph <ref type=""bibr"" target=""#b32"">[33]</ref>. Figure <ref type=""figure"">2</ref> visualizes an example e",1
"of edge types (i.e., relations) <ref type=""bibr"" target=""#b42"">[43]</ref>. 7  Recently, Dong et al. <ref type=""bibr"" target=""#b18"">[19]</ref> also proposed a strategy for sampling random walks from he",0
""">[41]</ref>. More generally, link prediction is closely related to statistical relational learning <ref type=""bibr"" target=""#b23"">[24]</ref>, where a common task is to predict missing relations betwe tion through time <ref type=""bibr"" target=""#b13"">[14]</ref>, which removes the need to run Equation <ref type=""bibr"" target=""#b23"">(24)</ref> to convergence. Adapting the GNN framework to use modern r",0
"ems, allowing relational knowledge about interacting entities to be efficiently stored and accessed <ref type=""bibr"" target=""#b1"">[2]</ref>.</p><p>However, graphs are not only useful as structured kno",0
"f><ref type=""bibr"" target=""#b52"">53,</ref><ref type=""bibr"" target=""#b55"">56]</ref>, column networks <ref type=""bibr"" target=""#b49"">[50]</ref>, and the GraphSAGE algorithm <ref type=""bibr"" target=""#b28",0
"above, there is a related-and chronologically prior-line of work on ""graph neural networks"" (GNNs) <ref type=""bibr"" target=""#b51"">[52]</ref>. Conceptually, the GNN idea is closely related to Algorith ng scaffolding for a ""message passing"" algorithm between nodes.</p><p>In the original GNN framework <ref type=""bibr"" target=""#b51"">[52]</ref> every node v i is initialized with a random embedding h 0 K i ), where g is an arbitrary differentiable function of the form g : R d ? R d . Scarselli et al. <ref type=""bibr"" target=""#b51"">[52]</ref> discuss various parameterizations of h and g based on mult any of the aggregation procedures described in Section 3.1 could be employed, but Scarselli et al. <ref type=""bibr"" target=""#b51"">[52]</ref> also suggest that the aggregation can be done by introduci",0
"istical relational learning <ref type=""bibr"" target=""#b42"">[43]</ref>, manifold learning algorithms <ref type=""bibr"" target=""#b37"">[38]</ref>, and geometric deep learning <ref type=""bibr"" target=""#b6"" the reader to <ref type=""bibr"" target=""#b32"">[33]</ref>, <ref type=""bibr"" target=""#b42"">[43]</ref>, <ref type=""bibr"" target=""#b37"">[38]</ref>, and <ref type=""bibr"" target=""#b6"">[7]</ref> for comprehen",0
"that higher-order structural motifs are essential to the structure and function of complex networks <ref type=""bibr"" target=""#b4"">[5]</ref>, and developing decoding algorithms that are capable of deco",0
"<ref type=""bibr"" target=""#b0"">[1]</ref>, GraRep <ref type=""bibr"" target=""#b8"">[9]</ref>, and HOPE <ref type=""bibr"" target=""#b44"">[45]</ref> all fall firmly within this class. In particular, all thre HOPE algorithm supports general similarity measures (e.g., based on Jaccard neighborhood overlaps) <ref type=""bibr"" target=""#b44"">[45]</ref>. These various different similarity functions trade-off be",0
"ristics, such as a fixed foreground-to-background ratio (1:3), or online hard example mining (OHEM) <ref type=""bibr"" target=""#b29"">[30]</ref>, are performed to maintain a manageable balance between fo ves, focusing all attention on the hard negative examples.</p><p>Online Hard Example Mining (OHEM): <ref type=""bibr"" target=""#b29"">[30]</ref> proposed to improve training of two-stage detectors by con hard example mining <ref type=""bibr"" target=""#b35"">[36,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b29"">30]</ref>.</p><p>In this paper, we propose a new loss function that a rous extensions to this framework have been proposed, e.g. <ref type=""bibr"" target=""#b18"">[19,</ref><ref type=""bibr"" target=""#b29"">30,</ref><ref type=""bibr"" target=""#b30"">31,</ref><ref type=""bibr"" tar rget=""#b31"">[32,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b29"">30,</ref><ref type=""bibr"" target=""#b20"">21]</ref> that samples hard e nt performance saturates. (d) FL outperforms the best variants of online hard example mining (OHEM) <ref type=""bibr"" target=""#b29"">[30,</ref><ref type=""bibr"" target=""#b20"">21]</ref> by over 3 points A",1
"/ref> helped extend dense detectors to more general object categories and had top results on PASCAL <ref type=""bibr"" target=""#b6"">[7]</ref> for many years. While the sliding-window approach was the le",0
"ng in the modern era of object detection. R-CNN was improved over the years, both in terms of speed <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b9"">10]</ref> and by using learne",0
"type=""bibr"" target=""#b27"">28]</ref> or hard example mining <ref type=""bibr"" target=""#b35"">[36,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b29"">30]</ref>.</p><p>In this paper, rd negative mining <ref type=""bibr"" target=""#b31"">[32,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b29"">30,</ref><ref type=""bibr"" targe ef type=""bibr"" target=""#b4"">[5]</ref> gave rise to effective methods for pedestrian detection. DPMs <ref type=""bibr"" target=""#b7"">[8]</ref> helped extend dense detectors to more general object categor detectors <ref type=""bibr"" target=""#b35"">[36,</ref><ref type=""bibr"" target=""#b4"">5]</ref> and DPMs <ref type=""bibr"" target=""#b7"">[8]</ref>, and more recent methods, like SSD <ref type=""bibr"" target=""",0
"type=""bibr"" target=""#b31"">[32,</ref><ref type=""bibr"" target=""#b27"">28]</ref> or hard example mining <ref type=""bibr"" target=""#b35"">[36,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" targ oss.</p><p>Class Imbalance: Both classic one-stage object detection methods, like boosted detectors <ref type=""bibr"" target=""#b35"">[36,</ref><ref type=""bibr"" target=""#b4"">5]</ref> and DPMs <ref type="" <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b34"">35]</ref>. Viola and Jones <ref type=""bibr"" target=""#b35"">[36]</ref> used boosted object detectors for face detection, leading n solution is to perform some form of hard negative mining <ref type=""bibr"" target=""#b31"">[32,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" targe",0
"on LR images directly and progressively reconstruct the sub-band residuals of HR images. Tai et al. <ref type=""bibr"" target=""#b33"">[34]</ref> proposed deep recursive residual network (DRRN) to address the estimated high-quality patch with the same resolution as the input low-quality patch. We follow <ref type=""bibr"" target=""#b33"">[34]</ref> to do data augmentation. For each task, we train a single",1
"e.g., image denoising <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b36"">37]</ref>, single-image super-r f> PCLR <ref type=""bibr"" target=""#b1"">[2]</ref> PGPD <ref type=""bibr"" target=""#b36"">[37]</ref> WNNM <ref type=""bibr"" target=""#b8"">[9]</ref> RED <ref type=""bibr"" target=""#b26"">[27]</ref>  </p></div> <d LR <ref type=""bibr"" target=""#b1"">[2]</ref>, PGPD <ref type=""bibr"" target=""#b36"">[37]</ref> and WNNM <ref type=""bibr"" target=""#b8"">[9]</ref>. The results are shown in Fig. <ref type=""figure"">7</ref>  S",0
"=""bibr"" target=""#b20"">[21]</ref>. Difference to DenseNet Another related work to MemNet is DenseNet <ref type=""bibr"" target=""#b13"">[14]</ref>, which also builds upon a densely connected principle. In lysis</head><p>We now illustrate how our gate unit affects different kinds of memories. Inspired by <ref type=""bibr"" target=""#b13"">[14]</ref>, we adopt a weight norm as an approximate for the dependen",0
"ang et al. adopted a cascaded sparse coding network to fully exploit the natural sparsity of images <ref type=""bibr"" target=""#b35"">[36]</ref>. In <ref type=""bibr"" target=""#b34"">[35]</ref>, a deep dual",0
"r convolutional auto-encoder network named RED for image denoising and SISR. Moreover, Zhang et al. <ref type=""bibr"" target=""#b39"">[40]</ref> propose a denoising convolutional neural network (DnCNN) t <ref type=""bibr"" target=""#b19"">[20]</ref>, DRCN <ref type=""bibr"" target=""#b20"">[21]</ref> and DnCNN <ref type=""bibr"" target=""#b39"">[40]</ref> (Fig. <ref type=""figure"" target=""#fig_0"">1(a)</ref>), adop Residual learning and adjustable gradient clipping are used to speed up the training. Zhang et al. <ref type=""bibr"" target=""#b39"">[40]</ref> introduced batch normalization into a DnCNN model to joint N <ref type=""bibr"" target=""#b7"">[8]</ref>, VDSR <ref type=""bibr"" target=""#b19"">[20]</ref> and DnCNN <ref type=""bibr"" target=""#b39"">[40]</ref> are compared using their public codes. MemNet recovers rel blocking Tab. 5 shows the JPEG deblocking results on Classic5 and LIVE1, by citing the results from <ref type=""bibr"" target=""#b39"">[40]</ref>.</p><p>Our network significantly outperforms the other met the residual image <ref type=""bibr"" target=""#b19"">[20,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b39"">40]</ref>. Therefore, our basic MemNet can be formulated as,</p><form",0
"learning sentence representations <ref type=""bibr"" target=""#b25"">(Lin et al., 2017)</ref>. However, <ref type=""bibr"" target=""#b38"">Vaswani et al. (2017)</ref> showed that not only self-attention can i we have found extending our mechanism to employ multi-head attention to be beneficial, similarly to <ref type=""bibr"" target=""#b38"">Vaswani et al. (2017)</ref>. Specifically, K independent attention me",1
"y <ref type=""bibr"" target=""#b24"">Li et al. (2016)</ref>, which propose to use gated recurrent units <ref type=""bibr"" target=""#b6"">(Cho et al., 2014)</ref> in the propagation step.</p><p>Nevertheless,",0
"antic segmentation <ref type=""bibr"" target=""#b21"">(Jégou et al., 2017)</ref> or machine translation <ref type=""bibr"" target=""#b13"">(Gehring et al., 2016)</ref>, where the underlying data representatio to standard in many sequence-based tasks <ref type=""bibr"" target=""#b2"">(Bahdanau et al., 2015;</ref><ref type=""bibr"" target=""#b13"">Gehring et al., 2016)</ref>. One of the benefits of attention mechani",0
"ied to a graph with a different structure.</p><p>On the other hand, we have non-spectral approaches <ref type=""bibr"" target=""#b11"">(Duvenaud et al., 2015;</ref><ref type=""bibr"" target=""#b1"">Atwood &am operty of CNNs. In some cases, this requires learning a specific weight matrix for each node degree <ref type=""bibr"" target=""#b11"">(Duvenaud et al., 2015)</ref>, using the powers of a transition matri",0
"ural Networks (GNNs) were introduced in <ref type=""bibr"" target=""#b15"">Gori et al. (2005)</ref> and <ref type=""bibr"" target=""#b33"">Scarselli et al. (2009)</ref> as a generalization of recursive neural",0
"n (LP) <ref type=""bibr"" target=""#b42"">(Zhu et al., 2003)</ref>, semi-supervised embedding (SemiEmb) <ref type=""bibr"" target=""#b39"">(Weston et al., 2012)</ref>, manifold regularization (ManiReg) <ref t 017)</ref> for  <ref type=""bibr"" target=""#b3"">(Belkin et al., 2006)</ref> 59.5% 60.1% 70.7% SemiEmb <ref type=""bibr"" target=""#b39"">(Weston et al., 2012)</ref> 59.0% 59.6% 71.7% LP <ref type=""bibr"" tar",0
"k, which produces an output for each node based on its state. This idea was adopted and improved by <ref type=""bibr"" target=""#b24"">Li et al. (2016)</ref>, which propose to use gated recurrent units <r",0
"of the spectral filters with smooth coefficients in order to make them spatially localized. Later, <ref type=""bibr"" target=""#b8"">Defferrard et al. (2016)</ref> proposed to approximate the filters by amp; Welling, 2017), as well as graph convolutional models utilising higher-order Chebyshev filters <ref type=""bibr"" target=""#b8"">(Defferrard et al., 2016)</ref>, and the MoNet model presented in <ref (2016)</ref> for state-of-the-art techniques. Specifically, for the Chebyshev filterbased approach <ref type=""bibr"" target=""#b8"">(Defferrard et al., 2016)</ref>, we provide the maximum reported perfo 3.9% Planetoid <ref type=""bibr"" target=""#b41"">(Yang et al., 2016)</ref> 75.7% 64.7% 77.2% Chebyshev <ref type=""bibr"" target=""#b8"">(Defferrard et al., 2016)</ref>  0.973 ± 0.002 the other techniques. S",0
"2"">[3,</ref><ref type=""bibr"" target=""#b1"">2]</ref> and FPGAs <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b25"">26]</ref>.</p><p>Activation unit is a non-linear function that some l",1
"ry. We designed a generic software structure to go from high level model representation from Torch7 <ref type=""bibr"" target=""#b3"">[4]</ref> down to an instruction stream that runs Snowflake. The main ref>. Snowball is the first to generate to custom instructions for hardware accelerator from Torch7 <ref type=""bibr"" target=""#b3"">[4]</ref> or Pytorch models.</p><p>The system in <ref type=""bibr"" targ",0
"timizations.</p><p>Memory transfer friendly computation tiling for CNN accelerators was explored in <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b18"">19]</ref>. In <ref type=""bib",0
"""bibr"" target=""#b20"">[21]</ref> and other designs using ASIC <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b1"">2]</ref> and FPGAs <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""b",0
"n network sparsity are techniques that lower memory bandwidth requirement for this type of workload <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b11"">12]</ref>.</p><p>[5] presents",0
"int locations based on hand-crafted features. Recent works <ref type=""bibr"" target=""#b26"">[27,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" targe ]</ref> uses deep consensus voting to vote the most probable location of keypoints. Gkioxary et al. <ref type=""bibr"" target=""#b13"">[14]</ref> and Zisserman et al. <ref type=""bibr"" target=""#b1"">[2]</re",1
"arget=""#b3"">4,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b38"">39,</ref><ref type=""bibr"" target=""#b41"">42]</ref> mostly rely on the development of convolutional neural netw",1
"esults. Our work is partly inspired by the works on generating and refining score maps. Yang et al. <ref type=""bibr"" target=""#b42"">[43]</ref> adopts pyramid features as inputs of the network in the pr",1
"imation mainly adopt the techniques of pictorial structures <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b0"">1]</ref> or graphical models <ref type=""bibr"" target=""#b6"">[7]</ref>. or graphical models <ref type=""bibr"" target=""#b6"">[7]</ref>. More specifically, the classical works <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b33"">34,</ref><ref type=""bibr"" targ",0
"<ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b0"">1]</ref> or graphical models <ref type=""bibr"" target=""#b6"">[7]</ref>. More specifically, the classical works <ref type=""bibr"" tar",0
"/ref>, which proposes a cascade of CNN pose regressors to deal with pose estimation. Tompson et al. <ref type=""bibr"" target=""#b36"">[37]</ref> attempt to solve the problem by predicting heatmaps of key",0
"n pose estimation.</p><p>Human Detection. Detection approaches are mainly guided by the RCNN family <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" ta",0
"rget=""#b6"">[7]</ref>. More specifically, the classical works <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b33"">34,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" tar",0
"by predicting heatmaps of keypoints using CNN and graphical models. Later works such as Wei et al. <ref type=""bibr"" target=""#b39"">[40]</ref>and Newell et al. <ref type=""bibr"" target=""#b26"">[27]</ref> via generating the score map of keypoints using very deep convolutional neural networks. Wei et al. <ref type=""bibr"" target=""#b39"">[40]</ref> propose a multi-stage architecture, i.e., first generate c",0
""">Experimental Setup</head><p>Dataset and Evaluation Metric. Our models are only trained on MS COCO <ref type=""bibr"" target=""#b24"">[25]</ref> trainval dataset (includes 57K images and 150K person inst",0
"et=""#b27"">[28,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b8"">9]</ref> interpret the process of detecting keypoints as a twostage pi",0
"pose estimation has been greatly improved by the involvement of deep convolutional neural networks <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b15"">16]</ref>. For example, in < =""bibr"" target=""#b41"">42]</ref> mostly rely on the development of convolutional neural network(CNN) <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b15"">16]</ref>, which largely imp",0
"rget=""#b26"">[27,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b38"">39,</ref><ref type=""bibr"" tar rget=""#b4"">[5,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b29"">30,</ref><ref type=""bibr"" target=""#b18"">19]</ref> directly predict all keypoints at first and assemble them i stimation results are obtained when person clusters are combined with labeled body parts. DeeperCut <ref type=""bibr"" target=""#b18"">[19]</ref> improves DeepCut <ref type=""bibr"" target=""#b29"">[30]</ref>",0
"s tackling the problem of human pose estimation mainly adopt the techniques of pictorial structures <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b0"">1]</ref> or graphical models <",0
"cation of keypoints. Gkioxary et al. <ref type=""bibr"" target=""#b13"">[14]</ref> and Zisserman et al. <ref type=""bibr"" target=""#b1"">[2]</ref> apply RNN-like architectures to sequentially refine the resu",0
"ng and testing phases. Recent studies have investigated the significant redundancy in deep networks <ref type=""bibr"" target=""#b5"">[6]</ref> and reduced the number of neurons and filters <ref type=""bib recent interest in reducing the redundancy of deep CNNs to achieve acceleration and compression. In <ref type=""bibr"" target=""#b5"">[6]</ref> the redundancy in the parameterization of deep learning mode >, the redundancy in the parameterization of deep learning models has been studied and demonstrated <ref type=""bibr"" target=""#b5"">[6]</ref>. We present NISP to efficiently propagate the importance sco",1
".g., our approach loses 1.43% accuracy on Alexnet and reduces FLOPs by 67.85% while Figurnov et al. <ref type=""bibr"" target=""#b10"">[11]</ref> loses more (2%) and reduces FLOPs less (50%). With almost a scaling factor in the training process and facilitated one channel-level pruning. Figurnov et al. <ref type=""bibr"" target=""#b10"">[11]</ref> speeded up the convolutional layers by skipping operations multiplication and the number of parameters following <ref type=""bibr"" target=""#b19"">[20]</ref> and <ref type=""bibr"" target=""#b10"">[11]</ref>, and denote them as [FLOPs?%] and [Params.?%] in the table and comparing the other.</p><p>On AlexNet, by achieving smaller accuracy loss (1.43% ours vs. 2.00% <ref type=""bibr"" target=""#b10"">[11]</ref>), our method NISP-A manages to reduce significantly more F >[11]</ref>), our method NISP-A manages to reduce significantly more FLOPs (67.85%) than the one in <ref type=""bibr"" target=""#b10"">[11]</ref> (50%), denoted as ""Perforate"" in the table; compare to the on, respectively. Finally, we benchmark the pruning results and compare to existing methods such as <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" ta",0
"ultiple standard CNN architectures such as LeNet <ref type=""bibr"" target=""#b22"">[23]</ref>, AlexNet <ref type=""bibr"" target=""#b21"">[22]</ref>, GoogLeNet <ref type=""bibr"" target=""#b36"">[37]</ref> and R type=""bibr"" target=""#b22"">[23]</ref>, Cifar-net<ref type=""foot"" target=""#foot_0"">3</ref> , AlexNet <ref type=""bibr"" target=""#b21"">[22]</ref>, GoogLeNet <ref type=""bibr"" target=""#b36"">[37]</ref> and R in the output layer to the corresponding row in the input layer. In each row of  Krizhevsky et al. <ref type=""bibr"" target=""#b21"">[22]</ref> proposed Local Response Normalization (LRN) to improve CNN",0
"#b5"">[6]</ref> and reduced the number of neurons and filters <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" tar layer (e.g., prune neurons with small magnitude of weights <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b13"">14]</ref>), or two consecutive layers <ref type=""bibr"" target=""#b28""> nce, other methods <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b28"">29,</ref><ref type=""bibr"" target=""#b13"">14]</ref> measure neuron importance by magnitude of weights. To study of a neuron. These methods prune the ""least important"" neurons layer-by-layer either independently <ref type=""bibr"" target=""#b13"">[14]</ref> or greedily <ref type=""bibr"" target=""#b24"">[25,</ref><ref ured matrices and used circulant matrices to represent FC layers, reducing storage cost. Han et al. <ref type=""bibr"" target=""#b13"">[14]</ref> studied the weight sparsity and compressed CNNs by combini",0
"get=""#b39"">40,</ref><ref type=""bibr"" target=""#b40"">41,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b47"">48,</ref><ref type=""bibr"" tar",0
"tasks. However, all kernels in Inception are sampled at the same center. A similar idea appears in <ref type=""bibr"" target=""#b6"">[3]</ref>, where an Atrous Spatial Pyramid Pooling (ASPP) is exploited 6"">[33,</ref><ref type=""bibr"" target=""#b35"">32,</ref><ref type=""bibr"" target=""#b34"">31]</ref>, ASPP <ref type=""bibr"" target=""#b6"">[3]</ref>, and Deformable CNN <ref type=""bibr"" target=""#b7"">[4]</ref>. ng the same number of parameters. This design has rapidly proved competent at semantic segmentation <ref type=""bibr"" target=""#b6"">[3]</ref>, and has also been adopted in some reputable object detector chitectures: We also compare our RFB with Inception <ref type=""bibr"" target=""#b36"">[33]</ref>, ASPP <ref type=""bibr"" target=""#b6"">[3]</ref> and Deformable CNN <ref type=""bibr"" target=""#b7"">[4]</ref>. s RFB does (termed ""Inception-L""). For ASPP, its primary parameters are tuned in image segmentation <ref type=""bibr"" target=""#b6"">[3]</ref> and the RFs are too large for detection, and our experiment,",1
"pe=""bibr"" target=""#b34"">31]</ref>, ASPP <ref type=""bibr"" target=""#b6"">[3]</ref>, and Deformable CNN <ref type=""bibr"" target=""#b7"">[4]</ref>.</p><p>The Inception block adopts multiple branches with dif all the positions equally, probably leading to confusion between object and context. Deformable CNN <ref type=""bibr"" target=""#b7"">[4]</ref> learns distinctive resolutions of individual objects, unfort pe=""bibr"" target=""#b36"">[33]</ref>, ASPP <ref type=""bibr"" target=""#b6"">[3]</ref> and Deformable CNN <ref type=""bibr"" target=""#b7"">[4]</ref>. For Inception, besides the original version, we change its",1
"bibr"" target=""#b28"">[25]</ref>, MobileNet <ref type=""bibr"" target=""#b15"">[12]</ref>, and ShuffleNet <ref type=""bibr"" target=""#b42"">[39]</ref>), we focus on this backbone to achieve direct comparison t =""bibr"" target=""#b15"">[12]</ref>, DarkNet <ref type=""bibr"" target=""#b28"">[25]</ref>, and ShuffleNet <ref type=""bibr"" target=""#b42"">[39]</ref>. To further test the generalization ability of the RFB mod",0
"n in the following section of experiments. All new conv-layers are initialized with the MSRA method <ref type=""bibr"" target=""#b13"">[10]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head",0
"ead n=""1"">Introduction</head><p>In recent years, Region-based Convolutional Neural Networks (R-CNN) <ref type=""bibr"" target=""#b11"">[8]</ref>, along with its representative updated descendants, e.g. Fa div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2"">Related Work</head><p>Two-stage detector: R-CNN <ref type=""bibr"" target=""#b11"">[8]</ref> straightforwardly combines the steps of cropping box propos",0
"ncurred by inaccurate boxes). A number of very recent efforts have confirmed such a Reproduced from <ref type=""bibr"" target=""#b39"">[36]</ref> with the permission from J. Winawer and H. Horiguchi (http eir retinotopic maps, and although varying between maps, it increases with eccentricity in each map <ref type=""bibr"" target=""#b39"">[36]</ref>, as illustrated in Fig. <ref type=""figure"" target=""#fig_0"" ften observe the pooled responses of many neurons, these models are thus commonly called pRF models <ref type=""bibr"" target=""#b39"">[36]</ref>. Based on fMRI and pRF modeling, it is possible to investi ex. At each cortical map, researchers find a positive correlation between pRF size and eccentricity <ref type=""bibr"" target=""#b39"">[36]</ref>, while the coefficient of correlation varies in visual fie ). The radius of each circle is the apparent RF size at the appropriate eccentricity.Reproduced from<ref type=""bibr"" target=""#b39"">[36]</ref> with the permission from J. Winawer and H. Horiguchi (http",0
"promoted the performance of object detection on major challenges and benchmarks, such as Pascal VOC <ref type=""bibr"" target=""#b8"">[5]</ref>, MS COCO <ref type=""bibr"" target=""#b24"">[21]</ref>, and ILSV ww.tei-c.org/ns/1.0""><head n=""4"">Experiments</head><p>We conduct experiments on the Pascal VOC 2007 <ref type=""bibr"" target=""#b8"">[5]</ref> and MS COCO <ref type=""bibr"" target=""#b24"">[21]</ref> datase",0
"ones of up-to-date deeper backbone network based detectors <ref type=""bibr"" target=""#b22"">[19,</ref><ref type=""bibr"" target=""#b21"">18,</ref><ref type=""bibr"" target=""#b23"">20]</ref> and retains the fas",0
"the activations of deep neural networks. For this, we consider the ""natural pre-image"" technique of <ref type=""bibr"" target=""#b20"">[21]</ref>, whose goal is to characterize the invariants learned by a n untrained deep convolutional generator can be used to replace the surrogate natural prior used in <ref type=""bibr"" target=""#b20"">[21]</ref> (the TV norm) with dramatically improved results. Since th antic segmentation) is highly detrimental.</p><p>Natural pre-image. The natural pre-image method of <ref type=""bibr"" target=""#b20"">[21]</ref> is a diagnostic tool to study the invariances of a lossy f e obtained by restricting the pre-image to a set X of natural images, called a natural pre-image in <ref type=""bibr"" target=""#b20"">[21]</ref>.</p><p>In practice, finding points in the natural pre-imag inding points in the natural pre-image can  Inversion with deep image prior Inversion with TV prior <ref type=""bibr"" target=""#b20"">[21]</ref> Pre-trained deep inverting network <ref type=""bibr"" target on ImageNet ISLVRC) using three different regularizers: the Deep Image prior, the TV norm prior of <ref type=""bibr"" target=""#b20"">[21]</ref>, and the network trained to invert representations on a ho ne by regularizing the data term similarly to the other inverse problems seen above. The authors of <ref type=""bibr"" target=""#b20"">[21]</ref> prefer to use the TV norm, which is a weak natural image p",1
"Nets) currently set the state-of-the-art in inverse image reconstruction problems such as denoising <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b19"">20]</ref> or single-image supe tribution is conditioned on a corrupted observation x 0 to solve inverse problems such as denoising <ref type=""bibr"" target=""#b4"">[5]</ref> and super-resolution <ref type=""bibr"" target=""#b6"">[7]</ref>",0
"in <ref type=""bibr"" target=""#b24"">[25]</ref>). The quantitative comparison on the standard data set <ref type=""bibr"" target=""#b13"">[14]</ref> for our method is given in table <ref type=""table"">1</ref>",0
"ults of inverting representations Φ obtained by considering progressively deeper subsets of AlexNet <ref type=""bibr"" target=""#b16"">[17]</ref>: conv1, conv2, ..., conv5, fc6, fc7, and fc8. Pre-images a",0
"tor z). The resulting approach can be seen as a non-linear generalization of guided image filtering <ref type=""bibr"" target=""#b12"">[13]</ref>. The results of the restoration are given in the fig. <ref",0
"RNA) <ref type=""bibr"" target=""#b11"">[12]</ref>, and the recurrent neural network transducer (RNN-T) <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b13"">14]</ref>. In particular, th nd-to-end models including attention-based models <ref type=""bibr"" target=""#b6"">[7]</ref> and RNN-T <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b13"">14]</ref> trained on ∼12,500 ""http://www.tei-c.org/ns/1.0""><head n=""2."">RNN-TRANSDUCER</head><p>The RNN-T was proposed by Graves <ref type=""bibr"" target=""#b12"">[13]</ref> as an extension to the connectionist temporal classificati ure <ref type=""figure"">1</ref>, consists of an encoder (referred to as the transcription network in <ref type=""bibr"" target=""#b12"">[13]</ref>), a prediction network and a joint network; as described i e=""bibr"" target=""#b13"">[14]</ref>. The entire network is trained jointly to optimize the RNN-T loss <ref type=""bibr"" target=""#b12"">[13]</ref>, which marginalizes over all alignments of target labels w p><p>During inference, the most likely label sequence is computed using beam search as described in <ref type=""bibr"" target=""#b12"">[13]</ref>, with a minor alteration which was found to make the algor nsive without degrading performance: we skip summation over prefixes in pref(y) (see Algorithm 1 in <ref type=""bibr"" target=""#b12"">[13]</ref>), unless multiple hypotheses are identical.</p><p>Note tha",1
"e state-of-the-art in terms of word error rate (WER) performance. For example, in our previous work <ref type=""bibr"" target=""#b14"">[15]</ref> we evaluated a number of end-to-end models including atten ch recognizer. A deficiency of end-to-end systems appears to be in their language modeling capacity <ref type=""bibr"" target=""#b14"">[15]</ref> which may be because large text-only data are not utilized ef type=""bibr"" target=""#b12"">[13]</ref>), a prediction network and a joint network; as described in <ref type=""bibr"" target=""#b14"">[15]</ref>, the RNN-T model can be compared to other encoder-decoder",1
"d-to-end ASR performance. Another contribution of this work is to investigate the use of wordpieces <ref type=""bibr"" target=""#b15"">[16]</ref>, which have been explored previously in the context of mac labels would be impractically slow. Therefore, as subword units, we use wordpieces as described in <ref type=""bibr"" target=""#b15"">[16]</ref>. We train a statistical wordpiece model with word counts o",1
"aled likelihood obtained by dividing the posterior with the prior, P (φ), in socalled hybrid models <ref type=""bibr"" target=""#b0"">[1]</ref>. Deep recurrent neural networks with long short-term memory",0
"s output units. We investigate encoder architectures with multitask training using hierarchical-CTC <ref type=""bibr"" target=""#b20"">[21]</ref> with various 'hierarchies' of CTC losses at various depths",0
"br"" target=""#b0"">[1]</ref>. Deep recurrent neural networks with long short-term memory (LSTM) cells <ref type=""bibr"" target=""#b1"">[2]</ref> have recently been shown to be ideal for this task <ref type",0
"end-toend ASR models <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b18"">19]</ref>. However, a major limitation of Fig. <ref type=""figure"">1</",0
"rks are trained with multiple simultaneous CTC losses which was beneficial for grapheme recognition <ref type=""bibr"" target=""#b21"">[22]</ref>. After pre-training all CTC losses and additional weights",0
"which we acknowledge may be addressed with recent advancements in training deeper recurrent models <ref type=""bibr"" target=""#b27"">[28]</ref> but are not tested as part of this work. The deeper 8-laye",0
"=""bibr"" target=""#b12"">[13]</ref> as an extension to the connectionist temporal classification (CTC) <ref type=""bibr"" target=""#b16"">[17]</ref> approach for sequence labeling tasks where the alignment b",0
"considerable interest in training end-to-end models for ASR <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b8"">9]</ref>, which directly output ng to a given input frame. CTC has been widely used in previous works to train end-toend ASR models <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" targ",0
"widely used in previous works to train end-toend ASR models <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b18"">19]</ref>. However, a major l",0
"Additional word pronunciations are learned from audio data using pronunciation learning techniques <ref type=""bibr"" target=""#b24"">[25]</ref>. For out-ofdictionary words a G2P model is trained using t",0
"language model components. A particular class of architecures known as sequence-to-sequence models <ref type=""bibr"" target=""#b9"">[10]</ref> are particularly suited for end-to-end ASR as they include",0
"dictionaries curated by expert human linguists, with back-off to a grapheme-to-phoneme (G2P) model <ref type=""bibr"" target=""#b5"">[6]</ref> for out of dictionary words. Finally, an N-gram model traine",0
"18"">[19,</ref><ref type=""bibr"" target=""#b15"">16]</ref> and transductive experimental design methods <ref type=""bibr"" target=""#b26"">[27]</ref>. These kinds of active learning algorithms are referred to ion matrix A. The selected samples are therefore considered to be the most representative.</p><p>In <ref type=""bibr"" target=""#b26"">[27]</ref>, an early active learning via a Transduction Experimental problem to solve, thus an approximate solution by a sequential optimization problem is proposed in <ref type=""bibr"" target=""#b26"">[27]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head nformative and representative examples for labeling using the min-max margin-based approach. 4. TED <ref type=""bibr"" target=""#b26"">[27]</ref> Active learning via Transduction Experimental Design is an",1
". Active learning is widely studied to solve this kind of sample selection problem. As discussed in <ref type=""bibr"" target=""#b17"">[18]</ref>, active learning methods can be divided into two categorie ertain number of labeled samples to evaluate the uncertainty of the unlabeled data or sampling bias <ref type=""bibr"" target=""#b17"">[18]</ref> will result. It is therefore recommended that such methods ive learning algorithms are referred to as early active learning or early stage experimental design <ref type=""bibr"" target=""#b17"">[18]</ref>. We illustrate the procedures of and example of the tradit a><p>Finding the optimal subset V ⊂ X in Eq. ( <ref type=""formula"">7</ref>) is NP-hard. Inspired by <ref type=""bibr"" target=""#b17"">[18]</ref>, we relax the problem to the following problem by introduc ime, the least squared loss used in Eq. ( <ref type=""formula"">8</ref>) is sensitive to the outliers <ref type=""bibr"" target=""#b17"">[18]</ref>, which makes the algorithm not robust.</p><p>We note that type=""bibr"" target=""#b25"">26]</ref>, the 2,1 -norm is used instead of the 2,0 -norm. It is shown in <ref type=""bibr"" target=""#b17"">[18]</ref> that the 2,1 -norm is the minimum convex hull of the 2,0 - 1.0""><head n=""2."">K-means</head><p>We use the K-means algorithm as another baseline algorithm as in <ref type=""bibr"" target=""#b17"">[18]</ref>. In each experiment, samples are ranked by their distances It formulates a regularized linear regression problem which minimizes reconstruction error. 5. RRSS <ref type=""bibr"" target=""#b17"">[18]</ref> Early active learning via Robust Representation and Struct ance of the linear methods with our algorithm. This is consistent with the mathematical analysis in <ref type=""bibr"" target=""#b17"">[18]</ref> that kernelization produces more discriminative representa ithm not robust.</p><p>We note that in previous researches <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b25"">26]</ref>, the 2,1 -norm is u s, minimization of A 2,1 will achieve the same result as A 2,0 when A is row-sparse. As analyzed in <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b29"">30]</ref>, the 2,1 -norm can",1
"s topic has attracted considerable attention in recent years <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target to identify the same person in different camera views among a potentially huge number of imposters <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b19"">20]</ref>. At the same time, p Laplacian matrix and D is the degree matrix with each element D ii = j S V (i, j). As discussed in <ref type=""bibr"" target=""#b8"">[9]</ref>, minimizing the pairwise constraint will force the similar r images of a person have a high probability of sharing the similar representation features in re-id <ref type=""bibr"" target=""#b8"">[9]</ref>, this will make early active learning schema more suitable f",1
"g for re-id on four widely referred benchmark datasets for person re-identification.</p><p>1. VIPeR <ref type=""bibr"" target=""#b3"">[4]</ref> The VIPeR dataset contains 1,264 images of 632 persons from",0
"ype=""bibr"" target=""#b0"">1,</ref><ref type=""bibr"" target=""#b21"">22]</ref> query by committee methods <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b2"">3]</ref>. Most of these activ",0
"al. It contains 476 images with large occlusions caused by luggage and viewpoint changes. 4. CAVIAR <ref type=""bibr"" target=""#b1"">[2]</ref> The CAVIAR dataset contains 72 individuals captured by two c",0
"ention in recent years <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" targ ula_17"">)</formula><p>We denote our Early Active Learning with Pairwise Constraint algorithm in Eq. <ref type=""bibr"" target=""#b9"">(10)</ref> as EALPC and the kenerlized version of our algorithm in Eq. gorithm for optimizing the proposed objective function.</p><p>Taking the derivative w.r.t. A in Eq. <ref type=""bibr"" target=""#b9"">(10)</ref> and setting it to zero, we obtain 1 :</p><formula xml:id=""f algorithm:</p><p>Theorem 1. Algorithm 1 monotonically decreases the objective function value of Eq. <ref type=""bibr"" target=""#b9"">(10)</ref> in each iteration. 1 In practice, when xi − Xai = 0, pii ca <ref type=""bibr"" target=""#b11"">(12)</ref>.</p><p>Algorithm 1: Algorithm for solving problem in Eq. <ref type=""bibr"" target=""#b9"">(10)</ref> Input: The data matrix X ∈ R d×n , parameters α and β. 1 In",0
"umination conditions <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" target=""#b22"">23]</ref>.</p><p>Supervised re-id methods can achieve promising resul",0
"ble supervised models. In the category of early active learning, there are clustering-based methods <ref type=""bibr"" target=""#b18"">[19,</ref><ref type=""bibr"" target=""#b15"">16]</ref> and transductive e",0
"nty sampling methods <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b0"">1,</ref><ref type=""bibr"" target=""#b21"">22]</ref> query by committee me",0
"=""#b17"">[18]</ref>, which makes the algorithm not robust.</p><p>We note that in previous researches <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" ta .tei-c.org/ns/1.0""><head n=""4"">Convergence Analysis</head><p>We first introduce a lemma proposed in <ref type=""bibr"" target=""#b16"">[17]</ref>:</p><p>Lemma 1. For any arbitrary vector m and n there is<",0
"pping camera views, and research on this topic has attracted considerable attention in recent years <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target",0
"early active learning, there are clustering-based methods <ref type=""bibr"" target=""#b18"">[19,</ref><ref type=""bibr"" target=""#b15"">16]</ref> and transductive experimental design methods <ref type=""bib",0
"ommonly used performance measure for person re-id algorithms <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b11"">12]</ref>. CMC calculates the",0
"rent camera. Variations in viewpoint and illumination conditions occur frequently in VIPeR. 2. PRID <ref type=""bibr"" target=""#b4"">[5]</ref> The PRID dataset contains images of 385 individuals from two",0
"we propose Tacotron, an end-to-end generative TTS model based on the sequence-to-sequence (seq2seq) <ref type=""bibr"" target=""#b17"">(Sutskever et al., 2014)</ref> with attention paradigm <ref type=""bib",1
"nce (seq2seq) <ref type=""bibr"" target=""#b17"">(Sutskever et al., 2014)</ref> with attention paradigm <ref type=""bibr"" target=""#b3"">(Bahdanau et al., 2014)</ref>. Our model takes characters as input and ><head n=""3"">MODEL ARCHITECTURE</head><p>The backbone of Tacotron is a seq2seq model with attention <ref type=""bibr"" target=""#b3"">(Bahdanau et al., 2014;</ref><ref type=""bibr"" target=""#b21"">Vinyals et",1
"1-D convolutions, whose outputs are added with the original input sequence via residual connections <ref type=""bibr"" target=""#b9"">(He et al., 2016)</ref>. Batch normalization <ref type=""bibr"">(Ioffe &",0
"ref> .</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2"">RELATED WORK</head><p>WaveNet <ref type=""bibr"" target=""#b20"">(van den Oord et al., 2016)</ref> is a powerful generative model of a rames allows the attention to move forward early in training.</p><p>A similar trick is also used in <ref type=""bibr"" target=""#b20"">Zen et al. (2016)</ref> but mainly to speed up inference.</p><p>The f used to predict alternative targets such as vocoder parameters, or as a WaveNet-like neural vocoder <ref type=""bibr"" target=""#b20"">(van den Oord et al., 2016;</ref><ref type=""bibr"" target=""#b13"">Mehri y include ratings where headphones were used. We compare our model with a parametric (based on LSTM <ref type=""bibr"" target=""#b20"">(Zen et al., 2016)</ref>) and a concatenative system <ref type=""bibr""",0
"nd a complex signal-processing-based vocoder <ref type=""bibr"" target=""#b24"">(Zen et al., 2009;</ref><ref type=""bibr"" target=""#b1"">Agiomyrgiannakis, 2015)</ref>. These components are based on extensive",0
"i-c.org/ns/1.0""><head n=""1"">INTRODUCTION</head><p>Modern text-to-speech (TTS) pipelines are complex <ref type=""bibr"" target=""#b18"">(Taylor, 2009)</ref>. For example, it is common for statistical param",0
"trained on phoneme inputs and the experimental results seem to be somewhat limited.</p><p>Char2Wav <ref type=""bibr"" target=""#b14"">(Sotelo et al., 2017)</ref> is an independently-developed end-to-end",0
"#fig_1"">2</ref>. CBHG consists of a bank of 1-D convolutional filters, followed by highway networks <ref type=""bibr"" target=""#b16"">(Srivastava et al., 2015)</ref> and a bidirectional gated recurrent u",0
"(based on LSTM <ref type=""bibr"" target=""#b20"">(Zen et al., 2016)</ref>) and a concatenative system <ref type=""bibr"" target=""#b7"">(Gonzalvo et al., 2016)</ref>, both of which are in production. As sho",0
"nowledge can be captured for recovering the high-frequency details in HR images.</p><p>Recent works <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b11"">12]</ref> have successfully tional neural network <ref type=""bibr"" target=""#b1"">[2]</ref>. Among them, the CNN-based approaches <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b11"">12]</ref> have recently set ork, making it easy to train. In addition, in previous works <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b10"">11]</ref>, only high-level features at top layers were used in the re formance. Instead of using interpolation for upscaling as in <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b10"">11]</ref>, recent studies <ref type=""bibr"" target=""#b2"">[3,</ref><ref ere studied and compared in our work. As in previous methods <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b10"">11]</ref>, only the feature maps at the top layer are used as input f to learn an end-to-end mapping for SR. Subsequently, a deep network with 20 layers was proposed in <ref type=""bibr"" target=""#b10"">[11]</ref> to improve the reconstruction accuracy of CNN. The residua on accuracy of CNN. The residuals between the HR images and the interpolated LR images were used in <ref type=""bibr"" target=""#b10"">[11]</ref> to speedup the converging speed in training and also to im yers</head><p>In previous SR methods such as SRCNN <ref type=""bibr"" target=""#b1"">[2]</ref> and VDSR <ref type=""bibr"" target=""#b10"">[11]</ref>, bicubic interpolation is used to upscale LR images to the plus <ref type=""bibr"" target=""#b23"">[24]</ref>, SRCNN <ref type=""bibr"" target=""#b1"">[2]</ref>, VDSR <ref type=""bibr"" target=""#b10"">[11]</ref> and DRCN <ref type=""bibr"" target=""#b11"">[12]</ref>. The im <ref type=""bibr"" target=""#b1"">[2]</ref> with 3-layer CNN and an increase of about 0.5 dB over VDSR <ref type=""bibr"" target=""#b10"">[11]</ref> with 20-layer CNN. It should be mentioned that the most si Aplus <ref type=""bibr"" target=""#b23"">[24]</ref> SRCNN <ref type=""bibr"" target=""#b1"">[2]</ref> VDSR <ref type=""bibr"" target=""#b10"">[11]</ref> DRCN <ref type=""bibr"" target=""#b11"">[12]</ref>   In additi",1
"form single image super-resolution (SISR), and significant improvements over shallow CNN structures <ref type=""bibr"" target=""#b1"">[2]</ref> have been observed. One benefit from using deeper networks i [4]</ref>, random forest <ref type=""bibr"" target=""#b19"">[20]</ref> and convolutional neural network <ref type=""bibr"" target=""#b1"">[2]</ref>. Among them, the CNN-based approaches <ref type=""bibr"" targe ef> have recently set state of the art for SISR. A network with three layers was first developed in <ref type=""bibr"" target=""#b1"">[2]</ref> to learn an end-to-end mapping for SR. Subsequently, a deep tei-c.org/ns/1.0""><head n=""3.2."">Deconvolution layers</head><p>In previous SR methods such as SRCNN <ref type=""bibr"" target=""#b1"">[2]</ref> and VDSR <ref type=""bibr"" target=""#b10"">[11]</ref>, bicubic using other SISR methods, including bicubic, Aplus <ref type=""bibr"" target=""#b23"">[24]</ref>, SRCNN <ref type=""bibr"" target=""#b1"">[2]</ref>, VDSR <ref type=""bibr"" target=""#b10"">[11]</ref> and DRCN <re datasets. On average, an increase of about 1.0 dB using the proposed method was achieved over SRCNN <ref type=""bibr"" target=""#b1"">[2]</ref> with 3-layer CNN and an increase of about 0.5 dB over VDSR < llowing us to train very deep Dataset Bicubic Aplus <ref type=""bibr"" target=""#b23"">[24]</ref> SRCNN <ref type=""bibr"" target=""#b1"">[2]</ref> VDSR <ref type=""bibr"" target=""#b10"">[11]</ref> DRCN <ref typ formation and gradient through the network, making it easy to train. In addition, in previous works <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b10"">11]</ref>, only high-level fea also to improve the reconstruction performance. Instead of using interpolation for upscaling as in <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b10"">11]</ref>, recent studies <ref different types of network structures were studied and compared in our work. As in previous methods <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b10"">11]</ref>, only the feature ma",1
"chniques have been developed to model the mapping from LR to HR space, including neighbor embedding <ref type=""bibr"" target=""#b3"">[4]</ref>, random forest <ref type=""bibr"" target=""#b19"">[20]</ref> and",0
"ref><ref type=""bibr"" target=""#b10"">11]</ref>, recent studies <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b20"">21]</ref> have demonstrated that the SR performance can be further im using deconvolution layers <ref type=""bibr"" target=""#b2"">[3]</ref> or sub-pixel convolution layers <ref type=""bibr"" target=""#b20"">[21]</ref>. In our work, we employ the very deep network and also int",0
"al networks, which demonstrates a great amount of redundancy in deep residual networks. FractalNets <ref type=""bibr"" target=""#b13"">[14]</ref> combines several parallel networks with different depths a",0
"s do not necessarily represent visually pleasing results. Recently, perceptual loss was proposed in <ref type=""bibr"" target=""#b9"">[10]</ref> for SR to replace the low-level pixel-wise loss. Further, a",0
"dversarial network (GAN) was added to the loss function in <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b18"">19]</ref> and photo-realistic SR images can be generated. Although th",0
"posed to address this problem. ResNets <ref type=""bibr"" target=""#b5"">[6]</ref> and Highway Networks <ref type=""bibr"" target=""#b21"">[22]</ref> use bypassing path between layers to effectively train net",0
",000 images were randomly selected from ImageNet for the training. During testing, the dataset Set5 <ref type=""bibr"" target=""#b0"">[1]</ref> and Set14 <ref type=""bibr"" target=""#b28"">[29]</ref> are ofte",0
"f skip connections. Many symmetric skip connections were introduced in an encoding-decoding network <ref type=""bibr"" target=""#b16"">[17]</ref> for image restoration tasks. However, the improvement of t ormation for solving the SR problem. Therefore, recent works <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b16"">17]</ref> have employed deconvolution layers to learn the upscaling f",0
"huge boost in performance using Deep-Learning based methods <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target yer. The network input is interpolated to the output size. As done in previous CNN-based SR methods <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" targe orted numerical were produced using the evaluation script of <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b9"">10]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=",1
"a single image has recently received a huge boost in performance using Deep-Learning based methods <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" targe CNN-based SR methods <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b3"">4]</ref>, we only learn the residual between the interpolated LR and i haustively trained for these conditions. In fact, ZSSR is significantly better than the older SRCNN <ref type=""bibr"" target=""#b3"">[4]</ref>, and in some cases achieves comparable or better results tha",1
"target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b12"">13]</ref>. The recent SotA (S s to the point where bicubic interpolation outperforms current SotA SR methods! Comparison to SRGAN <ref type=""bibr"" target=""#b11"">[12]</ref>: SRGAN is also trained for the ideal case. In those cases,",0
"use deep learning, also outperforms SotA SR methods. This supports the analysis and observations of <ref type=""bibr"" target=""#b17"">[18]</ref>, that (i) an accurate downscaling model is more important",0
"(when the downscaling kernel is unknown), Blind-Deblurring <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b1"">2]</ref>, Blind-Dehazing <ref type=""bibr"" target=""#b2"">[3]</ref>, and",0
"se approaches for Zero-Shot Learning <ref type=""bibr"" target=""#b22"">[23]</ref> or One-shot Learning <ref type=""bibr"" target=""#b18"">[19]</ref>, our approach does not require any side information/attrib",0
") method <ref type=""bibr"" target=""#b12"">[13]</ref> exceeds previous non-Deep SR methods (supervised <ref type=""bibr"" target=""#b21"">[22]</ref> or unsupervised <ref type=""bibr"" target=""#b4"">[5,</ref><re",0
"ref type=""bibr"" target=""#b14"">[15]</ref> (when the downscaling kernel is unknown), Blind-Deblurring <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b1"">2]</ref>, Blind-Dehazing <ref",0
"ven from very small LR images, the SR is performed gradually <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b20"">21]</ref>. Our algorithm is applied for several intermediate scale-fa",0
"these 8 outputs rather than their mean. We further combine it with the back-projection technique of <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b4"">5]</ref>, so that each of the 8",0
"ate a variable number of features to a fixed-length vector. Fisher Vectors were first introduced in <ref type=""bibr"" target=""#b15"">(Jaakkola and Haussler 1999)</ref> to combine the advantages of gener",1
"Vector encoding. The micro-expression detectors are trained using a linear kernel SVM using LibSVM <ref type=""bibr"" target=""#b3"">(Chang and Lin 2011)</ref>. The results are shown in Table <ref type=""",0
"elated with deception. Although it achieves high accuracy <ref type=""bibr"">(Kozel et al. 2005;</ref><ref type=""bibr"" target=""#b17"">Langleben and Moriarty 2013;</ref><ref type=""bibr"" target=""#b11"">Fara to the working mechanism, reliability and the experimental setting are still open research problems <ref type=""bibr"" target=""#b17"">(Langleben and Moriarty 2013;</ref><ref type=""bibr"" target=""#b11"">Far",0
""" target=""#b21"">(Pavlidis, Eberhardt, and Levine 2002)</ref> and measure the blood flow of the body <ref type=""bibr"" target=""#b1"">(Buddharaju et al. 2005)</ref>, but the technique requires expensive t",0
"terrogation, but their reliability is questionable. Thermal imaging can record the thermal patterns <ref type=""bibr"" target=""#b21"">(Pavlidis, Eberhardt, and Levine 2002)</ref> and measure the blood fl",0
"method should integrate information from more than one modality. Taking motivation from prior work <ref type=""bibr"" target=""#b23"">(Pérez-Rosas et al. 2015;</ref><ref type=""bibr"" target=""#b16"">Jaiswal ceptive behavior. We also include simple verbal and audio features, as other multi-modal approaches <ref type=""bibr"" target=""#b23"">(Pérez-Rosas et al. 2015;</ref><ref type=""bibr"" target=""#b16"">Jaiswal ed more on high stake deception detection, so that experiments are closer to real life settings. In <ref type=""bibr"" target=""#b23"">(Pérez-Rosas et al. 2015)</ref>, a new dataset containing reallife tr >We evaluate our automated deception detection approach on a real-life deception detection database <ref type=""bibr"" target=""#b23"">(Pérez-Rosas et al. 2015)</ref>. This database consists of 121 court nge or human editing. In the experiments shown below, we do not report the results, as described in <ref type=""bibr"" target=""#b23"">(Pérez-Rosas et al. 2015)</ref>. Instead, we re-implement the method is how the performance will be affected if we use the Ground Truth micro-expression features, as in <ref type=""bibr"" target=""#b23"">(Pérez-Rosas et al. 2015)</ref>. In the following experiment, we use",0
"for personal and public safety.</p><p>The ability of humans to detect deception is very limited. In <ref type=""bibr"" target=""#b0"">(Bond Jr and DePaulo 2006)</ref>, it was reported that the average acc",0
"ly tested with static images, so essential motion patterns in facial expressions were not captured. <ref type=""bibr"" target=""#b20"">(Michael et al. 2010)</ref> proposed a new feature called motion prof",0
"amics for recognizing facial micro-expressions. This coincides with the psychological insights from <ref type=""bibr"" target=""#b7"">(Duran et al. 2013)</ref>, in which the authors suggest focusing on dy patterns, e.g. micro facial expression, itself is a challenging problem. In addition, Duran et al. <ref type=""bibr"" target=""#b7"">(Duran et al. 2013)</ref> suggested that research should focus more on",0
"earning the properties of molecules from their structure:</p><p>1. The Harvard Clean Energy Project <ref type=""bibr"" target=""#b15"">(Hachmann et al., 2011)</ref>, consisting of 2.3 million organic comp",0
"on graphs has a long history in the kernels literature, including approaches based on random walks <ref type=""bibr"" target=""#b13"">(Gärtner, 2002;</ref><ref type=""bibr"" target=""#b1"">Borgwardt &amp; Kr ernel based learning, for example, invariant kernels have been constructed by counting random walks <ref type=""bibr"" target=""#b13"">(Gärtner, 2002)</ref>, matching eigenvalues of the graph Laplacian <r",0
"#b11"">(Fischler &amp; Elschlager, 1973;</ref><ref type=""bibr"" target=""#b29"">Ohta et al., 1978;</ref><ref type=""bibr"" target=""#b39"">Tu et al., 2005;</ref><ref type=""bibr"" target=""#b8"">Felzenszwalb &amp",0
"ge R-CNN framework <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b29"">30,</ref><ref type=""bibr"" target=""#b22"">23]</ref>, where detection is et=""#b12"">[13]</ref> introduced the idea of region-wise feature extraction. Later, the Faster R-CNN <ref type=""bibr"" target=""#b29"">[30]</ref> achieved further speeds-up by introducing a Region Proposa network. SSD <ref type=""bibr"" target=""#b24"">[25]</ref> detects objects in a way similar to the RPN <ref type=""bibr"" target=""#b29"">[30]</ref>, but uses multiple feature maps at different resolutions t We focus on modeling a multistage detection sub-network, and adopt, but are not limited to, the RPN <ref type=""bibr"" target=""#b29"">[30]</ref> for proposal detection.</p></div> <div xmlns=""http://www.t evels. At inference, since the majority of the hypotheses produced by a proposal detector, e.g. RPN <ref type=""bibr"" target=""#b29"">[30]</ref> or selective search <ref type=""bibr"" target=""#b32"">[33]</r "">Object Detection</head><p>In this paper, we extend the two-stage architecture of the Faster R-CNN <ref type=""bibr"" target=""#b29"">[30,</ref><ref type=""bibr"" target=""#b22"">23]</ref>, shown in Figure < zed by its mean and variance, i.e. is replaced by ′ =( − )/ . This is widely used in the literature <ref type=""bibr"" target=""#b29"">[30,</ref><ref type=""bibr"" target=""#b0"">1,</ref><ref type=""bibr"" targ PN+.</p><p>Detection Performance: Again, our implementations are better than the original detectors <ref type=""bibr"" target=""#b29"">[30,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" targ was further experimented on PAS-CAL VOC dataset <ref type=""bibr"" target=""#b7"">[8]</ref>. Following <ref type=""bibr"" target=""#b29"">[30,</ref><ref type=""bibr"" target=""#b24"">25]</ref>, the models were t e noted. The sampling of the first detection stage follows <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b29"">30]</ref>. In the following stages, resampling is implemented by simp",1
"only be optimal for a single quality level. This is known in the cost-sensitive learning literature <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b25"">26]</ref>, where the optimizat",0
"ositives. Many of the recently proposed object detectors are based on the two-stage R-CNN framework <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" ta ns=""http://www.tei-c.org/ns/1.0""><head n=""2."">Related Work</head><p>Due to the success of the R-CNN <ref type=""bibr"" target=""#b13"">[14]</ref> architecture, the two-stage detection framework, by combin",0
"etection architectures have also become popular, mostly due to their computational efficiency. YOLO <ref type=""bibr"" target=""#b28"">[29]</ref> outputs very sparse detection results and enables real tim",0
"s applied several times, to produce better bounding boxes. <ref type=""bibr"" target=""#b35"">[36,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b10"">11]</ref> used a multi-stage",0
"able place to look is human cognition <ref type=""bibr"" target=""#b7"">(Davis &amp; Marcus, 2015;</ref><ref type=""bibr"" target=""#b29"">Lake et al., 2016;</ref><ref type=""bibr"" target=""#b42"">Marcus, 2001;<",1
"close attention to map matching techniques on low-samplingrate trajectories. ST-Matching algorithm <ref type=""bibr"" target=""#b3"">[4]</ref> is the first one solving the problem about low-sampling-rate ter information, such as the data from mobile devices <ref type=""bibr"" target=""#b12"">[13]</ref>. In <ref type=""bibr"" target=""#b3"">[4]</ref>, the author combines the spatial structures of the road netw",1
"factor S(P i ). Usually, when object moves, it tend to be close to the max speed limit of the road <ref type=""bibr"" target=""#b17"">[18]</ref>. Therefore, we calculate the deviation between the average ng path using ST-Matching and MSTM respectively, denoted as P st and P mstm . We calculate the LCSS <ref type=""bibr"" target=""#b17"">[18]</ref> similarity of LCSS st (P true , P st ) and LCSS mstm (P tr",0
"to several services, such as route plan <ref type=""bibr"" target=""#b0"">[1]</ref>, hot route discover <ref type=""bibr"" target=""#b1"">[2]</ref> and even social relationship infer <ref type=""bibr"" target=""",0
"re algorithms <ref type=""bibr"" target=""#b13"">[14]</ref>, <ref type=""bibr"" target=""#b14"">[15]</ref>, <ref type=""bibr"" target=""#b15"">[16]</ref> try to solve the problem of low-sampling-rate trajectories",0
"rom company to home. This phenomenon can be illustrated by the conception of stay point proposed in <ref type=""bibr"" target=""#b16"">[17]</ref>.</p><p>Usually, a stay point s is determined by two parame ><p>Usually, we don't have to pay more attention to the stay points which are easy for map matching <ref type=""bibr"" target=""#b16"">[17]</ref>. Therefore, we partition the trajectories into several eff",0
"ction based models <ref type=""bibr"" target=""#b22"">[13,</ref><ref type=""bibr"" target=""#b31"">21,</ref><ref type=""bibr"" target=""#b39"">29]</ref>. Interaction based models thrive with encoding word-word tr to ranking signals <ref type=""bibr"" target=""#b20"">[11,</ref><ref type=""bibr"" target=""#b22"">13,</ref><ref type=""bibr"" target=""#b39"">29]</ref>. Learned end-to-end from user feedbacks <ref type=""bibr"" ta t=""#b39"">29]</ref>. Learned end-to-end from user feedbacks <ref type=""bibr"" target=""#b33"">[23,</ref><ref type=""bibr"" target=""#b39"">29]</ref>, the word embeddings can encode so matches tailored for rel nce than score-based ones like mean-pooling or max-pooling <ref type=""bibr"" target=""#b22"">[13,</ref><ref type=""bibr"" target=""#b39"">29]</ref>.</p><p>Kernel-pooling is applied to each M h q ,h d matrix hes are more e ective than weight-summing the similarities <ref type=""bibr"" target=""#b22"">[13,</ref><ref type=""bibr"" target=""#b39"">29]</ref>-""similarity does not necessarily mean relevance"" <ref type= lored for relevance ranking, which has signi cant advantages over traditional feature-based methods <ref type=""bibr"" target=""#b39"">[29,</ref><ref type=""bibr"" target=""#b40"">30]</ref>. ese initial succe d learningto-rank techniques are then used to combine the n-gram somatches to the nal ranking score <ref type=""bibr"" target=""#b39"">[29]</ref>.</p><p>e CNN is the key to modeling n-grams. Typical IR ap [30]</ref>.</p><p>K-NRM uni ed the progress of IR customized embeddings and interaction based model <ref type=""bibr"" target=""#b39"">[29]</ref>. It rst embeds words and builds the translation matrix usi log, K-NRM outperforms both neural IR methods and feature-based learning-to-rank by a large margin <ref type=""bibr"" target=""#b39"">[29]</ref>.</p><p>ough the so matching of n-grams in information retr -torank layer to calculate the ranking score using the n-gram translations M. is part extends K-NRM <ref type=""bibr"" target=""#b39"">[29]</ref> to n-grams. Kernel-pooling is a pooling technique that use ead><p>Conv-KNRM adds the ability of so matching n-grams to the recent state-of-the-art K-NRM model <ref type=""bibr"" target=""#b39"">[29]</ref> with convolutional neural networks (CNNs). Without CNNs, C g Conv-KNRM requires large-scale training data, for example, user clicks in a commercial search log <ref type=""bibr"" target=""#b39"">[29]</ref> or industryscale annotations <ref type=""bibr"" target=""#b31 raining data. ey are then used in the target domain to generate so -TF features Φ(M). Xiong, et al. <ref type=""bibr"" target=""#b39"">[29]</ref> showed that kernel-pooled so -TF features reveal di erent rnel is of low importance in search logs as all candidate documents already contain the query words <ref type=""bibr"" target=""#b39"">[29]</ref>; however, synonyms can be a strong signal in a recall-orie Log: Sogou.com is a major Chinese commercial search engine.</p><p>e same se ings as K-NRM were used <ref type=""bibr"" target=""#b39"">[29]</ref>. e same sample of Sogou log and training-testing splits ar IR baselines for stronger baseline performance. Body texts of training documents were not available <ref type=""bibr"" target=""#b39"">[29]</ref>. e Chinese text was segmented by ICTCLASS <ref type=""bibr"" ad><p>Training and testing labels on Sogou-Log and Bing-Log were generated following prior research <ref type=""bibr"" target=""#b39"">[29]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head> ref type=""bibr"" target=""#b33"">[23]</ref>, DRMM <ref type=""bibr"" target=""#b22"">[13]</ref>, and K-NRM <ref type=""bibr"" target=""#b39"">[29]</ref>.</p><p>CDSSM <ref type=""bibr"" target=""#b36"">[26]</ref> is ry and document representations on their words' le er-tri-grams (or Chinese characters in Sogou-Log <ref type=""bibr"" target=""#b39"">[29]</ref>). e ranking scores are calculated by the similarity betwee erwards.</p><p>K-NRM is a state-of-the-art neural model previously tested on the Sogou-Log dataset <ref type=""bibr"" target=""#b39"">[29]</ref>. It uses kernel-pooling instead of DRMM's histogram poolin earch logs, 5-fold cross validation were used to be consistent with the previous study on Sogou-Log <ref type=""bibr"" target=""#b39"">[29]</ref>. On ClueWeb09-B, the 10-fold cross validation splits from n Sogou-Log, traditional IR methods used both title and body, and neural IR methods only used title <ref type=""bibr"" target=""#b39"">[29]</ref>, as discussed in section 5.1. On Bing-Log, all methods use were all learned end-to-end using the query logs. For Sogou-log, we set embedding dimension L = 300 <ref type=""bibr"" target=""#b39"">[29]</ref> . For Bing-Log, we set L = 100 because our pilot study sho ers were: µ 1 = 0.9, µ 2 = 0.7, ..., µ 10 = −0.9.</p><p>e σ of the so match bins were set to be 0.1 <ref type=""bibr"" target=""#b39"">[29]</ref>. Model Implementation and E ciency: e model was implemente bout 12 hours on an AWS GPU machine. e training time is similar with prior work using only unigrams <ref type=""bibr"" target=""#b39"">[29]</ref>. Most computation time was spent on the embedding layer; t rrent neural IR methods to provide additional improvements <ref type=""bibr"" target=""#b32"">[22,</ref><ref type=""bibr"" target=""#b39"">29,</ref><ref type=""bibr"" target=""#b40"">30]</ref> Comparing the two s",1
"(neural IR) is the development of interaction based models <ref type=""bibr"" target=""#b22"">[13,</ref><ref type=""bibr"" target=""#b31"">21,</ref><ref type=""bibr"" target=""#b39"">29]</ref>. Interaction based eraction based model and the representation based model can also be combined in a duet architecture <ref type=""bibr"" target=""#b31"">[21]</ref>.</p><p>Another trend of neural IR research is to learn cus s in a commercial search log <ref type=""bibr"" target=""#b39"">[29]</ref> or industryscale annotations <ref type=""bibr"" target=""#b31"">[21]</ref>. However, for many search domains such as TREC benchmarks,",0
"or example, by emphasizing frequent and meaningful concepts <ref type=""bibr"" target=""#b12"">[3,</ref><ref type=""bibr"" target=""#b42"">32]</ref>. A more recent trend is to use entities to introduce explic",0
"a translation matrix of word pairs between query and document, and summarize it to a ranking score <ref type=""bibr"" target=""#b13"">[4]</ref>. e main challenge of translation models is that the word-pa",0
"document o en match at n-grams, such as phrases <ref type=""bibr"" target=""#b28"">[18]</ref>, concepts <ref type=""bibr"" target=""#b11"">[2]</ref>, and entities <ref type=""bibr"" target=""#b38"">[28]</ref>; ho",0
"r"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b15"">16]</ref> and natural language processing <ref type=""bibr"" target=""#b11"">[12]</ref>.</p><p>The novelty of Caser is to represent the previous L where d is the number of latent dimensions and the rows preserve the order of the items. Similar to <ref type=""bibr"" target=""#b11"">[12]</ref>, we regard this embedding matrix as the ""image"" of the L i r"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b15"">16]</ref> and natural language processing <ref type=""bibr"" target=""#b11"">[12]</ref>. Borrows the idea of using CNN in text classi cation <ref sing <ref type=""bibr"" target=""#b11"">[12]</ref>. Borrows the idea of using CNN in text classi cation <ref type=""bibr"" target=""#b11"">[12]</ref>, our approach regards the L × d matrix E as the ""image"" of",1
"ial recommendation.</p><p>Recurrent neural networks (RNN) was used for session-based recommendation <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b9"">10]</ref>. While RNN has shown modeling general user preferences. • GRU4Rec. This is the session-based recommendation proposed by <ref type=""bibr"" target=""#b7"">[8]</ref>. This model uses RNN to capture sequential dependencies and ing sets of the other three data sets despite the use of regularization and dropout as described in <ref type=""bibr"" target=""#b7"">[8]</ref>. In addition, GRU4Rec's recommendation is session-based, ins",1
"uccessful 2layers neural network that is applied to recommendation problems. Auto-encoder framework <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b28"">29]</ref> and its variant de",0
"n action occurs in the sequence S u , not the absolute timestamp as in temporal recommendation like <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b30"">31,</ref><ref type=""bibr"" ta",0
"target=""#b9"">10]</ref>. While RNN has shown to have an impressive capability in modeling sequences <ref type=""bibr"" target=""#b17"">[18]</ref>, its sequentially connected network structure may not work",0
"provide evidences of union-level in uences and skip behaviors, we mine sequential association rules <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b3"">4]</ref> of the following form l patterns because they do not model the order of actions. Early works on sequential pattern mining <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b3"">4]</ref> nd explicit sequential",0
"target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b29"">30]</ref> is an early approach to top-N sequential recommendation, wh",0
"type=""bibr"" target=""#b23"">[24]</ref>, matrix factorization <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b21"">22]</ref>, and top-N recommendation <ref type=""bibr"" target=""#b8"">[9]",0
"the absolute timestamp as in temporal recommendation like <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b30"">31,</ref><ref type=""bibr"" target=""#b33"">34]</ref>. Given all users' s >A related but di erent problem is temporal recommendation <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b30"">31,</ref><ref type=""bibr"" target=""#b33"">34]</ref>. For example, tempo",0
"=""2"">FURTHER RELATED WORK</head><p>Conventional recommendation methods, e.g., collaborative ltering <ref type=""bibr"" target=""#b23"">[24]</ref>, matrix factorization <ref type=""bibr"" target=""#b14"">[15,<",0
"ate to handle multi-relation QA due to the lack of reasoning ability.</p><p>Recent reasoning models <ref type=""bibr"" target=""#b16"">(Miller et al., 2016;</ref><ref type=""bibr"" target=""#b26"">Wang et al. manner during reasoning. MemNN <ref type=""bibr"" target=""#b27"">(Weston et al., 2015)</ref>, KVMemN2N <ref type=""bibr"" target=""#b16"">(Miller et al., 2016)</ref> and EviNet <ref type=""bibr"" target=""#b21"" re the settings are the same as <ref type=""bibr"" target=""#b8"">(Bordes et al., 2015)</ref>. KVMemN2N <ref type=""bibr"" target=""#b16"">(Miller et al., 2016)</ref> improves the MemN2N for KBQA as it divide",1
"notations, but lack the ability to generalize to other domains. In contrast, embedding-based models <ref type=""bibr"" target=""#b7"">(Bordes et al., 2014b;</ref><ref type=""bibr"" target=""#b13"">Hao et al., en executing logical queries on incomplete KBs. Our work follows the line of Embedding-based models <ref type=""bibr"" target=""#b7"">(Bordes et al., 2014b;</ref><ref type=""bibr"" target=""#b10"">Dong et al. ated to these three OOV relations during the test.</p><p>Several baselines are included here: Embed <ref type=""bibr"" target=""#b7"">(Bordes et al., 2014b)</ref> deals with factoid QA over KB by matching",1
"es are designed to comprehend documents. State-of-the-art memory-based Reading Comprehension models <ref type=""bibr"" target=""#b24"">(Sukhbaatar et al., 2015;</ref><ref type=""bibr"" target=""#b15"">Kumar e ts an LSTM to encode the input question sequence and another LSTM to decode the answer path. MemN2N <ref type=""bibr"" target=""#b24"">(Sukhbaatar et al., 2015)</ref> is an end-to-end memory network that",0
"ains. In contrast, embedding-based models <ref type=""bibr"" target=""#b7"">(Bordes et al., 2014b;</ref><ref type=""bibr"" target=""#b13"">Hao et al., 2017;</ref><ref type=""bibr"" target=""#b29"">Yavuz et al., 2 e=""bibr"" target=""#b10"">Dong et al., 2015;</ref><ref type=""bibr"" target=""#b28"">Xu et al., 2016;</ref><ref type=""bibr"" target=""#b13"">Hao et al., 2017;</ref><ref type=""bibr"" target=""#b29"">Yavuz et al., 2 Bordes et al., 2014a)</ref>, answer aspects <ref type=""bibr"" target=""#b10"">(Dong et al., 2015;</ref><ref type=""bibr"" target=""#b13"">Hao et al., 2017)</ref> and external contexts <ref type=""bibr"" target",0
"del is challenging. Neural Module Network <ref type=""bibr"" target=""#b1"">(Andreas et al., 2015;</ref><ref type=""bibr"" target=""#b2"">Andreas et al., 2016)</ref> customized network architectures for diffe",0
"I and this task has recently been facilitated by large-scale Knowledge Bases (KBs) such as Freebase <ref type=""bibr"" target=""#b4"">(Bollacker et al., 2008)</ref>. However, due to the variety and comple http://www.tei-c.org/ns/1.0""><head n=""4.1"">PathQuestion</head><p>We adopted two subsets of Freebase <ref type=""bibr"" target=""#b4"">(Bollacker et al., 2008)</ref> as Knowledge Bases to construct the Pat housands of entities. As for PathQuestion-Large, we adopted another more complex subset of Freebase <ref type=""bibr"" target=""#b4"">(Bollacker et al., 2008)</ref>. First, we extracted all the paths with",0
"target=""#b20"">(Pasupat and Liang, 2015;</ref><ref type=""bibr"" target=""#b31"">Yih et al., 2016;</ref><ref type=""bibr"" target=""#b0"">Abujabal et al., 2017)</ref>. These systems are effective but at the c",0
"al., 2015;</ref><ref type=""bibr"" target=""#b19"">Neelakantan et al., 2016)</ref> and Neural Enquirer <ref type=""bibr"" target=""#b32"">(Yin et al., 2015)</ref>, where deep networks are proposed to parse a",0
"""bibr"" target=""#b8"">(Bordes et al., 2015;</ref><ref type=""bibr"" target=""#b28"">Xu et al., 2016;</ref><ref type=""bibr"" target=""#b21"">Savenkov and Agichtein, 2017)</ref>. In comparison, reasoning over mu et al., 2015)</ref>, KVMemN2N <ref type=""bibr"" target=""#b16"">(Miller et al., 2016)</ref> and EviNet <ref type=""bibr"" target=""#b21"">(Savenkov and Agichtein, 2017)</ref> transferred the reading comprehe",0
"""bibr"" target=""#b22"">Shen et al., 2016;</ref><ref type=""bibr"" target=""#b26"">Wang et al., 2017;</ref><ref type=""bibr"" target=""#b9"">Celikyilmaz et al., 2017)</ref> make interactions between a question a",0
"head><p>The decision of what previous nodes to connect to allows the model to form skip connections <ref type=""bibr"" target=""#b15"">(He et al., 2016a;</ref><ref type=""bibr"" target=""#b46"">Zoph &amp; Le,",0
"target=""#b4"">(Brock et al., 2018)</ref>, which designs an architecture and then uses a hypernetwork <ref type=""bibr"" target=""#b13"">(Ha et al., 2017)</ref> to generate its weight. Such usage of the hyp ce. This is because the hypernetwork generates weights for SMASH's child models via tensor products <ref type=""bibr"" target=""#b13"">(Ha et al., 2017)</ref>, which suffer from a low-rank restriction as",0
"2017)</ref>. Each architecture search is run for 310 epochs. We initialize ω with He initialization <ref type=""bibr"" target=""#b14"">(He et al., 2015)</ref>. We also apply an 2 weight decay of 10 −4 . W",0
"the simple transformations between nodes in the constructed recurrent cell with highway connections <ref type=""bibr"" target=""#b45"">(Zilly et al., 2017)</ref>. For instance, instead of having</p><formu </ref>) VD 66 75.2 LSTM <ref type=""bibr"" target=""#b19"">(Inan et al., 2017)</ref> VD, WT 51 68.5 RHN <ref type=""bibr"" target=""#b45"">(Zilly et al., 2017)</ref> VD, WT 24 66.0 LSTM <ref type=""bibr"" targe expressiveness, and ii) the depth of the recurrent cell, which learns more complex transformations <ref type=""bibr"" target=""#b45"">(Zilly et al., 2017)</ref>.</p></div> <div xmlns=""http://www.tei-c.or",0
"ate-of-the-art among Penn Treebank's approaches that do not utilize post-training processing (56.0; <ref type=""bibr"" target=""#b42"">Yang et al. (2018)</ref>). Importantly, in all of our experiments, fo , which is on par with the existing state-of-the-art of 56.0 achieved by Mixture of Softmaxes (MoS) <ref type=""bibr"" target=""#b42"">(Yang et al., 2018)</ref>. Note that we do not apply MoS to the ENAS LSTM <ref type=""bibr"" target=""#b31"">(Melis et al., 2017)</ref> Hyper-parameters Search 24 59.5 LSTM <ref type=""bibr"" target=""#b42"">(Yang et al., 2018)</ref> VD, WT, 2, AWD, MoC 22 57.6 LSTM <ref type= 22 57.6 LSTM <ref type=""bibr"" target=""#b32"">(Merity et al., 2017)</ref> VD, WT, 2, AWD 24 57.3 LSTM <ref type=""bibr"" target=""#b42"">(Yang et al., 2018)</ref> VD, WT,  Our ENAS cell, visualized in Figur r ENAS cell is an average of 6 nodes. This behavior is similar to that of Mixture of Contexts (MoC) <ref type=""bibr"" target=""#b42"">(Yang et al., 2018)</ref>. Not only does ENAS independently discover",0
"lack-box security <ref type=""bibr"" target=""#b27"">(Tramèr et al., 2018)</ref>. We include one paper, <ref type=""bibr"" target=""#b14"">Ma et al. (2018)</ref>, that was not proposed as a defense per se, bu /head><p>LID is a general-purpose metric that measures the distance from an input to its neighbors. <ref type=""bibr"" target=""#b14"">Ma et al. (2018)</ref> propose using LID to characterize properties o study made complete source code available <ref type=""bibr"" target=""#b15"">(Madry et al., 2018;</ref><ref type=""bibr"" target=""#b14"">Ma et al., 2018;</ref><ref type=""bibr"" target=""#b9"">Guo et al., 2018;",0
"optimization attacks: obfuscated gradients, a term we define as a special case of gradient masking <ref type=""bibr"" target=""#b17"">(Papernot et al., 2017)</ref>. Without a good gradient, where followi o cause gradient masking if it ""does not have useful gradients"" for generating adversarial examples <ref type=""bibr"" target=""#b17"">(Papernot et al., 2017)</ref>; gradient masking is known to be an inc then black-box attacks (which do not use the gradient) often perform better than white-box attacks <ref type=""bibr"" target=""#b17"">(Papernot et al., 2017)</ref>.</p><p>Unbounded attacks do not reach 1 et al., 2017)</ref>; gradient masking is known to be an incomplete defense to adversarial examples <ref type=""bibr"" target=""#b17"">(Papernot et al., 2017;</ref><ref type=""bibr"" target=""#b27"">Tramèr et",0
"TIMATOR</head><p>As a special case, we first discuss what amounts to the straight-through estimator <ref type=""bibr"" target=""#b1"">(Bengio et al., 2013)</ref> applied to constructing adversarial exampl",0
"e function. We compute gradients of randomized defenses by applying Expectation Over Transformation <ref type=""bibr"" target=""#b0"">(Athalye et al., 2017)</ref>. We solve vanishing/exploding gradients t that employ randomized transformations to the input, we apply Expectation over Transformation (EOT) <ref type=""bibr"" target=""#b0"">(Athalye et al., 2017)</ref> to correctly compute the gradient over th",0
"rg/ns/1.0""><head>III. RU-NET AND R2U-NET ARCHITECTURES</head><p>Inspired by the deep residual model <ref type=""bibr"" target=""#b6"">[7]</ref>, RCNN <ref type=""bibr"" target=""#b40"">[41]</ref>, and U-Net < type=""bibr"" target=""#b4"">[5]</ref>, GoogleNet <ref type=""bibr"" target=""#b5"">[6]</ref>, Residual Net <ref type=""bibr"" target=""#b6"">[7]</ref>, DenseNet <ref type=""bibr"" target=""#b7"">[8]</ref>, and Capsu",1
"TECTURES</head><p>Inspired by the deep residual model <ref type=""bibr"" target=""#b6"">[7]</ref>, RCNN <ref type=""bibr"" target=""#b40"">[41]</ref>, and U-Net <ref type=""bibr"" target=""#b11"">[12]</ref>, we p RCL) are performed with respect to the discrete time steps that are expressed according to the RCNN <ref type=""bibr"" target=""#b40"">[41]</ref>. Let's consider the 𝑥 𝑙 input sample in the 𝑙 𝑡ℎ layer of",1
"ants have already shown superior performance on object recognition tasks using different benchmarks <ref type=""bibr"" target=""#b41"">[42,</ref><ref type=""bibr"" target=""#b42"">43]</ref>. The recurrent res",1
"ion based segmentation method <ref type=""bibr"" target=""#b17"">[18]</ref>, and the graph-cut approach <ref type=""bibr"" target=""#b18"">[19]</ref>. However, semantic segmentation approaches that utilize DL",0
"etwork (FCN) also provides state-of-the-art results for image segmentation tasks in computer vision <ref type=""bibr"" target=""#b1"">[2]</ref>. Another variant of FCN was also proposed which is called Se based segmentation methods based on FCN provide superior performance for natural image segmentation <ref type=""bibr"" target=""#b1"">[2]</ref>. One of the image patch-based architectures is called Random -the-art performance for image classification <ref type=""bibr"" target=""#b0"">[1]</ref>, segmentation <ref type=""bibr"" target=""#b1"">[2]</ref>, detection and tracking <ref type=""bibr"" target=""#b2"">[3]</r",0
"testing phase and achieved 0.8616, which is around 1.26% better than recently proposed alternatives <ref type=""bibr"" target=""#b59"">[62]</ref>. Furthermore, the JSC and F1 scores are calculated and the",0
"color images, and each image has a size of 700×605 pixels <ref type=""bibr"" target=""#b44"">[45,</ref><ref type=""bibr"" target=""#b45"">46]</ref>. Due to the smaller number of samples, two approaches are a",0
"en proposed that have proved that deeper networks are better for recognition and segmentation tasks <ref type=""bibr"" target=""#b4"">[5]</ref>. However, training very deep models is difficult due to the </ref>. SegNet consists of two parts, one is the encoding network which is a 13-layer VGG16 network <ref type=""bibr"" target=""#b4"">[5]</ref>, and the corresponding decoding network uses pixel-wise clas twork (DCNN) models have been proposed such as AlexNet <ref type=""bibr"" target=""#b0"">[1]</ref>, VGG <ref type=""bibr"" target=""#b4"">[5]</ref>, GoogleNet <ref type=""bibr"" target=""#b5"">[6]</ref>, Residual modern activation functions such as Rectified Linear Units (ReLU) or Exponential Linear Units (ELU) <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b5"">6]</ref>. Another solution to t",0
">. The second dataset, STARE, contains 20 color images, and each image has a size of 700×605 pixels <ref type=""bibr"" target=""#b44"">[45,</ref><ref type=""bibr"" target=""#b45"">46]</ref>. Due to the smalle",0
"type=""bibr"" target=""#b1"">[2]</ref>. Another variant of FCN was also proposed which is called SegNet <ref type=""bibr"" target=""#b9"">[10]</ref>. manual segmentation approaches, there is a significant dem ion of this paper is the way in which the decoder upsamples its lower resolution input feature maps <ref type=""bibr"" target=""#b9"">[10]</ref>. Later, an improved version of SegNet, which is called Baye . This approach is also generalizable, as it easily be applied deep learning models based on SegNet <ref type=""bibr"" target=""#b9"">[10]</ref>, 3D-UNet <ref type=""bibr"" target=""#b12"">[13]</ref>, and V-N",0
"arget=""#b0"">[1]</ref>, segmentation <ref type=""bibr"" target=""#b1"">[2]</ref>, detection and tracking <ref type=""bibr"" target=""#b2"">[3]</ref>, and captioning <ref type=""bibr"" target=""#b3"">[4]</ref>. Sin",0
"iomedical image segmentation tasks have been empirically evaluated with U-Net and residual networks <ref type=""bibr"" target=""#b32"">[33]</ref>. A deep contour-aware network called Deep Contour-Aware Ne",0
"are control flow checking is to partition the program into basic blocks (branch-free parts of code) <ref type=""bibr"" target=""#b14"">[14]</ref>. For each block a deterministic signature is calculated an > and On-line control flow error detection using relationship signatures among basic blocks (RSCFC) <ref type=""bibr"" target=""#b14"">[14]</ref>.</p><p>ECCA, firstly, assigns a unique prime number identi rget=""#b15"">[15]</ref> technique to the original code,  a safe one, obtained by applying the RSCFC <ref type=""bibr"" target=""#b14"">[14]</ref> technique to the original code,  a safe one, obtained by",1
"Table <ref type=""table"" target=""#tab_0"">2</ref> is calculated according to the equation 2 and 3 of <ref type=""bibr"" target=""#b17"">[17]</ref>. Table <ref type=""table"" target=""#tab_0"">2</ref> shows tha",0
", some are due to poor software development practices and lack of software security in applications <ref type=""bibr"" target=""#b1"">[2]</ref>. In this context, the integrity of system software and appli",0
"ches or memories, or glitches in combinational logics that can propagate and be captured in latches <ref type=""bibr"" target=""#b8"">[9]</ref>. If not handled properly, such errors can cause illegal acce",0
"attestation. The cores of trusted computing technology are trusted computing base and trusted chain <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b4"">5]</ref>, and trusted measureme",0
"eturn-into-libc attack by allowing the attacker arbitrary computation without calling any functions <ref type=""bibr"" target=""#b7"">[8]</ref>. In a traditional return-into libc attack, an attacker could",0
"tle as possible. We will elaborate.</p><p>There are previous works that achieve the first condition <ref type=""bibr"" target=""#b10"">(Cisse et al., 2017;</ref><ref type=""bibr"" target=""#b17"">Hein &amp; A ef><ref type=""bibr"" target=""#b16"">Haber &amp; Ruthotto, 2017)</ref>. For example, Parseval networks <ref type=""bibr"" target=""#b10"">(Cisse et al., 2017)</ref> bound the Lipschitz constant by requiring leads to degradation in nominal accuracy, average confidence gap and robustness. Parseval networks <ref type=""bibr"" target=""#b10"">(Cisse et al., 2017)</ref> can be viewed as models without L c term, weight matrices and shows its effect in reducing generalization gap. The work on Parseval networks <ref type=""bibr"" target=""#b10"">(Cisse et al., 2017)</ref> shows that it is possible to control Lipsc ing the first condition, allows greater degrees of freedom in parameter training than the scheme in <ref type=""bibr"" target=""#b10"">Cisse et al. (2017)</ref>; a new loss function is specially designed |M i,j | (2)</formula><p>The above is where our linear and convolution layers differ from those in <ref type=""bibr"" target=""#b10"">Cisse et al. (2017)</ref>: they require W W T to be an identity matri s to be 1; they also propose to restrict aggregation operations. The reported robustness results of <ref type=""bibr"" target=""#b10"">Cisse et al. (2017)</ref>, however, are much weaker than those by adv common parameter with (9).</p><p>ResNet-like reconvergence is referred to as aggregation layers in <ref type=""bibr"" target=""#b10"">Cisse et al. (2017)</ref> and a different formula was used:</p><formu abel></formula><p>) where α ∈ [0, 1] is a trainable parameter. Because splitting is not modified in <ref type=""bibr"" target=""#b10"">Cisse et al. (2017)</ref>, their scheme may seem approximately equiva preserve distances. In contrast, because splitting is not modified, at reconvergence the scheme of <ref type=""bibr"" target=""#b10"">Cisse et al. (2017)</ref> must apply the shrinking factor of 1 − α on ents. We can also have a different t per channel or even per entry.</p><p>To be fair, the scheme of <ref type=""bibr"" target=""#b10"">Cisse et al. (2017)</ref> has an advantage of being nonexpansive with",1
"ibr"" target=""#b2"">Athalye et al. (2018)</ref> reported that out of eight recent defense works, only <ref type=""bibr"" target=""#b21"">Madry et al. (2017)</ref> survived strong attacks. So far the mainstr </ref>) is one that combines with adversarial training. Our implementation applies the technique of <ref type=""bibr"" target=""#b21"">Madry et al. (2017)</ref> on the first loss term (4): we use distorte es robustness of L2NNN classifiers for MNIST and CIFAR-10 and compares against the state of the art <ref type=""bibr"" target=""#b21"">Madry et al. (2017)</ref>. The robustness metric is accuracy under wh amèr et al., 2017;</ref><ref type=""bibr"" target=""#b33"">Zantedeschi et al., 2017)</ref>. The work of <ref type=""bibr"" target=""#b21"">Madry et al. (2017)</ref> has the best results to date and effectivel et=""#b10"">Cisse et al. (2017)</ref>, however, are much weaker than those by adversarial training in <ref type=""bibr"" target=""#b21"">Madry et al. (2017)</ref>. We differ from Parseval networks in a numb "">5</ref>, we use two network architectures. The first has 4 layers and is the architecture used in <ref type=""bibr"" target=""#b21"">Madry et al. (2017)</ref>. The second has 22 layers and is the archit cates 10 runs each with 1000 iterations. Model 1 is an ordinarily network. Model 2 is the model from<ref type=""bibr"" target=""#b21"">Madry et al. (2017)</ref>. Model 3 is L2NNN without adversarial train #b24"">Raghunathan et al. (2018);</ref><ref type=""bibr"" target=""#b19"">Kolter &amp; Wong (2017)</ref>;<ref type=""bibr"" target=""#b21"">Madry et al. (2017)</ref>. Our MNIST L ∞ results are on par with<ref et al. (2018);</ref><ref type=""bibr"" target=""#b19"">Kolter &amp; Wong (2017)</ref> but not as good as<ref type=""bibr"" target=""#b21"">Madry et al. (2017)</ref>. Our CIFAR-10 Model 4 is on par with<ref ty good as<ref type=""bibr"" target=""#b21"">Madry et al. (2017)</ref>. Our CIFAR-10 Model 4 is on par with<ref type=""bibr"" target=""#b21"">Madry et al. (2017)</ref> for L ∞ defense.</figDesc><table><row><cell in weaker L2 robustness than the L2 robustness results from training with L∞-bounded adversaries in<ref type=""bibr"" target=""#b21"">Madry et al. (2017)</ref>. Therefore we choose to compare against the ed strong attacks. So far the mainstream and most successful remedy is that of adversarial training <ref type=""bibr"" target=""#b21"">(Madry et al., 2017)</ref>. However, as will be shown in Tables <ref nd CIFAR-10 classifiers, without needing any adversarial training, that exceed the state of the art <ref type=""bibr"" target=""#b21"">(Madry et al., 2017)</ref> in robustness against white-box L 2bounded ach bin in Figure <ref type=""figure"" target=""#fig_1"">2</ref>. We repeat this experiment for Model 2 <ref type=""bibr"" target=""#b21"">(Madry et al., 2017)</ref> and our Model 3 of Tables <ref type=""table >1</ref>. The first image is the original image of a zero. The second image is an attack on Model 2 <ref type=""bibr"" target=""#b21"">(Madry et al., 2017)</ref> found after 1K iterations, with noise L 2 e models (Model 2's in Tables1 and 2) were built by adversarial training with L∞-bounded adversaries<ref type=""bibr"" target=""#b21"">(Madry et al., 2017)</ref>. To the best of our knowledge,<ref type=""b",1
"e passing this pooling layer.</p><p>We replace max-pooling with norm-pooling, which was reported in <ref type=""bibr"" target=""#b4"">Boureau et al. (2010)</ref> to occasionally increase accuracy. Instead",0
"captioning <ref type=""bibr"" target=""#b9"">(Chen et al., 2017)</ref> and natural language processing <ref type=""bibr"" target=""#b13"">(Gao et al., 2018;</ref><ref type=""bibr"" target=""#b12"">Ebrahimi et al",0
"vely. Similar trade-offs have been reported by other robustness works including adversarial training<ref type=""bibr"" target=""#b30"">(Tsipras et al., 2019)</ref> and adversarial polytope<ref type=""bibr"" adversaries<ref type=""bibr"" target=""#b21"">(Madry et al., 2017)</ref>. To the best of our knowledge,<ref type=""bibr"" target=""#b30"">Tsipras et al. (2019)</ref> from the same lab is the only paper in th",0
"raining alone may also increase the minimum distortion limit for misclassification, as suggested in <ref type=""bibr"" target=""#b8"">Carlini et al. (2017)</ref> for a small network, that limit likely doe r to our work, it is the only work that achieves sizable white-box defense. It has been reported in <ref type=""bibr"" target=""#b8"">Carlini et al. (2017)</ref> that, for a small network, adversarial tra",0
"1"">Alzantot et al., 2018;</ref><ref type=""bibr"">Carlini &amp; Wagner, 2018)</ref>, image captioning <ref type=""bibr"" target=""#b9"">(Chen et al., 2017)</ref> and natural language processing <ref type=""b",0
"the output of a pitch tracker to generate reference annotations for melody and multi-f0 estimation <ref type=""bibr"" target=""#b18"">[18,</ref><ref type=""bibr"" target=""#b19"">19]</ref>.</p><p>In this pap n of 230 monophonic stems taken from MedleyDB and re-synthesized using the methodology presented in <ref type=""bibr"" target=""#b18"">[18]</ref>, which uses an analysis/synthesis approach to generate a s",1
"ies, the state of the art is achieved by YIN-based methods <ref type=""bibr"" target=""#b14"">[14,</ref><ref type=""bibr"" target=""#b15"">15]</ref>, with pYIN being the best performing method to date <ref ty",0
"ic information retrieval like chord ID <ref type=""bibr"" target=""#b16"">[16]</ref> and beat detection <ref type=""bibr"" target=""#b17"">[17]</ref>, where data-driven methods have been shown to consistently",0
"onophonic pitch tracking is used as a method to generate pitch annotations for multi-track datasets <ref type=""bibr"" target=""#b1"">[1]</ref> or as a core component of melody extraction systems <ref typ ompared algorithms is already very high. In light of this, we cannot use a dataset such as MedleyDB <ref type=""bibr"" target=""#b1"">[1]</ref>, since its annotation process includes manual corrections wh",0
"1]</ref> or as a core component of melody extraction systems <ref type=""bibr"" target=""#b2"">[2,</ref><ref type=""bibr"" target=""#b3"">3]</ref>. Pitch estimation is also important for speech analysis, wher",0
"tures across languages which can be mapped to the same space <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b10"">11]</ref>. Authors in <ref type=""bibr"" target=""#b11"">[12]</ref> looke",1
"using cross-entropy <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b13"">14]</ref>. This architecture can also be used as a ""bottleneck"" featu",1
"oustic model with a ""bottleneck"" layer using a frame based criterion on a large multilingual corpus <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target ilingual models can be adapted to the specific language to improve performance further. The work by <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b15"">16]</ref> presented bottleneck ific softmax layers, which are trained using cross-entropy <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b13"">14]</ref>. This architecture ca ) if X ∈ XL1 softmax(WL2e + bL2) if X ∈ XL2 . . . softmax(WLne + bLn) if X ∈ XLn</formula><p>Unlike <ref type=""bibr"" target=""#b4"">[5]</ref>, we do not have any bottleneck layer, and the whole model is",1
"nd to be significantly easier to train than those that have been trained using hidden Markov models <ref type=""bibr"" target=""#b20"">[20,</ref><ref type=""bibr"" target=""#b21"">21]</ref>. <ref type=""bibr"" ""><head>P (z|X) =</head><p>p∈CTC Path(z)</p><formula xml:id=""formula_0"">P (p|X)</formula><p>Like in <ref type=""bibr"" target=""#b20"">[20]</ref> we use this loss along with stacked Bidirectional LSTM lay t"" n=""2"" xml:id=""foot_1"">The code to train the multi-lingual model will be released as part of EESEN<ref type=""bibr"" target=""#b20"">[20]</ref>.</note> 		</body> 		<back> 			<div type=""references"">",0
"e-art speech recognition systems with human-like performance <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref> are trained on hundreds of hours of well-annotated speech. Si",0
"en Markov models <ref type=""bibr"" target=""#b20"">[20,</ref><ref type=""bibr"" target=""#b21"">21]</ref>. <ref type=""bibr"" target=""#b22"">[22]</ref> shows that multi-lingual CTC systems with shared phones ca",0
"rticulatory features <ref type=""bibr"" target=""#b7"">[8]</ref> to train HMM based systems. Authors in <ref type=""bibr"" target=""#b8"">[9]</ref> used subspace Gaussian mixture model to map phonemes of diff",0
"e space <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b10"">11]</ref>. Authors in <ref type=""bibr"" target=""#b11"">[12]</ref> looked at unsupervised pretraining on different languages",0
"and gave competitive results when compared to systems with mono-lingual features. Other approaches <ref type=""bibr"" target=""#b17"">[17,</ref><ref type=""bibr"" target=""#b18"">18]</ref> constructed a shar",0
"using a frame based criterion on a large multilingual corpus <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b6"">7]</ref>. The network up to the",0
"ing separated into multiple language-specific softmax layers, which are trained using cross-entropy <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" targ",0
"unts of training data may not be available, cannot be transcribed, or are otherwise hard to come by <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b3"">4]</ref>.</p><p>The standard ap s of the models shifted to learning features across languages which can be mapped to the same space <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b10"">11]</ref>. Authors in <ref typ",0
"f> used subspace Gaussian mixture model to map phonemes of different languages together. Authors in <ref type=""bibr"" target=""#b9"">[10]</ref> introduce the use of a shared phone set to build HMM based",0
"atures by sharing hidden layers across languages.</p><p>Connectionist Temporal Classification (CTC, <ref type=""bibr"" target=""#b19"">[19]</ref>) lends itself to low-resource multi-lingual experiments, b",0
"ed to systems with mono-lingual features. Other approaches <ref type=""bibr"" target=""#b17"">[17,</ref><ref type=""bibr"" target=""#b18"">18]</ref> constructed a shared language independent phone set, which",0
"DIP) <ref type=""bibr"" target=""#b14"">[15]</ref> and extends it with unified prefetching into the BTB <ref type=""bibr"" target=""#b12"">[13]</ref>. The scheme, called Boomerang, discovers BTB misses on the k has addressed this limitation by adding a BTB prefetch capability in a technique called Boomerang <ref type=""bibr"" target=""#b12"">[13]</ref>. Boomerang uses a basic-block-oriented BTB to detect BTB m guide local control flow tend to have very short displacements, typically within a few cache blocks <ref type=""bibr"" target=""#b12"">[13]</ref>, as shown by dashed arrows in Figure <ref type=""figure"" ta filling the BTBs, Shotgun takes a hybrid approach by incorporating the features from both Boomerang <ref type=""bibr"" target=""#b12"">[13]</ref> and Confluence <ref type=""bibr"" target=""#b9"">[10]</ref>. S depending on branch type. The rest of the predecoded branches are stored in the BTB Prefetch Buffer <ref type=""bibr"" target=""#b12"">[13]</ref>. On a hit to the BTB Prefetch Buffer, the accessed branch d the associated system software support, making it an expensive proposition as shown in prior work <ref type=""bibr"" target=""#b12"">[13]</ref>. The LLC tag array extension, for storing index table, cos dditional coverage. Confluence performs poorly on these applications, as also noted by Kumar et al. <ref type=""bibr"" target=""#b12"">[13]</ref>, owing to frequent LLC accesses for loading history metada",1
"arget=""#b5"">6,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b16"">17]</ref> and BTB <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""b",0
""">12,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b16"">17]</ref> and BTB <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b2"">3]</ref> prefetchers over the y",0
"chitecture researchers have proposed a number of instruction <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" targe combat the front-end bottleneck in servers. State-of-the-art prefetchers rely on temporal streaming <ref type=""bibr"" target=""#b5"">[6]</ref> to record and replay instruction cache or BTB access streams 1.0""><head n=""2.1"">Temporal streaming prefetching</head><p>Over the past decade, temporal streaming <ref type=""bibr"" target=""#b5"">[6]</ref> has been the dominant technique for front-end prefetching fo",0
"4"">15,</ref><ref type=""bibr"" target=""#b16"">17]</ref> and BTB <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b2"">3]</ref> prefetchers over the years to combat the front-end bottleneck concept has been applied to both instruction cache <ref type=""bibr"" target=""#b4"">[5]</ref> and BTB <ref type=""bibr"" target=""#b2"">[3]</ref> prefetching, and shown to be highly effective in eliminating store large amounts of metadata (hundreds of kilobytes per core) for capturing control flow history <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b4"">5]</ref>. To mitigate the cost,",0
". This observation has led to a flurry of ASIC proposals for DNN accelerators over the recent years <ref type=""bibr"" target=""#b2"">[3]</ref><ref type=""bibr"" target=""#b3"">[4]</ref><ref type=""bibr"" targe 3]</ref> have proposed meshes due to their flexibility to support all-to-all communication. Diannao <ref type=""bibr"" target=""#b2"">[3]</ref> and Shidiannao <ref type=""bibr"" target=""#b43"">[44]</ref> rel accelerator domain with an example of convolution accelerator for image processing domain. Diannao <ref type=""bibr"" target=""#b2"">[3]</ref>, DaDiannao <ref type=""bibr"" target=""#b14"">[15]</ref>, ShiDia ary trees are well-suited for performing reductions and have been used in prior DNN implementations <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target",1
"This is the approach most of the early DNN accelerators took <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b14"">15]</ref>. However, recently t almost every DNN accelerator has used a hierarchy of buses <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b33"">34]</ref> and/or trees <ref t",1
"s the idea of sparsity <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b19"">20]</ref> and has been popula target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b20"">21]</ref> to implement adder a recent accelerator for sparse CNNs, that leverages sparsity in activations and weights. Cnvlutin <ref type=""bibr"" target=""#b18"">[19]</ref> compresses activation values based on the ReLU operator. C",0
"5"">[6]</ref> uses separate buses for its scatters and gathers. CNN accelerators: Convolution Engine <ref type=""bibr"" target=""#b44"">[45]</ref> explored the trade-off between flexibility and efficiency",0
"and classification problems in image <ref type=""bibr"" target=""#b0"">[1]</ref> and speech recognition <ref type=""bibr"" target=""#b1"">[2]</ref> with accuracies surpassing those of humans.</p><p>The microa",0
"arget=""#b57"">[58]</ref> which has been implemented on FPGA <ref type=""bibr"" target=""#b58"">[59,</ref><ref type=""bibr"" target=""#b59"">60]</ref>. An ASIC implementation for accelerating the control of RNN",0
"=""#b26"">[27,</ref><ref type=""bibr"" target=""#b49"">[50]</ref><ref type=""bibr"" target=""#b50"">[51]</ref><ref type=""bibr"" target=""#b51"">[52]</ref>, hardware design of RNNs was not as active as that of CNNs",0
"assic topology for providing non-blocking bandwidth, and is used extensively in datacenter networks <ref type=""bibr"" target=""#b37"">[38]</ref>. However, such a topology is infeasible to build on-chip s",0
"over the recent years <ref type=""bibr"" target=""#b2"">[3]</ref><ref type=""bibr"" target=""#b3"">[4]</ref><ref type=""bibr"" target=""#b4"">[5]</ref><ref type=""bibr"" target=""#b5"">[6]</ref><ref type=""bibr"" targe",0
"some kind of transformation of this multidimensional loop <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b21"">22]</ref>. We propose to design DNN accelerators as a collection of m",0
"ns and argue their shortcomings when used in real systems.</p><p>OS-level page coloring Lin's study <ref type=""bibr"" target=""#b26"">[29]</ref> aims to offer cache partitioning by using OS-level page co",1
"andwidth reservation but may lead to the underutilized network (not work-conserving). Elasticswitch <ref type=""bibr"" target=""#b31"">[34]</ref>, EyeQ <ref type=""bibr"" target=""#b22"">[25]</ref> and Sliver",0
"propose to mitigate the performance interference caused by the shared LLC which dCat targets. CPI2 <ref type=""bibr"" target=""#b42"">[45]</ref> is a technique for semi-black-box performance isolation th",0
"-the ratio of core working set size (CWSS) to working set size (WSS) of these two workloads is high <ref type=""bibr"" target=""#b13"">[16]</ref>. It means they have high reuse of the data in the working",0
"et=""#b19"">[22,</ref><ref type=""bibr"" target=""#b20"">23,</ref><ref type=""bibr"" target=""#b24"">27,</ref><ref type=""bibr"" target=""#b33"">36,</ref><ref type=""bibr"" target=""#b34"">37,</ref><ref type=""bibr"" tar",0
"=""#b6"">[7]</ref>, <ref type=""bibr"" target=""#b7"">[8]</ref>, <ref type=""bibr"" target=""#b8"">[9]</ref>, <ref type=""bibr"" target=""#b9"">[10]</ref>, <ref type=""bibr"" target=""#b10"">[11]</ref>, <ref type=""bibr f type=""figure"" target=""#fig_0"">1</ref> shows the opportunity of temporal prefetching and what STMS <ref type=""bibr"" target=""#b9"">[10]</ref> and ISB <ref type=""bibr"" target=""#b12"">[13]</ref>, two stat e addresses that follow the missed address in the  history <ref type=""bibr"" target=""#b5"">[6]</ref>, <ref type=""bibr"" target=""#b9"">[10]</ref>. As it is evident from Figure <ref type=""figure"" target=""#f how that temporal prefetchers that rely on just one miss address to look up the history (e.g., STMS <ref type=""bibr"" target=""#b9"">[10]</ref>), frequently prefetch incorrectly.</p><p>Another interestin storage for STMS to have a reasonable coverage, both tables are placed off chip in the main memory <ref type=""bibr"" target=""#b9"">[10]</ref>. Consequently, every access to these tables (read or update data.</p><p>Figure <ref type=""figure"" target=""#fig_5"">6</ref> shows the timing of events with STMS <ref type=""bibr"" target=""#b9"">[10]</ref>. Upon a cache miss, a request is sent to the main memory to n offers the level of performance similar to that of the non-practical always-update implementation <ref type=""bibr"" target=""#b9"">[10]</ref>.</p><p>As both metadata tables are off-chip, the on-chip st ata misses. As the size of these two tables is very large (several megabytes), just like prior work <ref type=""bibr"" target=""#b9"">[10]</ref>, both tables are stored in the main memory.</p><p>Domino pr . The delay of the search is tolerable because it is considerably smaller than the off-chip latency <ref type=""bibr"" target=""#b9"">[10]</ref>. In case a match is not found, nothing will be done, and ot ementation <ref type=""bibr"" target=""#b12"">[13]</ref>.</p><p>Sampled Temporal Memory Streaming. STMS <ref type=""bibr"" target=""#b9"">[10]</ref> records miss sequences in a global per-core HT and locates al per-core HT and locates streams through an IT. It benefits from a stream-end detection heuristic <ref type=""bibr"" target=""#b9"">[10]</ref>, <ref type=""bibr"" target=""#b39"">[40]</ref> to reduce useles ISB <ref type=""bibr"" target=""#b12"">[13]</ref>, VLDP <ref type=""bibr"" target=""#b33"">[34]</ref>, STMS <ref type=""bibr"" target=""#b9"">[10]</ref>, and Digram <ref type=""bibr"" target=""#b20"">[21]</ref>. As a",1
"prefetching are considered in many pieces of recent work <ref type=""bibr"" target=""#b59"">[60]</ref>, <ref type=""bibr"" target=""#b60"">[61]</ref>, <ref type=""bibr"" target=""#b61"">[62]</ref>, <ref type=""bib",0
"of temporal prefetching has been implemented in IBM Blue Gene/Q, where it is named List Prefetching <ref type=""bibr"" target=""#b23"">[24]</ref>.</p><p>Temporal prefetching is suitable for accelerating c",0
""">[60]</ref>, <ref type=""bibr"" target=""#b60"">[61]</ref>, <ref type=""bibr"" target=""#b61"">[62]</ref>, <ref type=""bibr"" target=""#b62"">[63]</ref>, <ref type=""bibr"" target=""#b63"">[64]</ref>, <ref type=""bib",0
""">[53]</ref>, <ref type=""bibr"" target=""#b53"">[54]</ref>, <ref type=""bibr"" target=""#b54"">[55]</ref>, <ref type=""bibr"" target=""#b55"">[56]</ref>, <ref type=""bibr"" target=""#b56"">[57]</ref>, <ref type=""bib",0
"nd may refer to graphlets or orbits (graphlet automorphisms) <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b5"">6]</ref>. The HONE framework expresses a general family of embedding m id search over K ? {1, 2, 3, 4} using 10% of the labeled data. We use all 2-4 node connected orbits <ref type=""bibr"" target=""#b5"">[6]</ref> and set D ? = 16 for the local motif embeddings. All methods",1
"ifs. The term motif is used generally and may refer to graphlets or orbits (graphlet automorphisms) <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b5"">6]</ref>. The HONE framework ex",1
"xmlns=""http://www.tei-c.org/ns/1.0""><head n=""1"">INTRODUCTION</head><p>Roles represent node (or edge <ref type=""bibr"" target=""#b1"">[2]</ref>) connectivity patterns such as hubs, star-centers, star-edge",0
"e labeled data. Experiments are repeated for 10 random seed initializations. Data was obtained from <ref type=""bibr"" target=""#b6"">[7]</ref>.</p><p>We evaluate the HONE variants for link prediction. Gi",0
"<ref type=""bibr"" target=""#b3"">[4]</ref>, DeepWalk <ref type=""bibr"" target=""#b4"">[5]</ref>, and LINE <ref type=""bibr"" target=""#b8"">[9]</ref> correspond to those mentioned in <ref type=""bibr"" target=""#b",0
"gions of the graph. Intuitively, two nodes belong to the same role if they are structurally similar <ref type=""bibr"" target=""#b7"">[8]</ref>. Many network representation learning methods (including ran r proximity (e.g., near one another in the graph). However, such methods are insufficient for roles <ref type=""bibr"" target=""#b7"">[8]</ref> as they fail to capture the higher-order connectivity patter",0
"z i ? R D . For node2vec, we perform a grid search over p, q ? {0.25, 0.5, 1, 2, 4} as mentioned in <ref type=""bibr"" target=""#b3"">[4]</ref>. All other hyperparameters for node2vec <ref type=""bibr"" tar 4} as mentioned in <ref type=""bibr"" target=""#b3"">[4]</ref>. All other hyperparameters for node2vec <ref type=""bibr"" target=""#b3"">[4]</ref>, DeepWalk <ref type=""bibr"" target=""#b4"">[5]</ref>, and LINE =""#b4"">[5]</ref>, and LINE <ref type=""bibr"" target=""#b8"">[9]</ref> correspond to those mentioned in <ref type=""bibr"" target=""#b3"">[4]</ref>. In contrast, the HONE variants have only one hyperparameter",0
"[4]</ref>. All other hyperparameters for node2vec <ref type=""bibr"" target=""#b3"">[4]</ref>, DeepWalk <ref type=""bibr"" target=""#b4"">[5]</ref>, and LINE <ref type=""bibr"" target=""#b8"">[9]</ref> correspond",0
"d new items. State-of-art reinforcement learning methods usually apply the simple ϵ-greedy strategy <ref type=""bibr"" target=""#b29"">[31]</ref> or Upper Confidence Bound (UCB) <ref type=""bibr"" target=""# dynamic nature of news characteristics and user preference, we propose to use Deep Q-Learning (DQN) <ref type=""bibr"" target=""#b29"">[31]</ref> framework. This framework can consider current reward and avoid the harm to recommendation accuracy induced by classical exploration strategies like ϵ-greedy <ref type=""bibr"" target=""#b29"">[31]</ref> and Upper Confidence Bound <ref type=""bibr"" target=""#b21""> ss stored in the memory to update the network Q.</p><p>Here, we use the experience replay technique <ref type=""bibr"" target=""#b29"">[31]</ref> to update the network. Specifically, agent G maintains a m ture of news recommendation and the need to estimate future reward, we apply a Deep Q-Network (DQN) <ref type=""bibr"" target=""#b29"">[31]</ref> to model the probability that one user may click on one sp ead><p>The most straightforward strategies to do exploration in reinforcement learning are ϵ-greedy <ref type=""bibr"" target=""#b29"">[31]</ref> and UCB <ref type=""bibr"" target=""#b21"">[23]</ref>. ϵ-greed",1
"News recommendation algorithms</head><p>Recommender systems <ref type=""bibr"" target=""#b1"">[3,</ref><ref type=""bibr"" target=""#b2"">4]</ref> have been investigated extensively because of its direct conn",0
"to improve recommendation performance in the long run.</p><p>Second, current recommendation methods <ref type=""bibr"" target=""#b21"">[23,</ref><ref type=""bibr"" target=""#b33"">35,</ref><ref type=""bibr"" ta simple ϵ-greedy strategy <ref type=""bibr"" target=""#b29"">[31]</ref> or Upper Confidence Bound (UCB) <ref type=""bibr"" target=""#b21"">[23,</ref><ref type=""bibr"" target=""#b41"">43]</ref> (mainly for Multi- cement learning in recommendation either do not model the future reward explicitly (MAB-based works <ref type=""bibr"" target=""#b21"">[23,</ref><ref type=""bibr"" target=""#b41"">43]</ref>), or use discrete odels. A group of work <ref type=""bibr"" target=""#b3"">[5,</ref><ref type=""bibr"" target=""#b5"">7,</ref><ref type=""bibr"" target=""#b21"">23,</ref><ref type=""bibr"" target=""#b38"">40,</ref><ref type=""bibr"" tar mmendation has attracted a lot of attention in recent years <ref type=""bibr"" target=""#b9"">[11,</ref><ref type=""bibr"" target=""#b21"">23,</ref><ref type=""bibr"" target=""#b43"">45]</ref>. The current method e MAB group of methods <ref type=""bibr"" target=""#b3"">[5,</ref><ref type=""bibr"" target=""#b5"">7,</ref><ref type=""bibr"" target=""#b21"">23,</ref><ref type=""bibr"" target=""#b38"">40,</ref><ref type=""bibr"" tar s a Contextual Multi-Armed Bandit (MAB) problem, where the context contains user and item features. <ref type=""bibr"" target=""#b21"">[23]</ref> assumes the expected reward is a linear function of the co ation strategies like ϵ-greedy <ref type=""bibr"" target=""#b29"">[31]</ref> and Upper Confidence Bound <ref type=""bibr"" target=""#b21"">[23]</ref>.</p><p>Our method is significantly different from the MAB xploration in reinforcement learning are ϵ-greedy <ref type=""bibr"" target=""#b29"">[31]</ref> and UCB <ref type=""bibr"" target=""#b21"">[23]</ref>. ϵ-greedy will randomly recommend new items with a probabi (through a deep neural network embedding of the raw features) to predict the click label. • LinUCB <ref type=""bibr"" target=""#b21"">[23]</ref>. Linear Upper Confidence Bound <ref type=""bibr"" target=""#b the click label. • LinUCB <ref type=""bibr"" target=""#b21"">[23]</ref>. Linear Upper Confidence Bound <ref type=""bibr"" target=""#b21"">[23]</ref> can select an arm (i.e., recommend a piece of news) accord e set of parameters for different news, which actually performs better than the original setting in <ref type=""bibr"" target=""#b21"">[23]</ref> on our dataset.(An improved version of the original LinUCB",0
"r instance, one of the most popular online services, news aggregation services, such as Google News <ref type=""bibr"" target=""#b13"">[15]</ref> can provide overwhelming volume of content than the amount",0
"get=""#b20"">22,</ref><ref type=""bibr"" target=""#b31"">33]</ref>, collaborative filtering based methods <ref type=""bibr"" target=""#b9"">[11,</ref><ref type=""bibr"" target=""#b26"">28,</ref><ref type=""bibr"" tar s necessary to update the model periodically. Although there are some online recommendation methods <ref type=""bibr"" target=""#b9"">[11,</ref><ref type=""bibr"" target=""#b22"">24]</ref> that can capture th "">METHOD</head><p>Personalized news recommendation has attracted a lot of attention in recent years <ref type=""bibr"" target=""#b9"">[11,</ref><ref type=""bibr"" target=""#b21"">23,</ref><ref type=""bibr"" tar get=""#b20"">22,</ref><ref type=""bibr"" target=""#b31"">33]</ref>, collaborative filtering based methods <ref type=""bibr"" target=""#b9"">[11,</ref><ref type=""bibr"" target=""#b26"">28,</ref><ref type=""bibr"" tar will select news that is more similar to user profile. In contrast, collaborative filtering methods <ref type=""bibr"" target=""#b9"">[11]</ref> usually make rating prediction utilizing the past ratings o "" target=""#b26"">[28,</ref><ref type=""bibr"" target=""#b32"">34]</ref>, or the combination of these two <ref type=""bibr"" target=""#b9"">[11]</ref>. To combine the advantages of the former two groups of meth",0
"The field that gathers all these questions under a common umbrella is graph signal processing (GSP) <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b2"">[3]</ref>.</p><p>While the pr of the prior work that is more directly connected and in the spirit of signal processing on graphs, <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b2"">[3]</ref>. We organize the di /ref>. We organize the discussion along two main lines; some parts of the exposition follow closely <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b44"">[45]</ref>.</p><p>1) From al te the graph signal model for signals indexed by nodes of an arbitrary directed or undirected graph <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b50"">[51]</ref>. This choice is s /ref> studies time signals. Graph signal processing (GSP)<ref type=""foot"" target=""#foot_0"">1</ref>  <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b2"">[3]</ref>, <ref type=""bibr"" t erpretation of DSP can be extended to develop a linear time shift invariant Graph Signal Processing <ref type=""bibr"" target=""#b1"">[2]</ref>. Consider now a graph signal s ∈ C N , where the entries of lized Laplacian L = D −1/2 LD −1/2 .</formula><p>The adjacency matrix A can be adopted as the shift <ref type=""bibr"" target=""#b1"">[2]</ref> for this general graph. Other choices have been proposed, in nt if it commutes with the shift,</p><formula xml:id=""formula_18"">AH = HA.</formula><p>As proven in <ref type=""bibr"" target=""#b1"">[2]</ref>, if the characteristic polynomial p A (z) and the minimum po aph filtering to two graph Fourier transforms and a pointwise multiplication in the spectral domain <ref type=""bibr"" target=""#b1"">[2]</ref>. With a notion of frequency we can now consider the GSP equi Section II-C). If these conditions do not hold, the Jordan canonical form is used to obtain the GFT <ref type=""bibr"" target=""#b1"">[2]</ref>, but this is well known to be a numerically unstable procedu",1
"roposed solutions for different aspects of the problem <ref type=""bibr"" target=""#b119"">[120]</ref>, <ref type=""bibr"" target=""#b120"">[121]</ref>, <ref type=""bibr"" target=""#b121"">[122]</ref>. In particu arget=""#b121"">[122]</ref>. In particular, sampling results have been generalized to directed graphs <ref type=""bibr"" target=""#b120"">[121]</ref>, <ref type=""bibr"" target=""#b121"">[122]</ref> and to othe ng set selection from an experiment design perspective <ref type=""bibr"" target=""#b123"">[124]</ref>, <ref type=""bibr"" target=""#b120"">[121]</ref>, <ref type=""bibr"" target=""#b121"">[122]</ref> setting as ge-scale graphs. Some techniques require computing and storing the first K basis vectors of the GFT <ref type=""bibr"" target=""#b120"">[121]</ref>. For larger graph sizes, where this may not be practical always lead to performance comparable to those of more complex greedy optimization methods such as <ref type=""bibr"" target=""#b120"">[121]</ref>, <ref type=""bibr"" target=""#b121"">[122]</ref>.</p><p>Give",1
"exed by nodes of an arbitrary directed or undirected graph <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b50"">[51]</ref>. This choice is satisfying in the sense that, when the sig • 1 is norm 1, and A norm = 1 λmax A. Other norms could be used to define the total variation, see <ref type=""bibr"" target=""#b50"">[51]</ref> <ref type=""bibr"" target=""#b2"">[3]</ref>. Using this, graph ustified theoretically that the frequency bases obtained from the shift operator tend to be ordered <ref type=""bibr"" target=""#b50"">[51]</ref>.</p><p>Up to this point, we have focused primarily on freq odel makes it possible to detect outliers or abnormal values by highpass filtering and thresholding <ref type=""bibr"" target=""#b50"">[51]</ref>, <ref type=""bibr"" target=""#b154"">[155]</ref>, or to build f type=""bibr"" target=""#b4"">[5]</ref>, optimizing the prediction of unknown labels in classification <ref type=""bibr"" target=""#b50"">[51]</ref> or semisupervised learning problems <ref type=""bibr"" targe",1
"spectral interpretation and vertex domain localization <ref type=""bibr"" target=""#b129"">[130]</ref>, <ref type=""bibr"" target=""#b130"">[131]</ref>. Notions of stationarity can help develop probabilistic essing methods leading to graph-based Wiener filtering <ref type=""bibr"" target=""#b131"">[132]</ref>, <ref type=""bibr"" target=""#b130"">[131]</ref>.</p><p>A study of vertex/spectral localization and uncer",1
"t the observed data was obtained through graphbased diffusion. Examples of these approaches include <ref type=""bibr"" target=""#b145"">[146]</ref>, <ref type=""bibr"" target=""#b146"">[147]</ref>, <ref type=",0
"=""bibr"" target=""#b65"">[66]</ref>, filter banks on graphs <ref type=""bibr"" target=""#b66"">[67]</ref>, <ref type=""bibr"" target=""#b67"">[68]</ref>, de-noising <ref type=""bibr"" target=""#b68"">[69]</ref>, and vertex localized implementation. These types of filterbanks have been designed for bipartite graphs <ref type=""bibr"" target=""#b67"">[68]</ref>, <ref type=""bibr"" target=""#b101"">[102]</ref>, thus requiri ""#b101"">[102]</ref>, thus requiring the graph to be decomposed into a series of bipartite subgraphs <ref type=""bibr"" target=""#b67"">[68]</ref>, <ref type=""bibr"" target=""#b102"">[103]</ref>. An alternati",0
"erties of graph signal classes has further been addressed from the viewpoint of dictionary learning <ref type=""bibr"" target=""#b114"">[115]</ref>, <ref type=""bibr"" target=""#b115"">[116]</ref>, <ref type=",0
"es and hence a complete set of eigenvectors.</p><p>By the Cayley-Hamilton Theorem of Linear Algebra <ref type=""bibr"" target=""#b82"">[83]</ref>, <ref type=""bibr"" target=""#b83"">[84]</ref> degree(h(z)) =",0
"genvector of the Laplacian).</p><p>to introduce definitions appropriate for directed graphs as well <ref type=""bibr"" target=""#b88"">[89]</ref>, <ref type=""bibr"" target=""#b89"">[90]</ref>. In summary, a",0
""">[77]</ref>, <ref type=""bibr"" target=""#b77"">[78]</ref>, <ref type=""bibr"" target=""#b78"">[79]</ref>, <ref type=""bibr"" target=""#b79"">[80]</ref>, <ref type=""bibr"" target=""#b80"">[81]</ref> studies time si l in studying how signals are processed by filters. Clearly, given s(z) we can recover the signal s <ref type=""bibr"" target=""#b79"">[80]</ref>, <ref type=""bibr"" target=""#b80"">[81]</ref>.</p><p>The disc",0
"nce of papers <ref type=""bibr"" target=""#b45"">[46]</ref>, <ref type=""bibr"" target=""#b46"">[47]</ref>, <ref type=""bibr"" target=""#b47"">[48]</ref>, <ref type=""bibr"" target=""#b48"">[49]</ref>, <ref type=""bib l, the shift and the graph signal model revert to the classical time shift (delay) and signal model <ref type=""bibr"" target=""#b47"">[48]</ref> (see Section II). Subsequently, authors have proposed othe cessing (ASP) <ref type=""bibr"" target=""#b45"">[46]</ref>, <ref type=""bibr"" target=""#b46"">[47]</ref>, <ref type=""bibr"" target=""#b47"">[48]</ref>, <ref type=""bibr"" target=""#b48"">[49]</ref> or from the spe",0
""">[78]</ref>, <ref type=""bibr"" target=""#b78"">[79]</ref>, <ref type=""bibr"" target=""#b79"">[80]</ref>, <ref type=""bibr"" target=""#b80"">[81]</ref> studies time signals. Graph signal processing (GSP)<ref ty filters. Clearly, given s(z) we can recover the signal s <ref type=""bibr"" target=""#b79"">[80]</ref>, <ref type=""bibr"" target=""#b80"">[81]</ref>.</p><p>The discrete Fourier transform (DFT) of the signal<",0
"n infect any other node (full mixing or complete network). To account for the impact of the network <ref type=""bibr"" target=""#b16"">[17]</ref>, resorting to numerical studies is precluded except for ve requency response</p><formula xml:id=""formula_33"">[h (λ 0 ) • • • h (λ N −1 )]</formula><p>given by <ref type=""bibr"" target=""#b16"">(17)</ref>. Finally, an inverse graph Fourier transform computes the",0
"201"">[202]</ref>, data visualization applications on large real-world datasets of millions of nodes <ref type=""bibr"" target=""#b202"">[203]</ref>, <ref type=""bibr"" target=""#b193"">[194]</ref>, or in anal",0
"entations <ref type=""bibr"" target=""#b104"">[105]</ref>, <ref type=""bibr"" target=""#b105"">[106]</ref>, <ref type=""bibr"" target=""#b107"">[108]</ref>, <ref type=""bibr"" target=""#b108"">[109]</ref>, ii) develo",0
"gs, as in <ref type=""bibr"" target=""#b155"">[156]</ref>, <ref type=""bibr"" target=""#b156"">[157]</ref>, <ref type=""bibr"" target=""#b157"">[158]</ref>, which can potentially lead to significant savings in en",0
"characterized by their spectral properties) and the level of exposure of subject to different tasks <ref type=""bibr"" target=""#b172"">[173]</ref>. Some works further build on the multi-resolution proper",0
"a, which is an important step towards developing effective treatment of neuro-degenerative diseases <ref type=""bibr"" target=""#b180"">[181]</ref>. The growing number of publications studying brain activ",0
") or localization operators that have both a spectral interpretation and vertex domain localization <ref type=""bibr"" target=""#b129"">[130]</ref>, <ref type=""bibr"" target=""#b130"">[131]</ref>. Notions of",0
"les. The first problem formulation and a sufficient condition for unique recovery were presented in <ref type=""bibr"" target=""#b117"">[118]</ref>. A necessary and sufficient condition for unique recover",0
"t f , of dimension 300 for an input text (transcript), t.</p><p>Audio Feature Extraction openSMILE <ref type=""bibr"" target=""#b21"">[22]</ref> is an open-source toolkit used to extract high dimensional",1
"elow.</p><p>Visual Feature Extraction For extracting visual features from the videos, we use 3D-CNN <ref type=""bibr"" target=""#b15"">[16]</ref>. 3D-CNN has achieved state-of-the-art results in object cl /ref>. 3D-CNN has achieved state-of-the-art results in object classification on tridimensional data <ref type=""bibr"" target=""#b15"">[16]</ref>. 3D-CNN not only extracts features from each image frame,",1
"c.org/ns/1.0""><head>Textual Features Extraction</head><p>We use Convolutional Neural Networks (CNN) <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b18"">19]</ref> to extract feature",1
"action</head><p>We use Convolutional Neural Networks (CNN) <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b18"">19]</ref> to extract features from the transcript of a video, v. Firs",1
"ps. Subsequently, a full-connected layer with 300 neurons is used with rectified linear unit (ReLU) <ref type=""bibr"" target=""#b20"">[21]</ref> as the activation function. The activations of this full-c 1024 followed by a linear output layer. We use the rectified linear unit (ReLU) activation function <ref type=""bibr"" target=""#b20"">[21]</ref> for non-linearity at the hidden layer. A dropout <ref type",0
"without special aids is only 54% <ref type=""bibr"" target=""#b0"">[1]</ref>. A study by DePaulo et al. <ref type=""bibr"" target=""#b1"">[2]</ref> found that deception without any particular motivation or in",0
"by the subject under consideration. Use of more complex features such as psycholinguistic features <ref type=""bibr"" target=""#b6"">[7]</ref> based on the Linguistic Inquiry and Word Count (LIWC) lexico",0
"is used to perform voice normalization. To remove the background noise, we use SoX (Sound eXchange) <ref type=""bibr"" target=""#b22"">[23]</ref> audio processing tool. The noiseless input audio is then f",0
"as short involuntary expressions, which could potentially indicate deceptive behavior. Caso et al. <ref type=""bibr"" target=""#b11"">[12]</ref> identified particular hand gesture to be important to iden",0
"> identified particular hand gesture to be important to identify the act of deception. Cohen et al. <ref type=""bibr"" target=""#b12"">[13]</ref> found that fewer iconic hand gestures were a sign of a dec",0
"ave become the propellers for this political propaganda. Countries around the world, such as France <ref type=""bibr"" target=""#b4"">[5]</ref>, are employing methods that would prevent the spread of fake",0
"ly, the focus has been towards experiments in real life scenarios. Towards this, Pérez-Rosas et al. <ref type=""bibr"" target=""#b13"">[14]</ref> introduced a new multi-modal deception dataset having real are also considered to play an important role in detecting deceptive behavior. The data provided by <ref type=""bibr"" target=""#b13"">[14]</ref> contains 39 facial micro-expressions such as frowning, smi ><p>For evaluating our deception detection model, we use a real-life deception detection dataset by <ref type=""bibr"" target=""#b13"">[14]</ref>. This dataset contains 121 video clips of courtroom trials y our models against only Linear SVM (L-SVM) and Logistic Regression (LR).</p><p>Pérez-Rosas et al. <ref type=""bibr"" target=""#b13"">[14]</ref> use Decision Trees (DT) and Random Forest (RF) as their cl #tab_1"">2</ref> compares the performance our models with Decision Tree and Linear Regression models <ref type=""bibr"" target=""#b13"">[14]</ref>. Our model, M LP H+C , again outperforms other baselines b formance of our models; followed by Micro-Expressions and audio. This conforms with the findings by <ref type=""bibr"" target=""#b13"">[14]</ref> that facial display features and unigrams contribute the m converged faster in comparison with M LP C .</p><p>While our system performs well on the dataset by <ref type=""bibr"" target=""#b13"">[14]</ref>, we can not claim the same performance of our model for la",0
"features are based on the linguistic characteristics, such as n-grams and sentence count statistics <ref type=""bibr"" target=""#b5"">[6]</ref>, of the statement by the subject under consideration. Use of [7]</ref> based on the Linguistic Inquiry and Word Count (LIWC) lexicon, have also been explored by <ref type=""bibr"" target=""#b5"">[6]</ref> and shown that they are helpful in detecting deceptive behav",0
"tests are not reliable and often misleading as indicated by <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b9"">10]</ref> since judgment made by humans are often biased. Facial expre",0
">19]</ref> to extract features from the transcript of a video, v. First, we use pretrained Word2Vec <ref type=""bibr"" target=""#b19"">[20]</ref> model to extract the vector representations for every word",0
"subject under investigation. But these tests are not reliable and often misleading as indicated by <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b9"">10]</ref> since judgment made b",0
"va and Rudzicz studied the relation between the syntactic complexity of text and deceptive behavior <ref type=""bibr"" target=""#b7"">[8]</ref>. 1 In non-verbal deception detection, physiological measures",0
"emotional responses such as lowered self-esteem, increased suicidal thoughts, anger, and depression <ref type=""bibr"" target=""#b3"">[4]</ref>. Teenagers fall prey to these attacks due to their inability",0
"al expressions and hand gestures were found to be very helpful in detecting deceptive nature. Ekman <ref type=""bibr"" target=""#b10"">[11]</ref> defined micro-expressions as short involuntary expressions",0
"ocial media. Cyberbullying is increasingly becoming a common problem amongst the teenagers nowadays <ref type=""bibr"" target=""#b2"">[3]</ref>. These include spreading rumors about a person, threats, and",0
"-c.org/ns/1.0""><head n=""3.2.2."">Attention</head><p>ESPnet uses a location-aware attention mechanism <ref type=""bibr"" target=""#b34"">[35]</ref>, as a default attention. A dot-product attention <ref type conditions (e.g., <ref type=""bibr"" target=""#b32"">[33]</ref> does not use any language models, while <ref type=""bibr"" target=""#b34"">[35]</ref> and <ref type=""bibr"" target=""#b10"">[11]</ref> use a word-b",1
"two major end-to-end ASR implementations based on both connectionist temporal classification (CTC) <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" tar",1
"luding Google voice search, Amazon Alexa, and Apple Siri and open source activities including Kaldi <ref type=""bibr"" target=""#b0"">[1]</ref>, HTK <ref type=""bibr"" target=""#b1"">[2]</ref>, Sphinx <ref ty ""#b8"">[9]</ref>, as a main deep learning engine. ESPnet also follows the style of Kaldi ASR toolkit <ref type=""bibr"" target=""#b0"">[1]</ref> for data processing, feature extraction/format, and recipes",0
"mple SRILM <ref type=""bibr"" target=""#b5"">[6]</ref> network <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" tar",0
"get=""#b31"">[32]</ref> followed by BLSTM layers inspired by <ref type=""bibr"" target=""#b32"">[33,</ref><ref type=""bibr"" target=""#b33"">34]</ref>, that is</p><formula xml:id=""formula_1"">h 1:T = BLSTM(VGG2( translation <ref type=""bibr"" target=""#b40"">[41]</ref> and applied to end-to-end speech recognition <ref type=""bibr"" target=""#b33"">[34]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head",0
"HTK <ref type=""bibr"" target=""#b1"">[2]</ref>, Sphinx <ref type=""bibr"" target=""#b2"">[3]</ref>, Julius <ref type=""bibr"" target=""#b3"">[4]</ref>, RASR <ref type=""bibr"" target=""#b4"">[5]</ref> in addition to",0
">[6]</ref> network <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b15"">16]</ref>. Attention-based me represented by bidirectional long short-term memory (BLSTM) with subsampling (called pyramid BLSTM <ref type=""bibr"" target=""#b14"">[15]</ref>) given T -length speech feature sequence o1:T to extract h",0
"y-used dynamic neural network toolkits, Chainer <ref type=""bibr"" target=""#b7"">[8]</ref> and PyTorch <ref type=""bibr"" target=""#b8"">[9]</ref>, as a main deep learning engine. ESPnet also follows the sty",0
"19"">[20]</ref>, Corpus of Spontaneous Japanese (CSJ) <ref type=""bibr"" target=""#b20"">[21]</ref>, AMI <ref type=""bibr"" target=""#b21"">[22]</ref>, HKUST Mandarin CTS <ref type=""bibr"" target=""#b22"">[23]</r tion, the ESPnet recipes also include noise robust/far-field speech recognition tasks including AMI <ref type=""bibr"" target=""#b21"">[22]</ref>, CHiME-4 <ref type=""bibr"" target=""#b24"">[25]</ref>, and CH",0
"ph embedding methods <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b16"">17]</ref> to learn the embedding of each item, dubbed Base Graph Embe . Experiments are conducted to compare four methods: BGE, LINE, GES, and EGES. LINE was proposed in <ref type=""bibr"" target=""#b16"">[17]</ref>, which captures the first-order and second-order proximity",1
"tory and then apply the state-of-art graph embedding methods <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b16"">17]</ref> to learn the embedd this section, we give an overview of graph embedding and one of the most popular methods, DeepWalk <ref type=""bibr"" target=""#b14"">[15]</ref>, based on which we propose our graph embedding methods in spired by word2vec, Perozzi et al. proposed DeepWalk to learn the embedding of each node in a graph <ref type=""bibr"" target=""#b14"">[15]</ref>. They first generate sequences of nodes by running random walk based techniques <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b14"">15]</ref> use random walks on graphs to obtain node representations w",0
"gories, etc. Besides, textual information related to the nodes is incorporated into graph embedding <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" ta",0
"type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b15"">16]</ref>, content-based methods <ref type=""bibr"" target=""#b1"">[2]</ref>, and deep learning based methods <ref type=""bibr"" target=""#b",0
"ese methods could be categorized into three broad categories: 1) Factorization methods such as LINE <ref type=""bibr"" target=""#b0"">[1]</ref> try to approximately factorize the adjacency matrix and pres",0
"m recent works in image and audio generation that discretize the space, namely PixelRNN and Wavenet <ref type=""bibr"" target=""#b37"">(Oord et al., 2016a;</ref><ref type=""bibr"">b)</ref>. Discretization m",1
"ge, usually 4096 bytes, but even predicting at the page level would leave 2 52 possible targets. In <ref type=""bibr"" target=""#b38"">(Oord et al., 2016b)</ref>, they predict 16-bit integer values from a",1
"br"" target=""#b0"">(Ayers et al., 2018;</ref><ref type=""bibr"" target=""#b6"">Ferdman et al., 2012;</ref><ref type=""bibr"" target=""#b10"">Gutierrez et al., 2011;</ref><ref type=""bibr"" target=""#b11"">Hashemi e",0
"ibr"" target=""#b25"">Lai et al., 2001;</ref><ref type=""bibr"" target=""#b43"">Somogyi et al., 2006;</ref><ref type=""bibr"" target=""#b42"">Roth et al., 1998)</ref>. They store the past history of memory acces",0
"come the model-class of choice for many sequential prediction problems. Notably, speech recognition <ref type=""bibr"" target=""#b13"">(Hinton et al., 2012)</ref> and natural language processing <ref type",0
"differences between subsequent memory addresses) <ref type=""bibr"" target=""#b7"">(Gindele, 1977;</ref><ref type=""bibr"" target=""#b19"">Jouppi, 1990;</ref><ref type=""bibr"" target=""#b39"">Palacharla &amp; Ke",0
"ef type=""bibr"" target=""#b36"">[37]</ref> uses semantic segmentation for video deblurring. Zhu et al. <ref type=""bibr"" target=""#b51"">[52]</ref> propose an approach to generate new clothing on a wearer. nal bias at the input layer.</p><p>2) Compositional mapping -This method is identical to Zhu et al. <ref type=""bibr"" target=""#b51"">[52]</ref>. It decomposes an LR image based on the predicted semantic",1
"alistic and visually pleasing textures in comparison to state-of-the-art SRGAN [27]  and EnhanceNet <ref type=""bibr"" target=""#b37"">[38]</ref>.</p></div> 			</abstract> 		</profileDesc> 	</teiHeader> e instead of pixel space. Ledig et al. <ref type=""bibr"" target=""#b26"">[27]</ref> and Sajjadi et al. <ref type=""bibr"" target=""#b37"">[38]</ref> further propose adversarial loss to encourage the network 7]</ref> introduce an adversarial loss, generating images with more natural details. Sajjadi et al. <ref type=""bibr"" target=""#b37"">[38]</ref> develop a similar approach and further explore the local t pe=""figure"">5</ref>. GAN-based methods (SRGAN <ref type=""bibr"" target=""#b26"">[27]</ref>, EnhanceNet <ref type=""bibr"" target=""#b37"">[38]</ref> and ours) clearly outperform PSNR-oriented approaches in t ef>, and GAN-based methods, such as SRGAN <ref type=""bibr"" target=""#b26"">[27]</ref> and En-hanceNet <ref type=""bibr"" target=""#b37"">[38]</ref>. More results are provided in the supplementary material. -GAN and the counterpart generated by SRGAN <ref type=""bibr"" target=""#b26"">[27]</ref> or EnhanceNet <ref type=""bibr"" target=""#b37"">[38]</ref>. The users were asked to pick the image with more natural on, our method is ranked higher than SRGAN <ref type=""bibr"" target=""#b26"">[27]</ref> and EnhanceNet <ref type=""bibr"" target=""#b37"">[38]</ref>, especially in building, animal, and grass categories. Com r studies, comparing our method with SRGAN <ref type=""bibr"" target=""#b26"">[27]</ref> and EnhanceNet <ref type=""bibr"" target=""#b37"">[38]</ref>. Second row: our methods produce visual results that are r ser studies, comparing our method with SRGAN<ref type=""bibr"" target=""#b26"">[27]</ref> and EnhanceNet<ref type=""bibr"" target=""#b37"">[38]</ref>. Second row: our methods produce visual results that are r ref type=""bibr"" target=""#b2"">3]</ref> and adversarial loss <ref type=""bibr"" target=""#b26"">[27,</ref><ref type=""bibr"" target=""#b37"">38]</ref> are introduced to solve the regression-to-the-mean problem ur framework is based on adversarial learning, inspired by <ref type=""bibr"" target=""#b26"">[27,</ref><ref type=""bibr"" target=""#b37"">38]</ref>. Specifically, it consists of one generator G θ and one dis d n=""3.3."">Loss Function</head><p>We draw inspiration from <ref type=""bibr"" target=""#b26"">[27,</ref><ref type=""bibr"" target=""#b37"">38]</ref> and apply perceptual loss and adversarial loss in our model leasing textures, outperforming previous GAN-based methods <ref type=""bibr"" target=""#b26"">[27,</ref><ref type=""bibr"" target=""#b37"">38]</ref>.</p><p>Our work currently focuses on SR of outdoor scenes.",1
"b46"">[47]</ref>. They train specialized models for each semantic category on exemplar-based methods <ref type=""bibr"" target=""#b49"">[50,</ref><ref type=""bibr"" target=""#b45"">46]</ref> and show that SR r iors by training specialized models separately for each semantic category on exemplar-based methods <ref type=""bibr"" target=""#b49"">[50,</ref><ref type=""bibr"" target=""#b45"">46]</ref>. In contrast to th ing <ref type=""bibr"" target=""#b3"">[4]</ref>, sparse coding <ref type=""bibr"" target=""#b48"">[49,</ref><ref type=""bibr"" target=""#b49"">50,</ref><ref type=""bibr"" target=""#b44"">45,</ref><ref type=""bibr"" tar main evaluation on standard benchmarks such as Set5 <ref type=""bibr"" target=""#b1"">[2]</ref>, Set14 <ref type=""bibr"" target=""#b49"">[50]</ref> and BSD100 <ref type=""bibr"" target=""#b32"">[33]</ref> since",0
""" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" tar d has witnessed a variety of network architectures, such as a deeper network with residual learning <ref type=""bibr"" target=""#b21"">[22]</ref>, Laplacian pyramid structure <ref type=""bibr"" target=""#b25 models including PSNR-oriented methods, such as SRCNN <ref type=""bibr"" target=""#b6"">[7]</ref>, VDSR <ref type=""bibr"" target=""#b21"">[22]</ref>, LapSRN <ref type=""bibr"" target=""#b25"">[26]</ref>, DRRN <r",0
""">5]</ref>, semantic segments are used as input conditions to generate natural images. Gatys et al. <ref type=""bibr"" target=""#b13"">[14]</ref> use semantic maps to control perceptual factors in neural",0
"k.</p><p>Contemporary SR algorithms are mostly learning-based methods, including neighbor embedding <ref type=""bibr"" target=""#b3"">[4]</ref>, sparse coding <ref type=""bibr"" target=""#b48"">[49,</ref><ref",0
"ise manipulation but also spatial-wise transformation.</p><p>Semantic guidance. In image generation <ref type=""bibr"" target=""#b19"">[20,</ref><ref type=""bibr"" target=""#b4"">5]</ref>, semantic segments a",0
"odels for each semantic category on exemplar-based methods <ref type=""bibr"" target=""#b49"">[50,</ref><ref type=""bibr"" target=""#b45"">46]</ref> and show that SR results can be improved by semantic priors ately for each semantic category on exemplar-based methods <ref type=""bibr"" target=""#b49"">[50,</ref><ref type=""bibr"" target=""#b45"">46]</ref>. In contrast to these studies, we explore categorical prior et=""#b48"">[49,</ref><ref type=""bibr"" target=""#b49"">50,</ref><ref type=""bibr"" target=""#b44"">45,</ref><ref type=""bibr"" target=""#b45"">46]</ref> and random forest <ref type=""bibr"" target=""#b38"">[39]</ref>",0
"get=""#b25"">26,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b42"">43,</ref><ref type=""bibr"" target=""#b43"">44]</ref> constrain the solut f type=""bibr"" target=""#b26"">[27]</ref>, recursive learning <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b42"">43]</ref>, and densely connected network <ref type=""bibr"" target=""#b4 not tried other architectures for the SR network, we believe many contemporary models such as DRRN <ref type=""bibr"" target=""#b42"">[43]</ref> and MemNet <ref type=""bibr"" target=""#b43"">[44]</ref> are a R <ref type=""bibr"" target=""#b21"">[22]</ref>, LapSRN <ref type=""bibr"" target=""#b25"">[26]</ref>, DRRN <ref type=""bibr"" target=""#b42"">[43]</ref>, MemNet <ref type=""bibr"" target=""#b43"">[44]</ref>, and GAN",0
"ef>, sparse coding <ref type=""bibr"" target=""#b48"">[49,</ref><ref type=""bibr"" target=""#b49"">50,</ref><ref type=""bibr"" target=""#b44"">45,</ref><ref type=""bibr"" target=""#b45"">46]</ref> and random forest <",0
"ediction accuracy. Similar attention mechanisms have been proposed for natural image classification <ref type=""bibr"" target=""#b10"">[11]</ref> and captioning <ref type=""bibr"" target=""#b0"">[1]</ref> to ntributions of this work can be summarised as follows: • We take the attention approach proposed in <ref type=""bibr"" target=""#b10"">[11]</ref> a step further by proposing grid-based gating that allows intermediate space. In image captioning <ref type=""bibr"" target=""#b0"">[1]</ref> and classification <ref type=""bibr"" target=""#b10"">[11]</ref> tasks, the  softmax activation function is used to normali n. This results experimentally in better training convergence for the AG parameters. In contrast to <ref type=""bibr"" target=""#b10"">[11]</ref> we propose a grid-attention technique. In this case, gatin the feature-maps and map them to lower dimensional space for the gating operation. As suggested in <ref type=""bibr"" target=""#b10"">[11]</ref>, low-level feature-maps, i.e. the first skip connections, <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b29"">30]</ref>, and classification <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b30"">31,</ref><ref type=""bibr"" ta [2,</ref><ref type=""bibr"" target=""#b28"">29]</ref> and more recently applied to image classification <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b31"">32]</ref>. In <ref type=""bib was the top-performer in the ILSVRC 2017 image classification challenge. Self-attention techniques <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b32"">33]</ref> have been proposed tention is used in <ref type=""bibr"" target=""#b32"">[33]</ref> to capture long range dependencies. In <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b31"">32]</ref> self-attention is",1
"ype=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b27"">28]</ref> or multi-atlas techniques <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b33"">34]</ref>. In particular, at 25]</ref>, Dice similarity coefficients (DSC) for atlas-based frameworks ranges from 69.6% to 73.9% <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b33"">34]</ref>. In <ref type=""bib organ localisation models in image segmentation frameworks <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" tar",1
"erature in machine learning exploring different gating architectures. For example, highway networks <ref type=""bibr"" target=""#b6"">[7]</ref> make use of residual connections around the gate block to al",0
"(ROI) and make dense predictions on that particular ROI. The application areas include cardiac MRI <ref type=""bibr"" target=""#b13"">[14]</ref>, cardiac CT <ref type=""bibr"" target=""#b22"">[23]</ref>, abd r"" target=""#b35"">[36]</ref> and external organ localisation models in image segmentation frameworks <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" ta rks (CNNs) outperform traditional approaches in medical image analysis on public benchmark datasets <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b16"">17]</ref> while being an ord hat show large shape variability. In order to improve the accuracy, current segmentation frameworks <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" ta",0
"e soft attention is used in sentence-to-sentence translation <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b28"">29]</ref> and more recently applied to image classification <ref type semantic classes, we propose to learn multi-dimensional attention coefficients. This is inspired by <ref type=""bibr"" target=""#b28"">[29]</ref>, where multidimensional attention coefficients are used to",0
"rmance in various tasks including cardiac MR <ref type=""bibr"" target=""#b2"">[3]</ref>, brain tumours <ref type=""bibr"" target=""#b11"">[12]</ref> and abdominal CT <ref type=""bibr"" target=""#b25"">[26,</ref>",0
"e=""bibr"" target=""#b27"">28]</ref> or multi-atlas techniques <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b33"">34]</ref>. In particular, atlas approaches benefit from implicit shap DSC) for atlas-based frameworks ranges from 69.6% to 73.9% <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b33"">34]</ref>. In <ref type=""bibr"" target=""#b38"">[39]</ref> a classificat le being an order of magnitude faster than, e.g., graph-cut and multi-atlas segmentation techniques <ref type=""bibr"" target=""#b33"">[34]</ref>. This is mainly attributed to the fact that (I) domain spe ble <ref type=""table"" target=""#tab_2"">3</ref>. U-Net model outperforms traditional atlas techniques <ref type=""bibr"" target=""#b33"">[34]</ref> although it was trained on a disjoint dataset. Moreover, t",0
"hape and size. We evaluate our implementation on two commonly used benchmarks: TCIA Pancreas CT -82 <ref type=""bibr"" target=""#b24"">[25]</ref> and multi-class abdominal CT -150. The results show that A forced by propagation of manual annotations. However, in public benchmarks such as the TCIA dataset <ref type=""bibr"" target=""#b24"">[25]</ref>, Dice similarity coefficients (DSC) for atlas-based framew nced 3D CT scans with pancreas manual annotations performed slice-by-slice. This dataset (NIH-TCIA) <ref type=""bibr"" target=""#b24"">[25]</ref> is publicly available and commonly used to benchmark CT pa head><label>3</label><figDesc>Pancreas segmentation results obtained on the TCIA Pancreas-CT Dataset<ref type=""bibr"" target=""#b24"">[25]</ref>. The dataset contains in total 82 scans which are split in",0
"cardiac MR segmentation <ref type=""bibr"" target=""#b2"">[3]</ref> and cancerous lung nodule detection <ref type=""bibr"" target=""#b16"">[17]</ref>. High representation power, fast inference, and filter sha #b25"">[26,</ref><ref type=""bibr"" target=""#b26"">27]</ref> segmentation, and lung CT nodule detection <ref type=""bibr"" target=""#b16"">[17]</ref>. However, this approach leads to excessive and redundant u hes in medical image analysis on public benchmark datasets <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b16"">17]</ref> while being an order of magnitude faster than, e.g., graph-",0
"on the other hand, is enforced by design and categorised as hard-and soft-attention. Hard attention <ref type=""bibr"" target=""#b20"">[21]</ref>, e.g. iterative region proposal and cropping, is often non rd back-propagation updates without a need for sampling based update methods used in hard-attention <ref type=""bibr"" target=""#b20"">[21]</ref>.</p><p>Attention Gates in U-Net Model: The proposed AGs ar 4"">[25]</ref>. The dataset contains in total 82 scans which are split into training (61) and testing<ref type=""bibr"" target=""#b20"">(21)</ref> sets. The corresponding results are obtained before (BFT)",0
"d><p>Following YOLO9000 our system predicts bounding boxes using dimension clusters as anchor boxes <ref type=""bibr"" target=""#b14"">[15]</ref>. The network predicts 4 coordinates for each bounding box, ocation of filter application using a sigmoid function. This figure blatantly self-plagiarized from <ref type=""bibr"" target=""#b14"">[15]</ref>.</p><p>is not the best but does overlap a ground truth obj location of filter application using a sigmoid function. This figure blatantly self-plagiarized from<ref type=""bibr"" target=""#b14"">[15]</ref>.</figDesc></figure> <figure xmlns=""http://www.tei-c.org/ns",1
"Our system extracts features from those scales using a similar concept to feature pyramid networks <ref type=""bibr"" target=""#b7"">[8]</ref>. From our base feature extractor we add several convolutiona",1
"does overlap a ground truth object by more than some threshold we ignore the prediction, following <ref type=""bibr"" target=""#b17"">[17]</ref>. We use the threshold of .5. Unlike <ref type=""bibr"" targe prediction, following <ref type=""bibr"" target=""#b17"">[17]</ref>. We use the threshold of .5. Unlike <ref type=""bibr"" target=""#b17"">[17]</ref> our system only assigns one bounding box prior for each gr",0
"d was ""set deliberately low to account for inaccuracies in bounding boxes in the ground truth data"" <ref type=""bibr"" target=""#b1"">[2]</ref>. Does COCO have better labelling than VOC? This is definitel",0
"=""table"">3</ref> Table <ref type=""table"">3</ref>. I'm seriously just stealing all these tables from <ref type=""bibr"" target=""#b8"">[9]</ref> they take soooo long to make from scratch. Ok, YOLOv3 is doi",0
"bounding box with IOU of 0.3 and distinguish it from one with IOU 0.5 is sur-prisingly difficult."" <ref type=""bibr"" target=""#b18"">[18]</ref> If humans have a hard time telling the difference, how muc",0
"esearch a little.</p><p>Actually, that's what brings us here today. We have a camera-ready deadline <ref type=""bibr"" target=""#b3"">[4]</ref> and we need to cite some of the random updates I made to YOL",0
"ion are just doing happy, good stuff with it, like counting the number of zebras in a national park <ref type=""bibr"" target=""#b12"">[13]</ref>, or tracking their cat as it wanders around their house <r",0
"ation, all the standard stuff. We use the Darknet neural network framework for training and testing <ref type=""bibr"" target=""#b13"">[14]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head",0
"f architectures has gained tremendous popularity due to prominent applications to language modeling <ref type=""bibr"" target=""#b64"">(Sutskever et al., 2011;</ref><ref type=""bibr"" target=""#b25"">Graves,",0
"bibr"" target=""#b56"">Pascanu et al., 2013;</ref><ref type=""bibr"" target=""#b43"">Le et al., 2015;</ref><ref type=""bibr"" target=""#b1"">Arjovsky et al., 2016;</ref><ref type=""bibr"" target=""#b74"">Zhang et al e order of the sequence is permuted at random <ref type=""bibr"" target=""#b43"">(Le et al., 2015;</ref><ref type=""bibr"" target=""#b1"">Arjovsky et al., 2016;</ref><ref type=""bibr"" target=""#b69"">Wisdom et a",0
"ehring et al., 2017;</ref><ref type=""bibr"" target=""#b4"">Kalchbrenner et al., 2016)</ref>, attention <ref type=""bibr"" target=""#b10"">(Vaswani et al., 2017)</ref>, or a combination of recurrence and atte ></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2.1"">Transformer</head><p>The Transformer <ref type=""bibr"" target=""#b10"">(Vaswani et al., 2017)</ref> employs an encoder-decoder structure, co for all sequences, heads, and positions in a batch using parallel matrix multiplication operations <ref type=""bibr"" target=""#b10"">(Vaswani et al., 2017)</ref>. Without relative position representatio d><p>We compared our model using only relative position representations to the baseline Transformer <ref type=""bibr"" target=""#b10"">(Vaswani et al., 2017)</ref> with sinusoidal position encodings. We g e a deterministic function of position <ref type=""bibr"" target=""#b7"">(Sukhbaatar et al., 2015;</ref><ref type=""bibr"" target=""#b10"">Vaswani et al., 2017)</ref> or learned representations. Convolutional se in steps per second, but we were able to maintain the same model and batch sizes on P100 GPUs as <ref type=""bibr"" target=""#b10"">Vaswani et al. (2017)</ref>.</p></div> <div xmlns=""http://www.tei-c.o 1 = 0.9, β 2 = 0.98, and = 10 −9 . We used the same warmup and decay strategy for learning rate as <ref type=""bibr"" target=""#b10"">Vaswani et al. (2017)</ref>, with 4,000 warmup steps. During training",1
"tion information to the model. These position encodings can be a deterministic function of position <ref type=""bibr"" target=""#b7"">(Sukhbaatar et al., 2015;</ref><ref type=""bibr"" target=""#b10"">Vaswani",0
"ype=""bibr"" target=""#b10"">(Vaswani et al., 2017)</ref>, or a combination of recurrence and attention <ref type=""bibr"" target=""#b1"">(Bahdanau et al., 2014;</ref><ref type=""bibr"" target=""#b2"">Cho et al.,",0
"n-based models have therefore used position encodings or biased attention weights based on distance <ref type=""bibr"" target=""#b6"">(Parikh et al., 2016)</ref>.</p><p>In this work we present an efficien",0
"e=""bibr"" target=""#b2"">Cho et al., 2014;</ref><ref type=""bibr"" target=""#b5"">Luong et al., 2015;</ref><ref type=""bibr"" target=""#b12"">Wu et al., 2016)</ref> as basic building blocks. These approaches inc mately 36M sentence pairs. For all experiments, we split tokens into a 32,768 word-piece vocabulary <ref type=""bibr"" target=""#b12"">(Wu et al., 2016)</ref>. We batched sentence pairs by approximate len , 2016)</ref>. For evaluation, we used beam search with a beam size of 4 and length penalty α = 0.6 <ref type=""bibr"" target=""#b12"">(Wu et al., 2016)</ref>.</p><p>For our base model, we used 6 encoder",0
"approach. Demographic parity and related formulations have been considered in numerous papers (e.g. <ref type=""bibr"" target=""#b1"">Calders et al., 2009;</ref><ref type=""bibr"" target=""#b18"">Zafar et al.",0
". These trade-offs carry over to some extent to the case where we only equalize true positive rates <ref type=""bibr"" target=""#b15"">(Pleiss et al., 2017)</ref>.</p><p>A growing literature on fairness i",0
"has been much work in the social sciences on analyzing the effect of affirmative action (see e.g., <ref type=""bibr"" target=""#b12"">Keith et al., 1985;</ref><ref type=""bibr"" target=""#b11"">Kalev et al.,",0
"ed the equality of opportunity constraint and demonstrate limitations of a broad class of criteria. <ref type=""bibr"" target=""#b13"">Kleinberg et al. (2017)</ref> and Chouldechova (2016) point out the t",0
"ine learning tasks involve graph structured datasets, such as classifying posts in a social network <ref type=""bibr"" target=""#b10"">(Hamilton et al., 2017a)</ref>, predicting interfaces between protein o aggregate a local set of lower-level features. We refer to such an operator as a graph aggregator <ref type=""bibr"" target=""#b10"">(Hamilton et al., 2017a)</ref> and the set of local nodes as the rece ref type=""bibr"" target=""#b15"">Kipf and Welling, 2017</ref>) can be interpreted as graph aggregators <ref type=""bibr"" target=""#b10"">(Hamilton et al., 2017a)</ref>.</p><p>Graph aggregators are the basic t to the inductive node classification problem. We also improve the sampling strategy introduced in <ref type=""bibr"" target=""#b10"">(Hamilton et al., 2017a)</ref> to reduce the memory cost and increase oral forecasting problem. Extensive experiments on two node classification datasets, PPI and Reddit <ref type=""bibr"" target=""#b10"">(Hamilton et al., 2017a)</ref>, and one traffic speed forecasting dat rtional to the total number of nodes, which could be hundreds of thousands of nodes in large graphs <ref type=""bibr"" target=""#b10"">(Hamilton et al., 2017a)</ref>. To reduce memory usage and computatio ""bibr"" target=""#b10"">(Hamilton et al., 2017a)</ref>. To reduce memory usage and computational cost, <ref type=""bibr"" target=""#b10"">(Hamilton et al., 2017a)</ref> proposed the GraphSAGE framework that ble and the goal is to predict the labels of the unseen testing nodes. Our approach follows that of <ref type=""bibr"" target=""#b10"">(Hamilton et al., 2017a)</ref>, where a mini-batch of nodes are sampl dels in our framework and a two-layer fully connected neural network on the PPI and Reddit datasets <ref type=""bibr"" target=""#b10"">(Hamilton et al., 2017a)</ref>. The five baseline aggregators include the effectiveness of incorporating graph structures, we also evaluate a two-layer fully-connected  <ref type=""bibr"" target=""#b10"">(Hamilton et al., 2017a)</ref> (61.2)<ref type=""foot"" target=""#foot_0 hyperparameters for training. The training, validation, and testing splits are the same as that in <ref type=""bibr"" target=""#b10"">(Hamilton et al., 2017a)</ref>. The micro-averaged F1 score is used t ith the previous state-of-the-art methods on inductive node classification. This includes GraphSAGE <ref type=""bibr"" target=""#b10"">(Hamilton et al., 2017a)</ref>, GAT <ref type=""bibr"" target=""#b27"">(V can see steady improvement with larger sampling sizes, which is consistent with the observation in <ref type=""bibr"" target=""#b10"">(Hamilton et al., 2017a)</ref>.</p><p>Effect of output dimensions in r"" target=""#b15"">Kipf and Welling, 2017;</ref><ref type=""bibr"" target=""#b8"">Fout et al., 2017;</ref><ref type=""bibr"" target=""#b10"">Hamilton et al., 2017a;</ref><ref type=""bibr"" target=""#b27"">Veličkovi d on either pooling over neighborhoods <ref type=""bibr"" target=""#b15"">(Kipf and Welling, 2017;</ref><ref type=""bibr"" target=""#b10"">Hamilton et al., 2017a)</ref> or computing a weighted sum of the neig rget=""#b7"">(Duvenaud et al., 2015;</ref><ref type=""bibr"" target=""#b15"">Kipf and Welling, 2017;</ref><ref type=""bibr"" target=""#b10"">Hamilton et al., 2017a)</ref>, while others integrated edge features",1
"r"" target=""#b8"">Fout et al., 2017;</ref><ref type=""bibr"" target=""#b10"">Hamilton et al., 2017a;</ref><ref type=""bibr"" target=""#b27"">Veličković et al., 2018;</ref><ref type=""bibr"" target=""#b18"">Li et al 17)</ref>. It has later been adopted as a graph aggregator to solve the node classification problem <ref type=""bibr"" target=""#b27"">(Veličković et al., 2018)</ref>. A single attention head sums the ele rget=""#b10"">(Hamilton et al., 2017a)</ref> (61.2)<ref type=""foot"" target=""#foot_0"">1</ref> 95.4 GAT <ref type=""bibr"" target=""#b27"">(Veličković et al., 2018)</ref> 97.3 ± 0.2 -Fast GCN <ref type=""bibr"" ication. This includes GraphSAGE <ref type=""bibr"" target=""#b10"">(Hamilton et al., 2017a)</ref>, GAT <ref type=""bibr"" target=""#b27"">(Veličković et al., 2018)</ref>, and FastGCN <ref type=""bibr"" target= final output y i , which has dimension d o . The difference between our aggregator and that in GAT <ref type=""bibr"" target=""#b27"">(Veličković et al., 2018</ref>) is that we have adopted the key-value",1
"± 0.2 -Fast GCN <ref type=""bibr"">(Chen et</ref>  We train all the aggregator-based models with Adam <ref type=""bibr"" target=""#b14"">(Kingma and Ba, 2015)</ref> and early stopping on the validation set.",0
"esizing are eligible graph aggregators. One class of such functions is the neural attention network <ref type=""bibr"" target=""#b1"">(Bahdanau et al., 2015)</ref>, which uses a subnetwork to compute the",0
"ving these problems by graph convolution <ref type=""bibr"" target=""#b7"">(Duvenaud et al., 2015;</ref><ref type=""bibr"" target=""#b0"">Atwood and Towsley, 2016;</ref><ref type=""bibr"" target=""#b15"">Kipf and RU <ref type=""bibr"" target=""#b5"">(Chung et al., 2014)</ref> with the diffusion convolution operator <ref type=""bibr"" target=""#b0"">(Atwood and Towsley, 2016)</ref>. Furthermore, DCRNN takes the directi e=""bibr"" target=""#b10"">Hamilton et al., 2017a)</ref>, while others integrated edge features as well <ref type=""bibr"" target=""#b0"">(Atwood and Towsley, 2016;</ref><ref type=""bibr"" target=""#b8"">Fout et",0
"target=""#b10"">Hamilton et al., 2017a)</ref> or computing a weighted sum of the neighboring features <ref type=""bibr"" target=""#b21"">(Monti et al., 2017)</ref>. In essence, functions that are permutatio get gate strategy in Graph LSTM <ref type=""bibr"" target=""#b19"">(Liang et al., 2016)</ref> and MoNet <ref type=""bibr"" target=""#b21"">(Monti et al., 2017)</ref> employed pairwise sum aggregators with a s",0
"node, only a fixed number of neighborhoods are selected for aggregation. More recently, Chen et al. <ref type=""bibr"" target=""#b3"">(Chen et al., 2018)</ref> proposed a new sampling method that randomly al., 2017a)</ref>, GAT <ref type=""bibr"" target=""#b27"">(Veličković et al., 2018)</ref>, and FastGCN <ref type=""bibr"" target=""#b3"">(Chen et al., 2018)</ref>. The GraphSAGE model used a 2-layer sample a",0
"models, the multi-head attention model has been shown to be effective for machine translation tasks <ref type=""bibr"" target=""#b20"">(Lin et al., 2017;</ref><ref type=""bibr"" target=""#b26"">Vaswani et al. http://www.tei-c.org/ns/1.0""><head n=""6"">TRAFFIC SPEED FORECASTING</head><p>6.1 GRAPH GRU Following <ref type=""bibr"" target=""#b20"">(Lin et al., 2017)</ref>, we formulate traffic speed forecasting as a as Graph GRU (GGRU). GGRU can be used as the basic building block for RNN encoder-decoder structure <ref type=""bibr"" target=""#b20"">(Lin et al., 2017)</ref> to predict the future K steps of traffic spe speeds X 1 , X 2 , ..., X J . In the decoder, we use the scheduled sampling technique described in <ref type=""bibr"" target=""#b20"">(Lin et al., 2017)</ref>. Figure <ref type=""figure"" target=""#fig_5"">4",0
"ranslation using the recent Ten-sor2Tensor framework and the Transformer sequence-to-sequence model <ref type=""bibr"" target=""#b22"">(Vaswani et al., 2017)</ref>. We examine some of the critical paramet ""6."">Conclusion</head><p>We presented a broad range of basic experiments with the Transformer model <ref type=""bibr"" target=""#b22"">(Vaswani et al., 2017)</ref> for English-to-Czech neural machine tran training steps is given but no indication on ""how much converged"" the model was at that point, e.g. <ref type=""bibr"" target=""#b22"">Vaswani et al. (2017)</ref>. Most probably, the training was run unti r faking the global_step stored in the checkpoint) to make sure the learning rate is not too small. <ref type=""bibr"" target=""#b22"">Vaswani et al. (2017)</ref> suggest to average the last 20 checkpoint",1
"<ref type=""bibr"" target=""#b6"">(Bottou et al., 2016;</ref><ref type=""bibr"">Smith and Le, 2017;</ref><ref type=""bibr"" target=""#b11"">Jastrzebski et al., 2017)</ref>. <ref type=""bibr"">Smith and Le (2017) ents over the default learning rate -all had about the same BLEU curve after few hours of training. <ref type=""bibr"" target=""#b11"">Jastrzebski et al. (2017)</ref> shows that ""the invariance under simu",0
"(in Sections 4.2 and 4.6), we substitute CzEng 1.7 with an older and considerably smaller CzEng 1.0 <ref type=""bibr"" target=""#b1"">(Bojar et al., 2012)</ref> containing 15M sentence pairs (233M/206M of",0
"tsoever. Sometimes, they mention only an approximate number of days the model was trained for, e.g. <ref type=""bibr"" target=""#b0"">Bahdanau et al. (2015)</ref>, sometimes the exact number of training s",0
"struction performance. <ref type=""bibr"">Kim et al.</ref> propose a 20-layer CNN model known as VDSR <ref type=""bibr"" target=""#b11"">[12]</ref>, which adopts residual learning and adaptive gradient clip eover, the traditional convolutional networks usually adopt cascaded network topologies, e.g., VDSR <ref type=""bibr"" target=""#b11"">[12]</ref> and DRC-N <ref type=""bibr"" target=""#b12"">[13]</ref>. In th cise structure of the proposed IDN, it is much faster than several CNN-based SR methods, e.g., VDSR <ref type=""bibr"" target=""#b11"">[12]</ref>, DRCN <ref type=""bibr"" target=""#b12"">[13]</ref>, LapSRN <r o accelerate SRCNN in combination with smaller filter sizes and more convolution layers. Kim et al. <ref type=""bibr"" target=""#b11"">[12]</ref> propose a very deep CNN model with global residual archite nerate the residual image. The bias term of this transposed convolution can auto-Dataset Scale VDSR <ref type=""bibr"" target=""#b11"">[12]</ref> DRCN <ref type=""bibr"" target=""#b12"">[13]</ref> LapSRN <ref bicubic, SRCNN <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b3"">4]</ref>, VDSR <ref type=""bibr"" target=""#b11"">[12]</ref>, DRC-N <ref type=""bibr"" target=""#b12"">[13]</ref>, LapSRN < v> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.1.1"">Training datasets</head><p>By following <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" ta",1
"feature maps of each layer are sent to the sequential layer without distinction. However, Hu et al. <ref type=""bibr"" target=""#b8"">[9]</ref> experimentally demonstrate that adaptively recalibrating cha of the second module naturally become the local long-path features. Different from the approach in <ref type=""bibr"" target=""#b8"">[9]</ref>, we divide feature maps into two parts. One part represents",1
"s have been achieved dramatic improvement in SR. Dong et al. <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b3"">4]</ref> first exploit a three-layer convolutional neural network, nam posed method with other SR methods, including bicubic, SRCNN <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b3"">4]</ref>, VDSR <ref type=""bibr"" target=""#b11"">[12]</ref>, DRC-N <ref t",1
"R <ref type=""bibr"" target=""#b11"">[12]</ref>, DRCN <ref type=""bibr"" target=""#b12"">[13]</ref>, LapSRN <ref type=""bibr"" target=""#b14"">[15]</ref>, DRRN <ref type=""bibr"" target=""#b21"">[22]</ref> and MemNet l image restoration problem with encoderdecoder networks and symmetric skip connections. Lai et al. <ref type=""bibr"" target=""#b14"">[15]</ref> propose the laplacian pyramid super-resolution network (La DSR <ref type=""bibr"" target=""#b11"">[12]</ref> DRCN <ref type=""bibr"" target=""#b12"">[13]</ref> LapSRN <ref type=""bibr"" target=""#b14"">[15]</ref> DRRN <ref type=""bibr"" target=""#b21"">[22]</ref> MemNet <ref <ref type=""bibr"" target=""#b11"">[12]</ref>, DRC-N <ref type=""bibr"" target=""#b12"">[13]</ref>, LapSRN <ref type=""bibr"" target=""#b14"">[15]</ref>, DRRN <ref type=""bibr"" target=""#b21"">[22]</ref> and MemNet 0""><head n=""4.1.1"">Training datasets</head><p>By following <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" tar",0
". <ref type=""bibr"" target=""#b25"">[26]</ref> and 200 images from Berkeley Segmentation Dataset (BSD) <ref type=""bibr"" target=""#b17"">[18]</ref> as the training data. As in <ref type=""bibr"" target=""#b21"" t5 <ref type=""bibr"" target=""#b0"">[1]</ref>, Set14 <ref type=""bibr"" target=""#b26"">[27]</ref>, BSD100 <ref type=""bibr"" target=""#b17"">[18]</ref>, Urban100 <ref type=""bibr"" target=""#b9"">[10]</ref>. Among",0
"LR/HR patches, such as nearest neighbor <ref type=""bibr"" target=""#b6"">[7]</ref>, manifold embedding <ref type=""bibr"" target=""#b1"">[2]</ref>, random forest <ref type=""bibr"" target=""#b19"">[20]</ref> and",0
"e ground truths. <ref type=""bibr"">Tai et al.</ref> propose a deep recursive residual network (DRRN) <ref type=""bibr"" target=""#b21"">[22]</ref>, which employs parameters sharing strategy to alleviate th N <ref type=""bibr"" target=""#b12"">[13]</ref>, LapSRN <ref type=""bibr"" target=""#b14"">[15]</ref>, DRRN <ref type=""bibr"" target=""#b21"">[22]</ref> and MemNet <ref type=""bibr"" target=""#b22"">[23]</ref> as il l LR images as input and progressively reconstructs the sub-band residuals of HR images. Tai et al. <ref type=""bibr"" target=""#b21"">[22]</ref> propose the deep recursive residual network to effectively ey Segmentation Dataset (BSD) <ref type=""bibr"" target=""#b17"">[18]</ref> as the training data. As in <ref type=""bibr"" target=""#b21"">[22]</ref>, to make full use of the training data, we apply data augm RCN <ref type=""bibr"" target=""#b12"">[13]</ref> LapSRN <ref type=""bibr"" target=""#b14"">[15]</ref> DRRN <ref type=""bibr"" target=""#b21"">[22]</ref> MemNet <ref type=""bibr"" target=""#b22"">[23]</ref> IDN  mati N <ref type=""bibr"" target=""#b12"">[13]</ref>, LapSRN <ref type=""bibr"" target=""#b14"">[15]</ref>, DRRN <ref type=""bibr"" target=""#b21"">[22]</ref> and MemNet <ref type=""bibr"" target=""#b22"">[23]</ref>. Tabl d IFC. Meanwhile the inference time substantially exceeds the state-of-the-art methods such as DRRN <ref type=""bibr"" target=""#b21"">[22]</ref> and MemNet <ref type=""bibr"" target=""#b22"">[23]</ref>. This ad><p>By following <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b22"">23]</ref>, we use 91 images f",0
"parameters of network, we use the grouped convolution layer <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b23"">24]</ref> in the second and fourth layers in each enhancement unit wi",0
":id=""formula_3"">l M SE = 1 N N i=1 I i − Îi 2 2 .<label>(4)</label></formula><p>However, Lim et al. <ref type=""bibr"" target=""#b15"">[16]</ref> experimentally demonstrate that training with MSE loss is",0
", non-linear mapping and image reconstruction stages in an end-to-end manner. Afterwards Shi et al. <ref type=""bibr"" target=""#b20"">[21]</ref> propose an efficient sub-pixel convolutional neural networ",0
"e negative scope of LReLU is set as 0.05. We initialize the weights by using the method proposed in <ref type=""bibr"" target=""#b7"">[8]</ref> and the biases are set to zero. The proposed network is opti",0
"control the model parameters, the authors construct a deeply-recursive convolutional network (DRCN) <ref type=""bibr"" target=""#b12"">[13]</ref> by adopting recursive layer. To mitigate training difficul y adopt cascaded network topologies, e.g., VDSR <ref type=""bibr"" target=""#b11"">[12]</ref> and DRC-N <ref type=""bibr"" target=""#b12"">[13]</ref>. In this way, the feature maps of each layer are sent to t aster than several CNN-based SR methods, e.g., VDSR <ref type=""bibr"" target=""#b11"">[12]</ref>, DRCN <ref type=""bibr"" target=""#b12"">[13]</ref>, LapSRN <ref type=""bibr"" target=""#b14"">[15]</ref>, DRRN <r ch utilizes contextual information over large image regions. Another network designed by Kim et al. <ref type=""bibr"" target=""#b12"">[13]</ref>, which has recursive convolution with skip connection to a s transposed convolution can auto-Dataset Scale VDSR <ref type=""bibr"" target=""#b11"">[12]</ref> DRCN <ref type=""bibr"" target=""#b12"">[13]</ref> LapSRN <ref type=""bibr"" target=""#b14"">[15]</ref> DRRN <ref </ref><ref type=""bibr"" target=""#b3"">4]</ref>, VDSR <ref type=""bibr"" target=""#b11"">[12]</ref>, DRC-N <ref type=""bibr"" target=""#b12"">[13]</ref>, LapSRN <ref type=""bibr"" target=""#b14"">[15]</ref>, DRRN <r",0
"aluated on four widely used benchmark datasets: Set5 <ref type=""bibr"" target=""#b0"">[1]</ref>, Set14 <ref type=""bibr"" target=""#b26"">[27]</ref>, BSD100 <ref type=""bibr"" target=""#b17"">[18]</ref>, Urban10",0
"image and thus fails to generate satisfactory prediction for images of other classes. Huang et al. <ref type=""bibr"" target=""#b9"">[10]</ref> extend self-similarity based SR to handle the affine and pe ef type=""bibr"" target=""#b26"">[27]</ref>, BSD100 <ref type=""bibr"" target=""#b17"">[18]</ref>, Urban100 <ref type=""bibr"" target=""#b9"">[10]</ref>. Among these datasets, Set5, Set14 and BSD100 consist of na",0
"is load balancing; its use is a complex trade-off between its cost and its benefits. Work stealing <ref type=""bibr"" target=""#b3"">[4]</ref> can be considered the most widely used load balancing techni",1
"d memory runtimes which are capable of detecting and correcting task faults within parallel regions <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b25"">26]</ref>.</p></div> <div xm",0
"hese concepts as a library. For instance, HPX <ref type=""bibr"" target=""#b16"">[17]</ref> and Charm++ <ref type=""bibr"" target=""#b17"">[18]</ref> are asynchronous GAS runtimes.</p><p>This already very div",0
"hardware have also naturally developed with the emergence of accelerator and GPU computing; StarPU <ref type=""bibr"" target=""#b1"">[2]</ref> is an example of such an environment.</p><p>In addition, tas cies based on list scheduling and performance models are employed in some many-task runtime systems <ref type=""bibr"" target=""#b1"">[2]</ref>. Additionally, hybrid policies, which integrate static and d lelism, it is capable of generating MPI communication from a given task graph and data distribution <ref type=""bibr"" target=""#b1"">[2]</ref>; hence, it is marked with explicit support for distributed m 4,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b21"">22]</ref> or offline <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target",0
"of the API. For instance, some APIs offer arbitrary task graphs via manual task reference counting <ref type=""bibr"" target=""#b11"">[12]</ref>. This does not qualify as support in our classification. A",0
"noisy labeling, previous studies adopt multi-instance learning to consider the noises of instances <ref type=""bibr"" target=""#b13"">(Riedel, Yao, and McCallum 2010;</ref><ref type=""bibr"" target=""#b5"">H te this issue, many studies formulated relation classification as a multi-instance learning problem <ref type=""bibr"" target=""#b13"">(Riedel, Yao, and McCallum 2010;</ref><ref type=""bibr"" target=""#b5"">H the framework of reinforcement learning <ref type=""bibr"" target=""#b16"">(Sutton and Barto 1998;</ref><ref type=""bibr"" target=""#b13"">Narasimhan, Yala, and Barzilay 2016)</ref> and then predicts relation 4</ref> generated by the sentences in NYT<ref type=""foot"" target=""#foot_2"">5</ref> and developed by <ref type=""bibr"" target=""#b13"">(Riedel, Yao, and McCallum 2010)</ref>. There are 522,611 sentences,",1
"igh-quality annotated data.</p><p>In order to obtain large-scale training data, distant supervision <ref type=""bibr"" target=""#b11"">(Mintz et al. 2009</ref>) was proposed by assuming that if two entiti ion classification task.</p><p>The results are compared under the held-out evaluation configuration <ref type=""bibr"" target=""#b11"">(Mintz et al. 2009</ref>) which provides an approximate measure of re",0
"dos Santos, Xiang, and Zhou 2015;</ref><ref type=""bibr"" target=""#b12"">Mooney and Bunescu 2005;</ref><ref type=""bibr"" target=""#b20"">Yang et al. 2016)</ref> including convolutional neural networks, recu",0
"train the word embeddings on the NYT corpus. For entity embedding, we implemented the TransE model <ref type=""bibr"" target=""#b1"">(Bordes et al. 2013</ref>) and trained it on a set of Freebase fact tr",0
"ucial for our method, which is also widely recommended by many other reinforcement learning studies <ref type=""bibr"" target=""#b0"">(Bahdanau et al. 2016)</ref>. Algorithm 2 presents the details of the",0
"ion is built.</p><p>Objective Function We optimize the parameters of PNet using REINFORCE algorithm <ref type=""bibr"" target=""#b24"">(Williams 1992</ref>) and policy gradient methods <ref type=""bibr"" ta",1
"target=""#b25"">(Yang et al. 2016;</ref><ref type=""bibr"" target=""#b27"">Zhou, Wan, and Xiao 2016;</ref><ref type=""bibr"" target=""#b14"">Lin et al. 2017</ref>) use attention mechanisms to build representati selfattention mechanism and a special regularization term are used to construct sentence embedding <ref type=""bibr"" target=""#b14"">(Lin et al. 2017)</ref>.</p><p>The dimension of hidden vectors and th",0
"<ref type=""bibr"" target=""#b20"">(Socher et al. 2013;</ref><ref type=""bibr"" target=""#b19"">2011;</ref><ref type=""bibr"" target=""#b17"">Qian et al. 2015)</ref> use pre-specified parsing trees to build stru",0
"ng is a fundamental problem in AI, and particularly important for natural language processing (NLP) <ref type=""bibr"" target=""#b0"">(Bengio, Courville, and Vincent 2013;</ref><ref type=""bibr"" target=""#b",0
"ias by using corpus level constraints, but is only practical for models with specialized structure. <ref type=""bibr"" target=""#b13"">Kusner et al. (2017)</ref> propose the method based on causal inferen",1
"be closer to ""programmer"" than ""woman"" <ref type=""bibr"" target=""#b1"">(Bolukbasi et al., 2016;</ref><ref type=""bibr"" target=""#b3"">Caliskan et al., 2017)</ref>. Current state-of-art co-reference system mbeddings can encode sexist stereotypes <ref type=""bibr"" target=""#b1"">(Bolukbasi et al., 2016;</ref><ref type=""bibr"" target=""#b3"">Caliskan et al., 2017)</ref>. Similar observations have been made in v",0
"WinoBias.This dataset follows the winograd format <ref type=""bibr"" target=""#b9"">(Hirst, 1981;</ref><ref type=""bibr"" target=""#b20"">Rahman and Ng, 2012;</ref><ref type=""bibr"" target=""#b18"">Peng et al.,",0
"dataset. In combination with methods that remove bias from fixed resources such as word embeddings <ref type=""bibr"" target=""#b1"">(Bolukbasi et al., 2016)</ref>, our data augmentation approach complet ng their bias. To reduce bias from this resource, we replace GloVe embeddings with debiased vectors <ref type=""bibr"" target=""#b1"">(Bolukbasi et al., 2016)</ref>.</p><p>Gender Lists While current neura work has shown that they are severely biased: ""man"" tends to be closer to ""programmer"" than ""woman"" <ref type=""bibr"" target=""#b1"">(Bolukbasi et al., 2016;</ref><ref type=""bibr"" target=""#b3"">Caliskan e ematic instances have been demonstrated, for example, word embeddings can encode sexist stereotypes <ref type=""bibr"" target=""#b1"">(Bolukbasi et al., 2016;</ref><ref type=""bibr"" target=""#b3"">Caliskan e",0
"en proposed to produce ""fair"" classifiers <ref type=""bibr"" target=""#b2"">(Calders et al., 2009;</ref><ref type=""bibr"" target=""#b7"">Feldman et al., 2015;</ref><ref type=""bibr"" target=""#b16"">Misra et al.",0
"ons of deep clustering <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b2"">3]</ref>, deep attractor networks <ref type=""bibr"" target=""#b3"">[4,</r et=""#b17"">[18,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" target=""#b2"">3]</ref>. However, this usually only leads to small improvements, even proach for supervised speech separation is via T-F masking <ref type=""bibr"" target=""#b34"">[35,</ref><ref type=""bibr"" target=""#b2"">3]</ref>. The proposed approach is expected to produce even better sep reconstruction, it is necessary to first obtain a good enough magnitude estimate. Our recent study <ref type=""bibr"" target=""#b2"">[3]</ref> proposed a novel multi-task learning approach combining the gs:</p><formula xml:id=""formula_0"">LDC,classic = V V T − Y Y T 2 F (1)</formula><p>Our recent study <ref type=""bibr"" target=""#b2"">[3]</ref> suggests that an alternative loss function, which whitens th be discussed in Section 3.4. Following <ref type=""bibr"" target=""#b21"">[22]</ref>, our recent study <ref type=""bibr"" target=""#b2"">[3]</ref> proposed a chimera++ network combining the two approaches vi "" target=""#b13"">[14]</ref> only performs iterative reconstruction for each source independently. In <ref type=""bibr"" target=""#b2"">[3]</ref>, we therefore proposed to utilize the MISI algorithm <ref ty es remain fixed during iterations, while the phase of each source are iteratively reconstructed. In <ref type=""bibr"" target=""#b2"">[3]</ref>, the phase reconstruction was only added as a post-processin tained when applying five iterations of Griffin-Lim on each source independently, as is reported in <ref type=""bibr"" target=""#b2"">[3]</ref>. Performing end-to-end optimization using LWA improves the r ates directly in the time domain. Our result is 1.1 dB better than the previous state-of-the-art by <ref type=""bibr"" target=""#b2"">[3]</ref> in terms of both SI-SDR and SDR.</p></div> <div xmlns=""http:",1
"d by deep learning based speech enhancement and separation <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" tar as in <ref type=""bibr"" target=""#b20"">[21]</ref>, proposed in the same conference. A follow-up work <ref type=""bibr"" target=""#b18"">[19]</ref> of <ref type=""bibr"" target=""#b22"">[23]</ref> supplies clea",1
"ent and separation <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" target=""#b2"">3]</ref>. However, this usuall",1
"aphical modeling approaches <ref type=""bibr"" target=""#b7"">[8]</ref>, spectral clustering approaches <ref type=""bibr"" target=""#b8"">[9]</ref>, and CASA methods <ref type=""bibr"" target=""#b9"">[10]</ref>.<",0
"y, one stream of research is focused on iterative methods such as the classic Griffin-Lim algorithm <ref type=""bibr"" target=""#b13"">[14]</ref>, multiple input spectrogram inverse (MISI) <ref type=""bibr are multiple target sources to be separated in each mixture in our study. The Griffin-Lim algorithm <ref type=""bibr"" target=""#b13"">[14]</ref> only performs iterative reconstruction for each source ind",0
"/head><p>Sigmoidal units are dominantly used in the output layer of deep learning based T-F masking <ref type=""bibr"" target=""#b35"">[36,</ref><ref type=""bibr"" target=""#b34"">35]</ref>, partly because th arget=""#b36"">[37]</ref>, such as the IRM <ref type=""bibr"" target=""#b37"">[38]</ref> and its variants <ref type=""bibr"" target=""#b35"">[36]</ref>. Restricting the possible values of the T-F mask to lie in",0
"ve adversarial networks <ref type=""bibr"" target=""#b32"">[33]</ref>, or encoder-decoder architectures <ref type=""bibr"" target=""#b33"">[34]</ref>. Although they are promising approaches, the current state",0
"br"" target=""#b38"">39]</ref>, as well as the SDR metric computed using the bss eval sources software <ref type=""bibr"" target=""#b39"">[40]</ref> because it is used by other groups. We believe SI-SDR is a",0
""" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b6"">7]</ref> have dramatically improved the performance of single-channel a is to train a maskinference network to minimize the minimum loss over all permutations. Following <ref type=""bibr"" target=""#b6"">[7]</ref>, the phase-sensitive mask (PSM) <ref type=""bibr"" target=""#b2",0
"pproaches, the current stateof-the-art approach for supervised speech separation is via T-F masking <ref type=""bibr"" target=""#b34"">[35,</ref><ref type=""bibr"" target=""#b2"">3]</ref>. The proposed approa sed in the output layer of deep learning based T-F masking <ref type=""bibr"" target=""#b35"">[36,</ref><ref type=""bibr"" target=""#b34"">35]</ref>, partly because they can model well data with bi-modal dist",0
"all permutations. Following <ref type=""bibr"" target=""#b6"">[7]</ref>, the phase-sensitive mask (PSM) <ref type=""bibr"" target=""#b20"">[21]</ref> is used as the training target. It is common in phase-sens e mask for minimum squared error in the complex spectrum domain, when using the noisy phases, as in <ref type=""bibr"" target=""#b20"">[21]</ref>, proposed in the same conference. A follow-up work <ref ty source c for example. The real component is equivalent to the earlier proposed phase-sensitive mask <ref type=""bibr"" target=""#b20"">[21]</ref>, which contains patterns clearly predictable from energy-b o known as the FFT mask in <ref type=""bibr"" target=""#b37"">[38]</ref> or the ideal amplitude mask in <ref type=""bibr"" target=""#b20"">[21]</ref>). Clearly, this mask can go beyond one, because the underl r"" target=""#b20"">[21]</ref>, which contains patterns clearly predictable from energy-based features <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b24"">25]</ref>. However, recent s",0
"onsistency problem <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b12"">13]</ref>, especially for speech processing, where there is typically ting from the mixture phase and a good estimated magnitude by iteratively performing STFT and iSTFT <ref type=""bibr"" target=""#b12"">[13]</ref>. There are some previous attempts at naively applying such",0
"use phase is difficult to estimate. It is well-known that this incurs a phase inconsistency problem <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" ta",0
"[8]</ref>, spectral clustering approaches <ref type=""bibr"" target=""#b8"">[9]</ref>, and CASA methods <ref type=""bibr"" target=""#b9"">[10]</ref>.</p><p>However, all of these conduct separation on the magn",0
"=""bibr"" target=""#b34"">35]</ref>, partly because they can model well data with bi-modal distribution <ref type=""bibr"" target=""#b36"">[37]</ref>, such as the IRM <ref type=""bibr"" target=""#b37"">[38]</ref>",0
"arget=""#b91"">94,</ref><ref type=""bibr"" target=""#b55"">58,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b60"">63]</ref>. Here we make an attempt to actually find this structure. W arget=""#b91"">94,</ref><ref type=""bibr"" target=""#b55"">58,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b60"">63]</ref>. Unlike multi-task learning, we explicitly model the relati",1
""" target=""#b61"">64,</ref><ref type=""bibr"" target=""#b54"">57]</ref>, un/semi/self-supervised learning <ref type=""bibr"" target=""#b18"">[20,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" targ",0
"ed to recent works <ref type=""bibr"" target=""#b70"">[73,</ref><ref type=""bibr"" target=""#b67"">70,</ref><ref type=""bibr"" target=""#b46"">48,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" tar ucture among tasks <ref type=""bibr"" target=""#b87"">[90,</ref><ref type=""bibr"" target=""#b94"">97,</ref><ref type=""bibr"" target=""#b46"">48,</ref><ref type=""bibr"" target=""#b70"">73,</ref><ref type=""bibr"" tar get=""#b46"">48,</ref><ref type=""bibr"" target=""#b70"">73,</ref><ref type=""bibr"" target=""#b67"">70,</ref><ref type=""bibr"" target=""#b46"">48,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" tar -task learning targets developing systems that can provide multiple outputs for an input in one run <ref type=""bibr"" target=""#b46"">[48,</ref><ref type=""bibr"" target=""#b15"">16]</ref>. Multi-task learni",0
"et=""#b75"">[78,</ref><ref type=""bibr"" target=""#b21"">23,</ref><ref type=""bibr"" target=""#b20"">22,</ref><ref type=""bibr"" target=""#b64"">67,</ref><ref type=""bibr"" target=""#b80"">83]</ref>, transfer learning",0
""">[41]</ref> from known methods <ref type=""bibr"" target=""#b98"">[101,</ref><ref type=""bibr"">55,</ref><ref type=""bibr"" target=""#b52"">54,</ref><ref type=""bibr"" target=""#b72"">75]</ref>. See the supplement",0
"19"">(Schuster and Nakajima, 2012;</ref><ref type=""bibr"" target=""#b4"">Chitnis and DeNero, 2015;</ref><ref type=""bibr"" target=""#b21"">Sennrich et al., 2016;</ref><ref type=""bibr"" target=""#b31"">Wu et al., ref type=""table"">1</ref>: Multiple subword sequences encoding the same sentence ""Hello World"" (BPE) <ref type=""bibr"" target=""#b21"">(Sennrich et al., 2016)</ref> is a de facto standard subword segmenta type=""bibr"" target=""#b16"">(Nong et al., 2009)</ref>, where T is the size of the corpus. Similar to <ref type=""bibr"" target=""#b21"">(Sennrich et al., 2016)</ref>, we do not consider subwords that cross 2004)</ref>. The same mark is used in Table <ref type=""table"">4</ref> and<ref type=""table"">6</ref>. <ref type=""bibr"" target=""#b21"">(Sennrich et al., 2016)</ref> and our unigram model with or without s bword segmentations with language model 3.1 Byte-Pair-Encoding (BPE)</p><p>Byte-Pair-Encoding (BPE) <ref type=""bibr"" target=""#b21"">(Sennrich et al., 2016;</ref><ref type=""bibr"" target=""#b19"">Schuster",1
"up rare words into subword units <ref type=""bibr"" target=""#b19"">(Schuster and Nakajima, 2012;</ref><ref type=""bibr"" target=""#b4"">Chitnis and DeNero, 2015;</ref><ref type=""bibr"" target=""#b21"">Sennrich",0
">Neural Machine Translation (NMT) models <ref type=""bibr"" target=""#b1"">(Bahdanau et al., 2014;</ref><ref type=""bibr"" target=""#b13"">Luong et al., 2015;</ref><ref type=""bibr"" target=""#b31"">Wu et al., 20",0
"bstrings can be enumerated in O(T ) time and O(20T ) space with the Enhanced Suffix Array algorithm <ref type=""bibr"" target=""#b16"">(Nong et al., 2009)</ref>, where T is the size of the corpus. Similar",0
"the original inputs. There are a couple of studies that employ DAEs in natural language processing. <ref type=""bibr"" target=""#b12"">(Lample et al., 2017;</ref><ref type=""bibr"">Artetxe et al., 2017)</re",0
"bibr"" target=""#b43"">(Xiong et al., 2016;</ref><ref type=""bibr"" target=""#b31"">Seo et al., 2016;</ref><ref type=""bibr"" target=""#b2"">Bahdanau et al., 2015)</ref>. The resulting representation is encoded l language.</p><p>In this work, we consider attention-based neural machine translation (NMT) models <ref type=""bibr"" target=""#b2"">Bahdanau et al. (2015)</ref>; <ref type=""bibr"" target=""#b24"">Luong et",1
"type=""bibr"" target=""#b19"">(Kim, 2014;</ref><ref type=""bibr"" target=""#b9"">Gehring et al., 2017;</ref><ref type=""bibr"" target=""#b36"">Vaswani et al., 2017b;</ref><ref type=""bibr"" target=""#b32"">Shen et al",1
"ly, we utilize the publicly available codebase<ref type=""foot"" target=""#foot_4"">3</ref> provided by <ref type=""bibr"" target=""#b25"">Luong et al. (2017)</ref>, which replicates the Google's NMT (GNMT) s f> (4.5M sentence pairs). All data have been tokenized and split into subword units as described in <ref type=""bibr"" target=""#b25"">Luong et al. (2017)</ref>. All models share the same hyperparameters<",1
"ery and context. Then we learn the interactions between context and question by standard attentions <ref type=""bibr"" target=""#b43"">(Xiong et al., 2016;</ref><ref type=""bibr"" target=""#b31"">Seo et al., to-context attention, such as BiDaF <ref type=""bibr"" target=""#b31"">(Seo et al., 2016)</ref> and DCN <ref type=""bibr"" target=""#b43"">(Xiong et al., 2016)</ref>. Empirically, we find that, the DCN attent ing <ref type=""bibr"">(Wang et al., 2016)</ref> 65.5 / 75.1 70.4 / 78.8 Dynamic Coattention Networks <ref type=""bibr"" target=""#b43"">(Xiong et al., 2016)</ref> 66.2 / 75.9 66.2 / 75.9 FastQA <ref type="" BiDAF (Seo et al., 2016)</ref>, r-net <ref type=""bibr"" target=""#b38"">(Wang et al., 2017)</ref>, DCN <ref type=""bibr"" target=""#b43"">(Xiong et al., 2016)</ref>, ReasoNet <ref type=""bibr"" target=""#b33"">( our experiments and previous works, such as <ref type=""bibr"" target=""#b31"">(Seo et al., 2016;</ref><ref type=""bibr"" target=""#b43"">Xiong et al., 2016;</ref><ref type=""bibr"" target=""#b38"">Wang et al.,",1
"uccessful combination of these two ingredients is the Bidirectional Attention Flow (BiDAF) model by <ref type=""bibr"" target=""#b31"">Seo et al. (2016)</ref>, which achieve strong results on the SQuAD da e the word embedding and the convolution output of character embedding of x respectively. Following <ref type=""bibr"" target=""#b31"">Seo et al. (2016)</ref>, we also adopt a two-layer highway network <r 2"">query-to-context attention is B = S • S T • C T .</formula><p>4. Model Encoder Layer. Similar to <ref type=""bibr"" target=""#b31"">Seo et al. (2016)</ref>, the input of this layer at each position is mple in SQuAD is labeled with a span in the context containing the answer. We adopt the strategy of <ref type=""bibr"" target=""#b31"">Seo et al. (2016)</ref> to predict the probability of each position i ontext and question by standard attentions <ref type=""bibr"" target=""#b43"">(Xiong et al., 2016;</ref><ref type=""bibr"" target=""#b31"">Seo et al., 2016;</ref><ref type=""bibr"" target=""#b2"">Bahdanau et al., nce. As a simple comparison, our model can achieve the same accuracy (77.0 F1 score) as BiDAF model <ref type=""bibr"" target=""#b31"">(Seo et al., 2016)</ref> within 3 hours training that otherwise shoul el to train for 18 hours, it achieves an F1 score of 82.7 on the dev set, which is much better than <ref type=""bibr"" target=""#b31"">(Seo et al., 2016)</ref>, and is on par with best published results.< la_0"">A = S • Q T ∈ R n×d .</formula><p>The similarity function used here is the trilinear function <ref type=""bibr"" target=""#b31"">(Seo et al., 2016)</ref>:</p><formula xml:id=""formula_1"">f (q, c) = W Most high performing models additionally use some form of query-to-context attention, such as BiDaF <ref type=""bibr"" target=""#b31"">(Seo et al., 2016)</ref> and DCN <ref type=""bibr"" target=""#b43"">(Xion astQA <ref type=""bibr"" target=""#b40"">(Weissenborn et al., 2017)</ref> 68.4 / 77.1 68.4 / 77.1 BiDAF <ref type=""bibr"" target=""#b31"">(Seo et al., 2016)</ref> 68.0 / 77.3 68.0 / 77.3 SEDT <ref type=""bibr =""tab_4""><head>Table 4 :</head><label>4</label><figDesc>Speed comparison between our model and BiDAF<ref type=""bibr"" target=""#b31"">(Seo et al., 2016)</ref> on SQuAD dataset.</figDesc><table><row><cell , 2017a)</ref> 33.9 44.8 DCR <ref type=""bibr"" target=""#b45"">(Yu et al., 2016)</ref> 37.8 45.1 BiDAF <ref type=""bibr"" target=""#b31"">(Seo et al., 2016)</ref> 34.3 45.7 jNet <ref type=""bibr"" target=""#b46 requent submissions. According to the observations from our experiments and previous works, such as <ref type=""bibr"" target=""#b31"">(Seo et al., 2016;</ref><ref type=""bibr"" target=""#b43"">Xiong et al.,",1
"ing of each encoder layer consisting of sin and cos functions at varying wavelengths, as defined in <ref type=""bibr"" target=""#b35"">(Vaswani et al., 2017a)</ref>. Each sub-layer after the positional en a block is 4. For the self-attention-layer, we adopt the multi-head attention mechanism defined in <ref type=""bibr"" target=""#b35"">(Vaswani et al., 2017a)</ref> which, for each position in the input, ard-net) inside the encoder structure is wrapped inside a residual block.</p><p>used extensively in <ref type=""bibr"" target=""#b35"">Vaswani et al. (2017a)</ref>, the combination of convolutions and sel",1
"by full convolution or full attention architectures <ref type=""bibr"" target=""#b19"">(Kim, 2014;</ref><ref type=""bibr"" target=""#b9"">Gehring et al., 2017;</ref><ref type=""bibr"" target=""#b36"">Vaswani et a",0
"ibr"" target=""#b24"">Luong et al. (2015)</ref>, which have demonstrated excellent translation quality <ref type=""bibr"" target=""#b42"">Wu et al. (2016)</ref>, as the core models of our data augmentation p pe=""bibr"" target=""#b25"">Luong et al. (2017)</ref>, which replicates the Google's NMT (GNMT) systems <ref type=""bibr"" target=""#b42"">Wu et al. (2016)</ref>. We train 4-layer GNMT models on the public WM",0
"ing comprehension models such as <ref type=""bibr"" target=""#b40"">Weissenborn et al. (2017)</ref> and <ref type=""bibr"" target=""#b3"">Chen et al. (2017)</ref>. We use C and Q to denote the encoded context bibr"" target=""#b43"">Xiong et al., 2016;</ref><ref type=""bibr"" target=""#b38"">Wang et al., 2017;</ref><ref type=""bibr"" target=""#b3"">Chen et al., 2017)</ref>, the validation score is well correlated with t <ref type=""bibr"" target=""#b33"">(Shen et al., 2017b)</ref> 69.1 / 78.9 70.6 / 79.4 Document Reader <ref type=""bibr"" target=""#b3"">(Chen et al., 2017)</ref> 70.0 / 79.0 70.7 / 79.4 Ruminating Reader <r s respectively, as such layer numbers fall into the usual range of the reading comprehension models <ref type=""bibr"" target=""#b3"">(Chen et al., 2017)</ref>. All of these LSTMs have hidden size 128. Th ., 2016)</ref>, ReasoNet <ref type=""bibr"" target=""#b33"">(Shen et al., 2017b)</ref>, Document Reader <ref type=""bibr"" target=""#b3"">(Chen et al., 2017)</ref>, Interactive AoA Reader <ref type=""bibr"" tar",0
"ave been made to replace the recurrent networks by full convolution or full attention architectures <ref type=""bibr"" target=""#b19"">(Kim, 2014;</ref><ref type=""bibr"" target=""#b9"">Gehring et al., 2017;<",0
"pe=""bibr"" target=""#b30"">(Rajpurkar et al., 2016)</ref> 40.4 / 51.0 40.4 / 51.0 Dynamic Chunk Reader <ref type=""bibr"" target=""#b45"">(Yu et al., 2016)</ref> 62.5 / 71.0 62.5 / 71.0 Match-LSTM with Ans-P , 2016)</ref> 27.3 39.0 SEDT <ref type=""bibr"" target=""#b22"">(Liu et al., 2017a)</ref> 33.9 44.8 DCR <ref type=""bibr"" target=""#b45"">(Yu et al., 2016)</ref> 37.8 45.1 BiDAF <ref type=""bibr"" target=""#b31",0
"ata for down-stream tasks, in this case, the question answering (QA) task. It is worth to note that <ref type=""bibr"" target=""#b8"">(Dong et al., 2017)</ref> use paraphrasing techniques to improve QA; h",0
"/ 77.3 SEDT <ref type=""bibr"" target=""#b22"">(Liu et al., 2017a)</ref> 68.1 / 77.5 68.5 / 78.0 RaSoR <ref type=""bibr"" target=""#b21"">(Lee et al., 2016)</ref> 70.8 / 78.7 69.6 / 77.7 FastQAExt <ref type= 37.9 47.0 Ruminating <ref type=""bibr"" target=""#b10"">(Gong &amp; Bowman, 2017)</ref> 37.4 47.7 RaSOR <ref type=""bibr"" target=""#b21"">(Lee et al., 2016)</ref> 39.5 49.5 MPCM <ref type=""bibr"">(Wang et al.",0
"ype=""bibr"" target=""#b31"">Seo et al. (2016)</ref>, which achieve strong results on the SQuAD dataset <ref type=""bibr"" target=""#b30"">(Rajpurkar et al., 2016)</ref>. A weakness of these models is that th del and the data augmentation technique. We will primarily benchmark our model on the SQuAD dataset <ref type=""bibr"" target=""#b30"">(Rajpurkar et al., 2016)</ref>, considered to be one of the most comp EXPERIMENTAL SETTINGS</head><p>Dataset. We consider the Stanford Question Answering Dataset (SQuAD) <ref type=""bibr"" target=""#b30"">(Rajpurkar et al., 2016)</ref> for machine reading comprehension.<ref e the test data is hidden that one has to submit the code to a Codalab and work with the authors of <ref type=""bibr"" target=""#b30"">(Rajpurkar et al., 2016)</ref> to retrieve the final test score. In o with a stack of bidirectional Published 12 LeaderBoard 13 Single Model EM / F1 EM / F1 LR Baseline <ref type=""bibr"" target=""#b30"">(Rajpurkar et al., 2016)</ref> 40.4 / 51.0 40.4 / 51.0 Dynamic Chunk popularity can be attributed to an increase in publicly available annotated datasets, such as SQuAD <ref type=""bibr"" target=""#b30"">(Rajpurkar et al., 2016)</ref>, TriviaQA <ref type=""bibr"" target=""#b1 ut also make it robust to the adversarial sentences.</p><p>Single Model AddSent AddOneSent Logistic <ref type=""bibr"" target=""#b30"">(Rajpurkar et al., 2016)</ref> 23.2 30.4 Match <ref type=""bibr"" targe",0
"/ref>. For simple tasks such as text classification, with reinforcement learning techniques, models <ref type=""bibr"" target=""#b44"">(Yu et al., 2017)</ref> have been proposed to skip irrelevant tokens",0
"g <ref type=""bibr"" target=""#b31"">Seo et al. (2016)</ref>, we also adopt a two-layer highway network <ref type=""bibr"" target=""#b34"">(Srivastava et al., 2015)</ref> on top of this representation. For si",0
"h from the input to output of each block, where layernorm indicates layer-normalization proposed in <ref type=""bibr"" target=""#b1"">(Ba et al., 2016)</ref>. The total number of encoder blocks is 1. Note",0
"right of Figure <ref type=""figure"">1</ref>. We use depthwise separable convolutions (Chollet, 2016) <ref type=""bibr"" target=""#b18"">(Kaiser et al., 2017)</ref> rather than traditional ones, as we obser",0
"modeling long dependencies, although this is somewhat alleviated by the use of Gated Recurrent Unit <ref type=""bibr"" target=""#b5"">(Chung et al., 2014)</ref> or Long Short Term Memory architectures <re",0
"d data has the same syntactic structure as the original data, so they are not sufficiently diverse. <ref type=""bibr"" target=""#b48"">Zhou et al. (2017)</ref> improved the diversity of the SQuAD data by",0
"6 F1 score on the test set, which is significantly better than the best published result of 81.8 by <ref type=""bibr"" target=""#b14"">Hu et al. (2017)</ref>. <ref type=""foot"" target=""#foot_3"">2</ref> We answer the associated questions. As the text could be long, we adopt the data processing similar to <ref type=""bibr"" target=""#b14"">Hu et al. (2017)</ref>; <ref type=""bibr"" target=""#b17"">Joshi et al. ( ve AoA Reader <ref type=""bibr"" target=""#b7"">(Cui et al., 2017)</ref> and Reinforced Mnemonic Reader <ref type=""bibr"" target=""#b14"">(Hu et al., 2017)</ref>.</p><p>Recurrent Neural Networks (RNNs) have ef> 40.3 50.0 ReasoNet <ref type=""bibr"" target=""#b33"">(Shen et al., 2017b)</ref> 39.4 50.3 Mnemonic <ref type=""bibr"" target=""#b14"">(Hu et al., 2017)</ref> 46.6 56.0 QANet 45.2 55.7</p><p>Table <ref ty edia.</p><p>According to the previous work <ref type=""bibr"" target=""#b17"">(Joshi et al., 2017;</ref><ref type=""bibr"" target=""#b14"">Hu et al., 2017;</ref><ref type=""bibr"" target=""#b27"">Pan et al., 2017",0
"xt <ref type=""bibr"" target=""#b40"">(Weissenborn et al., 2017)</ref> 70.8 / 78.9 70.8 / 78.9 ReasoNet <ref type=""bibr"" target=""#b33"">(Shen et al., 2017b)</ref> 69.1 / 78.9 70.6 / 79.4 Document Reader <r >(Wang et al., 2017)</ref>, DCN <ref type=""bibr"" target=""#b43"">(Xiong et al., 2016)</ref>, ReasoNet <ref type=""bibr"" target=""#b33"">(Shen et al., 2017b)</ref>, Document Reader <ref type=""bibr"" target="" ee et al., 2016)</ref> 39.5 49.5 MPCM <ref type=""bibr"">(Wang et al., 2016)</ref> 40.3 50.0 ReasoNet <ref type=""bibr"" target=""#b33"">(Shen et al., 2017b)</ref> 39.4 50.3 Mnemonic <ref type=""bibr"" target",0
">) or instrinsic paraphrase evaluations <ref type=""bibr"" target=""#b41"">Wieting et al. (2017)</ref>; <ref type=""bibr"" target=""#b26"">Mallinson et al. (2017)</ref>. Our approach is a novel application of",0
"slation task Sennrich et al. ( <ref type=""formula"">2016</ref>) or instrinsic paraphrase evaluations <ref type=""bibr"" target=""#b41"">Wieting et al. (2017)</ref>; <ref type=""bibr"" target=""#b26"">Mallinson",0
"l., 2016)</ref>, TriviaQA <ref type=""bibr"" target=""#b17"">(Joshi et al., 2017)</ref>, CNN/Daily News <ref type=""bibr"" target=""#b11"">(Hermann et al., 2015)</ref>, WikiReading <ref type=""bibr"" target=""#b",0
"f>, Document Reader <ref type=""bibr"" target=""#b3"">(Chen et al., 2017)</ref>, Interactive AoA Reader <ref type=""bibr"" target=""#b7"">(Cui et al., 2017)</ref> and Reinforced Mnemonic Reader <ref type=""bib",0
"te networks do not always transfer to the target model, especially when conducting targeted attacks <ref type=""bibr"" target=""#b6"">(Chen et al., 2017;</ref><ref type=""bibr"" target=""#b19"">Narodytska &am :</p><p>Black-box setting. In this paper, we use the definition of black-box access as query access <ref type=""bibr"" target=""#b6"">(Chen et al., 2017;</ref><ref type=""bibr"" target=""#b15"">Liu et al., 20 orders of magnitude more query-efficient than previous methods based on gradient estimation such as <ref type=""bibr"" target=""#b6"">Chen et al. (2017)</ref>. We show that our approach reliably produces imated by querying the classifier rather than computed by autodifferentiation. This idea is used in <ref type=""bibr"" target=""#b6"">Chen et al. (2017)</ref>, where the gradient is estimated via pixel-by =""http://www.tei-c.org/ns/1.0""><head n=""4.1.2."">BLACK-BOX ATTACKS WITH GRADIENT</head><p>ESTIMATION <ref type=""bibr"" target=""#b6"">Chen et al. (2017)</ref> explore black-box gradient estimation methods mpability of the 2 and ∞ metric as well as the fixed-budget nature of the optimization algorithm in <ref type=""bibr"" target=""#b6"">Chen et al. (2017)</ref>, our method takes far fewer queries to genera",1
"finition of black-box access as query access <ref type=""bibr"" target=""#b6"">(Chen et al., 2017;</ref><ref type=""bibr"" target=""#b15"">Liu et al., 2017;</ref><ref type=""bibr"" target=""#b11"">Hayes &amp; Dan rediction API with small datasets like MNIST and successfully demonstrated an untargeted attack. As <ref type=""bibr"" target=""#b15"">Liu et al. (2017)</ref> demonstrated, it is more difficult to transfe ularly when attacking models trained on large datasets like ImageNet. Using ensemble-based methods, <ref type=""bibr"" target=""#b15"">Liu et al. (2017)</ref> overcame these limitations to attack the Clar",1
"ggio et al., 2013)</ref>. These adversarial examples can potentially be exploited in the real world <ref type=""bibr"" target=""#b14"">(Kurakin et al., 2016;</ref><ref type=""bibr"" target=""#b0"">Athalye et",0
"Carlini &amp; Wagner, 2017;</ref><ref type=""bibr"" target=""#b18"">Moosavi-Dezfooli et al., 2016;</ref><ref type=""bibr"" target=""#b17"">Moosavi-Dezfooli et al., 2017;</ref><ref type=""bibr"" target=""#b11"">Ha",0
"tentially be exploited in the real world <ref type=""bibr"" target=""#b14"">(Kurakin et al., 2016;</ref><ref type=""bibr"" target=""#b0"">Athalye et al., 2017;</ref><ref type=""bibr"" target=""#b26"">Sharif et al",0
"tiway image classifiers under a wider set of threat models rather than binary classifiers for PDFs. <ref type=""bibr"" target=""#b21"">Nguyen et al. (2014)</ref> is another work that uses genetic algorith",0
"is slightly perturbed to cause misclassification.</p><p>Prior work considers various threat models <ref type=""bibr"" target=""#b23"">(Papernot et al., 2016b;</ref><ref type=""bibr"" target=""#b4"">Carlini & s for generating adversarial examples <ref type=""bibr"" target=""#b9"">Goodfellow et al. (2015)</ref>; <ref type=""bibr"" target=""#b23"">Papernot et al. (2016b)</ref>; <ref type=""bibr"" target=""#b16"">Madry e",0
"predicted probabilities. Note that this is a generalization of the decision-only setting defined in <ref type=""bibr"" target=""#b3"">Brendel et al. (2018)</ref>, where k = 1, and the attacker only has ac 0""><head n=""4.2."">Adversarial attacks with limited information</head><p>Our work is concurrent with <ref type=""bibr"" target=""#b3"">Brendel et al. (2018)</ref>, which also explores the label-only case u",0
"ize to inverted images, rather than to demonstrate a novel attack or to consider a new threat model <ref type=""bibr"" target=""#b12"">(Hosseini et al., 2017)</ref>.</p></div> <div xmlns=""http://www.tei-c",0
"presented an intelligent video-based system for automated detection of suicide by hanging attempts <ref type=""bibr"" target=""#b1"">(Bouachir and Noumeir, 2016)</ref>. Unlike in <ref type=""bibr"" target=",1
"urveillance systems to automatically detect suicidal behaviors and trigger an alarm. In this sense, <ref type=""bibr"" target=""#b9"">(Lee et al., 2014)</ref> presented a method for automatically analyzin ide by hanging attempts <ref type=""bibr"" target=""#b1"">(Bouachir and Noumeir, 2016)</ref>. Unlike in <ref type=""bibr"" target=""#b9"">(Lee et al., 2014)</ref>, we performed our experiments on a large data",1
"ng and Tian, 2014) were proposed to generalize the concept of interest points and local descriptors <ref type=""bibr"" target=""#b0"">(Bay et al., 2006)</ref>. Despite their effectiveness to overcome some",0
"training data, and rebalance the latter using the SMOTE: Synthetic Minority Over-sampling Technique <ref type=""bibr"" target=""#b2"">(Chawla et al., 2002)</ref> For each experiment, we applied the algori . To overcome this problem, we propose to use the SMOTE: Synthetic Minority Over-sampling Technique <ref type=""bibr"" target=""#b2"">(Chawla et al., 2002)</ref> in order to rebalance the training data se",0
"and labeling the individual body parts. Numerous type of features have been used such as silhouette <ref type=""bibr"" target=""#b6"">(Gouiaa and Meunier, 2015)</ref>, contour <ref type=""bibr"" target=""#b3",0
"rn by actively suicidal inmates <ref type=""bibr"" target=""#b7"">(Hayes, 2013)</ref>. A top door alarm <ref type=""bibr"" target=""#b4"">(Cook, 2011)</ref>, which triggers an alarm if the door is used as a l",0
"ive clothing (Safety smocks and blankets) has been designed to be worn by actively suicidal inmates <ref type=""bibr"" target=""#b7"">(Hayes, 2013)</ref>. A top door alarm <ref type=""bibr"" target=""#b4"">(C",0
"een used such as silhouette <ref type=""bibr"" target=""#b6"">(Gouiaa and Meunier, 2015)</ref>, contour <ref type=""bibr"" target=""#b3"">(Cheema et al., 2011)</ref> or optical flow <ref type=""bibr"" target=""#",0
"addition, about one-third of prison deaths were caused by suicide in the USA between 2000 and 2010 <ref type=""bibr"" target=""#b10"">(Noonan and Ginder, 2013)</ref>. Generally, hanging is the most commo",0
"figuration of human body structure. This representation is derived from the principle, published in <ref type=""bibr"" target=""#b8"">(Johansson, 1973)</ref>, explaining how humans observe actions. This w",0
"chine learning tasks, deep neural networks have been shown to be susceptible to adversarial attacks <ref type=""bibr"" target=""#b20"">(Szegedy et al., 2014;</ref><ref type=""bibr"" target=""#b4"">Goodfellow assification, these perturbations cause the legitimate sample to be misclassified at inference time <ref type=""bibr"" target=""#b20"">(Szegedy et al., 2014;</ref><ref type=""bibr"" target=""#b4"">Goodfellow , adversarial training which augments the training data of the classifier with adversarial examples <ref type=""bibr"" target=""#b20"">(Szegedy et al., 2014;</ref><ref type=""bibr"" target=""#b4"">Goodfellow xamples designed to fool the substitute often end up being misclassified by the targeted classifier <ref type=""bibr"" target=""#b20"">(Szegedy et al., 2014;</ref><ref type=""bibr"" target=""#b19"">Papernot e ch to defend against adversarial noise is to augment the training dataset with adversarial examples <ref type=""bibr"" target=""#b20"">(Szegedy et al., 2014;</ref><ref type=""bibr"" target=""#b4"">Goodfellow",1
"pernot et al., 2016d)</ref> trains the classifier in two rounds using a variant of the distillation <ref type=""bibr"" target=""#b7"">(Hinton et al., 2014)</ref> method. This has the desirable effect of l",0
"o make the model more robust to white-box attacks than to black-box attacks due to gradient masking <ref type=""bibr"" target=""#b17"">(Papernot et al., 2016c;</ref><ref type=""bibr"">2017;</ref><ref type=""",0
"bibr"" target=""#b5"">(Gulrajani et al., 2017)</ref>, and alternative formulations have been proposed. <ref type=""bibr"" target=""#b1"">Arjovsky et al. (2017)</ref> introduced Wasserstein GANs (WGANs) which",0
"target=""#b10"">(LeCun et al., 1998)</ref> and the Fashion-MNIST (F-MNIST) clothing articles dataset <ref type=""bibr"" target=""#b22"">(Xiao et al., 2017)</ref>.</p><p>Both datasets consist of 60, 000 tra",0
"cases. In recent years, unsupervised learning has received increasing attention from the community <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b1"">2]</ref>.</p><p>Our novel appro metric in an unsupervised fashion, without any human annotations.</p><p>Exemplar CNN. Exemplar CNN <ref type=""bibr"" target=""#b4"">[5]</ref> appears similar to our work. The fundamental difference is t tecture <ref type=""bibr"" target=""#b17"">[18]</ref> in their original papers, except for exemplar CNN <ref type=""bibr"" target=""#b4"">[5]</ref>, whose results are reported with ResNet-101 <ref type=""bibr""",1
"s can also scale to millions. Popular techniques to reduce computation include hierarchical softmax <ref type=""bibr"" target=""#b25"">[26]</ref>, noise-contrastive estimation (NCE) <ref type=""bibr"" targe",0
"t on metric learning for face recognition is normalization <ref type=""bibr"" target=""#b34"">[35,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b42"">43]</ref>, which we also util",0
"pe=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b3"">4]</ref> and variational auto-encoder <ref type=""bibr"" target=""#b13"">[14]</ref> improve both generative qualities and feature learning.</p",0
"-CNN <ref type=""bibr"" target=""#b6"">[7]</ref> with AlexNet and VGG16 architectures, and Faster R-CNN <ref type=""bibr"" target=""#b31"">[32]</ref> with ResNet-50. When fine-tuning Fast R-CNN, the learning",0
"ome video generation methods have dealt with this problem by generating the entire sequence at once <ref type=""bibr"" target=""#b24"">[25]</ref> or in small batches <ref type=""bibr"" target=""#b19"">[20]</r d to handle videos <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b23"">24]</ref>.</p><p>Straight-for ibr"" target=""#b23"">24]</ref>.</p><p>Straight-forward adaptations of GANs for videos are proposed in <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b19"">20]</ref>, replacing the 2D",1
"enerating the entire sequence at once <ref type=""bibr"" target=""#b24"">[25]</ref> or in small batches <ref type=""bibr"" target=""#b19"">[20]</ref>. However, this introduces a lag in the generation process, to capture temporal dependencies but requires fixed length videos. This limitation was overcome in <ref type=""bibr"" target=""#b19"">[20]</ref> but constraints need to be imposed in the latent space to ght-forward adaptations of GANs for videos are proposed in <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b19"">20]</ref>, replacing the 2D convolutional layers with 3D convolutiona",1
") and GANs in its generating network and a 3D CNN as a sequence discriminator. Finally, Chen et al. <ref type=""bibr"" target=""#b3"">[4]</ref> propose a GAN-based encoder-decoder architecture that uses C",0
"with separate latent spaces for motion and content. This relies on the empirical evidence shown in <ref type=""bibr"" target=""#b17"">[18]</ref> that GANs perform better when the latent space is disentan",0
"sing the word error rate (WER) achieved by a pre-trained lip-reading model. We use the LipNet model <ref type=""bibr"" target=""#b1"">[2]</ref>, which surpasses the performance of human lipreaders on the",0
"person-specific database and combining them to form a video <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b21"">22]</ref>. Regardless of which approach is used these methods are sub ed on recurrent neural networks (RNNs) have been proposed in <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b21"">22]</ref>, producing realistic results but are subject dependent and re ""bin"" is followed by the word ""blue"".   The works that are closest to ours are those proposed in <ref type=""bibr"" target=""#b21"">[22]</ref> and <ref type=""bibr"" target=""#b4"">[5]</ref>. The former me",0
"et=""#b15"">[16,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b23"">24]</ref>.</p><p>Straight-forward adaptations of GANs for videos are be imposed in the latent space to generate consistent videos.</p><p>The MoCoGAN system proposed in <ref type=""bibr"" target=""#b23"">[24]</ref> uses an RNN-based generator, with separate latent spaces f of the spoken words. We verify the identity of the speaker using the average content distance (ACD) <ref type=""bibr"" target=""#b23"">[24]</ref>, which measures the average Euclidean distance of the stil",0
"measures the average Euclidean distance of the still image representation, obtained using OpenFace <ref type=""bibr"" target=""#b0"">[1]</ref>, from the representation of the generated frames. The accura",0
"dden Markov models (HMMs) to capture the dynamics of the video and speech sequences. Simons and Cox <ref type=""bibr"" target=""#b20"">[21]</ref> used vector quantization to achieve a compact representati",0
"n the presence of edges in the image and the frequency domain blurriness measure (FDBM) proposed in <ref type=""bibr"" target=""#b7"">[8]</ref>, which is based on the spectrum of the image. For these metr",0
"raining graph to boost accuracy without affecting the inference graph, including auxiliary training <ref type=""bibr"" target=""#b18"">[19]</ref>, multi-task learning <ref type=""bibr"" target=""#b3"">[4,</re nvergence of deep networks by adding auxiliary classifiers connected to certain intermediate layers <ref type=""bibr"" target=""#b18"">[19]</ref>. However, auxiliary classifiers require specific new desig tei-c.org/ns/1.0""><head n=""3.1"">Generation of training graph</head><p>Similar to auxiliary training <ref type=""bibr"" target=""#b18"">[19]</ref>, we add several new classifier heads into the original net",1
"graph, including auxiliary training <ref type=""bibr"" target=""#b18"">[19]</ref>, multi-task learning <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b2"">3]</ref>, and knowledge distill related tasks simultaneously so that knowledge obtained from each task can be reused by the others <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target ef type=""figure"" target=""#fig_1"">1 (c</ref>). This structure is very similar to multi-task learning <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b2"">3]</ref>, in which different su",1
"ing the first classifier head without head selection. All experiments are conducted with Tensorflow <ref type=""bibr"" target=""#b0"">[1]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=",0
"es additional regularization.</p><p>ILR sharing is somewhat related to the concept of hint training <ref type=""bibr"" target=""#b17"">[18]</ref>, in which a teacher transfers its knowledge to a student n parated classifier heads converge to the exact same one by forcing them to match. It is reported in <ref type=""bibr"" target=""#b17"">[18]</ref> that using hints can outperform distillation. To a certain",0
"le. Two-way distillation. Co-distillation of two instances of the same neural network is studied in <ref type=""bibr"" target=""#b1"">[2]</ref> with a focus on training speed-up in a distributed learning with each head one-by-one. This algorithm is used in both <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b1"">2]</ref>. In fact, alternative optimization is popular in generative a",0
"illation between two networks, which can use the same architecture or different, is also studied in <ref type=""bibr"" target=""#b22"">[23]</ref>. Each of them alternatively optimizes its own network para ning setup. Simultaneous vs alternative optimization. We repeat an experiment that was performed in <ref type=""bibr"" target=""#b22"">[23]</ref>.</p><p>It is just a special case of collaborative learning CIFAR-100 with T = 1, β = 0.5. The only difference is that we replace the alternative optimization <ref type=""bibr"" target=""#b22"">[23]</ref> with the simultaneous one. It is shown in Table <ref type= type=""table"" xml:id=""tab_1""><head>Table 2 :</head><label>2</label><figDesc>Alternative optimization<ref type=""bibr"" target=""#b22"">[23]</ref> vs simultaneous optimization (ours) in terms of test error natively update the parameters associated with each head one-by-one. This algorithm is used in both <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b1"">2]</ref>. In fact, alternativ",0
"sses, respectively. We conduct empirical studies on the CIFAR-10 dataset with ResNet-32, ResNet-110 <ref type=""bibr"" target=""#b8"">[9]</ref>, and DenseNet-40-12 <ref type=""bibr"" target=""#b10"">[11]</ref ve learning helps improve the performance of ResNet-50 network.</p><p>As following the notations in <ref type=""bibr"" target=""#b8"">[9]</ref>, we consider two heads sharing ILRs up to ""conv3_x"" block fo ><figDesc>Figure 4: Per-layer weight distribution in trained ResNet-50.As following the notations in<ref type=""bibr"" target=""#b8"">[9]</ref>, the two split points in the hierarchical sharing with four",0
"to recalculate a new prediction after updating its parameters. In terms of convergence, recent work <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b15"">16]</ref> reveals that simul",0
"The two CIFAR datasets, CIFAR-10 and CIFAR-100, consist of colored natural images with 32x32 pixels <ref type=""bibr"" target=""#b12"">[13]</ref> and have 10 and 100 classes, respectively. We conduct empi",0
"ILSVRC 2012 classification dataset consists of 1.2 million for training, and 50,000 for validation <ref type=""bibr"" target=""#b5"">[6]</ref>. We evaluate how collaborative learning helps improve the pe",0
"e x i t s h a 1 _ b a s e 6 4 = "" t A  We use primitives from an existing code generation framework <ref type=""bibr"" target=""#b8"">[9]</ref> to form S e . Our search space includes multi-level tiling o",1
"d is to use deep neural networks to perform program analysis <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b9"">10]</ref>. Our new problem setting and experiment environment can serv",0
">Black box optimization (auto-tuning) is used in high-performance computing libraries such as ATLAS <ref type=""bibr"" target=""#b42"">[43]</ref> and FFTW <ref type=""bibr"" target=""#b11"">[12]</ref>. Altern",0
"ign choice in the framework. Component evaluations were based on convolution workloads in ResNet-18 <ref type=""bibr"" target=""#b13"">[14]</ref> for ImageNet classification (Table <ref type=""table"" targe nd workload evaluation. We evaluated real world end-to-end DL inference workloads, including ResNet <ref type=""bibr"" target=""#b13"">[14]</ref>, MobileNet <ref type=""bibr"" target=""#b15"">[16]</ref>, LSTM",0
"target=""#b13"">[14]</ref>, MobileNet <ref type=""bibr"" target=""#b15"">[16]</ref>, LSTM Language Model <ref type=""bibr"" target=""#b43"">[44]</ref>, Deep Q Network (DQN) <ref type=""bibr"" target=""#b26"">[27]<",0
"ng to a single point, triplet loss enables documents with the same identity to reside on a manifold <ref type=""bibr"" target=""#b28"">[20]</ref>, and at the same time maintain a distance from other docum",1
"the structure of G to improve the global embedding.We use an unsupervised autoencoder architecture <ref type=""bibr"" target=""#b22"">[14,</ref><ref type=""bibr"" target=""#b34"">26]</ref> to learn from the global and local context. In our implementation, we use a variational version of graph auto-encoder <ref type=""bibr"" target=""#b22"">[14]</ref> by assuming Z is generated from a latent Gaussian distribu",1
"f type=""bibr"" target=""#b14"">[6]</ref>, and identifying users across multiple online social networks <ref type=""bibr"" target=""#b42"">[34]</ref>. However, despite much work that has been done, the proble",0
"ese methods are capable of utilizing graph topology and aggregate information from neighbors. GHOST <ref type=""bibr"" target=""#b13"">[5]</ref> builds document graph for each ambiguous name by co-authors s from the graph. The nal result is generated by agglomerative hierarchical clustering.</p><p>GHOST <ref type=""bibr"" target=""#b13"">[5]</ref>: The second method is purely based on coauthor names. For e",0
"than 130,000,000 researcher pro les and over 200,000,000 papers from multiple publication databases <ref type=""bibr"" target=""#b33"">[25]</ref>.</p><p>In this paper, we present the implementation and de lem with large data in an online fashion. AMiner is a free online academic search and mining system <ref type=""bibr"" target=""#b33"">[25]</ref>. The system extracts researchers' pro les automatically fr ner is a free online academic search and mining system and also the second generation of ArnetMiner <ref type=""bibr"" target=""#b33"">[25]</ref>, with the emphasis to o er approaches to gain a deeper und",0
"et=""#b38"">[30]</ref>, constructing canonicalized knowledge base based on facts extracted from texts <ref type=""bibr"" target=""#b14"">[6]</ref>, and identifying users across multiple online social networ",0
"original adjacency matrix A.</p><p>We instantiate 1 as a two-layer graph convolution network (GCN) <ref type=""bibr"" target=""#b21"">[13]</ref> due to its e ectiveness for modeling networked data:</p><f",0
"graphs into grid-like data to enable the use of CNNs directly. This idea was previously explored in <ref type=""bibr"" target=""#b19"">[20]</ref>. However, the transformation in <ref type=""bibr"" target=""# as previously explored in <ref type=""bibr"" target=""#b19"">[20]</ref>. However, the transformation in <ref type=""bibr"" target=""#b19"">[20]</ref> is implemented in the preprocessing process while our meth",1
"12"">13]</ref>, which achieved state-of-the-art performance in the ImageNet classification challenge <ref type=""bibr"" target=""#b15"">[16]</ref>.</p><p>In LGCNs, we first apply a graph embedding layer to",0
"ificial intelligence tasks. Among these deep learning methods, convolutional neural networks (CNNs) <ref type=""bibr"" target=""#b17"">[18]</ref> have demonstrated promising performance in many image-rela",0
"ations, such as image classification <ref type=""bibr"" target=""#b4"">[5]</ref>, semantic segmentation <ref type=""bibr"" target=""#b1"">[2]</ref>, and object detection <ref type=""bibr"" target=""#b9"">[10,</re",0
"> 67.2% 43.2% 65.3% Planetoid <ref type=""bibr"" target=""#b29"">[30]</ref> 75.7% 64.7% 77.2% Chebyshev <ref type=""bibr"" target=""#b3"">[4]</ref> 81.2% 69.8% 74.4% GCN <ref type=""bibr"" target=""#b14"">[15]</r",0
"al <ref type=""bibr"" target=""#b35"">[36]</ref>'s sequential sampling, and do it in a parallel manner <ref type=""bibr"" target=""#b15"">[16]</ref>. As described in Algorithm 1, for the r -th document π r ,",1
"e rank range from 1 to K. As pointed out above, this scheme pre-determines the weight. Rendle et al <ref type=""bibr"" target=""#b28"">[29]</ref> proposed an empirical weight for sampling a single positio",1
"cores for all documents. To get the student predicted rank for this document, we apply Weston et al <ref type=""bibr"" target=""#b35"">[36]</ref>'s sequential sampling, and do it in a parallel manner <ref",1
"ing Another related research area is semisupervised learning <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b42"">43]</ref>. Unlike the teacher-student model learning paradigm in know",0
"by other approaches. Several successful ranking models with neural networks have been investigated <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" ta ss list-wise loss in this work. The point-wise loss is widely used when relevance labels are binary <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b32"">33]</ref>. One typical point",0
"tiveness and efficiency has been a line of recent research <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b33"">34,</ref><ref type=""bibr"" target=""#b38"">39,</ref><ref type=""bibr"" tar runing and indexing to speed-up retrieval of related items <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b33"">34]</ref>, using fast models for candidate generation and applying ti",0
"users and items. Semi-Supervised Learning Another related research area is semisupervised learning <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b42"">43]</ref>. Unlike the teacher-",0
"he model effectiveness <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b14"">15]</ref>. The idea of KD is shown in Figure <ref type=""figure"">1a</r has an effectiveness comparable to that of the teacher model <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b19"">20]</ref> and can make more e on Knowledge distillation has been used in image recognition <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b30"">31]</ref> and neural machine -defined rules etc., the model can be trained with more guidance and has less variance in gradients <ref type=""bibr"" target=""#b14"">[15]</ref>. Figure <ref type=""figure"" target=""#fig_1"">2b</ref> shows",0
"for candidate generation and applying time-consuming models to the candidates for online inferences <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b23"">24]</ref>. These methods eithe",0
"ith the great impact of neural networks on computer vision <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b21"">22]</ref> and natural language processing <ref type=""bibr"" target=""#b",0
"ime-consuming models to the candidates for online inferences <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b23"">24]</ref>. These methods either lose much of effectiveness, due to th",0
"r"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b21"">22]</ref> and natural language processing <ref type=""bibr"" target=""#b18"">[19,</ref><ref type=""bibr"" target=""#b24"">25]</ref>, a new branch of I",0
"sers' sequences) based item recommendation (POP), the item based Collaborative Filtering 5 (ItemCF) <ref type=""bibr"" target=""#b31"">[32]</ref>, and the 5 We use Jaccard similarity measure and set the n",0
"trieval followed this direction, using weak-labeled or unlabeled data to construct test collections <ref type=""bibr"" target=""#b1"">[2]</ref>, to provide extra features <ref type=""bibr"" target=""#b10"">[1",0
"et=""#b13"">[14,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" target=""#b34"">35,</ref><ref type=""bibr"" target=""#b36"">37]</ref>. However, the size of such models (in terms of the number o get=""#b9"">[10,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b36"">37]</ref>, three different evaluation metrics used are Precision@n (P",0
"models can improve model predictions on all tasks by utilizing regularization and transfer learning <ref type=""bibr"" target=""#b7"">[8]</ref>. However, in practice, multi-task learning models do not alw <p>The backbone of MMoE is built upon the most commonly used Shared-Bottom multi-task DNN structure <ref type=""bibr"" target=""#b7"">[8]</ref>. The Shared-Bottom model structure is shown in Figure <ref t igure <ref type=""figure"" target=""#fig_0"">1</ref> (a), which is a framework proposed by Rich Caruana <ref type=""bibr"" target=""#b7"">[8]</ref> and widely adopted in many multi-task learning applications sks.</p><p>Prior works <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b7"">8]</ref> investigated task di erences in multi-task learning by assumi target=""#b3"">[4]</ref><ref type=""bibr"" target=""#b4"">[5]</ref><ref type=""bibr"" target=""#b5"">[6]</ref><ref type=""bibr"" target=""#b7"">8]</ref>.</p><p>Instead of sharing hidden layers and same model parame lt in both improved e ciency and model quality for each task <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b29"">30]</ref>. One of the widely us "" target=""#b29"">30]</ref>. One of the widely used multi-task learning models is proposed by Caruana <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b8"">9]</ref>, which has a shared-bo",1
"i-gate Mixture-of-Experts (MMoE) structure, which is inspired by the Mixture-of-Experts (MoE) model <ref type=""bibr"" target=""#b20"">[21]</ref> and the recent MoE layer <ref type=""bibr"" target=""#b15"">[1 ""4"">MODELING APPROACHES 4.1 Mixture-of-Experts</head><p>The Original Mixture-of-Experts (MoE) Model <ref type=""bibr"" target=""#b20"">[21]</ref> can be formulated as:</p><formula xml:id=""formula_5"">= n i",1
"dness so as to evaluate the e ectiveness of multitask models <ref type=""bibr"" target=""#b3"">[4]</ref><ref type=""bibr"" target=""#b4"">[5]</ref><ref type=""bibr"" target=""#b5"">[6]</ref><ref type=""bibr"" targe",0
"al datasets. The tuning algorithm is a Gaussian Process model similar to Spearmint as introduced in <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b31"">32]</ref>. To make the compa",0
"cale recommendation systems have adopted multi-task learning using Deep Neural Network (DNN) models <ref type=""bibr"" target=""#b2"">[3]</ref>.</p><p>Researchers have reported multi-task learning models commendations <ref type=""bibr"" target=""#b27"">[28,</ref><ref type=""bibr"" target=""#b34"">35]</ref>. In <ref type=""bibr"" target=""#b2"">[3]</ref>, a text recommendation task is improved by sharing feature r",0
"rks have been proven to be able to improve model performance <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b19"">20]</ref>.</p><p>Eigen et al <ref type=""bibr"" target=""#b15"">[16]</ref",0
"ed bottom network is a single-layer network with size=16. The model is implemented using TensorFlow <ref type=""bibr"" target=""#b0"">[1]</ref> and trained using Adam optimizer <ref type=""bibr"" target=""#b × ""hidden units per expert"". Our approach and all baseline methods are implemented using TensorFlow <ref type=""bibr"" target=""#b0"">[1]</ref>.</p><p>We tune the learning rates and the number of training",0
"=""#b23"">[24]</ref>, we set the regression model as a combination of sinusoidal functions as used in <ref type=""bibr"" target=""#b32"">[33]</ref>. Speci cally, we generate the synthetic data as follows.</",0
"idely used multi-task learning models is proposed by Caruana <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b8"">9]</ref>, which has a shared-bottom model structure, where the bottom nsemble models and ensemble of subnetworks have been proven to be able to improve model performance <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b19"">20]</ref>.</p><p>Eigen et al <",0
"tion and gating mechanisms can improve the trainability in training non-convex deep neural networks <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b18"">19]</ref>.</p><p>We further e within a range of hyper-parameter settings and model initializations.</p><p>Recently, Collins et al <ref type=""bibr"" target=""#b9"">[10]</ref> nd that some gated RNN models (like LSTM and GRU) we though Tuning</head><p>We adopt a hyper-parameter tuner, which is used in recent deep learning frameworks <ref type=""bibr"" target=""#b9"">[10]</ref>, to search the best hyperparameters for all the models in t",0
"rget=""#b9"">10,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" tar algorithm design, our work is most closely related to Hamilton et al. (2017a)'s GraphSAGE algorithm <ref type=""bibr"" target=""#b17"">[18]</ref> and the closely related follow-up work of <ref type=""bibr"" nodes to aggregate from allows us to control the memory footprint of the algorithm during training <ref type=""bibr"" target=""#b17"">[18]</ref>. Second, it allows Algorithm 1 to take into account the im ean); • mean-pooling-xent is the same as mean-pooling but uses the cross-entropy loss introduced in <ref type=""bibr"" target=""#b17"">[18]</ref>. • mean-pooling-hard is the same as mean-pooling, except t ing and cross-entropy settings are extensions of the best-performing GCN model from Hamilton et al. <ref type=""bibr"" target=""#b17"">[18]</ref>-other variants (e.g., based on Kipf et al. <ref type=""bibr",1
"sizes, ranging from 512 to 4096.</p><p>We use techniques similar to those proposed by Goyal et al. <ref type=""bibr"" target=""#b15"">[16]</ref> to ensure fast convergence and maintain training and gener",0
"ll as social graphs) <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" tar architectures known as Graph Convolutional Networks (GCNs) <ref type=""bibr"" target=""#b18"">[19,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" tar get=""#b10"">11,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" tar ificant performance gains when using concatenation operation instead of the average operation as in <ref type=""bibr"" target=""#b20"">[21]</ref>. Additionally, the normalization in Line 3 makes training amilton et al. <ref type=""bibr"" target=""#b17"">[18]</ref>-other variants (e.g., based on Kipf et al. <ref type=""bibr"" target=""#b20"">[21]</ref>) performed significantly worse in development tests and ar",0
"query item's embedding. Approximate KNN can be obtained efficiently via locality sensitive hashing <ref type=""bibr"" target=""#b1"">[2]</ref>. After the hash function is computed, retrieval of items can",0
"ded for the training to converge. To help with convergence, we develop a curriculum training scheme <ref type=""bibr"" target=""#b3"">[4]</ref>. In the first epoch of training, no hard negative items are",0
"t of user-level influence prediction models, most of which <ref type=""bibr"" target=""#b26"">[27,</ref><ref type=""bibr"" target=""#b52"">53,</ref><ref type=""bibr"" target=""#b53"">54]</ref> consider complicate k, while external sources are assumed to be not present.</p><p>Problem 1. Social Influence Locality <ref type=""bibr"" target=""#b52"">[53]</ref> Social influence locality models the probability of v's ac get=""#b53"">54]</ref> Weibo 6 is the most popular Chinese microblogging service. The dataset is from <ref type=""bibr"" target=""#b52"">[53]</ref> and can be downloaded here. 7 The complete dataset contain and the social action is defined to be whether a user retweets ""Higgs"" related tweets.</p><p>Weibo <ref type=""bibr"" target=""#b52"">[53,</ref><ref type=""bibr"" target=""#b53"">54]</ref> Weibo 6 is the mos .</p><p>Data Preparation We process the above four datasets following the practice in existing work <ref type=""bibr"" target=""#b52"">[53,</ref><ref type=""bibr"" target=""#b53"">54]</ref>. More concretely,",1
"al influence. Indeed, extensive work has been done on social influence prediction in the literature <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b31"">32,</ref><ref type=""bibr"" ta ial equations extended from the classic 'Susceptible-Infected' (SI) model; Most recently, Li et al. <ref type=""bibr"" target=""#b25"">[26]</ref> proposed an end-toend predictor for inferring cascade size efforts to detect those global patterns automatically using deep learning, e.g., the DeepCas model <ref type=""bibr"" target=""#b25"">[26]</ref> which formulate cascade prediction as a sequence problem a",1
"target=""#b48"">[49]</ref>, the attention function is instantiated with a dot product and a LeakyReLU <ref type=""bibr"" target=""#b30"">[31,</ref><ref type=""bibr"" target=""#b50"">51]</ref> nonlinearity. For",0
"cial influence analysis <ref type=""bibr"" target=""#b41"">[42]</ref> and graph representation learning <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b36"">37]</ref>.</p></div> <div xm",0
"type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b37"">38]</ref> and structural diversity <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b28"">29,</ref><ref type=""bibr"" ta",0
"ent cascade and linear threshold models assume a pairwise influence model; In social recommendation <ref type=""bibr"" target=""#b29"">[30]</ref>, a key assumption is social influence-the ratings and revi",0
"<ref type=""bibr"" target=""#b35"">[36]</ref>, LINE <ref type=""bibr"" target=""#b40"">[41]</ref>, node2vec <ref type=""bibr"" target=""#b19"">[20]</ref>, metap-ath2vec <ref type=""bibr"" target=""#b12"">[13]</ref>,",0
"neighbors and 2) the local network she is embedded in. and the celebrated ""structural diversity"" in <ref type=""bibr"" target=""#b45"">[46]</ref>. The above observations inspire a lot of user-level influe ighbors <ref type=""bibr"" target=""#b1"">[2]</ref>. Density of subgnetwork induced by active neighbors <ref type=""bibr"" target=""#b45"">[46]</ref>. #Connected components formed by active neighbors <ref typ ighbors <ref type=""bibr"" target=""#b45"">[46]</ref>. #Connected components formed by active neighbors <ref type=""bibr"" target=""#b45"">[46]</ref>. 300:1. To address this issue, we sample a more balanced d (RWR) <ref type=""bibr"" target=""#b44"">[45]</ref>. Inspired by <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b45"">46]</ref> which suggest that people are more likely to be influenced ive signals with noise. On the other hand, as pointed out by <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b45"">46]</ref>, active neighbors are more important than inactive neighbor ructural diversity <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b28"">29,</ref><ref type=""bibr"" target=""#b45"">46]</ref>. Such user-level models act as fundamental building blocks",0
"e <ref type=""bibr"" target=""#b41"">[42]</ref>, group formation <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b37"">38]</ref> and structural diversity <ref type=""bibr"" target=""#b13"">[14",0
"et=""#b25"">[26,</ref><ref type=""bibr"" target=""#b31"">32,</ref><ref type=""bibr"" target=""#b41"">42,</ref><ref type=""bibr"" target=""#b42"">43]</ref>. For example, Matsubara et al. <ref type=""bibr"" target=""#b3 f>, external influence sources <ref type=""bibr"" target=""#b32"">[33]</ref>, and conformity phenomenon <ref type=""bibr"" target=""#b42"">[43]</ref>. Recently, there have been efforts to detect those global",0
"arn both long-term interests and short-term interests of such implicit feedbacks. As Jannach et al. <ref type=""bibr"" target=""#b6"">[7]</ref> noted that both the users' short-term and long-term interest",1
""" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b9"">10]</ref>. Hidasi et al. <ref type=""bibr"" target=""#b4"">[5]</ref> use d each item a fixed weight based on the relative distance with response to the target item. Li et al. <ref type=""bibr"" target=""#b9"">[10]</ref> propose an RNN based encoder-decoder model (NARM), which ta transaction data is used in this study.</p><p>Following <ref type=""bibr"" target=""#b4"">[5]</ref> and <ref type=""bibr"" target=""#b9"">[10]</ref>, we filter out sessions of length 1 and items that appear l",1
"the uncertainty inherent in user behavior and the limited information provided by browser sessions <ref type=""bibr"" target=""#b17"">[18]</ref>.</p><p>Based on existing literature, almost all the RNN-ba",0
"ision, then the user is very likely to visit another digital camera brand catalog in the next move. <ref type=""bibr"" target=""#b1"">(2)</ref> If the current action is to add a camera into the shopping c d our model can effectively utilize such information through the temporal interests representation. <ref type=""bibr"" target=""#b1"">(2)</ref> The proposed attention mechanism can effectively capture lon olutions have been developed and some of which represent the state-of-the-art in SRS research field <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target",0
"divided into two categorise: global models that focused on identifying users' interests in general <ref type=""bibr"" target=""#b2"">[3]</ref>, and localized models that emphasize users' temporal interes m the co-occurrences of items in sessions. The third approach is the Markov chain (MC) based models <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b14"">15]</ref>, which utilize seque",0
"t.</p><p>Deep neural networks have proven to be very effective in modeling sequential data recently <ref type=""bibr"" target=""#b8"">[9]</ref>. Inspired by recent advances in natural language processing",0
"ion, it is highly likely that the user's next intended action is in response to the current action. <ref type=""bibr"" target=""#b0"">(1)</ref> If the current action is to browse the product description b riority model for session-based recommendations. Two important findings can be made from the study: <ref type=""bibr"" target=""#b0"">(1)</ref> The next move of a user is mostly affected by the last-click contains information about the sequence with a strong focus on the parts nearest to the next click <ref type=""bibr"" target=""#b0"">[1]</ref>, thus some general interest features of items with a long di",0
"o improve the SRS models by taking into consideration of both type of user interests. Rendle et al. <ref type=""bibr"" target=""#b12"">[13]</ref> proposed a hybrid model FPMC, which combined the power of",0
"RNN models have been proven useful in capturing users' general interests from a sequence of actions <ref type=""bibr"" target=""#b19"">[20]</ref>, learning to predict from sessions is still a challenging tation technique to improve the performance of the RNNs for session-based recommendation. Yu et al. <ref type=""bibr"" target=""#b19"">[20]</ref>propose a dynamic recurrent model, which applies RNN to lea idental high similarities between rarely visited items as in <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b19"">20]</ref>. </p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head",0
"onsisting of the whole historical transaction data. Another approach is called neighborhood methods <ref type=""bibr"" target=""#b13"">[14]</ref>, which try to make recommendations based on item similarit RS model that always recommends items based on occurrence frequency in the training set. • Item-KNN <ref type=""bibr"" target=""#b13"">[14]</ref>: An item-to-item model which recommends items similar to t",0
"al., 2015)</ref> firstly model sentences by RNN, and then use CNN to get the final representation. <ref type=""bibr"" target=""#b21"">Shi et al. (2016)</ref> replace convolution filters with deep LSTM, w of recurrent units. We find that using GRU as recurrent units outperforms LSTM which is utilized by <ref type=""bibr"" target=""#b21"">Shi et al. (2016)</ref>.</p><formula xml:id=""formula_0"">w 1 w 2 w 3 w",1
"classification <ref type=""bibr"" target=""#b35"">(Zhang and Lee, 2003)</ref> and topic classification <ref type=""bibr"" target=""#b24"">(Tong and Koller, 2001)</ref>. Nowadays, one of the most commonly use",0
"ing a sentence with multiple kinds of convolutional filters. To capture the relation between words, <ref type=""bibr"" target=""#b13"">Kalchbrenner et al. (2014)</ref> propose a novel CNN model with a dyn",0
"ierarchical GRU model so that the model can better capture the important information of a document. <ref type=""bibr"" target=""#b27"">Wang and Tian (2016)</ref> incorporate the residual networks <ref typ",0
"1.0""><head>Hybrid model</head><p>Some researchers attempt to combine the advantages of CNN and RNN. <ref type=""bibr"" target=""#b28"">(Xiao and Cho, 2016)</ref> extract local and global features by CNN a e that very deep CNN (VDCNN) <ref type=""bibr"" target=""#b5"">(Conneau et al., 2017)</ref>   Char-CRNN <ref type=""bibr"" target=""#b28"">(Xiao and Cho, 2016)</ref> in the fourth block is a model which combi",0
"ead n=""3.1"">Back-boost learning</head><p>Back-boost learning borrows the idea from back translation <ref type=""bibr"" target=""#b49"">(Sennrich et al., 2016)</ref> in NMT, referring to training a backwar",1
"vskaya et al., 2014)</ref> uses a classifier-based approach, which is improved by the latter system <ref type=""bibr"" target=""#b44"">(Rozovskaya and Roth, 2016)</ref> through combining with an SMT-based ""bibr"" target=""#b50"">(Susanto et al., 2014;</ref><ref type=""bibr"">Chollampatt et al., 2016b,a;</ref><ref type=""bibr"" target=""#b44"">Rozovskaya and Roth, 2016;</ref><ref type=""bibr"" target=""#b26"">Junczy",1
"ng an error correction model. Inspired by <ref type=""bibr"" target=""#b22"">He et al. (2016)</ref> and <ref type=""bibr"" target=""#b61"">Zhang et al. (2018)</ref>, we propose a dual-boost learning strategy,",1
"lore artificial error generation for GEC <ref type=""bibr"" target=""#b2"">(Brockett et al., 2006;</ref><ref type=""bibr"" target=""#b18"">Foster and Andersen, 2009;</ref><ref type=""bibr"">Rozovskaya and</ref>",0
"pe=""bibr"" target=""#b32"">Mizumoto and Matsumoto, 2016;</ref><ref type=""bibr"">Yuan et al., 2016;</ref><ref type=""bibr"" target=""#b23"">Hoang et al., 2016;</ref><ref type=""bibr"" target=""#b57"">Yannakoudakis",0
"ttp://www.tei-c.org/ns/1.0""><head n=""1"">Introduction</head><p>Sequence-to-sequence (seq2seq) models <ref type=""bibr"" target=""#b1"">(Cho et al., 2014;</ref><ref type=""bibr"" target=""#b51"">Sutskever et al N) based encoder-decoder seq2seq model <ref type=""bibr"" target=""#b51"">(Sutskever et al., 2014;</ref><ref type=""bibr"" target=""#b1"">Cho et al., 2014)</ref> with attention mechanism <ref type=""bibr"" targ r et al., 2014;</ref><ref type=""bibr"" target=""#b1"">Cho et al., 2014)</ref> with attention mechanism <ref type=""bibr"" target=""#b1"">(Bahdanau et al., 2014)</ref> to edit a raw sentence into the grammati",0
"head><p>Sequence-to-sequence (seq2seq) models <ref type=""bibr"" target=""#b1"">(Cho et al., 2014;</ref><ref type=""bibr"" target=""#b51"">Sutskever et al., 2014)</ref> for grammatical error correction (GEC) pical neural GEC approach uses a Recurrent Neural Network (RNN) based encoder-decoder seq2seq model <ref type=""bibr"" target=""#b51"">(Sutskever et al., 2014;</ref><ref type=""bibr"" target=""#b1"">Cho et al",0
"011;</ref><ref type=""bibr"" target=""#b52"">Tajiri et al., 2012)</ref>, Cambridge Learner Corpus (CLC) <ref type=""bibr"" target=""#b37"">(Nicholls, 2003)</ref> and NUS Corpus of Learner English (NUCLE) <ref",0
"ei-c.org/ns/1.0""><head n=""6"">Related work</head><p>Most of advanced GEC systems are classifierbased <ref type=""bibr"" target=""#b4"">(Chodorow et al., 2007;</ref><ref type=""bibr"" target=""#b15"">De Felice",0
"=""#b4"">(Chodorow et al., 2007;</ref><ref type=""bibr"" target=""#b15"">De Felice and Pulman, 2008;</ref><ref type=""bibr"" target=""#b21"">Han et al., 2010;</ref><ref type=""bibr"" target=""#b28"">Leacock et al.,",0
"st to conduct multi-round seq2seq inference for GEC, while similar ideas have been proposed for NMT <ref type=""bibr"" target=""#b55"">(Xia et al., 2017)</ref>.</p><p>In addition to the studies on GEC, th",0
"del inference, we set beam size to 5 and decode 1-best result with a 2-layer GRU RNN language model <ref type=""bibr"" target=""#b30"">(Mikolov et al., 2010)</ref> through shallow fusion <ref type=""bibr""",0
"advanced GEC systems are classifierbased <ref type=""bibr"" target=""#b4"">(Chodorow et al., 2007;</ref><ref type=""bibr"" target=""#b15"">De Felice and Pulman, 2008;</ref><ref type=""bibr"" target=""#b21"">Han e",0
"n the MDP state, which consists of the query, the preceding documents, and the remaining candidates <ref type=""bibr"" target=""#b32"">[33]</ref>.</p><p>The greedy sequential document selection simpli es 28]</ref> for the Game of Go, in this paper we propose to enhance the MDP model for diverse ranking <ref type=""bibr"" target=""#b32"">[33]</ref> with the Monte Carlo tree search (MCTS), for alleviating t anism <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b14"">15]</ref>. Xia et al. <ref type=""bibr"" target=""#b32"">[33]</ref> proposed to model the dynamics of the document utility wit <ref type=""bibr"" target=""#b37"">[38]</ref>, the log-based document re-ranking is modeled as a POMDP. <ref type=""bibr"" target=""#b32"">[33]</ref> and <ref type=""bibr"" target=""#b29"">[30]</ref> propose to m <p>In our experiments, for e ective training of the model parameters and following the practices in <ref type=""bibr"" target=""#b32"">[33]</ref>, we combined four TREC datasets and constructed a new data approach which automatically learns novelty features based on neural tensor networks.</p><p>MDP-DIV <ref type=""bibr"" target=""#b32"">[33]</ref>: a state-of-the-art learning approach which uses an MDP fo ning approach which uses an MDP for modeling the diverse ranking process. Following the practice in <ref type=""bibr"" target=""#b32"">[33]</ref>, we con gured the reward function in MDP-DIV as α-DCG and minary representations of the queries and the documents as their inputs. Following the practices in <ref type=""bibr"" target=""#b32"">[33]</ref>, in the experiments we used the query vector and document",1
"ural networks and max-pooling to model subtopic information explicitly with the attention mechanism <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b14"">15]</ref>. Xia et al. <ref t",0
"get=""#b30"">31,</ref><ref type=""bibr"" target=""#b31"">32,</ref><ref type=""bibr"" target=""#b33"">34,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" target=""#b38"">39]</ref>. The relational lea for search result diversi cation.</p><p>We also compared MDP-DIV with the learning methods: SVM-DIV <ref type=""bibr"" target=""#b35"">[36]</ref>: a learning approach which utilizes structural SVMs to opt",0
"et=""#b22"">[23,</ref><ref type=""bibr"" target=""#b30"">31,</ref><ref type=""bibr"" target=""#b31"">32,</ref><ref type=""bibr"" target=""#b33"">34,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" tar d novelty features <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b30"">31,</ref><ref type=""bibr"" target=""#b33"">34,</ref><ref type=""bibr"" target=""#b38"">39]</ref>. The novelty term i et=""#b17"">[18,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b33"">34,</ref><ref type=""bibr"" target=""#b34"">35,</ref><ref type=""bibr"" tar and its variations <ref type=""bibr"" target=""#b30"">[31,</ref><ref type=""bibr"" target=""#b31"">32,</ref><ref type=""bibr"" target=""#b33"">34]</ref> de ne the utilities based on the relevance features and the",0
", and those have been very promising <ref type=""bibr"" target=""#b17"">(Kipf &amp; Welling, 2017;</ref><ref type=""bibr"" target=""#b6"">Hamilton et al., 2017;</ref><ref type=""bibr"" target=""#b4"">Gilmer et al b28"">(Shervashidze et al., 2011;</ref><ref type=""bibr"" target=""#b17"">Kipf &amp; Welling, 2017;</ref><ref type=""bibr"" target=""#b6"">Hamilton et al., 2017)</ref>.</p><p>Yet, such aggregation schemes some • u∈ N (v) (deg(v)deg(u)) −1/2 h (l−1) u (2)</formula><p>where deg(v) is the degree of node v in G. <ref type=""bibr"" target=""#b6"">Hamilton et al. (2017)</ref> derived a variant of GCN that also works and can be viewed as a form of a ""skip connection"" between different layers.For COMBINE, GraphSAGE <ref type=""bibr"" target=""#b6"">(Hamilton et al., 2017)</ref> uses concatenation after a feature trans o select the important neighbors via an attention mechanism. The max-pooling operation in GraphSAGE <ref type=""bibr"" target=""#b6"">(Hamilton et al., 2017)</ref> implicitly selects the important nodes. ords features for each document (node) and citation links (edges) between documents. (II) On Reddit <ref type=""bibr"" target=""#b6"">(Hamilton et al., 2017)</ref>, the task is to predict the community to ataset contains word vectors as node features. (III) For protein-protein interaction networks (PPI) <ref type=""bibr"" target=""#b6"">(Hamilton et al., 2017)</ref> We compare against three baselines: Grap lutional Networks (GCN) <ref type=""bibr"" target=""#b17"">(Kipf &amp; Welling, 2017)</ref>, Graph-SAGE <ref type=""bibr"" target=""#b6"">(Hamilton et al., 2017)</ref> and Graph Attention Networks (GAT) <ref r gives 6 JK-Net variants. We follow exactly the same setting of GraphSAGE as in the original paper <ref type=""bibr"" target=""#b6"">(Hamilton et al., 2017)</ref>, where the model consists of 2 hidden la the well-behaved middle-sized communities to avoid the noisy cores and tree-like small communities <ref type=""bibr"" target=""#b6"">(Hamilton et al., 2017)</ref>. As a result, this graph is more regular",1
"et=""#b17"">(Kipf &amp; Welling, 2017;</ref><ref type=""bibr"" target=""#b6"">Hamilton et al., 2017;</ref><ref type=""bibr"" target=""#b4"">Gilmer et al., 2017;</ref><ref type=""bibr"" target=""#b32"">Veličković et =""bibr"" target=""#b3"">Defferrard et al., 2016)</ref>, are a specific instantiation of this framework <ref type=""bibr"" target=""#b4"">(Gilmer et al., 2017)</ref>, of the form</p><formula xml:id=""formula_3",0
"ated GNN <ref type=""bibr"" target=""#b21"">(Li et al., 2016)</ref> uses the Gated Recurrent Unit (GRU) <ref type=""bibr"" target=""#b1"">(Cho et al., 2014)</ref>. Another wellknown variant of skip connection",0
"networks apply convolution on graphs by using the graph Laplacian eigenvectors as the Fourier atoms <ref type=""bibr"" target=""#b0"">(Bruna et al., 2014;</ref><ref type=""bibr"" target=""#b29"">Shuman et al.",0
"r"" target=""#b6"">Hamilton et al., 2017;</ref><ref type=""bibr"" target=""#b4"">Gilmer et al., 2017;</ref><ref type=""bibr"" target=""#b32"">Veličković et al., 2018;</ref><ref type=""bibr"" target=""#b15"">Kearnes fluenced by some directions of expansion more than the others.</p><p>Graph Attention Networks (GAT) <ref type=""bibr"" target=""#b32"">(Veličković et al., 2018)</ref> and VAIN <ref type=""bibr"" target=""#b1 SAGE <ref type=""bibr"" target=""#b6"">(Hamilton et al., 2017)</ref> and Graph Attention Networks (GAT) <ref type=""bibr"" target=""#b32"">(Veličković et al., 2018)</ref>.</p></div> <div xmlns=""http://www.tei",0
"> interpolate the neighborhood representation and the node's previous representation, and Gated GNN <ref type=""bibr"" target=""#b21"">(Li et al., 2016)</ref> uses the Gated Recurrent Unit (GRU) <ref type",0
"ing. Various hardware <ref type=""bibr"" target=""#b0"">[2]</ref><ref type=""bibr"" target=""#b1"">[3]</ref><ref type=""bibr"" target=""#b2"">[4]</ref><ref type=""bibr"" target=""#b9"">11]</ref> are manufactured to c",1
"w.tei-c.org/ns/1.0""><head>Properties</head><p>WiTrack <ref type=""bibr"" target=""#b1"">[3]</ref> WiDeo <ref type=""bibr"" target=""#b9"">[11]</ref> Widar <ref type=""bibr"" target=""#b19"">[21]</ref> Dy. Music < cated by spreading outliers.</p><p>To select target parameters out of cluttering estimations, WiDeo <ref type=""bibr"" target=""#b9"">[11]</ref> proposes to use Hungarian algorithm <ref type=""bibr"" target >3]</ref> develops FMCW radar to accurately estimate ToFs of reflections in frequency domain. WiDeo <ref type=""bibr"" target=""#b9"">[11]</ref> uses full-duplex Wi-Fi that enables self-interference cance target=""#b0"">[2]</ref><ref type=""bibr"" target=""#b1"">[3]</ref><ref type=""bibr"" target=""#b2"">[4]</ref><ref type=""bibr"" target=""#b9"">11]</ref> are manufactured to capture extremely weak human reflections",0
"t=""#b14"">16,</ref><ref type=""bibr"" target=""#b15"">17]</ref>.</p><p>Techniques based on visible light <ref type=""bibr"" target=""#b42"">[44]</ref> and depth imaging [1] have been proposed and commercialize",0
"ing estimations, WiDeo <ref type=""bibr"" target=""#b9"">[11]</ref> proposes to use Hungarian algorithm <ref type=""bibr"" target=""#b13"">[15]</ref> to match parameters of adjacent estimations. However, this",0
"f><ref type=""bibr"" target=""#b31"">33,</ref><ref type=""bibr"" target=""#b40"">42]</ref>, millimetre wave <ref type=""bibr"" target=""#b33"">[35,</ref><ref type=""bibr"" target=""#b43"">45]</ref> and Wi-Fi <ref typ",0
"r"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b4"">5]</ref>. Conventional studies concentrate on the area of multilingual r"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b4"">5]</ref> for a long time, these researches are commonly limited to mak uage dependent softmax layers of SHL-MDNN are optimized jointly by multilingual datasets. SHL-MLSTM <ref type=""bibr"" target=""#b4"">[5]</ref> further explores long short-term memory (LSTM) <ref type=""bi nder the condition of language information being known during training. A comparison with SHL-MLSTM <ref type=""bibr"" target=""#b4"">[5]</ref> with residual learning is investigated on CALL-HOME datasets .tei-c.org/ns/1.0""><head n=""4.4."">Results</head><p>The baseline systems come from our previous work <ref type=""bibr"" target=""#b4"">[5]</ref> and all results are summarized in Table <ref type=""table"" ta",1
"pronunciation lexicon <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b9"">10]</ref>. <ref type=""bibr"">Chiu et al.</ref> shows that attention-bas new stateof-the-art WER on a 12500 hour English voice search task using the word piece models (WPM) <ref type=""bibr"" target=""#b9"">[10]</ref>. Our previous work <ref type=""bibr"" target=""#b8"">[9]</ref> gue that the modeling unit of sub-words allows for a much stronger decoder LM compared to graphemes <ref type=""bibr"" target=""#b9"">[10]</ref>, so sub-words encoded by BPE are employed as the multilingu",0
"Language information as output targets</head><p>Similar to <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b17"">18]</ref>, we expand the symbol vocabulary of the multilingual ASR Tr the end (Transformer-E) of the original sub-words sequence <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b17"">18]</ref>. What's more, if the language information of both training",0
"gual speech recognition has been investigated for many years <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target= <p>Although multilingual speech recognition has been studied <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=",0
"n (MA), English (EN), Japanese (JA), Arabic (AR), German (GE) and Spanish (SP). We follow the Kaldi <ref type=""bibr"" target=""#b18"">[19]</ref> recipe to process CALLHOME datasets <ref type=""foot"" targe",0
"els have been proposed in recent years, including methods inspired by convolutional neural networks <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target",1
"POOL model where assignment matrices are generated using a deterministic graph clustering algorithm <ref type=""bibr"" target=""#b8"">[9]</ref>. ? DIFFPOOL-NOLP is a variant of DIFFPOOL where the link pre layer community structures, which can be captured well with pre-computed graph clustering algorithm <ref type=""bibr"" target=""#b8"">[9]</ref>. One observation is that despite significant performance imp",0
"d chemoinformatics <ref type=""bibr"" target=""#b27"">[28,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" tar",0
"ocial network data <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b35"">36]</ref> or graph-based representations of molecules <ref type=""bibr get=""#b20"">21,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b28"">29,</ref><ref type=""bibr"" target=""#b35"">36]</ref>, recurrent neural networks <ref type=""bibr"" target=""#b24"">[",0
"earch based on the original author's guidelines.</p><p>Kernel-based algorithms. We use the GRAPHLET <ref type=""bibr"" target=""#b33"">[34]</ref>, the SHORTEST-PATH <ref type=""bibr"" target=""#b1"">[2]</ref>",0
"ing network inference algorithms from the biological network, involving various biomedical entities <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" targe",1
"3 to 5% of all in-hospital medication errors are due to ""preventable"" drug-drug interactions (DDIs) <ref type=""bibr"" target=""#b2"">[3]</ref>. DDIs have caused the withdrawal of certain high-profile dru",0
"inference model using machine learning algorithms was built for the prediction task. Takeda et al. <ref type=""bibr"" target=""#b21"">[22]</ref> constructed interaction networks which incorporate pharmac an also be increased by considering more number of features such as pathways, enzymes, transporters <ref type=""bibr"" target=""#b21"">[22]</ref> and gene ontology <ref type=""bibr"" target=""#b50"">[51]</ref",0
"ed a DDI retrieval system to search and predict interactions in the KEGG DRUG database. Cami et al. <ref type=""bibr"" target=""#b13"">[14]</ref> constructed a DDI network and used a multivariate model to",0
"vices is opening up new human-centered designs that blur the boundaries between humans and machines <ref type=""bibr"" target=""#b0"">[1]</ref>. Now, the frontier for research on data management is relate ng number of users and devices but also a diverse set of new applications and services. The work in <ref type=""bibr"" target=""#b0"">[1]</ref> introduced a system that can pervasively operate in any netw",1
"secure data sharing between the device and the edge cloud, and safe data storage on the edge cloud <ref type=""bibr"" target=""#b9"">[10]</ref><ref type=""bibr"" target=""#b10"">[11]</ref><ref type=""bibr"" ta",0
"odes have limited processing capability. The works in <ref type=""bibr"" target=""#b16"">[17]</ref> and <ref type=""bibr"" target=""#b17"">[18]</ref> proposed a novel deep reinforcement learning approach to s",0
"pect to cloud-based services, and to consume fewer resources and less energy to reduce the workload <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b2"">3]</ref>.</p><p>The edge comput migration is to determine S 1 , S 2 , . . . , S N to maximize the system reward defined by Equation <ref type=""bibr"" target=""#b1"">(2)</ref>.</p><p>Q-learning is one of the most popular Reinforcement L",0
"part in supporting latency-and privacysensitive applications <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b8"">9]</ref>. Therefore, the security challenges relate to the protection",0
"://www.tei-c.org/ns/1.0""><head n=""3.2"">Deep Supervision</head><p>We propose to use deep supervision <ref type=""bibr"" target=""#b5"">[6]</ref> in UNet++, enabling the model to operate in two modes: (1) a",1
"ef> proposed H-denseunet for liver and liver tumor segmentation. In the same spirit, Drozdzalet al. <ref type=""bibr"" target=""#b1"">[2]</ref> systematically investigated the importance of skip connectio",0
"segmentation. Inspired by DenseNet architecture <ref type=""bibr"" target=""#b4"">[5]</ref>, Li et al. <ref type=""bibr"" target=""#b6"">[7]</ref> proposed H-denseunet for liver and liver tumor segmentation.",0
"resolution networks. Previous works including EDSR <ref type=""bibr"" target=""#b18"">[19]</ref>, BTSRN <ref type=""bibr"" target=""#b6"">[7]</ref> and RDN <ref type=""bibr"" target=""#b41"">[42]</ref> found that br"" target=""#b30"">31]</ref>. It is also experimentally proved in single image super-resolution task <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" targ ""#b11"">[12]</ref> hinders the accuracy of image super-resolution. Thus, in recent image SR networks <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" targ small image patches (e.g. 48 × 48) and small mini-batch size (e.g. 16) are used to speedup training <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" targ ny kinds of regularizers, for examples, weight decaying and dropout, are not adopted in SR networks <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" targ a is augmented with random horizontal flips and rotations following common data augmentation methods<ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b18"">19]</ref>. During training, th",1
"pplied to the task of single image super-resolution (SISR) <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" tar rget=""#b6"">[7,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" tar of image super-resolution. Thus, in recent image SR networks <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b41"">42]</ref>, batch normalizatio rget=""#b6"">[7,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" tar rget=""#b6"">[7,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" tar f type=""bibr"" target=""#b13"">[14]</ref>, SRResNet <ref type=""bibr"" target=""#b16"">[17]</ref> and EDSR <ref type=""bibr"" target=""#b18"">[19]</ref>). The increasing of depth brings benefits to representatio ding blocks for image super-resolution networks. Compared with vanilla residual blocks used in EDSR <ref type=""bibr"" target=""#b18"">[19]</ref>, we introduce WDSR-A which has a slim identity mapping pat 5]</ref> leads to better accuracy for deep super-resolution networks. Previous works including EDSR <ref type=""bibr"" target=""#b18"">[19]</ref>, BTSRN <ref type=""bibr"" target=""#b6"">[7]</ref> and RDN <re e for training SR networks. However, with the increasing depth of neural networks for SR (e.g. MDSR <ref type=""bibr"" target=""#b18"">[19]</ref> has depth around 180), the networks without batch normaliz ing deeper and deeper (from 3-layer SRCNN <ref type=""bibr"" target=""#b3"">[4]</ref> to 160-layer MDSR <ref type=""bibr"" target=""#b18"">[19]</ref>), training becomes more difficult. Batch normalization lay f type=""figure"">1</ref>. Two-layer residual blocks are specifically studied following baseline EDSR <ref type=""bibr"" target=""#b18"">[19]</ref>. Assume the width of identity mapping pathway (Fig. <ref t <p>Figure <ref type=""figure"">2</ref>: Demonstration of our simplified SR network compared with EDSR <ref type=""bibr"" target=""#b18"">[19]</ref>.</p><p>In this part, we overview the WDSR network architec his part, we overview the WDSR network architectures. We made two major modifications based on EDSR <ref type=""bibr"" target=""#b18"">[19]</ref> super-resolution network.</p><p>Global residual pathway Fi set <ref type=""bibr"" target=""#b34"">[35]</ref>  In this part, we show results of baseline model EDSR <ref type=""bibr"" target=""#b18"">[19]</ref> and our proposed WDSR-A and WDSR-B for the task of image b e results suggest that our proposed WDSR-A and WDSR-B have better accuracy and efficiency than EDSR <ref type=""bibr"" target=""#b18"">[19]</ref>. WDSR-B with wider activation also has better or similar p curacy drop with our simpler form.</p><p>Upsampling layer Different from previous state-of-the-arts <ref type=""bibr"" target=""#b18"">[19,</ref><ref type=""bibr"" target=""#b41"">42]</ref> where one or more lips and rotations following common data augmentation methods<ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b18"">19]</ref>. During training, the input images are also subtracted with",1
"ve neural networks</head><p>The depth of neural networks is of central importance for deep learning <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b29"">30,</ref><ref type=""bibr"" targ omes more difficult. Batch normalization layers are one of the cures for this problem in many tasks <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b30"">31]</ref>. It is also introduc skip connection) for SISR. SRResNet <ref type=""bibr"" target=""#b16"">[17]</ref> proposed a ResNetlike <ref type=""bibr"" target=""#b8"">[9]</ref> network. Densely connected networks <ref type=""bibr"" target= ions.</p><p>1 × 1 convolutions are widely used for channel number expansion or reduction in ResNets <ref type=""bibr"" target=""#b8"">[9]</ref>, ResNeXts <ref type=""bibr"" target=""#b37"">[38]</ref> and Mobi",0
"networks (CNNs) have been successfully applied to the task of single image super-resolution (SISR) <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" ta om 3 to 5). They are inferior in accuracy compared with later proposed deep SR networks (e.g., VDSR <ref type=""bibr"" target=""#b13"">[14]</ref>, SRResNet <ref type=""bibr"" target=""#b16"">[17]</ref> and ED uper-resolution task <ref type=""bibr"" target=""#b41"">[42]</ref>. To address this contradictory, VDSR <ref type=""bibr"" target=""#b13"">[14]</ref> proposed a very deep VGG-like <ref type=""bibr"" target=""#b2 experimentally proved in single image super-resolution task <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" tar small mini-batch size (e.g. 16) are used to speedup training <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" tar weight decaying and dropout, are not adopted in SR networks <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" tar",0
"layers (usually represent low-level features). To address this issue, methods including SRDenseNet <ref type=""bibr"" target=""#b35"">[36]</ref>, RDN <ref type=""bibr"" target=""#b41"">[42]</ref>, MemNet <re tion, beating SR networks with complicated skip connections and concatenations including SRDenseNet <ref type=""bibr"" target=""#b35"">[36]</ref> and MemNet <ref type=""bibr"" target=""#b32"">[33]</ref>. The onnected networks <ref type=""bibr"" target=""#b10"">[11]</ref> are also adapted for SISR in SRDenseNet <ref type=""bibr"" target=""#b35"">[36]</ref>. MemNet <ref type=""bibr"" target=""#b32"">[33]</ref> integrat get=""#b16"">17,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" target=""#b41"">42]</ref>. These very deep ne get=""#b16"">17,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" target=""#b41"">42]</ref>, thus the mean and get=""#b16"">17,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" target=""#b41"">42]</ref>. 3) Unlike image cl",0
"ions in security, surveillance, satellite, medical imaging <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b33"">34]</ref> and can serve as a built-in module for other image restorat",0
"rget=""#b5"">[6,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b36"">37,</ref><ref type=""bibr"" target=""#b39"">40,</ref><ref type=""bibr"" target=""#b40"">41]</ref>.</p><p>Previous ima",0
"ype=""bibr"" target=""#b11"">[12]</ref> or no normalization, we find training with weight normalization <ref type=""bibr"" target=""#b24"">[25]</ref> leads to better accuracy for deep super-resolution network ""#b11"">[12]</ref> is not suitable for training deep SR networks, and introduce weight normalization <ref type=""bibr"" target=""#b24"">[25]</ref> for faster convergence and better accuracy. 4) We train pr of batch normalization (BN) <ref type=""bibr"" target=""#b11"">[12]</ref> and weight normalization (WN) <ref type=""bibr"" target=""#b24"">[25]</ref>. We offer three intuitions why batch normalization is not orm of v. With this formalization, we will have ||w|| = g, independent of parameters v. As shown in <ref type=""bibr"" target=""#b24"">[25]</ref>, the decouples of length and direction speed up convergenc",0
", sub-pixel convolution). Pixel shuffling is also believed to introduce less checkerboard artifacts <ref type=""bibr"" target=""#b21"">[22]</ref> than the deconvolutional layer.</p></div> <div xmlns=""http",0
"pled version of HR). It has many applications in security, surveillance, satellite, medical imaging <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b33"">34]</ref> and can serve as a",0
"the massive multi-level pathological information on nidus and its surrounding tissues.</p><p>WSISA <ref type=""bibr"" target=""#b21"">[21]</ref> was the first trial of moving survival prediction onto who n network. Different from previous DL-based survival models that basically act as feature extractor <ref type=""bibr"" target=""#b21"">[21]</ref>, DeepGraphSurv directly generates predicted risks. We inte e compared DeepGraphSurv with the state-of-theart deep learning based survival models on WSI. WSISA <ref type=""bibr"" target=""#b21"">[21]</ref> worked on clustered patches from WSIs, however, they simpl n little from topology. The previous GCN <ref type=""bibr"" target=""#b4"">[5]</ref> outperformed WSISA <ref type=""bibr"" target=""#b21"">[21]</ref> on most of datasets because it can aggregate node features e it can aggregate node features as graph representation of WSI according to graph structure, while <ref type=""bibr"" target=""#b21"">[21]</ref> cannot. However, <ref type=""bibr"" target=""#b4"">[5]</ref> s erns. This may explain the lift by DeepGraphSurv compared to <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b21"">21]</ref> who learn little from topology. The previous GCN <ref type=",1
"I is irregular with ?(G) ?(G). A spectral convolutional filter built based on spectral graph theory <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target",0
"seline survival methods include: LASSO-Cox model <ref type=""bibr"" target=""#b18"">[18]</ref>, BoostCI <ref type=""bibr"" target=""#b17"">[17]</ref> and Multi-Task Learning model for Survival Analysis (MTLSA",0
"al convolutional filter built based on spectral graph theory <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b20"">20]</ref> is more applicable to k ? k . (<label>1</label></formula><formula xml:id=""formula_2"">)</formula><p>Based on theorem from <ref type=""bibr"" target=""#b1"">[2]</ref>, spectral convolution on graph G with vertex features X ? R",0
"ers to the situation where a Recommender System (RS) encounters new users or items, with no ratings <ref type=""bibr"" target=""#b8"">(Guo, 2013)</ref>. Recent studies have considered utilizing user revie",0
"ital information contained in review texts to support preference prediction. Some existing works by <ref type=""bibr"" target=""#b13"">Leung et al., (2006)</ref> and <ref type=""bibr"" target=""#b37"">Zhang e",0
"ey basically provided ratings inferred from opinion texts to feed a recommender system. The work by <ref type=""bibr"" target=""#b39"">(Zhao et al., 2013b)</ref> proposed an Opinion-based CF method that r",0
""" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b6"">7]</ref>. Related to our work, for example, <ref type=""bibr"" target=""# ibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b6"">7]</ref>. Related to our work, for example, <ref type=""bibr"" target=""#b6"">[7]</ref> uses pre-trained word vectors in a LSTM-based acoustic model rget=""#b6"">[7]</ref> uses pre-trained word vectors in a LSTM-based acoustic model in parametric TTS <ref type=""bibr"" target=""#b6"">[7]</ref>. These studies consider learning methods within the traditio",1
"ron architecture specified in <ref type=""bibr"" target=""#b7"">[8]</ref>, where we use a GMM attention <ref type=""bibr"" target=""#b8"">[9]</ref>, LSTM-based decoder with zoneout regularization <ref type=""b",1
"s/1.0""><head n=""2."">PROPOSED APPROACH</head><p>We use a baseline Tacotron architecture specified in <ref type=""bibr"" target=""#b7"">[8]</ref>, where we use a GMM attention <ref type=""bibr"" target=""#b8"">",1
"ained on the same corpus as the word embedding module.</p><p>For decoder pre-training, we used VCTK <ref type=""bibr"" target=""#b17"">[18]</ref>, a publicly available corpus containing 44 hours of speech",0
"dio fidelity using a much simplified voice building pipeline <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b2"">3]</ref>. However, such models t",0
"rmation it needs from all word vectors. In this work, we use a simple tanh based additive attention <ref type=""bibr"" target=""#b14"">[15]</ref>.</p><p>Since the encoder weights are still trained from sc",0
"bedding module. The module maps each word to a 128-dimensional vector. We also tried word2vec (W2V) <ref type=""bibr"" target=""#b16"">[17]</ref> trained on the same corpus as the word embedding module.</",0
"to produce natural prosody with high audio fidelity using a much simplified voice building pipeline <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target ct. Specifically, we propose a simple yet effective semi-supervised framework for training Tacotron <ref type=""bibr"" target=""#b0"">[1]</ref>, a recently proposed end-to-end TTS model. We propose to tra",0
"e large text corpora, one can train real-valued word vectors that contain the meanings of the words <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b12"">13]</ref> or language models",0
"valued word vectors that contain the meanings of the words <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b12"">13]</ref> or language models that model grammatical and semantic cont",0
"type=""bibr"" target=""#b12"">13]</ref> or language models that model grammatical and semantic context <ref type=""bibr"" target=""#b13"">[14]</ref>. These word vectors can be added as auxiliary to a TTS mod f different lengths.</p><p>For encoder conditioning, we used a neural network language model (NNLM) <ref type=""bibr"" target=""#b13"">[14]</ref> trained on English Google News 200B corpus from TensorFlow",0
"ation of unsupervised and weakly supervised learning for TTS <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=",0
"xists previous work studying the application of unsupervised and weakly supervised learning for TTS <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target",0
"oth objective and subjective tests. For the objective metric, we use mel cepstral distortions (MCD) <ref type=""bibr"" target=""#b15"">[16]</ref>, which measures the distance between synthesis and ground",0
"M attention <ref type=""bibr"" target=""#b8"">[9]</ref>, LSTM-based decoder with zoneout regularization <ref type=""bibr"" target=""#b9"">[10]</ref> and phoneme inputs derived from normalized text. We use Gri",0
""" target=""#b19"">Huang et al., 2015;</ref><ref type=""bibr"" target=""#b7"">Chiu and Nichols, 2016;</ref><ref type=""bibr"" target=""#b22"">Lample et al., 2016;</ref><ref type=""bibr"" target=""#b46"">Yang et al., model we use to perform NER. We will first describe the basic hierarchical neural CRF tagging model <ref type=""bibr"" target=""#b22"">(Lample et al., 2016;</ref><ref type=""bibr"" target=""#b27"">Ma and Hovy labels and performs inference.</p><p>In this paper, we closely follow the architecture proposed by <ref type=""bibr"" target=""#b22"">Lample et al. (2016)</ref>, and use bidirectional LSTMs for both the",1
"get=""#b30"">(Mikolov et al., 2013a;</ref><ref type=""bibr"" target=""#b15"">Faruqui and Dyer, 2014;</ref><ref type=""bibr"" target=""#b2"">Artetxe et al., 2016;</ref><ref type=""bibr"" target=""#b38"">Smith et al. om the dictionary. Following previous work <ref type=""bibr"" target=""#b50"">(Zhang et al., 2016;</ref><ref type=""bibr"" target=""#b2"">Artetxe et al., 2016;</ref><ref type=""bibr"" target=""#b38"">Smith et al. every training pair an equal contribution to the objective and improving word translation accuracy <ref type=""bibr"" target=""#b2"">(Artetxe et al., 2016)</ref>. When training the NER model, however, we",0
"between languages through word alignment <ref type=""bibr"" target=""#b12"">(Ehrmann et al., 2011;</ref><ref type=""bibr"" target=""#b21"">Kim et al., 2012;</ref><ref type=""bibr"" target=""#b45"">Wang and Mannin",0
"arget=""#b10"">Das and Petrov, 2011;</ref><ref type=""bibr"" target=""#b39"">Täckström et al., 2013;</ref><ref type=""bibr"" target=""#b13"">Fang and Cohn, 2016)</ref>, mention detection <ref type=""bibr"" target",0
"Zitouni and Florian, 2008)</ref> and parsing <ref type=""bibr"" target=""#b20"">(Hwa et al., 2005;</ref><ref type=""bibr"" target=""#b29"">McDonald et al., 2011)</ref>.</p><p>Language independent transfer-bas",0
"complete KGs, extensive research efforts <ref type=""bibr"" target=""#b21"">(Nickel et al., 2011;</ref><ref type=""bibr"" target=""#b2"">Bordes et al., 2013</ref>  et <ref type=""bibr"">al., 2014;</ref><ref ty t al., 2011)</ref> is one of the earlier work that models the relationship using tensor operations. <ref type=""bibr"" target=""#b2"">Bordes et al. (2013)</ref> proposed to model relationships in the 1-D local connections in knowledge graph.</p><p>Although the entity embeddings from KG embedding models <ref type=""bibr"" target=""#b2"">(Bordes et al., 2013;</ref><ref type=""bibr"" target=""#b38"">Yang et al., embedding-based methods: RESCAL <ref type=""bibr"" target=""#b21"">(Nickel et al., 2011)</ref>, TransE <ref type=""bibr"" target=""#b2"">(Bordes et al., 2013)</ref>, Dist-Mult <ref type=""bibr"" target=""#b38"">",1
""" target=""#b19"">Munkhdalai and Yu, 2017;</ref><ref type=""bibr"" target=""#b7"">Finn et al., 2017;</ref><ref type=""bibr"" target=""#b13"">Li et al., 2017)</ref>, which aim to learn the optimization of model",0
", there are also some other styles of few-shot learning algorithms, e.g. Bayesian Program Induction <ref type=""bibr"" target=""#b11"">(Lake et al., 2015)</ref>, which represents concepts as simple progra",0
"r"" target=""#b33"">Vinyals et al., 2016;</ref><ref type=""bibr"" target=""#b26"">Snell et al., 2017;</ref><ref type=""bibr"" target=""#b39"">Yu et al., 2018)</ref>, which try to learn generalizable metrics and n learning <ref type=""bibr"" target=""#b6"">(Duan et al., 2017)</ref> domains. In the language domain, <ref type=""bibr"" target=""#b39"">Yu et al. (2018)</ref> proposed a multi-metric based approach for tex",0
"pe=""bibr"" target=""#b53"">53]</ref> in the network to infer important features. More advanced methods <ref type=""bibr"" target=""#b33"">[34,</ref><ref type=""bibr"" target=""#b45"">45]</ref> produce explanatio does it assume the features are independent. These are two key assumptions made by existing models <ref type=""bibr"" target=""#b33"">[34,</ref><ref type=""bibr"" target=""#b45"">45]</ref> which are often vi ddress this problem through approximation. However, this sacrifices the fidelity of the explanation <ref type=""bibr"" target=""#b33"">[34]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head thus LIME can pinpoint important features based on the regression coefficients. A recent work SHAP <ref type=""bibr"" target=""#b33"">[34]</ref> tries to extend LIME by adding weights to the artificially <note xmlns=""http://www.tei-c.org/ns/1.0"" place=""foot"" n=""4"" xml:id=""foot_2""><p>We have tested SHAP<ref type=""bibr"" target=""#b33"">[34]</ref>, which is an extension of LIME. We find that SHAP is very",1
"sis, binary code sequence analysis). Then, we integrate fused lasso into a mixture regression model <ref type=""bibr"" target=""#b27"">[28]</ref> to approximate locally nonlinear decision boundaries to su",0
"sed on a mixture regression model <ref type=""bibr"" target=""#b26"">[27]</ref> enhanced by fused lasso <ref type=""bibr"" target=""#b64"">[64]</ref>.</p><p>Our design is based on two key insights. First, a m A needs to take a very different design path from existing methods. First, we introduce fused lasso <ref type=""bibr"" target=""#b64"">[64]</ref> to handle the feature dependency problems that are often e prove the efficiency of resolving this equation, we can also an alternative algorithm introduced in <ref type=""bibr"" target=""#b64"">[64]</ref>. As is depicted in Figure <ref type=""figure"" target=""#fig_",0
"sequences and set the maximum length to 200. Then we feed the sequences into the RNN. We used Keras <ref type=""bibr"" target=""#b13"">[14]</ref> to train the model, with Theano [63] as a backend. We spli",0
"security of software <ref type=""bibr"" target=""#b75"">[75]</ref>, and (3) generating security patches <ref type=""bibr"" target=""#b56"">[56]</ref>. For years, binary analysis is primarily done manually by",0
"ibr"" target=""#b6"">(Levy and Goldberg, 2014</ref>)), and the Noise Contrastive Estimation methods of <ref type=""bibr"" target=""#b9"">(Mnih and Teh, 2012;</ref><ref type=""bibr"" target=""#b4"">Jozefowicz et pled per training example):</p><p>• For any K 1, a binary classification variant of NCE, as used by <ref type=""bibr"" target=""#b9"">(Mnih and Teh, 2012;</ref><ref type=""bibr"" target=""#b8"">Mikolov et al. n square error as the MLE) as K ! 1.</p><p>• We discuss application of our results to approaches of <ref type=""bibr"" target=""#b9"">(Mnih and Teh, 2012;</ref><ref type=""bibr"" target=""#b8"">Mikolov et al. history x. This is the most straightforward extension of NCE to the conditional case; it is used by <ref type=""bibr"" target=""#b9"">(Mnih and Teh, 2012)</ref>. It has the clear drawback however of intro o motivate the importance of the two algorithms, we now discuss their application in previous work. <ref type=""bibr"" target=""#b9"">Mnih and Teh (2012)</ref> consider language modeling, where x = w 1 w n of the parameters c</p><p>x corresponding to normalization terms for each history. Interestingly, <ref type=""bibr"" target=""#b9"">Mnih and Teh (2012)</ref> acknowledge the difficulties in maintaining",1
"6)</ref> partially motivate the ranking-based variant throught the importance sampling viewpoint of <ref type=""bibr"" target=""#b0"">Bengio and Senécal (2008)</ref>. However there are two critical differ 0"">Bengio and Senécal (2008)</ref>. However there are two critical differences: 1) the algorithm of <ref type=""bibr"" target=""#b0"">Bengio and Senécal (2008)</ref> does not lead to the same objective L",0
"this form in NLP. In log-linear models, including both the original work on maximum-entropy models <ref type=""bibr"" target=""#b1"">(Berger et al., 1996)</ref>, and later work on conditional random fiel",0
"y objective used in word2vec ( <ref type=""bibr"" target=""#b8"">(Mikolov et al., 2013)</ref>, see also <ref type=""bibr"" target=""#b6"">(Levy and Goldberg, 2014</ref>)), and the Noise Contrastive Estimation br"" target=""#b9"">(Mnih and Teh, 2012;</ref><ref type=""bibr"" target=""#b8"">Mikolov et al., 2013;</ref><ref type=""bibr"" target=""#b6"">Levy and Goldberg, 2014;</ref><ref type=""bibr"" target=""#b4"">Jozefowicz ) or equivalently PMI(x, y) = log p(y|x) p(y) = v 0 y • v x log H(✓)</formula><p>That is, following <ref type=""bibr"" target=""#b6"">(Levy and Goldberg, 2014)</ref>, the inner product v 0 y • v</p><p>x i",0
"the Noise Contrastive Estimation methods of <ref type=""bibr"" target=""#b9"">(Mnih and Teh, 2012;</ref><ref type=""bibr"" target=""#b4"">Jozefowicz et al., 2016)</ref> for estimation of language models.</p>< target=""#b8"">Mikolov et al., 2013;</ref><ref type=""bibr"" target=""#b6"">Levy and Goldberg, 2014;</ref><ref type=""bibr"" target=""#b4"">Jozefowicz et al., 2016)</ref> giving a unified account of these metho enough to incorporate log Z(x; ✓).</p><p>• For any K 1, a ranking-based variant of NCE, as used by <ref type=""bibr"" target=""#b4"">(Jozefowicz et al., 2016)</ref>, gives consistent parameter estimates er product v 0 y • v</p><p>x is an estimate of the PMI up to a constant offset H(✓).</p><p>Finally, <ref type=""bibr"" target=""#b4"">Jozefowicz et al. (2016)</ref> introduce the ranking-based variant of n to assumptions 2.2 and 2.1, or derive the consistency or efficiency results in the current paper. <ref type=""bibr"" target=""#b4"">Jozefowicz et al. (2016)</ref> partially motivate the ranking-based va",0
"""bibr"" target=""#b13"">(Liu et al., 2018;</ref><ref type=""bibr"" target=""#b14"">Ma and Hovy, 2016;</ref><ref type=""bibr"" target=""#b10"">Lample et al., 2016)</ref>. However, most existing methods require la ""bibr"" target=""#b13"">(Liu et al., 2018;</ref><ref type=""bibr"" target=""#b14"">Ma and Hovy, 2016;</ref><ref type=""bibr"" target=""#b10"">Lample et al., 2016)</ref> to define the score of the predicted seque ""bibr"" target=""#b13"">(Liu et al., 2018;</ref><ref type=""bibr"" target=""#b14"">Ma and Hovy, 2016;</ref><ref type=""bibr"" target=""#b10"">Lample et al., 2016;</ref><ref type=""bibr"" target=""#b3"">Finkel et al. ""bibr"" target=""#b13"">(Liu et al., 2018;</ref><ref type=""bibr"" target=""#b14"">Ma and Hovy, 2016;</ref><ref type=""bibr"" target=""#b10"">Lample et al., 2016;</ref><ref type=""bibr"" target=""#b18"">Ratinov and cannot deal with multi-label tokens. Therefore, we customize the conventional CRF layer in LSTM-CRF <ref type=""bibr"" target=""#b10"">(Lample et al., 2016)</ref> into a Fuzzy CRF layer, which allows each kens, such as ""Thus"" and ""by"", are labeled as O.</p><p>Fuzzy-LSTM-CRF. We revise the LSTM-CRF model <ref type=""bibr"" target=""#b10"">(Lample et al., 2016)</ref> to the Fuzzy-LSTM-CRF model to support th -Disease datasets, LM-LSTM-CRF <ref type=""bibr"" target=""#b13"">(Liu et al., 2018)</ref> and LSTM-CRF <ref type=""bibr"" target=""#b10"">(Lample et al., 2016)</ref> achieve the state-of-the-art F 1 scores w cent advances in neural models have freed do-main experts from handcrafting features for NER tasks. <ref type=""bibr"" target=""#b10"">(Lample et al., 2016;</ref><ref type=""bibr"" target=""#b14"">Ma and Hovy",1
"n (NER) models without handcrafting features <ref type=""bibr"" target=""#b13"">(Liu et al., 2018;</ref><ref type=""bibr"" target=""#b14"">Ma and Hovy, 2016;</ref><ref type=""bibr"" target=""#b10"">Lample et al., rvised models (e.g., neural sequence models) <ref type=""bibr"" target=""#b13"">(Liu et al., 2018;</ref><ref type=""bibr"" target=""#b14"">Ma and Hovy, 2016;</ref><ref type=""bibr"" target=""#b10"">Lample et al., s (CRF) with the IOB or IOBES tagging scheme <ref type=""bibr"" target=""#b13"">(Liu et al., 2018;</ref><ref type=""bibr"" target=""#b14"">Ma and Hovy, 2016;</ref><ref type=""bibr"" target=""#b10"">Lample et al., e label y j .</p><p>We follow previous works <ref type=""bibr"" target=""#b13"">(Liu et al., 2018;</ref><ref type=""bibr"" target=""#b14"">Ma and Hovy, 2016;</ref><ref type=""bibr"" target=""#b10"">Lample et al., from handcrafting features for NER tasks. <ref type=""bibr"" target=""#b10"">(Lample et al., 2016;</ref><ref type=""bibr"" target=""#b14"">Ma and Hovy, 2016;</ref><ref type=""bibr"" target=""#b13"">Liu et al., 20",0
"ets, we use the pre-trained 200dimension word vectors<ref type=""foot"" target=""#foot_5"">7</ref> from <ref type=""bibr"" target=""#b17"">(Pyysalo et al., 2013)</ref>, which are trained on the whole PubMed a",0
"common in the domain-specific NER tasks <ref type=""bibr"" target=""#b20"">(Sahu and Anand, 2016;</ref><ref type=""bibr"" target=""#b1"">Dernoncourt et al., 2017;</ref><ref type=""bibr"" target=""#b26"">Wang et",0
"beled datasets with word embeddings or bootstrapping techniques in tasks like gene name recognition <ref type=""bibr"" target=""#b9"">(Kuksa and Qi, 2010;</ref><ref type=""bibr"" target=""#b23"">Tang et al.,",0
"uestion answering <ref type=""bibr"" target=""#b22"">(Yu et al., 2017)</ref>, knowledge base population <ref type=""bibr"" target=""#b25"">(Zhang et al., 2017)</ref>, and biomedical knowledge discovery <ref t our group compared (1) and ( <ref type=""formula"" target=""#formula_1"">2</ref>) with sequence models <ref type=""bibr"" target=""#b25"">(Zhang et al., 2017)</ref>, and we report these results; for (3) we r TM), and showed that it outperforms several CNN and dependency-based models by a substantial margin <ref type=""bibr"" target=""#b25"">(Zhang et al., 2017)</ref>. We compare with this strong baseline, and etup</head><p>We conduct experiments on two relation extraction datasets: (1) TACRED: Introduced in <ref type=""bibr"" target=""#b25"">(Zhang et al., 2017)</ref>, TACRED contains over 106k mention pairs d f the two entities.</p><p>More recently, <ref type=""bibr"" target=""#b0"">Adel et al. (2016)</ref> and <ref type=""bibr"" target=""#b25"">Zhang et al. (2017)</ref> have shown that relatively simple neural mo la_1"">2</ref> For fair comparisons on the TACRED dataset, we follow the evaluation protocol used in <ref type=""bibr"" target=""#b25"">(Zhang et al., 2017</ref>) by selecting the model with the median dev",1
"ation <ref type=""bibr"" target=""#b25"">(Zhang et al., 2017)</ref>, and biomedical knowledge discovery <ref type=""bibr"" target=""#b14"">(Quirk and Poon, 2017)</ref>.</p><p>Models making use of dependency p",0
"ed to shortest dependency pathbased approaches. Compared to tree-structured models (e.g., Tree-LSTM <ref type=""bibr"" target=""#b17"">(Tai et al., 2015)</ref>), it not only is able to capture more global CNN over the path. <ref type=""bibr"" target=""#b11"">Miwa and Bansal (2016)</ref> applied a Tree-LSTM <ref type=""bibr"" target=""#b17"">(Tai et al., 2015)</ref>, a generalized form of LSTM over dependency on the shortest path between the subject and object entities in the dependency tree. (3) Tree-LSTM <ref type=""bibr"" target=""#b17"">(Tai et al., 2015)</ref>, which is a recursive model that generalizes",0
"<ref type=""bibr"" target=""#b11"">(Miwa and Bansal, 2016)</ref>. Another popular approach, inspired by <ref type=""bibr"" target=""#b1"">Bunescu and Mooney (2005)</ref>, is to reduce the parse tree to the sh rnels <ref type=""bibr"" target=""#b23"">(Zelenko et al., 2003)</ref> and dependency path-based kernels <ref type=""bibr"" target=""#b1"">(Bunescu and Mooney, 2005)</ref> are effective for this task.</p><p>Re",0
"sion of the graph convolutional network <ref type=""bibr"" target=""#b4"">(Kipf and Welling, 2017;</ref><ref type=""bibr"" target=""#b9"">Marcheggiani and Titov, 2017)</ref> that is tailored for relation extr loop edges; and (2) adding dependency relation-specific parameters for edge-wise gating, similar to <ref type=""bibr"" target=""#b9"">(Marcheggiani and Titov, 2017)</ref>. We found that modeling direction",0
"i-c.org/ns/1.0""><head n=""2"">Related Work</head><p>CNN Compression and Acceleration. Extensive works <ref type=""bibr"" target=""#b20"">[20,</ref><ref type=""bibr"" target=""#b19"">19,</ref><ref type=""bibr"" ta time. We observe a correlation between the pre-fine-tune accuracy and the post fine-tuning accuracy <ref type=""bibr"" target=""#b20"">[20,</ref><ref type=""bibr"" target=""#b22"">22]</ref>. As shown in Table For channel pruning, we use max response selection (pruning the weights according to the magnitude <ref type=""bibr"" target=""#b20"">[20]</ref>), and preserve Batch Normalization <ref type=""bibr"" target /ref>. However, it requires iterative prune &amp; fine-tune procedure to achieve decent performance <ref type=""bibr"" target=""#b20"">[20]</ref>, and single-shot pruning without retraining will greatly h",1
"model compression <ref type=""bibr"" target=""#b26"">[26,</ref><ref type=""bibr"" target=""#b19"">19,</ref><ref type=""bibr"" target=""#b22"">22]</ref>. The core of model compression technique is to determine th n previous studies <ref type=""bibr"" target=""#b38"">[38,</ref><ref type=""bibr"" target=""#b31"">31,</ref><ref type=""bibr"" target=""#b22"">22]</ref>. Take convolutional layer as an example. The shape of a wei ing. We compare our approach with three empirical policies <ref type=""bibr"" target=""#b31"">[31,</ref><ref type=""bibr"" target=""#b22"">22]</ref> illustrated in Figure <ref type=""figure"">2</ref>: uniform s e pre-fine-tune accuracy and the post fine-tuning accuracy <ref type=""bibr"" target=""#b20"">[20,</ref><ref type=""bibr"" target=""#b22"">22]</ref>. As shown in Table <ref type=""table"" target=""#tab_1"">2</ref b33"">[33]</ref> -3.58 SPP (handcraft) <ref type=""bibr"" target=""#b49"">[49]</ref> -2.3 CP (handcraft) <ref type=""bibr"" target=""#b22"">[22]</ref> -1.  <ref type=""figure"" target=""#fig_0"">4</ref>. We can fi ssion ratio on object detection task. mAP (%) mAP [0.5, 0.95] (%) baseline 68.7 36.7 2× handcrafted <ref type=""bibr"" target=""#b22"">[22]</ref> 68.3 (-0.4) 36.7 (-0.0) 4× handcrafted <ref type=""bibr"" ta 6.7 2× handcrafted <ref type=""bibr"" target=""#b22"">[22]</ref> 68.3 (-0.4) 36.7 (-0.0) 4× handcrafted <ref type=""bibr"" target=""#b22"">[22]</ref> 66.9 (-1.8)</p><p>35.1 (-1.6) 4× handcrafted <ref type=""bi",0
"n efficient neural networks MobileNet-V1 <ref type=""bibr"" target=""#b23"">[23]</ref> and MobileNet-V2 <ref type=""bibr"" target=""#b44"">[44]</ref>. Since the networks have already been very compact, it is",0
"</ref>, NT: Network Transformation <ref type=""bibr"" target=""#b5"">[6]</ref>, N2N: Network to Network <ref type=""bibr"" target=""#b1"">[2]</ref>, and AMC: AutoML for Model Compression. AMC distinguishes fr ploration via network transformation <ref type=""bibr"" target=""#b7"">[8]</ref>. Inspired by them, N2N <ref type=""bibr"" target=""#b1"">[2]</ref> integrated reinforcement learning into channel selection. In",0
"> decomposes weights into light-weight pieces, for example <ref type=""bibr"" target=""#b51"">[51,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b13"">14]</ref> proposed to acceler",0
"[2]</ref>). All of the models mentioned above are shallow networks (less than 5 layers). Kim et al. <ref type=""bibr"" target=""#b11"">[12]</ref> first introduced the residual architecture for training mu ly concatenate together, which leads to the underutilization of local features. In 2016, Kim et al. <ref type=""bibr"" target=""#b11"">[12]</ref> proposed a residual learning framework (Fig. <ref type=""fi r module, we design a set of comparative experiments to compare the performance with residual block <ref type=""bibr"" target=""#b11"">[12]</ref>, dense block <ref type=""bibr"" target=""#b23"">[24]</ref>   < figDesc>Fig. 5. Quantitative comparison of three different feature extraction blocks (residual block<ref type=""bibr"" target=""#b11"">[12]</ref>, dense block<ref type=""bibr"" target=""#b23"">[24]</ref>, and",1
"target=""#b5"">[6]</ref><ref type=""bibr"" target=""#b6"">[7]</ref><ref type=""bibr"" target=""#b7"">[8]</ref><ref type=""bibr"" target=""#b8"">[9]</ref> were proposed. EDSR <ref type=""bibr"" target=""#b8"">[9]</ref> ><ref type=""bibr"" target=""#b7"">[8]</ref><ref type=""bibr"" target=""#b8"">[9]</ref> were proposed. EDSR <ref type=""bibr"" target=""#b8"">[9]</ref> was the champion of the NTIRE2017 SR Challenge. It based on e reconstructed some classic SR models, such as SRCNN <ref type=""bibr"" target=""#b0"">[1]</ref>, EDSR <ref type=""bibr"" target=""#b8"">[9]</ref> and SRResNet <ref type=""bibr"" target=""#b7"">[8]</ref>. During <ref type=""bibr"" target=""#b5"">[6]</ref>, SRResNet <ref type=""bibr"" target=""#b7"">[8]</ref>, and EDSR <ref type=""bibr"" target=""#b8"">[9]</ref>. Unfortunately, these models become more and more deeper and CN <ref type=""bibr"" target=""#b4"">[5]</ref>, LapSRN <ref type=""bibr"" target=""#b5"">[6]</ref> and EDSR <ref type=""bibr"" target=""#b8"">[9]</ref>. For fair, we retrain most of these models (except for EDSR <ref type=""bibr"" target=""#b8"">[9]</ref>. For fair, we retrain most of these models (except for EDSR <ref type=""bibr"" target=""#b8"">[9]</ref>, the results of EDSR provided by their original papers).</p> t upscaling factors and test-datasets. It can be seen that our results are slightly lower than EDSR <ref type=""bibr"" target=""#b8"">[9]</ref>. But it is worth noting that EDSR <ref type=""bibr"" target=""# slightly lower than EDSR <ref type=""bibr"" target=""#b8"">[9]</ref>. But it is worth noting that EDSR <ref type=""bibr"" target=""#b8"">[9]</ref> use  RGB channels for training, meanwhile, the data augment nwhile, the data augment methods are different.</p><p>To better illustrate the difference with EDSR <ref type=""bibr"" target=""#b8"">[9]</ref>, we show a comparison of model specifications in Table <ref show a comparison of model specifications in Table <ref type=""table"" target=""#tab_3"">3</ref>. EDSR <ref type=""bibr"" target=""#b8"">[9]</ref> is an outstanding model gained amazing results. However, it memory, space and datasets. In contrast, the specifications of our model is much smaller than EDSR <ref type=""bibr"" target=""#b8"">[9]</ref>, which makes it easier to reproduce and promote.</p><p>In Fi nts the upscaling factor) mixed training method is used in <ref type=""bibr"" target=""#b3"">[4]</ref>, <ref type=""bibr"" target=""#b8"">[9]</ref>, and geometric selfensemble method is proposed in <ref type= 4]</ref>, <ref type=""bibr"" target=""#b8"">[9]</ref>, and geometric selfensemble method is proposed in <ref type=""bibr"" target=""#b8"">[9]</ref>. We believe that these training tricks can also improve our",1
"target=""#b4"">[5]</ref><ref type=""bibr"" target=""#b5"">[6]</ref><ref type=""bibr"" target=""#b6"">[7]</ref><ref type=""bibr"" target=""#b7"">[8]</ref><ref type=""bibr"" target=""#b8"">[9]</ref> were proposed. EDSR < =""bibr"" target=""#b8"">[9]</ref> was the champion of the NTIRE2017 SR Challenge. It based on SRResNet <ref type=""bibr"" target=""#b7"">[8]</ref> while enhanced the network by removing the normalization lay <ref type=""bibr"" target=""#b0"">[1]</ref>, EDSR <ref type=""bibr"" target=""#b8"">[9]</ref> and SRResNet <ref type=""bibr"" target=""#b7"">[8]</ref>. During the reconstruction experiments, we find most existin N <ref type=""bibr"" target=""#b6"">[7]</ref>, LapSRN <ref type=""bibr"" target=""#b5"">[6]</ref>, SRResNet <ref type=""bibr"" target=""#b7"">[8]</ref>, and EDSR <ref type=""bibr"" target=""#b8"">[9]</ref>. Unfortuna",0
"e choose five widely used benchmark datasets: Set5 <ref type=""bibr"" target=""#b16"">[17]</ref>, Set14 <ref type=""bibr"" target=""#b17"">[18]</ref>, BSDS100 <ref type=""bibr"" target=""#b18"">[19]</ref>, Ur-ban",0
"nt network to learn the mapping between LR and HR images so that a series of CNNs-based SISR models <ref type=""bibr"" target=""#b1"">[2]</ref><ref type=""bibr"" target=""#b2"">[3]</ref><ref type=""bibr"" targe as upscaled to HR space via an upsampling operator as bicubic. However, this method has been proved <ref type=""bibr"" target=""#b1"">[2]</ref> that it will add computational complexity and produce visibl RCNN <ref type=""bibr"" target=""#b2"">[3]</ref>) and Efficient Sub-pixel Convolutional Networks (ESPCN <ref type=""bibr"" target=""#b1"">[2]</ref>). All of the models mentioned above are shallow networks (le actor (x4), with no specific instructions given to migrate to other upscaling factors. PixelShuffle <ref type=""bibr"" target=""#b1"">[2]</ref> and deconvolutional layer are widely used in SISR tasks. As fig_4"">4</ref>(ours)), which is a simple, efficient, and flexible structure. Thanks to pixelshuffle <ref type=""bibr"" target=""#b1"">[2]</ref>, our modules can be migrated to any upscaling factor with mi xSR <ref type=""bibr"" target=""#b19"">[20]</ref>, SRCNN <ref type=""bibr"" target=""#b0"">[1]</ref>, ESPCN <ref type=""bibr"" target=""#b1"">[2]</ref>, FSRCNN <ref type=""bibr"" target=""#b2"">[3]</ref>, VDSR <ref t",0
"<ref type=""bibr"" target=""#b16"">[17]</ref>, Set14 <ref type=""bibr"" target=""#b17"">[18]</ref>, BSDS100 <ref type=""bibr"" target=""#b18"">[19]</ref>, Ur-ban100 <ref type=""bibr"" target=""#b19"">[20]</ref> and M",0
"×64 and an epoch having 1000 iterations of back-propagation. We train our model with ADAM optimizer <ref type=""bibr"" target=""#b21"">[22]</ref> by setting the learning rate lr = 0.0001. In our final mod",0
"hich 91 images are from <ref type=""bibr"" target=""#b13"">[14]</ref> and the other 200 images are from <ref type=""bibr"" target=""#b14"">[15]</ref>. And some methods take ImageNet <ref type=""bibr"" target=""#",0
"e=""bibr"" target=""#b18"">[19]</ref>, Ur-ban100 <ref type=""bibr"" target=""#b19"">[20]</ref> and Manga109 <ref type=""bibr"" target=""#b20"">[21]</ref>. These datasets contain a wide variety of images that can",0
"LR and HR images so that a series of CNNs-based SISR models <ref type=""bibr"" target=""#b1"">[2]</ref><ref type=""bibr"" target=""#b2"">[3]</ref><ref type=""bibr"" target=""#b3"">[4]</ref><ref type=""bibr"" targe this, new methods are proposed, such as Fast Super-Resolution Convolutional Neural Networks (FSRCNN <ref type=""bibr"" target=""#b2"">[3]</ref>) and Efficient Sub-pixel Convolutional Networks (ESPCN <ref RCNN <ref type=""bibr"" target=""#b0"">[1]</ref>, ESPCN <ref type=""bibr"" target=""#b1"">[2]</ref>, FSRCNN <ref type=""bibr"" target=""#b2"">[3]</ref>, VDSR <ref type=""bibr"" target=""#b3"">[4]</ref>, DR-CN <ref ty",0
"information which may be head pose, expression, or landmarks <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" tar",1
"et=""#b33"">[34,</ref><ref type=""bibr"" target=""#b40"">41]</ref>, taking into account additional images <ref type=""bibr"" target=""#b32"">[33]</ref> or 3D scans <ref type=""bibr"" target=""#b3"">[4]</ref>, or by",0
"ibr"" target=""#b18"">[19]</ref> are split into train/val set, leaving out the 1, 000 test images from <ref type=""bibr"" target=""#b21"">[22]</ref> as test set. The results on the test set are reported in T linear pose predictor from the driving vector performs only slightly worse than a supervised method <ref type=""bibr"" target=""#b21"">[22]</ref>, which has been trained for this task</p></div> <div xmlns //www.tei-c.org/ns/1.0""><head>Method</head><p>Roll Pitch Yaw MAE X2Face 5.85 7.59 14.62 9.36 KEPLER <ref type=""bibr"" target=""#b21"">[22]</ref> (supervised) 8.75 5.85 6.45 7.02</p></div> <div xmlns=""htt",0
"ated set of works consider how to frontalise a face in a still image using a generic reference face <ref type=""bibr"" target=""#b13"">[14]</ref>, transferring expressions of an actor to an avatar <ref ty",0
"</ref>, taking into account additional images <ref type=""bibr"" target=""#b32"">[33]</ref> or 3D scans <ref type=""bibr"" target=""#b3"">[4]</ref>, or by learning 3DMM parameters directly from RGB data witho driving frame in Section 5.2, pose information in Section 5.3, and audio information in Section 5. <ref type=""bibr"" target=""#b3"">4</ref></p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>.</he",0
"f its neighbor pairs <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" target=""#b27"">28]</ref>. For example, SiGMa <ref type=""bibr"" target=""#b8"">[9]</ref> enerate candidate user pairs from all the pairs. Following <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b27"">28]</ref>, we only keep the user pairs if their names are similar to gate the matching scores from a confidential seed set of user pairs to their neighbor pairs. COSNET <ref type=""bibr"" target=""#b27"">[28]</ref> proposed a supervised method to infer the marginal probabi ls of the unlabeled pairs and update the model based on the inferred labels and the user attributes <ref type=""bibr"" target=""#b27"">[28]</ref>. However, error propagations may be introduced in above me fore resorting to the model, we can easily select the most useful neighbor pairs by heuristic rules <ref type=""bibr"" target=""#b27"">[28]</ref>. This paper simply selects the neighbor pairs if their nam by propagating the matching scores (predicted by SVM) through the two input networks.</p><p>COSNET <ref type=""bibr"" target=""#b27"">[28]</ref>: is a factor graph model that incorporates the attributes",1
""" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b22"">23]</ref>. For example, Niepe ""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b22"">23]</ref>. For example, Niepert et al. <ref type=""bibr"" target=""#b15"">[16]</ref> generated a representation for a selected sequence of node t matched ego networks are positioned similarly in the corresponding embedding vectors. Inspired by <ref type=""bibr"" target=""#b15"">[16]</ref>, we address the problem by firstly normalizing the adjacen",0
"een done on modeling the graph-structured input by neural networks. Shallow models such as DeepWalk <ref type=""bibr"" target=""#b17"">[18]</ref>, LINE <ref type=""bibr"" target=""#b20"">[21]</ref> and node2v",0
"Recently, the attention mechanism used in neural networks is a well-adopted way to achieve the goal <ref type=""bibr"" target=""#b1"">[2]</ref>. Inspired by this, we propose using attention mechanisms to",0
"ral networks, which assign a state to each node in a graph based on the states of neighboring nodes <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b18"">19]</ref>. Recently, Li et al.",0
"representation to implement optimizations, e.g., auto differentiation and dynamic memory management <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target tional Graphs</head><p>Computational graphs are a common way to represent programs in DL frameworks <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""7"">Related Work</head><p>Deep learning frameworks <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target on graph DSLs are a typical way to represent and perform high-level optimizations. Tensorflow's XLA <ref type=""bibr"" target=""#b2"">[3]</ref> and the recently introduced DLVM <ref type=""bibr"" target=""#b",1
"sign and Implementation 591</p><p>Despite the emerging popularity of accelerators for deep learning <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b20"">21]</ref>, it remains unclea",0
"omes intractable with large search spaces. Instead, we run a parallel simulated annealing algorithm <ref type=""bibr"" target=""#b21"">[22]</ref>. The explorer starts with random configurations, and, at e",0
"re back-end. On CPUs, memory latency hiding is achieved implicitly with simultaneous multithreading <ref type=""bibr"" target=""#b13"">[14]</ref> or hardware prefetching <ref type=""bibr"" target=""#b9"">[10,",0
"ox optimization, i.e., autotuning. This method is used to tune high performance computing libraries <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b45"">46]</ref>. However, auto-tun .</p><p>High-performance libraries such as ATLAS <ref type=""bibr"" target=""#b45"">[46]</ref> and FFTW <ref type=""bibr"" target=""#b14"">[15]</ref> use auto-tuning to get the best performance. Tensor compre",0
"that the cost benefits brought by spot offerings can be realized with intuitive bidding strategies <ref type=""bibr"" target=""#b14"">[15]</ref>. However, choosing between spot instances and bid levels a e on-demand price since the user only pays the spot price anyway, as is commonly advocated (e.g. in <ref type=""bibr"" target=""#b14"">[15]</ref>) and used in practice. Under the given model, the WSD assu",1
"ed with the supply and demand for cloud resources. Thus, unlike prior works on bidding optimization <ref type=""bibr"" target=""#b9"">[10]</ref>, our model not only explicitly accounts for the interplay b old on 2017 spot data (see Figure <ref type=""figure"" target=""#fig_4"">3</ref>).</p><p>The authors of <ref type=""bibr"" target=""#b9"">[10]</ref> used a profit-maximization model to understand spot price d y explicitly considering job deadlines <ref type=""bibr"" target=""#b15"">[16]</ref>, cost minimization <ref type=""bibr"" target=""#b9"">[10]</ref>, <ref type=""bibr"" target=""#b16"">[17]</ref>, and task depend n-demand instances and a set B t ⊂ R + of bids from B t = |B t | spot instance requests. Many works <ref type=""bibr"" target=""#b9"">[10]</ref>, <ref type=""bibr"" target=""#b22"">[23]</ref>, <ref type=""bibr f this weak assumption has basis in both previous analyses of spot markets and other auctions (e.g. <ref type=""bibr"" target=""#b9"">[10]</ref> assumes bids are drawn from U[π, π]) as well in the simple ese variables by the total number of instances, i.e. define n t = N b t instead of B t ), we follow <ref type=""bibr"" target=""#b9"">[10]</ref> and assume that all bids b ∈ B t are drawn independently fr family. A better strategy would be to consider the collective behavior of the spot prices over time <ref type=""bibr"" target=""#b9"">[10]</ref>, which we do in this section by accounting for their tempor bly ill-posed (it tends to −∞ as b t → 0), and the fact that we have constraints on the state space <ref type=""bibr"" target=""#b9"">(10)</ref>. Therefore, we need to resort to algorithms that support mo",1
"bids from B t = |B t | spot instance requests. Many works <ref type=""bibr"" target=""#b9"">[10]</ref>, <ref type=""bibr"" target=""#b22"">[23]</ref>, <ref type=""bibr"" target=""#b23"">[24]</ref> assume that the",0
"ts. Many works <ref type=""bibr"" target=""#b9"">[10]</ref>, <ref type=""bibr"" target=""#b22"">[23]</ref>, <ref type=""bibr"" target=""#b23"">[24]</ref> assume that the CP's objective is to maximize its own prof",0
"sustained-use discount <ref type=""bibr"" target=""#b0"">[1]</ref> and Amazon EC2's reserved instances <ref type=""bibr"" target=""#b1"">[2]</ref>. However, while CPs can help to stabilize their resource dem",0
"01117C0052 and No. HR001117C0048, NSF grants CCF-1302518 and CCF-1527371, and ONR-N00014-16-1-2329. <ref type=""bibr"" target=""#b5"">[6]</ref>, <ref type=""bibr"" target=""#b6"">[7]</ref>. In this work, we p",0
"empted to address this issue, e.g., through job scheduling <ref type=""bibr"" target=""#b3"">[4]</ref>, <ref type=""bibr"" target=""#b4"">[5]</ref> or optimized virtual machine placement The work in this pape",0
"e unnormalized arrival-departure random variables are likely best captured by Poisson distributions <ref type=""bibr"" target=""#b24"">[25]</ref>- <ref type=""bibr"" target=""#b26"">[27]</ref>, we find the cl",0
"clude the paper in Section V. All proofs and technical details can be found in the technical report <ref type=""bibr"" target=""#b10"">[11]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head> parameter update admits a closed-form solution, as detailed in the Appendix of the technical report <ref type=""bibr"" target=""#b10"">[11]</ref>.</p><p>Estimating the distributions of these hidden states he best strategy to use in hindsight we look T  Ci steps ahead and compute the a posteriori payment <ref type=""bibr"" target=""#b10"">(11)</ref> for each instance type i. Finally, we compare the job comp",0
"/ref>. To limit the variance in the allocated resources, <ref type=""bibr"" target=""#b20"">[21]</ref>, <ref type=""bibr"" target=""#b21"">[22]</ref> suggest that users rent resources in both the on-demand ma",0
"les <ref type=""bibr"" target=""#b50"">[51]</ref>. The related study is presented in our previous paper <ref type=""bibr"" target=""#b49"">[50]</ref>.</p><formula xml:id=""formula_15"">Hmatch 2 = 1/(norm(|P U − 6""><head></head><label></label><figDesc>) T U describes learning styles. Referring to other research<ref type=""bibr"" target=""#b49"">[50]</ref>, we design the elements of learning styles as: T U = {CL,",1
"iv xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2.1."">Learner and LO models</head><p>Martins et al. <ref type=""bibr"" target=""#b19"">[20]</ref> mentioned that the user model should be conducive to makin",0
"43]</ref> applied the probabilitybased genetic algorithms to the information filtering. Yueh et al. <ref type=""bibr"" target=""#b43"">[44]</ref> studied Markov's chain model-based meta-rules to help lear",0
"odels in adaptive hypermedia systems such as the overlay model and the uncertainty-based user model <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b21"">22]</ref>. In the applicatio h some sequential pattern mining algorithms are applied to extracting learners' learning activities <ref type=""bibr"" target=""#b20"">[21]</ref>, it is not wise to predefine or utilize constant activity",0
"tructing concepts maps for adaptive learning systems based on data mining techniques. Colace et al. <ref type=""bibr"" target=""#b41"">[42]</ref> studied the adaptive hypermedia system by using the defini",0
"not visible at the node-level.</p><p>Graph kernels based on the k-WL have been proposed in the past <ref type=""bibr"" target=""#b27"">(Morris, Kersting, and Mutzel 2017)</ref>. However, a key advantage o ref type=""bibr"" target=""#b35"">(Shervashidze et al. 2011)</ref> as well as its higher-order variants <ref type=""bibr"" target=""#b27"">(Morris, Kersting, and Mutzel 2017)</ref>. Graphlet and Weisfeiler-Le <ref type=""bibr"" target=""#b22"">(Kriege, Giscard, and Wilson 2016)</ref>, and the global-local k-WL <ref type=""bibr"" target=""#b27"">(Morris, Kersting, and Mutzel 2017)</ref> with k in {2, 3} as kernel ""foot_0"">Note that the definition of the local neighborhood is different from the the one defined in<ref type=""bibr"" target=""#b27"">(Morris, Kersting, and Mutzel 2017)</ref> which is a superset of our e that we can scale our method to larger datasets by using sampling strategies introduced in, e.g., <ref type=""bibr"" target=""#b27"">(Morris, Kersting, and Mutzel 2017;</ref><ref type=""bibr"" target=""#b1",1
"mpleting the equivalence. Since the power of the 1-WL has been completely characterized, see, e.g., <ref type=""bibr"" target=""#b0"">(Arvind et al. 2015;</ref><ref type=""bibr"" target=""#b19"">Kiefer, Schwe comings of Both Approaches</head><p>The power of 1-WL has been completely characterized, see, e.g., <ref type=""bibr"" target=""#b0"">(Arvind et al. 2015)</ref>. Hence, by using Theorems 1 and 2, this cha",0
"ef>, GraphSAGE <ref type=""bibr"" target=""#b14"">(Hamilton, Ying, and Leskovec 2017a)</ref>, SplineCNN <ref type=""bibr"" target=""#b8"">(Fey et al. 2018)</ref>, and the spectral approaches proposed in <ref r connected nodes, computed from the provided node coordinates. The code was built upon the work of <ref type=""bibr"" target=""#b8"">(Fey et al. 2018)</ref> and is provided at https: //github.com/chrsmrr",0
"type=""bibr"" target=""#b5"">(Gao et al. 2012</ref>). However, traditional hypergraph learning methods <ref type=""bibr"" target=""#b23"">(Zhou, Huang, and Schölkopf 2007)</ref> suffer from their high comput ployed to model high-order correlation among data.</p><p>Hypergraph learning is first introduced in <ref type=""bibr"" target=""#b23"">(Zhou, Huang, and Schölkopf 2007)</ref>, as a propagation process on the hypergraph structure. The task can be formulated as a regularization framework as introduced by <ref type=""bibr"" target=""#b23"">(Zhou, Huang, and Schölkopf 2007)</ref>:</p><formula xml:id=""formula_",1
"Liu, and Metaxas 2009)</ref>, hypergraph learning is further employed in video object segmentation. <ref type=""bibr"" target=""#b9"">(Huang et al. 2010</ref>) used the hypergraph structure to model image",0
"s to classify citation data. Here, two widely used citation network datasets, i.e., Cora and Pubmed <ref type=""bibr"" target=""#b17"">(Sen et al. 2008</ref>) are employed. The experimental setup follows",0
"/p><p>For spectral approaches, the convolution operation is formulated in spectral domain of graph. <ref type=""bibr"" target=""#b1"">(Bruna et al. 2014)</ref> introduces the first graph CNN, which uses t",0
"rst graph CNN, which uses the graph Laplacian eigenbasis as an analogy of the Fourier transform. In <ref type=""bibr"" target=""#b8"">(Henaff, Bruna, and LeCun 2015)</ref>, the spectral filters can be par",0
"roaches for representation learning of graphs <ref type=""bibr"" target=""#b23"">(Li et al., 2016;</ref><ref type=""bibr"" target=""#b13"">Hamilton et al., 2017a;</ref><ref type=""bibr"" target=""#b21"">Kipf &amp arget=""#b6"">Defferrard et al., 2016;</ref><ref type=""bibr"" target=""#b8"">Duvenaud et al., 2015;</ref><ref type=""bibr"" target=""#b13"">Hamilton et al., 2017a;</ref><ref type=""bibr"" target=""#b19"">Kearnes e erage via attention <ref type=""bibr"" target=""#b34"">(Velickovic et al., 2018)</ref> and LSTM pooling <ref type=""bibr"" target=""#b13"">(Hamilton et al., 2017a;</ref><ref type=""bibr"" target=""#b24"">Murphy e",1
"et=""#b31"">(Scarselli et al., 2009b;</ref><ref type=""bibr"" target=""#b3"">Battaglia et al., 2016;</ref><ref type=""bibr"" target=""#b6"">Defferrard et al., 2016;</ref><ref type=""bibr"" target=""#b8"">Duvenaud e",0
"are highly related, we regard the MRC with unanswerable questions as a multi-task learning problem <ref type=""bibr"" target=""#b1"">(Caruana 1997</ref>) by sharing some meta-knowledge.</p><p>We propose from existing work, we regard the MRC with unanswerable questions as a multi-task learning problem <ref type=""bibr"" target=""#b1"">(Caruana 1997</ref>) by sharing some metaknowledge. Intuitively, answe",1
"""bibr"" target=""#b2"">(Hermann et al. 2015;</ref><ref type=""bibr"" target=""#b3"">Hill et al. 2015;</ref><ref type=""bibr"" target=""#b8"">Rajpurkar et al. 2016)</ref>, the end-to-end neural methods have achie st systems have even surpassed human performance on the Stanford Question Answering Dataset (SQuAD) <ref type=""bibr"" target=""#b8"">(Rajpurkar et al. 2016)</ref>, one of the most widely used MRC benchma aitly 2015)</ref> is used to predict the span boundaries of the answer. Specifically, in SQuAD test <ref type=""bibr"" target=""#b8"">(Rajpurkar et al. 2016)</ref>, there are approaches to combine match-L 1.1, we first embed both the question and the passage with the following features. Glove embedding <ref type=""bibr"" target=""#b8"">(Pennington, Socher, and Manning 2014)</ref>   represented as a d-dim",0
"text. Benefiting from the rapid development of deep learning techniques and large-scale benchmarks <ref type=""bibr"" target=""#b2"">(Hermann et al. 2015;</ref><ref type=""bibr"" target=""#b3"">Hill et al. 2 e=""bibr"" target=""#b10"">(Seo et al. 2016;</ref><ref type=""bibr"" target=""#b4"">Huang et al. 2017;</ref><ref type=""bibr"" target=""#b2"">Chen et al. 2017;</ref><ref type=""bibr"" target=""#b2"">Clark and Gardner pe=""bibr"" target=""#b4"">Huang et al. 2017;</ref><ref type=""bibr"" target=""#b2"">Chen et al. 2017;</ref><ref type=""bibr"" target=""#b2"">Clark and Gardner 2017;</ref><ref type=""bibr"">Hu et al. 2017)</ref>. T g a no-answer score to the score vector of the answer span <ref type=""bibr"">(Levy et al. 2017;</ref><ref type=""bibr"" target=""#b2"">Clark and Gardner 2017)</ref>. But this kind of approaches is relative tags, NER tags and lemmas tags of each text. We use 12 dimensions to embed POS tags, 8 for NER tags <ref type=""bibr"" target=""#b2"">(Chen et al. 2017)</ref>. We use 3 binary features: exact match, lower r"" target=""#b6"">Kumar et al. 2015;</ref><ref type=""bibr"" target=""#b11"">Sukhbaatar et al. 2015;</ref><ref type=""bibr"" target=""#b2"">Cui et al. 2016;</ref><ref type=""bibr"" target=""#b14"">Xiong, Zhong, and rget=""#b2"">Cui et al. 2016;</ref><ref type=""bibr"" target=""#b14"">Xiong, Zhong, and Socher 2016;</ref><ref type=""bibr"" target=""#b2"">Dhingra et al. 2016;</ref><ref type=""bibr"" target=""#b10"">Shen et al. 2 unanswerable questions is a more challenging task. Previous work <ref type=""bibr"">Levy et al.;</ref><ref type=""bibr"" target=""#b2"">Clark and Gardner (2017;</ref><ref type=""bibr"">2017)</ref> has attempt",0
"achieved great successes for machine reading comprehension <ref type=""bibr"">(Seo et al. 2016;</ref><ref type=""bibr"" target=""#b6"">Kumar et al. 2015;</ref><ref type=""bibr"" target=""#b11"">Sukhbaatar et a",0
"s have achieved promising results on MRC task <ref type=""bibr"" target=""#b10"">(Seo et al. 2016;</ref><ref type=""bibr"" target=""#b4"">Huang et al. 2017;</ref><ref type=""bibr"" target=""#b2"">Chen et al. 2017 h p ; H f p ] are concatenated by three-level representations, we followed previous work FusionNet <ref type=""bibr"" target=""#b4"">(Huang et al. 2017</ref>) to construct their iterations on three level",0
"ion) is a critical problem for many applications such as online advertising and recommender systems <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" targe hallenging for several reasons. First, the input features are extremely sparse and high-dimensional <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" targ atures, the machine learning models are easily overfitted. Second, as shown in extensive literature <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" targ on domain experts. Moreover, it is almost impossible to hand-craft all the meaningful combinations <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b25"">26]</ref>. One may ask that we ature interactions and impractical to capture high-order feature interactions. Recently, many works <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" targ is regarded significant for CTR prediction task, which has also been pointed out in existing works <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" targ implicit feature interactions and have been widely integrated into existing CTR prediction methods <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" targ nies <ref type=""bibr"">[8-10, 15, 21, 29, 43]</ref>. For example, Google developed the Wide&amp;Deep <ref type=""bibr"" target=""#b7"">[8]</ref> learning system for recommender systems, which combines the br"" target=""#b40"">[41]</ref>, DeepCrossing <ref type=""bibr"" target=""#b31"">[32]</ref>, Wide&amp;Deep <ref type=""bibr"" target=""#b7"">[8]</ref> and DeepFM <ref type=""bibr"" target=""#b10"">[11]</ref> utilize e name the joint model AutoInt+ and compare it with the following algorithms:</p><p>? Wide&amp;Deep <ref type=""bibr"" target=""#b7"">[8]</ref>. Wide&amp;Deep integrates the outputs of logistic regression combinatorial features to yield good prediction performance <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" targe",1
"ost impossible to hand-craft all the meaningful combinations <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b25"">26]</ref>. One may ask that we can enumerate all the possible high-or e modeling different orders of feature combinations.</p><p>For example, Factorization Machines (FM) <ref type=""bibr"" target=""#b25"">[26]</ref>, which combine polynomial regression models with factoriza herefore extensively studied in the literature. A well-known example is Factorization Machines (FM) <ref type=""bibr"" target=""#b25"">[26]</ref>, which were proposed to mainly capture the first-and secon of involved feature fields, and ?(?) is a non-additive combination function, such as multiplication <ref type=""bibr"" target=""#b25"">[26]</ref> and outer product <ref type=""bibr"" target=""#b18"">[19,</ref th model names accordingly.</p><p>LR (A). LR only models the linear combination of raw features. FM <ref type=""bibr"" target=""#b25"">[26]</ref> (B). FM uses factorization techniques to model secondorder arget=""#b7"">8,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b31"">32]</ref>. Specifically, we d",1
"get=""#b10"">11,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b31"">32]</ref>. In real-world applications, a considerable percentage of u rget=""#b7"">[8,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b31"">32]</ref>, high-order feature interactions <ref type=""foot"" target=""# get=""#b10"">11,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b31"">32]</ref>. Specifically, we define the high-order combinatorial featu work-based methods <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b31"">32]</ref>, contains nd parameters, where n is the dimension of sparse f type=""bibr"" target=""#b24"">[25]</ref>, FNN <ref type=""bibr"" target=""#b40"">[41]</ref>, DeepCrossing <ref type=""bibr"" target=""#b31"">[32]</ref>, Wide&amp;Deep <ref type=""bibr"" target=""#b7"">[8]</ref> and to distinguish the different importance of second-order combinatorial features.</p><p>DeepCrossing <ref type=""bibr"" target=""#b31"">[32]</ref> (C). DeepCrossing utilizes deep fully-connected neural net",0
"ecommender systems <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" target=""#b42"">43]</ref>. Vaswani et al. <ref type=""bibr"" target=""#b35"">[36]</ref> f",0
""">[35]</ref>, text summarization <ref type=""bibr"" target=""#b29"">[30]</ref>, and recommender systems <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" ta",0
"""foot"" target=""#foot_8"">8</ref> . To prevent overfitting, we use grid search to select dropout rate <ref type=""bibr"" target=""#b33"">[34]</ref> from {0.1 -0.9} for MovieLens-1M data set, and we found dr",0
"nnected neural networks have been shown inefficient in learning multiplicative feature interactions <ref type=""bibr"" target=""#b3"">[4]</ref>. Second, since these models learn the feature interactions i",0
"d recommender systems <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b14"">15]</ref>. The performance of the prediction has a direct impact on t",0
"ns between different feature fields.</p><p>Specifically, we adopt the key-value attention mechanism <ref type=""bibr"" target=""#b21"">[22]</ref> to determine which feature combinations are meaningful. Ta =""#b18"">[19]</ref><ref type=""bibr"" target=""#b19"">[20]</ref><ref type=""bibr"" target=""#b20"">[21]</ref><ref type=""bibr"" target=""#b21"">[22]</ref><ref type=""bibr"" target=""#b22"">[23]</ref><ref type=""bibr"" t",0
", such as graphs which encode the pairwise relationships. This includes examples of social networks <ref type=""bibr"" target=""#b2"">[3]</ref>, protein interfaces <ref type=""bibr"" target=""#b3"">[4]</ref>, assification, including Cora, Citeseer, Pubmed <ref type=""bibr"" target=""#b10"">[11]</ref> and Reddit <ref type=""bibr"" target=""#b2"">[3]</ref>. Intensive experiments verify the effectiveness of our metho lf-attention strategy.</p><p>More recently, two kinds of sampling-based methods including GraphSAGE <ref type=""bibr"" target=""#b2"">[3]</ref> and FastGCN <ref type=""bibr"" target=""#b20"">[21]</ref> were d and Extensions</head><p>Relation to other sampling methods. We contrast our approach with GraphSAGE <ref type=""bibr"" target=""#b2"">[3]</ref> and FastGC-N <ref type=""bibr"" target=""#b20"">[21]</ref> regar and Pubmed <ref type=""bibr"" target=""#b10"">[11]</ref>  community different posts belong to in Reddit <ref type=""bibr"" target=""#b2"">[3]</ref>. These graphs are varying in sizes from small to large. Part set to be 16. For the Reddit dataset, the hidden dimensions are selected to be 256 as suggested by <ref type=""bibr"" target=""#b2"">[3]</ref>. The numbers of the sampling nodes for all layers excluding ""><head n=""7.1"">Alation Studies on the Adaptive Sampling</head><p>Baselines. The codes of GraphSAGE <ref type=""bibr"" target=""#b2"">[3]</ref> and FastGCNN <ref type=""bibr"" target=""#b20"">[21]</ref> provi",1
"s of sampling-based methods including GraphSAGE <ref type=""bibr"" target=""#b2"">[3]</ref> and FastGCN <ref type=""bibr"" target=""#b20"">[21]</ref> were developed for fast representation learning on graphs. thods. We contrast our approach with GraphSAGE <ref type=""bibr"" target=""#b2"">[3]</ref> and FastGC-N <ref type=""bibr"" target=""#b20"">[21]</ref> regarding the following aspects:</p><p>1. The proposed lay t_3"">4</ref> and 10 5 vertices, respectively. Following the supervised learning scenario in FastGCN <ref type=""bibr"" target=""#b20"">[21]</ref>, we use all labels of the training examples for training. ing</head><p>Baselines. The codes of GraphSAGE <ref type=""bibr"" target=""#b2"">[3]</ref> and FastGCNN <ref type=""bibr"" target=""#b20"">[21]</ref> provided by the authors are implemented inconsistently; he re set to 5. For FastGCN, we adopt the Independent-Identical-Distribution (IID) sampler proposed by <ref type=""bibr"" target=""#b20"">[21]</ref> in Eq. ( <ref type=""formula"" target=""#formula_6"">5</ref>), ggregator with the default parameters. For FastGCN, we directly make use of the provided results by <ref type=""bibr"" target=""#b20"">[21]</ref>. For the baselines and our approach, we run the experiment",1
"icient message passing across distant nodes. Current methods <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b9"">10]</ref> resort to random walks to generate neighborhoods of various hoods for the GCN update in a similar way as the random walk <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b9"">10]</ref>. However, the random walk requires extra sampling to obtain cient to deliver promising performance in our experiments.</p><p>To minimize the hybrid loss in Eq. <ref type=""bibr"" target=""#b9"">(10)</ref>, it requires to perform gradient calculations. For the netw e reduction, we implement a variant of our model by setting the trade-off parameter as λ = 0 in Eq. <ref type=""bibr"" target=""#b9"">(10)</ref>. By this, the parameters of the self-dependent function are",0
"<ref type=""bibr"" target=""#b16"">[17]</ref> learns a weight matrix for each node degree, the work by <ref type=""bibr"" target=""#b17"">[18]</ref> defines multiple-hop neighborhoods by using the powers ser nel method KLED <ref type=""bibr"" target=""#b24"">[25]</ref> and Diffusion Convolutional Network (DCN) <ref type=""bibr"" target=""#b17"">[18]</ref>. We use the reported results of KLED and DCN on Cora and P =""bibr"" target=""#b17"">[18]</ref>. We use the reported results of KLED and DCN on Cora and Pubmed in <ref type=""bibr"" target=""#b17"">[18]</ref>. We also summarize the results of GraphSAGE and FastGCN by",0
"data, such as image classification <ref type=""bibr"" target=""#b0"">[1]</ref> and machine translation <ref type=""bibr"" target=""#b1"">[2]</ref>. By making use of local connection and weight sharing, CNNs",0
"ing their features/attributes. The closest work to this vein is the Graph Convolution Network (GCN) <ref type=""bibr"" target=""#b8"">[9]</ref> that applies connections between vertices as convolution fil ons between vertices as convolution filters to perform neighborhood aggregation. As demonstrated in <ref type=""bibr"" target=""#b8"">[9]</ref>, GCNs have achieved the state-of-the-art performance on node shev expansion of the graph Laplacian to avoid the eigendecomposition. Recently, GCN is proposed in <ref type=""bibr"" target=""#b8"">[9]</ref> to simplify previous methods with first-order expansion and ing the D-dimensional feature for node v i .</p><p>GCN. The GCN model developed by Kipf and Welling <ref type=""bibr"" target=""#b8"">[9]</ref> is one of the most successful convolutional networks for gra or all datasets. We train all models using early stopping with a window size of 30, as suggested by <ref type=""bibr"" target=""#b8"">[9]</ref>, and report the results corresponding to the best validation r various learning tasks (e.g. semi-supervised learning in <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b8"">9]</ref>), how to design efficient graph convolution networks has beco",0
"nes multiple-hop neighborhoods by using the powers series of a transition matrix, and other authors <ref type=""bibr"" target=""#b18"">[19]</ref> extracted normalized neighborhoods that contain a fixed nu",0
"is includes examples of social networks <ref type=""bibr"" target=""#b2"">[3]</ref>, protein interfaces <ref type=""bibr"" target=""#b3"">[4]</ref>, and 3D meshes <ref type=""bibr"" target=""#b4"">[5]</ref>. How",0
"ral approaches define convolution on graph by using the spatial connections directly. For instance, <ref type=""bibr"" target=""#b16"">[17]</ref> learns a weight matrix for each node degree, the work by <",0
"ady an important topic <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b7"">8]</ref>, this paper mainly focus on learning the representations for",0
"ce, while providing a glimpse on its internal operation. Our network design is based on MobileNetV1 <ref type=""bibr"" target=""#b26"">[27]</ref>. It retains its simplicity and does not require any specia st is 8 to 9 times smaller than that of standard convolutions at only a small reduction in accuracy <ref type=""bibr"" target=""#b26"">[27]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head cing the dimensionality of the operating space. This has been successfully exploited by MobileNetV1 <ref type=""bibr"" target=""#b26"">[27]</ref> to effectively trade off between computation and accuracy e use ReLU6 as the non-linearity because of its robustness when used with low-precision computation <ref type=""bibr"" target=""#b26"">[27]</ref>. We always use kernel size 3 × 3 as is standard for modern intermediate expansion layer is then 64 • 6 = 384 channels.</p><p>Trade-off hyper parameters As in <ref type=""bibr"" target=""#b26"">[27]</ref> we tailor our architecture to different performance points e model size vary between 1.7M and 6.9M parameters.</p><p>One minor implementation difference, with <ref type=""bibr"" target=""#b26"">[27]</ref> is that for multipliers less than one, we apply width mult alization after every layer, and the standard weight decay is set to 0.00004. Following MobileNetV1 <ref type=""bibr"" target=""#b26"">[27]</ref> setup we use initial learning rate of 0.045, and learning ise Separable Convolutions are a key building block for many efficient neural network architectures <ref type=""bibr"" target=""#b26"">[27,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" ta",1
"ding block for many efficient neural network architectures <ref type=""bibr"" target=""#b26"">[27,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b19"">20]</ref> and we use them in",1
"d to changing the connectivity structure of the internal convolutional blocks such as in ShuffleNet <ref type=""bibr"" target=""#b19"">[20]</ref> or introducing sparsity <ref type=""bibr"" target=""#b20"">[21 tiplier parameter, and has been incorporated into efficient model designs of other networks as well <ref type=""bibr"" target=""#b19"">[20]</ref>. Following that intuition, the width multiplier approach a "" target=""#b22"">[23]</ref> and related work. In this vein our approach is similar to those taken by <ref type=""bibr"" target=""#b19"">[20,</ref><ref type=""bibr"" target=""#b21"">22]</ref> and allows to furt work architectures <ref type=""bibr"" target=""#b26"">[27,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b19"">20]</ref> and we use them in the present work as well. The basic idea",1
"ler than 1, this is a classical residual convolutional block <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b29"">30]</ref>. However, for our purposes we show that expansion ratio gre portance of residual connection has been studied extensively <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b29"">30,</ref><ref type=""bibr"" target=""#b45"">46]</ref>. The new result rep",0
"rget=""#b19"">[20]</ref> or introducing sparsity <ref type=""bibr"" target=""#b20"">[21]</ref> and others <ref type=""bibr"" target=""#b21"">[22]</ref>.</p><p>Recently, <ref type=""bibr"" target=""#b22"">[23,</ref> rk. In this vein our approach is similar to those taken by <ref type=""bibr"" target=""#b19"">[20,</ref><ref type=""bibr"" target=""#b21"">22]</ref> and allows to further improve the performance, while provid",0
"of network pruning <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"">15,</ref><ref type=""bibr"" target=""#b15"">16,",0
"""#b38"">[39]</ref> for the task of mobile semantic segmentation. DeepLabv3 adopts atrous convolution <ref type=""bibr"" target=""#b39"">[40,</ref><ref type=""bibr"" target=""#b40"">41,</ref><ref type=""bibr"" ta",0
"studied extensively <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b29"">30,</ref><ref type=""bibr"" target=""#b45"">46]</ref>. The new result reported in this paper is that the shortcut",0
"ure maps, and builds five parallel heads including (a) Atrous Spatial Pyramid Pooling module (ASPP) <ref type=""bibr"" target=""#b42"">[43]</ref> containing three 3 × 3 convolutions with different atrous",0
"e PASCAL VOC 2012 dataset <ref type=""bibr"" target=""#b2"">[3]</ref>, with extra annotated images from <ref type=""bibr"" target=""#b44"">[45]</ref> and evaluation metric mIOU.</p><p>To build a mobile model,",0
"assification, COCO object detection <ref type=""bibr"" target=""#b1"">[2]</ref>, VOC image segmentation <ref type=""bibr"" target=""#b2"">[3]</ref>. We evaluate the trade-offs between accuracy, and number of stride = 16 or 8 for denser feature maps. We conduct the experiments on the PASCAL VOC 2012 dataset <ref type=""bibr"" target=""#b2"">[3]</ref>, with extra annotated images from <ref type=""bibr"" target=""#",0
"<ref type=""bibr"" target=""#b21"">[22]</ref>.</p><p>Recently, <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" tar",0
"#b33"">[34]</ref> on COCO dataset <ref type=""bibr"" target=""#b1"">[2]</ref>. We also compare to YOLOv2 <ref type=""bibr"" target=""#b34"">[35]</ref> and original SSD (with VGG-16 <ref type=""bibr"" target=""#b5 sistent with MobileNetV1 as all layers are attached to the feature map of the same output strides.  <ref type=""bibr"" target=""#b34"">[35]</ref>. The running time is reported for the large core of the Go",0
"nce with other architectures such as Faster-RCNN <ref type=""bibr"" target=""#b35"">[36]</ref> and RFCN <ref type=""bibr"" target=""#b36"">[37]</ref> since our focus is on mobile/real-time models.</p><p>SSDLi",0
"challenged by their vulnerability to adversarial examples <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b4"">5]</ref>, which are crafted by adding small, human-imperceptible noise icted labels and probabilities of these images given by the Inception v3. more varied training data <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b9"">10]</ref>.</p><p>With the knowl >, adversarial training is the most extensively investigated way to increase the robustness of DNNs <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" targe ef type=""bibr"" target=""#b22"">[23]</ref>, one-step gradient-based methods such as fast gradient sign <ref type=""bibr"" target=""#b4"">[5]</ref> and iterative variants of gradient-based methods <ref type="" t-based methods that iteratively perturb the input with the gradients to maximize the loss function <ref type=""bibr"" target=""#b4"">[5]</ref>, momentum-based methods accumulate a velocity vector in the e-box attacks and the transferability, and act as a stronger attack algorithm than one-step methods <ref type=""bibr"" target=""#b4"">[5]</ref> and vanilla iterative methods <ref type=""bibr"" target=""#b8""> ply derived.</p><p>One-step gradient-based approaches, such as the fast gradient sign method (FGSM) <ref type=""bibr"" target=""#b4"">[5]</ref>, find an adversarial example x * by maximizing the loss func",1
"images given by the Inception v3. more varied training data <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b9"">10]</ref>.</p><p>With the knowledge of the structure and parameters of dversarial examples generated by optimization-based and iterative methods have poor transferability <ref type=""bibr"" target=""#b9"">[10]</ref>, and thus make black-box attacks less effective. On the oth sferable adversarial examples, however they usually have a low success rate for the white-box model <ref type=""bibr"" target=""#b9"">[10]</ref>, making it ineffective for blackbox attacks. Given the diff ble of Inc-v3, Inc-v4, IncRes-v2, Res-152, Inc-v3 ens3 , Inc-v3 ens4 , IncRes-v2 ens and Inc-v3 adv <ref type=""bibr"" target=""#b9"">[10]</ref>. We adopt the ensemble in logits scheme. The ensemble weigh ethods are stronger whitebox adversaries than one-step methods at the cost of worse transferability <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b23"">24]</ref>.</p><p>Optimization rget=""#b12"">[13,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" targ ensively investigated way to increase the robustness of DNNs <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b23"">24]</ref>. By injecting advers",0
"rg/ns/1.0""><head n=""3.1."">Momentum iterative fast gradient sign method</head><p>The momentum method <ref type=""bibr"" target=""#b17"">[18]</ref> is a technique for accelerating gradient descent algorithm",0
"proving the robustness <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b1"">2]</ref>. The idea of ensemble can also be applied to adversarial atta",0
"arget model and tremendous number of queries, especially for large scale datasets such as Ima-geNet <ref type=""bibr"" target=""#b18"">[19]</ref>. Such requirements are impractical in real-world applicati ""4."">Experiments</head><p>In this section, we conduct extensive experiments on the ImageNet dataset <ref type=""bibr"" target=""#b18"">[19]</ref> to validate the effectiveness of the proposed methods. We",0
"ur suite of open-source benchmark circuits. This kind of coverage has been used in related academic <ref type=""bibr"" target=""#b12"">[13]</ref> and industrial <ref type=""bibr"" target=""#b4"">[5]</ref> wor tei-c.org/ns/1.0""><head n=""6"">RELATED WORK</head><p>The prior work most similar to rfuzz is MicroGP <ref type=""bibr"" target=""#b12"">[13]</ref> which focuses on maximizing statement coverage in the HDL",1
"st of our knowledge-the first CDG technique to be designed specifically with FPGA emulation in mind <ref type=""bibr"" target=""#b8"">[9]</ref>.</p><p>In this paper we lay the ground work for applying cov",0
"itial register state as part of the input and load the values through a scan chain before each test <ref type=""bibr"" target=""#b1"">(2)</ref> we can reset all registers to a predefined value before each ack to the fuzz engine in order to guide the search of the input space. While prior industrial work <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" targ",0
"checks to find design bugs while running on the FPGA.</p><p>The technique presented by Gent et al. <ref type=""bibr"" target=""#b3"">[4]</ref> is fully automated and also works directly on the RTL circui",0
"While formal verification has shown promising results for exhaustively testing high quality designs <ref type=""bibr"" target=""#b6"">[7]</ref>, dynamic verification is still the most easily accessible an",0
"astic gradient descent (SGD) for RNNLM optimization. All networks are implemented by ESPnet toolkit <ref type=""bibr"" target=""#b36"">[37]</ref> with pytorch backend <ref type=""bibr"" target=""#b37"">[38]</",1
". , h T ′ ) (T ′ ≤ T ), interleaved with subsampling layers to reduce the computational complexity <ref type=""bibr"" target=""#b25"">[26]</ref>. The decoder network generates a probability distribution",1
"peech (TTS) synthesis is investigated in the S2S framework <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b24"">25]</ref>. Since we are interested in the usage of linguistic context",0
"arget=""#b11"">[12]</ref><ref type=""bibr"" target=""#b12"">[13]</ref> and language feature vectors (LFV) <ref type=""bibr"" target=""#b13"">[14]</ref> (cross-lingual adaptation). To obtain a multilingual S2S m",0
"ing transfer learning <ref type=""bibr"" target=""#b5"">[6]</ref><ref type=""bibr"" target=""#b6"">[7]</ref><ref type=""bibr"" target=""#b7"">[8]</ref><ref type=""bibr"" target=""#b8"">[9]</ref> and additional featur",0
"teacher-forcing accuracy does not improve for the validation set at each epoch. Scheduled sampling <ref type=""bibr"" target=""#b35"">[36]</ref> with probability 0.4 and dropout for the encoder network w",0
"eck features (BNF) <ref type=""bibr"" target=""#b9"">[10]</ref><ref type=""bibr"" target=""#b10"">[11]</ref><ref type=""bibr"" target=""#b11"">[12]</ref><ref type=""bibr"" target=""#b12"">[13]</ref> and language feat",0
"lower layer in the decoder network with the pre-trained LM <ref type=""bibr"" target=""#b18"">[19,</ref><ref type=""bibr"" target=""#b21"">22]</ref>. However, we transfer almost all parameters in a multilingu",0
"/ref>. To encourage monotonic alignments, the auxiliary Connectionist Temporal Classification (CTC) <ref type=""bibr"" target=""#b26"">[27]</ref> objective is linearly interpolated <ref type=""bibr"" target",0
"entation of speech data based on text-tospeech (TTS) synthesis is investigated in the S2S framework <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b24"">25]</ref>. Since we are inte",0
"ence (seq2seq) models. Sub-word units were used in seq2seq <ref type=""bibr"" target=""#b11"">[12]</ref><ref type=""bibr"" target=""#b12"">[13]</ref><ref type=""bibr"" target=""#b13"">[14]</ref> and RNNT <ref typ",1
"n, by adding a 1-hot language vector to the A2B system, which has been shown to boost multi-dialect <ref type=""bibr"" target=""#b22"">[23]</ref> and multilingual <ref type=""bibr"" target=""#b23"">[24]</ref> on, a simple 1-hot language ID vector has been found to be effective improving multilingual systems <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b23"">24]</ref>. We also adopt thi",0
"ch has been shown to boost multi-dialect <ref type=""bibr"" target=""#b22"">[23]</ref> and multilingual <ref type=""bibr"" target=""#b23"">[24]</ref> system performance, we find that the multilingual A2B syst been found to be effective improving multilingual systems <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b23"">24]</ref>. We also adopt this 1-hot language ID vector as additional",0
"been attracting much interest in both academia and industry <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref>. Conventional phonetically-based speech processing systems re",0
"for speech processing <ref type=""bibr"" target=""#b3"">[4]</ref><ref type=""bibr"" target=""#b4"">[5]</ref><ref type=""bibr"" target=""#b5"">[6]</ref><ref type=""bibr"" target=""#b6"">[7]</ref>. For these systems, a",0
"igh quality for most languages, which can potentially improve the performance of end-to-end systems <ref type=""bibr"" target=""#b6"">[7]</ref>.</p><p>Sub-word representations have recently seen their suc d proposed a worddependent silence model to improve ASR accuracy; for use in end-to-end ASR models, <ref type=""bibr"" target=""#b6"">[7]</ref> investigated the value of a lexicon in end-to-end ASR. Sub-w",1
"igner. We call this method pronunciation-assisted sub-word modeling (PASM), which adopts fast align <ref type=""bibr"" target=""#b8"">[9]</ref> to align a pronunciation lexicon arXiv:1811.04284v2 [cs.CL]",1
"ibr"" target=""#b6"">[7]</ref>.</p><p>Sub-word representations have recently seen their success in ASR <ref type=""bibr"" target=""#b7"">[8]</ref>. Using sub-word features has a number of benefits for ASR, i 2"">3</ref>, we report the WER results on the LibriSpeech dataset, using the parameters described in <ref type=""bibr"" target=""#b7"">[8]</ref>. We have seen that PASM significantly improves the character",1
"of tasks, including automatic speech recognition (ASR) <ref type=""bibr"" target=""#b1"">[2]</ref> [3] <ref type=""bibr"" target=""#b3"">[4]</ref> and neural machine translation <ref type=""bibr"" target=""#b4"" s the encoder, location-based attention, and LSTM decoder, with a CTC-weight of 0.5 during training <ref type=""bibr"" target=""#b3"">[4]</ref>. To fully see the effect of sub-word methods, we do not perf",0
"get=""#b11"">[12]</ref> used sub-words units in building text-independent speech recognition systems. <ref type=""bibr"" target=""#b12"">[13]</ref> improved upon sub-word methods in WFST-based speech recogn",0
"has proven to be very successful in a number of tasks, including automatic speech recognition (ASR) <ref type=""bibr"" target=""#b1"">[2]</ref> [3] <ref type=""bibr"" target=""#b3"">[4]</ref> and neural machi",0
"head><p>We conduct our experiment using the open-source end-toend speech recognition toolkit ESPnet <ref type=""bibr"" target=""#b16"">[17]</ref>. We report the ASR performance on the Wall Street Journal",0
"-word dictionary by greedily keep the most frequent co-occurring character sequences. Concurrently, <ref type=""bibr"" target=""#b14"">[15]</ref> borrow the practice in voice search <ref type=""bibr"" targe",0
"chine translation. <ref type=""bibr"" target=""#b4"">[5]</ref> proposed to use byte-pair encoding (BPE) <ref type=""bibr"" target=""#b13"">[14]</ref> to build a sub-word dictionary by greedily keep the most f",0
"bibr"" target=""#b3"">[4]</ref> and neural machine translation <ref type=""bibr"" target=""#b4"">[5]</ref> <ref type=""bibr"" target=""#b5"">[6]</ref>.</p><p>Due to lack of a pronunciation dictionary, most end-t t=""#b15"">[16]</ref> to segment words into wordpiece which maximizes the language model probability. <ref type=""bibr"" target=""#b5"">[6]</ref> augments the training data with subword segmentation sampled",0
"type=""bibr"" target=""#b10"">[11]</ref> used sub-words units in particular for detecting unseen words. <ref type=""bibr"" target=""#b11"">[12]</ref> used sub-words units in building text-independent speech r",0
"etwork which implicitly models all three. Although modular training of those components is possible <ref type=""bibr"" target=""#b0"">[1]</ref>, an end-to-end model is usually jointly optimized during tra",0
""" target=""#b1"">[2]</ref> [3] <ref type=""bibr"" target=""#b3"">[4]</ref> and neural machine translation <ref type=""bibr"" target=""#b4"">[5]</ref> <ref type=""bibr"" target=""#b5"">[6]</ref>.</p><p>Due to lack o t recent tide of adopting sub-word representations is largely driven by neural machine translation. <ref type=""bibr"" target=""#b4"">[5]</ref> proposed to use byte-pair encoding (BPE) <ref type=""bibr"" ta e also compare our systems with BPE baselines. The BPE procedure follows the algorithm described in <ref type=""bibr"" target=""#b4"">[5]</ref>. All the PASM segmentation schemes are trained using the lex",0
"end ASR. Sub-word methods have a long history of application in a number of language related tasks. <ref type=""bibr"" target=""#b10"">[11]</ref> used sub-words units in particular for detecting unseen wo",0
"ype=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b9"">10]</ref> and pre-trained components <ref type=""bibr"" target=""#b10"">[11]</ref> in order to utilize weakly supervised data, i.e. speech-to ing so, both of them achieved better performance with the end-to-end model than the cascaded model. <ref type=""bibr"" target=""#b10"">[11]</ref> conducts experiments on a larger 236 hour English-to-Frenc n previous literature <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b14"">15]</ref> in order to improve",1
"echniques to mitigate this issue include multi-task learning <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b9"">10]</ref> and pre-trained components <ref type=""bibr"" target=""#b10"">[1 both fully supervised data and also weakly supervised data, <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b9"">10]</ref> use multi-task learning to train the ST model jointly with t g and multi-task learning as proposed in previous literature <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" targ",1
"TTS model</head><p>Recent TTS systems are able to synthesize speech with close to human naturalness <ref type=""bibr"" target=""#b23"">[24]</ref>, in varied speakers' voices <ref type=""bibr"" target=""#b24"" de speaker variation, we train models using data synthesized with the single speaker TTS model from <ref type=""bibr"" target=""#b23"">[24]</ref>. This model generates more natural speech than the multi-s",0
"o-sequence modeling have led to dramatic improvements in ASR <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref> and MT <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr ""#b21"">[22]</ref> containing 16K tokens.</p><p>ASR model: Our ASR model follows the architecture of <ref type=""bibr"" target=""#b1"">[2]</ref>. We use a 5 layer bidirectional LSTM encoder, with cell size peech corpora by adding varying degrees of background noise and reverberation in the same manner as <ref type=""bibr"" target=""#b1"">[2]</ref>. The WPM shared among all models is trained with the 70M MT",0
"vements in ASR <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref> and MT <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target",0
"=""#b1"">2]</ref> and MT <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target= ns/1.0""><head>Fine-tuning set</head><p>In-domain Out-of-domain Real + one-speaker TTS synthetic 59. <ref type=""bibr"" target=""#b4"">5</ref> 19.5 Only one-speaker TTS synthetic 38.5 13.8</p><p>Table <ref",0
"th close to human naturalness <ref type=""bibr"" target=""#b23"">[24]</ref>, in varied speakers' voices <ref type=""bibr"" target=""#b24"">[25]</ref>, create novel voices by sampling from a continuous speaker",0
"target=""#b24"">[25]</ref>, create novel voices by sampling from a continuous speaker embedding space <ref type=""bibr"" target=""#b25"">[26]</ref>.</p><p>In this work, we use the TTS model trained on Libri is work, we use the TTS model trained on LibriSpeech <ref type=""bibr"" target=""#b26"">[27]</ref> from <ref type=""bibr"" target=""#b25"">[26]</ref>, except that we use a Griffin-Lim <ref type=""bibr"" target= >[24]</ref>. This model generates more natural speech than the multi-speaker model used in Sec. 5.4 <ref type=""bibr"" target=""#b25"">[26]</ref>. To ensure a fair comparison, we use a Griffin-Lim vocoder",0
"or ST. All three models represent text using the same shared English/Spanish Word Piece Model (WPM) <ref type=""bibr"" target=""#b21"">[22]</ref> containing 16K tokens.</p><p>ASR model: Our ASR model foll",0
"cture used for NLP <ref type=""bibr"" target=""#b32"">[33,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b33"">34,</ref><ref type=""bibr"" target=""#b34"">35,</ref><ref type=""bibr"" tar on heads (as well as # attention channels) in multi-head attention layers similar to Shazeer et al. <ref type=""bibr"" target=""#b33"">[34]</ref>. Please refer to the supplementary material for a detailed line parallelism have been proposed as solutions to counter these challenges.</p><p>Mesh-Tensorflow <ref type=""bibr"" target=""#b33"">[34]</ref> follows the SPMD paradigm, which extends the Single Instru",1
"ora, neural machine translation (NMT) has become a benchmark task for any architecture used for NLP <ref type=""bibr"" target=""#b32"">[33,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" ta",0
"e the effectiveness of giant convolution networks on other image datasets through transfer learning <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b22"">23]</ref>. Specifically, we",0
"ors and supporting re-materialization on every accelerator <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b13"">14]</ref>. With GPipe, each model can be specified as a sequence of l ation</head><p>In order to reduce activation memory requirements, GPipe supports re-materialization <ref type=""bibr"" target=""#b13"">[14]</ref>. During forward computation, each accelerator only stores",0
"ef> are outperformed by their deeper and larger counterparts <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b3"">4]</ref>.</p><p>While larger models have brought remarkable quality im",0
"evidence to be retrieved from Wikipedia.</p><p>We constructed a purpose-built dataset for this task <ref type=""bibr"" target=""#b15"">(Thorne et al., 2018)</ref> that contains 185,445 human-generated cla nce when constructing the dataset was the trade-off between annotation velocity and evidence recall <ref type=""bibr"" target=""#b15"">(Thorne et al., 2018)</ref>. Evidence selected by annotators was ofte ata was released through the FEVER website. 1 We used the reserved portion of the data presented in <ref type=""bibr"" target=""#b15"">Thorne et al. (2018)</ref>   </p></div> <div xmlns=""http://www.tei-c. www.tei-c.org/ns/1.0""><head n=""2.2"">Scoring Metric</head><p>We used the scoring metric described in <ref type=""bibr"" target=""#b15"">Thorne et al. (2018)</ref> to evaluate the submissions. The FEVER sha pe=""table"" target=""#tab_2"">2</ref>). 19 of these teams scored higher than the baseline presented in <ref type=""bibr"" target=""#b15"">Thorne et al. (2018)</ref>. All participating teams were invited to s her individually or as a group, can be used as evidence. We retained the annotation guidelines from <ref type=""bibr"" target=""#b15"">Thorne et al. (2018)</ref> (see Sections A.7.1, A.7.3 and A.8 from th rom 86 submissions from 23 teams. 19 of these teams exceeded the score of the baseline presented in <ref type=""bibr"" target=""#b15"">Thorne et al. (2018)</ref>. For the teams which provided a system des",1
"sing vector representations of named entities. FujiXerox report representing sentences using DEISTE <ref type=""bibr"" target=""#b17"">(Yin et al., 2018)</ref>.</p><p>Training: BUPT-NLPer and SWEEPer mode ponent, we adopted DEISTE (Deep Explorations of Inter-Sentence interactions for Textual Entailment) <ref type=""bibr"" target=""#b17"">(Yin et al., 2018)</ref> model that is the state-of-the-art in RTE ta",0
"ibr"" target=""#b7"">(Khot et al., 2018)</ref> and a Gradient-Boosted Decision Trees (TalosTree) model <ref type=""bibr"" target=""#b1"">(Baird et al., 2017)</ref> for this task.</p><p>The experiments show t",0
"Transformer Model <ref type=""bibr"" target=""#b12"">(Radford and Salimans, 2018)</ref>, Random Forests <ref type=""bibr"" target=""#b14"">(Svetnik et al., 2003)</ref> and ensembles thereof, these are not spe",0
"against Wikipedia page titles or article bodies. BUPT-NLPer report using S-MART for entity linking <ref type=""bibr"" target=""#b16"">(Yang and Chang, 2015)</ref> and the highest scor-ing team, UNC-NLP,",0
"013) and</ref><ref type=""bibr"">TransH (Wang et al. 2014</ref>)), and bilinear models, e.g. DistMult <ref type=""bibr"" target=""#b22"">(Yang et al. 2015)</ref> and ComplEx <ref type=""bibr"" target=""#b20"">( t=""#b17"">(Nickel, Tresp, and Kriegel 2011)</ref>, a collective matrix factorization model. DistMult <ref type=""bibr"" target=""#b22"">(Yang et al. 2015)</ref> constrains W r as a diagonal matrix which re dness between embeddings h and t under the condition of relation r and is first adopted by DistMult <ref type=""bibr"" target=""#b22"">(Yang et al. 2015)</ref>. We employ this triple modeling technique fo istic KG embedding models TransE <ref type=""bibr"" target=""#b1"">(Bordes et al. 2013)</ref>, DistMult <ref type=""bibr"" target=""#b22"">(Yang et al. 2015)</ref> and ComplEx <ref type=""bibr"" target=""#b20"">( llon et al. 2016)</ref>, have achieved promising performance in many tasks, such as link prediction <ref type=""bibr"" target=""#b22"">(Yang et al. 2015;</ref><ref type=""bibr"" target=""#b20"">Trouillon et a ""bibr"" target=""#b1"">(Bordes et al. 2013;</ref><ref type=""bibr"" target=""#b20"">Wang et al. 2014;</ref><ref type=""bibr"" target=""#b22"">Yang et al. 2015)</ref>, while NL27k is a larger dataset. PPI5k is a",1
"ities, using the scoring function f (h, r, t) = h T W r t. This function is first adopted by RESCAL <ref type=""bibr"" target=""#b17"">(Nickel, Tresp, and Kriegel 2011)</ref>, a collective matrix factoriz",0
"sive efforts have been devoted into embedding deterministic KGs. Translational models, e.g., TransE <ref type=""bibr"" target=""#b1"">(Bordes et al. 2013) and</ref><ref type=""bibr"">TransH (Wang et al. 201 the entity embeddings projected in a relation-specific space. The forerunner of this family, TransE <ref type=""bibr"" target=""#b1"">(Bordes et al. 2013)</ref>, lays h r and t r in a common space as h an elines are considered in our comparison, which include (i) deterministic KG embedding models TransE <ref type=""bibr"" target=""#b1"">(Bordes et al. 2013)</ref>, DistMult <ref type=""bibr"" target=""#b22"">(Y et=""#b19"">(Szklarczyk et al. 2016)</ref> respectively. CN15k matches the number of nodes with FB15k <ref type=""bibr"" target=""#b1"">(Bordes et al. 2013</ref>) -the widely used benchmark dataset for dete ""#b1"">(Bordes et al. 2013</ref>) -the widely used benchmark dataset for deterministic KG embeddings <ref type=""bibr"" target=""#b1"">(Bordes et al. 2013;</ref><ref type=""bibr"" target=""#b20"">Wang et al. 2",0
"ation facts that describe semantic relations between entities; (ii) Uncertain KGs including ProBase <ref type=""bibr"" target=""#b21"">(Wu et al. 2012)</ref>, ConceptNet <ref type=""bibr"" target=""#b19"">(Sp preting real-world concepts that are ambiguous or intrinsically vague. The probabilistic KG Probase <ref type=""bibr"" target=""#b21"">(Wu et al. 2012)</ref> provides a prior probability distribution of c Net mainly come from the cooccurrence frequency of the labels in crowdsourced task results. Probase <ref type=""bibr"" target=""#b21"">(Wu et al. 2012</ref>) is a universal probabilistic taxonomy built by",0
"+ e −(wx+b)<label>(3)</label></formula><p>Bounded rectifier. Another mapping is a bounded rectifier <ref type=""bibr"" target=""#b3"">(Chen et al. 2015)</ref>:</p><formula xml:id=""formula_3"">φ(x) = min(ma",0
"bilinear models, e.g. DistMult <ref type=""bibr"" target=""#b22"">(Yang et al. 2015)</ref> and ComplEx <ref type=""bibr"" target=""#b20"">(Trouillon et al. 2016)</ref>, have achieved promising performance in es et al. 2013)</ref>, DistMult <ref type=""bibr"" target=""#b22"">(Yang et al. 2015)</ref> and ComplEx <ref type=""bibr"" target=""#b20"">(Trouillon et al. 2016)</ref>, (ii) an uncertain graph embedding mode mance in many tasks, such as link prediction <ref type=""bibr"" target=""#b22"">(Yang et al. 2015;</ref><ref type=""bibr"" target=""#b20"">Trouillon et al. 2016)</ref>, relation extraction (Weston, <ref type= behind a term that has critically supported short text understanding tasks involving disambiguation <ref type=""bibr"" target=""#b20"">(Wang et al. 2015;</ref><ref type=""bibr"">Wang and Wang 2016)</ref>. F ark dataset for deterministic KG embeddings <ref type=""bibr"" target=""#b1"">(Bordes et al. 2013;</ref><ref type=""bibr"" target=""#b20"">Wang et al. 2014;</ref><ref type=""bibr"" target=""#b22"">Yang et al. 201 is based on mean reciprocal rank (MRR) on the validation set. We adopt the implementation given by <ref type=""bibr"" target=""#b20"">(Trouillon et al. 2016</ref>) and choose the best hyper-parameters fo n facts from lowconfidence ones.</p><p>Evaluation protocol We follow a procedure that is similar to <ref type=""bibr"" target=""#b20"">(Wang et al. 2014)</ref>. Our test set consists of relation facts fro",0
"n the same train/validation/test splits of the same three datasets (CORA, CiteSeer and PubMed) from <ref type=""bibr"" target=""#b15"">Yang et al. [2016]</ref>. Such experimental setup favors the model th consider the problem of semi-supervised transductive node classification in a graph, as defined in <ref type=""bibr"" target=""#b15"">Yang et al. [2016]</ref>. In this paper we compare the four following raged over all datasets. See text for the definition. (b) Model accuracy on the Planetoid split from<ref type=""bibr"" target=""#b15"">Yang et al. [2016]</ref> and another split on the same datasets. Diff ute the following simple experiment. We run the 4 models on the datasets and respective splits from <ref type=""bibr"" target=""#b15"">[Yang et al., 2016]</ref>. As shown in Table <ref type=""table"" target",1
"tuned training procedure and / or hyperparameter configuration that unfairly benefits the new model <ref type=""bibr"" target=""#b7"">[Lipton and Steinhardt, 2018]</ref>.</p><p>In this paper we address th xperimental setups makes it hard to empirically identify the driver behind the improved performance <ref type=""bibr"" target=""#b7"">[Lipton and Steinhardt, 2018]</ref>. Thus, in our experiments we use a",1
"et al., 2017]</ref>, GraphSage <ref type=""bibr"" target=""#b3"">[Hamilton et al., 2017]</ref> and GAT <ref type=""bibr"" target=""#b14"">[Velickovic et al., 2018]</ref> -within the same framework. 1 In our ture and allows to learn adaptive convolution filters. The authors of Graph Attention Network (GAT) <ref type=""bibr"" target=""#b14"">[Velickovic et al., 2018]</ref> propose an attention mechanism that a",0
"head><p>Datasets For our experiments, we used the four well-known citation network datasets: PubMed <ref type=""bibr"" target=""#b12"">[Namata et al., 2012]</ref>, CiteSeer and CORA from <ref type=""bibr""",0
"structure. Label Propagation (LabelProp) and Normalized Laplacian Label Propagation (LabelProp NL) <ref type=""bibr"" target=""#b1"">[Chapelle et al., 2009]</ref>, on the other hand, only consider the gr",0
"nd Welling, 2017]</ref>, MoNet <ref type=""bibr"" target=""#b11"">[Monti et al., 2017]</ref>, GraphSage <ref type=""bibr"" target=""#b3"">[Hamilton et al., 2017]</ref> and GAT <ref type=""bibr"" target=""#b14"">[ llows to weigh nodes in the neighborhood differently during the aggregation step. Lastly, GraphSAGE <ref type=""bibr"" target=""#b3"">[Hamilton et al., 2017]</ref> focuses on inductive node classification",0
"performed equally carefully for all methods <ref type=""bibr"" target=""#b10"">[Melis et al., 2018</ref><ref type=""bibr"" target=""#b8"">, Lucic et al., 2017]</ref>. In future work, we plan to further invest",0
"further into account that the predictions of GNNs can greatly change under small data perturbations <ref type=""bibr"" target=""#b16"">[Zügner et al., 2018]</ref> clearly confirms the need for evaluation",0
"orm more sophisticated ones if hyperparameter tuning is performed equally carefully for all methods <ref type=""bibr"" target=""#b10"">[Melis et al., 2018</ref><ref type=""bibr"" target=""#b8"">, Lucic et al.",0
"datasets: PubMed <ref type=""bibr"" target=""#b12"">[Namata et al., 2012]</ref>, CiteSeer and CORA from <ref type=""bibr"" target=""#b13"">Sen et al. [2008]</ref>, as well as the extended version of CORA from",0
"nformation retrieval <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b33"">34,</ref><ref type=""bibr"" target=""#b46"">47,</ref><ref type=""bibr"" target=""#b63"">64]</ref>. A survey on the at",1
"mentioning that our model is applicable to many other scenarios, such as personalized movie search <ref type=""bibr"" target=""#b44"">[45]</ref> and academic article search <ref type=""bibr"" target=""#b56""",1
"f type=""bibr"" target=""#b57"">[58,</ref><ref type=""bibr"" target=""#b60"">61]</ref>. Recently, Ai et al. <ref type=""bibr"" target=""#b0"">[1]</ref> have presented a personalized product search method, which f roduct Search 19:5 for products is also critical to persuade users to purchase. Recently, Ai et al. <ref type=""bibr"" target=""#b0"">[1]</ref> proposed a personalized product search model and took into c hood Model <ref type=""bibr"" target=""#b68"">[69]</ref> and Extended Query Likelihood with User Models <ref type=""bibr"" target=""#b0"">[1]</ref> and (2) representation learning approaches based on latent s nt Semantic Entity <ref type=""bibr"" target=""#b58"">[59]</ref> and Hierarchical Embedding Model (HEM) <ref type=""bibr"" target=""#b0"">[1]</ref>. It is worth noting that the recently proposed HEM is the st h User Models (UQL). This model is first introduced to the personalized product search by Ai et al. <ref type=""bibr"" target=""#b0"">[1]</ref>. Specifically, let U be the set of the most frequent words < 2 ? i ? 4}.</formula><p>Hierarchical Embedding Model (HEM). This model (HEM) proposed in Reference <ref type=""bibr"" target=""#b0"">[1]</ref> is the state-of-the-art approach for the personalized produc oduct as the query in retrieval. Based on this observation and following the strategy of References <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b58"">59]</ref>, for each product a",1
"able yet in fact, it changes over time slightly and slowly <ref type=""bibr"" target=""#b57"">[58,</ref><ref type=""bibr"" target=""#b60"">61]</ref>. Recently, Ai et al. <ref type=""bibr"" target=""#b0"">[1]</ref nstruct the short-term user profile; methods in References <ref type=""bibr"" target=""#b57"">[58,</ref><ref type=""bibr"" target=""#b60"">61]</ref> determine the extent of using context (e.g., prior queries",0
"with different sizes: Phones, Toys, Clothing, and Electronics. Following the strategy in References <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b37"">38]</ref>, we extracted the",0
"in the first line <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b42"">43,</ref><ref type=""bibr"" target=""#b43"">44,</ref><ref type=""bibr"" tar",0
"and probabilistic similarities between queries to determine the duration of a session; Daoud et al. <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b14"">15]</ref> leveraged a predef",0
"as personalized movie search <ref type=""bibr"" target=""#b44"">[45]</ref> and academic article search <ref type=""bibr"" target=""#b56"">[57]</ref>.</p><p>In summary, our main contributions of this article",0
"type=""bibr"" target=""#b1"">[2]</ref> and information retrieval <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b33"">34,</ref><ref type=""bibr"" target=""#b46"">47,</ref><ref type=""bibr"" tar ism on user preference modeling. To name just a few, in the field of recommender systems, Li et al. <ref type=""bibr"" target=""#b33"">[34]</ref> introduced a neural attentive model to the session-based r sfully applied in the session-based product recommendation <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b33"">34]</ref>.</p><p>GRU was proposed in Reference <ref type=""bibr"" targe",0
"profile-based web search. Approaches in the first category <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b28"">29,</ref><ref type=""bibr"" target=""#b54"">55]</ref> capture the short-t ous interactions containing past submitted queries and clicked records within a specific time limit <ref type=""bibr"" target=""#b28"">[29]</ref>. 3 Pairs of queries and the corresponding purchased produc",0
"query could capture the user's search intention more accurately and avoid the risk of ""query drift"" <ref type=""bibr"" target=""#b70"">[71]</ref>. To achieve the goal, before fusing LTP, STP and the curre",0
"r Settings. In the training procedure of ALSTP, the parameters are initialized by the xavier method <ref type=""bibr"" target=""#b19"">[20]</ref> and then optimized with the standard Stochastic Gradient D",0
"of clicks providing a strong signal of a user's interest in an item, the methods in the first line <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" ta",0
"that the long-term user preference is stable yet in fact, it changes over time slightly and slowly <ref type=""bibr"" target=""#b57"">[58,</ref><ref type=""bibr"" target=""#b60"">61]</ref>. Recently, Ai et a fined ontology of semantic concepts to construct the short-term user profile; methods in References <ref type=""bibr"" target=""#b57"">[58,</ref><ref type=""bibr"" target=""#b60"">61]</ref> determine the exte",0
"t=""#b40"">[41,</ref><ref type=""bibr"" target=""#b50"">51]</ref> also share the similar idea. Guo et al. <ref type=""bibr"" target=""#b21"">[22]</ref> pointed out that the aforementioned methods only focus on acteristics. Pang et al. <ref type=""bibr"" target=""#b41"">[42]</ref> extended the method in Reference <ref type=""bibr"" target=""#b21"">[22]</ref> to better capture the intrinsic relevance and simulate the",0
"vely search the tree-structured architecture space. Motivated by these AutoML frameworks, He et al. <ref type=""bibr"" target=""#b10"">[11]</ref> leveraged the reinforcement learning to automatically prun",1
"improve the hardware efficiency, many researchers have proposed to directly design efficient models <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" ta factors influencing the hardware performance, such as memory access cost and degree of parallelism <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b20"">21]</ref>. Taking computatio e proxy signals (e.g., FLOPs, number of memory references) <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b24"">25]</ref>. However, as different hardware behaves very differently, t y study the quantization of MobileNet-V1 <ref type=""bibr"" target=""#b12"">[13]</ref> and MobileNet-V2 <ref type=""bibr"" target=""#b24"">[25]</ref>. Both MobileNets are inspired from the depthwise separable",0
"ef> edge accelerator, HW3: BISMO cloud accelerator, batch = 16).</p><p>both weights and activations <ref type=""bibr"" target=""#b13"">[14]</ref>. Besides industry, recently academia also works on the bit",0
"/p><p>We explain the difference of quantization policy between edge and cloud by the roofline model <ref type=""bibr"" target=""#b27"">[28]</ref>. Many previous works use FLOPs or BitOPs as metrics to mea",0
"h parallelism, the network appears to be computation bounded.</p><p>our edge device and Xilinx VU9P <ref type=""bibr"" target=""#b28"">[29]</ref> as our cloud device. Table <ref type=""table"" target=""#tab_",0
"o-rank (LTR) is challenging due to its biased nature. To address this bias problem, Joachims et al. <ref type=""bibr"" target=""#b19"">[19]</ref> proposed a counterfactual inference approach, providing an f type=""bibr"" target=""#b18"">[18]</ref>.</p><p>To handle biases in a principled way, Joachims et al. <ref type=""bibr"" target=""#b19"">[19]</ref> introduced an unbiased learning-to-rank framework, which i ecting the examination bias in learning to rank from implicit feedback. As shown by Joachims et al. <ref type=""bibr"" target=""#b19"">[19]</ref>, the parameters of the PBM can serve as propensity estimat ent d for query q.</p><p>While Pr(E = 1|k) can be used as an estimate of the examination propensity <ref type=""bibr"" target=""#b19"">[19]</ref>, it is a rather simplistic model since it assumes that exa max ]. In this case, randomly swapping results at positions k and k ′ before presenting the ranking <ref type=""bibr"" target=""#b19"">[19]</ref> makes the expected relevance of results at the two positio ck-through rates is a consistent estimator of the relative propensities p k and p k ′ under the PBM <ref type=""bibr"" target=""#b19"">[19]</ref>. Note that knowing the relative propensities with respect sufficient, since the counterfactual ERM learning objective is invariant to multiplicative scaling <ref type=""bibr"" target=""#b19"">[19]</ref>.</p><p>While this ratio estimator is a sensible approach f interventions were then used to get a gold-standard estimate of the propensities via the methods in <ref type=""bibr"" target=""#b19"">[19]</ref>. To avoid any confounding due to changes in the query dist <ref type=""table"" target=""#tab_0"">1</ref>. We then use the gold-standard propensity estimator from <ref type=""bibr"" target=""#b19"">[19]</ref> to learn two PBM models from the swap intervention data, o be expected, given that AllPairs makes more efficient use of the data than the ratio-estimates from <ref type=""bibr"" target=""#b19"">[19]</ref>.</p><p>Can AllPairs learn CPBM models with many context fe nce compared to using the propensities from the PBM.</p><p>We trained a Clipped Propensity SVM-Rank <ref type=""bibr"" target=""#b19"">[19]</ref> for each of the following three propensity models: PBM est ed via cross-validation. For rank r &gt; 21, we impute the propensity p r (x) = p 21 (x). Following <ref type=""bibr"" target=""#b19"">[19]</ref>, we measure test-set ranking performance via the average s imitations of existing propensity estimation methods for LTR <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b19"">19,</ref><ref type=""bibr"" target=""#b27"">27]</ref>. First, existing me or LTR algorithms like <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b19"">19]</ref>. We evaluate the fidelity of the CPBM model and the effecti <ref type=""bibr"" target=""#b23"">[23]</ref>. The most effective methods use randomized interventions <ref type=""bibr"" target=""#b19"">[19,</ref><ref type=""bibr"" target=""#b26"">26]</ref>, which unfortunate first review how explicit interventions have been used for estimating p k := Pr(E = 1|k) in the PBM <ref type=""bibr"" target=""#b19"">[19,</ref><ref type=""bibr"" target=""#b26"">26]</ref>. The PBM requires",1
"ike UBM <ref type=""bibr"" target=""#b13"">[13]</ref>, DBN <ref type=""bibr"" target=""#b8"">[8]</ref>, CCM <ref type=""bibr"" target=""#b14"">[14]</ref> and CSM <ref type=""bibr"" target=""#b5"">[6]</ref> were propo",0
"type of data suffers from various biases due to both the system and the user, such as position bias <ref type=""bibr"" target=""#b17"">[17]</ref>, presentation bias <ref type=""bibr"" target=""#b22"">[22]</re",0
"mpute context-dependent propensities for LTR algorithms like <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b19"">19]</ref>. We evaluate the fide s been commonly adopted for unbiased evaluation and learning <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b12"">12,</ref><ref type=""bibr"" targe",0
"taken into consideration, like query performance predictors <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b11"">11]</ref>.</p><p>Table <ref type=""table"" target=""#tab_1"">2</ref> show",0
"based methods <ref type=""bibr"" target=""#b55"">[56]</ref>, <ref type=""bibr"" target=""#b32"">[33]</ref>, <ref type=""bibr"" target=""#b5"">[6]</ref>, <ref type=""bibr"" target=""#b51"">[52]</ref> have demonstrated",1
"been employed <ref type=""bibr"" target=""#b48"">[49]</ref>, <ref type=""bibr"" target=""#b57"">[58]</ref>, <ref type=""bibr"" target=""#b58"">[59]</ref>. Kemelmacher et al. <ref type=""bibr"" target=""#b28"">[29]</r",0
""">[42]</ref>, <ref type=""bibr"" target=""#b53"">[54]</ref>, <ref type=""bibr"" target=""#b50"">[51]</ref>, <ref type=""bibr"" target=""#b23"">[24]</ref>, but CNN is rarely explored for video-based dense face rec etwork training <ref type=""bibr"" target=""#b3"">[4]</ref>, <ref type=""bibr"" target=""#b52"">[53]</ref>. <ref type=""bibr"" target=""#b23"">[24]</ref> proposes to directly regress volumes with CNN for a single our method with <ref type=""bibr"" target=""#b41"">[42]</ref>, <ref type=""bibr"" target=""#b2"">[3]</ref>, <ref type=""bibr"" target=""#b23"">[24]</ref>, <ref type=""bibr"" target=""#b45"">[46]</ref>, <ref type=""bib rehead regions). The reconstruction results of the methods <ref type=""bibr"" target=""#b2"">[3]</ref>, <ref type=""bibr"" target=""#b23"">[24]</ref>, <ref type=""bibr"" target=""#b45"">[46]</ref>, <ref type=""bib the results of <ref type=""bibr"" target=""#b41"">[42]</ref>, <ref type=""bibr"" target=""#b2"">[3]</ref>, <ref type=""bibr"" target=""#b23"">[24]</ref>, <ref type=""bibr"" target=""#b45"">[46]</ref>, <ref type=""bib",0
"ructure of a 3D face from a single image <ref type=""bibr"" target=""#b3"">[4]</ref> or multiple images <ref type=""bibr"" target=""#b1"">[2]</ref>, the facial details like wrinkles and folds are not possible",0
"fective low-dimensional representation in terms of latent variables and corresponding basis vectors <ref type=""bibr"" target=""#b49"">[50]</ref>. The model has been widely used in various computer vision",0
"statistics, there are tens of millions of compounds available in compound and bioactivity databases <ref type=""bibr"" target=""#b0"">[1]</ref><ref type=""bibr"" target=""#b1"">[2]</ref><ref type=""bibr"" targe cade. In this sense, the prominent bioactivity and compound data resources can be listed as PubChem <ref type=""bibr"" target=""#b0"">[1]</ref>, ChEMBL <ref type=""bibr"" target=""#b1"">[2]</ref>, DrugBank <r and nearly 2700 of human proteins are known to be targeted by either approved or experimental drugs <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b5"">6]</ref>. The 3D structure info",1
"se, VS has the potential to greatly reduce the cost and time required for high-throughput screening <ref type=""bibr"" target=""#b9"">[10]</ref>. Although the main purpose of VS is to identify new drug ca",0
"CNN architecture with a mixture of PCM and structurebased DTI prediction approach was also proposed <ref type=""bibr"" target=""#b248"">[249]</ref>. The method takes protein 3D structure information (i.e.",0
"1"">[112]</ref><ref type=""bibr"" target=""#b112"">[113]</ref><ref type=""bibr"" target=""#b113"">[114]</ref><ref type=""bibr"" target=""#b114"">[115]</ref><ref type=""bibr"" target=""#b115"">[116]</ref><ref type=""bib",0
"e queue was extensively studied around 2000. A comprehensive survey was carried out by Abella et al <ref type=""bibr"" target=""#b23"">[28]</ref>.</p><p>Butler et al. investigated the effect of several se",1
"e processors of Intel Sandy Bridge, Haswell, and Skylake <ref type=""bibr"" target=""#b12"">[14]</ref>- <ref type=""bibr"" target=""#b14"">[16]</ref>, and is also used as the main IQ of IBM POWER7 and 8 <ref me> 				</org> 			</listOrg> 			<div type=""annex""> <div xmlns=""http://www.tei-c.org/ns/1.0""><p>Fig. <ref type=""bibr"" target=""#b14"">16</ref>. IPC increase when varying the size of the processor.</p><p>",0
"se it is a main cause of issue conflicts. We used the number of function units in an ARM Cortex-A72 <ref type=""bibr"" target=""#b17"">[20]</ref>, which is a state-of-theart high-performance and very ener nd reorder buffer to 64 and 128, respectively, because their sizes in the Cortex-A72 are 66 and 128 <ref type=""bibr"" target=""#b17"">[20]</ref>, <ref type=""bibr"" target=""#b18"">[21]</ref>. We do not choo",0
"ch Prediction:</head><p>We estimate branch prediction confidence using saturated resetting counters <ref type=""bibr"" target=""#b1"">[3]</ref>. The conf tab, where the index is the PC of a branch (? ? ??",0
"alone or with an age matrix are used in modern processors <ref type=""bibr"" target=""#b9"">[11]</ref>- <ref type=""bibr"" target=""#b11"">[13]</ref>. In this paper, we assume a random queue without an age ma ef>- <ref type=""bibr"" target=""#b14"">[16]</ref>, and is also used as the main IQ of IBM POWER7 and 8 <ref type=""bibr"" target=""#b11"">[13]</ref>, <ref type=""bibr"" target=""#b15"">[17]</ref>. In contrast, A ring. To mitigate the IPC degradation, several processors <ref type=""bibr"" target=""#b9"">[11]</ref>- <ref type=""bibr"" target=""#b11"">[13]</ref> add a circuit called the age matrix to the IQ.</p><p>The a modern processors, a random queue with age matrix is used <ref type=""bibr"" target=""#b9"">[11]</ref>- <ref type=""bibr"" target=""#b11"">[13]</ref>. Because the position-based select logic cannot implement s=""http://www.tei-c.org/ns/1.0"" xml:id=""fig_12""><head>Fig. 13 .</head><label>13</label><figDesc>Fig.<ref type=""bibr"" target=""#b11"">13</ref>. Comparison of performance with processor with a large branc",0
"pe=""bibr"" target=""#b30"">[31]</ref>. A hybrid approach has been introduced to increase its viability <ref type=""bibr"" target=""#b31"">[32]</ref>.</p><p>The concept of STRAIGHT <ref type=""bibr"" target=""#b",1
"enabled by the ISA.</p><p>The concept of the ISA to eliminate register renaming is presented herein <ref type=""bibr"" target=""#b6"">[7]</ref>; however, the method to realize the microarchitecture and th resented to solve the false dependency hazard by the compiler, which eliminates register overwrites <ref type=""bibr"" target=""#b6"">[7]</ref>. Because the number of registers is finite, each register is to increase its viability <ref type=""bibr"" target=""#b31"">[32]</ref>.</p><p>The concept of STRAIGHT <ref type=""bibr"" target=""#b6"">[7]</ref> reduces the required hardware and improves the performance a",0
"everal tens of cycles with the 256-entry ROB on average, which considerably affects the performance <ref type=""bibr"" target=""#b13"">[14]</ref>.</p><p>To be exact, a CAM-based RMT can avoid this penalty significant for the superscalar. The effect is around 20% that is similar amount to the reported in <ref type=""bibr"" target=""#b13"">[14]</ref> on the integer programs when RAM based RMT and ROB walking",0
"r (ROB) scalability. ? The compilation algorithm that generates STRAIGHT machine code from the LLVM <ref type=""bibr"" target=""#b7"">[8]</ref> intermediate representation (IR) is developed. Padding with l?for?cond : SLT [2] [4] BEZ [1] Label?for?end ST [4] [7] ADDi [8] 4 # &amp;arr[i]</formula><p>RMOV <ref type=""bibr"" target=""#b7"">[8]</ref> # N RMOV <ref type=""bibr"" target=""#b7"">[8]</ref> # ?RETADDR [4] [7] ADDi [8] 4 # &amp;arr[i]</formula><p>RMOV <ref type=""bibr"" target=""#b7"">[8]</ref> # N RMOV <ref type=""bibr"" target=""#b7"">[8]</ref> # ?RETADDR ADDi <ref type=""bibr"" target=""#b7"">[8]</ref> 1 # type=""bibr"" target=""#b7"">[8]</ref> # N RMOV <ref type=""bibr"" target=""#b7"">[8]</ref> # ?RETADDR ADDi <ref type=""bibr"" target=""#b7"">[8]</ref> 1 # ++i J Label?for?cond Label?for?end : JR <ref type=""bibr""",0
"ibr"" target=""#b19"">[19]</ref>- <ref type=""bibr"" target=""#b21"">[21]</ref>, architecture optimization <ref type=""bibr"" target=""#b22"">[22]</ref>, and many more. A number of programable performance measur data centers and then they provide several insights for server architecture design in data centers <ref type=""bibr"" target=""#b22"">[22]</ref>. Chen et al. leveraged hardware-event sampling to generate",1
"the time axis of one (or both) sequences to achieve a better alignment. Dynamic time wrapping (DTW) <ref type=""bibr"" target=""#b40"">[40]</ref> is a technique for efficiently achieving this wrapping. It",0
"A number of programable performance measurement tools have therefore been developed, including PAPI <ref type=""bibr"" target=""#b23"">[23]</ref>, VTune <ref type=""bibr"" target=""#b24"">[24]</ref>, Perfmon",0
"ly provide 4-8 hardware counters to measure hundreds of crucial events such as cache and TLB misses <ref type=""bibr"" target=""#b1"">[1]</ref>- <ref type=""bibr"" target=""#b4"">[4]</ref>. These events can g tired instructions) and eight programmable counters per core (four per SMT thread if it is enabled) <ref type=""bibr"" target=""#b1"">[1]</ref>.</p><p>The events that can be measured by hardware counters",0
"#b7"">[7]</ref>- <ref type=""bibr"" target=""#b14"">[14]</ref>, performance optimization of applications <ref type=""bibr"" target=""#b15"">[15]</ref>- <ref type=""bibr"" target=""#b18"">[18]</ref>, compiler optim <p>1) Measurement Errors: while some prior works such as <ref type=""bibr"" target=""#b14"">[14]</ref>, <ref type=""bibr"" target=""#b15"">[15]</ref> employ OCOE to measure performance, MLPX is mandatory when ns. We find the GEV distribution best fits the long tail distributions, which has been confirmed by <ref type=""bibr"" target=""#b15"">[15]</ref>.</p><p>After knowing the value distributions of the events re counters, Zhang et al. proposed a CPU performance isolation strategy for shared compute clusters <ref type=""bibr"" target=""#b15"">[15]</ref>. Tam et al. firstly carefully analyzed the L2 cache miss r",0
"ibr"" target=""#b13"">[14]</ref>- <ref type=""bibr"" target=""#b16"">[17]</ref>, issuing prefetch requests <ref type=""bibr"" target=""#b17"">[18]</ref>, etc. However, the impact of these optimizations has not b ing into the dependence chain between them. For instance, we show that one such recent optimization <ref type=""bibr"" target=""#b17"">[18]</ref>, which prioritizes critical loads, does very well for SPEC 28]</ref>, caches <ref type=""bibr"" target=""#b7"">[8]</ref>, <ref type=""bibr"" target=""#b8"">[9]</ref>, <ref type=""bibr"" target=""#b17"">[18]</ref>, memory requests <ref type=""bibr"" target=""#b10"">[11]</ref> type=""bibr"" target=""#b28"">[29]</ref>- <ref type=""bibr"" target=""#b30"">[31]</ref>, issuing prefetches <ref type=""bibr"" target=""#b17"">[18]</ref>, etc. Until now, these optimizations have been primarily p in prioritizing two important resources -one for memory which issues prefetches for critical loads <ref type=""bibr"" target=""#b17"">[18]</ref> and another for ALU resources in instruction scheduling <r "">[33]</ref>. These proposals identify high-fanout loads to mark them as critical to issue prefetch <ref type=""bibr"" target=""#b17"">[18]</ref> and prioritize the critical instructions for ALU resource has also been shown to outperform the latency based ways of identifying and exploiting criticality <ref type=""bibr"" target=""#b17"">[18]</ref>, <ref type=""bibr"" target=""#b33"">[34]</ref>. We next evalua oritizing) and SPEC.float (34% from prefetching, 25% from prioritizing), re-affirming prior results <ref type=""bibr"" target=""#b17"">[18]</ref>, <ref type=""bibr"" target=""#b32"">[33]</ref>. Interestingly, 8"">[9]</ref>, <ref type=""bibr"" target=""#b10"">[11]</ref>, <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" target=""#b17"">[18]</ref> have targeted one critical instruction at a time, rather t e, 64KB d-cache, 2 cycle hit latency; 8-way 2MB L2 with System CLPT prefetcher (1024?7bits entries) <ref type=""bibr"" target=""#b17"">[18]</ref>   Data Processing call), and the 3-bit argument with it to 8"">[9]</ref>, <ref type=""bibr"" target=""#b10"">[11]</ref>, <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" target=""#b17"">[18]</ref>, <ref type=""bibr"" target=""#b78"">[79]</ref> or even backend airly extensive hardware to identify these chains, and optimizing for them, e.g. techniques such as <ref type=""bibr"" target=""#b17"">[18]</ref>, <ref type=""bibr"" target=""#b75"">[76]</ref>, <ref type=""bib",1
"f> and another for ALU resources in instruction scheduling <ref type=""bibr"" target=""#b3"">[4]</ref>, <ref type=""bibr"" target=""#b31"">[32]</ref>, <ref type=""bibr"" target=""#b32"">[33]</ref>. These proposal ibr"" target=""#b17"">[18]</ref> and prioritize the critical instructions for ALU resource allocations <ref type=""bibr"" target=""#b31"">[32]</ref>, <ref type=""bibr"" target=""#b32"">[33]</ref>. These techniqu rces proposed in <ref type=""bibr"" target=""#b32"">[33]</ref>, using the tracking hardware proposed in <ref type=""bibr"" target=""#b31"">[32]</ref>, which requires 1.5KB SRAM for maintaining the tokens. ? A",1
"target=""#b68"">[69]</ref>, and/or instruction prefetchers <ref type=""bibr"" target=""#b69"">[70]</ref>- <ref type=""bibr"" target=""#b73"">[74]</ref>. While adding sophisticated hardware for high end CPUs may type=""bibr"" target=""#b98"">[99]</ref>) prefetching (e.g. <ref type=""bibr"" target=""#b69"">[70]</ref>- <ref type=""bibr"" target=""#b73"">[74]</ref>), branch prediction (e.g <ref type=""bibr"" target=""#b65"">[6",0
"xecutable as an atomic entity (e.g., a macro instruction <ref type=""bibr"" target=""#b37"">[38]</ref>- <ref type=""bibr"" target=""#b43"">[44]</ref> consisting of several micro-instructions in the sequence)",0
"ide can be employed in all current ARM CPU based devices <ref type=""bibr"" target=""#b18"">[19]</ref>- <ref type=""bibr"" target=""#b20"">[21]</ref>. ? We have implemented a profiler on top of the AOSP emula intelligently employing the ARM 16-bit thumb compression <ref type=""bibr"" target=""#b18"">[19]</ref>- <ref type=""bibr"" target=""#b20"">[21]</ref>, <ref type=""bibr"" target=""#b50"">[51]</ref> mechanism. Fron",0
"piler (ART compiler) then generates optimized ARM binary using various compiler optimization passes <ref type=""bibr"" target=""#b49"">[50]</ref>. After these passes, we have implemented an additional ins",0
"evices serving more than 35% of the world population today <ref type=""bibr"" target=""#b0"">[1]</ref>- <ref type=""bibr"" target=""#b2"">[3]</ref>. To a large extent, the hardware and software evolution of t",0
"iently, common cases of user inputs are readily provided for many of these apps in standard formats <ref type=""bibr"" target=""#b46"">[47]</ref>, <ref type=""bibr"" target=""#b47"">[48]</ref>, that we avail",0
"feedback as a composition of user result examination and relevance judgment. Examination hypothesis <ref type=""bibr"" target=""#b7"">[8]</ref>, which is a fundamental assumption in click modeling, postul ns; and among them result examination plays a central role <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b7"">8]</ref>. Unfortunately, most applications of bandit algorithms simply the selected arm a t . Based on the examination hypothesis <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b7"">8]</ref>, when C at = 1, the chosen a t must be relevant to the user's",1
"ch as clicks, which is known to be biased and incomplete about users' evaluation of system's output <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b10"">11]</ref>. For example, a us y for log x (See Appendix B.2), a lower bound of the required form is achievable. When C = 1, by Eq <ref type=""bibr"" target=""#b15"">(16)</ref> in Appendix B.3, we have,</p><formula xml:id=""formula_7"">l",0
"his directly limits their application scenario in practice. The solution developed in Lagrée et al. <ref type=""bibr"" target=""#b17"">[18]</ref> is the closest to ours, which exploits bias in reward dist led explanations of our examination feature choice. We also added a new baseline here, i.e., PBMUCB <ref type=""bibr"" target=""#b17"">[18]</ref>, which assumes a position-based examination probability in",0
"not account for the additional discrepancy introduced by the variational inference. Abeille et al. <ref type=""bibr"" target=""#b1"">[2]</ref> suggests that an exact posterior is not a necessary conditio",0
"the values such that the likelihood on the observations is maximized. Similar to the choice made by <ref type=""bibr"" target=""#b11"">[12]</ref>, we choose the closed form update formulas of those variat",0
"aints. As a result, researchers have explored software-based techniques to tolerate hardware faults <ref type=""bibr"" target=""#b2"">[3]</ref>. Softwarebased techniques do not require any modification in",1
"s. Although the time cost was reduced, it brought a large number of false positives.</p><p>The work <ref type=""bibr"" target=""#b15"">[16]</ref> proposes a configurable protection technique for SDC-causi s, machine learning based methods are introduced to identify the SDC-causing instructions. The work <ref type=""bibr"" target=""#b15"">[16]</ref> proposes a machine learning algorithm based model, namely pproach in detail. We first define some terms used in this paper, some of which are drawn from work <ref type=""bibr"" target=""#b15"">[16]</ref>.</p><p>Dynamic Dependency Graph: A Dynamic Dependency Grap ased on prior work <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b15"">[16]</ref><ref type=""bibr"" target=""#b16"">[17]</ref><ref type=""bibr"" t are based on duplicating the backward slices of the instructions to protect, similar to prior work <ref type=""bibr"" target=""#b15"">[16]</ref>.</p><p>We insert a check immediately after the instruction tion efficiency and SDC impact are imperative parameters for evaluating our approach. In literature <ref type=""bibr"" target=""#b15"">[16]</ref>, the SDC detection efficiency (DE) is defined as the ratio",1
"ef> ran fault injections for the selected dynamic instruction sequences called ""pilots"". SymPLIFIED <ref type=""bibr"" target=""#b11"">[12]</ref> identified SDC-causing instructions by symbolic execution, study by either predicting their outcomes or showing them equivalent to other faults. SmartInjector <ref type=""bibr"" target=""#b11"">[12]</ref> firstly lists all possible faults in an application, and t of instruction. Features are extracted according to the above analysis and also based on prior work <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" ta",0
"t=""#b12"">13,</ref><ref type=""bibr"" target=""#b15"">[16]</ref><ref type=""bibr"" target=""#b16"">[17]</ref><ref type=""bibr"" target=""#b17"">[18]</ref>. In total, 62 features are extracted. We categorize these",0
"ip and the reduction of chip sizes, the transient fault rate of software will grow with Moore's Law <ref type=""bibr"" target=""#b1"">[2]</ref>. Therefore, it is necessary to protect these devices against",0
"#b19"">[20]</ref>, Stanford benchmarks <ref type=""bibr"" target=""#b20"">[21]</ref>, Parboil benchmarks <ref type=""bibr"" target=""#b21"">[22]</ref> and PARSEC benchmarks <ref type=""bibr"" target=""#b22"">[23]<",0
"structions have no effect on the program results. In this paper, we use the staticslicing technique <ref type=""bibr"" target=""#b23"">[24]</ref> to transform a program to an identical but smaller and sim",0
"ese approaches have already been used in mission critical systems for satellites and space missions <ref type=""bibr"" target=""#b3"">[4]</ref>.</p><p>Although software-based approaches such as full dupli",0
"which are drawn from SPEC benchmarks <ref type=""bibr"" target=""#b19"">[20]</ref>, Stanford benchmarks <ref type=""bibr"" target=""#b20"">[21]</ref>, Parboil benchmarks <ref type=""bibr"" target=""#b21"">[22]</r",0
"e shown that SDCs are caused by errors in a relatively small proportion of programs' data variables <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b8"">9]</ref>, and by selectively pr",0
"SDCs in real executions. However, it was even more time-consuming than fault injection. Shoestring <ref type=""bibr"" target=""#b12"">[13]</ref> assumed that instructions, which impact global memory or p pplications.</p><p>Another SDC identifying method is statistical vulnerability analysis. Shoestring <ref type=""bibr"" target=""#b12"">[13]</ref> uses a static analysis approach to identify the instructio cording to the above analysis and also based on prior work <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b15"">[16]</ref><ref type=""bibr"" ta",0
"gram level fault injection tool, which has been shown to be accurate for measuring SDCs in programs <ref type=""bibr"" target=""#b18"">[19]</ref>. LLFI works at the LLVM compiler's intermediate code level",0
"SDC impact subject to a given performance overhead, using a standard dynamic programming algorithm <ref type=""bibr"" target=""#b26"">[27]</ref>. Once we identify a set of instructions to protect, the ne",0
"applications.</p><p>Various efforts have been made to refine the injection framework. CriticalFault <ref type=""bibr"" target=""#b9"">[10]</ref> applied vulnerability analysis to avoid the injections that l fault injection (SFI) to model the soft error rate (SER) of targeted systems.</p><p>CriticalFault <ref type=""bibr"" target=""#b9"">[10]</ref> proposes a biased injection framework that employs vulnerab",0
"methods train image embeddings through the local relationships between images in the form of pairs <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref> or triplets <ref type=",1
"retrieval datasets <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b24"">25]</ref>.</p><p>Training Sampling Sampling informative training samp train/test split on four datasets: CARS-196 <ref type=""bibr"" target=""#b11"">[12]</ref>, CUB-200-2011 <ref type=""bibr"" target=""#b24"">[25]</ref>, Stanford Online Products (SOP) <ref type=""bibr"" target=""# and binary embeddings. Ablation studies on different design choices of our approach on CUB-200-2011 <ref type=""bibr"" target=""#b24"">[25]</ref> (Section 4.4) are provided as empirical justifications. Ne",0
"ng approaches propose to use classification-based training <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" tar proaches, the classification-based approach can be viewed as approximating each class using a proxy <ref type=""bibr"" target=""#b13"">[14]</ref>, and uses all proxies to provide global context for each t of-the-art performance. The theoretical link between classification and metric learning is shown in <ref type=""bibr"" target=""#b13"">[14]</ref>, and image retrieval tasks have some success adopting clas 3,</ref><ref type=""bibr"" target=""#b27"">28]</ref>. We follow the same derivation and notations as in <ref type=""bibr"" target=""#b13"">[14]</ref>. For embedding x of input image with class label y, the lo n loss for metric learning, the loss is bounded by the worst approximated examples within the class <ref type=""bibr"" target=""#b13"">[14]</ref>. A simple way to alleviate this is by including multiple e target=""#b13"">[14]</ref>, and image retrieval tasks have some success adopting classification loss <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b28"">29]</ref>. Though classifica",0
"earning image embeddings, is a core problem of a variety of applications including face recognition <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" ta ets, another school of deep metric learning approaches propose to use classification-based training <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" ta sses for Metric Learning Classification losses are widely adopted in face verification applications <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" ta ng is used for exaggerating the difference among classes and boosting the gradients as also used in <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" ta",0
"g including GoogleNet <ref type=""bibr"" target=""#b19"">[20]</ref>, GoogleNet with Batch Normalization <ref type=""bibr"" target=""#b7"">[8]</ref>, and ResNet50 <ref type=""bibr"" target=""#b3"">[4]</ref>. We in",0
"f type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b10"">11]</ref> and speech generation <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b12"">13]</ref> tasks. VAE has man ws the good performance of this method.</p><p>We have become aware of recent work by Akuzawa et al. <ref type=""bibr"" target=""#b11"">[12]</ref> which combines an autoregressive speech synthesis model wi",1
"ctly from characters have made rapid progress in recent years, and achieved very high voice quality <ref type=""bibr"" target=""#b0"">[1]</ref><ref type=""bibr"" target=""#b1"">[2]</ref><ref type=""bibr"" targe s the latent variable does in VAE. Therefore, in this paper we intend to introduce VAE to Tacotron2 <ref type=""bibr"" target=""#b0"">[1]</ref>, a state-of-the-art end-to-end speech synthesis model, to le ate before add operation. The attention module and decoder have the same architecture as Tacotron 2 <ref type=""bibr"" target=""#b0"">[1]</ref>. Then, WaveNet <ref type=""bibr"" target=""#b18"">[19]</ref> voc usually neutral speaking style, is approaching the extreme quality close to human expert recording <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b2"">3]</ref>, the interests in expr",1
"t encoder state is simply added by z and then is consumed by a location-sensitive attention network <ref type=""bibr"" target=""#b17"">[18]</ref> which converts encoded sequence to a fixed-length context",0
"ogress in recent years, and achieved very high voice quality <ref type=""bibr"" target=""#b0"">[1]</ref><ref type=""bibr"" target=""#b1"">[2]</ref><ref type=""bibr"" target=""#b2"">[3]</ref>. While the single sty er is utilized to reconstruct waveform.</p><p>The total loss of proposed model is shown in equation <ref type=""bibr"" target=""#b1"">(2)</ref>.</p><formula xml:id=""formula_1"">Loss = KL[q φ (z|x)||p θ (z)",0
"n <ref type=""bibr"" target=""#b8"">[9]</ref>, image generation <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b10"">11]</ref> and speech generation <ref type=""bibr"" target=""#b11"">[12,</",0
"=""bibr"" target=""#b14"">[15]</ref> LSTM <ref type=""bibr"" target=""#b15"">[16]</ref> layer using zoneout <ref type=""bibr"" target=""#b16"">[17]</ref> with probability 0.1. The output text encoder state is sim",0
"Autoencoder (VAE) <ref type=""bibr"" target=""#b6"">[7]</ref> and Generative Adversarial Network (GAN) <ref type=""bibr"" target=""#b7"">[8]</ref>, are powerful architectures which can learn complicated dist",0
"5 width and 512 channels followed by a bidirectional <ref type=""bibr"" target=""#b14"">[15]</ref> LSTM <ref type=""bibr"" target=""#b15"">[16]</ref> layer using zoneout <ref type=""bibr"" target=""#b16"">[17]</r",0
"type=""bibr"" target=""#b10"">11]</ref> and speech generation <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b12"">13]</ref> tasks. VAE has many merits, such as learning disentangled f",0
"oduce the expected reconstruction if the output of decoder is averaged over many samples of x and z <ref type=""bibr"" target=""#b13"">[14]</ref>. In the rest of the paper, −E q φ (z|x) [log p θ (x|z)] is",0
"arget=""#b36"">37,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" tar",1
"arget=""#b84"">86,</ref><ref type=""bibr"" target=""#b83"">85,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b76"">78,</ref><ref type=""bibr"" target=""#b51"">52,</ref><ref type=""bibr"" tar",0
"get=""#b62"">63,</ref><ref type=""bibr"" target=""#b60"">61,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b53"">54,</ref><ref type=""bibr"" target=""#b59"">60,</ref><ref type=""bibr"" tar e the communication cost. Furthermore, domain specific programming language systems, such as Galois <ref type=""bibr"" target=""#b53"">[54]</ref>, Green-Marl <ref type=""bibr"" target=""#b22"">[23]</ref> and",0
"arget=""#b6"">7,</ref><ref type=""bibr"" target=""#b78"">80,</ref><ref type=""bibr"" target=""#b82"">84,</ref><ref type=""bibr"" target=""#b64"">65,</ref><ref type=""bibr"" target=""#b86"">88,</ref><ref type=""bibr"" tar",0
"At the end of PageRank, we switch to the push model because the majority of the vertices are stable <ref type=""bibr"" target=""#b85"">[87]</ref>. The switch is decided by a decision tree.</p><p>Graph Ben t advance in graph computing falls in algorithm innovation <ref type=""bibr"" target=""#b50"">[51,</ref><ref type=""bibr"" target=""#b85"">87,</ref><ref type=""bibr"" target=""#b14"">15]</ref>, framework developm",0
"get=""#b25"">26,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b34"">35,</ref><ref type=""bibr"" target=""#b39"">40,</ref><ref type=""bibr"" target=""#b46"">47,</ref><ref type=""bibr"" tar e a novel face-focused cross-stream network (FFCSN). Different from the popular two-stream networks <ref type=""bibr"" target=""#b39"">[40,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" targ d for action recognition in videos and has been popular for many human-centric video analysis tasks <ref type=""bibr"" target=""#b39"">[40,</ref><ref type=""bibr"" target=""#b5"">6]</ref>. Various improvement",1
"igure"" target=""#fig_0"">1</ref>). For the training data scarcity problem, we introduce meta learning <ref type=""bibr"" target=""#b38"">[39,</ref><ref type=""bibr"" target=""#b51"">52,</ref><ref type=""bibr"" ta used as a data augmentation strategy to deal with the lack of training data. However, meta-learning <ref type=""bibr"" target=""#b38"">[39,</ref><ref type=""bibr"" target=""#b51"">52,</ref><ref type=""bibr"" ta number of model parameters. To deal with the data scarcity problem, we propose to use meta learning <ref type=""bibr"" target=""#b38"">[39,</ref><ref type=""bibr"" target=""#b51"">52,</ref><ref type=""bibr"" ta",1
"ler than, for example, those YouTube-collected action recognition benchmark datasets such as UCF101 <ref type=""bibr"" target=""#b40"">[41]</ref>. Our model differs significantly from existing deep ADD mo anch of our cross-stream base network as in <ref type=""bibr"" target=""#b46"">[47]</ref> on the UCF101 <ref type=""bibr"" target=""#b40"">[41]</ref> dataset. Moreover, for G and D of the adversarial learning",0
"rget=""#b43"">[44,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b44"">45,</ref><ref type=""bibr"" target=""#b7"">8]</ref>, an ADD model is non-",0
"=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b42"">43]</ref>, and loss of triplet network <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b14"">15]</ref>. The conventional no hat express the emotion content. Our FFCSN model is clearly effective in overcoming this challenge. <ref type=""bibr"" target=""#b1"">(2)</ref> The improvements obtained by our FFCSN model are really impr",0
"s such as temporal segment network (TSN) <ref type=""bibr"" target=""#b46"">[47]</ref> and its variants <ref type=""bibr"" target=""#b59"">[60,</ref><ref type=""bibr"" target=""#b61"">62]</ref> have been designed",0
"training data scarcity problem, we introduce meta learning <ref type=""bibr"" target=""#b38"">[39,</ref><ref type=""bibr"" target=""#b51"">52,</ref><ref type=""bibr"" target=""#b24"">25]</ref> and adversarial lea eal with the lack of training data. However, meta-learning <ref type=""bibr"" target=""#b38"">[39,</ref><ref type=""bibr"" target=""#b51"">52,</ref><ref type=""bibr"" target=""#b24"">25]</ref> was originally prop the data scarcity problem, we propose to use meta learning <ref type=""bibr"" target=""#b38"">[39,</ref><ref type=""bibr"" target=""#b51"">52,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" tar",0
"training samples. Similarly, <ref type=""bibr"" target=""#b34"">[35]</ref> proposed to add faster R-CNN <ref type=""bibr"" target=""#b37"">[38]</ref> so that attention can be focused on objects detected in a al stream (i.e. face expression branch) is a face detection model based on the popular faster R-CNN <ref type=""bibr"" target=""#b37"">[38]</ref>. This branch follows the deep learning framework of faster",0
"like the conventional physiological and biological methods <ref type=""bibr"" target=""#b43"">[44,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" targe",0
"rbal, and acoustic <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" tar eceptive behaviors <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" tar",0
"to guide the pixel denoiser; in contrast, our denoising is applied directly on features. Guo et al. <ref type=""bibr"" target=""#b7"">[8]</ref> transform the images via non-differentiable image preprocess ts of their non-differentiable computations <ref type=""bibr"" target=""#b0"">[1]</ref>. In contrast to <ref type=""bibr"" target=""#b7"">[8]</ref>, our feature denoising models are differentiable, but are st y increases as the image is propagated through the network <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b7"">8]</ref>, and non-existing activations in the feature maps are halluci",1
"s adversarially perturbed counterpart (bottom). The adversarial perturbation was produced using PGD <ref type=""bibr"" target=""#b15"">[16]</ref> with maximum perturbation ǫ =16 (out of 256). In this exam e-box attacks on ImageNet <ref type=""bibr"" target=""#b17"">[18]</ref>. Under 10-iteration PGD attacks <ref type=""bibr"" target=""#b15"">[16]</ref>, we report 55.7% classification accuracy on ImageNet, larg tacker based on the current parameters of the models. We use the Projected Gradient Descent (PGD) 2 <ref type=""bibr"" target=""#b15"">[16]</ref> as the white-box attacker for adversarial training.</p><p> ining, we can initialize the adversarial image by the clean image, or randomly within the allowed ǫ <ref type=""bibr"" target=""#b15"">[16]</ref>. We randomly choose from both initializations in the PGD a ot_2"">4</ref> We evaluate with ǫ =16, a challenging case for defenders on ImageNet.</p><p>Following <ref type=""bibr"" target=""#b15"">[16]</ref>, the PGD white-box attacker initializes the adversarial pe ts adversarially perturbed counterpart (bottom). The adversarial perturbation was produced using PGD<ref type=""bibr"" target=""#b15"">[16]</ref> with maximum perturbation ǫ =16 (out of 256). In this exam >Adversarial training <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b15"">16]</ref> defends against adversarial perturbations by training netwo riven by a successful implementation of adversarial training <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b15"">16]</ref>. In this section, we describe our implementation of adversa noising models.</p><p>The basic idea of adversarial training <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b15"">16]</ref> is to train the network on adversarially perturbed images.",0
"state-of-the-art in adversarial robustness against highly challenging white-box attacks on ImageNet <ref type=""bibr"" target=""#b17"">[18]</ref>. Under 10-iteration PGD attacks <ref type=""bibr"" target=""# ad n=""6."">Experiments</head><p>We evaluate feature denoising on the ImageNet classification dataset <ref type=""bibr"" target=""#b17"">[18]</ref> that has ∼1.28 million images in 1000 classes. Following c",0
"</p><p>Empirically, we find that the best performance is achieved by networks using non-local means <ref type=""bibr"" target=""#b1"">[2]</ref> for feature denoising, leading to models that are related to > and non-local blocks <ref type=""bibr"" target=""#b23"">[24]</ref>. However, only the non-local means <ref type=""bibr"" target=""#b1"">[2]</ref> operation in the denoising block is actually doing the denoi tiations of the denoising operation in our denoising blocks.</p><p>Non-local means. Non-local means <ref type=""bibr"" target=""#b1"">[2]</ref> compute a denoised feature map y of an input feature map x b",0
"lso evaluated other attackers, including FGSM<ref type=""bibr"" target=""#b5"">[6]</ref>, iterative FGSM<ref type=""bibr"" target=""#b11"">[12]</ref>, and its momentum variant<ref type=""bibr"" target=""#b2"">[3]",0
"m for the chest database creation or expansion, performing NER training and modeling using NeuroNER <ref type=""bibr"" target=""#b9"">[12]</ref> and then generating the current or the new model, and hence",1
"odes by matching the structured output generated consisting of findings and modifiers automatically <ref type=""bibr"" target=""#b1"">[2]</ref>  <ref type=""bibr"" target=""#b2"">[3]</ref>. In particularly, b",0
"no reporting style changes in creating their input radiology free-text document, Hassanpour et al. <ref type=""bibr"" target=""#b6"">[7]</ref> describe a machine learning system to annotate radiology rep",0
"structured medical records data and the inefficiency.</p><p>There are various aspects and researchs <ref type=""bibr"" target=""#b3"">[4]</ref> trying to solve the problem mentioned by deploying the skill",0
"nerated consisting of findings and modifiers automatically <ref type=""bibr"" target=""#b1"">[2]</ref>  <ref type=""bibr"" target=""#b2"">[3]</ref>. In particularly, based on the structured electronic medical",0
"and speed up the training process of BLSTM. This includes context-sensitive-chunk BLSTM (CSC-BLSTM) <ref type=""bibr"" target=""#b24"">[25]</ref> and latency-controlled BLSTM (LC-BLSTM) <ref type=""bibr"" t",1
"and training data, error rates have been reduced significantly across many speech recognition tasks <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b3"">4]</ref>. A wide variety of NN",0
"/www.tei-c.org/ns/1.0""><head n=""3.1"">Bidirectional LSTM</head><p>As discussed previously, the BLSTM <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b22"">23]</ref> is able to make us",0
"ion less valuable <ref type=""bibr"" target=""#b27"">[28]</ref><ref type=""bibr"" target=""#b28"">[29]</ref><ref type=""bibr"" target=""#b29"">[30]</ref>. Further, because an output gate is not used in GRUs, the",0
"s based on ReRAMs have been proposed (e.g., RPBFS <ref type=""bibr"" target=""#b17"">[18]</ref>, GraphR <ref type=""bibr"" target=""#b0"">[1]</ref>, and HyVE <ref type=""bibr"" target=""#b18"">[19]</ref>), achiev on, it still suffers from two main problems. (1) Heavy writing overheads. Previous work like GraphR <ref type=""bibr"" target=""#b0"">[1]</ref> converted the edge list into the adjacency matrix by sequent red for unweighted graph algorithms, leading to 3.77x lower EDP.</p><p>We take the design of GraphR <ref type=""bibr"" target=""#b0"">[1]</ref> as the baseline and compare it with GraphSAR in Figure <ref , v j , e i, j ) 5:</formula><p>end for 6: end while 7: return V To ensure the scalability, GraphR <ref type=""bibr"" target=""#b0"">[1]</ref> and HyVE <ref type=""bibr"" target=""#b18"">[19]</ref> divide a s subgraphs in the memory where these subgraphs are stored. Thus, GraphSAR is different from GraphR <ref type=""bibr"" target=""#b0"">[1]</ref> and HyVE <ref type=""bibr"" target=""#b18"">[19]</ref>, by both d on the hybrid-centric model. The order of blocks and edges in lists is the same as that in GraphR <ref type=""bibr"" target=""#b0"">[1]</ref>, where data are stored in the columnoriented order, leading ter of ReRAM cell model is from the same source <ref type=""bibr"" target=""#b19"">[20]</ref> in GraphR <ref type=""bibr"" target=""#b0"">[1]</ref> (read/write energy consumption: 1.08pJ/7.4pJ, <ref type=""foo ially when writing data to ReRAMs and only performing computation once over these data (e.g, GraphR <ref type=""bibr"" target=""#b0"">[1]</ref>). Compared with GraphR, GraphSAR actually exploits the proce (GAS) model is used to describe different graph algorithms. In GAS, processing one vertex includes: <ref type=""bibr"" target=""#b0"">(1)</ref> Gathering values from incoming neighbors; (2) Applying gathe on-</head><p>vert analog data to digital data. We share ADC among bitlines, based on Previous works <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b24"">25]</ref>.</p></div> <div xmln",1
"l data. We share ADC among bitlines, based on Previous works <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b24"">25]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>? memory design can also provide memory-capacityproportional <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b24"">25]</ref> computation units, rather than fixed number of processing u",0
"-inmemory designs <ref type=""bibr"" target=""#b11"">[12]</ref><ref type=""bibr"" target=""#b12"">[13]</ref><ref type=""bibr"" target=""#b13"">[14]</ref>.</p><p>The emerging metal-oxide resistive random access me",0
"x. Many graph processing architectures/accelerators based on ReRAMs have been proposed (e.g., RPBFS <ref type=""bibr"" target=""#b17"">[18]</ref>, GraphR <ref type=""bibr"" target=""#b0"">[1]</ref>, and HyVE M-based graph processing systems all store edges in ReRAMs to perform the edge-centric model. RPBFS <ref type=""bibr"" target=""#b17"">[18]</ref> uses a shared memory to store values of all vertices. Such",0
"om-Friendster (FS) <ref type=""bibr"" target=""#b25"">[26]</ref> 65.6 m 1.81 b community yahoo-web (YH) <ref type=""bibr"" target=""#b27"">[28]</ref> 1.41 b 6.64 b web graph 5.1.2 Configurations.To evaluate t",0
"ed to extend the typical one for different application scenarios, such as by adding personalization <ref type=""bibr"" target=""#b24"">[25]</ref>, content <ref type=""bibr"" target=""#b8"">[9]</ref> and conte tial recommendation literatures, such as Caser, GRURec and <ref type=""bibr"" target=""#b19"">[20,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" tar such as shown in Eq. ( <ref type=""formula"">3</ref>) ( see <ref type=""bibr"" target=""#b19"">[20,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b28"">29,</ref><ref type=""bibr"" tar d other RNN variants <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b26"">27]</ref>; <ref type=""bibr"" t",1
"mendations. While effective, these RNN-based models, such as <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b14"">15]</ref>, depend on a hidden state of the entire past that cannot fu ions. For example, a Gated Recurrent Unit (GRURec) architecture with a ranking loss was proposed by <ref type=""bibr"" target=""#b14"">[15]</ref> for session-based recommendation. In the follow-up papers, reported the evaluated results by three popular top-N metrics, namely MRR@N (Mean Reciprocal Rank) <ref type=""bibr"" target=""#b14"">[15]</ref>, HR@N (Hit Ratio) <ref type=""bibr"" target=""#b12"">[13]</ref veral data sets, and compare our model (called NextItNet) with the wellknown RNN-based model GRURec <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b27"">28]</ref> and the state-of-t",1
"uccessful use of CNNs in image tasks, a newly proposed sequential recommender, referred to as Caser <ref type=""bibr"" target=""#b28"">[29]</ref>, abandoned RNN structures, proposing instead a convolution la"">3</ref>) ( see <ref type=""bibr"" target=""#b19"">[20,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b28"">29,</ref><ref type=""bibr"" target=""#b29"">30]</ref>).</p><formula xml:i",1
"ce. Second, instead of using inefficient huge filters, we stack the 1D dilated convolutional layers <ref type=""bibr"" target=""#b30"">[31]</ref> on top of each other to increase the receptive fields when age generation tasks <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b30"">31]</ref>, and has been applied in other fields (e.g., acoustic <ref",0
"DE <ref type=""bibr"" target=""#b18"">[19]</ref>, PixelRNN/CNN <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b23"">24]</ref> in biological and image domains.</p><p>Owing to the ability",0
"apply either the sampled softmax <ref type=""bibr"" target=""#b16"">[17]</ref> or kernel based sampling <ref type=""bibr"" target=""#b1"">[2]</ref>. The recommendation accuracy by these negative sampling stra t=""#b19"">20,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b26"">27]</ref>; <ref type=""bibr"" target=""#b1"">(2)</ref> the GRURec baseline could be regarded as the state-of-the-ar",0
"was invented for dense prediction in image generation tasks <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b30"">31]</ref>, and has been appli ref>, and has been applied in other fields (e.g., acoustic <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b25"">26]</ref> and translation <ref type=""bibr"" target=""#b17"">[18]</ref> t",0
"ef type=""bibr"" target=""#b26"">[27]</ref>, attention mechanism <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b19"">20]</ref> and different ranking loss functions <ref type=""bibr"" targe t is worth noting that in previous sequential recommendation literatures, such as Caser, GRURec and <ref type=""bibr"" target=""#b19"">[20,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" ta plitting or shifting the input sequence), such as shown in Eq. ( <ref type=""formula"">3</ref>) ( see <ref type=""bibr"" target=""#b19"">[20,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" ta ndencies between x 0 and x 48 . To remedy this defect, when t &gt; 5, we follow the common approach <ref type=""bibr"" target=""#b19"">[20,</ref><ref type=""bibr"" target=""#b27"">28]</ref> by manually creati type=""bibr"" target=""#b29"">[30]</ref> and other RNN variants <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" tar",0
"2.3"">Related Work</head><p>Early work in sequential recommendations mostly rely on the markov chain <ref type=""bibr"" target=""#b4"">[5]</ref> and feature-based matrix factorization <ref type=""bibr"" targ",0
"s it is well suited to sequence processing and online learning in contrast with batch normalization <ref type=""bibr"" target=""#b15"">[16]</ref>.</p><p>Regarding the properties of the two residual networ",0
"/ref><ref type=""bibr"" target=""#b30"">31]</ref>, and has been applied in other fields (e.g., acoustic <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b25"">26]</ref> and translation <r the depth of the network. This makes it difficult to handle longrange sequences. Similar to Wavenet <ref type=""bibr"" target=""#b21"">[22]</ref>, we employ the dilated convolution to construct the propos",0
"get=""#b4"">[5]</ref> and feature-based matrix factorization <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b31"">[32]</ref><ref type=""bibr"" target=""#b32"">[33]</ref><ref type=""bibr"" t",0
"ities represent the most relevant recommendations. While effective, these RNN-based models, such as <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b14"">15]</ref>, depend on a hidden",0
""" target=""#b24"">[25]</ref>, content <ref type=""bibr"" target=""#b8"">[9]</ref> and contextual features <ref type=""bibr"" target=""#b26"">[27]</ref>, attention mechanism <ref type=""bibr"" target=""#b6"">[7,</re rget=""#b8"">[9,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b26"">27]</ref>; <ref type=""bibr"" target=""#b1"">(2)</ref> the GRURec baselin",0
"""#fig_1"">2</ref>). Specifically, each block is made up of the normalization, activation (e.g., ReLU <ref type=""bibr"" target=""#b20"">[21]</ref>), convolutional layers and a skip connection in a specific",0
"ly related to our approach are the Defense-GAN <ref type=""bibr"" target=""#b36"">[37]</ref> and MagNet <ref type=""bibr"" target=""#b24"">[25]</ref>, which first estimate the manifold of clean data to detect",1
"existing model-specific defense methods.</p><p>Closely related to our approach are the Defense-GAN <ref type=""bibr"" target=""#b36"">[37]</ref> and MagNet <ref type=""bibr"" target=""#b24"">[25]</ref>, whic",1
"n images with their corresponding adversarial images to improve robustness. Moosavi-Dezfooli et al. <ref type=""bibr"" target=""#b40"">[41]</ref>, however, showed that adversarial examples can also be gen",0
"s, which is approximated by deep networks <ref type=""bibr"" target=""#b43"">[44]</ref>. Gong et al. in <ref type=""bibr"" target=""#b44"">[45]</ref> showed that a simple binary classifier can successfully se",0
"ensive deployment in a wide range of computer vision tasks <ref type=""bibr"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b1"">[2]</ref>, including image classification <ref type=""bibr"" target=""#b2",0
"dient masking on the basis of characteristics defined in <ref type=""bibr"" target=""#b58"">[59]</ref>, <ref type=""bibr"" target=""#b59"">[60]</ref>.</p><p>Iterative attacks perform better than one-step atta",0
"ter smoothing <ref type=""bibr"" target=""#b16"">[17]</ref>, <ref type=""bibr"" target=""#b18"">[19]</ref>, <ref type=""bibr"" target=""#b19"">[20]</ref>, <ref type=""bibr"" target=""#b26"">[27]</ref>, <ref type=""bib /div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>C. Adversarial Defenses</head><p>Tremer et al. <ref type=""bibr"" target=""#b19"">[20]</ref> proposed ensemble adversarial training, which results in r y expensive model re-training and parameter optimization <ref type=""bibr"" target=""#b17"">[18]</ref>- <ref type=""bibr"" target=""#b19"">[20]</ref>, <ref type=""bibr"" target=""#b26"">[27]</ref>, our proposed d",0
"tural images and can be considered to lie on-the-manifold. Such images are referred to as in-domain <ref type=""bibr"" target=""#b45"">[46]</ref>. Corrupting these in-domain images by adding adversarial n",0
"stic defenses <ref type=""bibr"" target=""#b17"">[18]</ref>, <ref type=""bibr"" target=""#b25"">[26]</ref>, <ref type=""bibr"" target=""#b37"">[38]</ref>, <ref type=""bibr"" target=""#b38"">[39]</ref> (see Section IV he-art image transformation based defense schemes in the literature. These include JPEG Compression <ref type=""bibr"" target=""#b37"">[38]</ref>, Random Resizing and Padding <ref type=""bibr"" target=""#b17",0
"ecommendations based on collaborative filtering principles <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b22"">23]</ref>, they have not been",1
"Ns have been previously applied to make recommendations based on collaborative filtering principles <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" ta",1
"d pre-requisite graph meant to potentially help students towards higher rates of on-time graduation <ref type=""bibr"" target=""#b3"">[4]</ref>. Though high accuracy was achieved, their context was limite",0
"for future predictions.</p><p>We use a popular variant of RNNs called Long Short-Term Memory (LSTM) <ref type=""bibr"" target=""#b17"">[18]</ref>, which helps RNNs learn temporal dependencies with the add",0
"sed for assessment in the educational contexts of games, to predict outcomes based on game activity <ref type=""bibr"" target=""#b1"">[2]</ref>, and to predict responses to questions of various skills giv isite courses for other courses in the same department as the target course as a filter before step <ref type=""bibr"" target=""#b1"">(2)</ref>. For example, assuming that Table <ref type=""table"" target="" rigorousness of evaluation, we applied several filters to the enumerated input courses before step <ref type=""bibr"" target=""#b1"">(2)</ref>, which are listed as follows, with the first two filters bei",0
"s are trying to match course difficulty with their ability (i.e., ZPD) when making their selections <ref type=""bibr"" target=""#b21"">[22]</ref> and that, since the B models ). If the grade discriminatin",0
"dels of behavior prediction, to suggest the next resource a learner is likely to spend time on next <ref type=""bibr"" target=""#b23"">[24]</ref>. In the context of higher education, they have been used i",0
"elation between course outcomes and timely access to course materials among late enrolling students <ref type=""bibr"" target=""#b0"">[1]</ref> serves as a reminder of this.</p></div> <div xmlns=""http://w",0
"dents from enrollment sequences to be accurate, particularly after a student's second year of study <ref type=""bibr"" target=""#b20"">[21]</ref>.</p><p>The use of an RNN to predict student course grades",0
"fected by the system but GPA was, leading to an unexpected quarter of a grade point decrease in GPA <ref type=""bibr"" target=""#b7"">[8]</ref>. These findings underscore the daunting task of achieving me",0
"ods (GCN <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b21"">22]</ref>, GraphSAGE <ref type=""bibr"" target=""#b12"">[13]</ref>), etc.</p><p>Graph-level embedding. The most intuitive way",1
"eddings, either by a simple average or some weighted average <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b49"">50]</ref>, named the ""sum-based"" approaches <ref type=""bibr"" target=""",0
">Among many existing node embedding algorithms, we choose to use Graph Convolutional Networks (GCN) <ref type=""bibr"" target=""#b21"">[22]</ref>, as it is graph representation-invariant, as long as the i The time complexity associated with the generation of node-level and graph-level embeddings is O(E) <ref type=""bibr"" target=""#b21"">[22]</ref>, where E is the number of edges of the graph. Notice that =""#b42"">[43]</ref>), neighbor aggregation based methods (GCN <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b21"">22]</ref>, GraphSAGE <ref type=""bibr"" target=""#b12"">[13]</ref>), etc.",0
"r"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b49"">50]</ref>, named the ""sum-based"" approaches <ref type=""bibr"" target=""#b13"">[14]</ref>. A more sophisticated way to represent graphs can be achie",0
"fication framework <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b47"">48,</ref><ref type=""bibr"" target=""#b48"">49]</ref>, under which the total amount of exact graph similarity com et=""#b25"">[26,</ref><ref type=""bibr"" target=""#b43"">44,</ref><ref type=""bibr"" target=""#b47"">48,</ref><ref type=""bibr"" target=""#b48"">49,</ref><ref type=""bibr"" target=""#b50"">51]</ref>, graph classificati et=""#b25"">[26,</ref><ref type=""bibr"" target=""#b43"">44,</ref><ref type=""bibr"" target=""#b47"">48,</ref><ref type=""bibr"" target=""#b48"">49,</ref><ref type=""bibr"" target=""#b50"">51]</ref>. It contains 42,687 et=""#b25"">[26,</ref><ref type=""bibr"" target=""#b43"">44,</ref><ref type=""bibr"" target=""#b47"">48,</ref><ref type=""bibr"" target=""#b48"">49,</ref><ref type=""bibr"" target=""#b50"">51]</ref>. These studies focu",0
"ize the accuracy. Although it is not possible to obtain a universally good active learning strategy <ref type=""bibr"" target=""#b4"">(Dasgupta, 2004)</ref>, there exist many heuristics <ref type=""bibr"" t l side, it is shown that greedy active learning is not possible in algorithm and data agnostic case <ref type=""bibr"" target=""#b4"">(Dasgupta, 2005)</ref>. However, there are data dependent results show",1
"sserman, 2014)</ref> as the CNN architecture. We initialized all convolutional filters according to <ref type=""bibr"" target=""#b21"">He et al. (2016)</ref>. We optimized all models using RMSProp with a",0
". (2016)</ref>. We optimized all models using RMSProp with a learning rate of 1e-3 using Tensorflow <ref type=""bibr"" target=""#b0"">(Abadi et al., 2016)</ref>. We train CNNs from scratch after each iter",0
"g of cellular function under normal and disease conditions <ref type=""bibr"" target=""#b24"">[27]</ref><ref type=""bibr"" target=""#b25"">[28]</ref><ref type=""bibr"" target=""#b26"">[29]</ref><ref type=""bibr"" t",1
"tral role in the mechanistic understanding of cellular function under normal and disease conditions <ref type=""bibr"" target=""#b24"">[27]</ref><ref type=""bibr"" target=""#b25"">[28]</ref><ref type=""bibr"" t",0
"ndividuals have, the more likely that they know each other <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b13"">14]</ref>. Here, we offer direct empirical evidence across multiple d",0
"vation that the more common friends two individuals have, the more likely that they know each other <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b13"">14]</ref>. Here, we offer di",0
"o predict undetected, yet biologically relevant interactions <ref type=""bibr"" target=""#b6"">[7]</ref><ref type=""bibr"" target=""#b7"">[8]</ref><ref type=""bibr"" target=""#b8"">[9]</ref>. Such network-based l tory or 3D structure, used by some PPI prediction algorithms <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b29"">[32]</ref><ref type=""bibr"" targ arity values. (d) PPIs often require complementary interfaces<ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b7"">8]</ref>, hence two proteins, X and Y, with similar interfaces share m",0
"easure theoretical details, see <ref type=""bibr"" target=""#b0"">Daley and Vere-Jones (2003)</ref> and <ref type=""bibr"" target=""#b1"">Daley and Vere-Jones (2008)</ref>.</p></div> <div xmlns=""http://www.te",1
"ry is not assumed; for a much more thorough treatment with all the measure theoretical details, see <ref type=""bibr"" target=""#b0"">Daley and Vere-Jones (2003)</ref> and <ref type=""bibr"" target=""#b1"">Da fies the mean number of events in a region conditional on the past. Here we use the notation * from <ref type=""bibr"" target=""#b0"">Daley and Vere-Jones (2003)</ref> to remind ourselves that this densit",1
"e"" target=""#fig_1"">3</ref>. This process is a special case of the so-called self-correcting process <ref type=""bibr"" target=""#b6"">(Isham and Westcott, 1979)</ref>.</p><p>q q q qq q q q q 0 2 4 Note th",0
"ibution). For more on the Hawkes process, see e.g. <ref type=""bibr"" target=""#b3"">Hawkes (1971b</ref><ref type=""bibr"" target=""#b4"">Hawkes ( ,a, 1972))</ref>; <ref type=""bibr"" target=""#b5"">Hawkes and Oa",0
"e model checking on other aspects of the model (such as the summary statistics given for example in <ref type=""bibr"" target=""#b9"">Møller and Waagepetersen (2004)</ref>), which may not be caught so wel",0
"lue or a vector, depending on the choice of distribution). For more on the Hawkes process, see e.g. <ref type=""bibr"" target=""#b3"">Hawkes (1971b</ref><ref type=""bibr"" target=""#b4"">Hawkes ( ,a, 1972))</",0
"1.0""><head n=""4.2"">Ogata's modified thinning algorithm</head><p>Ogata's modified thinning algorithm <ref type=""bibr"" target=""#b10"">(Ogata, 1981)</ref> is a thinning algorithm based on simulating homog",0
"nt at University of Chinese Academy of Sciences (UCAS), Beijing, China. e-mail: baiye2016@ia.ac.cn. <ref type=""bibr"" target=""#b13"">[14]</ref>, <ref type=""bibr"">[15]</ref>, <ref type=""bibr"" target=""#b1",1
"<ref type=""bibr"" target=""#b16"">[18]</ref>. The source model is trained jointly on several languages <ref type=""bibr"" target=""#b17"">[19]</ref>, <ref type=""bibr"" target=""#b18"">[20]</ref>. In addition, l",1
"The source model is trained jointly on several languages <ref type=""bibr"" target=""#b17"">[19]</ref>, <ref type=""bibr"" target=""#b18"">[20]</ref>. In addition, language identification based multilingual t",1
"arial learning on domain adaptation <ref type=""bibr"" target=""#b31"">[33]</ref>, adversarial learning <ref type=""bibr"" target=""#b32"">[34]</ref> is used to ensure that the shared layers of the source mod",0
"ngle frame as the input, with no frame stacking. The BLSTM acoustic models are based on the work in <ref type=""bibr"" target=""#b42"">[44]</ref>, where each BLSTM layer consists of peephole connections a",0
"ef> use adversarial strategy to obtain bilingual lexicon without cross-lingual knowledge. Shinohara <ref type=""bibr"" target=""#b36"">[38]</ref> utilizes adversarial training to perform environment adapt",0
"=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b2"">[3]</ref>, <ref type=""bibr"" target=""#b3"">[4]</ref>. However it is still challenging to rapidly build an ASR sys",0
"raining is proposed to extract multilingual bottleneck features for low resource speech recognition <ref type=""bibr"" target=""#b19"">[21]</ref>, <ref type=""bibr"" target=""#b20"">[22]</ref>. The knowledge",0
"or parallel with target language specific hidden layers <ref type=""bibr"" target=""#b39"">[41]</ref>, <ref type=""bibr"" target=""#b45"">[47]</ref>. In this paper, the target model consists of share hidden reported WER as low as 45.7%. But many competitive teams <ref type=""bibr"" target=""#b44"">[46]</ref>, <ref type=""bibr"" target=""#b45"">[47]</ref> in NIST OpenKWS 2013 reported WER of 47.1% or higher. In a 7.1% or higher. In addition, lots of competitive systems <ref type=""bibr"" target=""#b44"">[46]</ref>, <ref type=""bibr"" target=""#b45"">[47]</ref> in NIST OpenKWS 2013 reported WER of 48.1% or higher on Tu",0
"acoustic models are able to share language invariant low-level components across various languages <ref type=""bibr"" target=""#b8"">[9]</ref>, <ref type=""bibr"" target=""#b9"">[10]</ref>, <ref type=""bibr""",0
"ed into two categories: transferring bottleneck features <ref type=""bibr"" target=""#b15"">[17]</ref>, <ref type=""bibr"" target=""#b21"">[23]</ref>, <ref type=""bibr"" target=""#b22"">[24]</ref>, <ref type=""bib Models are proposed to use hidden layers and softmax layers to learn language dependent information <ref type=""bibr"" target=""#b21"">[23]</ref>, <ref type=""bibr"" target=""#b30"">[32]</ref>. These models h lingual models <ref type=""bibr"" target=""#b9"">[10]</ref>, <ref type=""bibr"" target=""#b25"">[27]</ref>, <ref type=""bibr"" target=""#b21"">[23]</ref>, <ref type=""bibr"" target=""#b30"">[32]</ref>. The SHL-Model er kind of SHL-Models is proposed to use hidden layers to learn more language dependent information <ref type=""bibr"" target=""#b21"">[23]</ref>, <ref type=""bibr"" target=""#b30"">[32]</ref>. These models h",0
"d softmax layers to learn language dependent information <ref type=""bibr"" target=""#b21"">[23]</ref>, <ref type=""bibr"" target=""#b30"">[32]</ref>. These models have language specific hidden layers prior t "">[10]</ref>, <ref type=""bibr"" target=""#b25"">[27]</ref>, <ref type=""bibr"" target=""#b21"">[23]</ref>, <ref type=""bibr"" target=""#b30"">[32]</ref>. The SHL-Model is composed of shared hidden layers and lan dden layers to learn more language dependent information <ref type=""bibr"" target=""#b21"">[23]</ref>, <ref type=""bibr"" target=""#b30"">[32]</ref>. These models have language specific hidden layers prior t xperimental Setup</head><p>Our experiments are conducted using the Kaldi speech recognition toolkit <ref type=""bibr"" target=""#b30"">[32]</ref> and the open source deep learning framework called TensorF",0
"Pashto FLP condition using the Babel data reported WER as low as 45.7%. But many competitive teams <ref type=""bibr"" target=""#b44"">[46]</ref>, <ref type=""bibr"" target=""#b45"">[47]</ref> in NIST OpenKWS ref> in NIST OpenKWS 2013 reported WER of 47.1% or higher. In addition, lots of competitive systems <ref type=""bibr"" target=""#b44"">[46]</ref>, <ref type=""bibr"" target=""#b45"">[47]</ref> in NIST OpenKWS R of 48.1% or higher on Turkish FLP condition. Past work <ref type=""bibr"" target=""#b43"">[45]</ref>, <ref type=""bibr"" target=""#b44"">[46]</ref> on Vietnamese FLP condition using the Babel data reported",0
"del via transfer learning. The goal of transfer learning <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" target=""#b12"">[13]</ref> is to improve the performance of the target model via usin",0
"bottleneck features for low resource speech recognition <ref type=""bibr"" target=""#b19"">[21]</ref>, <ref type=""bibr"" target=""#b20"">[22]</ref>. The knowledge from the source model can be transferred to",0
"only with high numbers of client connections spreading requests out evenly over queues.</p><p>ZygOS <ref type=""bibr"" target=""#b44"">[46]</ref> improved on d-FCFS by implementing low-overhead task steal istributions.</p><p>We compare Shinjuku with IX <ref type=""bibr"" target=""#b14"">[16]</ref> and ZygOS <ref type=""bibr"" target=""#b44"">[46]</ref>, two state-of-the-art dataplane operating systems. Using s ""bibr"" target=""#b14"">[16]</ref>. Zy-gOS improves on IX by using work stealing to approximate c-FCFS <ref type=""bibr"" target=""#b44"">[46]</ref>. Linux applications built with libevent <ref type=""bibr"" t ficant departure from the common pattern in IX <ref type=""bibr"" target=""#b14"">[16]</ref> and Zy-gOS <ref type=""bibr"" target=""#b44"">[46]</ref>, which rely heavily on RSS to distribute incoming requests "">Evaluation</head><p>We compare Shinjuku to IX <ref type=""bibr"" target=""#b14"">[16]</ref> and ZygOS <ref type=""bibr"" target=""#b44"">[46]</ref>, two recent systems that use d-FCFS and approximate c-FCFS es its stealing overheads. A similar performance drop was also observed in the original ZygOS paper <ref type=""bibr"" target=""#b44"">[46]</ref>. Figure <ref type=""figure"">5c</ref> uses a Bimodal(99.5 -0 type=""bibr"" target=""#b37"">[39]</ref>, Chronos <ref type=""bibr"" target=""#b30"">[32]</ref>, and ZygOS <ref type=""bibr"" target=""#b44"">[46]</ref> fall in this category. Shinjuku improves on these systems duced in ELI for fast delivery of interrupts to VMs <ref type=""bibr"" target=""#b8"">[10]</ref>. ZygOS <ref type=""bibr"" target=""#b44"">[46]</ref> uses inter-processor interrupts for work stealing but does",1
"et=""#b42"">[44,</ref><ref type=""bibr"" target=""#b29"">31,</ref><ref type=""bibr"" target=""#b37"">39,</ref><ref type=""bibr"" target=""#b30"">32,</ref><ref type=""bibr"" target=""#b14"">16,</ref><ref type=""bibr"" tar get=""#b43"">45,</ref><ref type=""bibr"" target=""#b29"">31,</ref><ref type=""bibr"" target=""#b37"">39,</ref><ref type=""bibr"" target=""#b30"">32]</ref> sidestep the bloated kernel networking and thread managemen <ref type=""bibr"" target=""#b43"">[45]</ref>, MICA <ref type=""bibr"" target=""#b37"">[39]</ref>, Chronos <ref type=""bibr"" target=""#b30"">[32]</ref>, and ZygOS <ref type=""bibr"" target=""#b44"">[46]</ref> fall",0
"ousands of machines. End-to-end response times are then dominated by the slowest machine to respond <ref type=""bibr"" target=""#b21"">[23]</ref>. Reacting to user actions within tens of milliseconds requ nd scale <ref type=""bibr"" target=""#b12"">[14]</ref>. Low average or median latency is not sufficient <ref type=""bibr"" target=""#b21"">[23]</ref>. While tail latency can be improved through overprovisioni e=""bibr"" target=""#b36"">[38]</ref> or induced by activities such as garbage collection or compaction <ref type=""bibr"" target=""#b21"">[23,</ref><ref type=""bibr"" target=""#b3"">6,</ref><ref type=""bibr"" targ",0
"?3.5).</p><p>Similar to IX and ZygOS, Shinjuku leverages the Dune system for process virtualization <ref type=""bibr"" target=""#b13"">[15]</ref>. With Dune, Linux and the Dune kernel module run in VMX ro xplore the possibility of integrating the two systems.</p><p>Security model: The Dune kernel module <ref type=""bibr"" target=""#b13"">[15]</ref> uses hardware support for virtualization to isolate a Shin approximate c-FCFS respectively to improve tail latency. All three systems are built on top of Dune <ref type=""bibr"" target=""#b13"">[15]</ref>. We use the latest IX and ZygOS versions available at <ref essor interrupts for work stealing but does not implement preemptive scheduling. Shinjuku uses Dune <ref type=""bibr"" target=""#b13"">[15]</ref> to optimize processor-to-processor interrupts.</p><p>User-",0
"the dispatcher scalability would be a low-overhead message passing mechanism among different cores <ref type=""bibr"" target=""#b32"">[34,</ref><ref type=""bibr"" target=""#b49"">51]</ref>. Ideally, such a m",0
"ry to prior works deploying a hash table that supports only GET requests in a memcached environment <ref type=""bibr"" target=""#b53"">[55]</ref>. Furthermore, supporting such a hash table in the PHP envi erversside Javascript applications <ref type=""bibr"" target=""#b72"">[73]</ref> or memcached workloads <ref type=""bibr"" target=""#b53"">[55]</ref>). Note that we simulate an aggressive memory system with p e store the keys in the hash table itself, unlike the hash table designed for memcached deployments <ref type=""bibr"" target=""#b53"">[55]</ref>. Storing the keys directly in the hash table eases the tra table that supports only GET operation has been deployed in hardware before for memcached workloads <ref type=""bibr"" target=""#b53"">[55]</ref>. Furthermore, <ref type=""bibr"" target=""#b28"">[30]</ref> de in contrast to the most large-scale memcached deployments <ref type=""bibr"" target=""#b21"">[23,</ref><ref type=""bibr"" target=""#b53"">55]</ref> where the GET requests vastly outnumber the SET and other r get=""#b34"">36,</ref><ref type=""bibr"" target=""#b45"">47,</ref><ref type=""bibr"" target=""#b51"">53,</ref><ref type=""bibr"" target=""#b53"">55,</ref><ref type=""bibr"" target=""#b54"">56,</ref><ref type=""bibr"" tar",1
"eb documents, blog posts, news articles, and system logs) into appropriate HTML format. Prior works <ref type=""bibr"" target=""#b66"">[68]</ref> have realized the potential of hardware specialization for tasks include string finding, matching, replacing, trimming, comparing, etc. Previous work, such as <ref type=""bibr"" target=""#b66"">[68]</ref>, propose methods for string matching in hardware. However,",1
"s.</p><p>The desire to reduce datacenter load inspired the design of the HipHop JIT compiler (HHVM) <ref type=""bibr"" target=""#b17"">[19,</ref><ref type=""bibr"" target=""#b71"">72]</ref>, which translates type=""bibr"" target=""#b38"">[40]</ref>. Modern JIT compilers <ref type=""bibr"" target=""#b12"">[14,</ref><ref type=""bibr"" target=""#b17"">19]</ref> use inline caching (IC) to specialize code that accesses me to perform the regular hash map access in memory.</p><p>Design considerations The HHVM JIT compiler <ref type=""bibr"" target=""#b17"">[19]</ref> already uses an efficient hash computation function that c",0
"et=""#b32"">[34,</ref><ref type=""bibr"" target=""#b37"">39,</ref><ref type=""bibr"" target=""#b56"">58,</ref><ref type=""bibr"" target=""#b64"">66]</ref> are overly generic in nature for these PHP applications as egexp accelerators <ref type=""bibr"" target=""#b37"">[39,</ref><ref type=""bibr"" target=""#b56"">58,</ref><ref type=""bibr"" target=""#b64"">66]</ref> may deter their commercial deployment in the near future. I get=""#b55"">57,</ref><ref type=""bibr"" target=""#b56"">58,</ref><ref type=""bibr"" target=""#b59"">61,</ref><ref type=""bibr"" target=""#b64"">66]</ref>. In recent years, several works <ref type=""bibr"" target=""#b ars, several works <ref type=""bibr"" target=""#b37"">[39,</ref><ref type=""bibr"" target=""#b56"">58,</ref><ref type=""bibr"" target=""#b64"">66]</ref> attempt to parallelize processing by running the regexp sep",0
"al from prior work that introduces minimal changes to the cache subsystem to mitigate this overhead <ref type=""bibr"" target=""#b44"">[46]</ref>.</p><p>In addition to applying the above four optimization",0
"pular real-world PHP web applications -Word-Press <ref type=""bibr"" target=""#b13"">[15]</ref>, Drupal <ref type=""bibr"" target=""#b1"">[2]</ref>, and MediaWiki <ref type=""bibr"" target=""#b5"">[7]</ref> from",0
"egexp accelerators <ref type=""bibr"" target=""#b32"">[34,</ref><ref type=""bibr"" target=""#b37"">39,</ref><ref type=""bibr"" target=""#b56"">58,</ref><ref type=""bibr"" target=""#b64"">66]</ref> are overly generic hardware cost associated with parallel regexp accelerators <ref type=""bibr"" target=""#b37"">[39,</ref><ref type=""bibr"" target=""#b56"">58,</ref><ref type=""bibr"" target=""#b64"">66]</ref> may deter their com et=""#b24"">[26,</ref><ref type=""bibr"" target=""#b37"">39,</ref><ref type=""bibr"" target=""#b55"">57,</ref><ref type=""bibr"" target=""#b56"">58,</ref><ref type=""bibr"" target=""#b59"">61,</ref><ref type=""bibr"" tar r"" target=""#b64"">66]</ref>. In recent years, several works <ref type=""bibr"" target=""#b37"">[39,</ref><ref type=""bibr"" target=""#b56"">58,</ref><ref type=""bibr"" target=""#b64"">66]</ref> attempt to parallel",0
"accelerator requires a maximum of 3 cycles to process up to 64 character blocks. We use CACTI 6.5+ <ref type=""bibr"" target=""#b50"">[52]</ref> to estimate the access latency, energy and area of the rem o satisfy a request from a hardware free list. The content reuse table has 32 entries. We use McPAT <ref type=""bibr"" target=""#b50"">[52]</ref> to collect core power and energy.</p></div> <div xmlns=""ht",0
"propose instruction prefetching <ref type=""bibr"" target=""#b26"">[28]</ref>, preexecution techniques <ref type=""bibr"" target=""#b27"">[29]</ref> and modifying cache insertion policy <ref type=""bibr"" targ",0
"e=""bibr"" target=""#b69"">[70,</ref><ref type=""bibr"" target=""#b71"">72]</ref>, representing about 82.3% <ref type=""bibr"" target=""#b11"">[13]</ref> of all web applications. In particular, server-side PHP we ef type=""bibr"" target=""#b69"">[70,</ref><ref type=""bibr"" target=""#b71"">72]</ref>, representing 82.3% <ref type=""bibr"" target=""#b11"">[13]</ref> of all web applications. In this context, our work is the ascript is used predominantly whereas PHP is the language of choice for server-side web development <ref type=""bibr"" target=""#b11"">[13]</ref>. <ref type=""bibr"" target=""#b20"">[22,</ref><ref type=""bibr""",0
"ype=""bibr"" target=""#b9"">Daumé III, 2009;</ref><ref type=""bibr"" target=""#b5"">Coke et al., 2016;</ref><ref type=""bibr"" target=""#b22"">Littell et al., 2017)</ref>.</p><p>In this study, we examine whether h feature vectors from previous work based on the genetic and geographic distance between languages <ref type=""bibr"" target=""#b22"">(Littell et al., 2017)</ref>. Results show that the extracted represe up</head><p>Typology Database: To perform our analysis, we use the URIEL language typology database <ref type=""bibr"" target=""#b22"">(Littell et al., 2017)</ref>, which is a collection of binary feature not necessarily require pre-existing knowledge of the typological features in the language at hand, <ref type=""bibr"" target=""#b22"">Littell et al. (2017)</ref> have proposed a method for inferring typo",1
"ref>, POStagging <ref type=""bibr"" target=""#b38"">(Zhang et al., 2012)</ref>, and machine translation <ref type=""bibr"" target=""#b8"">(Daiber et al., 2016)</ref>.</p><p>However, the needs of NLP tasks dif",0
"classes/features. This study has been a scientific pursuit in its own right since the 19th century <ref type=""bibr"" target=""#b13"">(Greenberg, 1963;</ref><ref type=""bibr"" target=""#b7"">Comrie, 1989;</r",0
"nerative parsing in low-resource settings <ref type=""bibr"" target=""#b26"">(Naseem et al., 2012;</ref><ref type=""bibr"" target=""#b35"">Täckström et al., 2013)</ref>, phonological language modeling and loa",0
"or all languages, we divided the data into subwords using joint byte-pair encoding of all languages <ref type=""bibr"" target=""#b33"">(Sennrich et al., 2016)</ref> with 32K merge operations. We used LSTM",0
"of high inter-class similarities. Most recently, an additional center loss was introduced into CNNs <ref type=""bibr"" target=""#b43"">[44]</ref> to reduce the intra-class variations of the learned featur suffers from drastic data expansion when constructing image pairs from the training set. Wen et al. <ref type=""bibr"" target=""#b43"">[44]</ref> introduced a center loss for face recognition, which targe eview of Center Loss</head><p>As illustrated in Fig. <ref type=""figure"">1</ref>(b), the center loss <ref type=""bibr"" target=""#b43"">[44]</ref> explicitly reduces the intra-class variations by pushing s the CNN training.</p><p>1) Forward propagation: The center loss denoted as L C is defined in Eq. 1 <ref type=""bibr"" target=""#b43"">[44]</ref> as the summation of squared distances between samples and",1
"29"">[30]</ref>, <ref type=""bibr"" target=""#b7"">[8]</ref>, <ref type=""bibr"" target=""#b45"">[46]</ref>, <ref type=""bibr"" target=""#b2"">[3]</ref>.</p><p>Traditional CNNs are optimized using a softmax loss, 51"">[52]</ref>, <ref type=""bibr"" target=""#b7"">[8]</ref>, <ref type=""bibr"" target=""#b45"">[46]</ref>, <ref type=""bibr"" target=""#b2"">[3]</ref>. Among them, the deep CNNs have achieved promising recogniti W2016 challenge <ref type=""bibr"" target=""#b7"">[8]</ref>, <ref type=""bibr"" target=""#b45"">[46]</ref>, <ref type=""bibr"" target=""#b2"">[3]</ref>.</p><p>Most of the aforementioned CNN-based approaches adopt",0
"deep learning <ref type=""bibr"" target=""#b32"">[33]</ref>, <ref type=""bibr"" target=""#b39"">[40]</ref>, <ref type=""bibr"" target=""#b21"">[22]</ref>, <ref type=""bibr"" target=""#b6"">[7]</ref>, <ref type=""bibr""",0
"ated on three wellknown posed facial expression databases, i.e. Extended Cohn-Kanade database (CK+) <ref type=""bibr"" target=""#b13"">[14]</ref>, <ref type=""bibr"" target=""#b23"">[24]</ref>, Oulu-CASIA dat hmark expression databases including three posed facial expression databases, i.e. the CK+ database <ref type=""bibr"" target=""#b13"">[14]</ref>, <ref type=""bibr"" target=""#b23"">[24]</ref>, the MMI databa tp://www.tei-c.org/ns/1.0""><head>B. Experimental Datasets</head><p>1) CK+ Dataset: The CK+ database <ref type=""bibr"" target=""#b13"">[14]</ref>, <ref type=""bibr"" target=""#b23"">[24]</ref> consists of 327 ted from image sequences, the proposed IL-CNN is trained on static images, which is more favorable  <ref type=""bibr"" target=""#b13"">[14]</ref>, <ref type=""bibr"" target=""#b23"">[24]</ref> IN TERMS OF THE CONFUSION</head><label>I</label><figDesc>MATRIX OF THE PROPOSED IL-CNN EVALUATED ON THE CK+ DATASET<ref type=""bibr"" target=""#b13"">[14]</ref>,<ref type=""bibr"" target=""#b23"">[24]</ref>. THE GROUND TRUT",0
"based merely on implicit feedbacks, i.e., user clicks, in the current session.</p><p>Hidasi et al. <ref type=""bibr"" target=""#b11"">[12]</ref> apply recurrent neural networks (RNN) with Gated Recurrent al Networks (RNN) have been devised to model variable-length sequence data. Recently, Hidasi et al. <ref type=""bibr"" target=""#b11"">[12]</ref> apply RNN to sessionbased recommendation and achieve signi ARM. We use a RNN with Gated Recurrent Units (GRU) rather than a standard RNN because Hidasi et al. <ref type=""bibr"" target=""#b11"">[12]</ref> demonstrate that GRU can outperform the Long Short-Term Me re S i , To learn the parameters of the model, we do not utilize the proposed training procedure in <ref type=""bibr"" target=""#b11"">[12]</ref>, where the model is trained in a session-parallel, sequenc sessions of subsequent week for testing. Because we did not train NARM in a session-parallel manner <ref type=""bibr"" target=""#b11"">[12]</ref>, a </p><formula xml:id=""formula_15"">],V (x 2 ), ([x 1 , x nt representations when computing recommendation scores. • GRU-Rec: We denote the model proposed in <ref type=""bibr"" target=""#b11"">[12]</ref> as GRU-Rec, which utilizes session-parallel mini-batch tra",1
"r"" target=""#b41"">42]</ref> cannot perform well. To tackle this problem, sessionbased recommendation <ref type=""bibr"" target=""#b32"">[33]</ref> is proposed to predict the next item that the user is prob",0
"erarchical softmax layer <ref type=""bibr"" target=""#b23"">[24]</ref>, and negative sampling at random <ref type=""bibr"" target=""#b21"">[22]</ref>, they are not the best choices for our model.</p><p>We pro",0
"ibr"" target=""#b11"">[12]</ref> demonstrate that GRU can outperform the Long Short-Term Memory (LSTM) <ref type=""bibr"" target=""#b13"">[14]</ref> units for the session-based recommendation task. GRU is a",0
"t item embeddings. We adopt the dynamic setting in our model, more details will be described in §3. <ref type=""bibr"" target=""#b3"">4</ref>.</p><p>The basic idea of our work is to learn a recommendation egularization is also included to avoid coincidental high similarities between rarely visited items <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b19"">20]</ref>. • BPR-MF: BPR-MF <r",0
"rt from communication, their shared access to the wireless channel enables environmental perception <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b2"">[3]</ref>, <ref type=""bibr"" t xploited by various modalities, including RF-fluctuation, such as WiFi, FM-radio or software radios <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b3"">[4]</ref>, <ref type=""bibr"" t",1
"signal envelope <ref type=""bibr"" target=""#b6"">[7]</ref>, respiration rate exploiting Fresnel zones <ref type=""bibr"" target=""#b7"">[8]</ref> as well as emotion recognition from phase and time-domain si",0
"ing walking speed during experiments. Participants practiced using metronome before the experiments <ref type=""bibr"" target=""#b20"">[21]</ref>, <ref type=""bibr"" target=""#b21"">[22]</ref>. As further </p",0
"ref>. </p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>Approach</head><p>Accuracy FM-radio <ref type=""bibr"" target=""#b13"">[14]</ref> 0.64 RSSI (Active) <ref type=""bibr"" target=""#b17"">[18]</re ch as fitness tracking, attention monitoring, and health <ref type=""bibr"" target=""#b12"">[13]</ref>, <ref type=""bibr"" target=""#b13"">[14]</ref>. For instance, walking speed has been proposed as a reliab of walking speed from RF-fluctuation has been considered from variance in FM radio signal strength <ref type=""bibr"" target=""#b13"">[14]</ref>, custom SDR <ref type=""bibr"" target=""#b15"">[16]</ref> or R",0
"the lower spine of the body has been shown to achieve higher accuracy for walking speed estimation <ref type=""bibr"" target=""#b22"">[23]</ref>. Since we aimed to achieve realistic accuracy, we accepted",0
"learn node representations by utilizing information from distant neighbors. GCNs and their variants <ref type=""bibr"" target=""#b4"">(Hamilton et al., 2017a;</ref><ref type=""bibr"" target=""#b14"">Veličkovi sed node classification <ref type=""bibr"">(Kipf &amp; Welling, 2017)</ref>, inductive node embedding <ref type=""bibr"" target=""#b4"">(Hamilton et al., 2017a)</ref>, link prediction <ref type=""bibr"" targe >Implementation Details</head><p>Training with the CV estimator is similar as with the NS estimator <ref type=""bibr"" target=""#b4"">(Hamilton et al., 2017a)</ref>. Particularly, each iteration of the al r all the other multi-class datasets. The model is GCN for the former 4 datasets and GraphSAGE-mean <ref type=""bibr"" target=""#b4"">(Hamilton et al., 2017a)</ref> for the latter 2 datasets, see Appendix orted by <ref type=""bibr"" target=""#b2"">Chen et al. (2018)</ref>, while their NS baseline, GraphSAGE <ref type=""bibr"" target=""#b4"">(Hamilton et al., 2017a)</ref>, does not implement the preprocessing t sets because of their slow convergence and the requirement to fit the entire dataset in GPU memory. <ref type=""bibr"" target=""#b4"">Hamilton et al. (2017a)</ref> make an initial attempt to develop stoch www.tei-c.org/ns/1.0""><head n=""2.3."">Neighbor Sampling</head><p>To reduce the receptive field size, <ref type=""bibr"" target=""#b4"">Hamilton et al. (2017a)</ref> propose a neighbor sampling (NS) algorit id=""formula_8"">P (l) uv = n(u) D (l) P uv if v ∈ n(l) (u)</formula><p>, and P (l) uv = 0 otherwise. <ref type=""bibr"" target=""#b4"">Hamilton et al. (2017a)</ref> propose to perform an approximate forwar D (l) needs to be large for NS, to keep comparable predictive performance with the exact algorithm. <ref type=""bibr"" target=""#b4"">Hamilton et al. (2017a)</ref> choose D (1) = 10 and D (2) = 25, and th r, Cora, PubMed and NELL from <ref type=""bibr"">Kipf &amp; Welling (2017)</ref> and Reddit, PPI from <ref type=""bibr"" target=""#b4"">Hamilton et al. (2017a)</ref>, with the same train / validation / test ted to the task nor the model. Our algorithm is applicable to other models including GraphSAGE-mean <ref type=""bibr"" target=""#b4"">(Hamilton et al., 2017a</ref>) and graph attention networks (GAT) <ref e most GCNs only have two graph convolution layers <ref type=""bibr"">(Kipf &amp; Welling, 2017;</ref><ref type=""bibr"" target=""#b4"">Hamilton et al., 2017a)</ref>, this gives a significant reduction of t",1
"et al., 2017a)</ref>, link prediction <ref type=""bibr"" target=""#b6"">(Kipf &amp; Welling, 2016;</ref><ref type=""bibr"" target=""#b1"">Berg et al., 2017)</ref> and knowledge graphs <ref type=""bibr"" target= et al., 2018)</ref>, and other tasks <ref type=""bibr"" target=""#b6"">(Kipf &amp; Welling, 2016;</ref><ref type=""bibr"" target=""#b1"">Berg et al., 2017;</ref><ref type=""bibr"" target=""#b11"">Schlichtkrull e",0
"this section, we consider introducing a third source of randomness, the random dropout of features <ref type=""bibr"" target=""#b12"">(Srivastava et al., 2014)</ref>, which is adopted in various GCN mode rom dropout (ND) of different estimators.</p><p>Our method is based on the weight scaling procedure <ref type=""bibr"" target=""#b12"">(Srivastava et al., 2014)</ref> to approximately compute the mean µ</",0
"get=""#b11"">12,</ref><ref type=""bibr"" target=""#b18"">20,</ref><ref type=""bibr"" target=""#b19"">21,</ref><ref type=""bibr"" target=""#b22"">24,</ref><ref type=""bibr"" target=""#b40"">42]</ref>.</p><p>Single image target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b35"">37,</ref><ref type=""bibr"" target=""#b22"">24,</ref><ref type=""bibr"" target=""#b19"">21,</ref><ref type=""bibr"" tar tep. (c) Progressive upsampling uses a Laplacian pyramid network which gradually predicts SR images <ref type=""bibr"" target=""#b22"">[24]</ref>. (d) Iterative up and downsampling approach is proposed by can preserve HR components better.</p><p>(c) Progressive upsampling was recently proposed in LapSRN <ref type=""bibr"" target=""#b22"">[24]</ref>. It progressively reconstructs the multiple SR images with N <ref type=""bibr"" target=""#b20"">[22]</ref>, DRRN <ref type=""bibr"" target=""#b40"">[42]</ref>, LapSRN <ref type=""bibr"" target=""#b22"">[24]</ref>) on Set5 dataset for 4× enlargement.</p><p>the-art methods N <ref type=""bibr"" target=""#b20"">[22]</ref>, DRRN <ref type=""bibr"" target=""#b40"">[42]</ref>, LapSRN <ref type=""bibr"" target=""#b22"">[24]</ref>, and EDSR <ref type=""bibr"" target=""#b28"">[30]</ref>. We ca step. (c) Progressive upsampling uses a Laplacian pyramid network which gradually predicts SR images<ref type=""bibr"" target=""#b22"">[24]</ref>. (d) Iterative up and downsampling approach is proposed by",1
"ious studies have proven the effectivity of backprojection <ref type=""bibr"" target=""#b48"">[50,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" targe",0
"connection to simply guide the task for the relevant results <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b21"">23,</ref><ref type=""bibr"" target=""#b23"">25]</ref>. Perhaps hampered b",0
"2 × 12 convolutional layer with eight striding and two padding.  We initialize the weights based on <ref type=""bibr"" target=""#b13"">[14]</ref>. Here, std is computed by ( 2/n l ) where n l = f<ref type",0
"= 6) from the original DBPN.</p><p>In the feature extraction, we use conv(3, 128) followed by conv <ref type=""bibr"" target=""#b0"">(1,</ref><ref type=""bibr"" target=""#b30"">32)</ref>. Then, we use conv(1 rk which is the lighter version of the S network, (T = 2). We only use conv(3, 64) followed by conv <ref type=""bibr"" target=""#b0"">(1,</ref><ref type=""bibr"" target=""#b16"">18)</ref> for the initial feat 5 <ref type=""bibr"" target=""#b1"">[2]</ref>, Set14 <ref type=""bibr"" target=""#b47"">[49]</ref>, BSDS100 <ref type=""bibr"" target=""#b0"">[1]</ref>, Urban100 [16] and Manga109 <ref type=""bibr"" target=""#b30"">[",0
"ng works has paid special attention to embed bipartite networks. While a recent work by Dong et al. <ref type=""bibr"" target=""#b13"">[14]</ref> proposed metapath2vec++ for embedding heterogeneous networ ght be suboptimal for learning vertex representations for a bipartite network.</p><p>Metapath2vec++ <ref type=""bibr"" target=""#b13"">[14]</ref>, HNE <ref type=""bibr"" target=""#b26"">[27]</ref> and EOE <re We assign a probability to stop a random walk in each step. In contrast to DeepWalk and other work <ref type=""bibr"" target=""#b13"">[14]</ref> that apply a fixed length on the random walk, we allow the hyper-parameters p and q are set to 0.5 which has empirically shown good results. • Metapath2vec++ <ref type=""bibr"" target=""#b13"">[14]</ref>: This is the state-of-the-art method for embedding heterog",1
"-order and 2nd-order proximities between vertices to embed homogeneous networks. Specifically, LINE <ref type=""bibr"" target=""#b19"">[20]</ref> learns two separated embeddings for 1st-order and 2nd-orde ignal on constructing the bipartite network. Similar to the modeling of 1st-order proximity in LINE <ref type=""bibr"" target=""#b19"">[20]</ref>, we model explicit relations by considering the local prox of vertex sequences. Then the word2vec is applied on the corpus to learn vertex embeddings. • LINE <ref type=""bibr"" target=""#b19"">[20]</ref>: This approach optimizes both the 1st-order and 2nd-order vec inspire many works <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b19"">20]</ref> to use inner product to model the interaction between two e ural embedding methods <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b19"">20]</ref>, we parameterize the conditional probability P(u c |u i ) a",1
"network, where the edges can indicate users' click behaviors that provide valuable relevance signal <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref>; in another applicatio f> learns to rank vertices by capturing some semantic relations within a bipartite network. Co-HITS <ref type=""bibr"" target=""#b0"">[1]</ref> incorporates content information of vertices and the constra at contain the 2nd-order proximity between vertices of the same type. Following the idea of Co-HITS <ref type=""bibr"" target=""#b0"">[1]</ref>, we define the 2nd-order proximity between two vertices as:<",0
"are of the same type <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b7"">[8]</ref><ref type=""bibr"" target=""#b8"">[9]</ref><ref type=""bibr"" target=""#b9"">[10]</ref>. Following the pione target=""#b23"">[24]</ref>, textual content <ref type=""bibr"" target=""#b24"">[25]</ref>, user profiles <ref type=""bibr"" target=""#b8"">[9]</ref>, location information <ref type=""bibr"" target=""#b25"">[26]</r ]</ref>, textual descriptions <ref type=""bibr"" target=""#b42"">[43]</ref>, and among other attributes <ref type=""bibr"" target=""#b8"">[9]</ref>. In addition, the bipartite networks in many practical appli",0
"ampling method that caters the network data.</p><p>First we employ locality sensitive hashing (LSH) <ref type=""bibr"" target=""#b36"">[37]</ref> to block vertices after shingling each vertex by its ws-ho LSH can guarantee that dissimilar vertices are located in different buckets in a probabilistic way <ref type=""bibr"" target=""#b36"">[37]</ref>.</p><p>Let N ns S (u i ) denote the ns negative samples fo",0
"pling is to approximate the costly denominator term of softmax with some sampled negative instances <ref type=""bibr"" target=""#b35"">[36]</ref>. Then the learning can be performed by optimizing a point-",0
"re is no stationary distribution of random walks on bipartite networks due to the periodicity issue <ref type=""bibr"" target=""#b33"">[34]</ref>. To address this issue, we consider performing random walk",0
"the MF model for the ranking task, by towards a different pairwise regressionbased loss. • FISMauc <ref type=""bibr"" target=""#b39"">[40]</ref>: Distinct to MF, factored item similarity model (FISM) is",0
"target=""#b3"">[4]</ref><ref type=""bibr"" target=""#b4"">[5]</ref><ref type=""bibr"" target=""#b5"">[6]</ref><ref type=""bibr"" target=""#b6"">[7]</ref>. In particular, they embed vertices into a low dimensional s ence S ∈ D U , we can then approximate the conditional probability p(u c |u i ) defined in Equation <ref type=""bibr"" target=""#b6"">(7)</ref> as:</p><formula xml:id=""formula_14"">p(u c , N ns S (u i ) |u",0
"rmation, such as numerical features <ref type=""bibr"" target=""#b41"">[42]</ref>, textual descriptions <ref type=""bibr"" target=""#b42"">[43]</ref>, and among other attributes <ref type=""bibr"" target=""#b8"">",0
"target=""#b6"">(Collobert et al., 2011;</ref><ref type=""bibr"" target=""#b29"">Passos et al., 2014;</ref><ref type=""bibr"" target=""#b15"">Huang et al., 2015;</ref><ref type=""bibr"" target=""#b26"">Luo et al., 2 xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3"">Model</head><p>We follow the best English NER model <ref type=""bibr"" target=""#b15"">(Huang et al., 2015;</ref><ref type=""bibr"" target=""#b27"">Ma and Hovy,",1
"y predicted. The current stateof-the-art for English NER has been achieved by using LSTM-CRF models <ref type=""bibr"" target=""#b17"">(Lample et al., 2016;</ref><ref type=""bibr"" target=""#b27"">Ma and Hovy ibr"" target=""#b15"">(Huang et al., 2015;</ref><ref type=""bibr"" target=""#b27"">Ma and Hovy, 2016;</ref><ref type=""bibr"" target=""#b17"">Lample et al., 2016)</ref>, using LSTM-CRF as the main network struct epresentations Both character CNN <ref type=""bibr"" target=""#b27"">(Ma and Hovy, 2016)</ref> and LSTM <ref type=""bibr"" target=""#b17"">(Lample et al., 2016)</ref> have been used for representing the chara",1
"as been achieved by using LSTM-CRF models <ref type=""bibr"" target=""#b17"">(Lample et al., 2016;</ref><ref type=""bibr"" target=""#b27"">Ma and Hovy, 2016;</ref><ref type=""bibr"" target=""#b5"">Chiu and Nichol ad><p>We follow the best English NER model <ref type=""bibr"" target=""#b15"">(Huang et al., 2015;</ref><ref type=""bibr"" target=""#b27"">Ma and Hovy, 2016;</ref><ref type=""bibr"" target=""#b17"">Lample et al., concatenated as its representation:</p><p>Integrating character representations Both character CNN <ref type=""bibr"" target=""#b27"">(Ma and Hovy, 2016)</ref> and LSTM <ref type=""bibr"" target=""#b17"">(La",1
"br"" target=""#b17"">(Lample et al., 2016;</ref><ref type=""bibr"" target=""#b27"">Ma and Hovy, 2016;</ref><ref type=""bibr"" target=""#b5"">Chiu and Nichols, 2016;</ref><ref type=""bibr"" target=""#b21"">Liu et al. to some extent the role of a gazetteer <ref type=""bibr"" target=""#b36"">(Ratinov and Roth, 2009;</ref><ref type=""bibr"" target=""#b5"">Chiu and Nichols, 2016)</ref>, but not fully since there is no explici",0
"r"" target=""#b27"">Ma and Hovy, 2016;</ref><ref type=""bibr"" target=""#b5"">Chiu and Nichols, 2016;</ref><ref type=""bibr"" target=""#b21"">Liu et al., 2018)</ref> with character information being integrated i a single LSTM to obtain − → h c j and ← − h c j for each c j . It is similar with the structure of <ref type=""bibr"" target=""#b21"">Liu et al. (2018)</ref> but not uses the highway layer. The same LSTM",0
"/ref>. It has been shown that character-based methods outperform word-based methods for Chinese NER <ref type=""bibr"" target=""#b13"">(He and Wang, 2008;</ref><ref type=""bibr"" target=""#b24"">Liu et al., 2 and character-based methods for the task, showing that the latter is empirically a superior choice <ref type=""bibr"" target=""#b13"">(He and Wang, 2008;</ref><ref type=""bibr"" target=""#b24"">Liu et al., 2",0
"dataset leverage rich handcrafted features <ref type=""bibr"" target=""#b1"">(Chen et al., 2006a;</ref><ref type=""bibr"" target=""#b52"">Zhang et al., 2006;</ref><ref type=""bibr"" target=""#b54"">Zhou et al.,",0
"per, which include OntoNotes 4 <ref type=""bibr"" target=""#b44"">(Weischedel et al., 2011)</ref>, MSRA <ref type=""bibr"" target=""#b19"">(Levow, 2006)</ref>  For more variety in test domains, we collected a",0
"ibr"" target=""#b15"">Huang et al., 2015;</ref><ref type=""bibr"" target=""#b26"">Luo et al., 2015)</ref>. <ref type=""bibr"" target=""#b37"">Rei (2017)</ref> uses a word-level language modeling objective to aug",0
"M model.</p><p>Lattice structured RNNs can be viewed as a natural extension of tree-structured RNNs <ref type=""bibr"" target=""#b42"">(Tai et al., 2015)</ref> to DAGs. They have been used to model motion",0
"xperimental Settings</head><p>Data. Four datasets are used in this paper, which include OntoNotes 4 <ref type=""bibr"" target=""#b44"">(Weischedel et al., 2011)</ref>, MSRA <ref type=""bibr"" target=""#b19"">",0
"representing characters in word segmentation <ref type=""bibr"" target=""#b3"">(Chen et al., 2015;</ref><ref type=""bibr"" target=""#b50"">Yang et al., 2017a)</ref>. We augment the character-based model with or studying oracle situations where gold segmentation is given. We use the neural word segmentor of <ref type=""bibr"" target=""#b50"">Yang et al. (2017a)</ref> to automatically segment the development an old segmentation on their respective training sets. For Weibo and resume, we take the best model of <ref type=""bibr"" target=""#b50"">Yang et al. (2017a)</ref> off the shelf<ref type=""foot"" target=""#foot",0
"r"" target=""#b22"">(Liu and Zhang, 2012;</ref><ref type=""bibr"" target=""#b16"">Jiang et al., 2013;</ref><ref type=""bibr"" target=""#b23"">Liu et al., 2014;</ref><ref type=""bibr"" target=""#b35"">Qiu and Zhang,",0
"em can be severe in the open domain since crossdomain word segmentation remains an unsolved problem <ref type=""bibr"" target=""#b22"">(Liu and Zhang, 2012;</ref><ref type=""bibr"" target=""#b16"">Jiang et al",0
"targeted network, which corresponds to a more restrictive black box threat model.</p><p>Recent work <ref type=""bibr"" target=""#b4"">(Chen et al., 2017;</ref><ref type=""bibr"" target=""#b1"">Bhagoji et al., =""bibr"" target=""#b10"">Ilyas et al., 2017)</ref> provides a number of attacks for this threat model. <ref type=""bibr"" target=""#b4"">Chen et al. (2017)</ref> show how to use a basic primitive of zeroth o (x, ) (x l−1 + η s l ) with s l = Π ∂Bp(0,1) ∇ x L(x l−1 , y)<label>(4)</label></formula><p>Indeed, <ref type=""bibr"" target=""#b4"">Chen et al. (2017)</ref> were the first to use finite differences meth ty d=268,203 and thus this method would require 268,204 queries. (It is worth noting, however, that <ref type=""bibr"" target=""#b4"">Chen et al. (2017)</ref> developed additional methods to, at least par",1
"examples generated with these methods <ref type=""bibr"" target=""#b19"">Papernot et al. (2017)</ref>; <ref type=""bibr"" target=""#b14"">Liu et al. (2016)</ref> tend to transfer to a target MNIST or CIFAR c",0
"ve black box threat model.</p><p>Recent work <ref type=""bibr"" target=""#b4"">(Chen et al., 2017;</ref><ref type=""bibr"" target=""#b1"">Bhagoji et al., 2017;</ref><ref type=""bibr"" target=""#b10"">Ilyas et al.",0
"od builds on the framework of bandit optimization, a fundamental tool in online convex optimization <ref type=""bibr"" target=""#b9"">Hazan (2016)</ref>. In the bandit optimization framework, an agent pla or ∆ of the gradient −∇ v ∇L(x, y), v of the loss be the standard spherical gradient estimator (see <ref type=""bibr"" target=""#b9"">Hazan (2016)</ref>). We take a two-query estimate of the expectation, dating the latent vector v t . We will adapt here the canonical ""reduction from bandit information"" <ref type=""bibr"" target=""#b9"">(Hazan, 2016)</ref>. Specifically, our update procedure is parametrize",0
"aints that help generate more content rich responses that are based on a model of syntax and topics <ref type=""bibr"" target=""#b12"">(Griffiths et al., 2005)</ref> and semantic similarity <ref type=""bib o estimate these distributions, we leverage the unsupervised model of topics and syntax proposed by <ref type=""bibr"" target=""#b12"">Griffiths and Steyvers (2005)</ref>. The second constraint encourages",1
"ax and topics <ref type=""bibr"" target=""#b12"">(Griffiths et al., 2005)</ref> and semantic similarity <ref type=""bibr"" target=""#b0"">(Arora et al., 2016)</ref>. We evaluate our approach against a variety ar to the user's input; semantic similarity is measured using fixed-dimensional sentence embeddings <ref type=""bibr"" target=""#b0"">(Arora et al., 2016)</ref>.</p><p>After introducing distributional con edding. Next the first principal component of all the sentence embeddings in the corpus is removed. <ref type=""bibr"" target=""#b0"">(Arora et al., 2016)</ref> points that the first principal component h we want this encoding to be relatively efficient as it will be used many times during beam search. <ref type=""bibr"" target=""#b0"">Arora et. al. (2016)</ref> recently proposed a simple sentence embeddi",1
"models for data-driven response generation <ref type=""bibr"" target=""#b30"">(Shang et al., 2015;</ref><ref type=""bibr"" target=""#b32"">Sordoni et al., 2015;</ref><ref type=""bibr"" target=""#b17"">Li et al.,",0
"mizing likelihood of the response. This line of work was further extended with adversarial learning <ref type=""bibr"" target=""#b21"">(Li et al., 2017)</ref> that rewards generated conversations that are al evaluation of neural conversation models <ref type=""bibr"" target=""#b23"">(Lowe et al., 2017;</ref><ref type=""bibr"" target=""#b21"">Li et al., 2017)</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns",0
"l and adaptive conversation, in addition to new applications such as predictive response suggestion <ref type=""bibr"" target=""#b13"">(Kannan et al., 2016)</ref>, however many challenges remain.</p><p>A 2014)</ref> which has been used as a basis for a broad range of recent work on neural conversation <ref type=""bibr"" target=""#b13"">(Kannan et al., 2016;</ref><ref type=""bibr"" target=""#b17"">Li et al.,",0
"s representation, using attention, and generates the response using a neural network language model <ref type=""bibr"" target=""#b3"">(Bengio et al., 2003;</ref><ref type=""bibr"" target=""#b33"">Sutskever et",0
"t=""#b36"">(Vijayakumar et al., 2016;</ref><ref type=""bibr"" target=""#b18"">Li and Jurafsky, 2016;</ref><ref type=""bibr"" target=""#b19"">Li et al., 2016b)</ref> Following previous work we evaluated our appr",0
"get=""#b24"">(Mann and McCallum, 2008;</ref><ref type=""bibr"" target=""#b10"">Ganchev et al., 2010;</ref><ref type=""bibr"" target=""#b45"">Zhu et al., 2014)</ref>. Posterior regularization introduces similar",0
"ddition to an approach that conditions on topic models as additional context in neural conversation <ref type=""bibr"" target=""#b42"">(Xing et al., 2017)</ref>. While encouraging the model to generate le t=""#b17"">(Li et al., 2016a)</ref>. TA-Seq2Seq: Another relevant baseline is the TA-Seq2Seq model of <ref type=""bibr"" target=""#b42"">Xing et. al. (2017)</ref> that integrates information from a pre-trai",0
"d a stochastic approach to beam search that does segment-by-segment reranking to promote diversity. <ref type=""bibr"" target=""#b44"">Zhang et. al. (2018)</ref> develop models which converse while assumi",0
"iction by reinterpretation of fully connected layers of the classifier as a fully convolution layer <ref type=""bibr"" target=""#b24"">[25]</ref>. The FCN consists of an encoder of input images and a deco overcome this problem, skip connection methods are used <ref type=""bibr"" target=""#b17"">[18]</ref>, <ref type=""bibr"" target=""#b24"">[25]</ref>. The features extracted from PPL are upsampled and concate",1
"Ns) <ref type=""bibr"" target=""#b12"">[13]</ref>- <ref type=""bibr"" target=""#b16"">[17]</ref>, and U-Net <ref type=""bibr"" target=""#b17"">[18]</ref> algorithms have been used.</p><p>Although it is possible f er for upsampling.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>B. U-Net</head><p>U-Net <ref type=""bibr"" target=""#b17"">[18]</ref> is a modified FCN for yielding more precise segmentation. formation such as boundaries of objects. To overcome this problem, skip connection methods are used <ref type=""bibr"" target=""#b17"">[18]</ref>, <ref type=""bibr"" target=""#b24"">[25]</ref>. The features e U-Net. It is important to note that the compared U-Net is not the original architecture proposed in <ref type=""bibr"" target=""#b17"">[18]</ref> but a highly calibrated model for the enhanced capability",1
"ms that are applied to object segmentation based on CNNs <ref type=""bibr"" target=""#b21"">[22]</ref>- <ref type=""bibr"" target=""#b23"">[24]</ref>. Because the performance of deep learning algorithms depen",1
"eir original image size. The encoder part of FCN consists of visual geometry group network (VGGNet) <ref type=""bibr"" target=""#b25"">[26]</ref> that is a famous CNN classification model and the decoder",1
"lopment, and cartography as well as for emergency planning systems for evacuation and fire response <ref type=""bibr"" target=""#b2"">[3]</ref>.</p><p>The feature extraction method for creating or modifyi",0
"SYSTEMS</head><p>There are several algorithms that are applied to object segmentation based on CNNs <ref type=""bibr"" target=""#b21"">[22]</ref>- <ref type=""bibr"" target=""#b23"">[24]</ref>. Because the pe",0
"variety of industries including land inventory, vegetation monitoring, and environmental assessment <ref type=""bibr"" target=""#b1"">[2]</ref>. In particular, extraction of manufactured features such as",0
"have been carried out using aerial images in rural areas <ref type=""bibr"" target=""#b13"">[14]</ref>- <ref type=""bibr"" target=""#b15"">[16]</ref>. In addition, they have used many data from the OpenStreet",0
"h there are the large-scale data sets for classification <ref type=""bibr"" target=""#b18"">[19]</ref>, <ref type=""bibr"" target=""#b19"">[20]</ref>. Second, due to the lack of multiclass data sets, binary d",0
"utional neural networks (CNNs) have attracted much attention to segment objects in satellite images <ref type=""bibr"" target=""#b10"">[11]</ref>, <ref type=""bibr"" target=""#b11"">[12]</ref>. To segment roa",0
"ine and artificial neural networks have been studied in <ref type=""bibr"" target=""#b7"">[8]</ref> and <ref type=""bibr"" target=""#b8"">[9]</ref>. The automatic methods are objective without human intervent",0
"lized as performance indices. The experiments were implemented using the public platform Tensorflow <ref type=""bibr"" target=""#b31"">[32]</ref> and run on an Intel core 6 i7-7820X CPU at 3.6 GHz with 2",0
"train such deep learning algorithms, though there are the large-scale data sets for classification <ref type=""bibr"" target=""#b18"">[19]</ref>, <ref type=""bibr"" target=""#b19"">[20]</ref>. Second, due to",0
"emantic segmentation model, exploiting multiscale features by using the pyramid pooling layer (PPL) <ref type=""bibr"" target=""#b20"">[21]</ref> to extract information from various classes. The proposed an eight-level pyramid pooling module allows to obtaining the effective global context information <ref type=""bibr"" target=""#b20"">[21]</ref> and, therefore, produces a high-quality result on object s",0
". The numbers of the filter channels are increased. In this process, we utilize batch normalization <ref type=""bibr"" target=""#b27"">[28]</ref> to prevent the overfitting problem as shown in Fig. <ref t",0
"s/rights/index.html for more information.</p><p>research <ref type=""bibr"" target=""#b12"">[13]</ref>, <ref type=""bibr"" target=""#b13"">[14]</ref>. To address these problems, we make a large scale of the d egment objects such as roads and buildings have been carried out using aerial images in rural areas <ref type=""bibr"" target=""#b13"">[14]</ref>- <ref type=""bibr"" target=""#b15"">[16]</ref>. In addition, t ing segmentation due to the lack of multiclass data sets <ref type=""bibr"" target=""#b12"">[13]</ref>, <ref type=""bibr"" target=""#b13"">[14]</ref>. Moreover, classifying several classes at once can solve t",0
"esults have generally been discouraging due to various adverse factors (image noise, shadows, etc.) <ref type=""bibr"" target=""#b9"">[10]</ref>.</p><p>Recently, as the deep learning technology has evolve",0
"g techniques such as fully convolutional networks (FCNs) <ref type=""bibr"" target=""#b12"">[13]</ref>- <ref type=""bibr"" target=""#b16"">[17]</ref>, and U-Net <ref type=""bibr"" target=""#b17"">[18]</ref> algor",0
"between prediction results and the ground truth for training the proposed model. The Adam optimizer <ref type=""bibr"" target=""#b30"">[31]</ref> with the learning rate α = 10 −4 , the exponential decay r",0
"it.</p><p>This paper presents the results of our study of the nonuniform cache architecture (NUCA) <ref type=""bibr"" target=""#b36"">[35]</ref> characteristics of LLC in Intel processors where the LLC i target=""#b5"">6,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b23"">23,</ref><ref type=""bibr"" target=""#b36"">35,</ref><ref type=""bibr"" target=""#b66"">65,</ref><ref type=""bibr"" tar",1
"type=""bibr"" target=""#b89"">87]</ref> or compiler optimization <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b30"">30,</ref><ref type=""bibr"" tar",0
"lice mapping and reverse-engineer Intel's Complex Addressing <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b27"">27,</ref><ref type=""bibr"" target=""#b40"">39,</ref><ref type=""bibr"" tar erse engineer the Intel LLC Complex Addressing hash function <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b27"">27,</ref><ref type=""bibr"" target=""#b40"">39,</ref><ref type=""bibr"" tar",0
"e.g., using cgroups-cpusets) or monitoring/migration data (similar to the H/W features suggested by <ref type=""bibr"" target=""#b23"">[23,</ref><ref type=""bibr"" target=""#b46"">45]</ref>). Furthermore, app target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b23"">23,</ref><ref type=""bibr"" target=""#b36"">35,</ref><ref type=""bibr"" tar",0
"ation profiling without considering Intel's LLC Complex Addressing. In contrast, other works (e.g., <ref type=""bibr"" target=""#b61"">[60,</ref><ref type=""bibr"" target=""#b87"">85]</ref>) extended traditio",0
"n. It achieved topic and keyword identification that the cross-language transfer learning method in <ref type=""bibr"" target=""#b13"">[14]</ref> used to learn the characteristics of low-resourced languag",1
"tter <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b10"">11]</ref>. According to <ref type=""bibr"" target=""#b11"">[12]</ref>, the accuracy rate of the cross-language speech recognitio",1
"ved a relative improvement of 3%~5% in the continuous speech recognition task of a large vocabulary <ref type=""bibr"" target=""#b29"">[30]</ref><ref type=""bibr"" target=""#b30"">[31]</ref><ref type=""bibr"" t",0
"differences between the Tujia and Chinese languages according to the Jaccard similarity coefficient <ref type=""bibr"" target=""#b42"">[43]</ref>. The definition of the Jaccard index is as follows:</p><fo",0
"he BiLSTM and CTC networks has become a new standard combination in the field of speech recognition <ref type=""bibr"" target=""#b36"">[37]</ref><ref type=""bibr"" target=""#b37"">[38]</ref><ref type=""bibr"" t",0
"clude two semi-vowel initials. The finals consist of 6 monophthongs, 11 diphthongs, and 8 orinasals <ref type=""bibr"" target=""#b39"">[40,</ref><ref type=""bibr"" target=""#b40"">41]</ref>.</p></div> <div xm",0
"lassification model of deep belief networks (DBNs) was trained in five types of speech databases in <ref type=""bibr"" target=""#b14"">[15]</ref>, and the transfer learning method was used to improve the",0
"hich are based on coarse-grained modelling unit technology <ref type=""bibr"" target=""#b18"">[19,</ref><ref type=""bibr"" target=""#b19"">20]</ref>, has enabled progress to be made in recognition performance",0
"layer to estimate the posterior probability after clustering <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b8"">9]</ref>. Transfer learning is an emerging research field in machine l",0
"e timing information have become important research topics <ref type=""bibr"" target=""#b20"">[21]</ref><ref type=""bibr"" target=""#b21"">[22]</ref><ref type=""bibr"" target=""#b22"">[23]</ref>. In References <r",0
"ual to physical translations directly in memory. Similar in design to the recently proposed PoM-TLB <ref type=""bibr"" target=""#b48"">(Ryoo et al. 2017)</ref>, we propose a TLB in GPU DRAM (DRAM-TLB) tha to as a Stacked-TLB while a DRAM-TLB architected using system memory is referred to as a SysMem-TLB <ref type=""bibr"" target=""#b48"">(Ryoo et al. 2017)</ref>. DRAM-TLBs are physically placed in a large erage recent work that uses a simple predictor to learn the page size for any given virtual address <ref type=""bibr"" target=""#b48"">(Ryoo et al. 2017)</ref>. In this work, the authors propose a 512-ent -TLBs also resemble recent CPU-centric work that extends TLB reach of CPUs (referred to as PoM-TLB) <ref type=""bibr"" target=""#b48"">(Ryoo et al. 2017)</ref>. Our work explores the DRAM-TLB design space",1
"d Hill 1994;</ref><ref type=""bibr"" target=""#b54"">Talluri et al. 1992</ref>) due to memory imbalance <ref type=""bibr"" target=""#b18"">(Gaud et al. 2014)</ref>, memory fragmentation, paging <ref type=""bib target=""#b53"">(Talluri and Hill 1994;</ref><ref type=""bibr"" target=""#b54"">Talluri et al. 1992;</ref><ref type=""bibr"" target=""#b18"">Gaud et al. 2014;</ref><ref type=""bibr"" target=""#b11"">Chou et al. 201 target=""#b53"">(Talluri and Hill 1994;</ref><ref type=""bibr"" target=""#b54"">Talluri et al. 1992;</ref><ref type=""bibr"" target=""#b18"">Gaud et al. 2014;</ref><ref type=""bibr"" target=""#b11"">Chou et al. 201",0
"ibr"" target=""#b6"">(Bhattacharjee et al. 2011)</ref>. All caches use 128B cache line size with 32B   <ref type=""bibr"" target=""#b23"">(Jaleel et al. 2010)</ref>.</p><p>We model a hybrid memory subsystem y. To this end, we leverage the Dynamic Re-Reference Interval Prediction (DRRIP) replacement policy <ref type=""bibr"" target=""#b23"">(Jaleel et al. 2010)</ref>. In this policy, all UCAT insertions follo cy to consult the UCAT on an LLT miss. The baseline LLC (and UCAT) uses the DRRIP replacement policy<ref type=""bibr"" target=""#b23"">(Jaleel et al. 2010)</ref> while the L1 cache and TLBs use pseudo-LRU -rates and are often inefficiently used<ref type=""foot"" target=""#foot_3"">3</ref> (Lee and Kim 2012; <ref type=""bibr"" target=""#b23"">Jaleel et al. 2010;</ref><ref type=""bibr"" target=""#b45"">Qureshi et al ntly verifies) that the majority of LLC entries are unused after cache insertion (Lee and Kim 2012; <ref type=""bibr"" target=""#b23"">Jaleel et al. 2010;</ref><ref type=""bibr"" target=""#b45"">Qureshi et al",0
"U using Address Translation Services (ATS) <ref type=""bibr"" target=""#b56"">(Vesely et al. 2016;</ref><ref type=""bibr"" target=""#b1"">ATS 2009)</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><h",0
"ee and Martonosi 2010;</ref><ref type=""bibr"" target=""#b31"">Kandiraju and Sivasubramaniam 2002;</ref><ref type=""bibr"" target=""#b49"">Saulsbury et al. 2000;</ref><ref type=""bibr"" target=""#b44"">Power et a",0
"target=""#foot_3"">3</ref> (Lee and Kim 2012; <ref type=""bibr"" target=""#b23"">Jaleel et al. 2010;</ref><ref type=""bibr"" target=""#b45"">Qureshi et al. 2007;</ref><ref type=""bibr"" target=""#b58"">Wu et al. 20 ed after cache insertion (Lee and Kim 2012; <ref type=""bibr"" target=""#b23"">Jaleel et al. 2010;</ref><ref type=""bibr"" target=""#b45"">Qureshi et al. 2007;</ref><ref type=""bibr"" target=""#b58"">Wu et al. 20",0
"ding on the page table implementation, address translation requires one or more page table accesses <ref type=""bibr"" target=""#b4"">(Bhargava et al. 2008)</ref>. To avoid the long memory access latency,",0
"isses instead. The IOMMU (and GMMU) is configured with a highly-threaded hardware page table walker <ref type=""bibr"" target=""#b44"">(Power et al. 2014;</ref><ref type=""bibr"" target=""#b43"">Pichai et al. nt and high performing address translation on GPUs is an important area of research. Recent studies <ref type=""bibr"" target=""#b44"">(Power et al. 2014;</ref><ref type=""bibr"" target=""#b43"">Pichai et al. Kandiraju and Sivasubramaniam 2002;</ref><ref type=""bibr"" target=""#b49"">Saulsbury et al. 2000;</ref><ref type=""bibr"" target=""#b44"">Power et al. 2014)</ref>, or speculating on the address translation o rs to GPUs do not perform well. Consequently, novel mechanisms such as highly threaded page walkers <ref type=""bibr"" target=""#b44"">(Power et al. 2014</ref>) and intelligent page walk scheduling <ref t",0
"ccess, to generate a speculative value that does not necessarily exhibit value locality (e.g., DLVP <ref type=""bibr"" target=""#b2"">[3]</ref>). While value predictors can generate speculative results fo rk has shown that load-only predictors are most efficient with a modest hardware budget (e.g., 8KB) <ref type=""bibr"" target=""#b2"">[3]</ref>, <ref type=""bibr"" target=""#b3"">[4]</ref>.</p><p>In this stud r"" target=""#b6"">[7]</ref>, <ref type=""bibr"" target=""#b7"">[8]</ref> Context Address Prediction (CAP) <ref type=""bibr"" target=""#b2"">[3]</ref> one another. We found that no individual predictor is strict e focus only on predicting load values since that is most effective with limited hardware resources <ref type=""bibr"" target=""#b2"">[3]</ref>, <ref type=""bibr"" target=""#b3"">[4]</ref>.</p></div> <div xml sm needed to communicate the predicted values from the value-predicted producers to their consumers <ref type=""bibr"" target=""#b2"">[3]</ref>. Consumers of the load can use the prediction by reading the s practical implementations of value prediction, we encourage the readers to visit prior art papers <ref type=""bibr"" target=""#b2"">[3]</ref>, <ref type=""bibr"" target=""#b6"">[7]</ref>, <ref type=""bibr"" t about the baseline ISA, microarchitecture, and storage constraints (Sheikh reports similar findings <ref type=""bibr"" target=""#b2"">[3]</ref>, <ref type=""bibr"" target=""#b26"">[27]</ref>).</p><p>In all of ""#b7"">[8]</ref>, and subsequent work confirmed the same is true for load instructions in particular <ref type=""bibr"" target=""#b2"">[3]</ref>, <ref type=""bibr"" target=""#b3"">[4]</ref>.</p><p>Our implemen an directly generating values from We use the state-of-the-art DLVP predictor as a reference design <ref type=""bibr"" target=""#b2"">[3]</ref>. The predictor consists of one tagged table indexed by a has",1
"ception: dependent loads are not allowed to be reordered <ref type=""bibr"" target=""#b23"">[24]</ref>, <ref type=""bibr"" target=""#b24"">[25]</ref>. Value prediction can violate this rule. To avoid violatin",0
"target=""#b11"">[12]</ref>, media player <ref type=""bibr"" target=""#b12"">[13]</ref>, browser benchmark <ref type=""bibr"" target=""#b13"">[14]</ref>, and various Javascript benchmarks <ref type=""bibr"" target",0
""">[15]</ref>, <ref type=""bibr"" target=""#b15"">[16]</ref>, <ref type=""bibr"" target=""#b16"">[17]</ref>, <ref type=""bibr"" target=""#b17"">[18]</ref>, <ref type=""bibr"" target=""#b18"">[19]</ref>, <ref type=""bib",0
"tors, we use forward probabilistic counters to reduce the number of bits needed to track confidence <ref type=""bibr"" target=""#b27"">[28]</ref>. Using FPC, a confidence counter is incremented with proba",0
"the context of deep neural networks, and there is now a quickly growing body of work on this topic <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" targ e, the ∞ -ball around x has recently been studied as a natural notion for adversarial perturbations <ref type=""bibr"" target=""#b10"">[11]</ref>. While we focus on robustness against ∞ -bounded attacks i s. On the attack side, prior work has proposed methods such as the Fast Gradient Sign Method (FGSM) <ref type=""bibr"" target=""#b10"">[11]</ref> and multiple variations of it <ref type=""bibr"" target=""#b1 rget=""#b2"">[3]</ref> for an overview of earlier work).</p><p>Adversarial training was introduced in <ref type=""bibr"" target=""#b10"">[11]</ref>, however the adversary utilized was quite weak-it relied o arial training discusses the phenomenon of transferability <ref type=""bibr"" target=""#b27"">[28,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b28"">29]</ref>-adversarial example",1
"he above insights, we train networks on MNIST <ref type=""bibr"" target=""#b18"">[19]</ref> and CIFAR10 <ref type=""bibr"" target=""#b15"">[16]</ref> that are robust to a wide range of adversarial attacks. Ou incorporating wider layers by a factor of 10. This results in a network with 5 residual units with <ref type=""bibr"" target=""#b15"">(16,</ref><ref type=""bibr"">160,</ref><ref type=""bibr"">320,</ref><ref",0
"rget=""#b10"">[11,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" tar Gradient Sign Method (FGSM) <ref type=""bibr"" target=""#b10"">[11]</ref> and multiple variations of it <ref type=""bibr"" target=""#b17"">[18]</ref>. FGSM is an attack for an ∞ -bounded adversary and compute ve that the network overfits to these adversarial examples. This behavior is known as label leaking <ref type=""bibr"" target=""#b17"">[18]</ref> and stems from the fact that the adversary produces a very al training on ImageNet also observed that the model capacity is important for adversarial training <ref type=""bibr"" target=""#b17"">[18]</ref>. In contrast to this paper, we find that training against",0
"sign goal. While trained models tend to be very effective in classifying benign inputs, recent work <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" targ k. Unfortunately, ERM often does not yield models that are robust to adversarially crafted examples <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b27"">28]</ref>. Formally, there are",0
"gradient, preventing PGD from maximizing the loss. Attacking the model with a decision-based attack <ref type=""bibr"" target=""#b3"">[4]</ref> which does not rely on model gradients reveals that the mode ersarially trained networks, we also evaluate the performance of the Decision Boundary Attack (DBA) <ref type=""bibr"" target=""#b3"">[4]</ref> with 2000 steps and PGD on standard and adversarially traine versarially trained networks, we also evaluate the performance of the Decision Boundary Attack (DBA)<ref type=""bibr"" target=""#b3"">[4]</ref> with 2000 steps and PGD on standard and adversarially traine",0
"es in resource utilization and on-chip buffer requirements <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b39"">40]</ref>.</p><p>To overcome these challenges, we present Tangram, a t-product operations <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" target=""#b39"">40]</ref>.</p><p>A notable technique to further improve NN efficiency ied with the output vectors. They can be formulated as CONV and FC layers with different dimensions <ref type=""bibr"" target=""#b39"">[40,</ref><ref type=""bibr"" target=""#b43"">44]</ref>. Because the outpu pagation of CONV and FC layers can be formulated as new CONV or FC layers with different dimensions <ref type=""bibr"" target=""#b39"">[40,</ref><ref type=""bibr"" target=""#b43"">44]</ref>. The BSD optimizat he data forwarding needed for inter-layer pipelining is already available in tiled NN architectures <ref type=""bibr"" target=""#b39"">[40,</ref><ref type=""bibr"" target=""#b43"">44]</ref>. In Tangram, ALLO rget=""#b4"">[5,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b39"">40,</ref><ref type=""bibr"" target=""#b43"">44]</ref>. As shown in Figure get=""#b24"">25,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" target=""#b37"">38,</ref><ref type=""bibr"" target=""#b39"">40,</ref><ref type=""bibr"" target=""#b43"">44]</ref>. Such inter-layer p (all layers) can be mapped onto a single or multiple chips <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b39"">40,</ref><ref type=""bibr"" target=""#b43"">44]</ref>. This approach does tiles.</p><p>Inter-layer pipelining: ISAAC <ref type=""bibr"" target=""#b35"">[36]</ref> and PipeLayer <ref type=""bibr"" target=""#b39"">[40]</ref> used inter-layer pipelining in ReRAM-based accelerators, b",1
"r"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" tar critical in achieving high performance and energy efficiency <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" tar "" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" tar ngine runs at 500 MHz. The PE area and power are scaled from <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b27"">28]</ref>, assuming 0.004 mm ef type=""bibr"" target=""#b4"">5]</ref> or 2D spatial PE arrays <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" tar",0
"a local SRAM buffer <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b43"">44]</ref>. A network-on-chip multiple NN engines <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b39"">40,</ref><ref type=""bibr"" tar s tiled accelerators <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b43"">44]</ref>. Recent work has al a-layer parallelism) leads to significant data duplication <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b21"">22]</ref>, and pipelining Session: Machine Learning II ASPLOS <ref ty ns of a single layer <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b21"">22]</ref>. Table <ref type=""table"" target=""#tab_1"">1</ref> shows diff er controller, which notifies the PE array to start the computation when all data have been fetched <ref type=""bibr"" target=""#b21"">[22]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head to each of the 3D channels. Neurocube proposed a simple heuristic for NN partitioning across tiles <ref type=""bibr"" target=""#b21"">[22]</ref> and TETRIS extended it to hybrid partitioning schemes <ref",0
"nd energy efficiency <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b27"">28]</ref>.</p><p>The need for ch tile includes a small 2D PE array and a local SRAM buffer <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" tar ure <ref type=""figure"" target=""#fig_1"">2</ref> are scheduled <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b45"">46]</ref>. Since the total da ce is to build a tiled architecture with multiple NN engines <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" tar s can process in parallel the computations of a single layer <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b21"">22]</ref>. Table <ref type=""t n 3.3 to trim the design space. Our cost model is similar to <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b45"">46]</ref>. For each individua s. Tangram uses a similar architecture as tiled accelerators <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" tar es. Parallelizing a single NN layer (intra-layer parallelism) leads to significant data duplication <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b21"">22]</ref>, and pipelining Se h an optimized tiled NN accelerator (Figure <ref type=""figure"">4</ref>) similar to recent proposals <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b43"">44]</ref>. We consider 16 ? pe=""table"" target=""#tab_1"">1</ref> shows different schemes to leverage such intra-layer parallelism <ref type=""bibr"" target=""#b13"">[14]</ref>. Batch parallelization partitions the batch so that each e supports intra-layer parallelism using hybrid output and fmap parallelization as proposed in TETRIS <ref type=""bibr"" target=""#b13"">[14]</ref>. All engines fetch data in parallel from multiple off-chip e is not an issue as long as the hybrid parallelization partitions the data approximately uniformly <ref type=""bibr"" target=""#b13"">[14]</ref>.</p><p>The data forwarding needed for inter-layer pipelini Recent work has already established the need for hybrid parallelization for intra-layer processing <ref type=""bibr"" target=""#b13"">[14]</ref>. With inter-layer pipelining, we can either choose a deepe les <ref type=""bibr"" target=""#b21"">[22]</ref> and TETRIS extended it to hybrid partitioning schemes <ref type=""bibr"" target=""#b13"">[14]</ref>. They both suffered from the inefficiencies discussed in S",0
"o orders of magnitude improvements over CPUs and GPUs in terms of performance and energy efficiency <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target bytes. A larger SRAM buffer is shared by all PEs. Other NN accelerators use a similar architecture <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target perations and reduce memory footprints. Recent work either dynamically pruned zero and small values <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b32"">33]</ref>, or statically compr",0
"=""#b12"">[13]</ref><ref type=""bibr"" target=""#b13"">[14]</ref><ref type=""bibr"" target=""#b14"">[15]</ref><ref type=""bibr"" target=""#b15"">[16]</ref> try to improve the static injection framework. CriticalFau structions using symbolic execution, which enumerates all potential hardware errors.</p><p>The work <ref type=""bibr"" target=""#b15"">[16]</ref> presents a selective protection technique that allows user tion technique that allows users to selectively protect these SDC-prone data. The main idea of work <ref type=""bibr"" target=""#b15"">[16]</ref> is predicting the SDC proneness of a program's data firstl put program are selected by GA.</p><p>Step 3 strengthens the identified vulnerable blocks. The work <ref type=""bibr"" target=""#b15"">[16]</ref> introduces a prediction model named SDCAuto to predict the causing errors. Fig. <ref type=""figure"" target=""#fig_1"">2</ref> illustrates the diagram of the work <ref type=""bibr"" target=""#b15"">[16]</ref>. The work <ref type=""bibr"" target=""#b15"">[16]</ref> first _1"">2</ref> illustrates the diagram of the work <ref type=""bibr"" target=""#b15"">[16]</ref>. The work <ref type=""bibr"" target=""#b15"">[16]</ref> first compiles the source code into LLVM IR, and extracts sidered, since it always causes illegal opcode exception rather than SDC.</p><p>Finally, as in work <ref type=""bibr"" target=""#b15"">[16]</ref>, we assume that at most one fault occurs during a program' e our results with the work <ref type=""bibr"" target=""#b14"">[15]</ref> and SDCAuto presented in work <ref type=""bibr"" target=""#b15"">[16]</ref>. We use our approach to maximize SDC coverage under the us /1.0"" xml:id=""fig_1""><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Block diagram of the work<ref type=""bibr"" target=""#b15"">[16]</ref> </figDesc><graphic url=""image-2.png"" coords=""4,99.47,451.0",1
"ly, a lot of works <ref type=""bibr"" target=""#b9"">[10]</ref><ref type=""bibr"" target=""#b10"">[11]</ref><ref type=""bibr"" target=""#b11"">[12]</ref><ref type=""bibr"" target=""#b12"">[13]</ref><ref type=""bibr"" t reduced, the statistical fault injection (SFI) experiments are still time-consuming. SmartInjector <ref type=""bibr"" target=""#b11"">[12]</ref> proposes an intelligent fault injection framework to ident nly one representative fault is selected to implement fault injection for each group. SmartInjector <ref type=""bibr"" target=""#b11"">[12]</ref> proposes an intelligent fault injection framework to ident",1
">[21]</ref>, NAS parallel benchmarks <ref type=""bibr"" target=""#b21"">[22]</ref>, Stanford benchmarks <ref type=""bibr"" target=""#b22"">[23]</ref>, Parboil benchmarks <ref type=""bibr"" target=""#b23"">[24]</r >[21]</ref>, NAS parallel benchmarks <ref type=""bibr"" target=""#b21"">[22]</ref>, Stanford benchmarks <ref type=""bibr"" target=""#b22"">[23]</ref>, Parboil benchmarks <ref type=""bibr"" target=""#b23"">[24]</r",0
"#b21"">[22]</ref>, Stanford benchmarks <ref type=""bibr"" target=""#b22"">[23]</ref>, Parboil benchmarks <ref type=""bibr"" target=""#b23"">[24]</ref> and PARSEC benchmarks <ref type=""bibr"" target=""#b24"">[25]< #b21"">[22]</ref>, Stanford benchmarks <ref type=""bibr"" target=""#b22"">[23]</ref>, Parboil benchmarks <ref type=""bibr"" target=""#b23"">[24]</ref> and PARSEC benchmarks <ref type=""bibr"" target=""#b24"">[25]<",0
"l be integrated into the chips, so that soft errors of hardware will occur more and more frequently <ref type=""bibr"" target=""#b1"">[2]</ref>.Therefore, necessary protection measures should be adopted t",0
"rks tend to protect these SDC-prone instructions selectively <ref type=""bibr"" target=""#b4"">[5]</ref><ref type=""bibr"" target=""#b5"">[6]</ref><ref type=""bibr"" target=""#b6"">[7]</ref>.</p><p>It was recentl",0
"reduce performance overheads, recent works tend to protect these SDC-prone instructions selectively <ref type=""bibr"" target=""#b4"">[5]</ref><ref type=""bibr"" target=""#b5"">[6]</ref><ref type=""bibr"" targe",0
"b22"">[23]</ref>, Parboil benchmarks <ref type=""bibr"" target=""#b23"">[24]</ref> and PARSEC benchmarks <ref type=""bibr"" target=""#b24"">[25]</ref>. These benchmarks are divided into two groups randomly: tr b22"">[23]</ref>, Parboil benchmarks <ref type=""bibr"" target=""#b23"">[24]</ref> and PARSEC benchmarks <ref type=""bibr"" target=""#b24"">[25]</ref> mentioned in section 4.2.</p><p>We compile these benchmark",0
"s have no influence to the outputs of program. To simplify these programs, static-slicing technique <ref type=""bibr"" target=""#b25"">[26]</ref> are utilized to convert a program to a smaller and executa",0
"h are drawn from SPEC benchmarks <ref type=""bibr"" target=""#b20"">[21]</ref>, NAS parallel benchmarks <ref type=""bibr"" target=""#b21"">[22]</ref>, Stanford benchmarks <ref type=""bibr"" target=""#b22"">[23]</ are chosen form SPEC benchmarks <ref type=""bibr"" target=""#b20"">[21]</ref>, NAS parallel benchmarks <ref type=""bibr"" target=""#b21"">[22]</ref>, Stanford benchmarks <ref type=""bibr"" target=""#b22"">[23]</",0
"timated that up to 82% Americans take one or more drugs, and 29% take more than four drugs together <ref type=""bibr"" target=""#b2"">[3]</ref>. Consider all drugs have a small chance of side effects, tak DDAAs from MedHelp.org <ref type=""bibr"" target=""#b22"">[23]</ref>, a popular online health community <ref type=""bibr"" target=""#b2"">[3]</ref>. They first built a heterogeneous healthcare network compris elp online. A recent survey shows that 72% of Internet users went online to seek health information <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b20"">21]</ref>. Several attempts ha",1
"are preferentially selected as negative samples. Following that, PCA (principal component analysis) <ref type=""bibr"" target=""#b24"">[25]</ref> is performed on the raw drug-pair vectors to reduce the di",0
"=""#b11"">[12]</ref><ref type=""bibr"" target=""#b11"">[12]</ref><ref type=""bibr"" target=""#b12"">[13]</ref><ref type=""bibr"" target=""#b13"">[14]</ref><ref type=""bibr"" target=""#b14"">[15]</ref><ref type=""bibr"" t",0
"=""#b10"">[11]</ref><ref type=""bibr"" target=""#b11"">[12]</ref><ref type=""bibr"" target=""#b11"">[12]</ref><ref type=""bibr"" target=""#b12"">[13]</ref><ref type=""bibr"" target=""#b13"">[14]</ref><ref type=""bibr"" t",0
"and in vitro assays are employed to test new drugs with existing drugs to identify potential risks <ref type=""bibr"" target=""#b7"">[8]</ref>. However, it is experimentally infeasible to test every poss drug and all existing drugs; let alone, some DDIs manifest only after multiple periods of exposure <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b8"">9]</ref>. Therefore, post-marke",0
"graph).</p><p>Our method also connects to PinSage <ref type=""bibr"" target=""#b20"">[21]</ref> and GAT <ref type=""bibr"" target=""#b14"">[15]</ref>. But note that both PinSage and GAT are designed for homog ula><p>(5) 2 The knowledge graph G is treated undirected. 3 Technically, S(v) • Neighbor aggregator <ref type=""bibr"" target=""#b14"">[15]</ref> directly takes the neighborhood representation of entity v",1
"hod for a special type of graphs (i.e., knowledge graph).</p><p>Our method also connects to PinSage <ref type=""bibr"" target=""#b20"">[21]</ref> and GAT <ref type=""bibr"" target=""#b14"">[15]</ref>. But not",1
"ling rigorous semantic relatedness (e.g., TransE <ref type=""bibr"" target=""#b0"">[1]</ref> and TransR <ref type=""bibr"" target=""#b11"">[12]</ref> assume head + relation = tail), which are more suitable fo",0
"e=""bibr"" target=""#b16"">17]</ref>.</p><p>A few recent studies <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" tar pared with KG-free methods, incorporating KG into recommendation benefits the results in three ways <ref type=""bibr"" target=""#b17"">[18]</ref>: <ref type=""bibr"" target=""#b0"">(1)</ref> The rich semantic n manually designed meta-paths or meta-graphs, which are hardly to be optimal in reality. RippleNet <ref type=""bibr"" target=""#b17"">[18]</ref> is a memory-network-like model that propagates users' pote ecommendation. We implement CKE as CF plus a structural knowledge module in this paper. • RippleNet <ref type=""bibr"" target=""#b17"">[18]</ref> is a memory-network-like approach that propagates users' p more natural and intuitive way is to design a graph algorithm directly to exploit the KG structure <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" ta",0
"among items in a KG can help explore their latent connections and improve the precision of results; <ref type=""bibr"" target=""#b1"">(2)</ref> The various types of relations in a KG are helpful for exten ploring a non-uniform sampler (e.g., importance sampling) is an important direction of future work. <ref type=""bibr"" target=""#b1"">(2)</ref> This paper (and all literature) focuses on modeling item-end l methods represent graphs and perform convolution in the spectral space. For example, Bruna et al. <ref type=""bibr"" target=""#b1"">[2]</ref> define the convolution in Fourier domain and calculates the",0
"e=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b16"">17]</ref>.</p><p>A few recent studies <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" targ arget=""#b19"">[20]</ref>, which map entities and relations to low-dimensional representation vectors <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" targ",0
"A few recent studies <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b21"">[22]</ref><ref type=""bibr"" ta ties and relations to low-dimensional representation vectors <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b22"">23]</ref>. However, commonly-",0
"n in Fourier domain and calculates the eigendecomposition of the graph Laplacian, Defferrard et al. <ref type=""bibr"" target=""#b3"">[4]</ref> approximate the convolutional filters by Chebyshev expansion",0
"r the sparsity and improve the performance of recommendation <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b16"">17]</ref>.</p><p>A few recent studies <ref type=""bibr"" target=""#b8"">[",0
"ecific operation such as inner product <ref type=""bibr"" target=""#b15"">[16]</ref> or neural networks <ref type=""bibr"" target=""#b7"">[8]</ref>. However, CF-based methods usually suffer from sparsity of u e> 			<note xmlns=""http://www.tei-c.org/ns/1.0"" place=""foot"" n=""8"" xml:id=""foot_4"">We have tried NCF<ref type=""bibr"" target=""#b7"">[8]</ref>, i.e., replacing inner product with neural networks, but the",0
"es with few contributions in a prediction. Inspired by the theory of hierarchical abstract machines <ref type=""bibr"" target=""#b15"">(Parr and Russell 1998)</ref>, we cast the task of profile reviser as",1
"fication <ref type=""bibr"" target=""#b29"">(Zhang, Huang, and Zhao 2018)</ref>, information extraction <ref type=""bibr"" target=""#b14"">(Narasimhan, Yala, and Barzilay 2016)</ref>, question answering <ref",0
"pe=""bibr"" target=""#b12"">(Koren, Bell, and Volinsky 2009)</ref>, bayesian personalized ranking (BPR) <ref type=""bibr"" target=""#b17"">(Rendle et al. 2009</ref>) and factorization machine (FM) <ref type="" r"" target=""#b8"">(He et al. 2018</ref>). Baseline Methods. The comparison methods include:</p><p>BPR <ref type=""bibr"" target=""#b17"">(Rendle et al. 2009</ref>): optimizes a pairwise ranking loss for the forms, such as learning behavior analysis <ref type=""bibr"" target=""#b0"">(Anderson et al. 2014;</ref><ref type=""bibr"" target=""#b17"">Qiu et al. 2016;</ref><ref type=""bibr"" target=""#b16"">Qi et al. 2018</",0
"at decomposing complex tasks into multiple small tasks to reduce the complexity of decision making <ref type=""bibr"" target=""#b1"">(Barto and Mahadevan 2003)</ref>, where different HRLs such as option-",0
"rent items. Several researches are conducted on MOOCs platforms, such as learning behavior analysis <ref type=""bibr"" target=""#b0"">(Anderson et al. 2014;</ref><ref type=""bibr"" target=""#b17"">Qiu et al.",0
"in the framework of Bayesian filtering for a unified theory of adaptive gradient algorithms due to <ref type=""bibr"" target=""#b0"">Aitchison (2018)</ref>. After we posted a preliminary version of our c er for parameters we are more uncertain about and smaller for parameters we are more certain about. <ref type=""bibr"" target=""#b0"">Aitchison (2018)</ref> goes on to show that popular adaptive gradient han L 2 regularization that emerges through the straightforward application of Bayesian filtering."" <ref type=""bibr"" target=""#b0"">(Aitchison, 2018)</ref>. While full credit for this theory goes to Ait",1
"71.37%) and statistically significantly higher test set accuracy on ResNet (72.04% versus 61.34%). <ref type=""bibr"" target=""#b14"">Radford et al. (2018)</ref> employed AdamW to train Transformer <ref",0
"4%). <ref type=""bibr"" target=""#b14"">Radford et al. (2018)</ref> employed AdamW to train Transformer <ref type=""bibr"" target=""#b17"">(Vaswani et al., 2017)</ref> architectures to obtain new state-of-the",0
"bibr"" target=""#b5"">Gastaldi (2017)</ref>. We also perform experiments on the Im-ageNet32x32 dataset <ref type=""bibr"" target=""#b1"">(Chrabaszcz et al., 2017)</ref>, a downsampled version of the original =""formula"">15</ref>) and double-checked that this is not a coincidence on the ImageNet32x32 dataset <ref type=""bibr"" target=""#b1"">(Chrabaszcz et al., 2017)</ref>, a downsampled version of the original",0
"ref> have become a default method of choice for training feed-forward and recurrent neural networks <ref type=""bibr"" target=""#b21"">(Xu et al., 2015;</ref><ref type=""bibr"" target=""#b13"">Radford et al.,",0
"tain new state-of-the-art results on a wide range of benchmarks for natural language understanding. <ref type=""bibr"" target=""#b23"">Zhang et al. (2018)</ref> compared L 2 regularization vs. weight deca",0
"ssfully adopted to lead to new state-of-the-art results for popular image classification benchmarks <ref type=""bibr"" target=""#b7"">(Huang et al., 2017;</ref><ref type=""bibr"" target=""#b5"">Gastaldi, 2017",0
"t=""#b5"">(Gastaldi, 2017;</ref><ref type=""bibr"" target=""#b2"">Cubuk et al., 2018)</ref>. Furthermore, <ref type=""bibr"" target=""#b20"">Wilson et al. (2017)</ref> suggested that adaptive gradient methods d ype=""bibr"" target=""#b3"">Dinh et al., 2017)</ref> and inherent problems of adaptive gradient methods <ref type=""bibr"" target=""#b20"">(Wilson et al., 2017)</ref>. In this paper, we investigate whether it at adaptive gradient methods such as Adam might lead to worse generalization than SGD with momentum <ref type=""bibr"" target=""#b20"">(Wilson et al., 2017)</ref>, we identified and exposed the inequivale",0
"h node features and graph topological structural information to make predictions. Velickovic et al. <ref type=""bibr"" target=""#b28"">[27]</ref> adopt attention mechanism into graph learning, and propose ctions of the contents of a neighborhood or sequence. Therefore, they are adaptive to the contents. <ref type=""bibr"" target=""#b28"">[27]</ref> adapts an attention mechanism to graph learning and propos thus can help stabilize the process, compared with the previously used row normalization as in GAT <ref type=""bibr"" target=""#b28"">[27]</ref>:</p><formula xml:id=""formula_4"">E ijp = Êijp N j=1 Êijp<la ention based EGNN layer</head><p>We describe the attention based EGNN layer. The original GAT model <ref type=""bibr"" target=""#b28"">[27]</ref> is only able to handle one dimensional binary edge feature ula xml:id=""formula_9"">X l−1 i• , X l−1 j•</formula><p>and E ijp . In existing attention mechanisms <ref type=""bibr"" target=""#b28"">[27]</ref>, the attention coefficient depends on X i• and X j• only. yer. Indeed, the essential difference between GCN <ref type=""bibr"" target=""#b18"">[18]</ref> and GAT <ref type=""bibr"" target=""#b28"">[27]</ref> is whether we use the attention coefficients (i.e., matrix The three citation network datasets are also used in <ref type=""bibr"" target=""#b33"">[32]</ref> [18] <ref type=""bibr"" target=""#b28"">[27]</ref>. However, they all use a pre-processed version which disca class.</p><p>The baseline methods we used are GCN <ref type=""bibr"" target=""#b18"">[18]</ref> and GAT <ref type=""bibr"" target=""#b28"">[27]</ref>. To investigate the effectivenesses of each components, we",1
"hebyshev polynomials to the graph Laplacian, spatially localized filtering is obtained. Kipf et al. <ref type=""bibr"" target=""#b18"">[18]</ref> approximate the polynomials using a re-normalized first-or rmula_4"">E ijp = Êijp N j=1 Êijp<label>(5)</label></formula><p>or symmetric normalization as in GCN <ref type=""bibr"" target=""#b18"">[18]</ref>:</p><formula xml:id=""formula_5"">E ijp = Êijp N i=1 Êijp N e our EGNN(C) layer from the formula of EGNN(A) layer. Indeed, the essential difference between GCN <ref type=""bibr"" target=""#b18"">[18]</ref> and GAT <ref type=""bibr"" target=""#b28"">[27]</ref> is wheth nd 20% sized subsets, which is called ""dense"" splitting.</p><p>Following the experiment settings of <ref type=""bibr"" target=""#b18"">[18]</ref>[27], we use two layers of EGNN in all of our experiments f nd are penalized more in the loss than a majority class.</p><p>The baseline methods we used are GCN <ref type=""bibr"" target=""#b18"">[18]</ref> and GAT <ref type=""bibr"" target=""#b28"">[27]</ref>. To inve",1
"ensive; secondly, filtering in the Fourier domain may result in non-spatially localized effects. In <ref type=""bibr"" target=""#b13"">[13]</ref>, a parameterization of the Fourier filter with smooth coef",0
". One kind of embedding approaches are based on matrix-factorization, e.g., Laplacian Eigenmap (LE) <ref type=""bibr"" target=""#b4"">[4]</ref>, Graph Factorization (GF) algorithm <ref type=""bibr"" target=",0
"</head><p>For all the experiments, we implement the algorithms in Python on the TensorFlow platform <ref type=""bibr"" target=""#b0"">[1]</ref>. In all the experiments, models are trained with a Nvidia Te",0
"idden layers.</p><p>Our models are compared with two baseline models which are shown in MoleculeNet <ref type=""bibr"" target=""#b32"">[31]</ref>: Random Forest and Weave. Random Forest is a traditional l",0
"m <ref type=""bibr"" target=""#b1"">[2]</ref>, GraRep <ref type=""bibr"" target=""#b7"">[7]</ref>, and HOPE <ref type=""bibr"" target=""#b21"">[21]</ref>. Another class of approaches focus on employing a flexible",0
"c <ref type=""bibr"" target=""#b1"">[2]</ref>, LINE <ref type=""bibr"" target=""#b27"">[26]</ref>, and HARP <ref type=""bibr"" target=""#b9"">[9]</ref>. There are several limitations in matrix factorization-based",0
"erial images and LiDAR data. The developed network is based on a modified residual learning network <ref type=""bibr"" target=""#b13"">(He et al., 2016)</ref> that extracts robust low/mid/high-level featu res in the input images <ref type=""bibr"" target=""#b50"">(Zhang et al., 2016)</ref>. Previous studies <ref type=""bibr"" target=""#b13"">(He et al., 2016)</ref> have found that increasing the depth of neura e=""figure"" target=""#fig_2"">1</ref>).</p><p>A more detailed description of ResNet-50 can be found in <ref type=""bibr"" target=""#b13"">(He et al., 2016)</ref> and here ResNet-50 is modified as follows to cy may degrade after a saturation. This phenomenon is often referred to as the degradation problem. <ref type=""bibr"" target=""#b13"">He et al. (2016)</ref> recently proposed a Residual Network (ResNet)",1
"CNNs and FCNs have been widely used in urban objects extraction such as buildings, roads, and trees <ref type=""bibr"" target=""#b5"">(Cheng et al., 2017;</ref><ref type=""bibr"" target=""#b19"">Kaiser et al.",0
"oth hand-crafted features and the guided filtering technique to improve the classification results. <ref type=""bibr"" target=""#b42"">Wu et al. (2018)</ref> proposed an end-to-end segmentation network th",0
"ntation of remotely sensed data <ref type=""bibr"" target=""#b33"">(Paisitkriangkrai et al., 2016;</ref><ref type=""bibr"" target=""#b36"">Saito and Aoki, 2015)</ref>, in which each pixel is labeled with the",0
"traction methods <ref type=""bibr"" target=""#b21"">(Lee et al., 2008)</ref> mainly include image-based <ref type=""bibr"" target=""#b9"">(Ghanea et al., 2016;</ref><ref type=""bibr"" target=""#b15"">Huang and Zh on to distinguish buildings from non-building objects (e.g., roads, water, shadows, and vegetation) <ref type=""bibr"" target=""#b9"">(Ghanea et al., 2016)</ref>. The fastdeveloping technology of airborne",0
"pe=""bibr"" target=""#b0"">[1]</ref>, Giffin, et al., <ref type=""bibr"" target=""#b22"">[23]</ref>, Spivey <ref type=""bibr"" target=""#b23"">[24]</ref>, Bond and McKinley <ref type=""bibr"" target=""#b24"">[25]</re",1
"target=""#b7"">8,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b11"">12]</ref>. In general, the methods of software behavior were two patt",0
"type=""bibr"" target=""#b16"">17]</ref>, software behavior model <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b17"">18]</ref>, and finite state automaton model <ref type=""bibr"" target=""",0
"e subverting the intended data flow in the program, subverting machine-code execution. For example, <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target pe=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b16"">17]</ref>, software behavior model <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b17"">18]</ref>, and finite state au",0
"ticability. The research on system call is indispensable. Remarkable works include Forrest, et al., <ref type=""bibr"" target=""#b0"">[1]</ref>, Giffin, et al., <ref type=""bibr"" target=""#b22"">[23]</ref>,",0
"other approaches utilized LSTM <ref type=""bibr"" target=""#b0"">(Almahairi et al., 2015)</ref> or RNN <ref type=""bibr"" target=""#b1"">(Bansal et al., 2016)</ref>. To the best of authors' knowledge, the ma",0
"ollection of terms and is combined with MF to predict ratings. Ratings Meets Reviews (RMR) model by <ref type=""bibr"" target=""#b25"">Ling et al., (2014)</ref>, a topic based model was also included in t s verified by <ref type=""bibr"" target=""#b41"">Seo et al., (2017)</ref>, and HFT and CTR confirmed by <ref type=""bibr"" target=""#b25"">Ling et al., (2014)</ref>.</p></div> <div xmlns=""http://www.tei-c.org",0
"that pulls representations of noisy samples away from clean ones. Finally, mixup data augmentation <ref type=""bibr"" target=""#b34"">(Zhang et al., 2018)</ref> has recently demonstrated outstanding robu ushing the state-of-the-art one step forward by combining our approach with mixup data augmentation <ref type=""bibr"" target=""#b34"">(Zhang et al., 2018)</ref>.</p><p>4. Guiding mixup data augmentation bootstrapping <ref type=""bibr"" target=""#b22"">(Reed et al., 2015)</ref> and mixup data augmentation <ref type=""bibr"" target=""#b34"">(Zhang et al., 2018)</ref> to deal with the closed-set label noise sc i-c.org/ns/1.0""><head n=""3.3."">Joint label correction and mixup data augmentation</head><p>Recently <ref type=""bibr"" target=""#b34"">(Zhang et al., 2018)</ref> proposed a data augmentation technique nam ref type=""bibr"" target=""#b9"">(Hendrycks et al., 2018)</ref>), and use the configuration reported in <ref type=""bibr"" target=""#b34"">(Zhang et al., 2018)</ref> for mixup. We outperform the related work tab_8"">6</ref> shows the results of the proposed approaches M-DYR-H and MD-DYR-SH compared to mixup <ref type=""bibr"" target=""#b34"">(Zhang et al., 2018)</ref> on TinyImageNet to demonstrate that our ap onstrate that our approach is useful far from CIFAR data. The proposed approach clearly outperforms <ref type=""bibr"" target=""#b34"">(Zhang et al., 2018)</ref> for different levels of label noise, obtai ref type=""bibr"" target=""#b11"">Jiang et al., 2018b;</ref><ref type=""bibr"">Patrini et al., 2017;</ref><ref type=""bibr"" target=""#b34"">Zhang et al., 2018)</ref> modify either the loss directly, or the pro",1
"p>Existing literature on training with noisy labels focuses primarily on loss correction approaches <ref type=""bibr"" target=""#b22"">(Reed et al., 2015;</ref><ref type=""bibr"" target=""#b9"">Hendrycks et a ling using the network predictions to predict hard or soft labels.</p><p>Loss correction approaches <ref type=""bibr"" target=""#b22"">(Reed et al., 2015;</ref><ref type=""bibr"" target=""#b11"">Jiang et al., pe=""bibr"" target=""#b11"">Jiang et al., 2018b)</ref>. A well-known approach is the bootstrapping loss <ref type=""bibr"" target=""#b22"">(Reed et al., 2015)</ref>, which introduces a perceptual consistency ilities used to compute it, to compensate for the incorrect guidance provided by the noisy samples. <ref type=""bibr"" target=""#b22"">(Reed et al., 2015)</ref> extend the loss with a perceptual term that is unsupervised model to implement a loss correction approach that benefits both from bootstrapping <ref type=""bibr"" target=""#b22"">(Reed et al., 2015)</ref> and mixup data augmentation <ref type=""bibr ibr"" target=""#b33"">(Zhang et al., 2017)</ref>.</p><p>The static hard bootstrapping loss proposed in <ref type=""bibr"" target=""#b22"">(Reed et al., 2015)</ref> provides a mechanism to deal with label noi ) ,<label>(10)</label></formula><p>where w i weights the model prediction z i in the loss function. <ref type=""bibr"" target=""#b22"">(Reed et al., 2015)</ref> use w i = 0.2, ∀i. We refer to this approac Reed et al., 2015)</ref> use w i = 0.2, ∀i. We refer to this approach as static hard bootstrapping. <ref type=""bibr"" target=""#b22"">(Reed et al., 2015)</ref> also proposed a static soft bootstrapping l beling. We also run our proposed approach under these conditions in Subsection 4.5 for comparison.  <ref type=""bibr"" target=""#b22"">(Reed et al., 2015)</ref>. The overall results demonstrate that apply sed dynamic hard bootstrapping exhibits better performance than the state-of-the-art static version <ref type=""bibr"" target=""#b22"">(Reed et al., 2015)</ref>. It is, however, not better than the perfor d the 300 epochs training scheme (see Subsection 4.1) . We introduce bootstrapping in epoch 105 for <ref type=""bibr"" target=""#b22"">(Reed et al., 2015)</ref> for the proposed methods, estimate the T ma",1
"seful for learning visual representations <ref type=""bibr"" target=""#b18"">(Pathak et al., 2017;</ref><ref type=""bibr"" target=""#b6"">Gidaris et al., 2018)</ref>; however, a recent study on the generaliza",0
"he true label of a noisy sample x i may be outside S; i.e. x i may be an out-of-distribution sample <ref type=""bibr"" target=""#b13"">(Liang et al., 2018)</ref>. The remainder of this section briefly rev",0
"ht for each sample) to guide a student network that learns under label noise conditions. Similarly, <ref type=""bibr"" target=""#b7"">(Guo et al., 2018)</ref> present a curriculum learning approach based",0
"br"" target=""#b1"">Beluch et al., 2018;</ref><ref type=""bibr"" target=""#b21"">Redmon et al., 2016;</ref><ref type=""bibr"" target=""#b35"">Zhao et al., 2017;</ref><ref type=""bibr"" target=""#b12"">Krishna et al.",0
"s in biases that networks encounter during training, e.g., when a dataset contains class imbalances <ref type=""bibr"" target=""#b0"">(Alvi et al., 2018)</ref>. However, before fitting label noise, CNNs f",0
"l., 2017)</ref>. Their widespread use is attributable to their capability to model complex patterns <ref type=""bibr"" target=""#b23"">(Ren et al., 2018)</ref> when vast amounts of labeled data are availa its distribution in a feature space that benefits from training with both clean and noisy samples. <ref type=""bibr"" target=""#b23"">(Ren et al., 2018)</ref> weights each sample in the loss based on the ets of clean data during training in <ref type=""bibr"" target=""#b11"">(Jiang et al., 2018b)</ref> and <ref type=""bibr"" target=""#b23"">(Ren et al., 2018)</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ to formulate a robust learning procedure <ref type=""bibr"" target=""#b11"">(Jiang et al., 2018b;</ref><ref type=""bibr"" target=""#b23"">Ren et al., 2018)</ref>. Curriculum learning <ref type=""bibr"" target= ontrary to most successful recent approaches that assume the existence of a known set of clean data <ref type=""bibr"" target=""#b23"">(Ren et al., 2018;</ref><ref type=""bibr"" target=""#b9"">Hendrycks et al bibr"" target=""#b11"">(Jiang et al., 2018b;</ref><ref type=""bibr"" target=""#b14"">Ma et al., 2018;</ref><ref type=""bibr"" target=""#b23"">Ren et al., 2018;</ref><ref type=""bibr"">Wang et al., 2018b)</ref>. We",0
"obtained noisy labels have previously been demonstrated useful for learning visual representations <ref type=""bibr"" target=""#b18"">(Pathak et al., 2017;</ref><ref type=""bibr"" target=""#b6"">Gidaris et a",0
"unsupervised modeling technique <ref type=""bibr"" target=""#b24"">(Stauffer &amp; Grimson, 1999;</ref><ref type=""bibr"" target=""#b20"">Permuter et al., 2006;</ref><ref type=""bibr"" target=""#b15"">Ma &amp; L ><ref type=""bibr"" target=""#b15"">Ma &amp; Leijon, 2011)</ref>, with the Gaussian Mixture Model (GMM) <ref type=""bibr"" target=""#b20"">(Permuter et al., 2006)</ref> being the most popular. The probability",0
"get=""#b60"">[61,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b61"">62,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b52"">53,</ref><ref type=""bibr"" targe ting <ref type=""bibr"" target=""#b60"">[61,</ref><ref type=""bibr"" target=""#b9"">10]</ref>, sketch2image <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b42"">43]</ref>, and other image-to-",1
"rning has been demonstrated by recent GAN-based methods, in many different image manipulation tasks <ref type=""bibr"" target=""#b60"">[61,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" tar #b41"">42,</ref><ref type=""bibr"" target=""#b52"">53]</ref>. Examples include interactive image editing <ref type=""bibr"" target=""#b60"">[61,</ref><ref type=""bibr"" target=""#b9"">10]</ref>, sketch2image <ref",0
"get=""#b28"">29,</ref><ref type=""bibr"" target=""#b51"">52,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b23"">24]</ref>), we are the first explore it for internal learning from a",0
"volving randomness <ref type=""bibr"" target=""#b61"">[62,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" target=""#b62"">63]</ref>. The role of the convonlutional layers is to generate the m",0
"arget=""#b61"">62,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b52"">53,</ref><ref type=""bibr"" target=""#b55"">56,</ref><ref type=""bibr"" target=""#b41"">42,</ref><ref type=""bibr"" tar",0
"ad coverage of the protein universe, as found in the 17929 families of the recent Pfam 32.0 release <ref type=""bibr"" target=""#b10"">[11]</ref>. Recent work that applies deep learning is either restrict database is carefully curated, at least 25% of sequences have no experimentally validation function <ref type=""bibr"" target=""#b10"">[11]</ref>, and additional experimental functional characterization o of the art models including profile HMMs we use the highly curated Protein families (Pfam) database <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b22"">23]</ref>. The 17929 familie otKB have at least one Pfam family annotation, including 74.5% of proteins from reference proteomes <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b16"">17]</ref>. Many domains have",1
"ls that annotate open reading frames with function will play a central role in exploiting this data <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref>. Identifying proteins",0
"arget=""#b7"">[8]</ref>. Broad protein families require muliple HMM profiles to model their diversity <ref type=""bibr"" target=""#b8"">[9]</ref>, while more than 22% of the highly-curated families in Pfam",0
"es <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b12"">13]</ref>, while DeepFam <ref type=""bibr"" target=""#b13"">[14]</ref> considers 2892 COG families of more than 100 sequences eac from averaging.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>Discussion</head><p>Both  <ref type=""bibr"" target=""#b13"">[14]</ref>. Our ProtENN trained across Pfam full yields a three-fold",0
"hes that enjoy widespread use. For example, DeepSF classifies sequences into 1195 SCOP fold classes <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b12"">13]</ref>, while DeepFam <re",0
""" target=""#b0"">[1]</ref>, images <ref type=""bibr"" target=""#b4"">[5]</ref>, and even acoustic signals <ref type=""bibr"" target=""#b5"">[6]</ref> with impressive results. The Transformer model using self-at and the Transformer have been exploratorily applied to ASR, but so far with unsatisfactory results. <ref type=""bibr"" target=""#b5"">[6]</ref> found that self-attention in the encoder (acoustic model) wa tworks.</p><p>To adapt the encoder to long speech utterances, we follow the reshaping practice from <ref type=""bibr"" target=""#b5"">[6]</ref> by grouping consecutive frames into one step. Subsequently w c features to the positional encoding is harmful, potentially leading to divergence during training <ref type=""bibr"" target=""#b5"">[6]</ref>, we resolved that problem by simply projecting the concatena of using self-attention as the main component of ASR models has been investigated in various forms. <ref type=""bibr"" target=""#b5"">[6]</ref> combines self-attention with LSTMs, while <ref type=""bibr"" t rning rate over the training progress: lr = init lr * d −0.5 * min(step −0.5 , step * warmup −1.5 ) <ref type=""bibr"" target=""#b5"">(6)</ref> in which the init lr is set to 2, and we warm up the learnin",1
"fective, but combined with an LSTM brought marginal improvement and greater interpretability, while <ref type=""bibr"" target=""#b8"">[9]</ref> did not find any notable improvement using the Transformer i e Transformer has been applied to ASR with additional TDNN layers to downsample the acoustic signal <ref type=""bibr"" target=""#b8"">[9]</ref>. Though self-attention has provided various benefits such as",1
"stic depth for the Transformer inspired by the Stochastic Residual Network for image classification <ref type=""bibr"" target=""#b9"">[10]</ref>.</p><p>We discovered that its ability to regularize is the r"" target=""#b15"">[16]</ref>, and thus there are redundant layers. Motivated by the previous work of <ref type=""bibr"" target=""#b9"">[10]</ref>, we propose to apply stochastic residual layers into our Tr sub-layers inside). This way we have one hyper-parameter p for each layer.</p><p>• As suggested by <ref type=""bibr"" target=""#b9"">[10]</ref>, the lower layers of the networks handle raw-level acoustic",1
"in various forms. <ref type=""bibr"" target=""#b5"">[6]</ref> combines self-attention with LSTMs, while <ref type=""bibr"" target=""#b29"">[30]</ref> uses self-attention as an alternative in CTC models. A var",0
"e the filter-bank features, we did not employ any auxiliary features. We followed the approach from <ref type=""bibr"" target=""#b17"">[18]</ref> to generate a speech perturbation training set.</p><p>Extr",0
"ive decoder which attentively generates the output sequences <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref>.</p><p>In this context, we aim at reconsidering acoustic mode",0
"cally, the positional encoding offers a clear advantage compared to learnable positional embeddings <ref type=""bibr"" target=""#b11"">[12]</ref>, because the speech signals can be arbitrarily long with a",0
"apply character dropout <ref type=""bibr"" target=""#b20"">[21]</ref> with p = 0.1 and label smoothing <ref type=""bibr"" target=""#b21"">[22]</ref> with = 0.1. The experiment results on SWB testsets are sho",0
"memory neural networks (LSTM) <ref type=""bibr"" target=""#b2"">[3]</ref> or time-delay neural networks <ref type=""bibr"" target=""#b3"">[4]</ref> operating on top of frame-level features to learn sequence-l attention performs competitively compared to LSTMs <ref type=""bibr"" target=""#b2"">[3]</ref> or TDNNs <ref type=""bibr"" target=""#b3"">[4]</ref>. The former benefits strongly from building deep residual ne",0
"re residual connection and on the attention weights) is set at 0.2. We also apply character dropout <ref type=""bibr"" target=""#b20"">[21]</ref> with p = 0.1 and label smoothing <ref type=""bibr"" target=""",0
"can be reasonably trained with many layers leading to state-of-the-art performance in various tasks <ref type=""bibr"" target=""#b7"">[8]</ref>. Self-attention and the Transformer have been exploratorily reason for the performance breakthrough in recent works in both MT and natural language processing <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b13"">14]</ref>.</p></div> <div xmln",0
"actor from a set of queries Q, keys K and values K. The retrieval function is based on similarities <ref type=""bibr"" target=""#b10"">[11]</ref> between the queries and the keys, and in turn returns the",0
"ly increases the magnitude of the neuron values which is then alleviated by the layer-normalization <ref type=""bibr"" target=""#b12"">[13]</ref> layers placed after each residual connection.</p><p>The de self-attention, feedforward layers or even decoder-encoder attention. The layer normalization as in <ref type=""bibr"" target=""#b12"">[13]</ref> keeps the magnitude of the hidden layers from growing larg",0
";</ref><ref type=""bibr"">Kitaev et al., 2019, i.a.)</ref>.</p><p>In this context, the GLUE benchmark <ref type=""bibr"" target=""#b62"">(Wang et al., 2019a)</ref> has become a prominent evaluation framewor s, we use an average of the metrics. More information on the tasks included in GLUE can be found in <ref type=""bibr"" target=""#b62"">Wang et al. (2019a)</ref> and in <ref type=""bibr"">Warstadt et al. (20",1
"use the procedure specified in<ref type=""bibr"" target=""#b16"">Devlin et al. (2019)</ref>: We use Adam<ref type=""bibr"" target=""#b27"">(Kingma and Ba, 2014</ref></figDesc><table><row><cell>Model</cell><ce",0
"ntified the presence and amplification of many social biases in data-driven machine learning models <ref type=""bibr"" target=""#b38"">(Lu et al., 2018;</ref><ref type=""bibr"">Zhao et al., 2018, i.a.)</ref",0
"get span representations of the pronoun and noun phrase via a self-attention span-pooling operator <ref type=""bibr"" target=""#b31"">(Lee et al., 2017)</ref>, before feeding it into a logistic regressio",0
"ng ability of SNN. Several research efforts have explored STDP algorithms to enable learning in SNN <ref type=""bibr"" target=""#b2"">[3]</ref> [4] <ref type=""bibr"" target=""#b4"">[5]</ref>. Simulations of im shows comparable accuracy with the stateof-the-art SNN design with deterministic STDP from Diehl <ref type=""bibr"" target=""#b2"">[3]</ref>. In Diehl's work, the network yields an accuracy of 91.9% fo",1
"rated STDP learning have been presented such as Brain <ref type=""bibr"" target=""#b7"">[8]</ref>, NEST <ref type=""bibr"" target=""#b8"">[9]</ref> and CARLsim <ref type=""bibr"" target=""#b9"">[10]</ref>. In a r",0
". The STDP is a phenomenon observed in biology experiments <ref type=""bibr"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b1"">[2]</ref>, which can be used as the synapse model of SNN. With STDP, s",0
"t developments focused on achieving SNN simulation with higher accuracy and better simulation speed <ref type=""bibr"" target=""#b6"">[7]</ref>. In particular, advancements of Graphics Processing Units (G",0
"have explored STDP algorithms to enable learning in SNN <ref type=""bibr"" target=""#b2"">[3]</ref> [4] <ref type=""bibr"" target=""#b4"">[5]</ref>. Simulations of SNN has received significant attention in re",0
"conductances of memristors to exact values and thus the resistances are usually programmed instead <ref type=""bibr"" target=""#b12"">[13]</ref>, <ref type=""bibr"" target=""#b13"">[14]</ref>. After software",1
"mplementing these computations. Although general-purpose and specific hardware platforms, e.g., GPU <ref type=""bibr"" target=""#b0"">[1]</ref> and FPGA <ref type=""bibr"" target=""#b1"">[2]</ref>, have been",0
"bibr"" target=""#b19"">[20]</ref> were applied onto two different image datasets, Cifar10 and Cifar100 <ref type=""bibr"" target=""#b20"">[21]</ref>, respectively. The information of the neural networks and",0
"purpose and specific hardware platforms, e.g., GPU <ref type=""bibr"" target=""#b0"">[1]</ref> and FPGA <ref type=""bibr"" target=""#b1"">[2]</ref>, have been adopted to implement neural networks, power consu",0
"zed into two groups. The first approach is online training <ref type=""bibr"" target=""#b5"">[6]</ref>, <ref type=""bibr"" target=""#b6"">[7]</ref>. In this approach, the conductances of the memristors are fi e the aged upper and lower bounds of the resistance ranges using ( <ref type=""formula"">6</ref>) and <ref type=""bibr"" target=""#b6"">(7)</ref>. Afterwards, the mapping of the weights onto memristors can",0
"e number of MAC operations by directly pruning the CNN model <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b7"">8]</ref>. Unimportant weights ar GA without further information disclosed on resource utilization and working frequency. The work of <ref type=""bibr"" target=""#b1"">[2]</ref> presented an algorithm-hardware codesign scheme to improve t",1
"pruning the CNN model <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b7"">8]</ref>. Unimportant weights are forced to zero during the training ( el evaluated was of low complexity (0.44 GOP) and the performance achieved was only 31.79 GOP/s. In <ref type=""bibr"" target=""#b7"">[8]</ref>, the authors reported an design framework which mapped spars",0
"200 MHz. (each DSP can perform two 16/8-bit fixedpoint MACs).</p><p>The second category of designs <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b9"">10]</ref> perform convolution i </ref><ref type=""bibr"" target=""#b12"">13]</ref> are based on spatial convolution, while the works of <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b9"">10]</ref> use frequency domain is the reduction rate in MAC operation. For instance, up to 69.2% of the MAC operation is saved in <ref type=""bibr"" target=""#b2"">[3]</ref>, resulting in a theoretical speedup of 3.3? in peak performa ]</ref>, resulting in a theoretical speedup of 3.3? in peak performance. The throughput achieved by <ref type=""bibr"" target=""#b2"">[3]</ref> on a Intel HARP platform is 669.1 GOP/s, which is very close -based ones. The performance of the reported SpConv-based FPGA accelerators have not exceed that of <ref type=""bibr"" target=""#b2"">[3]</ref>.</p><p>From an architecture view, existing FPGA accelerators l operations (accumulate and multiply) is saved compared to SDConv, while the reduction over FDConv <ref type=""bibr"" target=""#b2"">[3]</ref> and SpConv <ref type=""bibr"" target=""#b6"">[7]</ref> are 47.1% ><ref type=""bibr"" target=""#b9"">10]</ref> use frequency domain convolution.</p><p>The latest work of <ref type=""bibr"" target=""#b2"">[3]</ref> uses a frequency domain convolution scheme which gains 3.3? duction rate of 3.06?. The implemented accelerator achieves 1.55? speedup in throughput compared to <ref type=""bibr"" target=""#b2"">[3]</ref> as a result of being able to utilize 1.6? accumulators to ac r scheme quantizes the CNN model in 8-bit, the precision of the datapath is of the same (16-bit) as <ref type=""bibr"" target=""#b2"">[3]</ref>. For AlexNet, the pruning scheme adopted by us only reduces pruning scheme adopted by us only reduces the total MAC operations by 2.3? (30%  lower than that of <ref type=""bibr"" target=""#b2"">[3]</ref>), but our scheme still improves the inference throughput by",0
"e number of addition and multiplication required when performing ABM-SpConv on a pruned VGG16 model <ref type=""bibr"" target=""#b6"">[7]</ref>. 83.6% of the total operations (accumulate and multiply) is pared to SDConv, while the reduction over FDConv <ref type=""bibr"" target=""#b2"">[3]</ref> and SpConv <ref type=""bibr"" target=""#b6"">[7]</ref> are 47.1% and 50%, respectively.</p></div> <div xmlns=""http: type=""table"" target=""#tab_4"">3</ref>. Both models were pruned by the scheme proposed by Han et al. <ref type=""bibr"" target=""#b6"">[7]</ref> and quantized with 8-bit precision <ref type=""bibr"" target=""",0
"ef> gives an example of the dimensional parameters and the number of operations (#OP) for the VGG16 <ref type=""bibr"" target=""#b4"">[5]</ref> model that has been widely used as a performance benchmark i",0
"r"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" tar evel power analysis <ref type=""bibr"" target=""#b20"">[21]</ref>, or a per-control-step ML power model <ref type=""bibr"" target=""#b13"">[14]</ref> for power estimation.</p><p>Compared to behavioral-level a",1
"ed register mapping is shown in Figure <ref type=""figure"" target=""#fig_5"">5b</ref>. We use node2vec <ref type=""bibr"" target=""#b9"">[10]</ref> for node embedding, then apply PCA <ref type=""bibr"" target= "" target=""#b10"">[11]</ref>, metis <ref type=""bibr"" target=""#b12"">[13]</ref>, and a node2vec package <ref type=""bibr"" target=""#b9"">[10]</ref>. MLP and CNN models are implemented using Keras <ref type=""",0
"arget=""#b6"">7,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" tar , and rely on RTL power analysis <ref type=""bibr"" target=""#b2"">[3]</ref>, gate-level power analysis <ref type=""bibr"" target=""#b20"">[21]</ref>, or a per-control-step ML power model <ref type=""bibr"" tar",0
"ec package <ref type=""bibr"" target=""#b9"">[10]</ref>. MLP and CNN models are implemented using Keras <ref type=""bibr"" target=""#b1"">[2]</ref>. Other ML models are realized in scikit-learn <ref type=""bib",0
"l shift, we ne-tune a VGG16 model <ref type=""bibr"" target=""#b21"">[22]</ref> pre-trained on ImageNet <ref type=""bibr"" target=""#b8"">[9]</ref> for window-by-window estimation. Compared with the ShueNet V",0
"lso are sensitive to small, worst-case perturbations of the input, so-called ""adversarial examples"" <ref type=""bibr"" target=""#b32"">(Szegedy et al., 2014)</ref>. This latter phenomenon has struck many et al., 2004;</ref><ref type=""bibr"" target=""#b3"">Biggio &amp; Roli, 2018)</ref>. Since the work of <ref type=""bibr"" target=""#b32"">Szegedy et al. (2014)</ref>, a subfield has focused specifically on t ly on the phenomenon of small adversarial perturbations of the input, or ""adversarial examples."" In <ref type=""bibr"" target=""#b32"">Szegedy et al. (2014)</ref> it was proposed these adversarial example mproved robustness to small perturbations.</p><p>In the introduction we referred to a question from <ref type=""bibr"" target=""#b32"">Szegedy et al. (2014)</ref> about why we find errors so close to our lem for every point in the test set <ref type=""bibr"" target=""#b23"">(Katz et al., 2017)</ref>. Since <ref type=""bibr"" target=""#b32"">Szegedy et al. (2014)</ref>, hundreds of adversarial defense papers h",1
"g studies general ways in which an adversary may interact with an ML system, and dates back to 2004 <ref type=""bibr"" target=""#b8"">(Dalvi et al., 2004;</ref><ref type=""bibr"" target=""#b3"">Biggio &amp; R",0
""">(Guo et al., 2017)</ref>, JPEG compression <ref type=""bibr"" target=""#b19"">(Guo et al., 2017;</ref><ref type=""bibr"" target=""#b13"">Dziugaite et al., 2016;</ref><ref type=""bibr"" target=""#b25"">Liu et al 9"">(Guo et al., 2017)</ref>, JPEG compression<ref type=""bibr"" target=""#b19"">(Guo et al., 2017;</ref><ref type=""bibr"" target=""#b13"">Dziugaite et al., 2016;</ref><ref type=""bibr"" target=""#b25"">Liu et al",0
", 2017b;</ref><ref type=""bibr"">a;</ref><ref type=""bibr"" target=""#b34"">Vasiljevic et al., 2016;</ref><ref type=""bibr"" target=""#b39"">Zheng et al., 2016)</ref>. Our work suggests that adversarial defense",0
"stimates between-image labels using the clustering technique <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b25"">26]</ref> or kNN-based methods",1
"ity and its performance drops dramatically for similarity based tasks, e.g. nearest neighbor search <ref type=""bibr"" target=""#b45"">[46,</ref><ref type=""bibr"" target=""#b47"">48,</ref><ref type=""bibr"" ta s usually built on classifier weights <ref type=""bibr"" target=""#b7"">[8]</ref> or memorized features <ref type=""bibr"" target=""#b45"">[46]</ref>, which has limited efficiency and discriminability. We pro images and predefined noise signals, which constrains the distribution between raw data and noises <ref type=""bibr"" target=""#b45"">[46]</ref>. Bolztmann Machines (RBMs) <ref type=""bibr"" target=""#b23""> discriminability. Softmax Embedding with Memory Bank. To improve the inferior efficiency, Wu et al. <ref type=""bibr"" target=""#b45"">[46]</ref> propose to set up a memory bank to store the instance feat tance feature rather than classifier weights <ref type=""bibr"" target=""#b7"">[8]</ref> or memory bank <ref type=""bibr"" target=""#b45"">[46]</ref>. To achieve the goal that features of the same instance un =""bibr"" target=""#b2"">[3]</ref> 67.6 Exemplar <ref type=""bibr"" target=""#b7"">[8]</ref> 74.5 NPSoftmax <ref type=""bibr"" target=""#b45"">[46]</ref> 80.8 NCE <ref type=""bibr"" target=""#b45"">[46]</ref> 80. </p ype=""bibr"" target=""#b7"">[8]</ref> 74.5 NPSoftmax <ref type=""bibr"" target=""#b45"">[46]</ref> 80.8 NCE <ref type=""bibr"" target=""#b45"">[46]</ref> 80. </p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><h ead n=""4.1."">Experiments on Seen Testing Categories</head><p>We follow the experimental settings in <ref type=""bibr"" target=""#b45"">[46]</ref> to conduct the experiments on CIFAR-10 <ref type=""bibr"" ta ColorJitter, RandomHorizontalFlip) in PyTorch with default parameters are adopted.</p><p>Following <ref type=""bibr"" target=""#b45"">[46]</ref>, we adopt weighted kNN classifier to evaluate the performa 200) nearest neighbors based on cosine similarity, then apply weighted voting to predict its label <ref type=""bibr"" target=""#b45"">[46]</ref>   plar CNN <ref type=""bibr"" target=""#b7"">[8]</ref>, NPSoft type=""bibr"" target=""#b45"">[46]</ref>   plar CNN <ref type=""bibr"" target=""#b7"">[8]</ref>, NPSoftmax <ref type=""bibr"" target=""#b45"">[46]</ref>, NCE <ref type=""bibr"" target=""#b45"">[46]</ref> and Triplet N <ref type=""bibr"" target=""#b7"">[8]</ref>, NPSoftmax <ref type=""bibr"" target=""#b45"">[46]</ref>, NCE <ref type=""bibr"" target=""#b45"">[46]</ref> and Triplet loss with and without hard mining. Triplet (ha and the margin parameter is set to 0.5. DeepCluster <ref type=""bibr"" target=""#b2"">[3]</ref> and NCE <ref type=""bibr"" target=""#b45"">[46]</ref> represent the state-of-the-art unsupervised feature learni classifier weights for training, the proposed method outperforms it by 9.1%. Compared to NPSoftmax <ref type=""bibr"" target=""#b45"">[46]</ref> and NCE <ref type=""bibr"" target=""#b45"">[46]</ref>, which u hod outperforms it by 9.1%. Compared to NPSoftmax <ref type=""bibr"" target=""#b45"">[46]</ref> and NCE <ref type=""bibr"" target=""#b45"">[46]</ref>, which use memorized feature for optimizing, the proposed target=""#fig_4"">3</ref>. The proposed method takes only 2 epochs to get a kNN accuracy of 60% while <ref type=""bibr"" target=""#b45"">[46]</ref> takes 25 epochs and <ref type=""bibr"" target=""#b7"">[8]</ref ance features rather than classifier weights <ref type=""bibr"" target=""#b7"">[8]</ref> or memory bank <ref type=""bibr"" target=""#b45"">[46]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head d images in ten The classifier is used to predict the label of test samples. We implement NPSoftmax <ref type=""bibr"" target=""#b45"">[46]</ref>, NCE <ref type=""bibr"" target=""#b45"">[46]</ref> and DeepClu ct the label of test samples. We implement NPSoftmax <ref type=""bibr"" target=""#b45"">[46]</ref>, NCE <ref type=""bibr"" target=""#b45"">[46]</ref> and DeepCluster <ref type=""bibr"" target=""#b2"">[3]</ref> (c the best accuracy with both classifiers (kNN: 74.1%, Linear: 69.5%), which are much better than NCE <ref type=""bibr"" target=""#b45"">[46]</ref> and DeepCluster <ref type=""bibr"" target=""#b2"">[3]</ref> un three state-of-the-art unsupervised methods (Exemplar <ref type=""bibr"" target=""#b7"">[8]</ref>, NCE <ref type=""bibr"" target=""#b45"">[46]</ref> and DeepCluster <ref type=""bibr"" target=""#b2"">[3]</ref>) o n Table <ref type=""table"">3</ref>.</p><p>Generally, the instance-wise feature learning methods (NCE <ref type=""bibr"" target=""#b45"">[46]</ref>, Examplar <ref type=""bibr"" target=""#b7"">[8]</ref>, Ours) o",1
"e-based softmax embedding method. Existing softmax embedding is usually built on classifier weights <ref type=""bibr"" target=""#b7"">[8]</ref> or memorized features <ref type=""bibr"" target=""#b45"">[46]</r ."">Instance-wise Softmax Embedding</head><p>Softmax Embedding with Classifier Weights. Exemplar CNN <ref type=""bibr"" target=""#b7"">[8]</ref> treats each image as a distinct class. Following the convent bedding learning, which directly optimizes the real instance feature rather than classifier weights <ref type=""bibr"" target=""#b7"">[8]</ref> or memory bank <ref type=""bibr"" target=""#b45"">[46]</ref>. To arget=""#b2"">[3]</ref> 44.4 DeepCluster (1000) <ref type=""bibr"" target=""#b2"">[3]</ref> 67.6 Exemplar <ref type=""bibr"" target=""#b7"">[8]</ref> 74.5 NPSoftmax <ref type=""bibr"" target=""#b45"">[46]</ref> 80. hen apply weighted voting to predict its label <ref type=""bibr"" target=""#b45"">[46]</ref>   plar CNN <ref type=""bibr"" target=""#b7"">[8]</ref>, NPSoftmax <ref type=""bibr"" target=""#b45"">[46]</ref>, NCE <r ite sensitive to cluster numbers, which is unsuitable for different tasks. Compared to Exemplar CNN <ref type=""bibr"" target=""#b7"">[8]</ref> which uses the classifier weights for training, the proposed hs to get a kNN accuracy of 60% while <ref type=""bibr"" target=""#b45"">[46]</ref> takes 25 epochs and <ref type=""bibr"" target=""#b7"">[8]</ref> takes 45 epochs to reach the same accuracy. It is obvious th ficiency is guaranteed by directly optimization on instance features rather than classifier weights <ref type=""bibr"" target=""#b7"">[8]</ref> or memory bank <ref type=""bibr"" target=""#b45"">[46]</ref>.</p ef type=""bibr"" target=""#b1"">[2]</ref>, Satck <ref type=""bibr"" target=""#b52"">[54]</ref> and Exemplar <ref type=""bibr"" target=""#b7"">[8]</ref>) are also reported. Those results are taken from <ref type="" vised metric learning. We implement the other three state-of-the-art unsupervised methods (Exemplar <ref type=""bibr"" target=""#b7"">[8]</ref>, NCE <ref type=""bibr"" target=""#b45"">[46]</ref> and DeepClust the instance-wise feature learning methods (NCE <ref type=""bibr"" target=""#b45"">[46]</ref>, Examplar <ref type=""bibr"" target=""#b7"">[8]</ref>, Ours) outperform non-instancewise feature learning methods",1
"d ""intermediate"" feature representation from unlabelled data <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b30"">31,</ref><ref type=""bibr"" tar e clustering technique <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b25"">26]</ref> or kNN-based methods <ref type=""bibr"" target=""#b40"">[41]</r",0
">[43]</ref>, Stanford Online Product (Product) <ref type=""bibr"" target=""#b31"">[32]</ref> and Car196 <ref type=""bibr"" target=""#b21"">[22]</ref>   Implementation Details. We implement the proposed method",0
"<head n=""1."">Introduction</head><p>Deep embedding learning is a fundamental task in computer vision <ref type=""bibr"" target=""#b13"">[14]</ref>, which aims at learning a feature embedding that has the f",0
"iv> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.1.2"">STL-10 Dataset</head><p>STL-10 dataset <ref type=""bibr"" target=""#b3"">[4]</ref> is an image recognition dataset with colored images of size",0
"s) <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b39"">40]</ref>, Auto-encoders <ref type=""bibr"" target=""#b19"">[20,</ref><ref type=""bibr"" target=""#b41"">42]</ref> and generative adv",0
"rious vision tasks <ref type=""bibr"" target=""#b27"">[28,</ref><ref type=""bibr"" target=""#b29"">30,</ref><ref type=""bibr"" target=""#b51"">53]</ref>. However, annotated data needed for supervised methods migh",0
"escribed in <ref type=""bibr"" target=""#b31"">[32]</ref> to conduct experiments on CUB200-2011(CUB200) <ref type=""bibr"" target=""#b42"">[43]</ref>, Stanford Online Product (Product) <ref type=""bibr"" target",0
"er-class variation <ref type=""bibr"" target=""#b31"">[32,</ref><ref type=""bibr"" target=""#b36"">37,</ref><ref type=""bibr"" target=""#b44"">45,</ref><ref type=""bibr"" target=""#b46"">47]</ref>. Most of them are d",0
"<ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b25"">26]</ref> or kNN-based methods <ref type=""bibr"" target=""#b40"">[41]</ref>, which provide label information. Then label information a",0
"on between raw data and noises <ref type=""bibr"" target=""#b45"">[46]</ref>. Bolztmann Machines (RBMs) <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b39"">40]</ref>, Auto-encoders <re",0
"arget task to fine-tune models (e.g., linear classifier, object detector, etc.) for the target task <ref type=""bibr"" target=""#b2"">[3]</ref>. However, the learned feature representation may not preserv been widely studied in literature. Existing works can be roughly categorized into three categories <ref type=""bibr"" target=""#b2"">[3]</ref>: 1) generative models, this approach aims at learning a para ltiple augmented images for each instance in the batch. Methods kNN RandomCNN 32.1 DeepCluster (10) <ref type=""bibr"" target=""#b2"">[3]</ref> 44.4 DeepCluster (1000) <ref type=""bibr"" target=""#b2"">[3]</r kNN RandomCNN 32.1 DeepCluster (10) <ref type=""bibr"" target=""#b2"">[3]</ref> 44.4 DeepCluster (1000) <ref type=""bibr"" target=""#b2"">[3]</ref> 67.6 Exemplar <ref type=""bibr"" target=""#b7"">[8]</ref> 74.5 N ning <ref type=""bibr"" target=""#b15"">[16]</ref>, and the margin parameter is set to 0.5. DeepCluster <ref type=""bibr"" target=""#b2"">[3]</ref> and NCE <ref type=""bibr"" target=""#b45"">[46]</ref> represent tes that our proposed method achieves the best performance (83.6%) with kNN classifier. DeepCluster <ref type=""bibr"" target=""#b2"">[3]</ref> performs well in learning good ""intermediate"" features with type=""bibr"" target=""#b45"">[46]</ref>, NCE <ref type=""bibr"" target=""#b45"">[46]</ref> and DeepCluster <ref type=""bibr"" target=""#b2"">[3]</ref> (cluster number 100) under the same settings with their rele r: 69.5%), which are much better than NCE <ref type=""bibr"" target=""#b45"">[46]</ref> and DeepCluster <ref type=""bibr"" target=""#b2"">[3]</ref> under the same evaluation protocol. Note that kNN measures t f type=""bibr"" target=""#b7"">[8]</ref>, NCE <ref type=""bibr"" target=""#b45"">[46]</ref> and DeepCluster <ref type=""bibr"" target=""#b2"">[3]</ref>) on three datasets with their released code under the same s br"" target=""#b7"">[8]</ref>, Ours) outperform non-instancewise feature learning methods (DeepCluster <ref type=""bibr"" target=""#b2"">[3]</ref>, MOM <ref type=""bibr"" target=""#b20"">[21]</ref>), especially ting Between-image Labels, it usually estimates between-image labels using the clustering technique <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target",0
"on. In comparison, softmax embedding achieves competitive performance without sampling requirement <ref type=""bibr"" target=""#b17"">[18]</ref>. Supervised learning has achieved superior performance on",0
"arge in order to express the huge number of interest profiles at Tmall. Deep Interest Network (DIN) <ref type=""bibr"" target=""#b29"">[30]</ref> makes the user representation vary over different items wi "">[3]</ref>. Besides the industrial applications proposed by <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b29"">30]</ref>, various types of deep models have gained significant atten",1
"dation. Inspired by the success of deep learning in computer vision and natural language processing <ref type=""bibr"" target=""#b15"">[16]</ref>, much effort has been put for developing deep learning-bas",0
"=""#formula_14"">10</ref>) is computationally prohibitive. Thus, we use the sampled softmax technique <ref type=""bibr"" target=""#b17"">[18]</ref> to make the objective function trackable and choose the Ad",0
"mbedding vectors from word embedding vectors and applies them to recommending scholarly microblogs. <ref type=""bibr"" target=""#b1"">[2]</ref> proposes a novel convolutional neural network based model th",0
"hesizing it from scratch. It is most similar to recent work on sequence-tosequence voice conversion <ref type=""bibr"" target=""#b15"">[16]</ref><ref type=""bibr"" target=""#b16"">[17]</ref><ref type=""bibr"" t #b15"">[16]</ref><ref type=""bibr"" target=""#b16"">[17]</ref><ref type=""bibr"" target=""#b17"">[18]</ref>. <ref type=""bibr"" target=""#b15"">[16]</ref> uses a similar end-toend model, conditioned on speaker ide dard speech. In the future, we plan to test it on other speech disorders, and adopt techniques from <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b29"">30]</ref> to preserve the sp",1
"voice conversion <ref type=""bibr"" target=""#b15"">[16]</ref><ref type=""bibr"" target=""#b16"">[17]</ref><ref type=""bibr"" target=""#b17"">[18]</ref>. <ref type=""bibr"" target=""#b15"">[16]</ref> uses a similar it helpful to multitask train the model to predict source speech phonemes. Finally, in contrast to <ref type=""bibr"" target=""#b17"">[18]</ref>, we train the model without auxiliary alignment or auto-en",1
"ime by a total factor of 4, decreasing the computation in the following layers. Batch normalization <ref type=""bibr"" target=""#b24"">[25]</ref> is applied after each layer.</p><p>This downsampled sequen",0
"2]</ref><ref type=""bibr"" target=""#b12"">[13]</ref>. Recent work has also addressed accent conversion <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b14"">15]</ref> . In this paper we",0
"cord many hours of speech in clean acoustic environment, we use Google's Parallel WaveNet-based TTS <ref type=""bibr"" target=""#b30"">[31]</ref> system to generate training targets from a large hand-tran",0
"eech (TTS) synthesis <ref type=""bibr"" target=""#b3"">[4]</ref> and Automatic Speech Recognition (ASR) <ref type=""bibr"" target=""#b4"">[5]</ref>, using a single neural network that directly generates the t",0
"ackle a diverse set of tasks in speech and natural language processing, such as machine translation <ref type=""bibr"" target=""#b0"">[1]</ref>, speech recognition <ref type=""bibr"" target=""#b1"">[2]</ref>, model architecture described in Section 2, using a spectrogram decoder employing additive attention <ref type=""bibr"" target=""#b0"">[1]</ref> without an auxiliary ASR loss. Adding a parallel decoder to",0
"op) transcription service and ASR systems for multiple years helped improve their articulation. See <ref type=""bibr"" target=""#b32"">[33]</ref> for more details.</p><p>We experiment with adapting the be",0
"ure is based on recent attention-based end-to-end ASR models <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b21"">22]</ref> and TTS models such as Tacotron <ref type=""bibr"" target=""#b",0
"type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b8"">9]</ref>, dynamic frequency warping <ref type=""bibr"" target=""#b9"">[10]</ref>, and Gaussian mixture models <ref type=""bibr"" target=""#b10""",0
"kolov et al., 2013)</ref>, ELMo <ref type=""bibr"" target=""#b21"">(Peters et al., 2018)</ref> and BERT <ref type=""bibr"" target=""#b3"">(Devlin et al., 2019)</ref> are trained and tested mainly on datasets )</ref> uses machine translation to embed context information into word representations.</p><p>BERT <ref type=""bibr"" target=""#b3"">(Devlin et al., 2019)</ref> is a contextualized word representation mo r"" target=""#b10"">(Krallinger et al., 2017)</ref>. Due to the space limitations, we refer readers to <ref type=""bibr"" target=""#b3"">Devlin et al. (2019)</ref> for a more detailed description of BERT.</p pora were used for pre-training, we initialized BioBERT with the pre-trained BERT model provided by <ref type=""bibr"" target=""#b3"">Devlin et al. (2019)</ref>. We define BioBERT as a language representa han an hour as the size of the training data is much smaller than that of the training data used by <ref type=""bibr"" target=""#b3"">Devlin et al. (2019)</ref>. On the other hand, it takes more than 20 e",1
"e greatly improved performance in biomedical named entity recognition (NER) over the last few years <ref type=""bibr"" target=""#b6"">(Giorgi and Bader, 2018;</ref><ref type=""bibr"" target=""#b7"">Habibi et omedical corpus. While most previous works were built upon different combinations of LSTMs and CRFs <ref type=""bibr"" target=""#b6"">(Giorgi and Bader, 2018;</ref><ref type=""bibr"" target=""#b7"">Habibi et ref> for a fair evaluation; however, the splits of LINAAEUS and Species-800 could not be found from <ref type=""bibr"" target=""#b6"">Giorgi and Bader (2018)</ref> and may be different. Like previous work based on multiple Bi-LSTM CRF models with character level CNNs, while the state-of-the-art model by <ref type=""bibr"" target=""#b6"">Giorgi and Bader (2018)</ref> trained on the LINNAEUS dataset uses a B and (ii) BioBERT BASE and BioBERT LARGE trained on domain-specific vocabulary based on WordPiece.   <ref type=""bibr"" target=""#b6"">Giorgi and Bader (2018)</ref> on LINNAEUS and Species-800.  Notes: Pre previous state-of-the-art models and (ii) different training/ test set splits used in previous work <ref type=""bibr"" target=""#b6"">(Giorgi and Bader, 2018)</ref>, which were unavailable.</p><p>The RE r",0
"henotype-gene RE <ref type=""bibr"" target=""#b26"">(Sousa et al., 2019)</ref> and clinical temporal RE <ref type=""bibr"" target=""#b13"">(Lin et al., 2019)</ref>. The following updated versions of BioBERT w",0
"cognition (NER) over the last few years <ref type=""bibr"" target=""#b6"">(Giorgi and Bader, 2018;</ref><ref type=""bibr"" target=""#b7"">Habibi et al., 2017;</ref><ref type=""bibr"" target=""#b32"">Wang et al., ifferent combinations of LSTMs and CRFs <ref type=""bibr"" target=""#b6"">(Giorgi and Bader, 2018;</ref><ref type=""bibr"" target=""#b7"">Habibi et al., 2017;</ref><ref type=""bibr"" target=""#b32"">Wang et al., t, recent models in biomedical text mining rely largely on adapted versions of word representations <ref type=""bibr"" target=""#b7"">(Habibi et al., 2017;</ref><ref type=""bibr"" target=""#b22"">Pyysalo et a while previous works in biomedical NER often used word embeddings trained on PubMed or PMC corpora <ref type=""bibr"" target=""#b7"">(Habibi et al., 2017;</ref><ref type=""bibr"" target=""#b36"">Yoon et al.,",0
"r"" target=""#b0"">2,</ref><ref type=""bibr"" target=""#b1"">3,</ref><ref type=""bibr"" target=""#b2"">4,</ref><ref type=""bibr"" target=""#b3"">5]</ref>. This pipeline system usually suffers from time delay, parame nd generates target words from left to right at each step [1, <ref type=""bibr"" target=""#b1"">3,</ref><ref type=""bibr"" target=""#b3"">5]</ref>. This model has also achieved promising results in ASR fields chitecture for all three tasks (ASR, ST and MT). The model architecture is similar with Transformer <ref type=""bibr"" target=""#b3"">[5]</ref>, which is the state-of-art model in MT task. Recently, this",1
"results in ASR fields <ref type=""bibr"" target=""#b0"">[2,</ref><ref type=""bibr"" target=""#b2"">4,</ref><ref type=""bibr"" target=""#b15"">17]</ref>. Recent works purpose a further attempt to combine these tw",0
"age classification <ref type=""bibr"" target=""#b10"">[12,</ref><ref type=""bibr"" target=""#b16"">18,</ref><ref type=""bibr"" target=""#b17"">19,</ref><ref type=""bibr"" target=""#b18"">20]</ref>, speech recognition",0
"model which translates the transcripts to target language [1, <ref type=""bibr"" target=""#b0"">2,</ref><ref type=""bibr"" target=""#b1"">3,</ref><ref type=""bibr"" target=""#b2"">4,</ref><ref type=""bibr"" target= opts an encoder-decoder architecture and generates target words from left to right at each step [1, <ref type=""bibr"" target=""#b1"">3,</ref><ref type=""bibr"" target=""#b3"">5]</ref>. This model has also ac",0
"e behaviour of teacher model, such as output probabilities <ref type=""bibr"" target=""#b10"">[12,</ref><ref type=""bibr"" target=""#b11"">13]</ref>, hidden representation <ref type=""bibr"" target=""#b12"">[14,< /ref>, speech recognition <ref type=""bibr"" target=""#b10"">[12]</ref> and natural language processing <ref type=""bibr"" target=""#b11"">[13,</ref><ref type=""bibr"" target=""#b14"">16,</ref><ref type=""bibr"" ta",0
"s guaranteed to converge to a local optimum of it.</p><p>Inspired by the previous work on CycleGANs <ref type=""bibr"" target=""#b29"">(Zhu et al., 2017)</ref> and dual learning <ref type=""bibr"" target=""#",1
"it with an n-gram language model, and further improved the system through iterative backtranslation <ref type=""bibr"" target=""#b15"">(Lample et al., 2018b;</ref><ref type=""bibr"" target=""#b2"">Artetxe et as argued that the modular architecture of phrase-based SMT was more suitable for this problem, and <ref type=""bibr"" target=""#b15"">Lample et al. (2018b)</ref> and <ref type=""bibr"" target=""#b2"">Artetxe and NMT to build hybrid unsupervised machine translation systems. This idea was already explored by <ref type=""bibr"" target=""#b15"">Lample et al. (2018b)</ref>, who aided the training of their unsuperv , and applies MERT to tune the model in the reverse direction, iterating until convergence, whereas <ref type=""bibr"" target=""#b15"">Lample et al. (2018b)</ref> do not perform any tuning at all. In what (2018)</ref>, yet our absolute gain on top of it is around 2.5 BLEU points higher. When compared to <ref type=""bibr"" target=""#b15"">Lample et al. (2018b)</ref>, we obtain an absolute gain of 5-6 BLEU p",0
"by combining standard back-translation with synthetic parallel data generated by unsupervised SMT. <ref type=""bibr"" target=""#b17"">Marie and Fujita (2018)</ref> go further and use synthetic parallel d difficult to improve upon. This way, our initial SMT system is about 4-5 BLEU points above that of <ref type=""bibr"" target=""#b17"">Marie and Fujita (2018)</ref>, yet our absolute gain on top of it is",0
"em to a shared space through self-learning <ref type=""bibr"" target=""#b0"">(Artetxe et al., 2017</ref><ref type=""bibr"" target=""#b1"">(Artetxe et al., , 2018a) )</ref> or adversarial training <ref type=""b s-lingual space using VecMap<ref type=""foot"" target=""#foot_2"">3</ref> with identical initialization <ref type=""bibr"" target=""#b1"">(Artetxe et al., 2018a)</ref>, which builds an initial solution by ali",0
"two languages and learn a linear transformation to map them to a shared space through self-learning <ref type=""bibr"" target=""#b0"">(Artetxe et al., 2017</ref><ref type=""bibr"" target=""#b1"">(Artetxe et a",0
"onal Networks (RGCNs) have been proposed as an extension of GCNs to the domain of relational graphs <ref type=""bibr"" target=""#b28"">(Schlichtkrull et al., 2018)</ref>. This model has achieved impressiv he kernels was beneficial for generalisation, although it comes at the cost of increased model bias <ref type=""bibr"" target=""#b28"">(Schlichtkrull et al., 2018)</ref>. We follow this approach, decompos j, 2015)</ref>, RDF2Vec <ref type=""bibr"" target=""#b25"">(Ristoski and Paulheim, 2016)</ref> and RGCN <ref type=""bibr"" target=""#b28"">(Schlichtkrull et al., 2018)</ref>, and (mean and standard deviation </p><p>We note that RGCN consistently outperforms RGAT on MUTAG, contrary to what might be expected <ref type=""bibr"" target=""#b28"">(Schlichtkrull et al., 2018)</ref>. The result is surprising given th j, 2015)</ref>, RDF2Vec <ref type=""bibr"" target=""#b25"">(Ristoski and Paulheim, 2016)</ref> and RGCN <ref type=""bibr"" target=""#b28"">(Schlichtkrull et al., 2018)</ref>, and (mean and standard deviation target=""#b33"">Veli?kovi? et al. (2017)</ref>, extending to the relational setting, using ideas from <ref type=""bibr"" target=""#b28"">Schlichtkrull et al. (2018)</ref>.</p></div> <div xmlns=""http://www.t diate representations Different relations convey distinct pieces of information. The update rule of <ref type=""bibr"" target=""#b28"">Schlichtkrull et al. (2018)</ref> made this manifest by assigning eac uation ( <ref type=""formula"" target=""#formula_8"">7</ref>) with the neighborhood aggregation step of <ref type=""bibr"" target=""#b28"">Schlichtkrull et al. (2018)</ref> gives</p><formula xml:id=""formula_9 ><p>We evaluate the models on transductive and inductive tasks. Following the experimental setup of <ref type=""bibr"" target=""#b28"">Schlichtkrull et al. (2018)</ref> for the transductive tasks, we eval ly.</p><p>Transductive baselines We consider as a baseline the recent state-of-the-art results from <ref type=""bibr"" target=""#b28"">Schlichtkrull et al. (2018)</ref> obtained with a two-layer RGCN mode conclusions, we compare against our own implementation of RGCN rather than the results reported in <ref type=""bibr"" target=""#b28"">Schlichtkrull et al. (2018)</ref>; <ref type=""bibr"" target=""#b35"">Wu evaluate RGAT on MUTAG and AIFB. With additive attention, WIRGAT outperforms ARGAT, consistent with <ref type=""bibr"" target=""#b28"">Schlichtkrull et al. (2018)</ref>. Interestingly, when employing mult",1
"NNs) successfully solve a variety of tasks in Euclidean grid-like domains, such as image captioning <ref type=""bibr"" target=""#b8"">(Donahue et al., 2017)</ref> and classifying videos <ref type=""bibr"" t",0
"""#b30"">(Sperduti and Starita, 1997;</ref><ref type=""bibr"" target=""#b10"">Frasconi et al., 1997;</ref><ref type=""bibr"" target=""#b13"">Gori et al., 2005)</ref>. Incorporating gating mechanisms led to the",0
"nductive datasets, please see <ref type=""bibr"" target=""#b25"">Ristoski and Paulheim (2016)</ref> and <ref type=""bibr"" target=""#b35"">Wu et al. (2018)</ref> respectively.</p><p>Transductive baselines We elines As baselines for Tox21, we compare against the most competitive methods on Tox21 reported in <ref type=""bibr"" target=""#b35"">Wu et al. (2018)</ref>. Specifically, we compare against deep multita tworks <ref type=""bibr"" target=""#b24"">Ramsundar et al. (2015)</ref>, deep bypass multitask networks <ref type=""bibr"" target=""#b35"">Wu et al. (2018)</ref>, Weave <ref type=""bibr"" target=""#b16"">Kearnes rameters once, ensure no data leakage, but also provide comparable benchmarks to those presented in <ref type=""bibr"" target=""#b35"">Wu et al. (2018)</ref>, three benchmark splits were taken from the Mo ther than the results reported in <ref type=""bibr"" target=""#b28"">Schlichtkrull et al. (2018)</ref>; <ref type=""bibr"" target=""#b35"">Wu et al. (2018)</ref>.</p><p>We will occasionally employ a one-sided on for (C-)WIRGAT and (C-)ARGAT (this work). Test performance is reported on the splits provided in <ref type=""bibr"" target=""#b35"">Wu et al. (2018)</ref> </p></div> <div xmlns=""http://www.tei-c.org/ns ues, we obtained the relative mean performance of 0.72% for RGCN compared to the result reported in <ref type=""bibr"" target=""#b35"">Wu et al. (2018)</ref>, providing a much stronger baseline.</p><p>Bot we present the training, validation and test set performance of our models in addition to those in <ref type=""bibr"" target=""#b35"">Wu et al. (2018)</ref> in Table <ref type=""table"">3</ref>.</p><p>Tabl d ARGAT (this work). Training, validation and test performance is reported on the splits provided in<ref type=""bibr"" target=""#b35"">Wu et al. (2018)</ref>. Best performance in class in boldened, and be over 3 splits) for Multitask <ref type=""bibr"" target=""#b24"">(Ramsundar et al., 2015)</ref>, Bypass <ref type=""bibr"" target=""#b35"">(Wu et al., 2018)</ref>, Weave <ref type=""bibr"" target=""#b16"">(Kearne over 3 splits) for Multitask <ref type=""bibr"" target=""#b24"">(Ramsundar et al., 2015)</ref>, Bypass <ref type=""bibr"" target=""#b35"">(Wu et al., 2018)</ref>, Weave <ref type=""bibr"" target=""#b16"">(Kearne over 3 splits) for Multitask <ref type=""bibr"" target=""#b24"">(Ramsundar et al., 2015)</ref>, Bypass <ref type=""bibr"" target=""#b35"">(Wu et al., 2018)</ref>, Weave <ref type=""bibr"" target=""#b16"">(Kearne",0
"ntion Networks (GATs), applied attention mechanisms to graphs, and does not share these limitations <ref type=""bibr"" target=""#b33"">(Veli?kovi? et al., 2017;</ref><ref type=""bibr"" target=""#b12"">Gong an ad n=""2.1"">Relational graph attention layer</head><p>We follow the construction of the GAT layer in <ref type=""bibr"" target=""#b33"">Veli?kovi? et al. (2017)</ref>, extending to the relational setting, (r) ? R F ?F are the learnable parameters of a shared linear transformation.</p><p>Logits Following <ref type=""bibr"" target=""#b33"">Veli?kovi? et al. (2017)</ref>; <ref type=""bibr"" target=""#b38"">Zhang ts an optional nonlinearity. Similar to <ref type=""bibr"" target=""#b32"">Vaswani et al. (2017)</ref>; <ref type=""bibr"" target=""#b33"">Veli?kovi? et al. (2017)</ref>, we also find that using multiple head",0
"he transferability <ref type=""bibr"" target=""#b11"">[10,</ref><ref type=""bibr"" target=""#b20"">19,</ref><ref type=""bibr"" target=""#b8"">7]</ref>, an adversarial example is usually generated for a single inp box attacks than FGSM at the cost of worse transferability <ref type=""bibr"" target=""#b17"">[16,</ref><ref type=""bibr"" target=""#b8"">7]</ref>.</p><p>Momentum Iterative Fast Gradient Sign Method (MI-FGSM) =""#b20"">[19]</ref> of adversarial examples can be used to attack a black-box model. Several methods <ref type=""bibr"" target=""#b8"">[7,</ref><ref type=""bibr"" target=""#b38"">37]</ref> have been proposed t rg/ns/1.0""><head n=""3.2."">Translation-Invariant Attack Method</head><p>Although many attack methods <ref type=""bibr"" target=""#b8"">[7,</ref><ref type=""bibr"" target=""#b38"">37]</ref> can generate adversa ref type=""bibr"" target=""#b8"">7]</ref>.</p><p>Momentum Iterative Fast Gradient Sign Method (MI-FGSM) <ref type=""bibr"" target=""#b8"">[7]</ref> proposes to improve the transferability of adversarial examp formula_13"">)</formula><p>The translation-invariant method can be similarly integrated into MI-FGSM <ref type=""bibr"" target=""#b8"">[7]</ref> and DIM <ref type=""bibr"" target=""#b38"">[37]</ref> as TI-MI-F ) <ref type=""bibr"" target=""#b11"">[10]</ref>, momentum iterative fast gradient sign method (MI-FGSM) <ref type=""bibr"" target=""#b8"">[7]</ref>, and diverse inputs method (DIM) <ref type=""bibr"" target=""#b target=""#b6"">[5]</ref> since that they are not good at generating transferable adversarial examples <ref type=""bibr"" target=""#b8"">[7]</ref>. We denote the attacks combined with our translation-invaria more likely to transfer to another black-box model.</p><p>We adopt the ensemble method proposed in <ref type=""bibr"" target=""#b8"">[7]</ref>, which fuses the logit activations of different models. We a",1
"rsarial examples can serve as an important surrogate to evaluate the robustness of different models <ref type=""bibr"" target=""#b6"">[5]</ref> and improve the robustness <ref type=""bibr"" target=""#b11"">[1 basic iterative method <ref type=""bibr"" target=""#b16"">[15]</ref>, and Carlini &amp; Wagner's method <ref type=""bibr"" target=""#b6"">[5]</ref>, which are known as white-box attacks. Moreover, it is shown mbased method to further improve the transferability.</p><p>Carlini &amp; Wagner's method (C&amp;W) <ref type=""bibr"" target=""#b6"">[5]</ref> is a powerful optimization-based method, which solves arg mi t include the basic iterative method <ref type=""bibr"" target=""#b16"">[15]</ref> and C&amp;W's method <ref type=""bibr"" target=""#b6"">[5]</ref> since that they are not good at generating transferable adve e of perturbations <ref type=""bibr"" target=""#b11"">[10,</ref><ref type=""bibr"" target=""#b16"">15,</ref><ref type=""bibr"" target=""#b6"">5]</ref>. They also exist in the physical world <ref type=""bibr"" targe",0
"target=""#b32"">[31]</ref>, Inception ResNet v2 <ref type=""bibr"" target=""#b31"">[30]</ref>, ResNet 152 <ref type=""bibr"" target=""#b13"">[12]</ref> and four defense models <ref type=""bibr"" target=""#b34"">[33",0
"ion maps while the defenses induce different attention maps. A similar observation is also found in <ref type=""bibr"" target=""#b35"">[34]</ref> that the gradients of the defenses in the input space alig",0
"e been shown to be highly vulnerable to adversarial examples <ref type=""bibr"" target=""#b4"">[3,</ref><ref type=""bibr"" target=""#b33"">32,</ref><ref type=""bibr"" target=""#b11"">10]</ref>. These maliciously neural networks have been shown to be vulnerable to adversarial examples first in the visual domain <ref type=""bibr"" target=""#b33"">[32]</ref>. Then several methods are proposed to generate adversarial",0
"r"" target=""#b19"">Maclaurin et al., 2015;</ref><ref type=""bibr"" target=""#b4"">Chen et al., 2015;</ref><ref type=""bibr"" target=""#b0"">Abadi et al., 2016;</ref><ref type=""bibr"" target=""#b23"">Paszke et al., ribution, and code generation (see, e.g., <ref type=""bibr"" target=""#b2"">Bergstra et al., 2010;</ref><ref type=""bibr"" target=""#b0"">Abadi et al., 2016)</ref>. But, because declarative DSLs prevent users bed in §4.6. TensorFlow graphs come with their own set of design principles, which are presented in <ref type=""bibr"" target=""#b0"">(Abadi et al., 2016)</ref>. The following terminology will be used in devices and parallelizes operations when possible. Readers interested in the runtime should consult <ref type=""bibr"" target=""#b0"">(Abadi et al., 2016)</ref>.</p><p>The function decorator supports code",1
"amming languages, for which <ref type=""bibr"" target=""#b14"">(Jørring &amp; Scherlis, 1986)</ref> and <ref type=""bibr"" target=""#b13"">(Jones et al., 1993)</ref> are classic references, respectively.</p>< ibr"" target=""#b24"">(Rompf &amp; Odersky, 2010)</ref>, which in turn is a form of partial evaluation <ref type=""bibr"" target=""#b13"">(Jones et al., 1993)</ref>. As stated in §4.1, we expose a uservisibl d of optimization is well-known, and, indeed, one of the primary motivations for partial evaluation <ref type=""bibr"" target=""#b13"">(Jones et al., 1993;</ref><ref type=""bibr"" target=""#b29"">Taha, 2004;<",0
"ese software packages in fact more closely resemble domain-specific languages (DSLs) than libraries <ref type=""bibr"" target=""#b12"">(Innes et al., 2018)</ref>. Indeed, models written using automatic di",0
"s introduced presents a thorough treatment of multi-stage programming that is more formal than ours <ref type=""bibr"" target=""#b7"">(DeVito et al., 2013)</ref>; as another example, OptiML is a Scala-emb",0
"timization <ref type=""bibr"" target=""#b3"">(Bengio, 2000)</ref>, or, more recently, few-shot learning <ref type=""bibr"" target=""#b11"">(Finn et al., 2017)</ref>. In essence, we turn the gradient-based opt </ref> or initial weights that enable rapid adaptation to new tasks or domains in few-shot learning <ref type=""bibr"" target=""#b11"">(Finn et al., 2017)</ref>.</p><p>Meta-gradients (e.g., gradients w.r. s is expensive both from a computational and a memory point-of-view.</p><p>To alleviate this issue, <ref type=""bibr"" target=""#b11"">Finn et al. (2017)</ref> propose a first-order approximation, leading thus more knowledge than all competing methods. First-order refers to the approximation proposed by <ref type=""bibr"" target=""#b11"">Finn et al. (2017)</ref>, i.e. ignoring all second-order derivatives.",1
"rget=""#b8"">Chapelle et al., 2006)</ref>, recently deep learning on graphs has gained much attention <ref type=""bibr"" target=""#b17"">(Monti et al., 2017;</ref><ref type=""bibr"" target=""#b4"">Bojchevski &a",0
"uber, 1992;</ref><ref type=""bibr"">Bengio et al., 1992)</ref>, or the activation function of a model <ref type=""bibr"" target=""#b1"">(Agostinelli et al., 2014)</ref>. Gradient-based hyperparameter optimi",0
"vily dependent on the end-to-end data.</p><p>As our second contribution, we apply a two-stage model <ref type=""bibr"" target=""#b23"">(Tu et al., 2017;</ref><ref type=""bibr"" target=""#b10"">Kano et al., 20 intermediate representation closely tied to the source text. The architecture has been proposed by <ref type=""bibr"" target=""#b23"">Tu et al. (2017)</ref> to realize a reconstruction objective, and a s iliary ASR and MT training data ( §3). This model is similar to the architecture first described by <ref type=""bibr"" target=""#b23"">Tu et al. (2017)</ref>. It combines two encoder-decoder models in a c , we apply beam search only for the second stage decoder. We do not use the two-phase beam search of<ref type=""bibr"" target=""#b23"">Tu et al. (2017)</ref> because of its prohibitive memory requirements",1
"tion recipe as a starting point, which has previously been shown to achieve competitive ASR results <ref type=""bibr"" target=""#b14"">(Neubig et al., 2018)</ref>. <ref type=""foot"" target=""#foot_5"">6</ref",0
"than the largest speech recognition corpora <ref type=""bibr"" target=""#b8"">(Cieri et al., 2004;</ref><ref type=""bibr"" target=""#b17"">Panayotov et al., 2015)</ref> (∼ 200 hours vs 2000 hours) and several",0
"onent, recent work has shown that it is feasible to use a single sequence-to-sequence model instead <ref type=""bibr"" target=""#b9"">(Duong et al., 2016;</ref><ref type=""bibr"" target=""#b24"">Weiss et al., r from data efficiency issues.</p><p>Direct end-to-end speech translation models were first used by <ref type=""bibr"" target=""#b9"">Duong et al. (2016)</ref>, although the authors did not actually evalu",0
"tion so far have been linear, based on various methods of factorizing the third-order binary tensor <ref type=""bibr"" target=""#b16"">(Nickel et al., 2011;</ref><ref type=""bibr"" target=""#b28"">Yang et al. expressiveness.</p><p>Finally, we show that several previous stateof-the-art linear models, RESCAL <ref type=""bibr"" target=""#b16"">(Nickel et al., 2011)</ref>, DistMult <ref type=""bibr"" target=""#b28""> lated Work</head><p>Several linear models for link prediction have previously been proposed: RESCAL <ref type=""bibr"" target=""#b16"">(Nickel et al., 2011)</ref>  HypER <ref type=""bibr"" target=""#b0"">(Bal d><p>Several previous tensor factorization models can be viewed as a special case of TuckER: RESCAL <ref type=""bibr"" target=""#b16"">(Nickel et al., 2011)</ref> Following the notation introduced in Sect",1
"ead n=""3.2"">Tucker Decomposition</head><p>Tucker decomposition, named after Ledyard R.</p><p>Tucker <ref type=""bibr"" target=""#b26"">(Tucker, 1964)</ref>, decomposes a tensor into a set of matrices and",1
"-c.org/ns/1.0""><head n=""6.2"">Implementation and Experiments</head><p>We implement TuckER in PyTorch <ref type=""bibr"" target=""#b17"">(Paszke et al., 2017)</ref> and make our code available on GitHub. 1",0
"position, used widely in machine learning <ref type=""bibr"" target=""#b18"">(Schein et al., 2016;</ref><ref type=""bibr"" target=""#b1"">Ben-Younes et al., 2017;</ref><ref type=""bibr"" target=""#b30"">Yang and",0
"ed using non-linear convolutional models <ref type=""bibr"" target=""#b4"">(Dettmers et al., 2018;</ref><ref type=""bibr"" target=""#b0"">Balažević et al., 2019)</ref>. Despite achieving very good performance previously been proposed: RESCAL <ref type=""bibr"" target=""#b16"">(Nickel et al., 2011)</ref>  HypER <ref type=""bibr"" target=""#b0"">(Balažević et al., 2019)</ref> is a simplified convolutional model, th",0
"elations compared to FB15k and FB15k-237, we set d e = 200 and d r = 30. We use batch normalization <ref type=""bibr"" target=""#b8"">(Ioffe and Szegedy, 2015)</ref> and dropout <ref type=""bibr"" target=""#",0
"a strong baseline for more elaborate models. Tucker decomposition, used widely in machine learning <ref type=""bibr"" target=""#b18"">(Schein et al., 2016;</ref><ref type=""bibr"" target=""#b1"">Ben-Younes e",0
""" target=""#b28"">Yang et al., 2015;</ref><ref type=""bibr"" target=""#b25"">Trouillon et al., 2016;</ref><ref type=""bibr"" target=""#b9"">Kazemi and Poole, 2018)</ref>. Recently, state-of-the-art results have s equivalent to a hard regularization of the core tensor of TuckER in the real domain.</p><p>SimplE <ref type=""bibr"" target=""#b9"">(Kazemi and Poole, 2018)</ref> The authors show that SimplE belongs to",0
"gle-person pose estimation adopt the probabilistic graphical model or the pictorial structure model <ref type=""bibr"" target=""#b78"">[79,</ref><ref type=""bibr"" target=""#b49"">50]</ref>, which is recently",1
"target=""#b16"">17,</ref><ref type=""bibr"" target=""#b70"">71]</ref>, video pose estimation and tracking <ref type=""bibr"" target=""#b48"">[49,</ref><ref type=""bibr"" target=""#b71"">72]</ref>, etc. The recent d",0
"earning for better modeling the unary and pair-wise energies <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b64"">65,</ref><ref type=""bibr"" target=""#b44"">45]</ref> or imitating the it",0
""" target=""#b67"">[68]</ref>, half body data augmentation is also involved. We use the Adam optimizer <ref type=""bibr"" target=""#b31"">[32]</ref>. The learning schedule follows the setting <ref type=""bibr",0
"esentations for users' historical behavior sequences. Specifically, inspired by the success of BERT <ref type=""bibr"" target=""#b5"">[6]</ref> in text understanding, we propose to apply the deep bidirect that it is beneficial to incorporate context from both sides for sequence representations learning <ref type=""bibr"" target=""#b5"">[6]</ref>. For rigid order assumption, our model is more suitable than to the original models. In contrast, Transformer <ref type=""bibr"" target=""#b52"">[52]</ref> and BERT <ref type=""bibr"" target=""#b5"">[6]</ref> are built solely on multi-head self-attention and achieve st yer to layer. In this work, following OpenAI GPT <ref type=""bibr"" target=""#b38"">[38]</ref> and BERT <ref type=""bibr"" target=""#b5"">[6]</ref>, we use a smoother GELU <ref type=""bibr"" target=""#b12"">[13]< ive: Cloze task <ref type=""bibr"" target=""#b50"">[50]</ref> (also known as ""Masked Language Model"" in <ref type=""bibr"" target=""#b5"">[6]</ref>) to sequential recommendation. It is a test consisting of a network would not learn anything useful.</p><p>To tackle this problem, we introduce the Cloze task <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b50"">50]</ref> to take the place of ial to jointly attend to information from different representation subspaces at different positions <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b29"">29,</ref><ref type=""bibr"" targ",1
"ertical convolutional filters. Chen et al. <ref type=""bibr"" target=""#b2"">[3]</ref> and Huang et al. <ref type=""bibr"" target=""#b18"">[19]</ref> employ Memory Network to improve sequential recommendation ative items that the user has not interacted with. To make the sampling reliable and representative <ref type=""bibr"" target=""#b18"">[19]</ref>, these 100 negative items are sampled according to their p",0
"by integrating the distributed item representations learned from auxiliary information, e.g., text <ref type=""bibr"" target=""#b23"">[23,</ref><ref type=""bibr"" target=""#b53"">53]</ref>, images <ref type=",0
"tion mechanism has shown promising potential in modeling sequential data, e.g., machine translation <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b52"">52]</ref> and text classificat",0
"-order MCs are also adopted to consider more previous items <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b10"">11]</ref>.</p><p>Recently, RNN and its variants, Gated Recurrent Unit",0
"ial patterns from user historical interactions using Markov chains (MCs). For example, Shani et al. <ref type=""bibr"" target=""#b45"">[45]</ref> formalized recommendation generation as a sequential optim",0
"17"">[18,</ref><ref type=""bibr"" target=""#b54"">54]</ref> due to various unobservable external factors <ref type=""bibr"" target=""#b4"">[5]</ref>. In such a situation, it is crucial to incorporate context f ferred by long sequence datasets as discussed in the analysis about head number h in ablation study <ref type=""bibr"" target=""#b4"">(5)</ref>. (3) LN, RC, and Dropout. These components are introduced ma",0
"lized using truncated normal distribution in the range [−0.02, 0.02]. We train the model using Adam <ref type=""bibr"" target=""#b24"">[24]</ref> with learning rate of 1e-4, β 1 = 0.9, β 2 = 0.999, ℓ 2 we",0
"stimates user preferences via Multi-Layer Perceptions (MLP) instead of inner product, while AutoRec <ref type=""bibr"" target=""#b44"">[44]</ref> and CDAE <ref type=""bibr"" target=""#b57"">[57]</ref> predict",0
"black blob). <ref type=""bibr"" target=""#b14"">[15]</ref>, <ref type=""bibr"" target=""#b22"">[23]</ref>, <ref type=""bibr"" target=""#b24"">[25]</ref>, <ref type=""bibr"" target=""#b25"">[26]</ref>, <ref type=""bib bibr"" target=""#b55"">[56]</ref>, as well as squeeze and excitation (SE) block presented by Hu et al. <ref type=""bibr"" target=""#b24"">[25]</ref>. The proposed Res2Net module introduces the scale dimensio sily integrate the cardinality dimension <ref type=""bibr"" target=""#b55"">[56]</ref> and the SE block <ref type=""bibr"" target=""#b24"">[25]</ref> with the proposed Res2Net module.</p></div> <div xmlns=""ht mension cardinality <ref type=""bibr"" target=""#b55"">[56]</ref> (replace conv with group conv) and SE <ref type=""bibr"" target=""#b24"">[25]</ref> blocks.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0"" calibrates channel-wise feature responses by explicitly modelling inter-dependencies among channels <ref type=""bibr"" target=""#b24"">[25]</ref>. Similar to <ref type=""bibr"" target=""#b24"">[25]</ref>, we y modelling inter-dependencies among channels <ref type=""bibr"" target=""#b24"">[25]</ref>. Similar to <ref type=""bibr"" target=""#b24"">[25]</ref>, we add the SE block right before the residual connections ref type=""bibr"" target=""#b22"">[23]</ref>, ResNeXt <ref type=""bibr"" target=""#b55"">[56]</ref>, SE-Net <ref type=""bibr"" target=""#b24"">[25]</ref>, bLResNet <ref type=""bibr"" target=""#b4"">[5]</ref>, and DLA",1
""">[28]</ref>, <ref type=""bibr"" target=""#b46"">[47]</ref>, <ref type=""bibr"" target=""#b50"">[51]</ref>, <ref type=""bibr"" target=""#b55"">[56]</ref>, <ref type=""bibr"" target=""#b59"">[60]</ref>, have made sign o existing dimensions of depth <ref type=""bibr"" target=""#b46"">[47]</ref>, width 2 , and cardinality <ref type=""bibr"" target=""#b55"">[56]</ref>. We state in Sec. 4.4 that increasing scale is more effect rformance of state-of-the-art CNNs, e.g., ResNet <ref type=""bibr"" target=""#b22"">[23]</ref>, ResNeXt <ref type=""bibr"" target=""#b55"">[56]</ref>, and DLA <ref type=""bibr"" target=""#b59"">[60]</ref>.</p><p> "">[28]</ref>, <ref type=""bibr"" target=""#b46"">[47]</ref>, <ref type=""bibr"" target=""#b50"">[51]</ref>, <ref type=""bibr"" target=""#b55"">[56]</ref>, <ref type=""bibr"" target=""#b59"">[60]</ref>, achieving stat modern backbone CNNs architectures, e.g., ResNet <ref type=""bibr"" target=""#b22"">[23]</ref>, ResNeXt <ref type=""bibr"" target=""#b55"">[56]</ref>, and DLA <ref type=""bibr"" target=""#b59"">[60]</ref>. Instea odules have been proposed in recent years, including cardinality dimension introduced by Xie et al. <ref type=""bibr"" target=""#b55"">[56]</ref>, as well as squeeze and excitation (SE) block presented by nts. As shown in Fig. <ref type=""figure"">3</ref>, we can easily integrate the cardinality dimension <ref type=""bibr"" target=""#b55"">[56]</ref> and the SE block <ref type=""bibr"" target=""#b24"">[25]</ref> sion cardinality.</head><p>The dimension cardinality indicates the number of groups within a filter <ref type=""bibr"" target=""#b55"">[56]</ref>. This dimension changes filters from single-branch to mult ig. <ref type=""figure"">3</ref>: The Res2Net module can be integrated with the dimension cardinality <ref type=""bibr"" target=""#b55"">[56]</ref> (replace conv with group conv) and SE <ref type=""bibr"" tar into the state-ofthe-art models, such as ResNet <ref type=""bibr"" target=""#b22"">[23]</ref>, ResNeXt <ref type=""bibr"" target=""#b55"">[56]</ref>, DLA <ref type=""bibr"" target=""#b59"">[60]</ref> and Big-Lit and bLRes2Net-50, respectively.</p><p>The proposed scale dimension is orthogonal to the cardinality <ref type=""bibr"" target=""#b55"">[56]</ref> dimension and width <ref type=""bibr"" target=""#b22"">[23]</r 4]</ref> dataset, we mainly use the ResNet-50 <ref type=""bibr"" target=""#b22"">[23]</ref>, ResNeXt-50 <ref type=""bibr"" target=""#b55"">[56]</ref>, DLA-60 <ref type=""bibr"" target=""#b59"">[60]</ref>, and bLR ments on the CIFAR <ref type=""bibr"" target=""#b26"">[27]</ref> dataset, we use the ResNeXt-29, 8c×64w <ref type=""bibr"" target=""#b55"">[56]</ref> as our baseline model. Empirical evaluations and discussio ons, we use the Pytorch implementation of ResNet <ref type=""bibr"" target=""#b22"">[23]</ref>, ResNeXt <ref type=""bibr"" target=""#b55"">[56]</ref>, DLA <ref type=""bibr"" target=""#b59"">[60]</ref> as well as type=""bibr"" target=""#b22"">[23]</ref>. On the CIFAR dataset, we use the implementation of ResNeXt-29 <ref type=""bibr"" target=""#b55"">[56]</ref>. For all tasks, we use the original implementations of bas ement of 0.73% in terms of top-1 error over the  <ref type=""bibr"" target=""#b22"">[23]</ref>, ResNeXt <ref type=""bibr"" target=""#b55"">[56]</ref>, SE-Net <ref type=""bibr"" target=""#b24"">[25]</ref>, bLResNe ve been shown to have stronger representation capability <ref type=""bibr"" target=""#b22"">[23]</ref>, <ref type=""bibr"" target=""#b55"">[56]</ref> for vision tasks. To validate our model with greater depth which contains 50k training images and 10k testing images from 100 classes. The ResNeXt-29, 8c×64w <ref type=""bibr"" target=""#b55"">[56]</ref> is used as the baseline model. We only replace the origina iv xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.4"">Scale Variation</head><p>Similar to Xie et al. <ref type=""bibr"" target=""#b55"">[56]</ref>, we evaluate the test performance of the baseline model by ng different CNN dimensions, including scale (Equation ( <ref type=""formula"">1</ref>)), cardinality <ref type=""bibr"" target=""#b55"">[56]</ref>, and depth <ref type=""bibr"" target=""#b46"">[47]</ref>. Whil fix all other dimensions. A series of networks are trained and evaluated under these changes. Since <ref type=""bibr"" target=""#b55"">[56]</ref> has already shown that increasing cardinality is more effe",1
"ype=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" target=""#b42"">[43]</ref>, skeleton extraction <ref type=""bibr"" target=""#b66"">[67]</ref>, stereo matching <ref type=""bibr"" target=""#b41"">[42]</ref> =""bibr"" target=""#b33"">[34]</ref>, <ref type=""bibr"" target=""#b64"">[65]</ref>, and skeleton detection <ref type=""bibr"" target=""#b66"">[67]</ref>, boosting the model performance of those fields.</p></div>",0
"pe=""bibr"" target=""#b5"">[6]</ref>, salient object detection <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b23"">[24]</ref>, object proposal <ref type=""bibr"" target=""#b11"">[12]</ref> et=""#b61"">[62]</ref> are proposed for improving salient object detection. More recently, Hou et al. <ref type=""bibr"" target=""#b23"">[24]</ref> introduce dense short connections among stages to provide locate both the holistic objects as well as their region details. Here we use the latest method DSS <ref type=""bibr"" target=""#b23"">[24]</ref> as our baseline. For a fair comparison, we only replace th with ResNet-50 and our proposed Res2Net-50, while keeping other configurations unchanged. Following <ref type=""bibr"" target=""#b23"">[24]</ref>, we train those two models using the MSRA-B dataset <ref t",0
"=""#b42"">[43]</ref>, attention prediction <ref type=""bibr"" target=""#b44"">[45]</ref>, target tracking <ref type=""bibr"" target=""#b62"">[63]</ref>, action recognition <ref type=""bibr"" target=""#b45"">[46]</r",0
"for an image of 224 × 224 pixels is around 4.2G for 50-layer networks. For experiments on the CIFAR <ref type=""bibr"" target=""#b26"">[27]</ref> dataset, we use the ResNeXt-29, 8c×64w <ref type=""bibr"" ta .org/ns/1.0""><head n=""4.3"">CIFAR</head><p>We also conduct some experiments on the CIFAR-100 dataset <ref type=""bibr"" target=""#b26"">[27]</ref>, which contains 50k training images and 10k testing images",0
"duces feature pyramid to extract features with different scales from a single image. The SSD method <ref type=""bibr"" target=""#b35"">[36]</ref> utilizes feature maps from different stages to process vis",0
"<ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b23"">[24]</ref>, object proposal <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" target=""#b42"">[43]</ref>, skeleton extra",0
"=""#b19"">[20]</ref> which contains 10582 training images and 1449 val images. We use the Deeplab v3+ <ref type=""bibr"" target=""#b7"">[8]</ref> as our segmentation method. All implementations remain the s et=""#b7"">[8]</ref> as our segmentation method. All implementations remain the same with Deeplab v3+ <ref type=""bibr"" target=""#b7"">[8]</ref> except that the backbone network is replaced with ResNet and",0
"[37]</ref>, semantic segmentation <ref type=""bibr"" target=""#b5"">[6]</ref>, salient object detection <ref type=""bibr"" target=""#b33"">[34]</ref>, <ref type=""bibr"" target=""#b64"">[65]</ref>, and skeleton d",0
"r, a pointwise non-linearity, and either an invariant or equivariant linear output layer. Recently, <ref type=""bibr"" target=""#b19"">Maron et al. (2019b)</ref> showed that by allowing higherorder tensor =""bibr"" target=""#b13"">Hornik et al., 1989;</ref><ref type=""bibr"" target=""#b20"">Pinkus, 1999)</ref>. <ref type=""bibr"" target=""#b19"">Maron et al. (2019b)</ref> recently proved that certain invariant GNN aces of continuous invariant (resp. equivariant) functions.</p><p>2 The case of invariant functions <ref type=""bibr"" target=""#b19"">Maron et al. (2019b)</ref> recently proved that invariant GNNs simila n the whole set G inv. , that is, for all numbers of nodes n n max simultaneously. On the contrary, <ref type=""bibr"" target=""#b19"">Maron et al. (2019b)</ref> work with a fixed n, and it does not seem on the order of tensorization k s . Indeed, through Noether's theorem on polynomials, the proof of <ref type=""bibr"" target=""#b19"">Maron et al. (2019b)</ref> shows that k s n d (n d − 1)/2 is sufficie previous invariant case could be easily extended to invariance to subgroups of O n , as is done by <ref type=""bibr"" target=""#b19"">Maron et al. (2019b)</ref>, for the equivariant case our theorem only ions in the rest of the introduction, in Section 2 we provide an alternative proof of the result of <ref type=""bibr"" target=""#b19"">(Maron et al., 2019b)</ref> for invariant GNNs (Theorem 1), which wil t. Theorem 1. For any ρ ∈ F MLP , N inv. (ρ) is dense in C(G inv. , d edit ).</p><p>Comparison with <ref type=""bibr"" target=""#b19"">(Maron et al., 2019b)</ref>. A variant of Theorem 1 was proved in <re th <ref type=""bibr"" target=""#b19"">(Maron et al., 2019b)</ref>. A variant of Theorem 1 was proved in <ref type=""bibr"" target=""#b19"">(Maron et al., 2019b)</ref>. The two proofs are however different: th See the next subsection for details.</p><p>One improvement of our result with respect to the one of <ref type=""bibr"" target=""#b19"">(Maron et al., 2019b)</ref> is that it can handle graphs of varying s bibr"" target=""#b0"">(Battaglia et al., 2016)</ref>. Another outstanding open question, formulated in <ref type=""bibr"" target=""#b19"">(Maron et al., 2019b)</ref>, is the characterization of the approxima ing a single hidden layer of such equivariant operators followed by an invariant layer is proved in <ref type=""bibr"" target=""#b19"">(Maron et al., 2019b</ref>) (see also <ref type=""bibr"">(Kondor et al.",1
"of physical systems <ref type=""bibr"" target=""#b0"">(Battaglia et al., 2016)</ref>, state prediction <ref type=""bibr"" target=""#b24"">(Sanchez-Gonzalez et al., 2018)</ref>, protein interface prediction <",0
"yers perceptron (MLP) with a single hidden layer <ref type=""bibr"" target=""#b6"">(Cybenko, 1989;</ref><ref type=""bibr"" target=""#b13"">Hornik et al., 1989;</ref><ref type=""bibr"" target=""#b20"">Pinkus, 1999 ct.</formula><p>Algebra of invariant GNNs. Unfortunately, N inv. (ρ) is not a subalgebra. Following <ref type=""bibr"" target=""#b13"">Hornik et al. (1989)</ref>, we first need to extend it to be closed u heorem, we have thus proved that N ⊗ inv. (ρ sig ) is dense in C(G inv. , d edit ). Then, following <ref type=""bibr"" target=""#b13"">Hornik et al. (1989)</ref>, we go back to the original class N inv. (",0
"019)</ref> for recent reviews. For regular-grid graphs, they match classical convolutional networks <ref type=""bibr"" target=""#b17"">(LeCun et al., 1989)</ref> which by design can only approximate trans",0
"vertices. Early definitions of these ""message passing"" architectures rely on fixed point iterations <ref type=""bibr"" target=""#b27"">(Scarselli et al., 2009)</ref>, while more recent constructions make",0
"gment(x), an augmentation of itself.</p><p>In the simplest case, for unlabeled points x, prior work <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b39"">40]</ref> adds the loss term btain an artificial target for an unlabeled example is common in consistency regularization methods <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b39"">40,</ref><ref type=""bibr"" ta itive to incorrect predictions. For this reason, it is often used as the unlabeled data loss in SSL <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b43"">44]</ref> as well as a measu b25"">[26]</ref>. We do not propagate gradients through computing the guessed labels, as is standard <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b43"">44,</ref><ref type=""bibr"" ta ines, we consider the four methods considered in <ref type=""bibr"" target=""#b34"">[35]</ref> (Π-Model <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b39"">40]</ref>, Mean Teacher <ref",1
"is loss term falls into one of three classes (discussed further in Section 2): entropy minimization <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b27"">28]</ref>-which encourages t re that the classifier output low-entropy predictions on unlabeled data. This is done explicitly in <ref type=""bibr"" target=""#b17"">[18]</ref> with a loss term which minimizes the entropy of p model (y",1
"ref type=""bibr"" target=""#b29"">[30,</ref><ref type=""bibr"" target=""#b45"">46]</ref>. We also use MixUp <ref type=""bibr"" target=""#b46"">[47]</ref> in MixMatch to encourage convex behavior ""between"" example abel <ref type=""bibr"" target=""#b27"">[28]</ref>) which are described in section 2. We also use MixUp <ref type=""bibr"" target=""#b46"">[47]</ref> on its own as a baseline. MixUp is designed as a regulariz",1
"p>In the simplest case, for unlabeled points x, prior work <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b39"">40]</ref> adds the loss term</p><formula xml:id=""formula_0"">p model ( ered in <ref type=""bibr"" target=""#b34"">[35]</ref> (Π-Model <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b39"">40]</ref>, Mean Teacher <ref type=""bibr"" target=""#b43"">[44]</ref>, Vi ed example is common in consistency regularization methods <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b39"">40,</ref><ref type=""bibr"" target=""#b43"">44]</ref>.</p><p>Algorithm 1",1
"target=""#b18"">[19]</ref>. We use weight decay which penalizes the L 2 norm of the model parameters <ref type=""bibr"" target=""#b29"">[30,</ref><ref type=""bibr"" target=""#b45"">46]</ref>. We also use MixUp",0
"s that we do not discuss here (e.g., ""transductive"" models <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b20"">21]</ref>, graph-based method",0
"ere is a wide literature on SSL techniques that we do not discuss here (e.g., ""transductive"" models <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" ta",0
"f type=""bibr"" target=""#b20"">21]</ref>, graph-based methods <ref type=""bibr"" target=""#b48"">[49,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b28"">29]</ref>, generative modeling",0
"decay which penalizes the L 2 norm of the model parameters <ref type=""bibr"" target=""#b29"">[30,</ref><ref type=""bibr"" target=""#b45"">46]</ref>. We also use MixUp <ref type=""bibr"" target=""#b46"">[47]</ref",0
"nge the pixel content of an image without altering its label <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b42"">43,</ref><ref type=""bibr"" target=""#b9"">10]</ref>. Roughly speaking, t",0
"ixUp has been previously applied to semi-supervised learning; in particular, the concurrent work of <ref type=""bibr"" target=""#b44"">[45]</ref> uses a subset of the methodology used in MixMatch. We clar thout mixing across labeled and unlabeled examples</p><p>• using Interpolation Consistency Training <ref type=""bibr"" target=""#b44"">[45]</ref>, which can be seen as a special case of this ablation stud",0
"<ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b28"">29]</ref>, generative modeling <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" targ",0
"evaluate the effectiveness of MixMatch on four standard benchmark datasets: CIFAR-10 and CIFAR-100 <ref type=""bibr"" target=""#b23"">[24]</ref>, SVHN <ref type=""bibr"" target=""#b31"">[32]</ref>, and STL-1",0
"get=""#b22"">23,</ref><ref type=""bibr"" target=""#b37"">38,</ref><ref type=""bibr"" target=""#b33"">34,</ref><ref type=""bibr"" target=""#b41"">42]</ref>, etc.). More comprehensive overviews are provided in <ref t",0
"<p>VAT achieved the previous state-of-the-art of 91.6% test accuracy for a privacy loss of ε = 4.96 <ref type=""bibr"" target=""#b36"">[37]</ref>.</p><p>Because MixMatch performs well with few labeled poi",0
"ef type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b20"">21]</ref>, graph-based methods <ref type=""bibr"" target=""#b48"">[49,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" targ </ref><ref type=""bibr"" target=""#b41"">42]</ref>, etc.). More comprehensive overviews are provided in <ref type=""bibr"" target=""#b48"">[49,</ref><ref type=""bibr"" target=""#b5"">6]</ref>. In the following, w",0
"in many tasks it is much easier or cheaper to obtain unlabeled data.</p><p>Semi-supervised learning <ref type=""bibr"" target=""#b5"">[6]</ref> (SSL) seeks to largely alleviate the need for labeled data b /ref>, etc.). More comprehensive overviews are provided in <ref type=""bibr"" target=""#b48"">[49,</ref><ref type=""bibr"" target=""#b5"">6]</ref>. In the following, we will refer to a generic model p model (",0
"standard benchmark datasets: CIFAR-10 and CIFAR-100 <ref type=""bibr"" target=""#b23"">[24]</ref>, SVHN <ref type=""bibr"" target=""#b31"">[32]</ref>, and STL-10 <ref type=""bibr"" target=""#b7"">[8]</ref>. Stand",0
"target=""#b13"">[14]</ref>, which was shown to be useful in large-scale conditional generation tasks <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b32"">34]</ref>.</p><p>The model-agn ""#b17"">[19]</ref>, but replace downsampling and upsampling layers with residual blocks similarly to <ref type=""bibr"" target=""#b5"">[6]</ref> (with batch normalization [15] replaced by instance normaliz tional and fully connected layers in all the networks. We also use self-attention blocks, following <ref type=""bibr"" target=""#b5"">[6]</ref> and <ref type=""bibr"" target=""#b40"">[42]</ref>. They are inse",1
"ystems that construct photo-realistic head models using sophisticated physical and optical modeling <ref type=""bibr"" target=""#b0"">[1]</ref>, it is still excessive for most practical telepresence scena",0
"cosine similarity (CSIM) between embedding vectors of the state-of-the-art face recognition network <ref type=""bibr"" target=""#b8"">[9]</ref> for measuring identity mismatch (note that this network has",0
"arget=""#b30"">[32]</ref>. Our meta-learning stage uses the adaptive instance normalization mechanism <ref type=""bibr"" target=""#b13"">[14]</ref>, which was shown to be useful in large-scale conditional g instance normalization layers, following the adaptive instance normalization technique proposed in <ref type=""bibr"" target=""#b13"">[14]</ref>, though we still use regular (non-adaptive) instance norma",0
"recent progress in generative modeling of images. Thus, our architecture uses adversarial training <ref type=""bibr"" target=""#b11"">[12]</ref> and, more specifically, the ideas behind conditional discr",0
"""#b19"">Suganuma et al., 2017;</ref><ref type=""bibr"">Zoph &amp; Le, 2017)</ref>. More recent studies <ref type=""bibr"" target=""#b3"">(Brock et al., 2018;</ref><ref type=""bibr"" target=""#b18"">Shirakawa et he most likely architecture, ? = argmax c p ? (c), from scratch, which is a commonly used technique <ref type=""bibr"" target=""#b3"">(Brock et al., 2018;</ref><ref type=""bibr"" target=""#b9"">Liu et al., 20 hree different existing approaches are reported. The 1st category is based on a meta-network. SMASH <ref type=""bibr"" target=""#b3"">(Brock et al., 2018)</ref> employs HyperNet that takes an architecture",1
""" target=""#b18"">Shirakawa et al., 2018;</ref><ref type=""bibr"" target=""#b16"">Pham et al., 2018;</ref><ref type=""bibr"" target=""#b9"">Liu et al., 2019;</ref><ref type=""bibr"" target=""#b22"">Xie et al., 2019 scratch, which is a commonly used technique <ref type=""bibr"" target=""#b3"">(Brock et al., 2018;</ref><ref type=""bibr"" target=""#b9"">Liu et al., 2019;</ref><ref type=""bibr"" target=""#b16"">Pham et al., 201 and architectures into optimization of a differentiable objective by means of continuous relaxation <ref type=""bibr"" target=""#b9"">(Liu et al., 2019;</ref><ref type=""bibr"" target=""#b22"">Xie et al., 201 set and adopt the standard preprocessing and data augmentation as done in the previous works, e.g., <ref type=""bibr"" target=""#b9"">Liu et al. (2019)</ref>; <ref type=""bibr"" target=""#b16"">Pham et al. (2 g the architecture search, we split the training dataset into halves as D = {D x , D ? } as done in <ref type=""bibr"" target=""#b9"">Liu et al. (2019)</ref>. The gradients ( <ref type=""formula"" target=""# each layer, along with a skip connection. The 2nd category is based on continuous relaxation. DARTS <ref type=""bibr"" target=""#b9"">(Liu et al., 2019)</ref> extends essentially categorical architecture",0
"sured by the validation error obtained after the training of the weights under a fixed architecture <ref type=""bibr"" target=""#b17"">(Real et al., 2017;</ref><ref type=""bibr"" target=""#b19"">Suganuma et a",0
"mization framework for our stochastic relaxation based on the so-called stochastic natural gradient <ref type=""bibr"" target=""#b1"">(Amari, 1998)</ref>. Our theoretical investigation derives a condition",0
"&amp; Le, 2017)</ref>. More recent studies <ref type=""bibr"" target=""#b3"">(Brock et al., 2018;</ref><ref type=""bibr"" target=""#b18"">Shirakawa et al., 2018;</ref><ref type=""bibr"" target=""#b16"">Pham et a et al., 2019;</ref><ref type=""bibr"" target=""#b22"">Xie et al., 2019)</ref> or stochastic relaxation <ref type=""bibr"" target=""#b18"">(Shirakawa et al., 2018;</ref><ref type=""bibr"" target=""#b16"">Pham et amework for one-shot NAS. Our strategy is based on stochastic relaxation. We generalize the work by <ref type=""bibr"" target=""#b18"">Shirakawa et al. (2018)</ref> to enable arbitrary types of architectu gory is based on stochastic relaxation, which is another approach enabling to use gradient descent. <ref type=""bibr"" target=""#b18"">Shirakawa et al. (2018)</ref> has introduced it to model connections",0
"nalysis to prove that our transductive model is a more general form than existing models (e.g., MNE <ref type=""bibr"" target=""#b42"">[43]</ref>). • Efficient and scalable learning algorithms for GATNE h beds networks with multiple views in a single collaborated embedding using attention mechanism. MNE <ref type=""bibr"" target=""#b42"">[43]</ref> uses one common embedding and several additional embedding r ∈ R s×d is a trainable transformation matrix.</p><p>Connection with Previous Work. We choose MNE <ref type=""bibr"" target=""#b42"">[43]</ref>, a recent representative work for MHEN, as the base model PMNE <ref type=""bibr"" target=""#b21"">[22]</ref>, MVE <ref type=""bibr"" target=""#b29"">[30]</ref>, MNE <ref type=""bibr"" target=""#b42"">[43]</ref>. We denote the three methods of PMNE as PMNE(n), PMNE(r) a",1
"type=""bibr"" target=""#b26"">[27]</ref>, LINE <ref type=""bibr"" target=""#b34"">[35]</ref>, and node2vec <ref type=""bibr"" target=""#b9"">[10]</ref> are pioneering works that introduce deep learning technique .org/ns/1.0""><head>Single</head><p>Single / LINE <ref type=""bibr"" target=""#b34"">[35]</ref> node2vec <ref type=""bibr"" target=""#b9"">[10]</ref> NetMF <ref type=""bibr"" target=""#b28"">[29]</ref> NetSMF <ref ons on large-scale networks while preserving both firstorder and second-order proximities. node2vec <ref type=""bibr"" target=""#b9"">[10]</ref> designs a biased random walk procedure to efficiently explo type=""bibr"" target=""#b26"">[27]</ref>, LINE <ref type=""bibr"" target=""#b34"">[35]</ref>, and node2vec <ref type=""bibr"" target=""#b9"">[10]</ref>. As these methods can only deal with HON, we feed separate or firstorder and second-order embeddings. The number of samples is set to 1000 million. • node2vec <ref type=""bibr"" target=""#b9"">[10]</ref>. The codes of node2vec are from the corresponding author's mization</head><p>We discuss how to learn the proposed transductive and inductive models. Following <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" tar",0
"> AANE <ref type=""bibr"" target=""#b14"">[15]</ref> SNE <ref type=""bibr"" target=""#b19"">[20]</ref> DANE <ref type=""bibr"" target=""#b8"">[9]</ref> ANRL <ref type=""bibr"" target=""#b43"">[44]</ref> Heterogeneous embedding social networks by capturing both the structural proximity and attribute proximity. DANE <ref type=""bibr"" target=""#b8"">[9]</ref> can capture the high nonlinearity and preserve various proxi",0
"tly, we can add dynamic information into node attributes. For example, we can use methods like LSTM <ref type=""bibr"" target=""#b13"">[14]</ref> to capture the dynamic activities of users. Secondly, the",0
"ef> SNE <ref type=""bibr"" target=""#b19"">[20]</ref> DANE <ref type=""bibr"" target=""#b8"">[9]</ref> ANRL <ref type=""bibr"" target=""#b43"">[44]</ref> Heterogeneous Network (HEN) PTE <ref type=""bibr"" target=""# nlinearity and preserve various proximities in both topological structure and node attributes. ANRL <ref type=""bibr"" target=""#b43"">[44]</ref> uses a neighbor enhancement autoencoder to model the node ed network embedding model.</p><p>Attributed Network Embedding Methods. The compared method is ANRL <ref type=""bibr"" target=""#b43"">[44]</ref>. ANRL uses a neighbor enhancement autoencoder to model the Alibaba distributed cloud platform.</p><p>A.2.4 Attributed Network Embedding Methods.</p><p>• ANRL <ref type=""bibr"" target=""#b43"">[44]</ref>. We use the codes from Alibaba's GitHub 14 . As YouTube an",0
"-to-video generation <ref type=""bibr"" target=""#b27"">[28,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b1"">2]</ref> to text-to-video generation <ref type=""bibr"" target=""#b22"">[2",1
"<ref type=""bibr"" target=""#b2"">[3]</ref> require extra VGG-M network pretrained on VGG Face dataset <ref type=""bibr"" target=""#b24"">[25]</ref> and Wilels et al. <ref type=""bibr"" target=""#b34"">[35]</ref",0
""" target=""#b19"">[20]</ref> and image/video generation task <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b36"">37,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" tar",0
"ndamental problem in computer vision, where applications are ranging from audio-to-video generation <ref type=""bibr"" target=""#b27"">[28,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" targ arget=""#b11"">12,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" target=""#b0"">1,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b34"">35]</ref> failed to consider proposed an encoder-decoder network which achieves long-term future prediction. Suwajanakorn et al. <ref type=""bibr"" target=""#b27"">[28]</ref> transferred the audio signal to lip shapes and then synthe",0
"of a specific person <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b28"">29]</ref>. For example, Suwajanakorn et al. <ref type=""bibr"" target="" br"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b28"">29]</ref>. For example, Suwajanakorn et al. <ref type=""bibr"" target=""#b28"">[29]</ref> synthesized a taking face of President Obama with accurate",0
"<body> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""1"">INTRODUCTION</head><p>Score matching <ref type=""bibr"" target=""#b12"">(Hyvärinen, 2005)</ref> is particularly suitable for learning unnorma MLE) can be difficult due to the intractable partition function Z θ . To avoid this, score matching <ref type=""bibr"" target=""#b12"">(Hyvärinen, 2005)</ref> minimizes the Fisher divergence between p d a not have access to the score function of the data s d (x).</p><p>By applying integration by parts, <ref type=""bibr"" target=""#b12"">Hyvärinen (2005)</ref> shows that L(θ) can be written as L(θ) = J(θ) </p><p>Other than our requirements on p v , the assumptions are exactly the same as in Theorem 1 of <ref type=""bibr"" target=""#b12"">Hyvärinen (2005)</ref>. We advise the interested readers to read Appe he consistency of <ref type=""bibr"">MLE (van der Vaart, 1998)</ref>. We also adopt the assumption in <ref type=""bibr"" target=""#b12"">Hyvärinen (2005)</ref> that all densities are strictly positive (Assu M . These two facts lead to consistency. For a complete proof, see Appendix B.3.</p><p>Remark 1. In <ref type=""bibr"" target=""#b12"">Hyvärinen (2005)</ref>, the authors only showed that J(θ) = 0 ⇔ θ = θ s a constant w.r.t. θ.</p><p>Proof. The basic idea of this proof is similar to that of Theorem 1 in <ref type=""bibr"" target=""#b12"">Hyvärinen (2005)</ref>. First, note that L(θ, p v ) can be expanded t",1
"turally adapted as an objective for estimating the score function of a data generating distribution <ref type=""bibr"" target=""#b24"">(Sasaki et al., 2014;</ref><ref type=""bibr"" target=""#b28"">Strathmann",0
"/head><p>Two prior methods for score estimation are based on a generalized form of Stein's identity <ref type=""bibr"" target=""#b27"">(Stein, 1981;</ref><ref type=""bibr"" target=""#b6"">Gorham &amp; Mackey,",0
"unction of a data generating distribution <ref type=""bibr"" target=""#b24"">(Sasaki et al., 2014;</ref><ref type=""bibr"" target=""#b28"">Strathmann et al., 2015)</ref> by training a score function model par",0
"GCN, an algorithm to design the batches based on efficient graph clustering algorithms (e.g., METIS <ref type=""bibr"" target=""#b8"">[8]</ref>). We take this idea further by proposing a stochastic multi- p>We use graph clustering algorithms to partition the graph. Graph clustering methods such as Metis <ref type=""bibr"" target=""#b8"">[8]</ref> and Graclus <ref type=""bibr"" target=""#b4"">[4]</ref> aim to c",1
"s=""http://www.tei-c.org/ns/1.0""><head n=""1"">INTRODUCTION</head><p>Graph convolutional network (GCN) <ref type=""bibr"" target=""#b9"">[9]</ref> has become increasingly popular in addressing many graph-bas popular in addressing many graph-based applications, including semi-supervised node classification <ref type=""bibr"" target=""#b9"">[9]</ref>, link prediction <ref type=""bibr"" target=""#b17"">[17]</ref> a sic GCN training algorithms.</p><p>• Full-batch gradient descent is proposed in the first GCN paper <ref type=""bibr"" target=""#b9"">[9]</ref>. To compute the full gradient, it requires storing all the i imensional feature vector and X ∈ R N ×F denotes the feature matrix for all N nodes. An L-layer GCN <ref type=""bibr"" target=""#b9"">[9]</ref> consists of L graph convolution layers and each of them cons r sub-graphs (other approaches) since they are fixed and usually not the main bottleneck.</p><p>GCN <ref type=""bibr"" target=""#b9"">[9]</ref> Vanilla SGD GraphSAGE <ref type=""bibr"" target=""#b5"">[5]</ref O(br L F + LF 2 ) O(brLF + LF 2 ) O(LN F + LF 2 ) O(bLF + LF 2 )</formula><p>In the original paper <ref type=""bibr"" target=""#b9"">[9]</ref>, full gradient descent is used for training GCN, but it suff d d (average degree) times in the upper layer. As a consequence, the original full gradient descent <ref type=""bibr"" target=""#b9"">[9]</ref> only needs to compute O(N L) embeddings per epoch, which mea .0""><head n=""3.3"">Issues of training deeper GCNs</head><p>Previous attempts of training deeper GCNs <ref type=""bibr"" target=""#b9"">[9]</ref> seem to suggest that adding more layers is not helpful. Howe the datasets used in the experiments may be too small to make a proper justification. For example, <ref type=""bibr"" target=""#b9"">[9]</ref> considered a graph with only a few hundreds of training node comes difficult as it may impede the information from the first few layers being passed through. In <ref type=""bibr"" target=""#b9"">[9]</ref>, they adopt a technique similar to residual connections <ref ]</ref>. For the other methods, we use all the original papers' code from their github pages. Since <ref type=""bibr"" target=""#b9"">[9]</ref> has difficulty to scale to large graphs, we do not compare w",1
"luding semi-supervised node classification <ref type=""bibr"" target=""#b9"">[9]</ref>, link prediction <ref type=""bibr"" target=""#b17"">[17]</ref> and recommender systems <ref type=""bibr"" target=""#b15"">[15",0
"osed to use a fixed size of neighborhood samples during back-propagation through layers and FastGCN <ref type=""bibr"" target=""#b1"">[1]</ref> proposed importance sampling, but the overhead of these meth ""bibr"" target=""#b9"">[9]</ref> Vanilla SGD GraphSAGE <ref type=""bibr"" target=""#b5"">[5]</ref> FastGCN <ref type=""bibr"" target=""#b1"">[1]</ref> VR-GCN <ref type=""bibr"" target=""#b2"">[2]</ref> Cluster-GCN T embedding computations for each loss term but also makes gradient estimation less accurate. FastGCN <ref type=""bibr"" target=""#b1"">[1]</ref> proposed an important sampling strategy to improve the gradi at mini-batch SGD can improve the training speed and memory requirement of GCN in some recent works <ref type=""bibr"" target=""#b1"">[1,</ref><ref type=""bibr"" target=""#b2"">2,</ref><ref type=""bibr"" target iv xmlns=""http://www.tei-c.org/ns/1.0""><head n=""6.2"">Implementation details</head><p>Previous works <ref type=""bibr"" target=""#b1"">[1,</ref><ref type=""bibr"" target=""#b2"">2]</ref> propose to pre-compute d indicate poor convergence.2-layer 3-layer 4-layer 5-layer 6-layer 7-layer 8-layer Cluster-GCN with<ref type=""bibr"" target=""#b1"">(1)</ref> </figDesc><table><row><cell></cell><cell>90.3</cell><cell>97",0
"tate-of-the-art test F1 score 99.36 on the PPI dataset, while the previous best result was 98.71 by <ref type=""bibr"" target=""#b16"">[16]</ref>.</p></div> 			</abstract> 		</profileDesc> 	</teiHeader>",0
"is adopted and the number of hidden units is the same for all methods. Note that techniques such as <ref type=""bibr"" target=""#b11"">(11)</ref> is not considered here. In each experiment, we consider th Figure <ref type=""figure"" target=""#fig_3"">5</ref>. With the proposed diagonal enhancement technique <ref type=""bibr"" target=""#b11"">(11)</ref>, the convergence can be improved significantly and similar ger graph with over 2 millions of nodes and 61 million edges based on Amazon co-purchasing networks <ref type=""bibr"" target=""#b11"">[11,</ref><ref type=""bibr"" target=""#b12"">12]</ref>. The raw co-purcha",0
"sampled sizes for each layer (S 1 = 25, S 2 = 10) in GraphSAGE. We implement our method in PyTorch <ref type=""bibr"" target=""#b13"">[13]</ref>. For the other methods, we use all the original papers' co",0
"nd 61 million edges based on Amazon co-purchasing networks <ref type=""bibr"" target=""#b11"">[11,</ref><ref type=""bibr"" target=""#b12"">12]</ref>. The raw co-purchase data is from Amazon-3M <ref type=""foot",0
"atasets and continuously set new stateof-the-art performance <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" tar − L)X is understood as features averaging and propagation <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b27"">28]</ref>. In graph signal pr rast to the recent design principle of graph neural networks <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b28"">29]</ref>, our results sugges 9]</ref>. Started with the early success of ChebNet <ref type=""bibr"" target=""#b5"">[6]</ref> and GCN <ref type=""bibr"" target=""#b15"">[16]</ref> at vertex classification, many variants of GNN have been p e observe that the parameters of a graph convolutional layer in a Graph Convolutional Network (GCN) <ref type=""bibr"" target=""#b15"">[16]</ref> only contribute to overfitting. Similar observations have problem and provide insights to the mechanism underlying the most commonly used baseline model GCN <ref type=""bibr"" target=""#b15"">[16]</ref>, and its simplified variant SGC <ref type=""bibr"" target=""# onding NNs using true features.</p><p>Theorem 7 implies that, under Assumption 1, both gfNN and GCN <ref type=""bibr"" target=""#b15"">[16]</ref> have similar high performance. Since gfNN does not require Network model by removing nonlinearity in the neural network and only averaging features.</p><p>GCN <ref type=""bibr"" target=""#b15"">[16]</ref> Graph Convolutional Neural Network ($) is the most commonl",1
"hat backpropagation improves neither accuracy nor detectability of a GCN-based GNN model. Li et al. <ref type=""bibr"" target=""#b17"">[18]</ref> empirically analyzed GCN models with many layers under the",1
"transformations), GSP has inspired the development of learning algorithms on graph-structured data <ref type=""bibr"" target=""#b22"">[23]</ref>. In a standard signal processing problem, it is common to",0
"filter. In contrast to the recent design trend involving GCN <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b23"">24]</ref> , our results imply that simply stacking GCN layers might o cently GCN-based GNNs have been applied in many important applications such as point-cloud analysis <ref type=""bibr"" target=""#b23"">[24]</ref> or weakly-supervised learning <ref type=""bibr"" target=""#b8",0
"e=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b9"">10]</ref>, natural language processing <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b31"">32]</ref>, computer vision <re",0
"""#b25"">26,</ref><ref type=""bibr"" target=""#b28"">29]</ref>. Started with the early success of ChebNet <ref type=""bibr"" target=""#b5"">[6]</ref> and GCN <ref type=""bibr"" target=""#b15"">[16]</ref> at vertex",0
"acian is 2 if and only if the graph contains a non-trivial bipartite graph as a connected component <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"">Lemma 1.7]</ref>. Therefore, for other graph",0
"citly performing eigendecomposition on the normalized Laplacian matrix, which requires O(n 3 ) time <ref type=""bibr"" target=""#b24"">[25]</ref>. Here, we refer to this augmented normalized adjacency mat",0
"section, we introduce the basic concepts of graph signal processing. We adopt a recent formulation <ref type=""bibr"" target=""#b10"">[11]</ref> of graph Fourier transform on irregular graphs.</p><p>Let",0
"sumption 1. Thus, we first verify this assumption in realworld datasets: Cora, Citeseer, and Pubmed <ref type=""bibr"" target=""#b21"">[22]</ref>. These are citation networks, in which vertices are scient orks, social networks, and biological networks) commonly used for graph neural network benchmarking <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b29"">30,</ref><ref type=""bibr"" ta",0
"ing process, which leads to better generalization. We develop an approach based on label smoothness <ref type=""bibr"" target=""#b34"">[35,</ref><ref type=""bibr"" target=""#b37"">38]</ref>, which assumes tha therefore learnable <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b34"">35]</ref>. Inspired by these methods, we design a module of label smo get=""#b9"">[10,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b34"">35,</ref><ref type=""bibr"" target=""#b37"">38]</ref>. Therefore, more re r the unlabeled nodes l * u (E\V). To solve the issue, we propose minimizing the leave-one-out loss <ref type=""bibr"" target=""#b34"">[35]</ref>. Suppose we hold out a single item v and treat it unlabele",1
"and hybrid methods <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b27"">28]</ref>. However, these approaches rely on manual feature engineeri (3) Hybrid methods <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b27"">28]</ref> combine the above two categories and learn user/item embedd ecommender systems <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b30"">31,</ref><ref type=""bibr"" tar ized user preferences and interests. To account for the relational heterogeneity in KGs, similar to <ref type=""bibr"" target=""#b27"">[28]</ref>, we use a trainable and personalized relation scoring func where GNNs can be used directly, while here we investigate GNNs for heterogeneous KGs. Wang et al. <ref type=""bibr"" target=""#b27"">[28]</ref> use GCNs in KGs for recommendation, but simply applying GC a user-personalized weighted graph, which characterizes user's preferences. To this end, similar to <ref type=""bibr"" target=""#b27"">[28]</ref>, we use a user-specific relation scoring function s u (r ) ear that the performance of KGNN-LS with a non-zero λ is better than λ = 0 (the case of Wang et al. <ref type=""bibr"" target=""#b27"">[28]</ref>), which justifies our claim that LS regularization can ass",1
"ype=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" target=""#b35"">36]</ref>, embedding-based methods <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" targ g KG-aware recommender systems can be classified into three categories: (1) Embedding-based methods <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" targ",0
"[3, 5-7, 11, 15]</ref>. Recently, several works developed GNNs architecture for recommender systems <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" ta "" target=""#b31"">[32]</ref> applies GNNs to the pin-board bipartite graph in Pinterest. Monti et al. <ref type=""bibr"" target=""#b13"">[14]</ref> and Berg et al. <ref type=""bibr"" target=""#b18"">[19]</ref>",0
"o path-based methods <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" target=""#b35"">36]</ref>, embedding-based methods <ref type=""bibr"" target=""#b8"">[9,< ) Path-based methods <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" target=""#b35"">36]</ref> explore various patterns of connections among items in a KG",0
"rget=""#b8"">[9,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" target=""#b33"">34]</ref>, and hybrid methods <ref type=""bibr"" target=""#b17"">[18,</re rget=""#b8"">[9,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" target=""#b33"">34]</ref> pre-process a KG with knowledge graph embedding (KGE) <ref nt"" for Dianping-Food. The settings of dimension and learning rate are the same as SVD.</p><p>• CKE <ref type=""bibr"" target=""#b33"">[34]</ref> is a representative of embedding-based methods, which comb",0
"Internet applications to meet user's personalized interests and alleviate the information overload <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b28"">29,</ref><ref type=""bibr"" targ",0
"ution"" (i.e., weighted average) to local neighbors of a node <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b14"">15]</ref>.</p><p>Recently, rese",0
"edding-based methods <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" target=""#b33"">34]</ref>, and hybrid methods edding-based methods <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" target=""#b33"">34]</ref> pre-process a KG wi",0
"(2) Edge weights are parameterized and therefore learnable <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b34"">35]</ref>. Inspired by these ount of prior works <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b34"">35,</ref><ref type=""bibr"" tar",0
"actions, and predict user preference based on the parameters <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b13"">14]</ref>.</p><p>Generally speaking, there are two key components in iltering models replace the MF interaction function of inner product with nonlinear neural networks <ref type=""bibr"" target=""#b13"">[14]</ref>; and translation-based CF models instead use Euclidean dis ods have to rely on the interaction function to make up for the deficiency of suboptimal embeddings <ref type=""bibr"" target=""#b13"">[14]</ref>.</p><p>While intuitively useful to integrate user-item int an end-to-end fashion. In traditional recommender models like MF and neural collaborative filtering <ref type=""bibr"" target=""#b13"">[14]</ref>, these ID embeddings are directly fed into an interaction f inner product. Other more complicated choices, such as neural network-based interaction functions <ref type=""bibr"" target=""#b13"">[14]</ref>, are left to explore in the future work.</p></div> <div xm nlinear feature interactions between users and items. For instance, neural CF models, such as NeuMF <ref type=""bibr"" target=""#b13"">[14]</ref>, employ nonlinear neural networks as the interaction funct xploits the user-item direct interactions only as the target value of interaction function. • NeuMF <ref type=""bibr"" target=""#b13"">[14]</ref>: The method is a state-of-the-art neural CF model which us dding Layer</head><p>Following mainstream recommender models <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b25"">26]</ref>, we describe a user "">Model-based CF Methods</head><p>Modern recommender systems <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b31"">32]</ref> parameterize users target=""#b14"">15]</ref>. Towards this end, recent efforts <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" tar ity makes it insufficient to reveal the complex and nonlinear relationships between users and items <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b14"">15]</ref>. Towards this end, eness of top-K recommendation and preference ranking, we adopt two widely-used evaluation protocols <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b38"">39]</ref>: recall@K and ndcg",1
"olution operations <ref type=""bibr"" target=""#b28"">[29,</ref><ref type=""bibr"" target=""#b40"">41,</ref><ref type=""bibr"" target=""#b41"">42]</ref>. GC-MC <ref type=""bibr"" target=""#b28"">[29]</ref> applies th t is captured on the level of item relations, rather than the collective user behaviors. SpectralCF <ref type=""bibr"" target=""#b41"">[42]</ref> proposes a spectral convolution operation to discover all , is used as suggested in <ref type=""bibr"" target=""#b28"">[29]</ref>.</p><p>We also tried SpectralCF <ref type=""bibr"" target=""#b41"">[42]</ref> but found that the eigen-decomposition leads to high time",1
"exploits the user-item interaction graph to infer user preference. Early efforts, such as ItemRank <ref type=""bibr"" target=""#b6"">[7]</ref> and BiRank <ref type=""bibr"" target=""#b11"">[12]</ref>, adopt",0
"raph neural networks <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b30"">31,</ref><ref type=""bibr"" target=""#b36"">37]</ref>, which can be seen as constructing information flows in the >Next we build upon the message-passing architecture of GNNs <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b36"">37]</ref> in order to capture CF signal along the graph structure and ameters to learn, and it has been shown quite effectively in a recent work of graph neural networks <ref type=""bibr"" target=""#b36"">[37]</ref>, which refers to layer-aggregation mechanism. Finally, we ectively. This emphasizes the significance of layer-aggregation mechanism, which is consistent with <ref type=""bibr"" target=""#b36"">[37]</ref>.</p><p>4.4.3 Effect of Dropout. Following the prior work <",0
"target=""#b29"">30]</ref>, social relations <ref type=""bibr"" target=""#b32"">[33]</ref>, item relations <ref type=""bibr"" target=""#b35"">[36]</ref>, user reviews <ref type=""bibr"" target=""#b2"">[3]</ref>, and",0
"bibr"" target=""#b2"">[3]</ref>, and external knowledge graph <ref type=""bibr"" target=""#b30"">[31,</ref><ref type=""bibr"" target=""#b33"">34]</ref>. While inner product can force user and item embeddings of",0
"f structural information can be useful for understanding user behaviors, such as the cross features <ref type=""bibr"" target=""#b39"">[40]</ref> in context-aware and semantics-rich recommendation <ref ty",0
"g procedure, which is commonly used to make graph convolution network runnable on large-scale graph <ref type=""bibr"" target=""#b24"">[25]</ref>. We will analyze the complexity in Section 2.5.2.</p></div",0
"ensure data quality. Amazon-book: Amazon-review is a widely used dataset for product recommendation <ref type=""bibr"" target=""#b8"">[9]</ref>. We select Amazon-book from the collection. Similarly, we us",0
"atures <ref type=""bibr"" target=""#b39"">[40]</ref> in context-aware and semantics-rich recommendation <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b26"">27]</ref>, item knowledge gr",0
"r's short-term search history becomes more important as the search session progresses. White et al. <ref type=""bibr"" target=""#b50"">[51]</ref> reported the use of users' on-task behavior yielded promis us clicks or queries <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b42"">43,</ref><ref type=""bibr"" target=""#b50"">51]</ref>, or manually crafted rules are introduced to characterize t ssion to re-rank document for future queries. White et al. <ref type=""bibr"" target=""#b49"">[50,</ref><ref type=""bibr"" target=""#b50"">51]</ref> develop a rich set of statistical features to quantify cont et=""#b12"">[13,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b29"">30,</ref><ref type=""bibr"" target=""#b50"">51]</ref>. However, as the users' information need and behavior patte",1
"solutions build predictive models about users' search intent or future search behavior. Cao et al. <ref type=""bibr"" target=""#b5"">[6]</ref> model the development of users' search intent in search sess",0
",m = 0, if it was not clicked). In general, user clicks serve as a good proxy of relevance feedback <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b22"">23]</ref>, and they serve as",0
"e recent success of neural network based retrieval solutions <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" tar t <ref type=""bibr"" target=""#b42"">[43]</ref> 0.242 0.224 0.212 0.275 0.332 Single-task Learning DRMM <ref type=""bibr"" target=""#b13"">[14]</ref> 0.201 0.228 0.129 0.223 0.264 DSSM <ref type=""bibr"" target",0
"ntroduced to characterize the changes in a search sequence <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b52"">53]</ref>. Those algorithms' exploration of contextual information is tistical features to quantify context information from users' on-task search behavior. Xiang et al. <ref type=""bibr"" target=""#b52"">[53]</ref> craft a collection of rules to characterize the search con",0
"oncise calculation process by conducting the locating and integrating at the same time, rather than <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b11"">12]</ref> which need two sep",1
"attention to decide when to stop and then performing soft attention to calculate, also rather than <ref type=""bibr"" target=""#b12"">[13]</ref> which needs a CTC trained model to conduct pre-partition b shows a clear performance advantage than other soft and monotonic models (e.g. triggered attention <ref type=""bibr"" target=""#b12"">[13]</ref>), but also matches or surpasses most of the published resu int CTC-attention model / ESPNet <ref type=""bibr"" target=""#b15"">[16]</ref> 27.4 Triggered Attention <ref type=""bibr"" target=""#b12"">[13]</ref> 30 where the membrane potential Um is constantly simulated",1
"f> which needs a CTC trained model to conduct pre-partition before the attention decoding.</p><p>In <ref type=""bibr"" target=""#b13"">[14]</ref>, Li al. present the important Adaptive Computation Steps (",1
"on but also locates acoustic boundaries. And we find inspirations from the integrate-and-fire model <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b5"">6]</ref>.</p><p>Integrate-and-f",0
"ous end-to-end models. Among them, the attention-based model <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref>, which builds a soft alignment between each decoder step and",0
"ery encoder step, not only shows a performance advantage in comparison with other end-to-end models <ref type=""bibr"" target=""#b2"">[3]</ref>, but also successfully challenges the dominance of HMM-LSTM",0
"ng the same setup as <ref type=""bibr"" target=""#b14"">[15]</ref> for all datasets. Speed perturbation <ref type=""bibr"" target=""#b19"">[20]</ref> with fixed ± 10% is applied for all training datasets. The",0
"and accurate with the spring up of various end-to-end models. Among them, the attention-based model <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref>, which builds a soft a",0
"ery competitive on all test sets and significantly improves the results achieved by the Chain model <ref type=""bibr"" target=""#b32"">[33]</ref>. </p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head [33]</ref>. </p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>Model CER</head><p>Chain-TDNN <ref type=""bibr"" target=""#b32"">[33]</ref> 23.7 Self-attention Aligner <ref type=""bibr"" target=""#b14""",0
"d the soft and monotonic alignment in end-to-end ASR models. <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b9"">10]</ref>  to be a forward-movin o be a forward-moving window that fits gaussian distribution <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b8"">9]</ref> or even heuristic rule <ref type=""bibr"" target=""#b9"">[10]</re",0
"end-to-end ASR models. <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b9"">10]</ref>  to be a forward-moving window that fits gaussian distributi ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b8"">9]</ref> or even heuristic rule <ref type=""bibr"" target=""#b9"">[10]</ref>, where the center and width of the window are predicted by",0
"70, mT = 2, p = 0.2 are applied to all models except the base model on Librisppech. We use the BPE <ref type=""bibr"" target=""#b21"">[22]</ref> toolkit generating 3722 word pieces for Librispeech by mer",0
"labels for AISHELL-2 and 3674 output labels for HKUST .</p><p>We implement our model on TensorFlow <ref type=""bibr"" target=""#b22"">[23]</ref>. For the selfattention networks (SANs) in our model, we us",0
"rget=""#b14"">[15]</ref> 24.1 Transformer <ref type=""bibr"" target=""#b33"">[34]</ref> 26.6 Extended-RNA <ref type=""bibr"" target=""#b34"">[35]</ref> 26.8 Joint CTC-attention model / ESPNet <ref type=""bibr"" t",0
"idth is all set to 3 except the base model on Librispeech is set to 5. Besides, layer normalization <ref type=""bibr"" target=""#b23"">[24]</ref> and a ReLU activation are applied after this convolution.",0
"><p>Speech animation for rigged models. Several related methods produce animation curves for speech <ref type=""bibr"" target=""#b12"">[Edwards et al. 2016;</ref><ref type=""bibr"" target=""#b59"">Taylor et a",1
"type=""bibr"" target=""#b23"">[Hunt and Black 1996]</ref>. Inspired by the latter, the VoCo project of <ref type=""bibr"" target=""#b27"">Jin et al. [2017]</ref> performs a search in the existing recording t h. For all results in this paper we use either the built-in speech synthesizer in Mac OS X, or Voco <ref type=""bibr"" target=""#b27"">[Jin et al. 2017</ref>]. Note however that our video synthesis pipeli dd audio we use audio synthesized either by the built in speech synthesizer in Mac OS X, or by VoCo <ref type=""bibr"" target=""#b27"">[Jin et al. 2017</ref>]. An alternative is to obtain an actual record",0
""" target=""#b60"">Tewari et al. 2018a</ref><ref type=""bibr"" target=""#b62"">Tewari et al. , 2017;;</ref><ref type=""bibr"" target=""#b64"">Tran et al. 2017]</ref>. Besides model parameters, other approaches r",0
"r"" target=""#b6"">[Bregler et al. 1997;</ref><ref type=""bibr"" target=""#b9"">Chang and Ezzat 2005;</ref><ref type=""bibr"" target=""#b13"">Ezzat et al. 2002;</ref><ref type=""bibr"" target=""#b36"">Liu and Osterm",0
"ten and rearrange speech for audio podcasts <ref type=""bibr"" target=""#b52"">[Rubin et al. 2013;</ref><ref type=""bibr"" target=""#b56"">Shin et al. 2016</ref>], annotate video with review feedback <ref typ",0
"it feedback, implicit feedback is more difficult to utilize because of the lack of negative samples <ref type=""bibr"" target=""#b10"">[Pan et al., 2008]</ref>. Secondly, generating top-k preferred items it operation. Finding approximate top-K items can be even finished in sublinear or logarithmic time <ref type=""bibr"" target=""#b10"">[Wang et al., 2012;</ref><ref type=""bibr"" target=""#b10"">Muja and Lowe n finished in sublinear or logarithmic time <ref type=""bibr"" target=""#b10"">[Wang et al., 2012;</ref><ref type=""bibr"" target=""#b10"">Muja and Lowe, 2009]</ref> by making use of index technique.</p><p>Se bibr"">[Zhou and Zha, 2012]</ref>, PPH <ref type=""bibr"" target=""#b12"">[Zhang et al., 2014]</ref>, CH <ref type=""bibr"" target=""#b10"">[Liu et al., 2014]</ref> incur large quantization loss <ref type=""bib rm for f (x) and β is its penalty coefficient. <ref type=""bibr"">[Giannessi and Tardella, 1998;</ref><ref type=""bibr"" target=""#b10"">Lucidi and Rinaldi, 2010]</ref> show that the above two problems are 13)</label></formula><p>In terms of the loss function, we employ the popular and effective BPR loss <ref type=""bibr"" target=""#b10"">[Rendle et al., 2009]</ref>. In particular, given a user matrix U and",1
"ices but it may incur the loss of information during the training process. To this end, inspired by <ref type=""bibr"" target=""#b3"">[Dai et al., 2016]</ref>, we transform the binary optimization problem",1
"trained GCN-CF model into a binarized model (DGCN-BinCF) with knowledge distillation technique (KD <ref type=""bibr"" target=""#b7"">[Hinton et al., 2015]</ref>). To be more specific, we introduce a nove e distance one. So it is easy to fall into local optima.</p><p>2.3 Distilling Knowledge for Ranking <ref type=""bibr"" target=""#b7"">[Hinton et al., 2015]</ref> was the first one that proposed method ""Kn utions via softmax function, and utilize cross entropy for penalizing the discrepancy. According to <ref type=""bibr"" target=""#b7"">[Hinton et al., 2015]</ref>, combining L Bin−CF with L rank as a multi",0
"k] in GCN-CF model, we hope that the rank keeps [i, j, k] in the binary model. According to ListNet <ref type=""bibr"" target=""#b1"">[Cao et al., 2007]</ref>, we can characterize sorting information in t",0
">[Zheng et al., 2018]</ref> combined collaborative filtering model with graph convolutional network <ref type=""bibr"" target=""#b6"">[Henaff et al., 2015]</ref> to mine hidden interactions between users tween distributions of sampled negative items. Noting that learning hash codes is generally NP-hard <ref type=""bibr"" target=""#b6"">[Håstad, 2001]</ref>, approximation methods are appropriate choices bu",0
"on. Some two-stage approximation methods like BCCF <ref type=""bibr"">[Zhou and Zha, 2012]</ref>, PPH <ref type=""bibr"" target=""#b12"">[Zhang et al., 2014]</ref>, CH <ref type=""bibr"" target=""#b10"">[Liu et 14]</ref>, CH <ref type=""bibr"" target=""#b10"">[Liu et al., 2014]</ref> incur large quantization loss <ref type=""bibr"" target=""#b12"">[Zhang et al., 2016]</ref>, and a direct optimization model DCF <ref loss <ref type=""bibr"" target=""#b12"">[Zhang et al., 2016]</ref>, and a direct optimization model DCF <ref type=""bibr"" target=""#b12"">[Zhang et al., 2016]</ref> is easy to fall into a local optimum becau ref> which relaxed binary constraints at first and then quantified binary codes.</p><p>Nonetheless, <ref type=""bibr"" target=""#b12"">[Zhang et al., 2016]</ref> proposed that those two-stage methods suff sentation. Later, following this, some two stage methods <ref type=""bibr"">[Zhou and Zha, 2012;</ref><ref type=""bibr"" target=""#b12"">Zhang et al., 2014]</ref> which relaxed binary constraints at first a",0
"shing for Collaborative Filtering</head><p>A pioneer work was to exploit Locality-Sensitive Hashing <ref type=""bibr"" target=""#b5"">[Datar et al., 2004]</ref> to generate binary codes for Google News re",0
"GCN to solve it such as SpectralCF <ref type=""bibr"" target=""#b13"">[Zheng et al., 2018]</ref>, GCMC <ref type=""bibr"" target=""#b0"">[Berg et al., 2017]</ref>, <ref type=""bibr"">RMGCNN [Monti et al., 2017",0
"items for each user is extremely time-consuming.</p><p>For the first problem, recently, Spec-tralCF <ref type=""bibr"" target=""#b13"">[Zheng et al., 2018]</ref> combined collaborative filtering model wit users and items from spectral domain, which showed enormous potential for implicit feedback problem <ref type=""bibr"" target=""#b13"">[Zheng et al., 2018]</ref>. However, SpectralCF ignores high-order fe bipartite graph is crucial for implicit feedback. Some work used GCN to solve it such as SpectralCF <ref type=""bibr"" target=""#b13"">[Zheng et al., 2018]</ref>, GCMC <ref type=""bibr"" target=""#b0"">[Berg used as the initialization of P ,Q in DGCN-BinCF. All parameters of SpectralCF are set according to <ref type=""bibr"" target=""#b13"">[Zheng et al., 2018]</ref>.</p><p>Besides, for DCF, BCCF and PPH, we",0
"d n=""1"">Introduction</head><p>Nowadays, recommender systerms are widely used in people's daily life <ref type=""bibr"" target=""#b9"">[Liu et al., 2011;</ref><ref type=""bibr"" target=""#b8"">Lian et al., 201",0
"f type=""bibr"" target=""#b2"">[3]</ref>, it seems worthwhile to be unbiased, including negative deltas <ref type=""bibr"" target=""#b3"">[4]</ref> as well. 4) Matrices that are too sparse or empty (mcf_s1536",1
"Proposal L1 &amp; L2' and KPCP over the single-core runs with non-prefetch (i.e. ?(IPCi/IPCalone_i) <ref type=""bibr"" target=""#b7"">[8]</ref>). It is clear that the multi-level prefetcher performs gener",0
"a Cache (Markov-chain)</head><p>Associativity: 16</p><p>Set index: Delta ?{-63, ..., 63} Delta Next <ref type=""bibr"" target=""#b6"">(7)</ref>, LFU bits (8) With respect to the replacement, we use an app ge cache. The page cache is set-associative, indexed by the page address. Page Tag (10), Delta Prev <ref type=""bibr"" target=""#b6"">(7)</ref>, Offset Prev <ref type=""bibr"" target=""#b5"">(6)</ref>, NRU bi transitions. There are similar approaches in related work <ref type=""bibr"" target=""#b4"">[5]</ref>, <ref type=""bibr"" target=""#b6"">[7]</ref>. This does not modify the decision part of Pangloss, as it o he Best-Offset Prefetcher <ref type=""bibr"" target=""#b2"">[3]</ref> (BOP) and the prefetcher from KPC <ref type=""bibr"" target=""#b6"">[7]</ref> (KPCP). The first was the winner of the previous Data Prefet",0
"ubsequent accesses. Distance prefetching is a generalisation of the common Markov model prefetchers <ref type=""bibr"" target=""#b0"">[1]</ref>, that uses deltas instead of addresses to build more general short repeating delta patterns, such as 1, 1, 2, 1, 3, 1, 1, 2, 1, 3, ..., the transitions (1, 1), <ref type=""bibr"" target=""#b0"">(1,</ref><ref type=""bibr"" target=""#b1"">2)</ref> and <ref type=""bibr"" t ansitions (1, 1), <ref type=""bibr"" target=""#b0"">(1,</ref><ref type=""bibr"" target=""#b1"">2)</ref> and <ref type=""bibr"" target=""#b0"">(1,</ref><ref type=""bibr"" target=""#b2"">3)</ref> would yield an equal p",0
"next line/sequential prefetcher).</p><p>3) Instead of only supporting a limited coverage of deltas <ref type=""bibr"" target=""#b2"">[3]</ref>, it seems worthwhile to be unbiased, including negative delt e the performance of our prefetcher to two state-of-the art prefetchers, the Best-Offset Prefetcher <ref type=""bibr"" target=""#b2"">[3]</ref> (BOP) and the prefetcher from KPC <ref type=""bibr"" target=""# remains relatively efficient, although allowing a delay could also prove beneficial for timeliness <ref type=""bibr"" target=""#b2"">[3]</ref>.</p><p>According to the use case, many parameters that impac et=""#b0"">(1,</ref><ref type=""bibr"" target=""#b1"">2)</ref> and <ref type=""bibr"" target=""#b0"">(1,</ref><ref type=""bibr"" target=""#b2"">3)</ref> would yield an equal probability. This in combination with ot",0
"e most widely used one, <ref type=""bibr"">BERT [Devlin et al., 2018]</ref> builds on the Transformer <ref type=""bibr"" target=""#b4"">[Vaswani et al., 2017]</ref> architecture and improves the pre-trainin -c.org/ns/1.0""><head n=""4.1"">Background of BERT</head><p>Based on a multi-layer Transformer encoder <ref type=""bibr"" target=""#b4"">[Vaswani et al., 2017]</ref> (The transformer architecture has been ub /1.0""><head>Visit Embedding</head><p>Similar to BERT, we use a multi-layer Transformer architecture <ref type=""bibr"" target=""#b4"">[Vaswani et al., 2017]</ref> as our visit encoder. The model takes the",1
"doctors in making medication recommendation <ref type=""bibr"" target=""#b5"">[Xiao et al., 2018a;</ref><ref type=""bibr"" target=""#b3"">Shang et al., 2019;</ref><ref type=""bibr"" target=""#b0"">Baytas et al., xample, a large number of patients who only have one hospital visit were discarded from training in <ref type=""bibr"" target=""#b3"">[Shang et al., 2019]</ref>.</p><p>2. Lack of hierarchical knowledge: F ge classification <ref type=""bibr"" target=""#b2"">[Hinton et al., 2006]</ref> and machine translation <ref type=""bibr"" target=""#b3"">[Ramachandran et al., 2016]</ref>. The unsupervised pre-training can b ady been demonstrated useful on EHR modeling <ref type=""bibr"" target=""#b1"">[Choi et al., 2017;</ref><ref type=""bibr"" target=""#b3"">Shang et al., 2019]</ref>. <ref type=""bibr"">GRAM [Choi et al., 2017]</",1
"bibr"" target=""#b5"">[Xiao et al., 2018a;</ref><ref type=""bibr"" target=""#b3"">Shang et al., 2019;</ref><ref type=""bibr"" target=""#b0"">Baytas et al., 2017;</ref><ref type=""bibr"">Choi et al., 2018;</ref><re n previous works for improving medical code representations <ref type=""bibr"">[Ma et al., 2018;</ref><ref type=""bibr"" target=""#b0"">Baytas et al., 2017;</ref><ref type=""bibr"">Choi et al., 2018]</ref>, t",0
"oral dependencies among clinical events, see <ref type=""bibr"" target=""#b1"">[Choi et al., 2016;</ref><ref type=""bibr"" target=""#b6"">Xiao et al., 2018b;</ref><ref type=""bibr"" target=""#b2"">Lipton et al.,",0
"ethod based on BERT for Instance-based methods focus on current health conditions. Among them, Leap <ref type=""bibr"" target=""#b7"">[Zhang et al., 2017]</ref> formulates a multi-instance multi-label lea",0
"l contributions:</p><p>1. Pre-training to leverage more data: Pre-training techniques, such as ELMo <ref type=""bibr"" target=""#b2"">[Peters et al., 2018]</ref>, OpenAI <ref type=""bibr"">GPT [Radford et a ""bibr"" target=""#b1"">[Choi et al., 2016;</ref><ref type=""bibr"" target=""#b6"">Xiao et al., 2018b;</ref><ref type=""bibr"" target=""#b2"">Lipton et al., 2015]</ref>. Among them, <ref type=""bibr"">RETAIN [Choi ions. Pre-training has been shown extremely effective in various areas such as image classification <ref type=""bibr"" target=""#b2"">[Hinton et al., 2006]</ref> and machine translation <ref type=""bibr"" t ef type=""bibr"">[Erhan et al., 2010]</ref>. Recently, language model pre-training techniques such as <ref type=""bibr"" target=""#b2"">[Peters et al., 2018;</ref><ref type=""bibr"">Radford et al., 2018;</ref osed to encode the graph-structure information, including graph convolutional neural networks (GCN) <ref type=""bibr"" target=""#b2"">[Kipf and Welling, 2017]</ref>, message passing networks (MPNN) <ref t .tei-c.org/ns/1.0""><head n=""5.1"">Experimental Setting Data</head><p>We used EHR data from MIMIC-III <ref type=""bibr"" target=""#b2"">[Johnson et al., 2016]</ref> and conducted all our experiments on a co 300 and thershold for final prediction as 0.3 for better performance. Training is done through Adam <ref type=""bibr"" target=""#b2"">[Kingma and Ba, 2014]</ref> at learning rate 5e-4. We fix the best mod",0
"diagnoses. Longitudinal-based methods leverage the temporal dependencies among clinical events, see <ref type=""bibr"" target=""#b1"">[Choi et al., 2016;</ref><ref type=""bibr"" target=""#b6"">Xiao et al., 20 ref type=""bibr"" target=""#b2"">[Peters et al., 2018;</ref><ref type=""bibr"">Radford et al., 2018;</ref><ref type=""bibr"" target=""#b1"">Devlin et al., 2018]</ref> have shown to largely improve the performan (GCN) <ref type=""bibr"" target=""#b2"">[Kipf and Welling, 2017]</ref>, message passing networks (MPNN) <ref type=""bibr"" target=""#b1"">[Gilmer et al., 2017]</ref>, graph attention networks (GAT) <ref type= =""bibr"">[Velickovic et al., 2017]</ref>. GNNs have already been demonstrated useful on EHR modeling <ref type=""bibr"" target=""#b1"">[Choi et al., 2017;</ref><ref type=""bibr"" target=""#b3"">Shang et al., 2 whether one sentence is the next sentence of the other.</p><p>A typical input to BERT is as follows <ref type=""bibr"" target=""#b1"">( [Devlin et al., 2018]</ref>):</p><formula xml:id=""formula_4"">Input = p>It is worth mentioning that our graph embedding method on medical ontology is different from GRAM <ref type=""bibr"" target=""#b1"">[Choi et al., 2017]</ref> from the following two aspects: 1. Initializ with weight as 1.1. For deep learning models, we implemented RNN using a gated recurrent unit (GRU) <ref type=""bibr"" target=""#b1"">[Cho et al., 2014]</ref> and utilize dropout with a probability of 0.4",0
"number of deep learning models were proposed to assist doctors in making medication recommendation <ref type=""bibr"" target=""#b5"">[Xiao et al., 2018a;</ref><ref type=""bibr"" target=""#b3"">Shang et al.,",0
"arize the distribution of zero weights, so that more all-zero rows and columns can be found. SNrram <ref type=""bibr"" target=""#b43"">[44]</ref> seeks to enable fine-grained column compression at the cos found. However, with ReCom, there are still many zero weights left in the compressed model. SNrram <ref type=""bibr"" target=""#b43"">[44]</ref> compresses the model at a finer level, i.e., allzero filte g OU rows are removed to reduce unnecessary computations. Note that we does not compare with SNrram <ref type=""bibr"" target=""#b43"">[44]</ref>, as SNrram uses model-based compression and its crossbar a /ref>, we also use arrows to indicate the weight compression ratio that can be obtained from SNrram <ref type=""bibr"" target=""#b43"">[44]</ref>. As SNrram uses model-based compression (i.e., finegrained , only all-zero rows can be removed; many zero weights still remain in the compressed model. SNrram <ref type=""bibr"" target=""#b43"">[44]</ref> is another ReRAMbased sparse DNN accelerator that compress utput indexing overhead.</p><p>Existing sparsity solutions <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b43"">44]</ref> are based on an over-idealized ReRAM crossbar architecture",1
"e=""bibr"" target=""#b26"">[27]</ref>, CIFAR-10 <ref type=""bibr"" target=""#b24"">[25]</ref>, and ImageNet <ref type=""bibr"" target=""#b10"">[11]</ref>. MNIST and CIFAR-10 are small-scale datasets, whereas Imag",0
"ata storage are separate, emerging memristor devices such as resistive random access memory (ReRAM) <ref type=""bibr"" target=""#b21"">[22]</ref> are able to perform arithmetic operations beyond data stor "">42,</ref><ref type=""bibr"" target=""#b46"">47]</ref>, we use one of the oxide-based ReRAM, WOx ReRAM <ref type=""bibr"" target=""#b21"">[22]</ref>, for our evaluation. We choose the R-ratio and resistance- 1"">[22]</ref>, for our evaluation. We choose the R-ratio and resistance-deviation (? ) of WOx ReRAM <ref type=""bibr"" target=""#b21"">[22]</ref> as the baseline setting (R b and ? b ), and analyze the in o/? to increase/shrink by 1 The evaluated NN models are described in Section 6. 3x of the WOx ReRAM <ref type=""bibr"" target=""#b21"">[22]</ref>, the inference accuracy drops considerably, especially at logy improvements enable the R-ratio/resistance-deviation to increase/shrink by 3x of the WOx ReRAM <ref type=""bibr"" target=""#b21"">[22]</ref>. 4 We synthesize the circuits under 28nm process as we are",0
"based ReRAM has good electronic properties (high density, low switching energy, and high endurance) <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b45"">46]</ref> and thus is common",0
"it sparsity, which is commonly done in digital CMOS-based accelerators to improve energy efficiency <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target ines sequentially. In this example, to compute the output neuron for the first input sliding window <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target matrix-vector multiplication of OU1 and the LSB of the first two values in the input sliding window <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"">0]</ref> is performed to obtain the output < <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"">0]</ref> is performed to obtain the output <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"">0]</ref>. The matrix-vector multiplication o e matrix-vector multiplication of OU2 and the LSB of the rest of values in the input sliding window <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b0"">1]</ref> is then performed at t echanisms to exploit weight sparsity <ref type=""bibr"" target=""#b48"">[49]</ref>, activation sparsity <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b7"">8]</ref>, or both <ref type=""bi "" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b0"">1]</ref>, the data we feed into the input register after decomposition nd the LSB of the rest of values in the input sliding window <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b0"">1]</ref> is then performed at the second cycle to yield the output <re ultiplier when an input activation is zero to save energy. Another sparse DNN accelerator, Cnvlutin <ref type=""bibr"" target=""#b0"">[1]</ref>, selects only non-zero activation values for delivery as mul",0
"ctural modifications <ref type=""bibr"">[10, 11, 18-20, 20, 22]</ref> or static control flow analysis <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b27"">28]</ref>. However, in WSC e ume some control flow and determine constructs such as loops which might be amenable to prefetching <ref type=""bibr"" target=""#b23"">[24]</ref>. In contrast, our approach leverages AsmDB to augment heur nted by w in Figure <ref type=""figure"" target=""#fig_4"">14</ref>. Existing compiler based approaches <ref type=""bibr"" target=""#b23"">[24]</ref> insert prefetches at a fixed number of instructions before by prefetching, but low-fan-in targets will incur significantly smaller overheads. Prior approaches <ref type=""bibr"" target=""#b23"">[24]</ref> had to aggressively insert prefetches in all paths to obta s so far, and each assumes significant complexity, cost, and risk to implement.</p><p>Luk and Mowry <ref type=""bibr"" target=""#b23"">[24]</ref> have the most similar approach to reducing i-cache misses",1
"truction of dynamic program control flow. Collection is built on top of the data source for AutoFDO <ref type=""bibr"" target=""#b4"">[5]</ref>, which similarly uses LBRs to reconstruct basic block execut rofiles <ref type=""bibr"" target=""#b24"">[25]</ref>, especially with sampling approaches like AutoFDO <ref type=""bibr"" target=""#b4"">[5]</ref>.</p><p>The high degree of fragmentation we observed suggests rofiling trace their beginnings to DCPI <ref type=""bibr"" target=""#b0"">[1]</ref>. Of these, Aut-oFDO <ref type=""bibr"" target=""#b4"">[5]</ref> (built on top of Google-wide-profiling (GWP) <ref type=""bibr",1
"<p>Datacenter-wide profiling. Modern systems for always-on profiling trace their beginnings to DCPI <ref type=""bibr"" target=""#b0"">[1]</ref>. Of these, Aut-oFDO <ref type=""bibr"" target=""#b4"">[5]</ref>",1
"the L1 or L2 instruction caches.</p><p>Simulation. We use a modified version of the ZSim simulator <ref type=""bibr"" target=""#b30"">[31]</ref>. We included a trace-driven execution mode, as well as mod",0
"torage, making them difficult to implement in commercial processors.</p><p>More recently, Boomerang <ref type=""bibr"" target=""#b21"">[22]</ref> combines fetch-directed instruction prefetching <ref type=",0
"over 100 times larger than the size of a L1 instruction cache (i-cache) and can easily overwhelm it <ref type=""bibr"" target=""#b1"">[2]</ref>. Studies show it expanding at rates of over 20% per year <re",0
"cale Computers (WSC) <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b25"">26]</ref>. The continued growth in cloud-based, digital services has",0
"e=""bibr"" target=""#b15"">[16]</ref> and on isolated benchmarks <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b16"">17]</ref>, have previously id",0
"kilobytes per core <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b19"">20]</ref> or per chip <ref type=""bibr"" target=""#b17"">[18]</ref>. Rece f on-chip storage <ref type=""bibr"" target=""#b17"">[18]</ref><ref type=""bibr"" target=""#b18"">[19]</ref><ref type=""bibr"" target=""#b19"">[20]</ref>. However, they still require several megabytes of total ch",0
"torage costs in the range of hundreds of kilobytes per core <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b19"">20]</ref> or per chip <ref ty",0
"her than the worst cases in desktop-class applications, commonly represented by SPEC CPU benchmarks <ref type=""bibr"" target=""#b8"">[9]</ref>.</p><p>Because the performance of a general-purpose processo forts, both on production WSCs <ref type=""bibr"" target=""#b15"">[16]</ref> and on isolated benchmarks <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" targ",0
", they typically have enormous on-chip storage costs in the range of hundreds of kilobytes per core <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" tar",0
"ions. (For instance, in SPEC CPU2006, mcf has an IPC of 0.2 whereas dealII has an IPC of almost 2.0 <ref type=""bibr"" target=""#b26"">[27]</ref>.) Our approach addresses this issue by leveraging per-appl",0
"tion in addition to instruction cache misses. However, it is still limited by BTB capacity. Shotgun <ref type=""bibr"" target=""#b20"">[21]</ref> addresses this limitation by optimizing BTB storage for ma",0
"tchers have reduced the required amount of on-chip storage <ref type=""bibr"" target=""#b17"">[18]</ref><ref type=""bibr"" target=""#b18"">[19]</ref><ref type=""bibr"" target=""#b19"">[20]</ref>. However, they st",0
"/head><p>Perceptron learning for microarchitectural prediction was introduced for branch prediction <ref type=""bibr"" target=""#b19"">[20]</ref>. Our predictor uses a version of microarchitectural percep on all levels of cache hierarchies. Branch prediction is done using the perceptron branch predictor <ref type=""bibr"" target=""#b19"">[20]</ref>. The page size is 4KB. ChampSim operates all the prefetche org/ns/1.0""><head n=""7.5"">Perceptrons in Cache Management</head><p>In addition to branch prediction <ref type=""bibr"" target=""#b19"">[20]</ref>, perceptron-based learning has been applied to the area of",1
"</ref>. Modern prefetching mechanisms are more sophisticated as they look into past memory behavior <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b10"">11]</ref>, locality <ref type",0
"ing approach.</p><p>Wang and Lou propose a similar work where perceptrons filter useless prefetches <ref type=""bibr"" target=""#b42"">[43]</ref>. In their design's primary focus was on improving the accu",0
"]</ref>, locality <ref type=""bibr"" target=""#b11"">[12]</ref><ref type=""bibr"" target=""#b12"">[13]</ref><ref type=""bibr"" target=""#b13"">[14]</ref><ref type=""bibr"" target=""#b14"">[15]</ref><ref type=""bibr"" t",0
"weights are then thresholded to generate a prediction. When a block from one of a few sampled sets <ref type=""bibr"" target=""#b43"">[44]</ref> is reused or evicted, the corresponding weights are decrem",0
"o the root causes of relatively low IPC using the Top-down Microarchitecture Analysis Method (TMAM) <ref type=""bibr"" target=""#b62"">[63]</ref> to categorize processor pipelines' execution stalls, as re f> comprises both latency-sensitive and throughput-oriented scale-out cloud workloads. Yasin et al. <ref type=""bibr"" target=""#b62"">[63]</ref> perform a microarchitectural characterization of several C mark suite studies <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b62"">63,</ref><ref type=""bibr"" target=""#b95"">96,</ref><ref type=""bibr"" tar",1
"re negligible for others. For example, microsecondscale overheads that arise from accesses to Flash <ref type=""bibr"" target=""#b39"">[40]</ref>, emerging memory technologies like 3D XPoint by Intel and",0
"3"">[104]</ref><ref type=""bibr"" target=""#b104"">[105]</ref><ref type=""bibr"" target=""#b105"">[106]</ref><ref type=""bibr"" target=""#b106"">[107]</ref>, or reducing front-end stalls <ref type=""bibr"" target=""#",0
"heterogeneity <ref type=""bibr"" target=""#b100"">[101]</ref><ref type=""bibr"" target=""#b101"">[102]</ref><ref type=""bibr"" target=""#b102"">[103]</ref>, trading memory latency/bandwidth <ref type=""bibr"" targe",0
"each configurable knob.</p><p>(1) Core frequency. Our servers enable Intel's Turbo Boost technology <ref type=""bibr"" target=""#b78"">[79]</ref>. ?SKU scales core frequency from 1.6 GHz to 2.2 GHz (defau and QPI controller, etc.) CPU components. The current production configuration enables Turbo Boost <ref type=""bibr"" target=""#b78"">[79]</ref> and runs Web (Skylake and Broadwell) at 2.2 GHz and Ads1 a",0
"have explored cross-domain generalization ability of recognition models. For example, recent works <ref type=""bibr"" target=""#b19"">[20,</ref><ref type=""bibr"" target=""#b49"">50]</ref> borrow the ideas f ethods such as transfer learning <ref type=""bibr"" target=""#b49"">[50]</ref> and adversarial learning <ref type=""bibr"" target=""#b19"">[20]</ref> latently generate features of data samples in the target d ns are added. As an example, we evaluate the performance of an adversarial learning based model, EI <ref type=""bibr"" target=""#b19"">[20]</ref> over different domain factors (e.g., environment, location ral alternative state-of-the-arts methodologies, CARM <ref type=""bibr"" target=""#b43"">[44]</ref>, EI <ref type=""bibr"" target=""#b19"">[20]</ref> and CrossSense <ref type=""bibr"" target=""#b49"">[50]</ref>, ually adopted to shift the task of separating gesture-related features from domain-related ones. EI <ref type=""bibr"" target=""#b19"">[20]</ref> incorporates an adversarial network to obtain domain-indep =""#b52"">53]</ref> and developing domain-independent features <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" target=""#b36"">37]</ref>. In the former type",1
"#b31"">32,</ref><ref type=""bibr"" target=""#b43"">44]</ref>, ToF <ref type=""bibr"" target=""#b1"">[2]</ref><ref type=""bibr"" target=""#b2"">[3]</ref><ref type=""bibr"" target=""#b3"">[4]</ref><ref type=""bibr"" targe devices used, parameters with different extent of accuracy and resolution can be obtained. WiTrack <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b3"">4]</ref> develops FMCW radar wi",0
">[8,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b35"">36]</ref> or sonar <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b28"">29,</ref><ref type=""bibr"" ta",0
"ue to complexity of human activity, existing approaches extract signal features, either statistical <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" ta",0
"get=""#b14"">15,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b29"">30,</ref><ref type=""bibr"" target=""#b44"">45,</ref><ref type=""bibr"" tar strength distribution of commercial Wi-Fi signals and KNN to recognize human activities. Niu et al. <ref type=""bibr"" target=""#b29"">[30]</ref> uses signal waveforms for fine-grained gesture recognition",0
"c.org/ns/1.0""><head n=""4.2.2"">Predicting Performance in Solo-Mode.</head><p>Referring to prior work <ref type=""bibr"" target=""#b11"">[12]</ref>, we design shadow solo-cycle accounting (SSCA) approach to e prediction method of QoSMT is inspired by PTA. PTA uses MLP correction to achieve higher accuracy <ref type=""bibr"" target=""#b11"">[12]</ref>. However, we can not get an application's MLP without offl SMT throughput and fairness, but they did not take performance control into account. Eyerman et al. <ref type=""bibr"" target=""#b11"">[12]</ref> proposed the per-thread cycle accounting (PTA) mechanism t",1
"interference, We divide these shared resources into three categories according to previous studies <ref type=""bibr"" target=""#b10"">[11]</ref>.</p><p>Front-end resources: Instruction fetch unit is the nges. Transparent thread also aims to maximize HPT's performance with reasonable overall throughput <ref type=""bibr"" target=""#b10"">[11]</ref>. Unlike QoSMT, it does not support precise performance con",0
"ation. To address this issue, Intel's processors recently support Cache Allocation Technology (CAT) <ref type=""bibr"" target=""#b17"">[18]</ref> for L3 cache. But L1 and L2 caches shared by multiple thre",0
"re processors, each physical core supports two to eight logical threads. Intel and AMD's processors <ref type=""bibr"" target=""#b1"">[2]</ref> usually have two logical threads while IBM's POWER8 <ref typ",0
"><p>To obtain typical workload pairs among these benchmarks, we refer to the balanced random method <ref type=""bibr"" target=""#b29"">[32]</ref> to select 48 pairs of benchmarks for experiments. Then we",0
"y information for character-based model. To integrate words information into character-based model, <ref type=""bibr"" target=""#b40"">Zhang and Yang (2018)</ref> propose a lattice-structured LSTM model t characterbased model. The character baseline denotes the original character-based BiLSTM-CRF model. <ref type=""bibr"" target=""#b40"">Zhang and Yang (2018)</ref> propose a lattice LSTM to exploit word in y information into Chinese NER task. Another way to obtain word boundary information is proposed by <ref type=""bibr"" target=""#b40"">(Zhang and Yang, 2018)</ref>, using a lattice LSTM to integrate word where b &lt; i and c b,i matches a word in lexicon D. The lexicon D is the same as the one used in <ref type=""bibr"" target=""#b40"">(Zhang and Yang, 2018)</ref>, which is built by using automatically s x c i with x − → ws i to utilize word information. And this is quite different from the way used in <ref type=""bibr"" target=""#b40"">(Zhang and Yang, 2018)</ref>, since they use extra shortcut paths to j , y j )}| N j=1 , we minimize the sentence-level negative loglikelihood loss to train the model:  <ref type=""bibr"" target=""#b40"">(Zhang and Yang, 2018)</ref>.</p><formula xml:id=""formula_16"">L = − j rget=""#tab_0"">1</ref>. Implementation Details. We utilize the character and word embeddings used in <ref type=""bibr"" target=""#b40"">(Zhang and Yang, 2018)</ref>, both of which are pre-trained on Chines ng, 2018)</ref>, both of which are pre-trained on Chinese Giga-Word using word2vec model. Following <ref type=""bibr"" target=""#b40"">(Zhang and Yang, 2018)</ref>, we use the word embedding dictionary as with other parameters.</p><p>For hyper-parameter configurations, we mostly refer to the settings in <ref type=""bibr"" target=""#b40"">(Zhang and Yang, 2018)</ref>. We set both character embedding size an > 91.28 90.62 90.95 <ref type=""bibr"" target=""#b0"">Cao et al. (2018)</ref> 91.73 89.58 90.64 Lattice <ref type=""bibr"" target=""#b40"">(Zhang and Yang, 2018)</ref>   approach to integrating word informati n Chinese Resume dataset. Consistent with the previous results, our models outperform lattice model <ref type=""bibr"" target=""#b40"">(Zhang and Yang, 2018)</ref>. The above experimental results strongly some comparative experiments on training time and convergence speed. The lattice model proposed in <ref type=""bibr"" target=""#b40"">(Zhang and Yang, 2018)</ref> is our principal comparison object, sinc ns/1.0"" type=""table"" xml:id=""tab_5""><head></head><label></label><figDesc>are the most common methods<ref type=""bibr"" target=""#b40"">Zhang and Yang, 2018)</ref> 94.81 94.11 94.46 Character baseline 93.2",1
"(2016)</ref> first proposed to jointly train Chinese NER with Chinese word segmentation(CWS) task. <ref type=""bibr"" target=""#b0"">Cao et al. (2018)</ref> apply adversarial transfer learning framework 3)</ref> 91.86 88.75 90.28 <ref type=""bibr"" target=""#b7"">Dong et al. (2016)</ref> 91.28 90.62 90.95 <ref type=""bibr"" target=""#b0"">Cao et al. (2018)</ref> 91.73 89.58 90.64 Lattice <ref type=""bibr"" tar ef type=""bibr"" target=""#b7"">Dong et al. (2016)</ref> exploit radical features in Chinese character. <ref type=""bibr"" target=""#b0"">Cao et al. (2018)</ref> joint train Chinese NER task with Chinese word ance. Multi-task learning <ref type=""bibr"">(Peng and</ref><ref type=""bibr"">Dredze, 2015, 2016;</ref><ref type=""bibr"" target=""#b0"">Cao et al., 2018)</ref> and semi-supervised learning <ref type=""bibr""",0
"y Models(ME) <ref type=""bibr"" target=""#b3"">(Chieu and Ng, 2003)</ref>, Support Vector Machines(SVM) <ref type=""bibr"" target=""#b8"">(Ekbal and Bandyopadhyay, 2010)</ref> and Conditional Random Fields(CR",0
"underline) denote predicted labels, and blue labels(with underline) denote gold labels.</p><p>works <ref type=""bibr"" target=""#b13"">(Huang et al., 2015;</ref><ref type=""bibr"" target=""#b15"">Lample et al target=""#b6"">Collobert et al. (2011)</ref> replace the hand-crafted features with word embeddings. <ref type=""bibr"" target=""#b13"">Huang et al. (2015)</ref> propose a BiLSTM-CRF model for NER and achi",0
"r"" target=""#b13"">(Huang et al., 2015;</ref><ref type=""bibr"" target=""#b15"">Lample et al., 2016;</ref><ref type=""bibr"" target=""#b11"">Habibi et al., 2017)</ref> have been introduced to NER task. To avoid",0
"=""#b10"">[11]</ref><ref type=""bibr"" target=""#b11"">[12]</ref><ref type=""bibr"" target=""#b12"">[13]</ref><ref type=""bibr"" target=""#b13"">[14]</ref><ref type=""bibr"" target=""#b14"">[15]</ref><ref type=""bibr"" t static pre-trained models <ref type=""bibr"" target=""#b16"">[17]</ref>. In recent work, Brendel et al. <ref type=""bibr"" target=""#b13"">[14]</ref> proposed Boundary Attack, which generates adversarial exam ttacks</head><p>Most related to our work is the Boundary Attack method introduced by Brendel et al. <ref type=""bibr"" target=""#b13"">[14]</ref>. Boundary Attack is an iterative algorithm based on reject s: We compare HopSkipJumpAttack with three state-of-the-art decision-based attacks: Boundary Attack <ref type=""bibr"" target=""#b13"">[14]</ref>, Limited Attack <ref type=""bibr"" target=""#b8"">[9]</ref> an ibr"" target=""#b5"">[6]</ref>. A version normalized by image dimension was employed by Brendel et al. <ref type=""bibr"" target=""#b13"">[14]</ref> for evaluating Boundary Attack. The As an alternative metr <formula xml:id=""formula_38"">|E[φ x (x t + δ t u)]| &gt; 0,</formula><p>as we can see from Equation <ref type=""bibr"" target=""#b13"">(14)</ref>. To attempt to control the variance, we introduce a baseli",1
"ost identical to original samples in human perception, but cause models to make incorrect decisions <ref type=""bibr"" target=""#b0"">[1]</ref>. The vulnerability of neural networks to adversarial example witnessed a flurry of research on the design of new algorithms for generating adversarial examples <ref type=""bibr"" target=""#b0"">[1]</ref><ref type=""bibr"" target=""#b1"">[2]</ref><ref type=""bibr"" targe ptimization problem, which is solved either via treating misclassification loss as a regularization <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b5"">6]</ref> or via tackling the du",0
"target=""#b3"">[4]</ref><ref type=""bibr"" target=""#b4"">[5]</ref><ref type=""bibr"" target=""#b5"">[6]</ref><ref type=""bibr"" target=""#b6"">[7]</ref><ref type=""bibr"" target=""#b7"">[8]</ref><ref type=""bibr"" targe te a publicly available model trained through a robust optimization method proposed by Madry et al. <ref type=""bibr"" target=""#b6"">[7]</ref>. We further evaluate our attack method by constructing a non to evaluate defenses designed for ∞ distance, such as adversarial training proposed by Madry et al. <ref type=""bibr"" target=""#b6"">[7]</ref>.</p><p>On a distilled model, when the ∞ -distance is thresho led model, when the ∞ -distance is thresholded at 0.3, a perturbation size proposed by Madry et al. <ref type=""bibr"" target=""#b6"">[7]</ref> to measure adversarial robustness, HopSkipJumpAttack achieve d optimization problem <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b6"">7]</ref>. In the black-box setting, an attacker can only access output milar to previous work <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b6"">7]</ref>.</p><formula xml:id=""formula_18"">IV. A DECISION-BASED ALGORIT . Adversarial training <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b16"">17]</ref> is known to be one of b3"">[4]</ref> for minimizing 2 -distance, and FGSM <ref type=""bibr"" target=""#b1"">[2]</ref>, and BIM <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b40"">41]</ref> for minimizing ∞ -di",0
"ize the gradients <ref type=""bibr"" target=""#b34"">[35]</ref><ref type=""bibr"" target=""#b35"">[36]</ref><ref type=""bibr"" target=""#b36"">[37]</ref><ref type=""bibr"" target=""#b37"">[38]</ref><ref type=""bibr"" t",0
"ple variants have been proposed to randomize the gradients <ref type=""bibr"" target=""#b34"">[35]</ref><ref type=""bibr"" target=""#b35"">[36]</ref><ref type=""bibr"" target=""#b36"">[37]</ref><ref type=""bibr"" t",0
"nese poetry generation have been mostly rulebased or template-based. Recurrent Neural Network (RNN) <ref type=""bibr"" target=""#b10"">[11]</ref> was recently introduced as it has been proved to be effect",1
"f type=""bibr"" target=""#b13"">Yang et al. (2017)</ref>  <ref type=""bibr"" target=""#b13"">[14]</ref> and <ref type=""bibr"" target=""#b12"">Wang et al. (2016)</ref>  <ref type=""bibr"" target=""#b12"">[13]</ref> e f type=""bibr"" target=""#b13"">[14]</ref> and <ref type=""bibr"" target=""#b12"">Wang et al. (2016)</ref>  <ref type=""bibr"" target=""#b12"">[13]</ref> employ a two-stage approach,</p><formula xml:id=""formula_3",1
"t generation, we implement truncated top-k sampling instead of beam-search to generate diverse text <ref type=""bibr"" target=""#b5"">[6]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=",1
"e text generation, a classical Chinese poem should normally meet both form and content requirements <ref type=""bibr"" target=""#b0"">[1]</ref>. The form requirements includes the regulations on the numbe ce of content throughout a poem. For example, <ref type=""bibr"" target=""#b0"">Yi et al. (2018)</ref>  <ref type=""bibr"" target=""#b0"">[1]</ref> propose a salientclue mechanism which automatically selects cluding Xijiangyue(西江 月), Manjianghong(满江红), Shuidiaogetou(水调歌头), etc .</p><p>Various methods e.g., <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref> have been proposed to , some other works have been investigating the coherence of content throughout a poem. For example, <ref type=""bibr"" target=""#b0"">Yi et al. (2018)</ref>  <ref type=""bibr"" target=""#b0"">[1]</ref> propos",0
"满江红), Shuidiaogetou(水调歌头), etc .</p><p>Various methods e.g., <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref> have been proposed to generate classical Chinese poetry. Howe",0
"</head><p>Early works <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b7"">8]</ref> on Chinese poetry gener",0
"r generating high quality classical Chinese poetry with Generative Pre-trained Language Model (GPT) <ref type=""bibr"" target=""#b4"">[5]</ref>. The method adopts a simple GPT model, without using any hum blog<ref type=""foot"" target=""#foot_0"">3</ref> or the papers <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b3"">4]</ref> to get a basic understa",0
"target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b7"">8]</ref> on Chinese poetry generation have been mostly rulebased or te",0
"pairing of a generation are correct, various strategies have been adopted. For example, Yan (2016) <ref type=""bibr"" target=""#b11"">[12]</ref> proposes an iterative polishing schema, which refines the",0
">3</ref> or the papers <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b3"">4]</ref> to get a basic understanding of Transformer, which is the und",0
"-c.org/ns/1.0""><head n=""4"">Related Work</head><p>Early works <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target",0
"st salient characters from the so-far generated lines as a theme clue for generating the next line. <ref type=""bibr"" target=""#b13"">Yang et al. (2017)</ref>  <ref type=""bibr"" target=""#b13"">[14]</ref> a a theme clue for generating the next line. <ref type=""bibr"" target=""#b13"">Yang et al. (2017)</ref>  <ref type=""bibr"" target=""#b13"">[14]</ref> and <ref type=""bibr"" target=""#b12"">Wang et al. (2016)</ref",0
"ls</head><p>We refer the readers to the blog<ref type=""foot"" target=""#foot_0"">3</ref> or the papers <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target",0
"rmula></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4"">Related Work</head><p>Early works <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" targe",0
"ariability in behavioural and environmental patterns have stymied predictive modeling of this kind. <ref type=""bibr"" target=""#b8"">(Mikelsons et al., 2018)</ref> have tried to predict stress of student tp://www.tei-c.org/ns/1.0""><head n=""3.2.1."">LOCATION FEATURE BASED MLP</head><p>In the work done by <ref type=""bibr"" target=""#b8"">(Mikelsons et al., 2018)</ref>, a Multilayer Perceptron (MLP) with 4 f ><head n=""4."">Result</head><p>Due to a heavy imbalance of class labels on a scale of 1-5, we follow <ref type=""bibr"" target=""#b8"">(Mikelsons et al., 2018)</ref>, converting the five stress label scale ses of 23 students, totaling to 1183 data points achieving roughly equal amount of training data in <ref type=""bibr"" target=""#b8"">(Mikelsons et al., 2018)</ref>. These 1183 data points have the follow",1
"ct and have high intersubject variability. To learn personalized models for each student, we follow <ref type=""bibr"" target=""#b3"">(Jaques et al., 2017)</ref> and use a Multitask approach which compris",1
"hnologies for detecting stress has been accomplished. Few use heart rate and heart rate variability <ref type=""bibr"" target=""#b17"">(Vrijkotte et al., 2000)</ref>, cortisol levels <ref type=""bibr"" targ",0
"ironment causes cardiovascular diseases <ref type=""bibr"" target=""#b10"">(Rozanski et al., 1999;</ref><ref type=""bibr"" target=""#b5"">Kario et al., 2003)</ref>, alterations of the brain causing difference",0
"ef type=""bibr"" target=""#b9"">(Rahman et al., 2014)</ref> and surveys like the Perceived Stress Scale <ref type=""bibr"" target=""#b0"">(Cohen et al., 1983)</ref>.</p><p>With the induction of high quality,",0
"e system <ref type=""bibr"" target=""#b6"">(Khansari et al., 1990)</ref>, and poor academic performance <ref type=""bibr"" target=""#b12"">(Sano et al., 2015;</ref><ref type=""bibr"" target=""#b16"">Trokel et al. <p>Contemporary research such as <ref type=""bibr"" target=""#b11"">(Sano &amp; Picard, 2013)</ref> and <ref type=""bibr"" target=""#b12"">(Sano et al., 2015)</ref> has leveraged similar type of data from sen arget=""#b11"">Sano &amp; Picard, 2013;</ref><ref type=""bibr"" target=""#b16"">Trokel et al., 2000;</ref><ref type=""bibr"" target=""#b12"">Sano et al., 2015)</ref> etc.</p><p>Among the discrete sequence data,",0
"k. A prolonged exposure to stressful academic and social environment causes cardiovascular diseases <ref type=""bibr"" target=""#b10"">(Rozanski et al., 1999;</ref><ref type=""bibr"" target=""#b5"">Kario et a",0
"sol levels <ref type=""bibr"" target=""#b1"">(Dickerson &amp; Kemenyr, 2004</ref>) and skin conductance <ref type=""bibr"" target=""#b13"">(Setz et al., 2010)</ref>. Other techniques do not depend on sensors",0
"=""foot"" target=""#foot_0"">3</ref> black box attacks that satisfy the content preserving threat model <ref type=""bibr"" target=""#b12"">[13]</ref>. This observation was also made in concurrent work <ref ty",1
"true. In particular we demonstrate that the recently proposed AutoAugment data augmentation policy <ref type=""bibr"" target=""#b5"">[6]</ref> achieves state-of-the-art results on the CIFAR-10-C benchmar mentation strategies. Towards this end, we investigated the learned augmentation policy AutoAugment <ref type=""bibr"" target=""#b5"">[6]</ref>. AutoAugment applies a learned mixture of image transformati",1
"c coordinate with respect to the image center; we call these matrices the 2D Fourier basis matrices <ref type=""bibr"" target=""#b3"">[4]</ref>.</p><p>Given a model and a validation image X, we can genera",0
"at compression removes high frequency information, JPEG compression has been proposed several times <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" targ",0
"ormation, JPEG compression has been proposed several times <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b6"">7]</ref> as a method for improvi",0
"st time.</p><p>As a simple example of this principle, consider Figure <ref type=""figure"">8</ref> in <ref type=""bibr"" target=""#b18"">[19]</ref>. The authors experimented with training models on a ""cheat",0
"follow-up work has utilized AutoAugment in a way to achieve state-of-the-art results on ImageNet-C <ref type=""bibr"" target=""#b0"">[1]</ref>. Some of our observations could be of interest to research o we note that a follow-up work has utilized AutoAugment in a way to achieve state-of-the-art results <ref type=""bibr"" target=""#b0"">[1]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=",0
"ess to noise and blurring corruptions on the CIFAR-10-C and ImageNet-C common corruption benchmarks <ref type=""bibr"" target=""#b16"">[17]</ref>, while significantly degrading performance on the fog and t average corruption test accuracy of 86%. Using the mean corruption error (mCE) metric proposed in <ref type=""bibr"" target=""#b16"">[17]</ref> with the naturally trained model being the baseline (see a p><p>As for the ImageNet-C benchmark, instead of using the compressed ImageNet-C images provided in <ref type=""bibr"" target=""#b16"">[17]</ref>, we evaluate the models on corruptions applied in memory,",0
"uch as random noise, contrast change, and blurring, can lead to significant performance degradation <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b2"">3]</ref>. Improving distributio",0
"0""><head n=""1"">Introduction</head><p>Bidirectional Encoder Representations from Transformers (BERT) <ref type=""bibr"" target=""#b8"">(Devlin et al., 2019)</ref> has become enormously popular and proven t RT-wwm), we suggest taking another pre-training steps on the task data, which was also suggested by <ref type=""bibr"" target=""#b8"">(Devlin et al., 2019)</ref>.</p><p>• As there are so many possibilitie settings and data statistics in different task. † represents the dataset was also evaluated by BERT<ref type=""bibr"" target=""#b8"">(Devlin et al., 2019)</ref>. ‡ represents the dataset was also evaluat , which is beneficial for the researcher to design more powerful models based on them.</p><p>Before <ref type=""bibr"" target=""#b8"">Devlin et al. (2019)</ref> releasing BERT with whole word masking, <re <ref type=""foot"" target=""#foot_1"">3</ref> , and pre-processed with WikiExtractor.py as suggested by <ref type=""bibr"" target=""#b8"">Devlin et al. (2019)</ref>  <ref type=""bibr"" target=""#b8"">Devlin et al sed with WikiExtractor.py as suggested by <ref type=""bibr"" target=""#b8"">Devlin et al. (2019)</ref>  <ref type=""bibr"" target=""#b8"">Devlin et al. (2019)</ref>, for computation efficiency and learning lo",1
"nsion benchmarks, such as SQuAD <ref type=""bibr"" target=""#b14"">(Rajpurkar et al., 2018)</ref>, CoQA <ref type=""bibr"" target=""#b17"">(Reddy et al., 2019)</ref>, QuAC <ref type=""bibr"" target=""#b3"">(Choi is is also a span-extraction MRC dataset, but in Traditional Chinese.</p><p>• CJRC: Similar to CoQA <ref type=""bibr"" target=""#b17"">(Reddy et al., 2019)</ref>, which has yes/no questions, noanswer ques",0
"Reading Comprehension (MRC): CMRC 2018 <ref type=""bibr"" target=""#b6"">(Cui et al., 2019)</ref>, DRCD <ref type=""bibr"" target=""#b18"">(Shao et al., 2018</ref><ref type=""bibr"">), CJRC (Duan et al., 2019</",0
"search of graph embedding ( <ref type=""bibr"" target=""#b15"">Perozzi, Al-Rfou, and Skiena, 2014;</ref><ref type=""bibr"" target=""#b4"">Grover and Leskovec, 2016)</ref>, where graph topology and node relati he performance of our algorithm against several unsupervised graph learning counter-parts: Node2Vec <ref type=""bibr"" target=""#b4"">(Grover and Leskovec, 2016)</ref>, VGAE <ref type=""bibr"" target=""#b9""> ds. DeepWalk <ref type=""bibr"" target=""#b15"">(Perozzi, Al-Rfou, and Skiena, 2014)</ref> and Node2vec <ref type=""bibr"" target=""#b4"">(Grover and Leskovec, 2016)</ref> are representative random walk-based ns from nodes' raw features, without using any graph structure information incorporated. • Node2Vec <ref type=""bibr"" target=""#b4"">(Grover and Leskovec, 2016)</ref>: This approach is an extension of Wo",1
"gy and node relations are embedded as vector space. In light of the rapid advances of deep learning <ref type=""bibr"" target=""#b11"">(LeCun, Bengio, and Hinton, 2015)</ref>, current research attention h",0
"ubbed as Graph Neural Networks (GNNs) <ref type=""bibr"">(Gori, Monfardini, and Scarselli, 2005;</ref><ref type=""bibr"" target=""#b17"">Scarselli et al., 2009)</ref>. To their essential characteristics, GN",0
"first alignment is to employ the work <ref type=""bibr"" target=""#b3"">(Goodfellow et al., 2014;</ref><ref type=""bibr"" target=""#b1"">Ganin et al., 2015)</ref> to the graph representation learning domain.",0
"n homogeneous graphs, such as LINE <ref type=""bibr"" target=""#b19"">(Tang et al., 2015)</ref>, GraRep <ref type=""bibr"" target=""#b0"">(Cao, Lu, and Xu, 2015)</ref>, SDNE <ref type=""bibr"" target=""#b21"">(Wa",0
"parameters using 600⇥600 images for training. In this paper, we focus on the ResNet-50 architecture <ref type=""bibr"" target=""#b10"">[11]</ref> due to its good accuracy/cost tradeoff (25.6M parameters) -of-the-art neural network architectures with no modifications, We consider in particular ResNet-50 <ref type=""bibr"" target=""#b10"">[11]</ref>. For larger experiments, we use PNASNet-5-Large <ref type= although this means that sev- eral forward passes are required to classify one image. For example, <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" ta Another performanceboosting strategy is to classify an image by feeding it at multiple resolutions <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b29"">30,</ref><ref type=""bibr"" ta",1
"ibr"" target=""#b26"">[27]</ref>, inpainting <ref type=""bibr"" target=""#b36"">[37]</ref>, style transfer <ref type=""bibr"" target=""#b8"">[9]</ref> and even image compression <ref type=""bibr"" target=""#b27"">[2",0
"n has demonstrated improved performance by considering larger networks and higher resolution images <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b21"">22]</ref>. For instance, the",0
"curacy, an improvement of 1.4 percentage points. This concurs with prior findings in the literature <ref type=""bibr"" target=""#b11"">[12]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head",0
"y, ""feature-wise linear modulations"" (FiLM) were introduced in the visual question answering domain <ref type=""bibr"" target=""#b10"">(Perez et al., 2017)</ref>. Here, the hypernetwork is fed with an enc the extreme case in which the dimension of each chunk is 1, this method coincides with the ideas of <ref type=""bibr"" target=""#b10"">Perez et al. (2017)</ref>, who propose to use layers of element-wise",1
"rivial neural networks is computationally expensive.</p><p>Approaches to mitigate this exist (e.g., <ref type=""bibr"" target=""#b17"">Wu et al. (2019)</ref> handle this in natural language processing), b only the hypernetwork parameters ? f, ,c . This is somewhat less desirable than the related idea of <ref type=""bibr"" target=""#b17"">Wu et al. (2019)</ref>, which operates on sequences, where sharing be",0
"h can be a simple bounded nonlinearity (e.g. tanh), a fully connected layer, or layer normalisation <ref type=""bibr"" target=""#b1"">(Ba et al., 2016)</ref>, or any combination of these.</p></div> <div x",0
"from completely unseen projects.</p><p>Due to the inherent cost of training models on this dataset <ref type=""bibr"" target=""#b2"">(Balog et al. (2019)</ref> provide an in-depth performance analysis),",0
"rces is a common problem in neural network design. A recent trend has been the use of hypernetworks <ref type=""bibr"" target=""#b6"">(Ha et al., 2017)</ref>, neural networks that compute the weights of o S</head><p>Hypernetworks (i.e., neural networks computing the parameters of another neural network) <ref type=""bibr"" target=""#b6"">(Ha et al., 2017)</ref> have been successfully applied to a number of",0
"ustness for classical neural networks/robust training (e.g. <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b19"">20]</ref>), we tackle various ertifiable robustness <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b19"">20]</ref> providing guarantee ginal sample measured by, e.g., the infinity-norm or L2-norm <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b19"">20]</ref>, often e.g. ϵ &lt; /p><p>For this work, specifically the class of methods based on convex relaxations are of relevance <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b19"">20]</ref>. They construct a the remaining layers, since the input to them is no longer binary, we adapt the bounds proposed in <ref type=""bibr"" target=""#b17"">[18]</ref>. Generalized to the GNN we therefore obtain:</p><formula x",1
"ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b21"">22]</ref> and node embeddings <ref type=""bibr"" target=""#b0"">[1]</ref>. All of these works focus on generating adversarial examples",0
"n that such approaches are vulnerable to adversarial attacks <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b21"">22]</ref>: Even only slight d to investigate adversarial attacks on graph neural networks <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b21"">22]</ref> and node embeddings . In contrast, we provide the first work to certify and improve the robustness of GNNs. As shown in <ref type=""bibr"" target=""#b20"">[21]</ref>, both perturbations to the node attributes as well as the onsidered so far.</p><p>Therefore, motivated by the existing works on adversarial attacks to graphs <ref type=""bibr"" target=""#b20"">[21]</ref>, we consider a more realistic scenario: We define the set d nodes. Please note that we do not need to compare against graph adversarial attack models such as <ref type=""bibr"" target=""#b20"">[21]</ref> since our method gives provable guarantees on the robustne e certifiably non-robust (i.e. we can find adversarial examples), confirming the issues reported in <ref type=""bibr"" target=""#b20"">[21]</ref>. PubMed behaves similarly (as we will see later, e.g., in",0
"""bibr"" target=""#b11"">[12]</ref>. While there exist many classical approaches to node classification <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b14"">15]</ref>, recently graph neur the target, i.e. for a GNN with a single hidden layer (L = 3) we have R (2) , S (2) ∈ R N 1 (t )×h <ref type=""bibr"" target=""#b1"">(2)</ref> .</p><p>Corollary 4.6. Eqs. ( <ref type=""formula"">12</ref>)",0
"ion and improved the state of the art in node classification <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" targe",0
"Neural Networks (CNNs) establish state-ofthe-art performance for many Computer Vision applications <ref type=""bibr"" target=""#b14"">(Krizhevsky et al., 2012;</ref><ref type=""bibr"" target=""#b21"">Szegedy )</ref>. Further, these filters are automatically recovered by training CNNs on natural images (see <ref type=""bibr"" target=""#b14"">Krizhevsky et al. (2012)</ref>; <ref type=""bibr"" target=""#b15"">Lee et",0
"f type=""bibr"" target=""#b11"">(Ioffe &amp; Szegedy, 2015)</ref>, element-wise activation, and Dropout <ref type=""bibr"" target=""#b20"">(Srivastava et al., 2014)</ref>. In this section, we discuss approach",0
"antum chemistry and materials science (e.g., <ref type=""bibr"" target=""#b19"">Hirn et al., 2017;</ref><ref type=""bibr"" target=""#b13"">Eickenberg et al., 2017;</ref><ref type=""bibr"">2018;</ref><ref type=""",0
"G, but several works, including <ref type=""bibr"" target=""#b3"">Atwood &amp; Towsley (2016)</ref> and <ref type=""bibr"" target=""#b39"">Zhang et al. (2018)</ref>, propagate information via a random walk. W",0
"st Meltdown, a different speculative execution vulnerability <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b33"">34]</ref>. PTI introduced a new trampoline used during system calls t",1
"rnel, although some progress has been made to leverage LLVM to run applications as complex as Linux <ref type=""bibr"" target=""#b2"">[3]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=",0
"information only available at runtime, much as a JIT compiler may employ polymorphic inline caching <ref type=""bibr"" target=""#b21"">[22]</ref>. Unlike binary translation, JumpSwitches are integrated in",0
"el architectures before Skylake, speculation for RET instructions were always computed from the RSB <ref type=""bibr"" target=""#b13"">[14]</ref>. On Skylake and after however, the contents of the BTB may BRS), Single Thread Indirect Branch Predictors (STIBP) and Indirect Branch Predictor Barrier (IBPB) <ref type=""bibr"" target=""#b13"">[14]</ref>. IBRS works by defining four privilege levels: host and gu anced IBRS) <ref type=""bibr"" target=""#b15"">[16]</ref> and control flow enforcement technology (CET) <ref type=""bibr"" target=""#b13"">[14]</ref>. Enhanced IBRS eliminates the overhead of IBRS by removing ume that only indirect branches and returns are vulnerable to Spectre variant 2, as stated by Intel <ref type=""bibr"" target=""#b13"">[14]</ref> and the original Spectre disclosure <ref type=""bibr"" targe",0
"el under different workloads results in different profiles <ref type=""bibr"" target=""#b53"">[54,</ref><ref type=""bibr"" target=""#b54"">55]</ref>. Second, mechanisms such as KASLR <ref type=""bibr"" target=""",0
"b16"">[17]</ref>.</p><p>A more direct approach to LTR metric optimization was proposed by Qin et al. <ref type=""bibr"" target=""#b13"">[14]</ref>, where the rank variable in the definition of metrics like Recent hardware and software advances in the training of neural networks, however, make the work in <ref type=""bibr"" target=""#b13"">[14]</ref> relevant again and potentially allow us to harvest the eff ng state-of-the-art LTR algorithms such as LambdaMART. We give an overview of LTR and in particular <ref type=""bibr"" target=""#b13"">[14]</ref> in Section 2. We discuss experimental results in Section 3 ""#b14"">[15]</ref>, boosting <ref type=""bibr"" target=""#b18"">[19]</ref>, and approximating the metric <ref type=""bibr"" target=""#b13"">[14]</ref>. It is the latter that can tightly bound any ranking metri "" target=""#b13"">[14]</ref>. It is the latter that can tightly bound any ranking metric such as NDCG <ref type=""bibr"" target=""#b13"">[14]</ref> and can be easily optimized with gradient descent.</p><p>S adient descent.</p><p>Surprisingly, despite its attractive theoretical properties, the framework in <ref type=""bibr"" target=""#b13"">[14]</ref> has received little attention in LTR studies in the decade ts to optimize NDCG-referred to as ApproxNDCG. Our results show that  the theoretical guarantees in <ref type=""bibr"" target=""#b13"">[14]</ref> materialize in practice. Before we go any further, we give the indicator which is 1 if s &lt; t and 0 otherwise. <ref type=""bibr"">Qin et al.</ref> propose in <ref type=""bibr"" target=""#b13"">[14]</ref> a smooth approximation of Equation <ref type=""formula"" tar ture of ranking utility functions.</p><p>In this work, we set out to revisit the work of Qin et al. <ref type=""bibr"" target=""#b13"">[14]</ref> which formulates a smooth approximation to any ranking met any ranking metric such as NDCG. Unlike many other existing surrogate LTR losses, the framework in <ref type=""bibr"" target=""#b13"">[14]</ref> offers a way to directly optimize ranking metrics. Because g metrics rather than loosely related surrogate losses; and (b) that the approximation framework in <ref type=""bibr"" target=""#b13"">[14]</ref> could lay out the foundation of deep neural networks in LT",1
"leased our implementation of ApproxNDCG in Tensorflow in the open-source Tensorflow Ranking library <ref type=""bibr"" target=""#b11"">[12]</ref>. <ref type=""foot"" target=""#foot_0"">1</ref></p></div> <div",1
"as SoftRank <ref type=""bibr"" target=""#b14"">[15]</ref> and indirect boosting methods like LambdaMART <ref type=""bibr"" target=""#b16"">[17]</ref>.</p><p>A more direct approach to LTR metric optimization w",0
"#b5"">6]</ref>, support vector machines <ref type=""bibr"" target=""#b8"">[9]</ref>, and neural networks <ref type=""bibr"" target=""#b1"">[2]</ref>. In this paper, we model f using the latter.</p><p>The loss",0
"/p><p>The hyperparameters for LambdaMART models are based on those reported in previous work (e.g., <ref type=""bibr"" target=""#b15"">[16]</ref>) and further fine-tuned on the validation set. Specificall Note that our NDCG measurements for λMART GBM are lower than those reported in previous work (e.g., <ref type=""bibr"" target=""#b15"">[16]</ref>). This is because LightGBM computes an NDCG of 1.0 for que",0
"ion methods include linear functions <ref type=""bibr"" target=""#b8"">[9]</ref>, boosted weak learners <ref type=""bibr"" target=""#b18"">[19]</ref>, gradientboosted trees <ref type=""bibr"" target=""#b2"">[3,</ ibr"" target=""#b10"">[11]</ref>, smoothing scores <ref type=""bibr"" target=""#b14"">[15]</ref>, boosting <ref type=""bibr"" target=""#b18"">[19]</ref>, and approximating the metric <ref type=""bibr"" target=""#b1",0
"he legacy RankLib implementation (λMART RankLib ). We implemented ListMLE and RankNet in Tensorflow <ref type=""bibr"" target=""#b0"">[1]</ref>, a deep learning framework. In all of our experiments, we ru",0
"ethora of schemes to improve metrics like NDCG, including metric smoothing methods such as SoftRank <ref type=""bibr"" target=""#b14"">[15]</ref> and indirect boosting methods like LambdaMART <ref type=""b nking metric by using coordinate ascent <ref type=""bibr"" target=""#b10"">[11]</ref>, smoothing scores <ref type=""bibr"" target=""#b14"">[15]</ref>, boosting <ref type=""bibr"" target=""#b18"">[19]</ref>, and a",0
"n=""3.2"">Models</head><p>We have compared our results with existing ranking models including ListMLE <ref type=""bibr"" target=""#b17"">[18]</ref>, RankNet and LambdaMART <ref type=""bibr"" target=""#b2"">[3]<",0
"sets</head><p>We conduct exhaustive experiments on two publicly available LTR datasets: MSLR-WEB30K <ref type=""bibr"" target=""#b12"">[13]</ref> and Yahoo! LTR Challenge <ref type=""bibr"" target=""#b4"">[5]",0
"ecent embedding for node i. The use of max operator is inspired from learning on general point sets <ref type=""bibr"" target=""#b34"">(Qi et al., 2017)</ref>. By applying max-pooling operator element-wis",1
"t al., 2018)</ref>. There exists a rich body of literature on temporal modeling of dynamic networks <ref type=""bibr"" target=""#b22"">(Kim et al., 2017)</ref>, that focus on link prediction tasks but the r dynamic networks. There exists a rich body of literature on temporal modeling of dynamic networks <ref type=""bibr"" target=""#b22"">(Kim et al., 2017</ref>) that focus on link prediction tasks but thei",0
"necessarily connected) nodes which leads to temporary information flow between them (Farine, 2017; <ref type=""bibr"" target=""#b1"">Artime et al., 2017)</ref>. We, then, posit our goal of learning node",0
"(amino acids). Specifically, we augment the autoregressive self-attention of recent sequence models <ref type=""bibr"" target=""#b6"">[7]</ref> with graph-based descriptions of the 3D structure. By compos dependent, contextual embeddings of each residue in the 3D structure with multi-head self-attention <ref type=""bibr"" target=""#b6"">[7]</ref> on the spatial k-nearest neigbors graph. An autoregressive d Structured Transformer model that draws inspiration from the selfattention based Transformer model <ref type=""bibr"" target=""#b6"">[7]</ref> and is augmented for scalable incorporation of relational in an attend to a separate subspace of the embeddings via learned query, key and value transformations <ref type=""bibr"" target=""#b6"">[7]</ref>.</p><p>The queries are derived from the current embedding at een these self-attention layers and position-wise feedforward layers as in the original Transformer <ref type=""bibr"" target=""#b6"">[7]</ref>. We stack multiple layers atop each other, and thereby obtai rained models using the learning rate schedule and initialization of the original Transformer paper <ref type=""bibr"" target=""#b6"">[7]</ref>, a dropout rate of 10% <ref type=""bibr"" target=""#b41"">[42]</ f their structure. Our model augments the traditional sequence-level self-attention of Transformers <ref type=""bibr"" target=""#b6"">[7]</ref> with relational 3D structural encodings and is able to lever ndependent, contextual embeddings of each residue in the 3D structure with multi-head self-attention<ref type=""bibr"" target=""#b6"">[7]</ref> on the spatial k-nearest neigbors graph. An autoregressive d",1
"endencies in sequence are generally short-range in 3D space <ref type=""bibr"" target=""#b9"">[10]</ref><ref type=""bibr"" target=""#b10"">[11]</ref><ref type=""bibr"" target=""#b11"">[12]</ref>. By making the gr",0
"</p><p>Several groups have obtained promising results using (unconditional) protein language models <ref type=""bibr"" target=""#b21"">[22]</ref><ref type=""bibr"" target=""#b22"">[23]</ref><ref type=""bibr"" t",0
""">[13]</ref>. Recently <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b13"">14]</ref> proposed neural network-based models for sequences given 3D",0
"type=""bibr"" target=""#b1"">[2]</ref>, enzymes <ref type=""bibr"" target=""#b2"">[3]</ref>, and complexes <ref type=""bibr"" target=""#b3"">[4]</ref>. However, the current practice often requires multiple round",0
"e two algorithm mention candidates lie across sentences. We base on recent Transformer architecture <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b35"">36]</ref> to build this module ><formula xml:id=""formula_4"">H Ak = A k + Conv( A k ).<label>(4)</label></formula><p>We follow BERT <ref type=""bibr"" target=""#b6"">[7]</ref> which recently achieves great success in multiple NLP tasks,",1
"n single sentence relation extraction with an exception of <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b36"">37]</ref>, which focus on general documents while not targeting on a e extracted dependency parse tree, where the tree roots of different sentences are linked together. <ref type=""bibr"" target=""#b36"">[37]</ref> proposes a method using self-attention <ref type=""bibr"" ta the self-attention of the words, and use a convolutional layer in self-attention blocks similar to <ref type=""bibr"" target=""#b36"">[37]</ref> to alleviate the burden on the model to attend to local fe",1
"rk based methods have achieved great success in relation extraction, including CNN-based approaches <ref type=""bibr"" target=""#b39"">[40,</ref><ref type=""bibr"" target=""#b40"">41]</ref> and LSTMbased appr ions in a paragraph. To this end, our model consists of a single-sentence module with Piecewise CNN <ref type=""bibr"" target=""#b39"">[40]</ref>, and a cross-sentence module which leverages self-attentio based methods <ref type=""bibr"" target=""#b19"">[20]</ref>, and supervised relation extraction methods <ref type=""bibr"" target=""#b39"">[40]</ref>. The pattern-based method <ref type=""bibr"" target=""#b13"">[ . In the following we introduce evaluated methods in detail.</p><p>PCNN_single: Piecewise CNN model <ref type=""bibr"" target=""#b39"">[40]</ref>, which is one of the state of art single-sentence relation",1
"the abbreviation, as a short form of text, is prone to ambiguity. Word sense disambiguation methods <ref type=""bibr"" target=""#b22"">[23]</ref> have been studied to disambiguate word senses, however, de be expensive and the collected datasets are normally small in size.</p><p>Word sense disambiguation <ref type=""bibr"" target=""#b22"">[23]</ref> is a type of technique used to distinguish ambiguous word </p><p>Inspired by word sense disambiguation methods that label super sense types for word clusters <ref type=""bibr"" target=""#b22"">[23]</ref>, we jointly predict the types for abbreviation candidates",1
"e linked together. <ref type=""bibr"" target=""#b36"">[37]</ref> proposes a method using self-attention <ref type=""bibr"" target=""#b35"">[36]</ref> and bi-affine scoring algorithm to predict biological rela 14"">[15]</ref> and Convolutional neural networks (CNNs).</p><p>Self-Attention. We adapt Transformer <ref type=""bibr"" target=""#b35"">[36]</ref> to encode word sequences in a paragraph, where we calculat across sentences. We base on recent Transformer architecture <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b35"">36]</ref> to build this module, due to its better performance in enco",1
"he supervised relation extraction focus on single sentence relation extraction with an exception of <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b36"">37]</ref>, which focus on ge /ref> proposes to construct cross-sentence relation data for entities with minimal-span assumption. <ref type=""bibr"" target=""#b24"">[25]</ref> proposes to use a Graph-LSTM to encode the shortest path i",0
"step assisting with this process is taxonomy construction <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b31"">32,</ref><ref type=""bibr"" tar xonomy construction methods mostly focus on isA relation. They either rely on pattern-based methods <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b31"">32]</ref> which extract hier ns may convey a meaning related to comparative relation. Unsupervised pattern-based methods such as <ref type=""bibr"" target=""#b13"">[14]</ref> focus on isA relation, which are not suitable for finding ds have been proposed to extract these hierarchical relation, either leveraging linguistic patterns <ref type=""bibr"" target=""#b13"">[14]</ref> or hierarchical clustering of concepts which implicitly ca sed relation extraction methods <ref type=""bibr"" target=""#b39"">[40]</ref>. The pattern-based method <ref type=""bibr"" target=""#b13"">[14]</ref> is not compared due to its low recall in our task.</p><p>T",0
"e others apply distant supervision to link entity mentions <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b27"">28]</ref> in the text to the knowledge base entities <ref type=""bibr""",0
"ommunity, while most of the works focus on news and web data <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b28"">29]</ref>. Recent neural network based methods have achieved great su Predicting whether an algorithm candidate pair is compared forms a multi-instance learning problem <ref type=""bibr"" target=""#b28"">[29,</ref><ref type=""bibr"" target=""#b34"">35]</ref>. For each pair, a",0
"/ref> to alleviate the burden on the model to attend to local features. We add residual connections <ref type=""bibr"" target=""#b11"">[12]</ref> to both multihead attention and convolutional layers. The",0
"ref><ref type=""bibr"" target=""#b33"">34]</ref> or harvest knowledge with specific linguistic patterns <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b38"">39]</ref>. Taxonomy can be vie",0
"f type=""bibr"" target=""#b8"">[9]</ref> and DCGAN (Deep Convolutional Generative Adversarial Networks) <ref type=""bibr"" target=""#b26"">[27]</ref>. A directed edge GAN → DCGAN represents ""DCGAN"" is a succe",0
"/ref> or harvest knowledge with specific linguistic patterns <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b38"">39]</ref>. Taxonomy can be viewed as a tree-structure knowledge graph",0
"et=""#b10"">[11,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b31"">32,</ref><ref type=""bibr"" target=""#b41"">42]</ref>, which extracts concepts from a collection of documents and everaging linguistic features, or clustering-based methods <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b41"">42]</ref>, which cluster concepts to induce an implicit hierarchy.</p 4]</ref> or hierarchical clustering of concepts which implicitly captures the hierarchical relation <ref type=""bibr"" target=""#b41"">[42]</ref>. These methods mainly focus on the general domain, harvest",0
"apers and iterate this process.</p><p>One step assisting with this process is taxonomy construction <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" ta ef> which extract hierarchical relation leveraging linguistic features, or clustering-based methods <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b41"">42]</ref>, which cluster con",0
"not. Evaluated methods can be divided to unsupervised methods including co-occurrence based methods <ref type=""bibr"" target=""#b9"">[10]</ref>, word-similarity based methods <ref type=""bibr"" target=""#b1 also used.</p><p>Sent_cooccur: A method similar to co-occurrence method used in hypernym detection <ref type=""bibr"" target=""#b9"">[10]</ref>. Sent_cooccur calculates the co-occurrence frequency of can",0
"r"" target=""#b12"">[13]</ref> studied the evolution of scientific topics through dynamic topic models <ref type=""bibr"" target=""#b20"">[21]</ref> modeling implicit topics and obscure relations, and some t",0
", <ref type=""bibr"" target=""#b3"">[4]</ref> collected a dataset for scientific taxonomy construction, <ref type=""bibr"" target=""#b12"">[13]</ref> studied the evolution of scientific topics through dynamic",0
"domain, which either extract facts from Wikipedia info-boxes <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b33"">34]</ref> or harvest knowledge with specific linguistic patterns <ref",0
"onomy construction <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b31"">32,</ref><ref type=""bibr"" target=""#b41"">42]</ref>, which extracts con on isA relation. They either rely on pattern-based methods <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b31"">32]</ref> which extract hierarchical relation leveraging linguistic f",0
"input dimension for the convolutional layer in each Transformer block. We apply layer normalization <ref type=""bibr"" target=""#b1"">[2]</ref> to each component of transformer block, and adopt dropout <r",0
"ess in relation extraction, including CNN-based approaches <ref type=""bibr"" target=""#b39"">[40,</ref><ref type=""bibr"" target=""#b40"">41]</ref> and LSTMbased approaches <ref type=""bibr"" target=""#b30"">[31",0
"ef type=""figure"">1</ref>, for example, a roadmap for algorithm Generative Adversarial Network (GAN) <ref type=""bibr"" target=""#b8"">[9]</ref>, describes its successors and competitors in the scientific in the computer science domain, there are algorithms such as GAN (Generative Adversarial Networks) <ref type=""bibr"" target=""#b8"">[9]</ref> and DCGAN (Deep Convolutional Generative Adversarial Network ly published in this conference. ""GAN"" (Generative Adversarial Networks) is a deep generative model <ref type=""bibr"" target=""#b8"">[9]</ref>, which has been extensively cited since proposed. Researcher",0
"ic publications are prohibitively expensive. Existing datasets or curated in-domain knowledge bases <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b3"">4]</ref> are rather small and f h pattern or statistics.</p><p>Many works focus on for mining scientific publications, for example, <ref type=""bibr"" target=""#b0"">[1]</ref> proposed a keyphrase and relation extraction competition for",0
"has attracted much attention from the community, while most of the works focus on news and web data <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b28"">29]</ref>. Recent neural netwo",0
"ser intent.</p><p>As a general information modeling method, Heterogeneous Information Network (HIN) <ref type=""bibr"" target=""#b17"">[18]</ref>, consisting of multiple types of objects and links, has be y data mining tasks <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b17"">18]</ref>. In this paper, we propose to model the intent recommendati",1
"p>Inspired by the basic idea of the GCNs which generates object embeddings based on local neighbors <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b20"">21]</ref>, we first propose",0
"which generates object embeddings based on local neighbors <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b20"">21]</ref>, we first propose a UIQ path UQI path metapath-guided heter",0
"s) as a sequence data. Recurrent Neural Network (RNN), especially the Long Short Term Memory (LSTM) <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b4"">5]</ref> has been proved to per",0
"<head n=""4.3"">Detailed Implementation</head><p>We implement the proposed method based on Tensorflow <ref type=""bibr"" target=""#b0"">[1]</ref>. For our method, we set the dimension of term embedding as 6",0
"e their remarkable performance, recent studies show that GCNs are vulnerable to adversarial attacks <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b36"">37]</ref>, i.e. carefully desi bibr"" target=""#b37"">38]</ref> try to attack the model by changing training data and evasion attacks <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b36"">37]</ref> try to generate fake after (evasion attacks) the training phase of GCNs. • Targeted or Non-targeted. In targeted attacks <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b36"">37]</ref>, the attacker focus eted attacks can be further divided into two categories based on attack settings. In direct attacks <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b36"">37]</ref>, the attacker can di r example, the attacker tends to connect nodes from different communities to confuse the classifier <ref type=""bibr"" target=""#b6"">[7]</ref>. While plain vectors cannot adapt to such changes, Gaussian into the graph. We regard this method as an illustrating example of non-targeted attacks. • RL-S2V <ref type=""bibr"" target=""#b6"">[7]</ref> <ref type=""foot"" target=""#foot_2"">3</ref> : This method gene",1
"ble research attention <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b17"">18]</ref>. State-of-the-art GCNs usually follow a ""message-passing"" f ion of the Laplacian matrix is avoided, thus reducing the overall time complexity. Kipf and Welling <ref type=""bibr"" target=""#b17"">[18]</ref> further propose to simplify the graph convolution using on GCN methods have been proposed, here we focus on a representative one proposed by Kipf and Welling <ref type=""bibr"" target=""#b17"">[18]</ref>. Here, the (l + 1) t h convolutional layer is defined as:< istributions by using our Gaussian-based Graph Convolutions.</p><p>Following the original GCN model <ref type=""bibr"" target=""#b17"">[18]</ref>, we also impose L 2 regularization on parameters of the fi To evaluate the robustness of RGCN, we compare it with two state-of-the-art GCN models:</p><p>• GCN <ref type=""bibr"" target=""#b17"">[18]</ref>: As introduced in Section 3.2 , this is the original GCN m methods is evaluated on a separate test set of 1000 labels. We adopt the same dataset splits as in <ref type=""bibr"" target=""#b17"">[18]</ref> and report the average results of 10 runs. In experiments, ectiveness of our proposed method, we adopt three citation networks commonly used in previous works <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b29"">30]</ref>: Cora, Citeseer an ""table"" target=""#tab_0"">1</ref>.</p><p>We closely follow the experimental setting in previous works <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b29"">30]</ref>. Specifically, we . In experiments, we set the number of layers as two for all methods as suggested by previous works <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b29"">30]</ref>. For GCN and RGCN,",1
"r"" target=""#b2"">[3]</ref> first introduce convolutions for graph data using graph signal processing <ref type=""bibr"" target=""#b24"">[25]</ref>. By using the eigen-decomposition of the graph Laplacian m",0
"to improve efficiency <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b32"">33]</ref> considering edge at directly applied, e.g. <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b32"">33]</ref>.</p></div> <div xml",0
"raphs that recently attracts considerable research attention <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b17"">18]</ref>. State-of-the-art GCN e time complexity of such graph convolution is very high. To solve the efficiency problem, Cheb-Net <ref type=""bibr"" target=""#b7"">[8]</ref> proposes to use a K-order Chebyshev polynomial <ref type=""bi",0
"ects of such unexpected adversarial changes in the variances <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b35"">36]</ref>. As a result, using Gaussian distributions can enhance the",0
"type=""bibr"" target=""#b16"">[17]</ref>. MPNNs <ref type=""bibr"" target=""#b9"">[10]</ref> and GraphSAGE <ref type=""bibr"" target=""#b11"">[12]</ref> unify these approaches using the ""message-passing"" framewo",0
"=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b29"">30]</ref>: Cora, Citeseer and Pubmed <ref type=""bibr"" target=""#b23"">[24]</ref>, where nodes represent documents and edges represent citat",0
", I) ,<label>(13)</label></formula><p>where KL(•||•) is the KL-divergence between two distributions <ref type=""bibr"" target=""#b18"">[19]</ref>.</p><p>Note that we only need to regularize H (1) as deepe",0
"matrices. The non-linear activation is ELU (•) <ref type=""bibr"" target=""#b5"">[6]</ref> and ReLU (•) <ref type=""bibr"" target=""#b20"">[21]</ref> for means and variances respectively since variances are r",0
"with the success of deep neural networks (DNNs), some researches have applied DNNs to precipitation <ref type=""bibr"" target=""#b20"">[21]</ref> and radar echo <ref type=""bibr"" target=""#b31"">[32]</ref> n ation and intensity of rain and snow. To solve the problem of spatiotemporal dependency, Shi et al. <ref type=""bibr"" target=""#b20"">[21]</ref> developed the conventional LSTM and propose convolutional , with a goal to overcome the drawbacks of FC-LSTM in handling spatial-temporal data such as videos <ref type=""bibr"" target=""#b20"">[21]</ref>. Specifically, in the ConvLSTM network, the fully-connecte",1
"/head><p>The Weather Research and Forecasting (WRF<ref type=""foot"" target=""#foot_0"">1</ref> ) Model <ref type=""bibr"" target=""#b22"">[23]</ref> is a nextgeneration numerical weather prediction system de",0
"e grid-wise precipitation nowcasting task. Furthermore, they also proposed the trajectory GRU model <ref type=""bibr"" target=""#b21"">[22]</ref> that improves ConvLSTM by actively learning the recurrent",0
"den state of the last moment, respectively. The detailed calculation steps of (5) were presented in <ref type=""bibr"" target=""#b1"">(2)</ref>. Observation Encoder. The observation encoder is in charge o r loading the initial values, the prediction decoder will run following the calculation as shown in <ref type=""bibr"" target=""#b1"">(2)</ref>. In particular, the input and the prediction at a moment t & nfiguration of the prediction decoder. In addition, inspired by the method from machine translation <ref type=""bibr"" target=""#b1"">[2]</ref>, the prediction for the moment t will be sent as the input o",0
"proposed a new parameterization by combining PR92 and cloud droplet concentration. Deierling et al. <ref type=""bibr"" target=""#b5"">[6]</ref> used groundbased dual-polarimetric radar and total lightning",0
"ef type=""bibr"" target=""#b33"">34]</ref>, and hybrid methods <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b27"">28]</ref>. However, these app hs, which are hard to tune in practice. (3) Hybrid methods <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b27"">28]</ref> combine the above t ht for KG part is 0.1 for all datasets. The learning rate are the same as in SVD.</p><p>• RippleNet <ref type=""bibr"" target=""#b23"">[24]</ref> is a representative of hybrid methods, which is a memory-n",1
"and hybrid methods <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b27"">28]</ref>. However, these approaches rely on manual feature engineeri (3) Hybrid methods <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b27"">28]</ref> combine the above two categories and learn user/item embedd ecommender systems <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b30"">31,</ref><ref type=""bibr"" tar ized user preferences and interests. To account for the relational heterogeneity in KGs, similar to <ref type=""bibr"" target=""#b27"">[28]</ref>, we use a trainable and personalized relation scoring func where GCNs can be used directly, while here we investigate GCNs for heterogeneous KGs. Wang et al. <ref type=""bibr"" target=""#b27"">[28]</ref> use GCNs in KGs for recommendation, but simply applying GC o a user-personalized weighted graph that characterizes user's preferences. To this end, similar to <ref type=""bibr"" target=""#b27"">[28]</ref>, we use a user-specific relation scoring function s u (r ) ear that the performance of KGNN-LS with a non-zero λ is better than λ = 0 (the case of Wang et al. <ref type=""bibr"" target=""#b27"">[28]</ref>), which justifies our claim that LS regularization can ass",1
"ution"" (i.e., weighted average) to local neighbors of a node <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b14"">15]</ref>.</p><p>Recently, rese s for the architecture of our model, e.g., GCN<ref type=""bibr"" target=""#b10"">[11]</ref> or GraphSAGE<ref type=""bibr"" target=""#b6"">[7]</ref>. Here we use GCN<ref type=""bibr"" target=""#b10"">[11]</ref> as",0
"[3, 5-7, 11, 15]</ref>. Recently, several works developed GNNs architecture for recommender systems <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" ta "" target=""#b31"">[32]</ref> applies GCNs to the pin-board bipartite graph in Pinterest. Monti et al. <ref type=""bibr"" target=""#b13"">[14]</ref> and Berg et al. <ref type=""bibr"" target=""#b18"">[19]</ref>",0
"captured by the KG. Existing KG-aware recommender systems can be classified into path-based methods <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" targ ddition, embedding-based methods usually lack an end-to-end way of training. (2) Path-based methods <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" targ",0
"o path-based methods <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" target=""#b35"">36]</ref>, embedding-based methods <ref type=""bibr"" target=""#b8"">[9,< ) Path-based methods <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" target=""#b35"">36]</ref> explore various patterns of connections among items in a KG",0
"all KG-aware methods. The hyperparameter setting of KGNN-LS is provided in Appendix B.</p><p>• SVD <ref type=""bibr"" target=""#b11"">[12]</ref> is a classic CF-based model using inner product to model u",0
"V is the average of its neighboring entities, which leads to the following label propagation scheme <ref type=""bibr"" target=""#b38"">[39]</ref>: Theorem 2. Repeating the following two steps: <ref type=""",0
"works developed GNNs architecture for recommender systems <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" tar ipartite graph in Pinterest. Monti et al. <ref type=""bibr"" target=""#b13"">[14]</ref> and Berg et al. <ref type=""bibr"" target=""#b18"">[19]</ref> model recommender systems as matrix completion and design",0
"(2) Edge weights are parameterized and therefore learnable <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b34"">35]</ref>. Inspired by these ount of prior works <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b34"">35,</ref><ref type=""bibr"" tar",0
"rmance degradation as we will show later. Schlichtkrull et al. also propose using GCNs to model KGs <ref type=""bibr"" target=""#b16"">[17]</ref>, but not for the purpose of recommendations.</p></div> <di",0
"pproximate the convolutional filters by Chebyshev expansion of the graph Laplacian, and Kipf et al. <ref type=""bibr"" target=""#b10"">[11]</ref> propose a convolutional architecture via a first-order app arget=""#b1"">2</ref> There are several candidate designs for the architecture of our model, e.g., GCN<ref type=""bibr"" target=""#b10"">[11]</ref> or GraphSAGE<ref type=""bibr"" target=""#b6"">[7]</ref>. Here ""bibr"" target=""#b10"">[11]</ref> or GraphSAGE<ref type=""bibr"" target=""#b6"">[7]</ref>. Here we use GCN<ref type=""bibr"" target=""#b10"">[11]</ref> as our base model.</note> 			<note xmlns=""http://www.tei-c",0
"ref type=""bibr"" target=""#b37"">38]</ref>; (2) Edge weights are parameterized and therefore learnable <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" tar ssential role in representation learning on graphs, as highlighted by a large amount of prior works <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" tar",0
"weights are assumed to be given as input and therefore fixed <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b36"">37,</ref><ref type=""bibr"" target=""#b37"">38]</ref>; (2) Edge weights a",0
">(Peters et al., 2018)</ref>, GPT <ref type=""bibr"" target=""#b39"">(Radford et al., 2018)</ref>, BERT <ref type=""bibr"" target=""#b11"">(Devlin et al., 2019)</ref>, XLM <ref type=""bibr"" target=""#b25"">(Lamp with changes in data size or composition.</p><p>We present a replication study of BERT pretraining <ref type=""bibr"" target=""#b11"">(Devlin et al., 2019)</ref>, which includes a careful evaluation of t cently published methods. We release our model, pretraining and fine-tuning code.</p><p>Setup: BERT <ref type=""bibr"" target=""#b11"">(Devlin et al., 2019)</ref> takes as input a concatenation of two seg ive training formats:</p><p>• SEGMENT-PAIR+NSP: This follows the original input format used in BERT <ref type=""bibr"" target=""#b11"">(Devlin et al., 2019)</ref>, with the NSP loss. Each input has a pair d diverse corpora, such as the ones considered in this work.</p><p>The original BERT implementation <ref type=""bibr"" target=""#b11"">(Devlin et al., 2019)</ref>  Early experiments revealed only minor di SQUAD RESULTS</head><p>We adopt a much simpler approach for SQuAD compared to past work. While BERT <ref type=""bibr"" target=""#b11"">(Devlin et al., 2019)</ref> and XLNet <ref type=""bibr"" target=""#b56""> submit RoBERTa to the public SQuAD 2.0 leaderboard. Most of the top systems build upon either BERT <ref type=""bibr"" target=""#b11"">(Devlin et al., 2019)</ref> or XLNet <ref type=""bibr"" target=""#b56"">( )</ref>. This formulation significantly simplifies the task, but is not directly comparable to BERT <ref type=""bibr"" target=""#b11"">(Devlin et al., 2019)</ref>. Following recent work, we adopt the rank training with large batch sizes.</p><p>We pretrain with sequences of at most T = 512 tokens. Unlike <ref type=""bibr"" target=""#b11"">Devlin et al. (2019)</ref>, we do not randomly inject short sequences b4"">(Bowman et al., 2015)</ref>, which require predicting relationships between pairs of sentences. <ref type=""bibr"" target=""#b11"">Devlin et al. (2019)</ref> observe that removing NSP hurts performanc sults for the four different settings. We first compare the original SEGMENT-PAIR input format from <ref type=""bibr"" target=""#b11"">Devlin et al. (2019)</ref> to the SENTENCE-PAIR format; both formats that removing the NSP loss matches or slightly improves downstream task performance, in contrast to <ref type=""bibr"" target=""#b11"">Devlin et al. (2019)</ref>. It is possible that the original BERT imp and end-task accuracy for BERT BASE as we increase the batch size, while tuning the learning rate. <ref type=""bibr"" target=""#b11"">Devlin et al. (2019)</ref> originally trained BERT BASE for 1M steps alf as many optimization steps, thus seeing four times as many sequences in pretraining compared to <ref type=""bibr"" target=""#b11"">Devlin et al. (2019)</ref>.</p><p>To help disentangle the importance ers). We pretrain for 100K steps over a comparable BOOKCORPUS plus WIKIPEDIA dataset as was used in <ref type=""bibr"" target=""#b11"">Devlin et al. (2019)</ref>. We pretrain our model using 1024 V100 GPU et=""#b56"">Yang et al. (2019)</ref>.</p><p>For SQuAD v1.1 we follow the same finetuning procedure as <ref type=""bibr"" target=""#b11"">Devlin et al. (2019)</ref>. For SQuAD v2.0, we additionally classify ead>Table 7 :</head><label>7</label><figDesc>Comparison between the published BERT BASE results from<ref type=""bibr"" target=""#b11"">Devlin et al. (2019)</ref> to our reimplementation with either static ranslation <ref type=""bibr"" target=""#b30"">(McCann et al., 2017)</ref>, and masked language modeling <ref type=""bibr"" target=""#b11"">(Devlin et al., 2019;</ref><ref type=""bibr"" target=""#b25"">Lample &amp et al., 2019)</ref>. Performance is also typically improved by training bigger models on more data <ref type=""bibr"" target=""#b11"">(Devlin et al., 2019;</ref><ref type=""bibr"" target=""#b1"">Baevski et a",1
"erformance. Several efforts have trained on datasets larger and more diverse than the original BERT <ref type=""bibr"" target=""#b40"">(Radford et al., 2019;</ref><ref type=""bibr"" target=""#b56"">Yang et al 5"">(Gokaslan &amp; Cohen, 2019)</ref>, an open-source recreation of the WebText corpus described in <ref type=""bibr"" target=""#b40"">Radford et al. (2019)</ref>, containing web content extracted from UR ibr"" target=""#b1"">Baevski et al., 2019;</ref><ref type=""bibr"" target=""#b56"">Yang et al., 2019;</ref><ref type=""bibr"" target=""#b40"">Radford et al., 2019)</ref>. Our goal was to replicate, simplify, and",0
"of the query pronoun and referent. We then finetune RoBERTa using a variation of the approach from <ref type=""bibr"" target=""#b23"">Kocijan et al. (2019)</ref>. In particular, for a given input sentenc ive referent phrases than for any of the generated negative candidate phrases.</p><p>In contrast to <ref type=""bibr"" target=""#b23"">Kocijan et al. (2019)</ref>, who finetune BERT using a margin ranking",0
"0""><head n=""3.2"">DATA</head><p>BERT-style pretraining crucially relies on large quantities of text. <ref type=""bibr"" target=""#b1"">Baevski et al. (2019)</ref> demonstrate that increasing data size can ed by training bigger models on more data <ref type=""bibr"" target=""#b11"">(Devlin et al., 2019;</ref><ref type=""bibr"" target=""#b1"">Baevski et al., 2019;</ref><ref type=""bibr"" target=""#b56"">Yang et al.,",0
"tei-c.org/ns/1.0""><head>Next Sentence</head><p>Data: BERT is trained on a combination of BOOKCORPUS <ref type=""bibr"" target=""#b59"">(Zhu et al., 2015)</ref> plus English WIKIPEDIA, which totals 16GB of orpora of varying sizes and domains, totaling over 160GB of uncompressed text: (1&amp;2) BOOKCORPUS <ref type=""bibr"" target=""#b59"">(Zhu et al., 2015)</ref> plus English WIKIPEDIA, which is the origina",0
"is nondifferentiable. Therefore, for S with model parameters Φ, we use the policy gradient based RL <ref type=""bibr"" target=""#b13"">[Sutton et al., 2000]</ref> to derive its gradient:</p><formula xml:i",1
"al-aware or context-rich recommender systems <ref type=""bibr"" target=""#b12"">[Lin et al., 2019;</ref><ref type=""bibr"" target=""#b16"">Zhang et al., 2019b]</ref>, and other related fields such as network",0
"egative instances to train a better recommender as prediction model. The most related work is KBGAN <ref type=""bibr"" target=""#b1"">[Cai and Wang, 2018</ref>] that generates hard negative instances to t selects the item with the highest prediction score among X randomly sampled negatives.</p><p>-KBGAN <ref type=""bibr"" target=""#b1"">[Cai and Wang, 2018]</ref>. With the generator serving as an adversari",0
"implicit feedback-based recommender systems <ref type=""bibr"" target=""#b0"">[Bayer et al., 2017;</ref><ref type=""bibr"" target=""#b14"">Yang et al., 2018]</ref>. The key challenge in learning from implicit ic negative sampling (DNS) that oversamples the hard negative instances during the training process <ref type=""bibr"" target=""#b14"">[Zhang et al., 2013;</ref><ref type=""bibr"">Rendle and Freudenthaler, t generates hard negative instances to construct informative item pairs during the training process <ref type=""bibr"" target=""#b14"">[Zhang et al., 2013;</ref><ref type=""bibr"" target=""#b15"">Zhang et al. implicit feedback.</p><p>For methods related to the adversarial sampler, we choose:</p><p>-BPR-DNS <ref type=""bibr"" target=""#b14"">[Zhang et al., 2013]</ref>. Dynamic Negative Sampling (DNS) selects t",0
"is to uniformly sample negative instances from the missing data (i.e., the unobserved interactions) <ref type=""bibr"" target=""#b8"">[Jiang et al., 2018;</ref><ref type=""bibr"" target=""#b7"">He et al., 201",0
"on operation is set to 2, then all the potential words can easily fuse into corresponding positions <ref type=""bibr"" target=""#b4"">[Kim, 2014]</ref>. As shown in Figure <ref type=""figure"" target=""#fig_ tional efficiency. In general, end-to-end CNNs in NLP have mainly been used for text classification <ref type=""bibr"" target=""#b4"">[Kim, 2014]</ref>. For sequence labeling tasks, CNNs have been mainly state ← − h w i , which are concatenated for the NER prediction.</p><p>CNN. We apply a standard CNN <ref type=""bibr"" target=""#b4"">[Kim, 2014]</ref> structure on the character or word sequence to obtai 0""><head n=""4.3"">Hyper-Parameter Settings</head><p>For all four of the datasets, we used the Adamax <ref type=""bibr"" target=""#b4"">[Kingma and Ba, 2014]</ref> optimization to train our networks. The in",1
"to perform Chinese NER is to first perform word segmentation and then apply word sequence labeling <ref type=""bibr"" target=""#b7"">[Yang et al., 2016;</ref><ref type=""bibr"">He and Sun, 2017]</ref>.</p> rd information for NER has attracted research attention <ref type=""bibr"">[Passos et al., 2014;</ref><ref type=""bibr"" target=""#b7"">Zhang and Yang, 2018]</ref>. In particular, to exploit explicit word i ational efficiency <ref type=""bibr"">[Strubell et al., 2017]</ref>.</p><p>Specifically, lattice LSTM <ref type=""bibr"" target=""#b7"">[Zhang and Yang, 2018]</ref> employs double recurrent transition compu hinese word segmentation, character-based name taggers can outperform their word-based counterparts <ref type=""bibr"" target=""#b7"">[Zhang and Yang, 2018]</ref>. <ref type=""bibr"" target=""#b7"">Zhang and ilation. This method achieves great performance in the English NER task. Lattice LSTM. Lattice LSTM <ref type=""bibr"" target=""#b7"">[Zhang and Yang, 2018]</ref> can model the characters in sequence and ></head><label></label><figDesc>, Weibo NER[Peng and Dredze, 2015; He and Sun, 2016], and Resume NER<ref type=""bibr"" target=""#b7"">[Zhang and Yang, 2018]</ref>. The splitting methods follow those in<re NER<ref type=""bibr"" target=""#b7"">[Zhang and Yang, 2018]</ref>. The splitting methods follow those in<ref type=""bibr"" target=""#b7"">[Zhang and Yang, 2018]</ref>.The OntoNotes and MSRA are the newswire d utperform their word-based counterparts <ref type=""bibr"" target=""#b7"">[Zhang and Yang, 2018]</ref>. <ref type=""bibr"" target=""#b7"">Zhang and Yang [2018]</ref> exploit an RNNbased lattice structure to s",0
", . . . , and L-gram X L m ). In this work, we propose the use of the multi-scale feature attention <ref type=""bibr"" target=""#b6"">[Wang et al., 2018]</ref> to adaptively selects the features of differ that are competitive to the state-of-the-art <ref type=""bibr"" target=""#b1"">[Che et al., 2013;</ref><ref type=""bibr"" target=""#b6"">Wang et al., 2013]</ref>. However, due to the influence of the word se",0
"ks, including information retrieval <ref type=""bibr"">[Chen et al., 2015]</ref>, relation extraction <ref type=""bibr"" target=""#b0"">[Bunescu and Mooney, 2005]</ref>, question answering systems <ref type",0
"traction <ref type=""bibr"" target=""#b0"">[Bunescu and Mooney, 2005]</ref>, question answering systems <ref type=""bibr"" target=""#b2"">[Diefenbach et al., 2018]</ref>, and other applications. Compared with ""#fig_0"">2</ref>). To incorporate the lexicon feature effectively, we use the vectorbased attention <ref type=""bibr"" target=""#b2"">[Chen et al., 2018]</ref> to combine the l gram feature with the word",0
"abulous semantic segmentation models such as <ref type=""bibr"">FCN [Long et al., 2015]</ref>, SegNet <ref type=""bibr"" target=""#b0"">[Badrinarayanan et al., 2017]</ref>, <ref type=""bibr"">DeepLab-v3 [Chen work is based on the classic encoderdecoder network architecture without the fully-connected layers <ref type=""bibr"" target=""#b0"">[Badrinarayanan et al., 2017]</ref> and we improve it by adding the re semantic segmentation networks -FCN <ref type=""bibr"" target=""#b7"">[Long et al., 2015]</ref>, SegNet <ref type=""bibr"" target=""#b0"">[Badrinarayanan et al., 2017]</ref>, and DeepLab-V3 <ref type=""bibr"" t r network with that of <ref type=""bibr"">FCN-32s, FCN-16s, FCN-8s [Long et al., 2015]</ref>, Seg-Net <ref type=""bibr"" target=""#b0"">[Badrinarayanan et al., 2017], and</ref><ref type=""bibr"">DeepLab-v3 [C",1
"ibr"" target=""#b2"">Cordts et al., 2016;</ref><ref type=""bibr"" target=""#b1"">Caesar et al., 2018;</ref><ref type=""bibr"" target=""#b9"">Ros et al., 2016]</ref> contain substantially insufficient grassland i",0
""" target=""#b7"">[Mottaghi et al., 2014;</ref><ref type=""bibr"" target=""#b2"">Cordts et al., 2016;</ref><ref type=""bibr"" target=""#b1"">Caesar et al., 2018;</ref><ref type=""bibr"" target=""#b9"">Ros et al., 20 owards furthering botanical taxonomy, as illustrated by the wealth of research regarding this topic <ref type=""bibr"" target=""#b1"">[Cerutti et al., 2011;</ref><ref type=""bibr"" target=""#b5"">Kebapci et a",0
"ce and easy sample overwhelming, we substitute the classic Cross Entropy loss with a new Focal Loss <ref type=""bibr"" target=""#b6"">[Lin et al., 2017]</ref> to reduce the weight of easy samples. In this problem of class imbalance, we combine the advantages of reducing the class imbalance in Focal Loss<ref type=""bibr"" target=""#b6"">[Lin et al., 2017]</ref> and increasing class distance in smoothed Hin",0
"ertification.</p><p>The emergence of indicator plants is an important sign of grassland degradation <ref type=""bibr"" target=""#b12"">[Zhao et al., 2004]</ref>. Many countries have successfully used spec h five degradation stages before desertification, with the coverage of SC building up in each stage <ref type=""bibr"" target=""#b12"">[Zhao et al., 2004]</ref>, as shown in Table <ref type=""table"" target hip between SC coverage and the degradation stage (Table <ref type=""table"" target=""#tab_0"">1</ref>) <ref type=""bibr"" target=""#b12"">[Zhao et al., 2004]</ref> to obtain the degradation estimation. In th [Badrinarayanan et al., 2017]</ref>, <ref type=""bibr"">DeepLab-v3 [Chen et al., 2018a]</ref>, PSPNet <ref type=""bibr"" target=""#b12"">[Zhao et al., 2017]</ref> and etc. Leong first proposed Fully Convolu ""bibr"" target=""#b7"">[Long et al., 2015;</ref><ref type=""bibr"" target=""#b3"">Huang et al., 2015;</ref><ref type=""bibr"" target=""#b12"">He et al., 2017]</ref>. Considering class imbalance and easy sample o",0
"e nodes into a K-dimensional vector space, which preserves certain properties among nodes. Deepwalk <ref type=""bibr"" target=""#b6"">[Tang et al., 2015]</ref>, LINE <ref type=""bibr"" target=""#b6"">[Tang et tain properties among nodes. Deepwalk <ref type=""bibr"" target=""#b6"">[Tang et al., 2015]</ref>, LINE <ref type=""bibr"" target=""#b6"">[Tang et al., 2015]</ref> and Node2vec <ref type=""bibr"" target=""#b2"">[",1
"y anomalous edges detection, is then highly needed before the data are fed into the following tasks <ref type=""bibr"" target=""#b1"">[Akoglu et al., 2015;</ref><ref type=""bibr"" target=""#b4"">Ranshous et a d model which is inspired by <ref type=""bibr"" target=""#b3"">[Liu et al., 2017]</ref> and proposed by <ref type=""bibr"" target=""#b1"">[Cui et al., 2017]</ref>. In our framework, we construct short state o l></formula><p>GRU is a variant of LSTM network. It is simpler and more effective than LSTM network <ref type=""bibr"" target=""#b1"">[Chung et al., 2014]</ref>. GRU can record long-term information, and oss entropy to distinguish the existing edges and the generated ones. We then take the same idea in <ref type=""bibr"" target=""#b1"">[Bordes et al., 2013]</ref> and use marginbased pairwise loss in train ally build the required datasets because the ground-truth for the test phase is difficult to obtain <ref type=""bibr"" target=""#b1"">[Akoglu et al., 2015]</ref>, and we follow the approach used in <ref t",1
"the short-term pattern of nodes, we apply the contextual attention-based model which is inspired by <ref type=""bibr"" target=""#b3"">[Liu et al., 2017]</ref> and proposed by <ref type=""bibr"" target=""#b1""",1
"culty of detection. The similar anomaly pattern appears in the network attack against IP-IP network <ref type=""bibr"" target=""#b2"">[Eswaran et al., 2018]</ref>, where there are sudden large number of c tional Network) is a representative model to combine the content and structural features in a graph <ref type=""bibr"" target=""#b2"">[Kipf and Welling, 2017]</ref>. Compared with traditional graph method Tang et al., 2015]</ref>, LINE <ref type=""bibr"" target=""#b6"">[Tang et al., 2015]</ref> and Node2vec <ref type=""bibr"" target=""#b2"">[Grover and Leskovec, 2016]</ref> are the methods to yield node embedd xtends the idea of convolution model over regular graphs (i.e., image) to general graphs. The works <ref type=""bibr"" target=""#b2"">[Defferrard et al., 2016;</ref><ref type=""bibr"" target=""#b2"">Kipf and ., image) to general graphs. The works <ref type=""bibr"" target=""#b2"">[Defferrard et al., 2016;</ref><ref type=""bibr"" target=""#b2"">Kipf and Welling, 2017]</ref>improve the performance of basic GCN from",0
"ns=""http://www.tei-c.org/ns/1.0""><head n=""2.1"">Anomaly Detection in Dynamic Graph</head><p>Goutlier <ref type=""bibr"" target=""#b0"">[Aggarwal et al., 2011]</ref> is proposed with an observation that ano tasets.</p><p>Baselines. We compare AddGraph with three anomaly detection methods.</p><p>• GOutlier <ref type=""bibr"" target=""#b0"">[Aggarwal et al., 2011]</ref>. It builds a generative model for edges",0
"the data are fed into the following tasks <ref type=""bibr"" target=""#b1"">[Akoglu et al., 2015;</ref><ref type=""bibr"" target=""#b4"">Ranshous et al., 2015]</ref>.</p><p>It is not trivial to detect the an inly us-ing structural features. Other works <ref type=""bibr"" target=""#b7"">[Zhao and Yu, 2013;</ref><ref type=""bibr"" target=""#b4"">McConville et al., 2015]</ref> consider content feature or even tempor sides the structural features, the temporal ones are considered in the anomaly detection. CM-Sketch <ref type=""bibr"" target=""#b4"">[Ranshous et al., 2016]</ref> is a sketch-based method, which uses the de cluster, and the model can also be used to produce anomalous score for a given edge. • CM-Sketch <ref type=""bibr"" target=""#b4"">[Ranshous et al., 2016]</ref>. It uses the local structural feature an",0
"tant measure in detecting anomalous edges.</p><p>The works <ref type=""bibr"">[Sun et al., 2006;</ref><ref type=""bibr"" target=""#b5"">Shin et al., 2016;</ref><ref type=""bibr"" target=""#b5"">Shin et al., 201 orks <ref type=""bibr"">[Sun et al., 2006;</ref><ref type=""bibr"" target=""#b5"">Shin et al., 2016;</ref><ref type=""bibr"" target=""#b5"">Shin et al., 2017]</ref> view the anomaly as a dense sub-graph. <ref t </ref><ref type=""bibr"" target=""#b5"">Shin et al., 2017]</ref> view the anomaly as a dense sub-graph. <ref type=""bibr"" target=""#b5"">Shin [2016;</ref><ref type=""bibr"">2017]</ref> define a density functio",0
"ey define a density function and discover the target mainly us-ing structural features. Other works <ref type=""bibr"" target=""#b7"">[Zhao and Yu, 2013;</ref><ref type=""bibr"" target=""#b4"">McConville et a graph, while the method mainly focuses on structural features and cannot catch long-term anomalies. <ref type=""bibr"" target=""#b7"">[Yu et al., 2018]</ref> proposed NetWalk, a dynamic graph embedding mo btain <ref type=""bibr"" target=""#b1"">[Akoglu et al., 2015]</ref>, and we follow the approach used in <ref type=""bibr"" target=""#b7"">[Yu et al., 2018]</ref> to inject anomalous edges into two datasets.</ ure and historical behavior near an edge to measure whether the edge is anomalous or not. • NetWalk <ref type=""bibr"" target=""#b7"">[Yu et al., 2018]</ref>. The method first builds node embeddings based esults are shown in Table <ref type=""table"">1</ref>, in which the data of baselines are reported by <ref type=""bibr"" target=""#b7"">[Yu et al., 2018]</ref>. We can see that Ad-dGraph beats all baselines 3"">2</ref> illustrates the results on dynamic graph, in which the data of baselines are reported by <ref type=""bibr"" target=""#b7"">[Yu et al., 2018]</ref>. The results indicate that AddGraph beats base",0
"lution streams. This paper represents a very substantial extension of our previous conference paper <ref type=""bibr"" target=""#b105"">[105]</ref> with an additional material added from our unpublished t t object detection and instance segmentation frameworks. The main technical novelties compared with <ref type=""bibr"" target=""#b105"">[105]</ref> lie in threefold. <ref type=""bibr"" target=""#b0"">(1)</ref efold. <ref type=""bibr"" target=""#b0"">(1)</ref> We extend the network (named as HRNetV1) proposed in <ref type=""bibr"" target=""#b105"">[105]</ref>, to two versions: HRNetV2 and HRNetV2p, which explore al",1
"8]</ref>, ResNet <ref type=""bibr"" target=""#b39"">[39]</ref>, etc., follow the design rule of LeNet-5 <ref type=""bibr"" target=""#b61"">[61]</ref>. This is depicted in Figure <ref type=""figure"">1 (a)</ref>",0
"epeated several times which is inspired by deep fusion <ref type=""bibr"" target=""#b104"">[104]</ref>, <ref type=""bibr"" target=""#b117"">[117]</ref>, <ref type=""bibr"" target=""#b126"">[126]</ref>, <ref type=",0
"asses (59 classes + background). In both cases, HRNetV2-W48 achieves stateof-the-art results except <ref type=""bibr"" target=""#b36"">[36]</ref>.</p><p>LIP. The LIP dataset <ref type=""bibr"" target=""#b33""",0
"ctures (e.g., NASNet, AmoebaNet) are not efficient for inference. Recent hardware-aware NAS methods <ref type=""bibr"" target=""#b3"">(Cai et al., 2019;</ref><ref type=""bibr"" target=""#b32"">Tan et al., 201 yers and skip the last N -D layers, rather than keeping any D layers as done in current NAS methods <ref type=""bibr"" target=""#b3"">(Cai et al., 2019;</ref><ref type=""bibr"" target=""#b35"">Wu et al., 2019 nd input image size<ref type=""foot"" target=""#foot_1"">2</ref> . We also build a latency lookup table <ref type=""bibr"" target=""#b3"">(Cai et al., 2019)</ref> on each target hardware platform to predict t rms (Figure <ref type=""figure"" target=""#fig_10"">11</ref>) using the ProxylessNAS architecture space <ref type=""bibr"" target=""#b3"">(Cai et al., 2019)</ref>. OFA consistently improves the trade-off betw e). It is impossible for previous NAS methods<ref type=""bibr"" target=""#b32"">(Tan et al., 2019;</ref><ref type=""bibr"" target=""#b3"">Cai et al., 2019)</ref> due to the prohibitive training cost.</figDesc",1
"ype=""bibr"" target=""#b40"">Zhang et al., 2018)</ref> or accelerate the existing models by compression <ref type=""bibr"" target=""#b9"">(Han et al., 2016;</ref><ref type=""bibr"" target=""#b12"">He et al., 2018 iu et al., 2017)</ref>, and quantization that reduces the bit width for the weights and activations <ref type=""bibr"" target=""#b9"">(Han et al., 2016;</ref><ref type=""bibr"" target=""#b5"">Courbariaux et a g et al., 2018)</ref>, etc. Orthogonal to architecting efficient neural networks, model compression <ref type=""bibr"" target=""#b9"">(Han et al., 2016)</ref> is another very effective technique for effic",0
"ng data. We also use the knowledge distillation technique after training the largest neural network <ref type=""bibr"" target=""#b13"">(Hinton et al., 2015;</ref><ref type=""bibr"" target=""#b0"">Ashok et al.",0
"utomating the architecture design process <ref type=""bibr"" target=""#b42"">(Zoph &amp; Le, 2017;</ref><ref type=""bibr"" target=""#b43"">Zoph et al., 2018;</ref><ref type=""bibr"" target=""#b29"">Real et al., 2 "">Cai et al., 2018a;</ref><ref type=""bibr"" target=""#b24"">Liu et al., 2019)</ref>. Early NAS methods <ref type=""bibr"" target=""#b43"">(Zoph et al., 2018;</ref><ref type=""bibr"" target=""#b29"">Real et al.,",0
"cles respectively, which are determined to be a good set up experimentally. In fact, Jim?nez et al. <ref type=""bibr"" target=""#b2"">[3]</ref> shows a 50:1 of ""execution epoch vs sampling period"" is a re "">[31]</ref> studies the effect of hardware prefetching in virtualized environments. Jimenez et al. <ref type=""bibr"" target=""#b2"">[3]</ref> proposed an adaptive prefetch control to independently adjus",1
"LLC called Cache Allocation Technology (CAT) <ref type=""bibr"" target=""#b14"">[15]</ref>. Recent work <ref type=""bibr"" target=""#b15"">[16]</ref>- <ref type=""bibr"" target=""#b17"">[18]</ref> utilized CAT fo itecture work ( <ref type=""bibr"" target=""#b7"">[8]</ref>, <ref type=""bibr"" target=""#b10"">[11]</ref>, <ref type=""bibr"" target=""#b15"">[16]</ref>- <ref type=""bibr"" target=""#b17"">[18]</ref>), we use harmon plication. It should be noted that different partitions cannot overlap with each other. Cook et al. <ref type=""bibr"" target=""#b15"">[16]</ref> evaluates the effect of cache partitioning on real Intel m",0
"normalized turnaround time (ANTT), which is the reciprocal of harmonic speedup (HS) (Eyerman et al. <ref type=""bibr"" target=""#b21"">[22]</ref>). The calculation of HS requires the knowledge of an appli rg/ns/1.0""><head>C. Evaluation Metrics</head><p>Prior work <ref type=""bibr"" target=""#b4"">[5]</ref>, <ref type=""bibr"" target=""#b21"">[22]</ref>, <ref type=""bibr"" target=""#b26"">[27]</ref>- <ref type=""bib multicore processor. HS considers both system performance and fairness. According to Eyerman et al. <ref type=""bibr"" target=""#b21"">[22]</ref>, 1/HS is equal to the average turnaround time, which is a",0
"p><p>CP has also been researched intensively. Early work <ref type=""bibr"" target=""#b12"">[13]</ref>, <ref type=""bibr"" target=""#b13"">[14]</ref> proposed theory and michroarchitectural techniques. Recent </div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>C. Cache Partitioning</head><p>Qureshi et al. <ref type=""bibr"" target=""#b13"">[14]</ref> proposes a utility-based cache partitioning (UCP) mechanis",0
"prefetching to mitigate intra-application prefetch interference on Intel architectures. Liao et al. <ref type=""bibr"" target=""#b3"">[4]</ref> proposes a machine learning-based model to adjust the prefet",0
"ata</head><p>All experiments are performed on the publicly available Lib-riSpeech audio book corpus <ref type=""bibr"" target=""#b10"">[11]</ref>. We use the ""train-clean-100"" set as the paired data set, blic domain books. The books were selected such that there is no overlap with the dev and test sets <ref type=""bibr"" target=""#b10"">[11]</ref>. On the other hand, the training data set transcriptions a for the apostrophe in contractions (we replace hyphens with a space). Unlike the original LM corpus <ref type=""bibr"" target=""#b10"">[11]</ref> we take no steps to replace non-standard words with a cano",1
"ence model is an encoder-decoder architecture with attention <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b6"">7]</ref>. Let X = [X 1 , . . . , P (y u | X, y &lt;u ) = h(S u , Q u ).<label>(4)</label></formula><p>The function g(•) is a GRU RNN <ref type=""bibr"" target=""#b5"">[6]</ref> which encodes the previous token and query vector Q u−1 to p",1
"2."">MODEL</head><p>Our sequence-to-sequence model is an encoder-decoder architecture with attention <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target",1
"tecture with attention <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b6"">7]</ref>. Let X = [X 1 , . . . , X T ] be the frames of an utterance w",0
"target=""#b7"">[8]</ref>. We predict 5,000 sub-word targets generated with the SentencePiece toolkit <ref type=""bibr"" target=""#b13"">[14]</ref> using only ""train-clean-100"" as training data.</p></div> <",0
"rove pseudo-label quality, e.g. confidence-based filtering <ref type=""bibr"" target=""#b26"">[27,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b28"">29]</ref> and agreementbased",0
"d-to-end models seems to more significantly degrade the amount of available training data decreases <ref type=""bibr"" target=""#b0"">[1]</ref>. Transcribing large quantities of audio is both expensive an the clean dev and test sets are lower by more than 30% relative. On the other hand, Lüscher et al. <ref type=""bibr"" target=""#b0"">[1]</ref> use the sequence-tosequence model proposed in <ref type=""bib",0
"""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b28"">29]</ref> and agreementbased selection <ref type=""bibr"" target=""#b29"">[30]</ref> that also takes advantage of outputs from multiple systems",0
"ype=""bibr"" target=""#b2"">[3]</ref>, in addition to tasks in computer vision such as object detection <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b24"">25]</ref> and image classifi",0
"the text data set described in Section 4.1 using the same model architecture and training recipe as <ref type=""bibr"" target=""#b18"">[19]</ref>. In the following experiments, unless specified, we apply",0
"nclean-100"" subset of LibriSpeech as the labelled data set <ref type=""bibr"" target=""#b19"">[20,</ref><ref type=""bibr"" target=""#b20"">21]</ref>. Table <ref type=""table"" target=""#tab_0"">1</ref> shows the an LM.</p><p>""train-clean-100"" as well as several other results from the literature. Hayashi et al. <ref type=""bibr"" target=""#b20"">[21]</ref> use a sequence-to-sequence model with a BiLSTM-based encod a synthetic data set, but target hidden state representations instead of acoustic features directly <ref type=""bibr"" target=""#b20"">[21]</ref>. Alternatively, both unpaired audio and text can be used b",0
"as in the ""trainclean-100"" baseline. All experiments are implemented in the wav2letter++ framework <ref type=""bibr"" target=""#b16"">[17]</ref>.</p><p>Prior to generating pseudo-labels with any new comb",0
"pe=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b24"">25]</ref> and image classification <ref type=""bibr"" target=""#b25"">[26]</ref>.</p><p>In automatic speech recognition, self-training-styl",0
"ong outputs and (2) the model can predict the EOS token too early leading to an overly short output <ref type=""bibr"" target=""#b9"">[10]</ref>.</p><p>We filter for the first failure case by removing exa",0
"or poorly tuned supervised baseline model, a common issue with semi-supervised learning in general <ref type=""bibr"" target=""#b36"">[37]</ref>. In contrast, we compare our self-trained models to a well",0
"t al., 2018)</ref>, OpenAI GPT <ref type=""bibr"" target=""#b15"">(Radford et al., 2018)</ref> and BERT <ref type=""bibr"" target=""#b4"">(Devlin et al., 2018)</ref>. It has been shown that language modeling target=""#b2"">(Castro et al., 2018;</ref><ref type=""bibr"" target=""#b1"">de Araujo et al., 2018;</ref><ref type=""bibr"" target=""#b4"">Fernandes et al., 2018)</ref>. The model is composed of two bidirectio ained word embeddings were explored by <ref type=""bibr"" target=""#b2"">Castro et al. (2018)</ref> and <ref type=""bibr"" target=""#b4"">Fernandes et al. (2018)</ref> compared it to 3 other architectures. Th mming. During evaluation, the most likely sequence is obtained by Viterbi decoding. As described in <ref type=""bibr"" target=""#b4"">Devlin et al. (2018)</ref>, WordPiece tokenization requires prediction ead of using only the last hidden representation layer of BERT, we sum the last 4 layers, following <ref type=""bibr"" target=""#b4"">Devlin et al. (2018)</ref>. The resulting architecture resembles the L , we use document context for input examples instead of sentence context. Following the approach of <ref type=""bibr"" target=""#b4"">Devlin et al. (2018)</ref> on the SQuAD dataset, examples larger than pment set comprised of 10% of the First HAREM training set. We use the customized Adam optimizer of <ref type=""bibr"" target=""#b4"">Devlin et al. (2018)</ref>.</p><p>For the feature-based approach, we u",1
"dard datasets for training and evaluating Portuguese NER task are the HAREM Golden Collections (GC) <ref type=""bibr"" target=""#b17"">(Santos et al., 2006;</ref><ref type=""bibr"" target=""#b5"">Freitas et a in order to take into account vagueness and indeterminacy that can arise in sentence interpretation <ref type=""bibr"" target=""#b17"">(Santos et al., 2006)</ref>.</p><p>Despite enriching the datasets by",0
")</ref> has been commonly used in NER task <ref type=""bibr"" target=""#b2"">(Castro et al., 2018;</ref><ref type=""bibr"" target=""#b1"">de Araujo et al., 2018;</ref><ref type=""bibr"" target=""#b4"">Fernandes e",0
"cture <ref type=""bibr"" target=""#b10"">(Lample et al., 2016)</ref> has been commonly used in NER task <ref type=""bibr"" target=""#b2"">(Castro et al., 2018;</ref><ref type=""bibr"" target=""#b1"">de Araujo et sification is then performed by the CRF layer. Several pre-trained word embeddings were explored by <ref type=""bibr"" target=""#b2"">Castro et al. (2018)</ref> and <ref type=""bibr"" target=""#b4"">Fernandes l 10 classes. This is the same setup used by <ref type=""bibr"">Santos and Guimaraes (2015)</ref> and <ref type=""bibr"" target=""#b2"">Castro et al. (2018)</ref>.</p><p>Vagueness and indeterminacy: some te narios (total and selective) to the works of <ref type=""bibr"">Santos and Guimaraes (2015)</ref> and <ref type=""bibr"" target=""#b2"">Castro et al. (2018)</ref>. To make the results comparable to both wor f the fine-tuning approach, as expected. BERT-LSTM-CRF outperforms the reported results of LSTM-CRF <ref type=""bibr"" target=""#b2"">(Castro et al., 2018)</ref> by about 1.5 points on the selective scena",0
"</ref>) combined a CRF model with Local Grammars, following a similar approach.</p><p>Starting with <ref type=""bibr"" target=""#b3"">Collobert et al. (2011)</ref>, neural network NER systems have become )</ref>. The CharWNN model <ref type=""bibr"">(Santos and Guimaraes, 2015)</ref> extended the work of <ref type=""bibr"" target=""#b3"">Collobert et al. (2011)</ref> by employing a convolutional layer to ex",0
"are many definitions of named entity and evaluation criteria, introducing evaluation complications <ref type=""bibr"" target=""#b12"">(Marrero et al., 2013)</ref>.</p><p>Current state-of-the-art NER syst",0
"word embeddings and then used to perform sequential classification.</p><p>The LSTM-CRF architecture <ref type=""bibr"" target=""#b10"">(Lample et al., 2016)</ref> has been commonly used in NER task <ref t om tag i to tag j. A includes 2 additional states: start and end of sequence.</p><p>As described by <ref type=""bibr"" target=""#b10"">Lample et al. (2016)</ref>, for an input sequence X = (x 1 , x 2 , .. r"" target=""#b4"">Devlin et al. (2018)</ref>. The resulting architecture resembles the LSTM-CRF model <ref type=""bibr"" target=""#b10"">Lample et al. (2016)</ref> replacing its embedding techniques by BERT",0
"ems employ neural architectures that have been pre-trained on language modeling tasks, such as ELMo <ref type=""bibr"" target=""#b13"">(Peters et al., 2018)</ref>, OpenAI GPT <ref type=""bibr"" target=""#b15",0
"2017;</ref><ref type=""bibr"" target=""#b39"">Veličković et al., 2018;</ref><ref type=""bibr"">2019;</ref><ref type=""bibr"" target=""#b33"">Qu et al., 2019;</ref><ref type=""bibr"" target=""#b13"">Gao &amp; Ji, 20 rs and the edge weights between them correspond to the degree of trust between the users. Following <ref type=""bibr"" target=""#b33"">(Qu et al., 2019)</ref>, we treat edges with weights greater than 3 a t state-of-the-art methods GAT <ref type=""bibr"" target=""#b39"">(Veličković et al., 2018)</ref>, GMNN <ref type=""bibr"" target=""#b33"">(Qu et al., 2019)</ref> and Graph U-Net <ref type=""bibr"" target=""#b13 the results of GraphMix(GCN) are comparable with the recently proposed state-of-the-art method GMNN <ref type=""bibr"" target=""#b33"">(Qu et al., 2019)</ref>. Since GraphMix consists of various component ± 0.3% GraphScan <ref type=""bibr"" target=""#b11"">(Ding et al., 2018)</ref> 83.3 ±1.3 73.1±1.8 -GMNN <ref type=""bibr"" target=""#b33"">(Qu et al., 2019)</ref> 83.7% 73.1% 81.8% DisenGCN <ref type=""bibr"" t ; Welling, 2017)</ref>, GAT <ref type=""bibr"" target=""#b39"">(Veličković et al., 2018)</ref> and GMNN <ref type=""bibr"" target=""#b33"">(Qu et al., 2019)</ref>, among others. This architecture has one hidd hence much of the recent attention is dedicated to proposing architectural changes to these methods <ref type=""bibr"" target=""#b33"">(Qu et al., 2019;</ref><ref type=""bibr"" target=""#b13"">Gao &amp; Ji, 2",1
"t=""#b8"">Defferrard et al., 2016;</ref><ref type=""bibr"" target=""#b20"">Kipf &amp; Welling, 2016;</ref><ref type=""bibr"" target=""#b15"">Gilmer et al., 2017;</ref><ref type=""bibr"" target=""#b18"">Hamilton et ref type=""bibr"" target=""#b39"">(Veličković et al., 2018)</ref>, or any general message passing layer <ref type=""bibr"" target=""#b15"">(Gilmer et al., 2017)</ref>. Formally, let H l ∈ R n×k be a matrix co",0
"regions at multiple scales of the original image.</p><p>Lastly, inspired by the recent Transformer <ref type=""bibr"" target=""#b22"">[23]</ref> model in dealing with a number of difficult NLP tasks such idal way, following by corresponding pooling strategy. Moreover, inspired by the recent Transformer <ref type=""bibr"" target=""#b22"">[23]</ref> model in dealing with a number of difficult NLP tasks such iation between these local regions are still not explored. Inspired by the recent Transformer model <ref type=""bibr"" target=""#b22"">[23]</ref>, we implant a self attention module adapted to image featu g sentiment class number of datasets, and then tunes the parameters of all layers. • Self-Attention <ref type=""bibr"" target=""#b22"">[23]</ref> is a variant derived from the Transformer model. Transform",1
"""bibr"" target=""#b5"">[6]</ref>, SIFT-based bags of features <ref type=""bibr"" target=""#b2"">[3]</ref>, <ref type=""bibr"" target=""#b8"">[9]</ref>, Gist features <ref type=""bibr"" target=""#b8"">[9]</ref>, <ref res <ref type=""bibr"" target=""#b2"">[3]</ref>, <ref type=""bibr"" target=""#b8"">[9]</ref>, Gist features <ref type=""bibr"" target=""#b8"">[9]</ref>, <ref type=""bibr"" target=""#b9"">[10]</ref>, and middle attrib ""bibr"" target=""#b5"">[6]</ref>, SIFT-based bags of features <ref type=""bibr"" target=""#b2"">[3]</ref>, <ref type=""bibr"" target=""#b8"">[9]</ref>, Gist features <ref type=""bibr"" target=""#b8"">[9]</ref>, <ref res <ref type=""bibr"" target=""#b2"">[3]</ref>, <ref type=""bibr"" target=""#b8"">[9]</ref>, Gist features <ref type=""bibr"" target=""#b8"">[9]</ref>, <ref type=""bibr"" target=""#b9"">[10]</ref>, and so on. Beside",0
"years, the rise of convolutional neural networks (CNNs) <ref type=""bibr"" target=""#b12"">[13]</ref>, <ref type=""bibr"" target=""#b13"">[14]</ref> has made it possible to easily and automatically extract d rinciples-of-art (PAEF) for the image emotion classification and scoring task. • Fine-tuned AlexNet <ref type=""bibr"" target=""#b13"">[14]</ref>, VGG16 <ref type=""bibr"" target=""#b20"">[21]</ref>, Inceptio",0
"ifies the images with a linear SVM <ref type=""bibr"" target=""#b34"">[35]</ref>.</p><p>• DeepSentiBank <ref type=""bibr"" target=""#b35"">[36]</ref> classifies the images with a Linear SVM <ref type=""bibr"" t",0
"hallenges above, we propose to model the attributed networks with graph convolutional network (GCN) <ref type=""bibr"" target=""#b15"">[16]</ref>. GCN, which takes the topological structure and nodal attr se a new type of attributed network encoder inspired by the graph convolutional network (GCN) model <ref type=""bibr"" target=""#b15"">[16]</ref>. Specifically, GCN considers the high-order node proximity a particular layer, the convolution operation is D − 1 2 A D − 1 2 XW, and its complexity is O(mdh) <ref type=""bibr"" target=""#b15"">[16]</ref> as AX can be efficiently implemented using sparse-dense ma rning performance by considering neighbors of nodes that are multiple hops away. In particular, GCN <ref type=""bibr"" target=""#b15"">[16]</ref> takes the structure and attribute information as input, an autoencoder architecture. Meanwhile, recent research advances on graph convolutional network (GCN) <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" targ",1
"ent research advances on graph convolutional network (GCN) <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b11"">12]</ref> demonstrate superior",0
"k datasets that have been widely used in previous research <ref type=""bibr"" target=""#b18"">[19,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b7"">8]</ref> in our experiments:</",0
"previous research <ref type=""bibr"" target=""#b18"">[19,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b7"">8]</ref> in our experiments:</p><p>• BlogCatalog: BlogCatalog is a blo n. In particular, we refer to two anomaly injection methods that has been used in previous research <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b30"">31]</ref> to generate a combin d, to perturb the topological structure of an attributed network, we adopt the method introduced by <ref type=""bibr"" target=""#b7"">[8]</ref> to generate some small cliques. The intuition behind this me",0
"arget=""#b32"">33,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" target=""#b2"">3]</ref>. As one of the first",0
"out the learning process would be beneficial to enhance the OOD discriminative power of the system. <ref type=""bibr"" target=""#b3"">Hendrycks et al. (2019)</ref> demonstrate that utilizing auxiliary dat ing higher likelihood estimates on unseen OOD samples. The ominous observation is presented also by <ref type=""bibr"" target=""#b3"">Hendrycks et al. (2019)</ref>, but they concentrate on improving the O r"" target=""#b8"">Nalisnick et al., 2019a;</ref><ref type=""bibr"" target=""#b0"">Choi et al., 2018;</ref><ref type=""bibr"" target=""#b3"">Hendrycks et al., 2019)</ref>. These works report that despite intuiti",1
"ora of work demonstrates the effectiveness of deep generative models in this regard, recent work of <ref type=""bibr"" target=""#b8"">Nalisnick et al. (2019a)</ref> and <ref type=""bibr"" target=""#b0"">Choi y influences the phenomenon The results suggest that the intriguing phenomenon in VAEs discussed by <ref type=""bibr"" target=""#b8"">Nalisnick et al. (2019a)</ref> and <ref type=""bibr"" target=""#b0"">Choi noise model from Bernoulli to Gaussian (and otherwise remaining in the same experimental setting as <ref type=""bibr"" target=""#b8"">Nalisnick et al. (2019a)</ref>), the issue of assigning higher likelih s hardly feasible, with below-1/2 AUC scores. Meanwhile, with a Bernoulli noise model (also used in <ref type=""bibr"" target=""#b8"">Nalisnick et al. (2019a)</ref>) both the likelihoodestimates and the K the table (where models are trained on MNIST) confirm the asymmetric behaviour already described by <ref type=""bibr"" target=""#b8"">Nalisnick et al. (2019a)</ref>, that is, switching the roles of the in re> <figure xmlns=""http://www.tei-c.org/ns/1.0"" xml:id=""fig_1""><head></head><label></label><figDesc><ref type=""bibr"" target=""#b8"">Nalisnick et al. (2019a)</ref> examine the phenomenon in detail, focus luation of generative models on OOD data <ref type=""bibr"" target=""#b12"">(Shafaei et al., 2018;</ref><ref type=""bibr"" target=""#b8"">Nalisnick et al., 2019a;</ref><ref type=""bibr"" target=""#b0"">Choi et al",0
">(Xiao et al., 2017)</ref>: 28x28x1, 60.000 train + 10.000 test, 10 classes</p><p>• Kuzushiji-MNIST <ref type=""bibr"" target=""#b1"">(Clanuwat et al., 2018)</ref>: 28x28x1, 60.000 train + 10.000 test, 10",0
"btained negative samples are richer in features and semantically more meaningful for the task. (See <ref type=""bibr"" target=""#b6"">Lee et al. (2018)</ref> for an incarnation of this idea in the context asets carefully to obtain robust OOD detection.</p><p>Within the context of uncertainty estimation, <ref type=""bibr"" target=""#b6"">Lee et al. (2018)</ref> demonstrate that adversarially generated sampl samples of GANs is closest to our approach of using generated data points as negative samples, but <ref type=""bibr"" target=""#b6"">Lee et al. (2018)</ref> work within a classification setting. <ref typ",0
"re mostly inspired by and related to recent work on the evaluation of generative models on OOD data <ref type=""bibr"" target=""#b12"">(Shafaei et al., 2018;</ref><ref type=""bibr"" target=""#b8"">Nalisnick e",0
"t=""#b13"">(Kipf and Welling, 2017;</ref><ref type=""bibr"" target=""#b27"">Velickovic et al., 2018;</ref><ref type=""bibr"" target=""#b19"">Palm et al., 2018)</ref>, we propose an evidence reasoning network (E",1
"ype=""bibr"" target=""#b15"">Malon (2018)</ref> fine-tunes the generative pretraining transformer (GPT) <ref type=""bibr"" target=""#b22"">(Radford et al., 2018)</ref> for FV. Based on the methods mentioned a ation models such as ELMo <ref type=""bibr"" target=""#b21"">(Peters et al., 2018)</ref> and OpenAI GPT <ref type=""bibr"" target=""#b22"">(Radford et al., 2018)</ref> are proven to be effective on many NLP t",0
"r"" target=""#b1"">(Bowman et al., 2015;</ref><ref type=""bibr"" target=""#b20"">Parikh et al., 2016;</ref><ref type=""bibr"" target=""#b23"">Sha et al., 2016;</ref><ref type=""bibr"">Chen et al., 2017b,c;</ref><r",0
"ral large-scale datasets have been proposed to promote the research in this direction, such as SNLI <ref type=""bibr"" target=""#b1"">(Bowman et al., 2015)</ref> and Multi-NLI <ref type=""bibr"" target=""#b2 ade it feasible to train complicated neural models which have achieved the state-of-the-art results <ref type=""bibr"" target=""#b1"">(Bowman et al., 2015;</ref><ref type=""bibr"" target=""#b20"">Parikh et al",0
"rget=""#b16"">Munkhdalai and Yu, 2017;</ref><ref type=""bibr"" target=""#b17"">Nie and Bansal, 2017;</ref><ref type=""bibr"" target=""#b5"">Conneau et al., 2017;</ref><ref type=""bibr"" target=""#b9"">Gong et al.,",0
"2015) )</ref> and recurrent neural network <ref type=""bibr"" target=""#b22"">(Zhang et al., 2015;</ref><ref type=""bibr"" target=""#b23"">Zhou et al., 2016)</ref>. To automatically obtain a large training da ying relations, we apply an attention mechanism over a BiLSTM Encoder, which is first introduced in <ref type=""bibr"" target=""#b23"">(Zhou et al., 2016)</ref> for RC. The model architecture is illustrat g et al., 2015)</ref> is also commonly used for RE with the help of position embeddings. BiLSTM+ATT <ref type=""bibr"" target=""#b23"">(Zhou et al., 2016)</ref> adds an attention mechanism into BiLSTM to",1
"2014</ref><ref type=""bibr"" target=""#b20"">(Zeng et al., , 2015) )</ref> and recurrent neural network <ref type=""bibr"" target=""#b22"">(Zhang et al., 2015;</ref><ref type=""bibr"" target=""#b23"">Zhou et al., ef> is a revision of CNN which uses piecewise max-pooling to extract more relation features. BiLSTM <ref type=""bibr"" target=""#b22"">(Zhang et al., 2015)</ref> is also commonly used for RE with the help",1
"by the utilization of better model-based pattern extractor, and resorting to latent variable model <ref type=""bibr"" target=""#b7"">(Kim et al., 2018)</ref> for jointly modeling instance selector. What",0
"ef><ref type=""bibr"" target=""#b3"">Hamon and Nazarenko, 2001)</ref>. Among them, the generative model <ref type=""bibr"" target=""#b17"">(Takamatsu et al., 2012)</ref> directly models the labeling process o r"" target=""#b5"">(Hearst, 1992;</ref><ref type=""bibr"" target=""#b3"">Hamon and Nazarenko, 2001)</ref>. <ref type=""bibr"" target=""#b17"">Takamatsu et al. (2012)</ref> directly models the labeling process of",0
"( , 2017) )</ref> proposes to fuse DS-based labels and manual relation patterns for reducing noise. <ref type=""bibr"" target=""#b1"">Feng et al. (2018a)</ref> presents a pattern extractor based on RL and",0
"se problem. First, multiinstance learning <ref type=""bibr"" target=""#b15"">(Riedel et al., 2010;</ref><ref type=""bibr"" target=""#b8"">Lin et al., 2016;</ref><ref type=""bibr"" target=""#b16"">Surdeanu et al., thod is based on multi-instance  learning <ref type=""bibr"" target=""#b15"">(Riedel et al., 2010;</ref><ref type=""bibr"" target=""#b8"">Lin et al., 2016;</ref><ref type=""bibr"" target=""#b16"">Surdeanu et al., mpare our ARNOR framework with several strong baselines for noise reduction as follows: PCNN+SelATT <ref type=""bibr"" target=""#b8"">(Lin et al., 2016)</ref> is a bag-level RC model. It adopts an attenti",0
"e=""bibr"" target=""#b10"">Qin et al., 2018a;</ref><ref type=""bibr"" target=""#b4"">Han et al., 2018;</ref><ref type=""bibr"" target=""#b18"">Xiangrong et al., 2018;</ref><ref type=""bibr"" target=""#b11"">Qin et al on. The second kind of approach utilizes RL <ref type=""bibr"" target=""#b2"">(Feng et al., 2018b;</ref><ref type=""bibr"" target=""#b18"">Xiangrong et al., 2018;</ref><ref type=""bibr"" target=""#b11"">Qin et al",0
"fornia"". Such indicating words is called patterns <ref type=""bibr"" target=""#b5"">(Hearst, 1992;</ref><ref type=""bibr"" target=""#b3"">Hamon and Nazarenko, 2001)</ref>. The bold words ""was born in"" in s 1 traction is widely used in information extraction <ref type=""bibr"" target=""#b5"">(Hearst, 1992;</ref><ref type=""bibr"" target=""#b3"">Hamon and Nazarenko, 2001)</ref>. Among them, the generative model <re ances. The third research line relies on patterns <ref type=""bibr"" target=""#b5"">(Hearst, 1992;</ref><ref type=""bibr"" target=""#b3"">Hamon and Nazarenko, 2001)</ref>. <ref type=""bibr"" target=""#b17"">Takam",0
"ibr"" target=""#b8"">Lin et al., 2016;</ref><ref type=""bibr"" target=""#b16"">Surdeanu et al., 2012;</ref><ref type=""bibr"" target=""#b20"">Zeng et al., 2015)</ref> relaxes the DS assumption as at-least-one. I ibr"" target=""#b8"">Lin et al., 2016;</ref><ref type=""bibr"" target=""#b16"">Surdeanu et al., 2012;</ref><ref type=""bibr"" target=""#b20"">Zeng et al., 2015)</ref>. However, it models noise problem on a bag o tures, such as convolutional neural networks <ref type=""bibr"" target=""#b21"">(Zeng et al., 2014</ref><ref type=""bibr"" target=""#b20"">(Zeng et al., , 2015) )</ref> and recurrent neural network <ref type= e for RE. It introduces position embeddings to represent the location of an input entity pair. PCNN <ref type=""bibr"" target=""#b20"">(Zeng et al., 2015)</ref> is a revision of CNN which uses piecewise m",0
"type ""place of birth"" for ""Bill Lockyer"" and ""California"". Such indicating words is called patterns <ref type=""bibr"" target=""#b5"">(Hearst, 1992;</ref><ref type=""bibr"" target=""#b3"">Hamon and Nazarenko, thod relies on relation patterns. Pattern-based extraction is widely used in information extraction <ref type=""bibr"" target=""#b5"">(Hearst, 1992;</ref><ref type=""bibr"" target=""#b3"">Hamon and Nazarenko, "">Han et al., 2018)</ref> to select trustable instances. The third research line relies on patterns <ref type=""bibr"" target=""#b5"">(Hearst, 1992;</ref><ref type=""bibr"" target=""#b3"">Hamon and Nazarenko,",0
"characters and the gazetteers. Combined with an adapted Gated Graph Sequence Neural Networks (GGNN) <ref type=""bibr"" target=""#b10"">(Li et al., 2016)</ref> and a standard bidirectional LSTM-CRF <ref ty N <ref type=""bibr"" target=""#b6"">(Kipf and Welling, 2017)</ref>.</p><p>However, the traditional GGNN <ref type=""bibr"" target=""#b10"">(Li et al., 2016</ref>) is unable to distinguish edges with different",1
"(GGNN) <ref type=""bibr"" target=""#b10"">(Li et al., 2016)</ref> and a standard bidirectional LSTM-CRF <ref type=""bibr"" target=""#b7"">(Lample et al., 2016)</ref> (BiLSTM-CRF), our model learns a weighted are respec-tively 39.70%, 44.75%, 36.10% and 46.05%.</p><p>Models for Comparison. We use BiLSTM-CRF <ref type=""bibr"" target=""#b7"">(Lample et al., 2016)</ref> with character+bigram embedding without us",1
"imental Setup</head><p>Dataset. The three public datasets used in our experiments are OntoNotes 4.0 <ref type=""bibr"" target=""#b17"">(Weischedel et al., 2010)</ref>, MSRA <ref type=""bibr"" target=""#b8"">(",0
"s consisting of newswire text. Weibo-NER is in the domain of social media. We use the same split as <ref type=""bibr"" target=""#b1"">Che et al. (2013)</ref> and <ref type=""bibr"" target=""#b11"">Peng and Dr",0
"experiments are OntoNotes 4.0 <ref type=""bibr"" target=""#b17"">(Weischedel et al., 2010)</ref>, MSRA <ref type=""bibr"" target=""#b8"">(Levow, 2006)</ref>, and Weibo-NER <ref type=""bibr"" target=""#b11"">(Pen",0
"/1.0"" place=""foot"" n=""6"" xml:id=""foot_4"">We implemented the baseline models using the NCRFPP toolkit<ref type=""bibr"" target=""#b18"">(Yang and Zhang, 2018)</ref>.</note> 		</body> 		<back>  			<div type",0
"ers may also contain irrelevant and even erroneous information which harms the system's performance <ref type=""bibr"" target=""#b3"">(Chiu and Nichols, 2016)</ref>. This is especially the case for Chines eers may also contain irrelevant and even erroneous information which harms the system's performance<ref type=""bibr"" target=""#b3"">(Chiu and Nichols, 2016)</ref>. This is especially the case for Chines",0
"sting methods often rely on hand-crafted templates or predefined selection strategies. For example, <ref type=""bibr"" target=""#b12"">Qi et al. (2019)</ref> defined several n-gram templates to construct",0
"sentence <ref type=""bibr"" target=""#b15"">(Shang et al., 2018)</ref>, or maximum matching with rules <ref type=""bibr"" target=""#b13"">(Sassano, 2014)</ref>. Though general, these strategies are unable to a sentence<ref type=""bibr"" target=""#b15"">(Shang et al., 2018)</ref>, or maximum matching with rules<ref type=""bibr"" target=""#b13"">(Sassano, 2014)</ref>. Though general, these strategies are unable to",0
"ype=""bibr"" target=""#b20"">(Venugopalan et al. 2015)</ref> and M 3 (multimodal memory modeling) model <ref type=""bibr"" target=""#b22"">(Wang et al. 2016)</ref>, which is shown in Figure <ref type=""figure"" stage.</p><p>We compare our feature fusion models with several video caption models, including M 3 <ref type=""bibr"" target=""#b22"">(Wang et al. 2016)</ref>, Visual model (our basic video caption model 9"">6</ref> presents some sentences generated by GA (models with only generated audio features), M 3 <ref type=""bibr"" target=""#b22"">(Wang et al. 2016</ref>), V-ShaWei-GA models and human-annotated grou",1
"6"">(Yao et al. 2015)</ref> and extracting the last hidden state of recurrent visual feature encoder <ref type=""bibr"" target=""#b20"">(Venugopalan et al. 2015)</ref>.</p><p>Those feature encoding methods Our basic video caption framework is extended from S2VT (sequence to sequence: video to text) model <ref type=""bibr"" target=""#b20"">(Venugopalan et al. 2015)</ref> and M 3 (multimodal memory modeling)",1
"which take inspiration from image caption <ref type=""bibr"" target=""#b21"">(Vinyals et al. 2015;</ref><ref type=""bibr"" target=""#b3"">Donahue et al. 2015)</ref>.</p><p>We argue that these video caption me",0
"e Microsoft Research-Video to Text Dataset (MSR-VTT) and Microsoft Video Description Dataset (MSVD) <ref type=""bibr"" target=""#b1"">(Chen and Dolan 2011)</ref>. Their split method can be found in <ref t",0
"ype=""bibr"" target=""#b26"">Yao et al. 2015;</ref><ref type=""bibr"" target=""#b8"">Pan et al. 2016a;</ref><ref type=""bibr"" target=""#b9"">2016b)</ref>, which take inspiration from image caption <ref type=""bib",0
"some predefined sentence templates <ref type=""bibr"" target=""#b6"">(Krishnamoorthy et al. 2013;</ref><ref type=""bibr"" target=""#b18"">Thomason et al. 2014)</ref>. Then, probabilistic graphical model was",0
". 2016a;</ref><ref type=""bibr"" target=""#b9"">2016b)</ref>, which take inspiration from image caption <ref type=""bibr"" target=""#b21"">(Vinyals et al. 2015;</ref><ref type=""bibr"" target=""#b3"">Donahue et a",0
"<p>The second category treat video caption as a retrieval problem. They tagged videos with metadata <ref type=""bibr"" target=""#b0"">(Aradhye, Toderici, and Yagnik 2009)</ref> and then clustered videos a",0
"nvolutional neural networks (CNN) <ref type=""bibr"" target=""#b14"">(Simonyan and Zisserman 2014;</ref><ref type=""bibr"" target=""#b16"">Szegedy et al. 2015;</ref><ref type=""bibr"">2016)</ref>, recurrent neu",0
"0""><head n=""2.6.1"">Implementation</head><p>We implement the neural network using the torch7 library <ref type=""bibr"" target=""#b7"">(Collobert et al., 2011a)</ref>. Training and inference are done on a",1
"sible by recent advancements in unsupervised learning of word embeddings on massive amounts of data <ref type=""bibr"" target=""#b6"">(Collobert and Weston, 2008;</ref><ref type=""bibr"" target=""#b23"">Mikol",0
"996)</ref>. Recently, RNNs have shown great success in diverse NLP tasks such as speech recognition <ref type=""bibr"" target=""#b13"">(Graves et al., 2013)</ref>, machine translation <ref type=""bibr"" tar ides of a word and eliminates the problem of limited context that applies to any feed-forward model <ref type=""bibr"" target=""#b13"">(Graves et al., 2013)</ref>. While LSTMs have been studied in the pas ""2.1"">Sequence-labelling with BLSTM</head><p>Following the speech-recognition framework outlined by <ref type=""bibr"" target=""#b13"">Graves et al. (2013)</ref>, we employed a stacked 1 bi-directional re",0
"present no significant improvement over 50 dimensional embeddings -a result previously reported by <ref type=""bibr"" target=""#b35"">Turian et al. (2010)</ref>.</p><p>One possible reason that Collobert",0
"Zeiler and Fergus, 2014)</ref> of the input. The best networks are using more than 150 layers as in <ref type=""bibr"" target=""#b6"">(He et al., 2016a;</ref><ref type=""bibr"" target=""#b7"">He et al., 2016b n computer vision, in particular <ref type=""bibr"" target=""#b17"">(Simonyan and Zisserman, 2015;</ref><ref type=""bibr"" target=""#b6"">He et al., 2016a)</ref>.</p><p>This paper is structured as follows. Th ayers <ref type=""bibr"" target=""#b17"">(Simonyan and Zisserman, 2015)</ref>, or even up to 152 layers <ref type=""bibr"" target=""#b6"">(He et al., 2016a)</ref>. In the remainder of this paper, we describe serman, 2015)</ref>. We have also investigated the same kind of ""ResNet shortcut"" connections as in <ref type=""bibr"" target=""#b6"">(He et al., 2016a)</ref>, namely identity and 1 × 1 convolutions (see ng even deeper degrades accuracy. Shortcut connections help reduce the degradation. As described in <ref type=""bibr"" target=""#b6"">(He et al., 2016a)</ref>, the gain in accuracy due to the the increase onnections between convolutional blocks that allow the gradients to flow more easily in the network <ref type=""bibr"" target=""#b6"">(He et al., 2016a)</ref>.</p><p>We evaluate the impact of shortcut con works to temporal convolutions as we think this a milestone for going deeper in NLP. Residual units <ref type=""bibr"" target=""#b6"">(He et al., 2016a)</ref>  work and ImageNet is that the latter deals w",1
"rs. The design of our architecture is inspired by recent progress in computer vision, in particular <ref type=""bibr"" target=""#b17"">(Simonyan and Zisserman, 2015;</ref><ref type=""bibr"" target=""#b6"">He ch deeper networks <ref type=""bibr"" target=""#b12"">(Krizhevsky et al., 2012)</ref>, namely 19 layers <ref type=""bibr"" target=""#b17"">(Simonyan and Zisserman, 2015)</ref>, or even up to 152 layers <ref t erarchical manner. Our architecture can be in fact seen as a temporal adaptation of the VGG network <ref type=""bibr"" target=""#b17"">(Simonyan and Zisserman, 2015)</ref>. We have also investigated the s",1
"""#b3"">Collobert et al., 2011)</ref>. They have been subsequently applied to sentence classification <ref type=""bibr"" target=""#b11"">(Kim, 2014;</ref><ref type=""bibr"" target=""#b10"">Kalchbrenner et al., hich are projected into a high-dimensional space.</p><p>A rather shallow neural net was proposed in <ref type=""bibr"" target=""#b11"">(Kim, 2014)</ref>: one convolutional layer (using multiple widths and",0
"certainly LSTMs <ref type=""bibr"">(Hochreiter and</ref>   <ref type=""bibr"">meyer et al., 2012;</ref><ref type=""bibr"" target=""#b20"">Sutskever et al., 2014)</ref> to name just a few. However, we argue t",0
"words as basic units. An important step was the introduction of continuous representations of words <ref type=""bibr"" target=""#b1"">(Bengio et al., 2003)</ref>. These word embeddings are now the state-o",0
"At each node, the left and right context are combined using weights which are shared for all nodes <ref type=""bibr"" target=""#b18"">(Socher et al., 2011)</ref>. The state of the top node is fed to the",0
"""#b0"">(Bengio et al. (2001)</ref>, <ref type=""bibr"" target=""#b2"">Collobert and Weston (2008)</ref>, <ref type=""bibr"" target=""#b3"">Collobert et al. (2011)</ref> among others), the use of neural network neural networks for NLP appeared in <ref type=""bibr"" target=""#b2"">(Collobert and Weston, 2008;</ref><ref type=""bibr"" target=""#b3"">Collobert et al., 2011)</ref>. They have been subsequently applied to",0
". The level of granularity of this processing can range from individual characters to subword units <ref type=""bibr"" target=""#b16"">(Sennrich et al., 2016)</ref> or words up to whole sentences or even",0
"networks are using more than 150 layers as in <ref type=""bibr"" target=""#b6"">(He et al., 2016a;</ref><ref type=""bibr"" target=""#b7"">He et al., 2016b)</ref>.</p><p>Many NLP approaches consider words as b",0
"on social media given users' posts, connections among users, and a small number of labelled users. <ref type=""bibr"" target=""#b34"">Rahimi et al. (2018)</ref> apply GCNs with highway connections on thi",1
"get=""#b32"">(Monti et al., 2017;</ref><ref type=""bibr"" target=""#b12"">Duran &amp; Niepert, 2017;</ref><ref type=""bibr"" target=""#b27"">Li et al., 2018)</ref>; we refer to <ref type=""bibr"" target=""#b57"">Zh",1
"mp; Stephens, 1988)</ref> eventually gave rise to nonlinear CNNs with learned convolutional kernels <ref type=""bibr"" target=""#b44"">(Waibel et al., 1989;</ref><ref type=""bibr"" target=""#b25"">LeCun et al",0
"b10"">Chen et al. (2018)</ref> propose an efficient variant of GCN based on importance sampling, and <ref type=""bibr"" target=""#b15"">Hamilton et al. (2017)</ref> propose a framework based on sampling an ""bibr"" target=""#b52"">(Zhang et al., 2018a)</ref>, supervised and unsupervised variants of GraphSAGE <ref type=""bibr"" target=""#b15"">(Hamilton et al., 2017)</ref>, FastGCN, and DGI. Table <ref type=""tab either sampling to reduce neighborhood size <ref type=""bibr"" target=""#b10"">(Chen et al., 2018;</ref><ref type=""bibr"" target=""#b15"">Hamilton et al., 2017)</ref> or limiting their model sizes <ref type=",0
"ied chemistry <ref type=""bibr"" target=""#b29"">(Liao et al., 2019)</ref>, natural language processing <ref type=""bibr"" target=""#b51"">(Yao et al., 2019;</ref><ref type=""bibr"" target=""#b16"">Han et al., 20",0
"om initial simplicity to need-driven complexity. For instance, limitations of the linear Perceptron <ref type=""bibr"" target=""#b37"">(Rosenblatt, 1958)</ref> motivated the development of the more comple",0
"target=""#b56"">Zhou et al., 2004;</ref><ref type=""bibr"" target=""#b3"">Belkin &amp; Niyogi, 2004;</ref><ref type=""bibr"" target=""#b4"">Belkin et al., 2006)</ref> introduce a regularization term based on gr",0
"g et al., 2018c)</ref>, and computer vision <ref type=""bibr"" target=""#b45"">(Wang et al., 2018;</ref><ref type=""bibr"" target=""#b21"">Kampffmeyer et al., 2018)</ref>.</p><p>Historically, the development t=""#b41"">Thekumparampil et al., 2018;</ref><ref type=""bibr"" target=""#b52"">Zhang et al., 2018a;</ref><ref type=""bibr"" target=""#b21"">Kampffmeyer et al., 2018)</ref>. However, the attention mechanism usu",0
"nstream application demonstrates that learned graph convolution filters are superfluous; similar to <ref type=""bibr"" target=""#b9"">Changpinyo et al. (2018)</ref>'s observation that GCNs may not be nece",0
"=""bibr"" target=""#b51"">(Yao et al., 2019;</ref><ref type=""bibr"" target=""#b16"">Han et al., 2012;</ref><ref type=""bibr"" target=""#b55"">Zhang et al., 2018c)</ref>, and computer vision <ref type=""bibr"" targ Ta- Relation extraction involves predicting the relation between subject and object in a sentence. <ref type=""bibr"" target=""#b55"">Zhang et al. (2018c)</ref> propose C-GCN which uses an LSTM (Hochreit",0
">Duran &amp; Niepert, 2017;</ref><ref type=""bibr"" target=""#b27"">Li et al., 2018)</ref>; we refer to <ref type=""bibr"" target=""#b57"">Zhou et al. (2018)</ref>; <ref type=""bibr"" target=""#b2"">Battaglia et",0
"echanism usually adds significant overhead to computation and memory usage. We refer the readers to <ref type=""bibr"" target=""#b26"">Lee et al. (2018)</ref> for further comparison.</p></div> <div xmlns=",0
"neural network models have shown that end-to-end learning like convolutional neural networks (CNNs) <ref type=""bibr"" target=""#b24"">(Ma and Hovy, 2016a)</ref> or bidirectional long short-term memory (B",1
"p>Many recent sequence labeling frameworks <ref type=""bibr"" target=""#b25"">(Ma and Hovy, 2016b;</ref><ref type=""bibr"" target=""#b27"">Misawa et al., 2017)</ref> share a very basic structure: a bidirectio",1
"g/ns/1.0""><head n=""4.1"">The Base Model: BLSTM-CRF</head><p>Many recent sequence labeling frameworks <ref type=""bibr"" target=""#b25"">(Ma and Hovy, 2016b;</ref><ref type=""bibr"" target=""#b27"">Misawa et al",1
"g has been demonstrated to be an effective way of fulfilling the label consumption of neural models <ref type=""bibr"" target=""#b9"">(Guan et al., 2017;</ref><ref type=""bibr"" target=""#b18"">Lin et al., 20",0
"on crowd-sourced training set then feeding the generated labels to a Sequence Labeling Model (SLM) <ref type=""bibr"" target=""#b21"">(Liu et al., 2017)</ref>; (2) feeding multi-source data to a Multi-Ta",0
"pe=""bibr"" target=""#b24"">(Ma and Hovy, 2016a)</ref> or bidirectional long short-term memory (BLSTMs) <ref type=""bibr"" target=""#b13"">(Lample et al., 2016)</ref> can largely eliminate humancrafted featur largely eliminate humancrafted features.</p><p>BLSTM-CRF models have achieved promising performance <ref type=""bibr"" target=""#b13"">(Lample et al., 2016)</ref> and are used as our base sequence tagging",0
"979)</ref> proposes the pioneering work to aggregate crowd annotations to estimate true labels, and <ref type=""bibr"" target=""#b39"">Snow et al. (2008)</ref> shows its effectiveness with Amazon's Mechan",0
"cation.</p><p>Recent research shows the strength of multitask framework in semi-supervised learning <ref type=""bibr"" target=""#b14"">(Lan et al., 2018;</ref><ref type=""bibr"" target=""#b2"">Clark et al., 2",0
"Lan et al., 2018;</ref><ref type=""bibr"" target=""#b2"">Clark et al., 2018)</ref>, cross-type learning <ref type=""bibr"" target=""#b42"">(Wang et al., 2018)</ref>, and learning with entity triggers <ref typ arget=""#b21"">(Liu et al., 2017)</ref>; (2) feeding multi-source data to a Multi-Task Learning (MTL) <ref type=""bibr"" target=""#b42"">(Wang et al., 2018)</ref> model then aggregating multiple predicted l",0
"for one-to-one domain adaptation and does not model the differences among multiple source domains. <ref type=""bibr"" target=""#b44"">Yang and Eisenstein (2015)</ref> represents each domain with a vector",0
"titask framework in semi-supervised learning <ref type=""bibr"" target=""#b14"">(Lan et al., 2018;</ref><ref type=""bibr"" target=""#b2"">Clark et al., 2018)</ref>, cross-type learning <ref type=""bibr"" target",0
"of-speech (POS) tagging <ref type=""bibr"" target=""#b31"">(Ratnaparkhi, 1996)</ref>, word segmentation <ref type=""bibr"" target=""#b23"">(Low et al., 2005)</ref>, and named entity recognition (NER) <ref typ",0
"model takes Transformer as the encoder for CRF, which has shown its effectiveness in many NLP tasks <ref type=""bibr"" target=""#b41"">(Vaswani et al., 2017;</ref><ref type=""bibr"" target=""#b5"">Devlin et a quences with self-attention and eliminates all recurrence. Following the experimental settings from <ref type=""bibr"" target=""#b41"">(Vaswani et al., 2017)</ref>, we set the number of heads for multihea",0
"s no labels at all for target corpora. <ref type=""bibr"" target=""#b37"">Saito et al. (2017)</ref> and <ref type=""bibr"" target=""#b36"">Ruder and Plank (2018)</ref> explored bootstrapping with multitask tr unsupervised one-to-one domain adaptation <ref type=""bibr"" target=""#b37"">(Saito et al., 2017;</ref><ref type=""bibr"" target=""#b36"">Ruder and Plank, 2018)</ref>.</p></div> <div xmlns=""http://www.tei-c.",0
"fic and latency costs of accessing an off-chip Markov table <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b42"">43]</ref> GHB-Based Temporal Prefetchers. Wenisch et al. find that ta",1
"s directly target pointers by either using compiler hints or hardware structures to detect pointers <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" ta",0
"2]</ref> and strided <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b31"">32,</ref><ref type=""bibr"" tar",0
"to detect pointers <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b37"">38]</ref>. For example, Conte",0
"namely, Sampled Temporal Memory Streaming (STMS) <ref type=""bibr"" target=""#b44"">[45]</ref>, Domino <ref type=""bibr"" target=""#b3"">[4]</ref>, and MISB <ref type=""bibr"" target=""#b46"">[47]</ref>. STMS, D",0
"ion in detail and then extrapolate to the entire execution <ref type=""bibr"" target=""#b27"">[28,</ref><ref type=""bibr"" target=""#b33"">34]</ref>. A major challenge in sampled evaluation however is to quic is transferable across both hardware and software changes <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b33"">34]</ref>. Functional warming  warms up the microarchitecture state b es not allow for software changes. Functional warming (FW) <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b33"">34]</ref> does not incur any storage overhead, allows for software ch ect number of representative detailed regions that are evaluated in detail to then extrapolate from <ref type=""bibr"" target=""#b33"">[34]</ref>. The key challenge in sampling is to get (i) the correct a unctional fast-forwarding, checkpointing and virtualized fastforwarding. Functional fast-forwarding <ref type=""bibr"" target=""#b33"">[34]</ref> leverages functional simulation to get to the next represe a detailed warm-up using a small number of instructions (e.g., 30,000) prior to the detailed region <ref type=""bibr"" target=""#b33"">[34]</ref>. With this small amount of warming, only a small part of t instructions. Prior research shows that the highest accuracy is achieved for small detailed regions <ref type=""bibr"" target=""#b33"">[34]</ref>; larger detailed regions will likely make DeLorean even mo d to keep the caches warm using functional simulation in-between detailed regions as done in SMARTS <ref type=""bibr"" target=""#b33"">[34]</ref>. • CoolSim: Randomized Statistical Warming (RSW) is employ ture state using all memory references between two consecutive detailed regions, which is very slow <ref type=""bibr"" target=""#b33"">[34]</ref>. Various approaches have been proposed to reduce the warm-",1
"d does not incur any storage overhead and is transferable across both hardware and software changes <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b33"">34]</ref>. Functional warmin t, requires huge storage overhead, and does not allow for software changes. Functional warming (FW) <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b33"">34]</ref> does not incur any on, which is fast but does not allow for changes to the software. Virtualized Fast-Forwarding (VFF) <ref type=""bibr"" target=""#b25"">[26]</ref> leverages hardware virtualization to quickly get to the ne 31]</ref> extend on the concept of BLRL using a form of hardware state checkpoints. Sandberg et al. <ref type=""bibr"" target=""#b25"">[26]</ref> propose a method that uses two parallel simulations, pessi",1
"nds statistical cache modeling to cover other replacement algorithms, including pseudo-LRU and NMRU <ref type=""bibr"" target=""#b24"">[25]</ref>. Sen and Wood <ref type=""bibr"" target=""#b26"">[27]</ref> su al cache modeling was successfully demonstrated to model caches with different replacement policies <ref type=""bibr"" target=""#b24"">[25]</ref>, multi-programming workloads <ref type=""bibr"" target=""#b9""",0
"tance distribution -can be used to model other replacement algorithms as well. Beckmann and Sanchez <ref type=""bibr"" target=""#b1"">[2]</ref> propose probabilistic methods to model age-based replacement",0
"he same cacheline. These accesses are typically handled as Miss Status Holding Register (MSHR) hits <ref type=""bibr"" target=""#b2"">[3]</ref>. DSW models MSHR hits as a cache hit (in case of cache simul mlns=""http://www.tei-c.org/ns/1.0"" type=""table"" xml:id=""tab_3""><head></head><label></label><figDesc><ref type=""bibr"" target=""#b2"">3</ref>, warm-up cost can be amortized across multiple parallel simula",0
"urately approximated by tracking a subset of randomly selected reuse distances and memory locations <ref type=""bibr"" target=""#b4"">[5]</ref>. Finally, statistical cache modeling has been generalized an ations and computes their reuses during the warm-up interval prior to a detailed region. Prior work <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b22"">23]</ref> uses watchpoints to",0
"re changes -although there exist solutions to make checkpoints transferable across cache structures <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b31"">32,</ref><ref type=""bibr"" targ checkpoint size, in the context of SimPoint <ref type=""bibr"" target=""#b27"">[28]</ref>. Barr et al. <ref type=""bibr"" target=""#b0"">[1]</ref> propose the Memory Timestamp Record (MTR), a method to recor",0
"kpoint of the microarchitecture state prior to each region <ref type=""bibr"" target=""#b31"">[32,</ref><ref type=""bibr"" target=""#b32"">33]</ref>. Unfortunately, checkpoints require huge amounts of disk sp oss cache structures <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b31"">32,</ref><ref type=""bibr"" target=""#b32"">33]</ref>. Functional Warming (FW) on the other hand does not incur a e next representative region, which is slow. Checkpointing <ref type=""bibr"" target=""#b31"">[32,</ref><ref type=""bibr"" target=""#b32"">33]</ref> takes a snapshot of the architecture state for each region, s mentioned in the introduction, checkpointed warming (CW) <ref type=""bibr"" target=""#b31"">[32,</ref><ref type=""bibr"" target=""#b32"">33]</ref> is fast, requires huge storage overhead, and does not allow ns across the different passes, we count the number of dynamically executed user-space instructions <ref type=""bibr"" target=""#b32"">[33]</ref>.</p><p>Explorers: We develop multiple instances of gem5: S in SMARTS. Flex points are large (20 MiB to 100 MiB). In follow-up work, they introduce Live points <ref type=""bibr"" target=""#b32"">[33]</ref> and reduce the space requirements for each checkpoint down",0
"ny storage overhead, allows for software changes, but is slow. Randomized statistical warming (RSW) <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b22"">23]</ref> shares all the ben",0
"</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.2"">Multi-Programming</head><p>StatCC <ref type=""bibr"" target=""#b9"">[10]</ref> is a method in which sparse reuse information, collected se fferent replacement policies <ref type=""bibr"" target=""#b24"">[25]</ref>, multi-programming workloads <ref type=""bibr"" target=""#b9"">[10]</ref> as well as multithreaded workloads <ref type=""bibr"" target=",0
"d of these three operations is the critical path of the IQ, and is a critical path of the processor <ref type=""bibr"" target=""#b17"">[21]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head target=""#b26"">30]</ref>.</p><p>The most widely known circuit used as select logic is a tree arbiter <ref type=""bibr"" target=""#b17"">[21]</ref>. This circuit uses a tree structure to connect multiple sm t priority in order from the bottom of the stacked arbiters <ref type=""bibr"" target=""#b8"">[12,</ref><ref type=""bibr"" target=""#b17"">21]</ref>.</p><p>Note that the priority in arbitration is not flexibl e instructions in descending order in terms of the priority <ref type=""bibr"" target=""#b8"">[12,</ref><ref type=""bibr"" target=""#b17"">21]</ref>; that is, ?rant 0 is the grant signal of the instruction wi",1
"""#b25"">[29]</ref>. In fact, Intel switched from 6T to 8T cells for its 45nm line of Core processors <ref type=""bibr"" target=""#b14"">[18]</ref>. When using 8T cells, each bitline is single and an invert",0
"e, RAND is not used alone. A circuit called the age matrix <ref type=""bibr"" target=""#b18"">[22,</ref><ref type=""bibr"" target=""#b20"">24]</ref>, which selects the single oldest ready instruction, is used n in the RAND, an additional circuit called the age matrix <ref type=""bibr"" target=""#b18"">[22,</ref><ref type=""bibr"" target=""#b20"">24]</ref> is used. We refer to this type of IQ as AGE. The age matrix allocates transposed issue request lines for a group of instructions to reduce the age matrix width <ref type=""bibr"" target=""#b20"">[24]</ref>. The downside of this scheme is that it still requires an",0
"h misprediction penalty by issuing instructions in an unconfident branch slice as early as possible <ref type=""bibr"" target=""#b1"">[5]</ref>. Unconfident branch slices are defined as instructions that",0
"anizations, depending on the type of wakeup logic circuit: content-addressable memory (CAM) and RAM <ref type=""bibr"" target=""#b9"">[13]</ref>. These are both used in commercial processors. For example, future work.</p><p>The IQ mainly comprises the wakeup logic, select logic, tag RAM, and payload RAM <ref type=""bibr"" target=""#b9"">[13,</ref><ref type=""bibr"" target=""#b16"">20,</ref><ref type=""bibr"" tar",0
"segmentation <ref type=""bibr"" target=""#b19"">[20]</ref>, <ref type=""bibr"" target=""#b27"">[28]</ref>, <ref type=""bibr"" target=""#b38"">[39]</ref>. In dilated convolutional layers, filter weights are emplo",1
"ETS</head><p>Dilated convolutions were originally proposed for the computation of wavelet transform <ref type=""bibr"" target=""#b37"">[38]</ref> and employed in the deep learning context (as an alternati",1
"ifically, this technique is based upon an architecture composed exclusively on dilated convolutions <ref type=""bibr"" target=""#b27"">[28]</ref>, which are capable of processing input patch of varying si in most works <ref type=""bibr"" target=""#b16"">[17]</ref>, <ref type=""bibr"" target=""#b21"">[22]</ref>, <ref type=""bibr"" target=""#b27"">[28]</ref>), (ii) the multi-context strategy is exploited during the o deconvolution layers) mainly for semantic segmentation <ref type=""bibr"" target=""#b19"">[20]</ref>, <ref type=""bibr"" target=""#b27"">[28]</ref>, <ref type=""bibr"" target=""#b38"">[39]</ref>. In dilated con he expansion of the receptive field without increasing the number of trainable parameters per layer <ref type=""bibr"" target=""#b27"">[28]</ref>, which reduces the computational burden, and (ii) preserve d>B. Architectures</head><p>As presented in Section III, the properties of the dilated convolutions <ref type=""bibr"" target=""#b27"">[28]</ref> make them fit perfectly into the proposed multi-context me se map and, consequently, the final upsampled map. Such problem is overcome by dilated convolutions <ref type=""bibr"" target=""#b27"">[28]</ref>, which are allowed to process patches of any size, without get=""#b17"">[18]</ref> architecture) and exploited as a baseline in this work. (iii) dilated network <ref type=""bibr"" target=""#b27"">[28]</ref>, which is, in this case, the Dilated6Pooling (Figure <ref ility to have gaps between the filter weights, a special characteristic of the dilated convolutions <ref type=""bibr"" target=""#b27"">[28]</ref>. Such aspect makes all the difference since dilated convol The 5 remaining images (with <ref type=""bibr"">IDs 11,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b29"">30,</ref><ref type=""bibr"" tar rget=""#b6"">(7,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b34"">35,</ref><ref type=""bibr"" tar",1
"is usually performed using two strategies: (i) empirically <ref type=""bibr"" target=""#b7"">[8]</ref>, <ref type=""bibr"" target=""#b19"">[20]</ref>, by evaluating several sizes and selecting the best one, w ""#foot_0"">1</ref> . Multi-context paradigm has been proven to be essential for segmentation methods <ref type=""bibr"" target=""#b19"">[20]</ref>, <ref type=""bibr"" target=""#b22"">[23]</ref>, given that it et=""#b32"">[33]</ref>. Towards a better understanding of the Earth's surface, a myriad of techniques <ref type=""bibr"" target=""#b19"">[20]</ref>- <ref type=""bibr"" target=""#b21"">[22]</ref>, <ref type=""bib al predictions due to the combination of distinct fields of view and spatial context.</p><p>Sherrah <ref type=""bibr"" target=""#b19"">[20]</ref> proposed methods based on fully convolutional networks <re deep learning context (as an alternative to deconvolution layers) mainly for semantic segmentation <ref type=""bibr"" target=""#b19"">[20]</ref>, <ref type=""bibr"" target=""#b27"">[28]</ref>, <ref type=""bib , the obtained results, that do not have any post-processing, are better than others, such as DST 2 <ref type=""bibr"" target=""#b19"">[20]</ref>, that employ CRF as  Aside from this, the proposed work (U ork achieved competitive results, appearing in third place according to the overall accuracy. DST 5 <ref type=""bibr"" target=""#b19"">[20]</ref> and RIT L7 <ref type=""bibr"" target=""#b49"">[50]</ref> are t",1
"-of-the-art method for semantic segmentation is based on a resurgent approach, called deep learning <ref type=""bibr"" target=""#b10"">[11]</ref>, that can learn specific and adaptable spatial features di /p><p>Among all networks, a specific type, called Convolutional (Neural) Networks, ConvNets or CNNs <ref type=""bibr"" target=""#b10"">[11]</ref>, is the most traditional one for learning visual features tion, more parameters are included in the final model, resulting in a more complex learning process <ref type=""bibr"" target=""#b10"">[11]</ref>.</p><p>In this work, we propose a novel technique to perfo he sampled batch) represents only a single step (iteration) of the mini-batch optimization strategy <ref type=""bibr"" target=""#b10"">[11]</ref> (and not the full train) that processes one whole batch to loss and the accuracy.</p><p>In the first case, the loss is a measure (obtained using cross entropy <ref type=""bibr"" target=""#b10"">[11]</ref>, in this case) that represents the error generated in term comparing the same network trained using two distinct methods: (i) the traditional training process <ref type=""bibr"" target=""#b10"">[11]</ref>, in which the network is trained using patches of constant",0
"onsidered as uncategorized or unclassified.</p><p>3) Vaihingen Dataset: As introduced, this dataset <ref type=""bibr"" target=""#b42"">[43]</ref> was released for the 2D semantic labeling contest of the I s used for test, since this dataset has a clear definition of training/testing.</p><p>For Vaihingen <ref type=""bibr"" target=""#b42"">[43]</ref> and Potsdam <ref type=""bibr"" target=""#b44"">[45]</ref> data ><ref type=""bibr"">70)</ref>, i.e., the range varying from small to large patch sizes. For Vaihingen <ref type=""bibr"" target=""#b42"">[43]</ref>, the intermediate range <ref type=""bibr"" target=""#b44"">(45 >[42]</ref>, consisting of very high-resolution of visible spectrum images, (iii) Vaihingen dataset <ref type=""bibr"" target=""#b42"">[43]</ref>, composed of multispectral high-resolution images and norm",0
"best result was obtained when considering the largest range <ref type=""bibr"" target=""#b6"">(7,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" tar",0
"larger ones (for instance, 256 × 256).</p><p>Considering this, a full set of experiments (guided by <ref type=""bibr"" target=""#b39"">[40]</ref>) was performed in order to define the best architectures. .04.3 LTS was used as operating system.</p><p>As previously stated, a set of experiments (guided by <ref type=""bibr"" target=""#b39"">[40]</ref>) was executed to define the hyperparameters. After all the",0
"f type=""bibr"" target=""#b11"">[12]</ref>, such as lowlevel <ref type=""bibr"" target=""#b12"">[13]</ref>- <ref type=""bibr"" target=""#b14"">[15]</ref> and mid-level (e.g. Bag of Visual Words <ref type=""bibr"" t by our models, a very interesting feature specially when working with small amounts of labeled data <ref type=""bibr"" target=""#b14"">[15]</ref>.</p><p>In practice, these are the contributions of this wo full resolution of the image. Finally, the last strategy evaluated by the authors was to fine-tune <ref type=""bibr"" target=""#b14"">[15]</ref> pretrained networks over the remote sensing datasets. None cted by our models, a very important process mainly when working with small amounts of labeled data <ref type=""bibr"" target=""#b14"">[15]</ref>.</p><note type=""other"">Image nDSM Ground-Truth Dilated6 De d images were used to train the network. The 5 remaining images (with <ref type=""bibr"">IDs 11,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" tar",0
"o misclassify a different object as a car.</p><p>Given the importance of such task, several methods <ref type=""bibr"" target=""#b8"">[9]</ref>, <ref type=""bibr"" target=""#b9"">[10]</ref> have been proposed",0
"ntee for the best patch configuration), and (ii) imposed <ref type=""bibr"" target=""#b20"">[21]</ref>, <ref type=""bibr"" target=""#b21"">[22]</ref>, in which the patch size is defined by network constraints n be useful in other scenarios. Therefore, several works <ref type=""bibr"" target=""#b20"">[21]</ref>, <ref type=""bibr"" target=""#b21"">[22]</ref>, <ref type=""bibr"" target=""#b23"">[24]</ref>- <ref type=""bib istinct layers or networks, one for each context, and combine them for the final prediction. Others <ref type=""bibr"" target=""#b21"">[22]</ref>, <ref type=""bibr"" target=""#b24"">[25]</ref>, <ref type=""bib rstanding of the Earth's surface, a myriad of techniques <ref type=""bibr"" target=""#b19"">[20]</ref>- <ref type=""bibr"" target=""#b21"">[22]</ref>, <ref type=""bibr"" target=""#b23"">[24]</ref>- <ref type=""bib 35]</ref> are used as a post-processing method in an attempt to improve the final results.</p><p>In <ref type=""bibr"" target=""#b21"">[22]</ref>, the authors proposed multi-context methods that combine b the input data (a common process performed in most works <ref type=""bibr"" target=""#b16"">[17]</ref>, <ref type=""bibr"" target=""#b21"">[22]</ref>, <ref type=""bibr"" target=""#b27"">[28]</ref>), (ii) the mult ded competitive results. The best result, in terms of overall accuracy, was 90.3% achieved by DLR 9 <ref type=""bibr"" target=""#b21"">[22]</ref> and GSN3 <ref type=""bibr"" target=""#b26"">[27]</ref>. Our be ). In fact, the number of parameters of the network is so relevant that authors of DLR 9 submission <ref type=""bibr"" target=""#b21"">[22]</ref>, one of the best results but with a higher number of param",0
"has been proven to be essential for segmentation methods <ref type=""bibr"" target=""#b19"">[20]</ref>, <ref type=""bibr"" target=""#b22"">[23]</ref>, given that it allows the model to extract and capture pat this order), which are the most useful and representative ones for discriminating vegetation areas <ref type=""bibr"" target=""#b22"">[23]</ref>. More specifically, the dataset consists of 1,250,000 pixe",0
"8</ref>.</p><p>4) Potsdam Dataset: Also proposed for the 2D semantic labeling contest, this dataset <ref type=""bibr"" target=""#b44"">[45]</ref> has 38 tiles of the same size (6000 × 6000 pixels), with a tion of training/testing.</p><p>For Vaihingen <ref type=""bibr"" target=""#b42"">[43]</ref> and Potsdam <ref type=""bibr"" target=""#b44"">[45]</ref> datasets, we followed the protocol proposed by <ref type="" multispectral high-resolution images and normalized Digital Surface Model, and (iv) Potsdam dataset <ref type=""bibr"" target=""#b44"">[45]</ref>, also composed of multispectral high-resolution images and large patch sizes. For Vaihingen <ref type=""bibr"" target=""#b42"">[43]</ref>, the intermediate range <ref type=""bibr"" target=""#b44"">(45,</ref><ref type=""bibr"">55,</ref><ref type=""bibr"">65,</ref><ref ty",0
"mental monitoring, intelligent agriculture <ref type=""bibr"" target=""#b4"">[5]</ref>, disaster relief <ref type=""bibr"" target=""#b5"">[6]</ref>, <ref type=""bibr"" target=""#b6"">[7]</ref>, urban planning <re",0
"emantic segmentation by combining ConvNets, hand-crafted descriptors, and Conditional Random Fields <ref type=""bibr"" target=""#b34"">[35]</ref>. Specifically, they trained three ConvNets, each one with hand-crafted ones and classified using random forest classifier. Finally, Conditional Random Fields <ref type=""bibr"" target=""#b34"">[35]</ref> are used as a post-processing method in an attempt to impr elated to the proposed work do not use any post-processing, such as Conditional Random Fields (CRF) <ref type=""bibr"" target=""#b34"">[35]</ref>.</p><p>Some official results reported by the organization get=""#b13"">14,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b34"">35,</ref><ref type=""bibr"" target=""#b41"">42,</ref><ref type=""bibr"" tar",0
"of the network to capture multi-context low-and highlevel information. They fine-tuned a ResNet-101 <ref type=""bibr"" target=""#b36"">[37]</ref> to extract salient information from 600 × 600 patches. Fea",0
"get=""#b27"">28,</ref><ref type=""bibr"" target=""#b34"">35,</ref><ref type=""bibr"" target=""#b41"">42,</ref><ref type=""bibr"" target=""#b48"">49,</ref><ref type=""bibr"">56,</ref><ref type=""bibr"">63,</ref><ref typ in fifth place by yielding 89.4% of overall accuracy, outperforming several methods, such as ADL 3 <ref type=""bibr"" target=""#b48"">[49]</ref> and RIT L8 <ref type=""bibr"" target=""#b49"">[50]</ref>, that",0
", outperforming several methods, such as ADL 3 <ref type=""bibr"" target=""#b48"">[49]</ref> and RIT L8 <ref type=""bibr"" target=""#b49"">[50]</ref>, that also tried to aggregate multi-context information. H place according to the overall accuracy. DST 5 <ref type=""bibr"" target=""#b19"">[20]</ref> and RIT L7 <ref type=""bibr"" target=""#b49"">[50]</ref> are the best result in terms of overall accuracy. However,",0
"f the image. For the GRSS Data Fusion Dataset, we employed, as baseline, the work of Santana et al. <ref type=""bibr"" target=""#b45"">[46]</ref>. Their algorithm extracts features with many levels of con though all of them achieved comparable results, the best outcome was yielded by the Deep Contextual <ref type=""bibr"" target=""#b45"">[46]</ref>. This method also leverages from multicontext information, proposed approach, which improves the results for all metrics when compared to the Deep Contextual <ref type=""bibr"" target=""#b45"">[46]</ref> approach. This reaffirms the effectiveness of the proposed",0
"get=""#b14"">14,</ref><ref type=""bibr"" target=""#b36"">36,</ref><ref type=""bibr"" target=""#b39"">39,</ref><ref type=""bibr"" target=""#b38"">38]</ref>. Although considerable progress has been achieved in image get=""#b17"">17,</ref><ref type=""bibr"" target=""#b30"">30,</ref><ref type=""bibr"" target=""#b39"">39,</ref><ref type=""bibr"" target=""#b38"">38]</ref>. Due to space limitation, we here briefly review works rela arget=""#b13"">13,</ref><ref type=""bibr"" target=""#b6"">6,</ref><ref type=""bibr"" target=""#b39"">39,</ref><ref type=""bibr"" target=""#b38"">38]</ref>. For example, Dong et al. <ref type=""bibr"" target=""#b2"">[ 2 ing discriminative ability of the network. Channel attention <ref type=""bibr"" target=""#b9"">[9,</ref><ref type=""bibr"" target=""#b38"">38]</ref> has been shown to be effective for better discriminative re rget=""#b20"">[20,</ref><ref type=""bibr"" target=""#b6"">6,</ref><ref type=""bibr"" target=""#b39"">39,</ref><ref type=""bibr"" target=""#b38"">38]</ref>, we use 800 high-resolution images from DIV2K dataset <ref >[38]</ref>. As in <ref type=""bibr"" target=""#b20"">[20,</ref><ref type=""bibr"" target=""#b39"">39,</ref><ref type=""bibr"" target=""#b38"">38]</ref>, we also adopt self-ensemble method to further improve our f the network, some other networks, such as NLRN <ref type=""bibr"" target=""#b22"">[22]</ref> and RCAN <ref type=""bibr"" target=""#b38"">[38]</ref>, improve the performance by considering feature correlatio classification.</p><p>Recently, SENet was introduced to deep CNNs to further improve SR performance <ref type=""bibr"" target=""#b38"">[38]</ref>. However, SENet only explores first-order statistics (e.g. se feature interdependencies. Difference to Residual Channel Attention Network (RCAN). Zhang et al. <ref type=""bibr"" target=""#b38"">[38]</ref> proposed a residual in residual structure to form a very d sidual blocks in each LSRAG, thus resulting in deep network with over 400 convolution layers. As in <ref type=""bibr"" target=""#b38"">[38]</ref>, we also add long and short skip connections in Base model BPN <ref type=""bibr"" target=""#b6"">[6]</ref>, RDN <ref type=""bibr"" target=""#b39"">[39]</ref> and RCAN <ref type=""bibr"" target=""#b38"">[38]</ref>. As in <ref type=""bibr"" target=""#b20"">[20,</ref><ref type= <ref type=""bibr"" target=""#b36"">[36]</ref>, RDN <ref type=""bibr"" target=""#b39"">[39]</ref>, and RCAN <ref type=""bibr"" target=""#b38"">[38]</ref>. All the results on 3× are shown in Table <ref type=""table",1
"to improve the performance of CNNs for various tasks, such as image and video classification tasks <ref type=""bibr"" target=""#b9"">[9,</ref><ref type=""bibr"" target=""#b33"">33]</ref>. Wang et al. <ref ty ks. The second one is the way of enhancing discriminative ability of the network. Channel attention <ref type=""bibr"" target=""#b9"">[9,</ref><ref type=""bibr"" target=""#b38"">38]</ref> has been shown to be rate non-local operations for spatial attention in video classification. On the contrary, Hu et al. <ref type=""bibr"" target=""#b9"">[ 9]</ref> proposed SENet to exploit channel-wise relationships to ach N-based SR models do not consider the feature interdependencies. To utilize such information, SENet <ref type=""bibr"" target=""#b9"">[9]</ref> was introduced in CNNs to rescale the channelwise features f he aggregated information by global covariance pooling, we apply a gating mechanism. As explored in <ref type=""bibr"" target=""#b9"">[9]</ref>, the simple sigmoid function can serve as a proper gating fu",1
"ef type=""bibr"" target=""#b36"">[36]</ref> (e) EDSR <ref type=""bibr"" target=""#b36"">[36]</ref> (f) DBPN <ref type=""bibr"" target=""#b20"">[20]</ref> (g) RDN <ref type=""bibr"" target=""#b6"">[6]</ref> (h) Ours</ with more than 16 layers based on residual learning. To further improve the performance, Lim et al. <ref type=""bibr"" target=""#b20"">[20]</ref> proposed a very deep and wide network EDSR by stacking mod <ref type=""bibr"" target=""#b14"">[14]</ref>, Mem-Net <ref type=""bibr"" target=""#b30"">[30]</ref>, EDSR <ref type=""bibr"" target=""#b20"">[20]</ref>, SRMD <ref type=""bibr"" target=""#b36"">[36]</ref>, NLRN <ref module, and reconstruction part. Given I LR and I SR as the input and output of SAN. As explored in <ref type=""bibr"" target=""#b20"">[20,</ref><ref type=""bibr"" target=""#b39"">39]</ref>, we apply only one ndencies.</p><p>It has been verified that stacking residual blocks is helpful to form a deep CNN in <ref type=""bibr"" target=""#b20"">[20,</ref><ref type=""bibr"" target=""#b39"">39]</ref>. However, very dee filter are set as 3 × 3 and C =6 4 , respectively. For upscale part H ↑ (•), we follow the works in <ref type=""bibr"" target=""#b20"">[20,</ref><ref type=""bibr"" target=""#b39"">39]</ref> and apply ESPCNN < ments</head></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.1."">Setup</head><p>Following <ref type=""bibr"" target=""#b20"">[20,</ref><ref type=""bibr"" target=""#b6"">6,</ref><ref type=""bibr"" targ <ref type=""bibr"" target=""#b39"">[39]</ref> and RCAN <ref type=""bibr"" target=""#b38"">[38]</ref>. As in <ref type=""bibr"" target=""#b20"">[20,</ref><ref type=""bibr"" target=""#b39"">39,</ref><ref type=""bibr"" ta 30"">30]</ref>, L 1 <ref type=""bibr"" target=""#b14"">[14,</ref><ref type=""bibr"" target=""#b15"">15,</ref><ref type=""bibr"" target=""#b20"">20,</ref><ref type=""bibr"" target=""#b39"">39]</ref>, perceptual losses",1
"h blur-down degradation (BD) model. We compare our method with 8 state-of-the-art SR methods: SPMSR <ref type=""bibr"" target=""#b24"">[24]</ref>, SRCNN <ref type=""bibr"" target=""#b2"">[2]</ref>, FSRCNN <re",0
"rget=""#b2"">[2,</ref><ref type=""bibr"" target=""#b29"">29,</ref><ref type=""bibr"" target=""#b14"">14,</ref><ref type=""bibr"" target=""#b13"">13,</ref><ref type=""bibr"" target=""#b29"">29,</ref><ref type=""bibr"" tar er network structure <ref type=""bibr"" target=""#b2"">[2,</ref><ref type=""bibr"" target=""#b12"">12,</ref><ref type=""bibr"" target=""#b13"">13,</ref><ref type=""bibr"" target=""#b6"">6,</ref><ref type=""bibr"" targe formance. Later, Kim et al. designed deeper VDSR <ref type=""bibr"" target=""#b12"">[12]</ref> and DRCN <ref type=""bibr"" target=""#b13"">[13]</ref> with more than 16 layers based on residual learning. To fu",0
"act as upscale part, such as transposed convolution <ref type=""bibr"" target=""#b3"">[3]</ref>, ESPCN <ref type=""bibr"" target=""#b28"">[28]</ref>.</p><p>The way of embedding upscaling feature in the last <ref type=""bibr"" target=""#b20"">[20,</ref><ref type=""bibr"" target=""#b39"">39]</ref> and apply ESPCNN <ref type=""bibr"" target=""#b28"">[28]</ref> to upscale the deep features, followed by one final convol",0
"<ref type=""bibr"" target=""#b20"">20,</ref><ref type=""bibr"" target=""#b39"">39]</ref>, perceptual losses <ref type=""bibr"" target=""#b11"">[11,</ref><ref type=""bibr"" target=""#b26"">26]</ref>. To verify the eff n-Schulz iteration to speed up the computation of covariance normalization. Specifically, from Equ. <ref type=""bibr"" target=""#b11"">(11)</ref>, the Σ has square root as Σ 1/2 = Y = Udiag(λ 1/2 i )U T .",0
"various tasks, such as image and video classification tasks <ref type=""bibr"" target=""#b9"">[9,</ref><ref type=""bibr"" target=""#b33"">33]</ref>. Wang et al. <ref type=""bibr"" target=""#b33"">[ 33]</ref> pro tasks <ref type=""bibr"" target=""#b9"">[9,</ref><ref type=""bibr"" target=""#b33"">33]</ref>. Wang et al. <ref type=""bibr"" target=""#b33"">[ 33]</ref> proposed non-local neural network to incorporate non-loca in HR nature scenes by RL-NL modules plugged before and after the SSRG. The nonlocal neural network <ref type=""bibr"" target=""#b33"">[33]</ref> is proposed to capture the computation of long-range depen",0
"d, ranging from early interpolation-based <ref type=""bibr"" target=""#b37"">[37]</ref> and model-based <ref type=""bibr"" target=""#b4"">[4]</ref>, to recent learning-based methods <ref type=""bibr"" target=""# rs, such as non-local similarity prior <ref type=""bibr"" target=""#b34"">[34]</ref> and sparsity prior <ref type=""bibr"" target=""#b4"">[4]</ref>. Although such model-based methods are flexible to produce r",0
"ly means that human visual systems adaptively process visual information and focus on salient areas <ref type=""bibr"" target=""#b16"">[16]</ref>. In recent years, several trials have embeded attention pr",0
"en reduced to half every 200 epochs. Our proposed SAN has been implemented on the Pytorch framework <ref type=""bibr"" target=""#b23"">[23]</ref> on an Nvidia 1080Ti GPU.</p></div> <div xmlns=""http://www.",0
"get=""#b14"">14,</ref><ref type=""bibr"" target=""#b13"">13,</ref><ref type=""bibr"" target=""#b29"">29,</ref><ref type=""bibr"" target=""#b17"">17,</ref><ref type=""bibr"" target=""#b30"">30,</ref><ref type=""bibr"" tar",0
"recently achieved unprecedented success in various problems <ref type=""bibr"" target=""#b7"">[7,</ref><ref type=""bibr"" target=""#b25"">25]</ref>. The powerful feature representation and end-to-end trainin",0
"type=""bibr"" target=""#b34"">[34]</ref>, and CNN-based methods <ref type=""bibr"" target=""#b2"">[2,</ref><ref type=""bibr"" target=""#b29"">29,</ref><ref type=""bibr"" target=""#b14"">14,</ref><ref type=""bibr"" tar get=""#b29"">29,</ref><ref type=""bibr"" target=""#b14"">14,</ref><ref type=""bibr"" target=""#b13"">13,</ref><ref type=""bibr"" target=""#b29"">29,</ref><ref type=""bibr"" target=""#b17"">17,</ref><ref type=""bibr"" tar ly used, such as L 2 <ref type=""bibr"" target=""#b2"">[2,</ref><ref type=""bibr"" target=""#b12"">12,</ref><ref type=""bibr"" target=""#b29"">29,</ref><ref type=""bibr"" target=""#b30"">30]</ref>, L 1 <ref type=""bib",0
"rget=""#b2"">[2,</ref><ref type=""bibr"" target=""#b12"">12,</ref><ref type=""bibr"" target=""#b14"">14,</ref><ref type=""bibr"" target=""#b36"">36,</ref><ref type=""bibr"" target=""#b39"">39,</ref><ref type=""bibr"" tar Specifically, we 1 (a) HR (b) FSRCNN (c) LapSRN <ref type=""bibr"" target=""#b14"">[14]</ref> (d) SRMD <ref type=""bibr"" target=""#b36"">[36]</ref> (e) EDSR <ref type=""bibr"" target=""#b36"">[36]</ref> (f) DBP ef type=""bibr"" target=""#b14"">[14]</ref> (d) SRMD <ref type=""bibr"" target=""#b36"">[36]</ref> (e) EDSR <ref type=""bibr"" target=""#b36"">[36]</ref> (f) DBPN <ref type=""bibr"" target=""#b20"">[20]</ref> (g) RDN racteristics. We carry out experiments with Bicubic (BI) and Blur-downscale (BD) degradation models <ref type=""bibr"" target=""#b36"">[36]</ref>. All the SR results are evaluated by PSNR and SSIM metrics Net <ref type=""bibr"" target=""#b30"">[30]</ref>, EDSR <ref type=""bibr"" target=""#b20"">[20]</ref>, SRMD <ref type=""bibr"" target=""#b36"">[36]</ref>, NLRN <ref type=""bibr"" target=""#b22"">[22]</ref>, DBPN <ref R <ref type=""bibr"" target=""#b12"">[12]</ref>, IR-CNN <ref type=""bibr"" target=""#b35"">[35]</ref>, SRMD <ref type=""bibr"" target=""#b36"">[36]</ref>, RDN <ref type=""bibr"" target=""#b39"">[39]</ref>, and RCAN < ww.tei-c.org/ns/1.0""><head n=""4.4."">Results with Blur-downscale Degradation (BD)</head><p>Following <ref type=""bibr"" target=""#b36"">[36,</ref><ref type=""bibr"" target=""#b39"">39]</ref>, we also compare v",0
"as the training progresses <ref type=""bibr"" target=""#b7"">[8]</ref>. Madry, Makelov, Schmidt, et al. <ref type=""bibr"" target=""#b24"">[25]</ref> used adversarial training on the cifar dataset, which stil gest known attack for this metric. PGD has been conjectured to be a near-optimal first-order attack <ref type=""bibr"" target=""#b24"">[25]</ref>. We use the Fo olBox library for the implementation of the we also wish to address ac o n c e r nr a i s e db yM a d r y ,M a k e l o v ,S c h m i d t ,et al. <ref type=""bibr"" target=""#b24"">[25]</ref>, which is the computational cost of a threat model. They a",1
"hat would occur in the real world would render adversarial attacks a non-issue for physical systems <ref type=""bibr"" target=""#b8"">[9]</ref>. However, it was later shown that these difficulties could b",0
"sification (e.g., <ref type=""bibr"" target=""#b11"">[12]</ref><ref type=""bibr"" target=""#b12"">[13]</ref><ref type=""bibr"" target=""#b13"">[14]</ref>). As far as we are aware, these types of defenses have all",0
"ther ensure the attacker's strength, we follow recommendations from Demontis, Melis, Pintor, et al. <ref type=""bibr"" target=""#b35"">[36]</ref>, and attempt to optimize for the adversary in the L ∞ ball",0
"odel is familiar with the transformations we apply at test time. Following Biggio, Fumera, and Roli <ref type=""bibr"" target=""#b33"">[34]</ref>, we will now fully state the threat model that we will ope",0
"the target user and item based on historical interactions <ref type=""bibr"" target=""#b38"">[38,</ref><ref type=""bibr"" target=""#b41"">41]</ref>. For example, given two paths p 1 u 1 → i 1 → u 2 → i 2 and terests; meanwhile, the user groups can also profile items <ref type=""bibr"" target=""#b38"">[38,</ref><ref type=""bibr"" target=""#b41"">41]</ref>. Hence, in each modality (e.g., visual), we aggregate signa e select LeakyReLU(•) as the nonlinear activation function <ref type=""bibr"" target=""#b38"">[38,</ref><ref type=""bibr"" target=""#b41"">41]</ref>. Such aggregation method assumes that different neighbors w ntegrate multi-modal features as the node features to learn the representation of each node. • NGCF <ref type=""bibr"" target=""#b41"">[41]</ref>. This method represent a novel recommendation framework to",1
"has not interacted before. We use the widely-used protocols <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b18"">19]</ref>: Precision@K, Recall@K, and NDCG@K to evaluate the performa",1
"can be summarized as the paths connecting the target user and item based on historical interactions <ref type=""bibr"" target=""#b38"">[38,</ref><ref type=""bibr"" target=""#b41"">41]</ref>. For example, give al behaviors of users reflect personal interests; meanwhile, the user groups can also profile items <ref type=""bibr"" target=""#b38"">[38,</ref><ref type=""bibr"" target=""#b41"">41]</ref>. Hence, in each mo e d ′ m is the transformation size; and we select LeakyReLU(•) as the nonlinear activation function <ref type=""bibr"" target=""#b38"">[38,</ref><ref type=""bibr"" target=""#b41"">41]</ref>. Such aggregation y based on CF models <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b38"">[38]</ref><ref type=""bibr"" target=""#b39"">[39]</ref><ref type=""bibr"" t",1
"0]</ref> to learn the acoustic deep learning features. For textual modality, we use Sentence2Vector <ref type=""bibr"" target=""#b0"">[1]</ref> to derive the textual features from microvideos' description",0
"br"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b22"">23]</ref> and natural language processing <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b32"">32]</ref>, neural networks are",0
"d items in multiple modalities. Inspired by the recent success of graph convolution networks (GCNs) <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b21"">22]</ref>, we use the inform ties. However, existing GNN efforts (e.g., GCN <ref type=""bibr"" target=""#b21"">[22]</ref>, GraphSage <ref type=""bibr"" target=""#b13"">[14]</ref>, GAT <ref type=""bibr"" target=""#b33"">[33]</ref>) only consi as one component of the micro-video, which is consistent with the idea of standard ACF. • GraphSAGE <ref type=""bibr"" target=""#b13"">[14]</ref>. Such model is based on the general inductive framework th 5"">26,</ref><ref type=""bibr"" target=""#b28"">29]</ref>. Towards video recommendation, Hamilton et al. <ref type=""bibr"" target=""#b13"">[14]</ref> proposed a general inductive framework which leverages the",0
"=""#b35"">[35]</ref><ref type=""bibr"" target=""#b36"">[36]</ref><ref type=""bibr"" target=""#b37"">[37]</ref><ref type=""bibr"" target=""#b43"">43]</ref>. Using neural networks, the function fusing the different m",0
"get=""#b5"">[6,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b38"">[38]</ref><ref type=""bibr"" target=""#b39"">[39]</ref><ref type=""bibr"" target=""#b40"">[40]</ref>. CF-based models",0
"information into a joint representation can be learned. Besides, the probabilistic graphical models <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b12"">13]</ref> are another way to c",0
"t information and the collaborative filtering effects for recommendation. For instance, Chen et al. <ref type=""bibr"" target=""#b6"">[7]</ref> constructed a uservideo-query tripartite graph and performed",0
"tent information to generate node representation for unseen data. Based on this method, Ying et al. <ref type=""bibr"" target=""#b42"">[42]</ref> developed and deployed a large-scale deep recommendation e",0
"by the recent success of graph convolution networks (GCNs) <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b21"">22]</ref>, we use the information-propagation mechanism to encode hig wn feature and the interaction among different modalities. However, existing GNN efforts (e.g., GCN <ref type=""bibr"" target=""#b21"">[22]</ref>, GraphSage <ref type=""bibr"" target=""#b13"">[14]</ref>, GAT icro-videos, which is widespread in recommendation systems <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" tar",0
"><p>The multi-modal representation is one of the most important problem in multi-modal applications <ref type=""bibr"" target=""#b26"">[27]</ref>. However, there are few prior works that focus on multi-mo",0
"ef>, neural networks are increasingly used in the multi-modal domain, on multimodal representations <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" tar",0
"tation is the concatenation of single-modal features. Recently, with its success in computer vision <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" targ",0
"/ref>, but it has recently become more common to use models based on the ""Transformer"" architecture <ref type=""bibr"" target=""#b108"">[Vaswani et al., 2017]</ref>. The Transformer was initially shown to ding a comprehensive definition of this model, we refer the interested reader to the original paper <ref type=""bibr"" target=""#b108"">[Vaswani et al., 2017]</ref> or follow-up tutorials 3,4 for a more d verall, our encoder-decoder Transformer implementation closely follows its originally-proposed form <ref type=""bibr"" target=""#b108"">[Vaswani et al., 2017]</ref>. First, an input sequence of tokens is to generate the answer token-by-token. For WMT English to German, we use the same training data as <ref type=""bibr"" target=""#b108"">[Vaswani et al., 2017]</ref> (i.e. News Commentary v13, Common Crawl =""3.1.1"">Model</head><p>For our model, we use a standard encoder-decoder Transformer as proposed by <ref type=""bibr"" target=""#b108"">Vaswani et al. [2017]</ref>. While many modern approaches to transfe",1
"ient-side inference or federated learning <ref type=""bibr"" target=""#b50"">[Kone?n? et al., 2015</ref><ref type=""bibr"" target=""#b51"">[Kone?n? et al., , 2016]]</ref>. Relatedly, one beneficial use of tra",0
"pe=""bibr"" target=""#b99"">[Socher et al., 2013]</ref>)</p><p>? Paraphrasing/sentence similarity (MRPC <ref type=""bibr"" target=""#b24"">[Dolan and Brockett, 2005]</ref>, STS-B <ref type=""bibr"" target=""#b14",0
"meters of a small classifier that was fed sentence embeddings produced by a fixed pre-trained model <ref type=""bibr"" target=""#b102"">[Subramanian et al., 2018;</ref><ref type=""bibr"" target=""#b48"">Kiros",0
"pplied where it will have the most impact. Some current work along these lines include distillation <ref type=""bibr"" target=""#b34"">[Hinton et al., 2015;</ref><ref type=""bibr"" target=""#b90"">Sanh et al.",0
"s, etc.</p><p>Inspired by the current trend of formalizing NLP problems as question answering tasks <ref type=""bibr"" target=""#b21"">(Levy et al., 2017;</ref><ref type=""bibr"" target=""#b28"">McCann et al. two years, there has been a trend of transforming NLP tasks to MRC question answering. For example, <ref type=""bibr"" target=""#b21"">Levy et al. (2017)</ref> transformed the task of relation extraction",1
"ng NLP problems as question answering tasks <ref type=""bibr"" target=""#b21"">(Levy et al., 2017;</ref><ref type=""bibr"" target=""#b28"">McCann et al., 2018;</ref><ref type=""bibr"" target=""#b22"">Li et al., 2 swer y can be extracted from a sentence, it means the relation label for the current sentence is R. <ref type=""bibr"" target=""#b28"">McCann et al. (2018)</ref> transformed NLP tasks such as summarizatio",1
"br"" target=""#b21"">(Levy et al., 2017;</ref><ref type=""bibr"" target=""#b28"">McCann et al., 2018;</ref><ref type=""bibr"" target=""#b22"">Li et al., 2019)</ref>, we propose a new framework that is capable of formalized as answering the question ""What is the summary?"". Our work is significantly inspired by <ref type=""bibr"" target=""#b22"">Li et al. (2019)</ref>, which formalized the task of entity-relation sk of entity-relation extraction as a multi-turn question answering task. Different from this work, <ref type=""bibr"" target=""#b22"">Li et al. (2019)</ref> focused on relation extraction rather than NER target=""#b22"">Li et al. (2019)</ref> focused on relation extraction rather than NER. Additionally, <ref type=""bibr"" target=""#b22"">Li et al. (2019)</ref> utilized a template-based procedure for constr nt influence on the final results. Different ways have been proposed for question generation, e.g., <ref type=""bibr"" target=""#b22"">Li et al. (2019)</ref> utilized a template-based procedure for constr",1
"ref><ref type=""bibr"">Wang et al., 2016;</ref><ref type=""bibr"" target=""#b41"">Shen et al., 2017;</ref><ref type=""bibr"" target=""#b2"">Chen et al., 2017)</ref> extract answer spans from a passage through a",0
"target=""#b15"">Ju et al. (2018)</ref> dynamically stacked flat NER layers in a hierarchical manner. <ref type=""bibr"" target=""#b23"">Lin et al. (2019a)</ref> proposed the Anchor-Region Networks (ARNs) a rget=""#tab_5"">2</ref>   <ref type=""bibr"" target=""#b47"">(Wang and Lu, 2018)</ref> 76.8 72.3 74.5 ARN <ref type=""bibr"" target=""#b23"">(Lin et al., 2019a)</ref> 76.2 73.6 74.9 Path-BERT <ref type=""bibr"" t",0
"ile sequence labeling models with nested NER <ref type=""bibr"" target=""#b0"">(Alex et al., 2007;</ref><ref type=""bibr"" target=""#b1"">Byrne, 2007;</ref><ref type=""bibr"" target=""#b10"">Finkel and Manning, 2",0
"f type=""bibr"" target=""#b45"">(Strubell et al., 2017)</ref> with other MRC based models such as QAnet <ref type=""bibr"" target=""#b54"">(Yu et al., 2018)</ref> and BiDAF <ref type=""bibr"">(Seo et al., 2017)",0
"><ref type=""bibr"">Wang et al., 2016;</ref><ref type=""bibr"" target=""#b48"">Wang and Jiang, 2016;</ref><ref type=""bibr"" target=""#b51"">Xiong et al., 2016</ref><ref type=""bibr"" target=""#b52"">Xiong et al.,",0
">The task of flat NER is commonly formalized as a sequence labeling task: a sequence labeling model <ref type=""bibr"" target=""#b4"">(Chiu and Nichols, 2016;</ref><ref type=""bibr"" target=""#b27"">Ma and Ho",0
"ref> provided inference model that extracts entities iteratively from outermost ones to inner ones. <ref type=""bibr"" target=""#b44"">Straková et al. (2019)</ref> viewed nested NER as a sequence-tosequen",0
"""bibr"" target=""#b1"">Byrne, 2007;</ref><ref type=""bibr"" target=""#b10"">Finkel and Manning, 2009;</ref><ref type=""bibr"" target=""#b25"">Lu and Roth, 2015;</ref><ref type=""bibr"" target=""#b16"">Katiyar and Ca trees. They made the assumption that one mention is fully contained by the other when they overlap. <ref type=""bibr"" target=""#b25"">Lu and Roth (2015)</ref> proposed to use mention hyper-graphs for rec",0
"target=""#b10"">Finkel and Manning, 2009;</ref><ref type=""bibr"" target=""#b25"">Lu and Roth, 2015;</ref><ref type=""bibr"" target=""#b16"">Katiyar and Cardie, 2018)</ref>, mostly based on the pipelined system Ohta et al., 2002)</ref> For the GENIA dataset, we use GENIAcorpus3.02p. We follow the protocols in <ref type=""bibr"" target=""#b16"">Katiyar and Cardie (2018)</ref>.</p><formula xml:id=""formula_7"">L = α , γ ∈ [0,</formula></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>KBP2017</head><p>We follow <ref type=""bibr"" target=""#b16"">Katiyar and Cardie (2018)</ref> and evaluate our model on the 2017 En",0
"fense strategies into consideration <ref type=""bibr"">(Athalye et al., 2018b)</ref>.</p><p>Recently, <ref type=""bibr"" target=""#b13"">Shafahi et al. (2019)</ref> showed that, for two classes of data dist for general classifiers, and their relationship to some recent works in literature.</p><p>Recently, <ref type=""bibr"" target=""#b13"">Shafahi et al. (2019)</ref> shows that no classifier can achieve low",1
". Various adversarial algorithms have since been developed to efficiently find adversarial examples <ref type=""bibr"" target=""#b1"">(Goodfellow et al., 2015;</ref><ref type=""bibr"" target=""#b2"">Moosavi-D ial example attacks: Adversarial training <ref type=""bibr"" target=""#b0"">(Szegedy et al., 2014;</ref><ref type=""bibr"" target=""#b1"">Goodfellow et al., 2015)</ref>; Defensive distillation <ref type=""bibr hreat? Theoretical analysis for understanding adversarial examples is needed to address this issue. <ref type=""bibr"" target=""#b1"">Goodfellow et al. (2015)</ref>; <ref type=""bibr"" target=""#b14"">Fawzi e",0
"1"">(Goodfellow et al., 2015;</ref><ref type=""bibr"" target=""#b2"">Moosavi-Dezfooli et al., 2016;</ref><ref type=""bibr"" target=""#b3"">Carlini and Wagner, 2017;</ref><ref type=""bibr"" target=""#b4"">Madry et ctor x is another data vector 2 (tanh 2x 3 + 1) and then displayed in grey scale as a 19 × 19 image <ref type=""bibr"" target=""#b3"">(Carlini and Wagner, 2017)</ref>. The two means µ + and µ − are chosen",0
"ion systems in physical world applications <ref type=""bibr"" target=""#b5"">(Sharif et al., 2016;</ref><ref type=""bibr"" target=""#b6"">Brown et al., 2017;</ref><ref type=""bibr"" target=""#b7"">Kurakin et al.,",0
"on-based models <ref type=""bibr"" target=""#b4"">[5]</ref>, <ref type=""bibr"" target=""#b12"">[12]</ref>, <ref type=""bibr"" target=""#b52"">[43]</ref> widely used in information retrieval. The interaction-base d by each person to 100.</p><p>The hyper-parameters of the RBF kernel functions are set the same as <ref type=""bibr"" target=""#b52"">[43]</ref>. We use 11 RBF kernels, with the hyper-parameters m = f1;",1
"13 602 challenge-1<ref type=""bibr"" target=""#b7"">[7]</ref>,<ref type=""bibr"" target=""#b19"">[19]</ref>,<ref type=""bibr"" target=""#b60"">[48]</ref>. We train a GBDT model to esti-603 mate a matching probabi l the participations devoted to featureengineering methods <ref type=""bibr"" target=""#b7"">[7]</ref>, <ref type=""bibr"" target=""#b60"">[48]</ref>.</p><p>Entity Linking. Entity linking aims at linking the",0
"andidate person c, which can be pre-trained by Word2-Vec <ref type=""bibr"" target=""#b18"">[18]</ref>, <ref type=""bibr"" target=""#b23"">[23]</ref> or BERT <ref type=""bibr"" target=""#b6"">[6]</ref>.</p><p>Agg",0
"target=""#b23"">(Peters et al., 2018;</ref><ref type=""bibr"" target=""#b24"">Radford et al., 2018;</ref><ref type=""bibr"" target=""#b5"">Devlin et al., 2019)</ref>, which has improved performances on various ngth of n by the same WordPiece tokenizer <ref type=""bibr"" target=""#b35"">(Wu et al., 2016)</ref> in <ref type=""bibr"" target=""#b5"">Devlin et al. (2019)</ref>. Next, as shown in Fig. <ref type=""figure""> s generated by the cross-modality encoder. For the cross-modality output, following the practice in <ref type=""bibr"" target=""#b5"">Devlin et al. (2019)</ref>, we append a special token [CLS] (denoted a and each of them only focuses on a single modality (i.e., language or vision). Different from BERT <ref type=""bibr"" target=""#b5"">(Devlin et al., 2019)</ref>, which applies the transformer encoder onl om branch of Fig. <ref type=""figure"" target=""#fig_0"">2</ref>, the task setup is almost same to BERT <ref type=""bibr"" target=""#b5"">(Devlin et al., 2019)</ref>: words are randomly masked with a probabil n image and a sentence match each other. This task is similar to 'Next Sentence Prediction' in BERT <ref type=""bibr"" target=""#b5"">(Devlin et al., 2019)</ref>.</p><p>Image Question Answering (QA) In or by the WordPiece tokenizer <ref type=""bibr"" target=""#b35"">(Wu et al., 2016)</ref> provided in BERT <ref type=""bibr"" target=""#b5"">(Devlin et al., 2019)</ref> image to maximize the pre-training compute s. We take Adam (Kingma and Ba, 2014) as the optimizer with a linear-decayed learning-rate schedule <ref type=""bibr"" target=""#b5"">(Devlin et al., 2019)</ref> and a peak learning rate at 1e − 4. We tra .</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""5.1"">BERT versus LXMERT</head><p>BERT <ref type=""bibr"" target=""#b5"">(Devlin et al., 2019</ref>) is a pre-trained language encoder which im",1
"Lin et al., 2014;</ref><ref type=""bibr"" target=""#b16"">Krishna et al., 2017)</ref>. Pioneering works <ref type=""bibr"" target=""#b8"">(Girshick et al., 2014;</ref><ref type=""bibr"" target=""#b36"">Xu et al.,",0
"l question-answering datasets, VQA <ref type=""bibr"" target=""#b1"">(Antol et al., 2015)</ref> and GQA <ref type=""bibr"" target=""#b13"">(Hudson and Manning, 2019)</ref>. Our model outperforms previous work",0
"cross-modality representations of the two images and then build the classifier with GeLU activation <ref type=""bibr"" target=""#b11"">(Hendrycks and Gimpel, 2016)</ref>. Suppose that LXMERT(img, sent) is",0
"et=""#b25"">[26,</ref><ref type=""bibr"" target=""#b50"">51,</ref><ref type=""bibr"" target=""#b43"">44,</ref><ref type=""bibr"" target=""#b55"">56]</ref>. Modern classifiers are over-complete in terms of parameter ends beyond the manifold is less constrained, contributing to the existence of adversarial examples <ref type=""bibr"" target=""#b55"">[56,</ref><ref type=""bibr"" target=""#b58"">59]</ref>. For examples, it r its out of manifold part, adversarial perturbations lead to a tilting effect on the data manifold <ref type=""bibr"" target=""#b55"">[56]</ref>; at places where the classification boundary is far from t",1
"ent label prediction <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b54"">55,</ref><ref type=""bibr"" target=""#b23"">24]</ref>. Adversarial examples have been shown to be ubiquitous beyo "" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b70"">71]</ref>. Among them, adversarial training <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b35"">36]</ref> is one of the most side in a large, contiguous region and a significant portion of the adversarial subspaces is shared <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" ta e sense of robustness against adversarial attacks due to gradient masking, and adversarial training <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b31"">32,</ref><ref type=""bibr"" ta se method against adversarial attacks. It improves model robustness by solving a minimax problem as <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b35"">36]</ref>:</p><formula xml:i he proposed formulation deviates from the conventional minimax formulation for adversarial training <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b35"">36]</ref>. More specifically ≡ i L θ (x i , y i ), the proposed approach reduces to the conventional adversarial training setup <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b35"">36]</ref>. The overall proce gn method (FGSM) for adversarial attack generation is developed and used in adversarial training in <ref type=""bibr"" target=""#b23"">[24]</ref>. Many variants of attacks have been developed later <ref t inner maximization can be solved approximately, using for example a one-step approach such as FGSM <ref type=""bibr"" target=""#b23"">[24]</ref>, or a multi-step projected gradient descent (PGD) method < y measuring the accuracy of the model under different adversarial attacks, including white-box FGSM <ref type=""bibr"" target=""#b23"">[24]</ref>, PGD <ref type=""bibr"" target=""#b35"">[36]</ref>, CW <ref ty",1
"attack generation <ref type=""bibr"" target=""#b31"">[32,</ref><ref type=""bibr"" target=""#b64"">65,</ref><ref type=""bibr"" target=""#b60"">61]</ref>. Increasing the attack iterations will increase the trainin aset that is widely use in adversarial training literature <ref type=""bibr"" target=""#b35"">[36,</ref><ref type=""bibr"" target=""#b60"">61]</ref> with 10 classes, 5K training images per class and 10K test ethod performs adversarial training with both image and label adversarial perturbations (Bilateral) <ref type=""bibr"" target=""#b60"">[61]</ref>. For training, the initial learning rate γ is 0.1 for CIFA of the trained model stabilized before 100 epochs. The training scheduling of 200 epochs similar to <ref type=""bibr"" target=""#b60"">[61]</ref> with the same transition epochs used as we empirically obs ef type=""figure"" target=""#fig_3"">3</ref>    <ref type=""bibr"" target=""#b35"">[36]</ref> and Bilateral <ref type=""bibr"" target=""#b60"">[61]</ref> methods on CIFAR10 under different threat models.</p><p>fu target=""#tab_1"">1</ref>, it is also observed that the Proposed approach also outperforms Bilateral <ref type=""bibr"" target=""#b60"">[61]</ref> under all variants of PGD and CW attacks. We will use a PG antly, which is about 20% better than Madry <ref type=""bibr"" target=""#b35"">[36]</ref> and Bilateral <ref type=""bibr"" target=""#b60"">[61]</ref> under PGD attack and about 10% better under CW attack. The",0
"arget=""#b7"">8,</ref><ref type=""bibr"" target=""#b53"">54,</ref><ref type=""bibr"" target=""#b61"">62,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b5"">6]</ref>. In the mean time, many",0
"o be effective in capturing the inter-sample relationships <ref type=""bibr"" target=""#b50"">[51,</ref><ref type=""bibr"" target=""#b21"">22]</ref>. Natural images live on a low-dimensional manifold, with th elationship for learning dates back to the seminal work of <ref type=""bibr"" target=""#b50"">[51,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b47"">48]</ref>. This type of local",0
"rget=""#b23"">[24,</ref><ref type=""bibr"" target=""#b35"">36]</ref> is one of the most popular technique <ref type=""bibr"" target=""#b1"">[2]</ref>, which conducts model training using the adversarially pertu f><ref type=""bibr"" target=""#b45"">46,</ref><ref type=""bibr"" target=""#b34"">35]</ref>.</p><p>Recently, <ref type=""bibr"" target=""#b1"">[2]</ref> showed that many existing defence methods suffer from a fals e by-passed with a substitute model for generating attacks, thus giving a false sense of robustness <ref type=""bibr"" target=""#b1"">[2]</ref>.</p><formula xml:id=""formula_2"">C T {x i } f ✓ (x i ) {x 0 j /ref>  <ref type=""bibr"" target=""#b35"">[36]</ref>, which is one of the most effective defense method <ref type=""bibr"" target=""#b1"">[2]</ref>, iii) another recent method performs adversarial training wi the -cube around x, and then goes through several PGD steps with a step size of α as shown in Eqn. <ref type=""bibr"" target=""#b1"">(2)</ref>.</p><p>Label leaking <ref type=""bibr"" target=""#b31"">[32]</re d gradient masking <ref type=""bibr"" target=""#b42"">[43,</ref><ref type=""bibr"" target=""#b57"">58,</ref><ref type=""bibr"" target=""#b1"">2]</ref> are some well-known issues that hinder the adversarial traini . Gradient masking <ref type=""bibr"" target=""#b42"">[43,</ref><ref type=""bibr"" target=""#b57"">58,</ref><ref type=""bibr"" target=""#b1"">2]</ref> refers to the effect that the adversarially trained model lea",0
"/ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" target=""#b9"">10]</ref>, auto-encoding <ref type=""bibr"" target=""#b56"">[57]</ref> and dictionary learning <ref type=""bibr"" target=""#b46"">[47",0
"neighborhood structure have been proven to be effective in capturing the inter-sample relationships <ref type=""bibr"" target=""#b50"">[51,</ref><ref type=""bibr"" target=""#b21"">22]</ref>. Natural images li ry. The idea of leveraging inter-sample relationship for learning dates back to the seminal work of <ref type=""bibr"" target=""#b50"">[51,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" ta d, with the training and testing images as samples from it <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b50"">51,</ref><ref type=""bibr"" target=""#b43"">44,</ref><ref type=""bibr"" tar",0
"itous beyond classification, ranging from object detection <ref type=""bibr"" target=""#b63"">[64,</ref><ref type=""bibr"" target=""#b17"">18]</ref> to speech recognition <ref type=""bibr"" target=""#b10"">[11,</",0
"get=""#b35"">36,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" target=""#b66"">67,</ref><ref type=""bibr"" target=""#b71"">72,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" tar",0
"nject high-level variations with latent variables. Variational Hierarchical Conversation RNN (VHCR) <ref type=""bibr"" target=""#b30"">(Park et al., 2018)</ref> is a most similar model to ours, which also",1
"f type=""bibr"" target=""#b11"">(Goodfellow et al., 2014)</ref>. To model dependencies among sentences, <ref type=""bibr"" target=""#b21"">Li et al. (2015)</ref> utilized a hierarchical recurrent neural netwo",0
"bibr"" target=""#b18"">(Kukich, 1983;</ref><ref type=""bibr"" target=""#b5"">Dalianis and Hovy, 1993;</ref><ref type=""bibr"" target=""#b13"">Hovy, 1993)</ref> or automatically-learnt rules <ref type=""bibr"" targ",0
"en specifications about a product. Different from our task, the advertising text generation task in <ref type=""bibr"" target=""#b3"">(Chen et al., 2019)</ref> is to generate personalized product descript",0
"e for the 'happy' category while the others not. In this paper, inspired by the attention mechanism <ref type=""bibr"" target=""#b13"">[14]</ref> of machine translation and the neural aggregation networks",1
"atures and this global representation f v .</p><p>Inspired by the relation-Net in low-shot learning <ref type=""bibr"" target=""#b15"">[16]</ref>, we use the sample concatenation and another FC layer to e",0
"toolbox. By default, for feature embedding, we use the ResNet18 which is pre-trained on MS-Celeb-1M <ref type=""bibr"" target=""#b20"">[21]</ref> face recognition dataset and FER Plus expression dataset <",0
"tic image emotion recognition which can be both hand-crafted <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref> and learned <ref type=""bibr"" target=""#b2"">[3,</ref><ref type= ropose to use a bank of 2D Gabor filters to extract facial features for videobased FER. Shan et al. <ref type=""bibr"" target=""#b1"">[2]</ref> use local binary patterns (LBP) and LBP histogram for facial",0
">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref> and learned <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b4"">5]</ref>. For the hand-crafted f cial features from deep CNNs trained on large face datasets or trained with multi-level supervision <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b4"">5]</ref>.</p><p>Spatial-tempora ts to form a fixed-length video representation by frame averaging or frame expansion. Bargal et al. <ref type=""bibr"" target=""#b3"">[4]</ref> propose a statistical encoding module (STAT) to aggregate fr",0
"utions.</formula><p>as a means for, e.g., model debugging or architecture selection. A recent paper <ref type=""bibr"" target=""#b3"">(Jain and Wallace, 2019)</ref> points to possible pitfalls that may ca ntion Might be Explanation</head><p>In this section, we briefly describe the experimental design of <ref type=""bibr"" target=""#b3"">Jain and Wallace (2019)</ref> and look at the results they provide to",1
"and their suitability for providing explanations for model predictions is a topic of high interest <ref type=""bibr"" target=""#b17"">(Xu et al., 2015;</ref><ref type=""bibr"" target=""#b11"">Rocktäschel et",0
"tion), as plausible. Works such as <ref type=""bibr"" target=""#b8"">Mullenbach et al. (2018)</ref> and <ref type=""bibr"" target=""#b1"">Ehsan et al. (2019)</ref> use human evaluation to evaluate explanation",0
"t=""#b11"">Rocktäschel et al., 2015;</ref><ref type=""bibr"" target=""#b8"">Mullenbach et al., 2018;</ref><ref type=""bibr"" target=""#b16"">Thorne et al., 2019;</ref><ref type=""bibr"" target=""#b14"">Serrano and",0
"#b11"">[12]</ref>, which also proposed a combinatorial algorithm to compute embeddings. Ganea et al. <ref type=""bibr"" target=""#b16"">[17]</ref> and Gulcehre et al. <ref type=""bibr"" target=""#b20"">[21]</r taking its outputs. An alternative to prevent such collapse would be to introduce bias terms as in <ref type=""bibr"" target=""#b16"">[17]</ref>. Importantly, when applying the non-linearity directly on )y 1 + 2 x, y + x 2 y 2<label>(8)</label></formula><p>Similar to the Euclidean case, and following <ref type=""bibr"" target=""#b16"">[17]</ref>, we use x = x 0 . On the Poincaré ball, we employ pointwis",1
"orks <ref type=""bibr"" target=""#b0"">[1]</ref>. Furthermore, <ref type=""bibr"" target=""#b26"">[27,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b27"">28]</ref> exploited the propert",0
"=""#b25"">[26,</ref><ref type=""bibr"" target=""#b33"">34,</ref><ref type=""bibr"" target=""#b44"">45]</ref>. <ref type=""bibr"" target=""#b21"">[22]</ref> showed that graph neural networks can be scaled to large-s",0
"earn inductive models of graphs, GNNs have found promising applications in molecular fingerprinting <ref type=""bibr"" target=""#b13"">[14]</ref> and quantum chemistry <ref type=""bibr"" target=""#b17"">[18]< ms. Applications include molecular design <ref type=""bibr"" target=""#b30"">[31]</ref>, fingerprinting <ref type=""bibr"" target=""#b13"">[14]</ref> and poly pharmaceutical side-effect modeling <ref type=""bi",0
"path routing in large-scale communication networks.</p><p>Concurrently with this work, Chami et al. <ref type=""bibr"" target=""#b9"">[10]</ref> also proposed an extension of graph neural networks to hype",0
"domain is challenging and expensive. We release SCIBERT, a pretrained language model based on BERT <ref type=""bibr"" target=""#b5"">(Devlin et al., 2019)</ref> to address the lack of highquality, large- eters et al., 2018)</ref>, GPT <ref type=""bibr"" target=""#b25"">(Radford et al., 2018)</ref> and BERT <ref type=""bibr"" target=""#b5"">(Devlin et al., 2019)</ref>, unsupervised pretraining of language mode s=""http://www.tei-c.org/ns/1.0""><head n=""2"">Methods</head><p>Background The BERT model architecture <ref type=""bibr"" target=""#b5"">(Devlin et al., 2019)</ref> is based on a multilayer bidirectional Tra ead n=""3.3"">Pretrained BERT Variants</head><p>BERT-Base We use the pretrained weights for BERT-Base <ref type=""bibr"" target=""#b5"">(Devlin et al., 2019)</ref> released with the original BERT code. <ref 9"">(Howard and Ruder, 2018)</ref> which is equivalent to the linear warmup followed by linear decay <ref type=""bibr"" target=""#b5"">(Devlin et al., 2019)</ref>. For each dataset and BERT variant, we pic h using AllenNLP <ref type=""bibr"" target=""#b8"">(Gardner et al., 2017)</ref>.</p><p>Casing We follow <ref type=""bibr"" target=""#b5"">Devlin et al. (2019)</ref> in using the cased models for NER and the u T</head><p>We mostly follow the same architecture, optimization, and hyperparameter choices used in <ref type=""bibr"" target=""#b5"">Devlin et al. (2019)</ref>. For text classification (i.e. CLS and REL)",1
"ans describing the Participants, Interventions, Comparisons, and Outcomes in a clinical trial paper <ref type=""bibr"" target=""#b15"">(Kim et al., 2011)</ref>. REL is a special case of text classificatio",0
"omain texts.</p><p>Corpus We train SCIBERT on a random sample of 1.14M papers from Semantic Scholar <ref type=""bibr"" target=""#b1"">(Ammar et al., 2018)</ref>. This corpus consists of 18% papers from th d on the full text of 1.14M biomedical and computer science papers from the Semantic Scholar corpus <ref type=""bibr"" target=""#b1"">(Ammar et al., 2018)</ref>. Furthermore, SCIBERT uses an in-domain voc",0
"ral angle is a commonly used distance metric to measure the difference between two spectral vectors <ref type=""bibr"" target=""#b22"">(Kruse et al., 1993)</ref>. The reflectance of individual pixel is re",1
"2.1."">Multiscale segmentation used</head><p>A region based multiscale image segmentation named MSEG <ref type=""bibr"" target=""#b36"">(Tzotsos and Argialas, 2006</ref>) is used to generate the initial se D have a size of × 500 500 pixels. The multiscale segmentation used in this study is MSEG algorithm <ref type=""bibr"" target=""#b36"">(Tzotsos and Argialas, 2006)</ref>, which is implemented under C++ en ed on multi-resolution segmentation (MSEG) <ref type=""bibr"" target=""#b0"">(Benz et al. (2004)</ref>; <ref type=""bibr"" target=""#b36"">Tzotsos and Argialas, 2006)</ref>, the proposed method is implemented",1
"entation <ref type=""bibr"" target=""#b37"">(Wang et al., 2018)</ref> and multi-resolution segmentation <ref type=""bibr"" target=""#b0"">(Benz et al., 2004)</ref> have been widely used to generate image obje nitial segmentation results for HR images. MSEG is developed from the multi-resolution segmentation <ref type=""bibr"" target=""#b0"">(Benz et al., 2004)</ref>. It initially generates an oversegmentation ough the segmentation algorithm used in this study is based on multi-resolution segmentation (MSEG) <ref type=""bibr"" target=""#b0"">(Benz et al. (2004)</ref>; <ref type=""bibr"" target=""#b36"">Tzotsos and",1
"ts and tend to design a measure to evaluate the quality of segmentation results at different scales <ref type=""bibr"" target=""#b10"">(Espindola et al., 2006;</ref><ref type=""bibr"" target=""#b21"">Karl and",0
"f GEOBIA and usually represent a kind of land covers such as farm lands, buildings and water bodies <ref type=""bibr"" target=""#b42"">(Yang et al., 2015a)</ref>. Therefore, the segmentation performance h",0
"ritical effect on the later steps of GEOBIA, including object feature extraction and classification <ref type=""bibr"" target=""#b13"">(Gonçalves et al., 2019)</ref>.</p><p>There are a number of HR image selection methods have drawn more attention <ref type=""bibr"" target=""#b2"">(Böck et al., 2017;</ref><ref type=""bibr"" target=""#b13"">Gonçalves et al., 2019)</ref>. Many studies consider that the spectra",0
"ibr"" target=""#b34"">(Tian and Chen, 2007;</ref><ref type=""bibr"" target=""#b25"">Liu et al., 2012;</ref><ref type=""bibr"" target=""#b3"">Cardoso and Corte-Real, 2005;</ref><ref type=""bibr"">Witharana et al.,",0
"variety of scale optimization methods <ref type=""bibr"" target=""#b11"">(Georganos et al., 2018a;</ref><ref type=""bibr"" target=""#b19"">Hu et al., 2018)</ref>. Theses scale optimization methods usually can",0
"tained in different land cover categories and then synthesized as a multiscale segmentation results <ref type=""bibr"" target=""#b45"">(Yi et al., 2012)</ref>. The drawback of this method is that the land scales with different land cover, which requires land covers classification before scale selection <ref type=""bibr"" target=""#b45"">(Yi et al., 2012)</ref>. Another solution tries to optimize the segme",0
"ity are employed as measurements to search the suitable objects until some conditions are satisfied <ref type=""bibr"" target=""#b8"">(Drǎguţ et al., 2010)</ref>. In <ref type=""bibr"" target=""#b43"">Yang et",0
"gmentation results at different scales <ref type=""bibr"" target=""#b10"">(Espindola et al., 2006;</ref><ref type=""bibr"" target=""#b21"">Karl and Maurer, 2010;</ref><ref type=""bibr"" target=""#b31"">Stein and",0
"fine the segmentation result. Other studies try to optimize segmentation at several distinct scales <ref type=""bibr"" target=""#b44"">(Yang et al., 2014)</ref>. For instance, in <ref type=""bibr"" target=""",0
"target=""#b31"">Stein and de Beurs, 2005;</ref><ref type=""bibr"" target=""#b49"">Zhou et al., 2017;</ref><ref type=""bibr"" target=""#b12"">Georganos et al., 2018b;</ref><ref type=""bibr"" target=""#b17"">Hu et al",0
"scale parameter, the cross-scale strategy has emerged and drawn attention in the scale optimization <ref type=""bibr"" target=""#b20"">(Johnson and Xie, 2011;</ref><ref type=""bibr"" target=""#b48"">Zhang et performance indexes such as global score, Moran index, have been applied in segmentation evaluation <ref type=""bibr"" target=""#b20"">(Johnson and Xie, 2011;</ref><ref type=""bibr"" target=""#b47"">Zhang et in current scale. results <ref type=""bibr"" target=""#b41"">(Xiao et al., 2018)</ref>. For example, in <ref type=""bibr"" target=""#b20"">Johnson and Xie (2011)</ref>, a global optimal scale is first selecte segmentation with more than one scale <ref type=""bibr"" target=""#b41"">(Xiao et al., 2018)</ref>. In <ref type=""bibr"" target=""#b20"">Johnson and Xie (2011)</ref> and <ref type=""bibr"" target=""#b32"">Su (2 r comparison</head><p>In this study, an unsupervised multiscale segmentation optimization algorithm <ref type=""bibr"" target=""#b20"">(Johnson and Xie, 2011)</ref> is used in comparison with the proposed tation evaluation</head><p>To evaluate the optimization performance, the Johnson &amp; Xie's method <ref type=""bibr"" target=""#b20"">(Johnson and Xie, 2011)</ref> and two global optimization approaches",0
"<ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b75"">76]</ref> and the heat kernel <ref type=""bibr"" target=""#b33"">[34]</ref>, with which GDC achieves a linear runtime O(N ). Furthermo e=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" target=""#b36"">37]</ref>, especially for clustering <ref type=""bibr"" target=""#b33"">[34]</ref>, semi-supervised classification <ref type=""bibr"" target=""#",1
"Most importantly, there are fast approximations for both PPR <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b75"">76]</ref> and the heat kernel <ref type=""bibr"" target=""#b33"">[34]</re",1
"usion are personalized PageRank (PPR) <ref type=""bibr"" target=""#b55"">[56]</ref> and the heat kernel <ref type=""bibr"" target=""#b35"">[36]</ref>. PPR corresponds to choosing T = T rw and θ PPR k = α(1 − cal graph learning <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" target=""#b36"">37]</ref>, especially for clu",1
"consistent results for graph classification (e.g. 2.5 percentage points with GCN on the DD dataset <ref type=""bibr"" target=""#b17"">[18]</ref>). We found no improvement for the inductive  <ref type=""fi",0
"or top-k.</p><p>Datasets and models. We evaluate GDC on six datasets: The citation graphs CITESEER <ref type=""bibr"" target=""#b64"">[65]</ref>, CORA <ref type=""bibr"" target=""#b46"">[47]</ref>, and PUBME",0
"PR corresponds to choosing T = T rw and θ PPR k = α(1 − α) k , with teleport probability α ∈ (0, 1) <ref type=""bibr"" target=""#b13"">[14]</ref>. The heat kernel uses T = T rw and θ HK k = e −t t k k! , b13"">[14]</ref>. The heat kernel uses T = T rw and θ HK k = e −t t k k! , with the diffusion time t <ref type=""bibr"" target=""#b13"">[14]</ref>. Another special case of generalized graph diffusion is th have been extensively studied in classical graph learning <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" tar",0
"target=""#b5"">[6]</ref>, and the name GNN first appeared in <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b63"">64]</ref>. However, they only became widely adopted in recent years,",0
"nce it is localized and has a limited number of parameters <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b26"">27]</ref>:</p><formula xml:id=""formula_3"">g ξ (L) = J j=0 ξ j L j = U",0
"ctral-based models <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b31"">32,</ref><ref type=""bibr"" tar",0
"es not preserve the eigenvectors and its effect therefore cannot be calculated precisely. Wu et al. <ref type=""bibr"" target=""#b76"">[77]</ref> empirically found that adding self-loops shrinks the graph",0
"e GDC on six datasets: The citation graphs CITESEER <ref type=""bibr"" target=""#b64"">[65]</ref>, CORA <ref type=""bibr"" target=""#b46"">[47]</ref>, and PUBMED <ref type=""bibr"" target=""#b51"">[52]</ref>, the",0
"tasks, as suggested by Klicpera et al. <ref type=""bibr"" target=""#b32"">[33]</ref> and Shchur et al. <ref type=""bibr"" target=""#b65"">[66]</ref>. We report performance on a test set that was used exactly "">[47]</ref>, and PUBMED <ref type=""bibr"" target=""#b51"">[52]</ref>, the co-author graph COAUTHOR CS <ref type=""bibr"" target=""#b65"">[66]</ref>, and the co-purchase graphs AMAZON COMPUTERS and AMAZON PH d the co-purchase graphs AMAZON COMPUTERS and AMAZON PHOTO <ref type=""bibr"" target=""#b45"">[46,</ref><ref type=""bibr"" target=""#b65"">66]</ref>. We only use their largest connected components. We show ho",0
"/ref> and Baskin et al. <ref type=""bibr"" target=""#b5"">[6]</ref>, and the name GNN first appeared in <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b63"">64]</ref>. However, they onl",0
"target=""#b61"">[62]</ref>. Other unsupervised models learn Gaussian distributions instead of vectors <ref type=""bibr"" target=""#b9"">[10]</ref>, use an auto-encoder <ref type=""bibr"" target=""#b30"">[31]</r ically fall within a narrow range of α ∈ [0.05, 0.2] and t ∈ <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b9"">10]</ref>. We also tried obtaining θ k from models that learn analogou",0
"dhuber, 1997)</ref> system can readily outperform the previous state-of-the-art system, CogCompTime <ref type=""bibr"" target=""#b32"">(Ning et al., 2018d)</ref>, by a large margin. The fact that a standa ead><label>2</label><figDesc>Performances on the MATRES test set (i.e., the PT section). CogCompTime<ref type=""bibr"" target=""#b32"">(Ning et al., 2018d)</ref> is the previous state-of-the-art feature-b hat in Table <ref type=""table"" target=""#tab_2"">2</ref>, CogCompTime performed slightly different to <ref type=""bibr"" target=""#b32"">Ning et al. (2018d)</ref>: Cog-CompTime reportedly had F 1 =65.9 (Tab f type=""table"" target=""#tab_2"">2</ref> Line 3 therein) and here we obtained F 1 =66.6. In addition, <ref type=""bibr"" target=""#b32"">Ning et al. (2018d)</ref> only reported F 1 scores, while we also use",1
"nal attempts to TempRel extraction include <ref type=""bibr"" target=""#b23"">Mani et al. (2006)</ref>; <ref type=""bibr"" target=""#b10"">Chambers et al. (2007)</ref>; <ref type=""bibr"" target=""#b1"">Bethard e",0
"sk, or showed only moderate improvements <ref type=""bibr"" target=""#b14"">(Dligach et al., 2017;</ref><ref type=""bibr"" target=""#b21"">Lin et al., 2017;</ref><ref type=""bibr"" target=""#b24"">Meng and Rumshi h attempts, e.g., in clinical narratives <ref type=""bibr"" target=""#b14"">(Dligach et al., 2017;</ref><ref type=""bibr"" target=""#b21"">Lin et al., 2017;</ref><ref type=""bibr"" target=""#b38"">Tourille et al. enberg and Moens, 2018)</ref>. However, their improvements over feature-based methods were moderate <ref type=""bibr"" target=""#b21"">(Lin et al. (2017)</ref> even showed negative results). Given the low",0
"r"" target=""#b0"">Bethard et al., 2015</ref><ref type=""bibr"" target=""#b2"">Bethard et al., , 2016</ref><ref type=""bibr"" target=""#b3"">Bethard et al., , 2017))</ref>, and significant progresses were made i",0
"few data but achieve good performance. A typical example of this approach is prototypical networks <ref type=""bibr"" target=""#b13"">(Snell et al., 2017)</ref>, which averages the vector of few support http://www.tei-c.org/ns/1.0""><head n=""4.3"">Prototypical Networks</head><p>The prototypical networks <ref type=""bibr"" target=""#b13"">(Snell et al., 2017)</ref> has achieved excellent performance in few- cia and Bruna, 2018)</ref>, SNAIL <ref type=""bibr"" target=""#b10"">(Mishra et al., 2018)</ref>, Proto <ref type=""bibr"" target=""#b13"">(Snell et al., 2017)</ref> and PHATT <ref type=""bibr"" target=""#b3"">(G its label, and obviate the need for fine-tuning to adapt to new class types. Prototypical networks <ref type=""bibr"" target=""#b13"">(Snell et al., 2017</ref>) learns a metric space in which the model c (y = l i q) = exp(−d(g θ (q), c i ) Σ L l=1 exp(−d(g θ (q), c l )<label>(9)</label></formula><p>As <ref type=""bibr"" target=""#b13"">Snell et al. (2017)</ref> mentioned, squared Euclidean distance is a he implementation details are as follows.</p><p>For FewRel dataset, we cite the results reported by <ref type=""bibr"" target=""#b13"">Snell et al. (2017)</ref> which includes Finetune, kNN, MetaN, GNN, a",1
"obtained from meta-training tasks for a newly seen few-shot task such as intention classification, <ref type=""bibr"" target=""#b5"">Han et al. (2018)</ref> present a relation classification dataset -Few one layer convolutional neural networks (CNN). For ease of comparison, its details are the same as <ref type=""bibr"" target=""#b5"">Han et al. (2018)</ref> proposed. Hierarchical Attention In order to g with the CNN encoder. For the neural networks based baselines, we use the same hyper parameters as <ref type=""bibr"" target=""#b5"">Han et al. (2018)</ref> proposed.</p><p>For our hierarchical attention or 5 way 5 shot and 10 way 5 shot settings on FewRel test set.</figDesc><table /><note>* reported by<ref type=""bibr"" target=""#b5"">Han et al. (2018)</ref> and ◇ reported by<ref type=""bibr"" target=""#b3"" as achieved excellent performance in few-shot image classification and few-shot text classification <ref type=""bibr"" target=""#b5"">(Han et al., 2018;</ref><ref type=""bibr"" target=""#b3"">Gao et al., 2019 ttp://www.tei-c.org/ns/1.0""><head n=""5.1"">Datasets</head><p>FewRel Few-Shot Relation Classification <ref type=""bibr"" target=""#b5"">(Han et al., 2018)</ref>  </p></div> <div xmlns=""http://www.tei-c.org/",1
"ification dataset -FewRel, and adapt most recent state-of-the-art few-shot learning methods for it, <ref type=""bibr"" target=""#b3"">Gao et al. (2019)</ref> propose a hybrid attention-based prototypical and useless at the same time.</p><p>So we apply a CNN-based feature attention mechanism similar to <ref type=""bibr"" target=""#b3"">Gao et al. (2019)</ref> proposed as a class feature extractor. It depe 17)</ref> which includes Finetune, kNN, MetaN, GNN, and SNAIL, then we cite the results reported by <ref type=""bibr"" target=""#b3"">Gao et al. (2019)</ref> which includes Proto and PHATT. For a fair com ><table /><note>* reported by<ref type=""bibr"" target=""#b5"">Han et al. (2018)</ref> and ◇ reported by<ref type=""bibr"" target=""#b3"">Gao et al. (2019)</ref>.</note></figure> 			<note xmlns=""http://www.te assification and few-shot text classification <ref type=""bibr"" target=""#b5"">(Han et al., 2018;</ref><ref type=""bibr"" target=""#b3"">Gao et al., 2019)</ref> tasks respectively, so our model is based on p shra et al., 2018)</ref>, Proto <ref type=""bibr"" target=""#b13"">(Snell et al., 2017)</ref> and PHATT <ref type=""bibr"" target=""#b3"">(Gao et al., 2019)</ref> respectively.</p></div> <div xmlns=""http://ww",1
"type=""bibr"" target=""#b7"">(Kim, 2014;</ref><ref type=""bibr"" target=""#b23"">Zhang et al., 2015a;</ref><ref type=""bibr"" target=""#b20"">Yang et al., 2016;</ref><ref type=""bibr"" target=""#b18"">Wang et al., 2 out which words are useful and which words are useless. Therefore, we apply an attention mechanism <ref type=""bibr"" target=""#b20"">(Yang et al., 2016)</ref> to get those important words and assemble t",1
"/ns/1.0""><head n=""1"">Introduction</head><p>The dominant text classification models in deep learning <ref type=""bibr"" target=""#b7"">(Kim, 2014;</ref><ref type=""bibr"" target=""#b23"">Zhang et al., 2015a;</ VMs <ref type=""bibr"" target=""#b16"">(Tang et al., 2015)</ref>. The neural network based methods like <ref type=""bibr"" target=""#b7"">Kim (2014)</ref>  </p></div> <div xmlns=""http://www.tei-c.org/ns/1.0"">",0
"uch as bagof-words or n-grams <ref type=""bibr"" target=""#b19"">(Wang and Manning, 2012)</ref> or SVMs <ref type=""bibr"" target=""#b16"">(Tang et al., 2015)</ref>. The neural network based methods like <ref",0
"class, and it can apply to unseen classes. The early works aim to use transfer learning approaches, <ref type=""bibr"" target=""#b1"">Caruana (1994)</ref> and <ref type=""bibr"" target=""#b0"">Bengio (2011)</",0
"g/ns/1.0""><head n=""4.2.2"">Encoding Layer</head><p>Following we apply a convolutional neural network <ref type=""bibr"" target=""#b22"">Zeng et al. (2014)</ref> as encoding layer to get the hidden annotati",0
"ype=""bibr"" target=""#b0"">Bengio (2011)</ref> adopt the target task from the pre-trained models. Then <ref type=""bibr"" target=""#b8"">Koch et al. (2015)</ref> explore a method for learning siamese neural",0
"late the instance level vector s j through the weighted sum of α j t and h j t . As memory networks <ref type=""bibr"" target=""#b14"">(Sukhbaatar et al., 2015)</ref> proposed, u w can help us to select t",0
"dominant text classification models in deep learning <ref type=""bibr"" target=""#b7"">(Kim, 2014;</ref><ref type=""bibr"" target=""#b23"">Zhang et al., 2015a;</ref><ref type=""bibr"" target=""#b20"">Yang et al.,",0
"d embeddings and hyperparameters of instance encoder as PHATT proposed. In detail, we use the Glove <ref type=""bibr"" target=""#b12"">(Pennington et al., 2014)</ref> consisting of 6B tokens and 400K voca",0
", which learns to compare query against few-shot labeled sample support data. Dual TriNet structure <ref type=""bibr"" target=""#b2"">(Chen et al., 2018)</ref> can efficiently and directly augment multi-l",0
"r model with five state-of-the-art fewshot learning models based on neural networks, they are MetaN <ref type=""bibr"" target=""#b11"">(Munkhdalai and Yu, 2017)</ref>, GNN <ref type=""bibr"" target=""#b4"">(G",0
"lve it. The traditional methods mainly focus on feature engineerings such as bagof-words or n-grams <ref type=""bibr"" target=""#b19"">(Wang and Manning, 2012)</ref> or SVMs <ref type=""bibr"" target=""#b16""",0
"ibr"" target=""#b23"">Zhang et al., 2015a;</ref><ref type=""bibr"" target=""#b20"">Yang et al., 2016;</ref><ref type=""bibr"" target=""#b18"">Wang et al., 2018)</ref> require a considerable amount of labeled dat",0
"dataset, we implement all above seven baseline models and our models. we use the Baidu Encyclopedia <ref type=""bibr"" target=""#b9"">(Li et al., 2018)</ref> as our initialized word representation, it inc",0
"nd prototype representations of each class and classify the query to the nearest prototype's class. <ref type=""bibr"" target=""#b15"">Sung et al. (2018)</ref> propose a two-branch relation networks, whic",0
"ule</head><p>This module is a bi-direction recurrent neural network with self-attention as shown in <ref type=""bibr"" target=""#b10"">Lin et al. (2017)</ref>. Given an input text x = (w 1 , w 2 , ..., w",1
"e target problem <ref type=""bibr"" target=""#b14"">(Qi et al., 2018)</ref>. The approaches proposed by <ref type=""bibr"" target=""#b18"">Snell et al. (2017)</ref> and <ref type=""bibr"" target=""#b20"">Sung et ication</head><p>Few-shot classification <ref type=""bibr"" target=""#b21"">(Vinyals et al., 2016;</ref><ref type=""bibr"" target=""#b18"">Snell et al., 2017)</ref> is a task in which a classifier must be ada ng Networks <ref type=""bibr"" target=""#b21"">(Vinyals et al., 2016)</ref> 65.73 Prototypical Networks <ref type=""bibr"" target=""#b18"">(Snell et al., 2017)</ref> 68.17 Graph Network <ref type=""bibr"" targe </p><p>• Prototypical Networks: a deep metric-based method using sample average as class prototypes <ref type=""bibr"" target=""#b18"">(Snell et al., 2017)</ref>.</p><p>• Graph Network: a graph-based few- the performance by few-shot classification accuracy following previous studies in few-shot learning <ref type=""bibr"" target=""#b18"">(Snell et al., 2017;</ref><ref type=""bibr"" target=""#b20"">Sung et al.,",1
"s for few-shot learning still confront many important problems, including the imposed strong priors <ref type=""bibr"" target=""#b3"">(Fei-Fei et al., 2006)</ref>, complex gradient transfer between tasks hot learning dates back to the early 2000s <ref type=""bibr"" target=""#b2"">(Fe-Fei et al., 2003;</ref><ref type=""bibr"" target=""#b3"">Fei-Fei et al., 2006)</ref>. The authors combined generative models wi",0
"one or very few examples challenges the standard fine-tuning method in deep learning. Early studies <ref type=""bibr"" target=""#b17"">(Salamon and Bello, 2017)</ref> applied data augmentation and regular",0
"1} .</p><p>All parameters of the three modules are trained jointly by backpropagation. The Adagrad <ref type=""bibr"" target=""#b1"">(Duchi et al., 2011)</ref> is used on all parameters in each training",0
"http://www.tei-c.org/ns/1.0""><head n=""3.1"">Few-Shot Classification</head><p>Few-shot classification <ref type=""bibr"" target=""#b21"">(Vinyals et al., 2016;</ref><ref type=""bibr"" target=""#b18"">Snell et a a-learning phase and mimic the few-shot learning setting via episode-based training, as proposed in <ref type=""bibr"" target=""#b21"">Vinyals et al. (2016)</ref>. We construct an meta-episode to compute for each label in the test set. We create 5-shot learning models on this dataset. Matching Networks <ref type=""bibr"" target=""#b21"">(Vinyals et al., 2016)</ref> 65.73 Prototypical Networks <ref type=""b follows.</p><p>• Matching Networks: a few-shot learning model using a metric-based attention method <ref type=""bibr"" target=""#b21"">(Vinyals et al., 2016)</ref>.</p><p>• Prototypical Networks: a deep m",0
"tion problem with large labeled datasets. Unlike their work, we study few-shot text classification. <ref type=""bibr"" target=""#b23"">Xia et al. (2018)</ref> reused the supervised model similar to that o",0
"7"">Yu et al. (2018)</ref>, we use the multiple tasks with the multi-domain sentiment classification <ref type=""bibr"" target=""#b0"">(Blitzer et al., 2007)</ref> dataset. The dataset comprises English re",0
"018)</ref>. The approaches proposed by <ref type=""bibr"" target=""#b18"">Snell et al. (2017)</ref> and <ref type=""bibr"" target=""#b20"">Sung et al. (2018)</ref>, which combine non-parametric methods and me thout suffering from overfitting. In general, these approaches can be divided into two categories.  <ref type=""bibr"" target=""#b20"">(Sung et al., 2018)</ref>.</p></div> <div xmlns=""http://www.tei-c.org 7 Graph Network <ref type=""bibr"" target=""#b5"">(Garcia and Bruna, 2017)</ref> 82.61 Relation Network <ref type=""bibr"" target=""#b20"">(Sung et al., 2018)</ref> 83.07 SNAIL <ref type=""bibr"" target=""#b11""> eural network as the distance metric and sums up sample vectors in the support set as class vectors <ref type=""bibr"" target=""#b20"">(Sung et al., 2018)</ref>.</p><p>• SNAIL: a class of simple and gener wing previous studies in few-shot learning <ref type=""bibr"" target=""#b18"">(Snell et al., 2017;</ref><ref type=""bibr"" target=""#b20"">Sung et al., 2018)</ref>. To evaluate the proposed model with the bas",0
"nvariant semantic relationships between lower level sample features and higher level class features <ref type=""bibr"" target=""#b7"">(Hinton et al., 2011)</ref>. To ensure the class vector encapsulates t",0
">1</ref> depicts an example of 3-shot link prediction in KGs.</p><p>To do few-shot link prediction, <ref type=""bibr"" target=""#b22"">Xiong et al. (2018)</ref> made the first trial and proposed GMatching ed parameters, it's like ""a gradient through a gradient"".</p><p>As far as we know, work proposed by <ref type=""bibr"" target=""#b22"">Xiong et al. (2018)</ref> is the first research on few-shot learning and Evaluation Metrics</head><p>We use two datasets, NELL-One and Wiki-One which are constructed by <ref type=""bibr"" target=""#b22"">Xiong et al. (2018)</ref>. NELL-One and Wiki-One are derived from NEL simple TransE embedding model, denoted as -g -r. The result under the third setting is copied from <ref type=""bibr"" target=""#b22"">Xiong et al. (2018)</ref>. It uses the triples from background graph, e transferring relation meta to incomplete triples during prediction.</p><p>Compared with GMatching <ref type=""bibr"" target=""#b22"">(Xiong et al., 2018</ref>) which relies on a background knowledge gra e heavily rely on rich training instances <ref type=""bibr"" target=""#b26"">(Zhang et al., 2019b;</ref><ref type=""bibr"" target=""#b22"">Xiong et al., 2018)</ref>, thus are limited to do few-shot link predi r"" target=""#b17"">Vinyals et al., 2016;</ref><ref type=""bibr"" target=""#b15"">Snell et al., 2017;</ref><ref type=""bibr"" target=""#b22"">Xiong et al., 2018)</ref>, which tries to learn a matching metric bet f> is a typical method using symmetric twin networks to compute the metric of two inputs. GMatching <ref type=""bibr"" target=""#b22"">(Xiong et al., 2018)</ref>, the first trial on one-shot link predicti own in Table <ref type=""table"" target=""#tab_6"">4</ref>. The baseline in our experiment is GMatching <ref type=""bibr"" target=""#b22"">(Xiong et al., 2018)</ref>, which made the first trial on few-shot li",1
"local graph structures which also can be regarded as a metric-based method. (2) Model-based method <ref type=""bibr"" target=""#b14"">(Santoro et al., 2016;</ref><ref type=""bibr"" target=""#b12"">Munkhdalai",0
"thods so far: (1) Metric-based meta-learning <ref type=""bibr"" target=""#b7"">(Koch et al., 2015;</ref><ref type=""bibr"" target=""#b17"">Vinyals et al., 2016;</ref><ref type=""bibr"" target=""#b15"">Snell et al",0
"rediction, predicting new triples based on existing ones. For link prediction, KG embedding methods <ref type=""bibr"" target=""#b1"">(Bordes et al., 2013;</ref><ref type=""bibr"" target=""#b13"">Nickel et al mbedding of r as in normal knowledge graph embedding methods. One line of work is started by TransE <ref type=""bibr"" target=""#b1"">(Bordes et al., 2013)</ref> with distance score function. TransH <ref p>where x represents the L2 norm of vector x.</p><p>We design the score function inspired by TransE <ref type=""bibr"" target=""#b1"">(Bordes et al., 2013)</ref> which assumes the head entity embedding h,",0
"get=""#b14"">(Santoro et al., 2016;</ref><ref type=""bibr"" target=""#b12"">Munkhdalai and Yu, 2017;</ref><ref type=""bibr"" target=""#b11"">Mishra et al., 2018)</ref>, which uses a specially designed part like",0
"et=""#b51"">[52,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b48"">49,</ref><ref type=""bibr"" target=""#b16"">17]</ref>. A popular learning paradigm is graphbased / hypergraph-bas ral network f (G, X) <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b16"">17]</ref> (X contains the initial features on the vertices for exampl learning problem on the approximation. While the state-of-the-art hypergraph neural networks (HGNN) <ref type=""bibr"" target=""#b16"">[17]</ref> approximates each hyperedge by a clique and hence requires detailed experimentation, we demonstrate their effectiveness compared to the state-of-the art HGNN <ref type=""bibr"" target=""#b16"">[17]</ref> and other baselines (Sections 5, and 7). • We thoroughly d =""bibr"" target=""#b49"">50,</ref><ref type=""bibr"" target=""#b42"">43]</ref>. Hypergraph neural networks <ref type=""bibr"" target=""#b16"">[17]</ref> and their variants <ref type=""bibr"" target=""#b22"">[23,</re 2 . Our approach requires at most a linear number of edges (1 and 2|e| − 3 respectively) while HGNN <ref type=""bibr"" target=""#b16"">[17]</ref> requires a quadratic number of edges for each hyperedge. < yperGCN and FastHyperGCN against the following baselines:</p><p>• Hypergraph neural networks (HGNN) <ref type=""bibr"" target=""#b16"">[17]</ref> uses the clique expansion <ref type=""bibr"" target=""#b51"">[ [7].Our approach requires at most a linear number of edges (1 and 2|e| − 3 respectively) while HGNN<ref type=""bibr"" target=""#b16"">[17]</ref> requires a quadratic number of edges for each hyperedge.</",1
">43]</ref>. Hypergraph neural networks <ref type=""bibr"" target=""#b16"">[17]</ref> and their variants <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b23"">24]</ref> use the clique exp",1
"line of work uses mathematically appealing tensor methods <ref type=""bibr"" target=""#b39"">[40,</ref><ref type=""bibr"" target=""#b5"">6]</ref>, but they are limited to uniform hypergraphs.</p></div> <div",0
"aturaly motivates the problem of learning with hypergraphs <ref type=""bibr"" target=""#b51"">[52,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b48"">49,</ref><ref type=""bibr"" tar which are a standard practice in hypergraph-based learning <ref type=""bibr"" target=""#b51"">[52,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b48"">49,</ref><ref type=""bibr"" tar esearchers have fully utilised the hypergraph structure also through non-linear Laplacian operators <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b31"">32,</ref><ref type=""bibr"" ta extension of the hypergraph cut) which considers the maximally disparate vertices in each hyperedge <ref type=""bibr"" target=""#b21"">[22]</ref>. Recent developements have extended these non-linear opera model can be seen as performing implicit regularisation based on the total variation on hypergraphs <ref type=""bibr"" target=""#b21"">[22]</ref>. In that prior work, explicit regularisation and only the this method has consistently been shown to be superior to the primal dual hybrid gradient (PDHG) of <ref type=""bibr"" target=""#b21"">[22]</ref> and also <ref type=""bibr"" target=""#b51"">[52]</ref>. Hence,",0
"le yet effective way to overcome the limitations is to introduce hyperedge-dependent vertex weights <ref type=""bibr"" target=""#b13"">[14]</ref>.</p><p>Researchers have fully utilised the hypergraph stru",0
"ve used explicit Laplacian regularisation in the objective <ref type=""bibr"" target=""#b50"">[51,</ref><ref type=""bibr"" target=""#b52"">53,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" tar",0
"to a comprehensive literature review <ref type=""bibr"" target=""#b4"">[5]</ref> and extensive surveys <ref type=""bibr"" target=""#b19"">[20,</ref><ref type=""bibr"" target=""#b3"">4]</ref> on this topic of dee",0
"N's effectiveness in SSL and combinatorial optimisation. Approaches that assign importance to nodes <ref type=""bibr"" target=""#b45"">[46,</ref><ref type=""bibr"" target=""#b34"">35,</ref><ref type=""bibr"" ta",0
"orial optimisation <ref type=""bibr"" target=""#b18"">[19,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b37"">38,</ref><ref type=""bibr"" target=""#b30"">31]</ref>, computer vision <r h models as learning-based approaches for NP-hard problems <ref type=""bibr"" target=""#b30"">[31,</ref><ref type=""bibr"" target=""#b37"">38,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" tar . <ref type=""bibr"" target=""#b30"">[31]</ref>, the decision version of the traveling salesman problem <ref type=""bibr"" target=""#b37"">[38]</ref>, graph colouring <ref type=""bibr"" target=""#b25"">[26]</ref>",0
"et=""#b38"">[39,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b49"">50,</ref><ref type=""bibr"" target=""#b42"">43]</ref>. Hypergraph neural networks <ref type=""bibr"" target=""#b16"">",0
"train. Our TDNN-F was trained using the lattice-free maximum mutual information objective criterion <ref type=""bibr"" target=""#b21"">[22]</ref>. No parameter tuning was performed during neural network t",1
"model with fewer parameters. The recently-introduced factorised time-delay neural networks (TDNN-F) <ref type=""bibr"" target=""#b12"">[13]</ref> utilise half the number of parameters than the hybrid netw to the parameter matrices of TDNN layers, ASR performance can be improved in lowresource situations <ref type=""bibr"" target=""#b12"">[13]</ref>. Consequently, a TDNN-F acoustic model (10 time-delay laye",1
"raining. However, in this case three-fold data augmentation was applied prior to feature extraction <ref type=""bibr"" target=""#b20"">[21]</ref> and the acoustic features comprised 40-dimensional MFCCs (",1
"ompiled from national radio news bulletins and consists of a mix of prepared and spontaneous speech <ref type=""bibr"" target=""#b16"">[17]</ref>. The total transcribed multilingual speech data available",0
"ecome an accepted platform for sharing opinions and concerns <ref type=""bibr"" target=""#b0"">[1]</ref><ref type=""bibr"" target=""#b1"">[2]</ref><ref type=""bibr"" target=""#b2"">[3]</ref>. Surveys conducted by",0
"radio phone-in shows <ref type=""bibr"" target=""#b3"">[4]</ref><ref type=""bibr"" target=""#b4"">[5]</ref><ref type=""bibr"" target=""#b5"">[6]</ref>. Therefore, to support its humanitarian relief efforts in ru",0
"opinions and concerns <ref type=""bibr"" target=""#b0"">[1]</ref><ref type=""bibr"" target=""#b1"">[2]</ref><ref type=""bibr"" target=""#b2"">[3]</ref>. Surveys conducted by the United Nations (UN) in places lack",0
"rnet infrastructure, social media has become an accepted platform for sharing opinions and concerns <ref type=""bibr"" target=""#b0"">[1]</ref><ref type=""bibr"" target=""#b1"">[2]</ref><ref type=""bibr"" targe",0
"r"" target=""#b8"">[9]</ref>. Furthermore, two corpora taken from the Leipzig Corpora Collection (LCC) <ref type=""bibr"" target=""#b17"">[18]</ref> were included in our language model (LM) data collection.",0
"t unrelated languages <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b10"">11]</ref>, a system using a hybrid neural network acoustic model was",0
"een applied successfully in some other low-resource settings <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b13"">[14]</ref><ref type=""bibr"" target=""#b14"">[15]</ref><ref type=""bibr"" t M) neural network can lower language model perplexity when incorporated as additional training data <ref type=""bibr"" target=""#b13"">[14]</ref>. Hence we trained an LSTM network on the Somali acoustic t been shown that semi-supervised training can improve ASR performance in an under-resourced scenario<ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b14"">15]</ref>. As we only have l",0
""">Acoustic modelling</head><p>The Kaldi speech recognition toolkit was used for all ASR experiments <ref type=""bibr"" target=""#b19"">[20]</ref>. All experiments were performed using a PC with an 8-core",0
"able resources from better-resourced but unrelated languages <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b10"">11]</ref>, a system using a hy",0
"et=""#b7"">[8,</ref><ref type=""bibr"" target=""#b13"">[14]</ref><ref type=""bibr"" target=""#b14"">[15]</ref><ref type=""bibr"" target=""#b15"">[16]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head",0
"oped parts of Uganda, the UN has piloted radio browsing systems in three of the country's languages <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b7"">8]</ref>.</p><p>The success in",0
"atic comparison of BART with BERT <ref type=""bibr"" target=""#b2"">(Devlin et al., 2019)</ref> and GPT <ref type=""bibr"" target=""#b21"">(Radford et al., 2018)</ref>.</p><p>English, by propagation through B ooks and Wikipedia data. We compare the following approaches:</p><p>Language Model Similarly to GPT <ref type=""bibr"" target=""#b21"">(Radford et al., 2018)</ref>, we train a left-to-right Transformer la ><head n=""7"">Related Work</head><p>Early methods for pretraining were based on language models. GPT <ref type=""bibr"" target=""#b21"">(Radford et al., 2018)</ref> only models leftward context, which is p",1
"target=""#b17"">(Mikolov et al., 2013;</ref><ref type=""bibr"" target=""#b20"">Peters et al., 2018;</ref><ref type=""bibr"" target=""#b2"">Devlin et al., 2019;</ref><ref type=""bibr"" target=""#b11"">Joshi et al., e of the decoder.</p><p>Figure <ref type=""figure"">1</ref>: A schematic comparison of BART with BERT <ref type=""bibr"" target=""#b2"">(Devlin et al., 2019)</ref> and GPT <ref type=""bibr"" target=""#b21"">(Ra are shown in Figure <ref type=""figure"" target=""#fig_0"">2</ref>.</p><p>Token Masking Following BERT <ref type=""bibr"" target=""#b2"">(Devlin et al., 2019)</ref>, random tokens are sampled and replaced wi onal embeddings or attention across segments from XLNet.</p><p>Masked Language Model Following BERT <ref type=""bibr"" target=""#b2"">(Devlin et al., 2019)</ref>, we replace 15% of tokens with [MASK] symb kipedia paragraphs. Answers are text spans extracted from a given document context. Similar to BERT <ref type=""bibr"" target=""#b2"">(Devlin et al., 2019)</ref>, we use concatenated question and context g during pre-training.</p><p>Bidirectional encoders are crucial for SQuAD As noted in previous work <ref type=""bibr"" target=""#b2"">(Devlin et al., 2019)</ref>, just left-to-right decoder performs poorl > demonstrated that very large language models can act as unsupervised multitask models.</p><p>BERT <ref type=""bibr"" target=""#b2"">(Devlin et al., 2019)</ref> introduced masked language modelling, whic",1
"ef type=""bibr"">Agirre et al., 2007;</ref><ref type=""bibr"" target=""#b32"">Williams et al., 2018;</ref><ref type=""bibr"" target=""#b1"">Dagan et al., 2006;</ref><ref type=""bibr"" target=""#b14"">Levesque et al",0
"ibr"" target=""#b15"">(Yang et al., 2019)</ref>, and the available context for replacing masked tokens <ref type=""bibr"" target=""#b5"">(Dong et al., 2019)</ref>. However, these methods typically focus on p del to independently predict the original tokens.</p><p>Multitask Masked Language Model As in UniLM <ref type=""bibr"" target=""#b5"">(Dong et al., 2019)</ref>, we train a Masked Language Model with addit re not made auto-regressively, reducing the effectiveness of BERT for generation tasks.</p><p>UniLM <ref type=""bibr"" target=""#b5"">(Dong et al., 2019)</ref> fine-tunes BERT with an ensemble of masks, s",0
"del from the input to the output text. During finetuning we use a label smoothed cross entropy loss <ref type=""bibr"" target=""#b19"">(Pereyra et al., 2017)</ref>, with the smoothing parameter set to 0.1",0
"ype=""bibr"" target=""#b4"">Dolan &amp; Brockett, 2005;</ref><ref type=""bibr"">Agirre et al., 2007;</ref><ref type=""bibr"" target=""#b32"">Williams et al., 2018;</ref><ref type=""bibr"" target=""#b1"">Dagan et al",0
"o BERT, the representation of the EOS token is used to classify the sentences relations.</p><p>ELI5 <ref type=""bibr"" target=""#b8"">(Fan et al., 2019)</ref>, a long-form abstractive question answering d",0
"/expressing emotions and emotionally interacting with the interlocutors. In literature, Zhou et al. <ref type=""bibr"" target=""#b45"">[46]</ref> successfully build an emotional chat machine (ECM) that is enerate plausible emotional sentence without sacrificing grammatical fluency and semantic coherence <ref type=""bibr"" target=""#b45"">[46]</ref>. Hence, the response generation problem faces a significan tional factors, which are most related to our proposed conversation generation problem. Zhou et al. <ref type=""bibr"" target=""#b45"">[46]</ref> develop an Emotional Chat Machine (ECM) model using three "" target=""#b32"">[33]</ref>, to evaluate our experimental results. In particular, we follow the work <ref type=""bibr"" target=""#b45"">[46]</ref> to train an emotion classifier for assigning emotional lab ifferent datasets, i.e., NLPCC 2013 2 and NLPCC 2014 3 emotion classification datasets by following <ref type=""bibr"" target=""#b45"">[46]</ref>, which contain 29, 417 manually annotated data in total, a o any emotion information, and rare emotion categories like fear are removed. In particular, unlike <ref type=""bibr"" target=""#b45"">[46]</ref> using solely one label for classification, we consider bot rget=""#b35"">[36]</ref>, the traditional Seq2seq model is adopted as one of our baselines.</p><p>ECM <ref type=""bibr"" target=""#b45"">[46]</ref>, as mentioned, ECM model is improper to directly be as the onal matrix (if used). The parameters of imemory and ememory in ECM are the same as the settings in <ref type=""bibr"" target=""#b45"">[46]</ref>. We use stochastic gradient descent (SGD) with mini-batch <note xmlns=""http://www.tei-c.org/ns/1.0"" place=""foot"" n=""1"" xml:id=""foot_0"">Here we follow the work<ref type=""bibr"" target=""#b45"">[46]</ref>, where the emotion categories are {Angry, Disgust, Happy, o the detected post's emotion over EIPs.</p><p>Seq2seq-emb <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b45"">46]</ref>, Seq2seq with emotion embedding (Seq2seqemb) is also adopte",1
"responses according to a pre-defined emotion category, and several similar efforts are also made by <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b24"">25]</ref>, such as <ref type ng the most frequent response's emotion to the detected post's emotion over EIPs.</p><p>Seq2seq-emb <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b45"">46]</ref>, Seq2seq with emot tional variational auto-encoder framework to generate responses conditioned on the given emojis. In <ref type=""bibr"" target=""#b10"">[11]</ref>, Huang et al. propose three different models that are capa",0
"ent specific word embedding (SSWE) <ref type=""bibr"" target=""#b36"">[37]</ref> and word2vec embedding <ref type=""bibr"" target=""#b21"">[22]</ref> in our model. More concretely, SSWE encodes sentiment info",0
"nerate more diverse and informative responses, such as Maximum Mutual Information (MMI) based model <ref type=""bibr"" target=""#b13"">[14]</ref> and enhanced beam-search based model <ref type=""bibr"" targ otion perspective. As a result, in this paper we adopt distinct-1 and distinct-2 by follow the work <ref type=""bibr"" target=""#b13"">[14]</ref> to be as the metrics for evaluating the diversity of the g rent models.</p><p>Automatic Evaluation. As mentioned, we consider to use distinct-1 and distinct-2 <ref type=""bibr"" target=""#b13"">[14]</ref> to be as our automatic evaluation metric. Distinct-n is de",0
"th the ability of emotional communication with humans are essential for enhancing user satisfaction <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" targ",0
"structure in graphs <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" target=""#b34"">35,</ref><ref type=""bibr"" tar graph-based ML tasks <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" target=""#b45"">46]</ref>. DeepGL <ref type="" -1.</formula><p>Our work differs from previous approaches <ref type=""bibr"" target=""#b27"">[28,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" target=""#b33"">34,</ref><ref type=""bibr"" tar /ref>] in several key points. Specifically, in contrast to <ref type=""bibr"" target=""#b27"">[28,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" target=""#b33"">34,</ref><ref type=""bibr"" tar neral class of graph convolution networks which utilize weighted multi-hop motif adjacency matrices <ref type=""bibr"" target=""#b32"">[33]</ref> to capture higher-order neighborhoods in graphs. The weigh neighborhoods in graphs. The weighted adjacency matrices are computed using various network motifs <ref type=""bibr"" target=""#b32"">[33]</ref>. Fig. <ref type=""figure"" target=""#fig_3"">1</ref> shows an ions of relational operators applied to a base graph function such as triangle counts. Rossi et al. <ref type=""bibr"" target=""#b32"">[33]</ref> proposed the notion of higher-order network embeddings and ased matrix formulation as a function ? : R N ?N ? R N ?N over a motif adjacency A t ? A similar to <ref type=""bibr"" target=""#b32"">[33]</ref>. Given a function ?, we can obtain motif-based matrices ?t While the K-step motif-based adjacencies defined here share some similarity to that of Rossi et al. <ref type=""bibr"" target=""#b32"">[33]</ref> we would like to point out that there is an important dist",1
"as a way for models to attend to important parts of the data <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b25"">26]</ref>. The technique has been successfully adopted by models solv successfully adopted by models solving a variety of tasks. For instance, it was used by Mnih et al. <ref type=""bibr"" target=""#b25"">[26]</ref> to take glimpses of relevant parts of an input image for i ctions affect the outcome, we continue this process until we reach the first layer. Please refer to <ref type=""bibr"" target=""#b25"">[26]</ref> for a more detailed explanation of the REINFORCE rule for",0
"o allow each node to focus on features in its neighborhood that were more relevant while Lee et al. <ref type=""bibr"" target=""#b21"">[22]</ref> used attention to guide a walk in the graph to learn an em ing training. Finally, to reduce model variance we can also include an advantage term (see Eq. 2 in <ref type=""bibr"" target=""#b21"">[22]</ref>, for instance). Our final loss can then be written as:</p>",0
". More specialized methods of graph attention models include <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b13"">14]</ref> with Choi et al. <ref type=""bibr"" target=""#b6"">[7]</ref> us et=""#b6"">[7]</ref> using attention on a medical ontology graph for medical diagnosis and Han et al. <ref type=""bibr"" target=""#b13"">[14]</ref> using attention on a knowledge graph for the task of entit",0
"target=""#b28"">29,</ref><ref type=""bibr"" target=""#b44"">45]</ref> and has spawned variants including <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b41"">42]</ref>. We introduce a gene ious work, we take 1,000 nodes per dataset for testing and utilize an additional 500 for validation <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" targ (k) is equivalent to using a k-layer GCN or GAT model, extensive experiments by Abu-El-Haija et al. <ref type=""bibr"" target=""#b0"">[1]</ref> have shown that GCNs don't necessarily benefit from a wider",0
"rget=""#b5"">[6,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" target=""#b38"">39]</ref> work under the mech s, some baseline methods on GNN, for example, GCN <ref type=""bibr"" target=""#b11"">[12]</ref> and GAT <ref type=""bibr"" target=""#b32"">[33]</ref>, are demonstrated to be capable of extracting features of t can be used to generate node embeddings, e.g., GCN <ref type=""bibr"" target=""#b11"">[12]</ref>, GAT <ref type=""bibr"" target=""#b32"">[33]</ref> and gated graph networks <ref type=""bibr"" target=""#b17"">[1 get=""#b19"">[20]</ref>.</p><p>As suggested in previous work <ref type=""bibr"" target=""#b31"">[32,</ref><ref type=""bibr"" target=""#b32"">33]</ref>, the multi-head attention can help to stabilize the trainin",1
"ition pattern is more complicated. • The recent approaches <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b37"">38]</ref>, which divide the u the fairness and the convenience of comparison, we follow <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b37"">38]</ref> to filter out sessi , s,i −1 ] with the last item s,i −1 as l abel . Following <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b37"">38]</ref>, for the Yoochoose tention on the last item after encoding with a RNN. To alleviate the influence of time order, STAMP <ref type=""bibr"" target=""#b18"">[19]</ref> only utilizes the self-attention mechanism without RNN. SR to sum up as the session embedding. To further alleviate the bias introduced by time series, STAMP <ref type=""bibr"" target=""#b18"">[19]</ref> entirely replaces the recurrent encoder with an attention h enables the model to explicitly emphasize on the more important parts of the input.</p><p>• STAMP <ref type=""bibr"" target=""#b18"">[19]</ref> uses attention layers to replace all RNN encoders in previ ith different lengths because the length varies greatly within one dataset. Following previous work <ref type=""bibr"" target=""#b18"">[19,</ref><ref type=""bibr"" target=""#b37"">38]</ref>, sessions in Yooch",1
"of RNN <ref type=""bibr"" target=""#b9"">[10]</ref> applied on the sequence data, for instance, GRU4REC <ref type=""bibr"" target=""#b8"">[9]</ref> and NARM <ref type=""bibr"" target=""#b15"">[16]</ref> mainly mo target=""#b35"">36]</ref>, which is naturally designed for processing sequential data. Hidasi et al. <ref type=""bibr"" target=""#b8"">[9]</ref> proposed the GRU4REC, which applies a multi layer GRU <ref t arget=""#b15"">[16]</ref>, we omit the user feature directly because of the unavailability. • GRU4REC <ref type=""bibr"" target=""#b8"">[9]</ref> stacks multiple GRU layers to encode the session sequence in",0
"corporation of different deep learning tools, for instance, zeroshot learning and domain adaptation <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b14"">15]</ref>. These methods all",0
""" target=""#b25"">26]</ref>. In recent years, many GNN methods <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" tar Figure <ref type=""figure"">1</ref>. In recent years, some baseline methods on GNN, for example, GCN <ref type=""bibr"" target=""#b11"">[12]</ref> and GAT <ref type=""bibr"" target=""#b32"">[33]</ref>, are dem above, there are many different GNN layers that can be used to generate node embeddings, e.g., GCN <ref type=""bibr"" target=""#b11"">[12]</ref>, GAT <ref type=""bibr"" target=""#b32"">[33]</ref> and gated g",0
"their learned feature and uses a normal neural network layer to process the sorted nodes. DiffPool <ref type=""bibr"" target=""#b39"">[40]</ref> designs two sets of GNN for every layer to learn a new den",0
"in the deep learning society. Initially, GNN is applied to the simple situation on directed graphs <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b25"">26]</ref>. In recent years, ma",0
", the content-based RS <ref type=""bibr"" target=""#b20"">[21]</ref> and the collaborative filtering RS <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b26"">27]</ref> are two widely used prevalence of deep learning, neural networks are widely used. Neural collaborative filtering (NCF) <ref type=""bibr"" target=""#b6"">[7]</ref> first proposes to use the multi layer perceptron to approxim",0
"Regularization is also introduced to avoid the rare high similarities for unvisited items. • BPR-MF <ref type=""bibr"" target=""#b21"">[22]</ref> proposes a BPR objective function which calculates a pairw",0
"simple.</p><p>• S-POP always recommends the most popular items for the current session. • Item-KNN <ref type=""bibr"" target=""#b23"">[24]</ref> computes the similarity of items by the cosine distance of",0
"ttp://www.tei-c.org/ns/1.0""><head n=""1"">Introduction</head><p>The Transformer neural sequence model <ref type=""bibr"" target=""#b4"">[Vaswani et al., 2017]</ref> has emerged as a popular alternative to r rg/ns/1.0""><head n=""2.2"">Multi-head Attention</head><p>The ""Transformer"" seuqence-to-sequence model <ref type=""bibr"" target=""#b4"">[Vaswani et al., 2017]</ref> uses h different attention layers (heads) , hmv-&gt;hv "" , weig hts , V) y = t f . einsum ( "" hv , hdv-&gt;d "" , o , P_o) r e t u r n y Note: <ref type=""bibr"" target=""#b4"">[Vaswani et al., 2017]</ref> include a constant scaling factor on the values. In addition, we process a batch of b different non-interacting sequences at once. Following <ref type=""bibr"" target=""#b4"">[Vaswani et al., 2017]</ref>, in an autoregressive model, we can preve ing assumptions:</p><formula xml:id=""formula_0"">? m = n ? k = v = d</formula><p>h , as suggested by <ref type=""bibr"" target=""#b4"">[Vaswani et al., 2017]</ref> ? n ? d</p><p>The total number of arithme allel. An example is a self-attention layer in an autoregressive language model such as Transformer <ref type=""bibr"" target=""#b4"">[Vaswani et al., 2017]</ref>. The queries produced at each position at </head><p>We introduce multi-query Attention as a variation of multi-head attention as described in <ref type=""bibr"" target=""#b4"">[Vaswani et al., 2017]</ref>. Multi-head attention consists of multipl </div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.1"">Experimental Setup</head><p>Following <ref type=""bibr"" target=""#b4"">[Vaswani et al., 2017]</ref>, we evaluate on the WMT 2014 English-Germ",1
"riments using ""transformer-decoder"" language models on the Billion-Word Language Modeling Benchmark <ref type=""bibr"" target=""#b1"">[Chelba et al., 2013]</ref>. For the baseline, we use a model with 6 l",0
"tei-c.org/ns/1.0""><head n=""2"">Background: Neural Attention</head><p>Neural Attention, introduced by <ref type=""bibr"" target=""#b0"">[Bahdanau et al., 2014]</ref>, is a powerful tool for manipulating var",0
"ssing the number of memory positions, as in <ref type=""bibr"" target=""#b2"">[Liu et al., 2018]</ref>, <ref type=""bibr"" target=""#b5"">[Zhang et al., 2018]</ref>, <ref type=""bibr"" target=""#b3"">[Povey et al",0
"ttending to a local neighborhood, or by otherwise compressing the number of memory positions, as in <ref type=""bibr"" target=""#b2"">[Liu et al., 2018]</ref>, <ref type=""bibr"" target=""#b5"">[Zhang et al.,",0
"r any combination of input parameters ( § 3).</p><p>• Based on the red-blue pebble game abstraction <ref type=""bibr"" target=""#b34"">[34]</ref>, we provide a new method of deriving I/O lower bounds (Lem o parents (or no children, respectively). Red-Blue Pebble Game Hong and Kung's red-blue pebble game <ref type=""bibr"" target=""#b34"">[34]</ref> models an execution of an algorithm in a two-level memory achinery for deriving I/O lower bounds for general CDAGs. We extend the main lemma by Hong and Kung <ref type=""bibr"" target=""#b34"">[34]</ref>, which provides a method to nd an I/O lower bound for a gi rresponding pebbling. Hong and Kung use a speci c variant of this partition, denoted as S-partition <ref type=""bibr"" target=""#b34"">[34]</ref>.</p><p>We rst introduce our generalization of S-partition, ation that each V i performs at least S I/O operations, we phrase the lemma by Hong and Kung: L 1 ( <ref type=""bibr"" target=""#b34"">[34]</ref>). e minimal number Q of I/O operations for any valid execu ulation of the CDAG, where a calculation is a sequence of allowed moves in the red-blue pebble game <ref type=""bibr"" target=""#b34"">[34]</ref>. Divide the complete calculation into h consecutive subcom d Lemma. We now use the above de nitions and observations to generalize the result of Hong and Kung <ref type=""bibr"" target=""#b34"">[34]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head> he remaining properties of a valid X -partition S(X ), we use the same reasoning as originally done <ref type=""bibr"" target=""#b34"">[34]</ref>.</p><p>erefore, a complete calculation performing q &gt; ( , and therefore may be seen as the last step in the long sequence of improved bounds. Hong and Kung <ref type=""bibr"" target=""#b34"">[34]</ref> derived an asymptotic bound Ω n 3 / √ S for the sequential xmlns=""http://www.tei-c.org/ns/1.0""><head n=""10.1"">General I/O Lower Bounds</head><p>Hong and Kung <ref type=""bibr"" target=""#b34"">[34]</ref> analyzed the I/O complexity for general CDAGs in their the",1
"ref>, graph processing <ref type=""bibr"" target=""#b4"">[4,</ref><ref type=""bibr"" target=""#b8"">8,</ref><ref type=""bibr"" target=""#b18"">18,</ref><ref type=""bibr"" target=""#b36"">36,</ref><ref type=""bibr"" tar",0
"se our work. As a special case, they derived an asymptotic bound Ω n 3 / √ S for MMM. Elango et al. <ref type=""bibr"" target=""#b23"">[23]</ref> extended this work to the red-blue-white game and Liu and",0
"erent variants of pebble games in the context of memory space and parallel time. Aggarwal and Vi er <ref type=""bibr"" target=""#b1"">[2]</ref> introduced a two-memory machine that models a blocked access ns. Bender et al. <ref type=""bibr"" target=""#b7"">[7]</ref> extended Aggarwal's external memory model <ref type=""bibr"" target=""#b1"">[2]</ref> and showed I/O complexity of the sparse matrix-vector (SpMV)",0
"=""bibr"" target=""#b61"">61]</ref> or computational chemistry <ref type=""bibr"" target=""#b45"">[45,</ref><ref type=""bibr"" target=""#b49"">49]</ref>. ey introduced CARMA <ref type=""bibr"" target=""#b22"">[22]</r aLAPACK.  </p><formula xml:id=""formula_67"">• • • • • • • • • • • • • • • CARMA [21]</formula><p>CTF <ref type=""bibr"" target=""#b49"">[49]</ref> COSMA (this work)</p><p>ScaLAPACK <ref type=""bibr"" target= ""#b14"">[14]</ref>   </p><formula xml:id=""formula_68"">• • • • • • • • • • CARMA [21]</formula><p>CTF <ref type=""bibr"" target=""#b49"">[49]</ref> COSMA (this work)</p><p>ScaLAPACK <ref type=""bibr"" target= /p><formula xml:id=""formula_69"">n = m = k = pS 3 • • • • • • • • • • • • CARMA [21]</formula><p>CTF <ref type=""bibr"" target=""#b49"">[49]</ref> COSMA (this work)</p><p>ScaLAPACK <ref type=""bibr"" target= _70"">• • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • CARMA [21]</formula><p>CTF <ref type=""bibr"" target=""#b49"">[49]</ref> COSMA (this work) ScaLAPACK <ref type=""bibr"" target=""#b14"" Limited memory,m = n = 979p 1 3 , k =1.184p 2 3 • • • • • • • • • • • • CARMA [21]</formula><p>CTF <ref type=""bibr"" target=""#b49"">[49]</ref> COSMA (this work)</p><p>ScaLAPACK <ref type=""bibr"" target= xml:id=""formula_72"">• • • • • • • • • • • • • • • • • • • • •• • • • • • CARMA [21]</formula><p>CTF <ref type=""bibr"" target=""#b49"">[49]</ref> COSMA (this work)</p><p>ScaLAPACK <ref type=""bibr"" target= • • • • • • •• • • • • • • • • • • • • • • • • • • • • • • • • • • • • • CARMA [21]</formula><p>CTF <ref type=""bibr"" target=""#b49"">[49]</ref> COSMA (this work)</p><p>ScaLAPACK <ref type=""bibr"" target= k = (p 2/3 S )/3  </p><formula xml:id=""formula_75"">• • • • • • • • • • • CARMA [21]</formula><p>CTF <ref type=""bibr"" target=""#b49"">[49]</ref> COSMA (this work)</p><p>ScaLAPACK <ref type=""bibr"" target= xml:id=""formula_76"">• • • • • • • • • • • • • • • • • • • • •• • • • •• CARMA [21]</formula><p>CTF <ref type=""bibr"" target=""#b49"">[49]</ref> COSMA (this work)</p><p>ScaLAPACK <ref type=""bibr"" target= 3 , k =1.184p 2 3 • • • • • • • • • • • • • • • • • • • • • • • • • •• • CARMA [21]</formula><p>CTF <ref type=""bibr"" target=""#b49"">[49]</ref> COSMA (this work)</p><p>ScaLAPACK <ref type=""bibr"" target=",0
"n years later, Zheng et al. evaluated the performance of exclusive caches with respect to inclusive <ref type=""bibr"" target=""#b39"">[40]</ref>. They found that exclusive caching is beneficial for most",1
"eger and floating point. We also used three single threaded server workload traces from Cloud-Suite <ref type=""bibr"" target=""#b8"">[9]</ref> (data_caching, sat_solver and graph_analytics using the defa",0
"chitecture) had an exclusive L2 (LLC), while its rival at the time, the Pentium 4 (from Willamette) <ref type=""bibr"" target=""#b13"">[14]</ref> had inclusive L2 (LLC). Currently, the latest AMD Zen arch",0
"P <ref type=""bibr"" target=""#b20"">[21]</ref> X SDBP <ref type=""bibr"" target=""#b24"">[25]</ref> X SHiP <ref type=""bibr"" target=""#b35"">[36]</ref> X GIPPR <ref type=""bibr"" target=""#b21"">[22]</ref> X MDPP < the program counter as a signature to predict dead blocks <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b35"">36]</ref>.</p><p>Evicted Address Filter. Seshadri et al. proposed evi",0
"f type=""bibr"" target=""#b32"">[33]</ref> X EAF <ref type=""bibr"" target=""#b27"">[28]</ref> X Perceptron <ref type=""bibr"" target=""#b33"">[34]</ref> X KPCR <ref type=""bibr"" target=""#b25"">[26]</ref> X Hawkeye get=""#b21"">22,</ref><ref type=""bibr"" target=""#b29"">30,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" target=""#b33"">34]</ref>. Jaleel et al. proposed an exclusive version of RRIP where",0
"due to maintaining cache coherency. A notable proposed scalable system architecture is described in <ref type=""bibr"" target=""#b33"">[34]</ref> in which, NAND flash is used as main memory technology and",1
"alability of a design was first investigated and prototyped in the EU funded FP7 project EuroServer <ref type=""bibr"" target=""#b27"">[28]</ref>.</p><p>Following the EuroServer project, a number of subse",0
"er increasing this tension. There is also the issue of voltage drop in wires, also known as IR drop <ref type=""bibr"" target=""#b64"">[65]</ref>, which can reduce the actual core voltage and hence signal",0
"ingle package.</p><p>Historically, research on the Multi-chip modules (MCMs) dates back to the 80s. <ref type=""bibr"" target=""#b13"">[14]</ref>. Currently, the evolution of chip manufacturing process al",0
"16]</ref>, Unified Parallel C (UPC) <ref type=""bibr"" target=""#b28"">[29]</ref>, and Co-array Fortran <ref type=""bibr"" target=""#b56"">[57]</ref>. They often allow a weak memory consistency to counter the",0
"Peters et al., 2018)</ref>, GPT-2 <ref type=""bibr"" target=""#b26"">(Radford et al., 2019)</ref>, BERT <ref type=""bibr"" target=""#b4"">(Devlin et al., 2019)</ref>, <ref type=""bibr"">XLNet (Yang et al., 2019 as reestablished the new state-ofthe-art baselines across various tasks, such as question answering <ref type=""bibr"" target=""#b4"">(Devlin et al., 2019)</ref>, coreference resolution <ref type=""bibr"" t in the pre-training stage, such as updating the model using only short sequences in the early stage <ref type=""bibr"" target=""#b4"">(Devlin et al., 2019)</ref>.</p><p>Common strategies for reducing memo ction. Following the paradigm of language model pre-training and down-stream task fine-tuning, BERT <ref type=""bibr"" target=""#b4"">(Devlin et al., 2019)</ref> consists of multiple layers of bidirection lti-head self-attention layer and a position-wise feed-forward layer. Using the same notation as in <ref type=""bibr"" target=""#b4"">(Devlin et al., 2019)</ref>, we denote the number of Transformer layer ource-intensive process. For instance, the training of BERT-family models is notoriously expensive. <ref type=""bibr"" target=""#b4"">Devlin et al. (2019)</ref> report that it takes four days for pre-trai e compare BlockBERT with the following baselines:</p><p>Google BERT The pre-trained base model from <ref type=""bibr"" target=""#b4"">Devlin et al. (2019)</ref>.</p><p>RoBERTa-2seq and RoBERTa-1seq We com ""figure"">6</ref>.</p><p>For all the pre-trained models, we adopt the same fine-tuning QA setup from <ref type=""bibr"" target=""#b4"">Devlin et al. (2019)</ref>.</p><p>The tokenized paragraph (p 1 , • • • n units H leads to significant performance degradation <ref type=""bibr"">(Vaswani et al., 2017;</ref><ref type=""bibr"" target=""#b4"">Devlin et al., 2019)</ref> and does not address the long sequence issu target=""#b3"">Dai et al., 2019)</ref> and its successful application on language model pre-training <ref type=""bibr"" target=""#b4"">(Devlin et al., 2019;</ref><ref type=""bibr"" target=""#b26"">Radford et a",1
"precision for training neural networks. This technique has been widely used in Transformer training <ref type=""bibr"" target=""#b24"">(Ott et al., 2019;</ref><ref type=""bibr"" target=""#b19"">Liu et al., 20",0
"bibr"" target=""#b23"">(Nogueira &amp; Cho, 2019)</ref> or split their sequences with a sliding window <ref type=""bibr"" target=""#b13"">(Joshi et al., 2019a;</ref><ref type=""bibr"">b)</ref>. Ad-hoc handling SETTINGS</head><p>Our fine-tuning is implemented based on code base from HuggingFace 9 and SpanBERT <ref type=""bibr"" target=""#b13"">(Joshi et al., 2019a)</ref>. We use max sequence length=N , i.e., we",0
"al., 2017)</ref>, TriviaQA <ref type=""bibr"" target=""#b12"">(Joshi et al., 2017)</ref> and NaturalQA <ref type=""bibr"" target=""#b16"">(Kwiatkowski et al., 2019)</ref> These QA datasets have different par",0
"ler et al., 2017)</ref>, SearchQA <ref type=""bibr"" target=""#b5"">(Dunn et al., 2017)</ref>, TriviaQA <ref type=""bibr"" target=""#b12"">(Joshi et al., 2017)</ref> and NaturalQA <ref type=""bibr"" target=""#b1",0
"l., 2018;</ref><ref type=""bibr"" target=""#b12"">Gui et al., 2019b)</ref>.</p><p>Recently, Transformer <ref type=""bibr"" target=""#b34"">(Vaswani et al., 2017</ref>) began to prevail in various NLP tasks, l ""#b34"">(Vaswani et al., 2017</ref>) began to prevail in various NLP tasks, like machine translation <ref type=""bibr"" target=""#b34"">(Vaswani et al., 2017)</ref>, language modeling <ref type=""bibr"" targ mlns=""http://www.tei-c.org/ns/1.0""><head n=""2.2"">Transformer</head><p>Transformer was introduced by <ref type=""bibr"" target=""#b34"">(Vaswani et al., 2017)</ref>, which was mainly based on self-attentio =""bibr"" target=""#b7"">Devlin et al., 2018)</ref>. Instead of using the sinusoidal position embedding <ref type=""bibr"" target=""#b34"">(Vaswani et al., 2017)</ref> and learned absolute position embedding, 1"">Transformer Encoder Architecture</head><p>We first introduce the Transformer encoder proposed in <ref type=""bibr"" target=""#b34"">(Vaswani et al., 2017)</ref>. The Transformer encoder takes in an mat e Transformer encoder includes layer normalization and Residual connection, we use them the same as <ref type=""bibr"" target=""#b34"">(Vaswani et al., 2017)</ref>.</p></div> <div xmlns=""http://www.tei-c. ng it unable to capture the sequential characteristic of languages. In order to solve this problem, <ref type=""bibr"" target=""#b34"">(Vaswani et al., 2017)</ref> suggested to use position embeddings gen two tokens. For any fixed offset k, P E t+k can be represented by a linear transformation of P E t <ref type=""bibr"" target=""#b34"">(Vaswani et al., 2017)</ref>. In TENER, Transformer encoder is used n d in the Transformer is unaware of positions, to avoid this shortage, position embeddings were used <ref type=""bibr"" target=""#b34"">(Vaswani et al., 2017;</ref><ref type=""bibr"" target=""#b7"">Devlin et a",1
"arget=""#b28"">(Peters et al., 2017</ref><ref type=""bibr"" target=""#b27"">(Peters et al., , 2018;;</ref><ref type=""bibr"" target=""#b0"">Akbik et al., 2018)</ref>. ELMo introduced by <ref type=""bibr"" target=",0
"en extensively used in the field of NER <ref type=""bibr"" target=""#b4"">(Chiu and Nichols, 2016;</ref><ref type=""bibr"" target=""#b8"">Dong et al., 2016;</ref><ref type=""bibr"" target=""#b37"">Yang et al., 20",0
"bility than RNNs. However, in the NER task, Transformer encoder has been reported to perform poorly <ref type=""bibr"" target=""#b13"">(Guo et al., 2019)</ref>, our experiments also confirm this result. T =""#tab_3"">3</ref>. The poor performance of the Transformer in the NER datasets was also reported by <ref type=""bibr"" target=""#b13"">(Guo et al., 2019)</ref>. Although performance of the Transformer is ibr"" target=""#b13"">(Guo et al., 2019)</ref>. Although performance of the Transformer is higher than <ref type=""bibr"" target=""#b13"">(Guo et al., 2019)</ref>, it still lags behind the BiLSTM-based model",0
"sm. Therefore, CNN has been proposed by <ref type=""bibr"" target=""#b33"">(Strubell et al., 2017;</ref><ref type=""bibr"" target=""#b11"">Gui et al., 2019a)</ref> to encode words concurrently. In order to en https://github. com/fastnlp/TENER.  <ref type=""bibr"" target=""#b39"">(Zhang and Yang, 2018)</ref> and <ref type=""bibr"" target=""#b11"">(Gui et al., 2019a)</ref>, respectively. ""w/ scale"" means TENER using",0
"9"">10]</ref>. Finally, our proposed robust training method, unlike the previous works in this arena <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" ta the activation functions' responses to the inputs. The closest to our work are the results given in <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" ta r spectral norm regularization against a single threshold β. All the previous works on this subject <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" ta his approach limits the performance on the clean data set and does not produce the most robust DNN. <ref type=""bibr"" target=""#b10"">[11]</ref> uses PAC-Bayes generalization analysis to estimate the rob area. When we consider the accuracy under attack, our Lyapunov approach dominates prior baselines. <ref type=""bibr"" target=""#b10"">[11]</ref> for instance, obtained an accuracy of 62% at = 0.1 with ad estimate the spectral norm of the weight matrix at a specific layer during training as proposed in <ref type=""bibr"" target=""#b10"">[11]</ref>. Further, the pooling layers inside a DNN do not affect th s performed to provide a robustness comparison between our proposed approach and the works given in <ref type=""bibr"" target=""#b10"">[11]</ref>, <ref type=""bibr"" target=""#b27"">[28]</ref>, <ref type=""bib >[37]</ref> and <ref type=""bibr"" target=""#b7"">[8]</ref>. <ref type=""bibr"" target=""#b27"">[28]</ref>, <ref type=""bibr"" target=""#b10"">[11]</ref> propose training networks with the same spectral regulariz 8]</ref> may be seen as subsets of the works given in <ref type=""bibr"" target=""#b27"">[28]</ref> and <ref type=""bibr"" target=""#b10"">[11]</ref>, where ρ(W l ) ≤ β for l = 1, ..., n and β = 1.0 are selec",1
"et=""#b10"">[11,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b36"">37,</ref><ref type=""bibr"" target=""#b7"">8]</ref>, regularizes the spectral norm of the weight matrix at each i et=""#b10"">[11,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b36"">37,</ref><ref type=""bibr"" target=""#b7"">8]</ref>. <ref type=""bibr"" target=""#b7"">[8]</ref> utilizes Lipschitz p et=""#b10"">[11,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b36"">37,</ref><ref type=""bibr"" target=""#b7"">8]</ref>, keep β constant across layers. These harder constraints over get=""#b27"">28,</ref><ref type=""bibr"" target=""#b36"">37,</ref><ref type=""bibr"" target=""#b7"">8]</ref>. <ref type=""bibr"" target=""#b7"">[8]</ref> utilizes Lipschitz properties of the DNN to improve robustne utilizes Lipschitz properties of the DNN to improve robustness against adversarial attacks. Unlike <ref type=""bibr"" target=""#b7"">[8]</ref>, our approach does not require a predetermined set of hyper- 11]</ref>, <ref type=""bibr"" target=""#b27"">[28]</ref>, <ref type=""bibr"" target=""#b36"">[37]</ref> and <ref type=""bibr"" target=""#b7"">[8]</ref>. <ref type=""bibr"" target=""#b27"">[28]</ref>, <ref type=""bibr"" their papers: β = 1.0, 1.6, 2.0. The 2 works given in <ref type=""bibr"" target=""#b36"">[37]</ref> and <ref type=""bibr"" target=""#b7"">[8]</ref> may be seen as subsets of the works given in <ref type=""bibr",1
"rget=""#b35"">[36,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b8"">9]</ref>. The bounds provided by these methods are usually loose, and",0
"es learned during training make the development of robust solutions against the adversary difficult <ref type=""bibr"" target=""#b13"">[14]</ref>.</p><p>In this work, we use Lyapunov theory of stability a",0
"ion on the mapping from the input to the output of the DNN <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b18"">19]</ref>. Finally, we will s",0
"to the input generally improves robustness. Works such as <ref type=""bibr"" target=""#b34"">[35,</ref><ref type=""bibr"" target=""#b39"">40]</ref> focus on certifying robustness for a DNN by calculating the",0
"es' freedom to alter the input. While this does not capture the full scope of potential adversaries <ref type=""bibr"" target=""#b3"">[4]</ref>, attacks of this form have proven difficult to prevent <ref",0
"he same prediction <ref type=""bibr"" target=""#b35"">[36,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b8"">9]</ref>. The bounds provided by",0
"theory of stability and robustness of nonlinear systems which dates back to more than a century ago <ref type=""bibr"" target=""#b21"">[22]</ref>. We treat each layer of the DNN as a nonlinear system and e Euclidean space, when their respective input signals are also close enough in the Euclidean space <ref type=""bibr"" target=""#b21"">[22]</ref>. Mathematically, these definitions can be represented as f et=""#b21"">[22]</ref>. Mathematically, these definitions can be represented as follows, Definition 1 <ref type=""bibr"" target=""#b21"">[22]</ref> System H is instantaneously incrementally finite-gain (IIF ysis of systems based on a generalized notion of the ""energy"" supplied and dissipated in the system <ref type=""bibr"" target=""#b21"">[22]</ref>. This generalized notion of ""energy"" is defined based on a tness in one layer may be compensated for by an excess of robustness and stability in another layer <ref type=""bibr"" target=""#b21"">[22]</ref>. In Subsection 4.3, we provide an interpretation of the ab f the above relations and outline the implications behind the selection of ν and δ.</p><p>Theorem 1 <ref type=""bibr"" target=""#b21"">[22]</ref> If the nonlinear system H is IIFOFP with δ &gt; 0, then it ger ν implying a larger a and a larger distance between the lower-bound of the cone and the ∆U axis <ref type=""bibr"" target=""#b21"">[22]</ref>. We are encouraging the pair (∆ 1 , ∆ n ) to be instantane of feed-forward connections on robustness of nonlinear systems have been explored in control theory <ref type=""bibr"" target=""#b21"">[22]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head> ce any continuously differentiable condition on the mapping from the input to the output of the DNN <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" ta",0
"skip-connections of these blocks contribute to their lack of robustness against adversarial attacks <ref type=""bibr"" target=""#b12"">[13]</ref> and that these blocks require more restrictive Lyapunov co",0
"ce and is not structured for human communication (e.g., unlike words).</p><p>Several recent studies <ref type=""bibr"" target=""#b60"">[61,</ref><ref type=""bibr"" target=""#b45"">46,</ref><ref type=""bibr"" ta be used with various pretext tasks. In this paper, we follow a simple instance discrimination task <ref type=""bibr"" target=""#b60"">[61,</ref><ref type=""bibr"" target=""#b62"">63,</ref><ref type=""bibr"" ta 8"">[29]</ref>. Contrastive learning is at the core of several recent works on unsupervised learning <ref type=""bibr"" target=""#b60"">[61,</ref><ref type=""bibr"" target=""#b45"">46,</ref><ref type=""bibr"" ta </ref>. Overall, all three mechanisms benefit from a larger K. A similar trend has been observed in <ref type=""bibr"" target=""#b60"">[61,</ref><ref type=""bibr"" target=""#b55"">56]</ref> under the memory b t tasks can be based on some form of contrastive loss functions. The instance discrimination method <ref type=""bibr"" target=""#b60"">[61]</ref> is related to the exemplar-based task <ref type=""bibr"" tar +/τ ) K i=0 exp(q•ki/τ )<label>(1)</label></formula><p>where τ is a temperature hyper-parameter per <ref type=""bibr"" target=""#b60"">[61]</ref>. The sum is over one positive and K negative samples. Intu these networks to downstream tasks.</p><p>Another mechanism is the memory bank approach proposed by <ref type=""bibr"" target=""#b60"">[61]</ref> (Figure <ref type=""figure"">2b</ref>). A memory bank consis ver the past epoch and thus are less consistent. A momentum update is adopted on the memory bank in <ref type=""bibr"" target=""#b60"">[61]</ref>. Its momentum update is on the representations of the same igning a new pretext task, we use a simple one mainly following the instance discrimination task in <ref type=""bibr"" target=""#b60"">[61]</ref>, to which some recent works <ref type=""bibr"" target=""#b62"" =""bibr"" target=""#b62"">[63,</ref><ref type=""bibr"" target=""#b1"">2]</ref> are related.</p><p>Following <ref type=""bibr"" target=""#b60"">[61]</ref>, we consider a query and a key as a positive pair if they ose last fully-connected layer (after global average pooling) has a fixed-dimensional output (128-D <ref type=""bibr"" target=""#b60"">[61]</ref>). This output vector is normalized by its L2-norm <ref typ (128-D <ref type=""bibr"" target=""#b60"">[61]</ref>). This output vector is normalized by its L2-norm <ref type=""bibr"" target=""#b60"">[61]</ref>. This is the representation of the query or key. The tempe or key. The temperature τ in Eqn.( <ref type=""formula"" target=""#formula_0"">1</ref>) is set as 0.07 <ref type=""bibr"" target=""#b60"">[61]</ref>. The data augmentation setting follows <ref type=""bibr"" ta f>) is set as 0.07 <ref type=""bibr"" target=""#b60"">[61]</ref>. The data augmentation setting follows <ref type=""bibr"" target=""#b60"">[61]</ref>: a 224×224-pixel crop is taken from a randomly resized ima ate of 0.03. We train for 200 epochs with the learning rate multiplied by 0.1 at 120 and 160 epochs <ref type=""bibr"" target=""#b60"">[61]</ref>, taking ∼53 hours training ResNet-50. For IG-1B, we use a ecause the positive key is in the same mini-batch). The network is ResNet-50.</p><p>The memory bank <ref type=""bibr"" target=""#b60"">[61]</ref> mechanism can support a larger dictionary size. But it is We hope an advanced pretext task will improve this. Beyond the simple instance discrimination task <ref type=""bibr"" target=""#b60"">[61]</ref>, it is possible to adopt MoCo for pretext tasks like maske 1"">Here 58.0% is with InfoNCE and K=65536. We reproduce 54.3% when using NCE and K=4096 (the same as<ref type=""bibr"" target=""#b60"">[61]</ref>), close to 54.0% in<ref type=""bibr"" target=""#b60"">[61]</re when using NCE and K=4096 (the same as<ref type=""bibr"" target=""#b60"">[61]</ref>), close to 54.0% in<ref type=""bibr"" target=""#b60"">[61]</ref>.</note> 			<note xmlns=""http://www.tei-c.org/ns/1.0"" place sed on other forms <ref type=""bibr"" target=""#b28"">[29,</ref><ref type=""bibr"" target=""#b58"">59,</ref><ref type=""bibr"" target=""#b60"">61,</ref><ref type=""bibr"" target=""#b35"">36]</ref>, such as margin-bas specific pretext task. The input x q and x k can be images <ref type=""bibr"" target=""#b28"">[29,</ref><ref type=""bibr"" target=""#b60"">61,</ref><ref type=""bibr"" target=""#b62"">63]</ref>, patches <ref type= backbone, by default used in existing ResNet-based results <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b60"">61,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" tar",1
"his paper, we follow a simple instance discrimination task <ref type=""bibr"" target=""#b60"">[61,</ref><ref type=""bibr"" target=""#b62"">63,</ref><ref type=""bibr"" target=""#b1"">2]</ref>: a query matches a ke et=""#b28"">[29,</ref><ref type=""bibr"" target=""#b45"">46,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" target=""#b62"">63,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" targe x k can be images <ref type=""bibr"" target=""#b28"">[29,</ref><ref type=""bibr"" target=""#b60"">61,</ref><ref type=""bibr"" target=""#b62"">63]</ref>, patches <ref type=""bibr"" target=""#b45"">[46]</ref>, or cont k can be identical <ref type=""bibr"" target=""#b28"">[29,</ref><ref type=""bibr"" target=""#b58"">59,</ref><ref type=""bibr"" target=""#b62"">63]</ref>, partially shared <ref type=""bibr"" target=""#b45"">[46,</ref> stance discrimination task in <ref type=""bibr"" target=""#b60"">[61]</ref>, to which some recent works <ref type=""bibr"" target=""#b62"">[63,</ref><ref type=""bibr"" target=""#b1"">2]</ref> are related.</p><p>F tive pair if they originate from the same image, and otherwise as a negative sample pair. Following <ref type=""bibr"" target=""#b62"">[63,</ref><ref type=""bibr"" target=""#b1"">2]</ref>, we take two random",1
".7 points in AP dp 75 , in this highly localization-sensitive task. LVIS v0.5 instance segmentation <ref type=""bibr"" target=""#b26"">[27]</ref>: this task has ∼1000 long-tailed distributed categories. S lementation: LVIS instance segmentation</head><p>We use Mask R-CNN with R50-FPN, fine-tuned in LVIS <ref type=""bibr"" target=""#b26"">[27]</ref> train v0.5 and evaluated in val v0.5. We follow the baseli ype=""bibr"" target=""#b26"">[27]</ref> train v0.5 and evaluated in val v0.5. We follow the baseline in <ref type=""bibr"" target=""#b26"">[27]</ref> (arXiv v3 Appendix B).</p><p>LVIS is a new dataset and mod",0
"ng is most influential when serving as the initialization for finetuning in downstream tasks (e.g., <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" ta",0
"get=""#b45"">46,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" target=""#b65"">66,</ref><ref type=""bibr"" target=""#b34"">35,</ref><ref type=""bibr"" target=""#b55"">56,</ref><ref type=""bibr"" tar get=""#b45"">46,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" target=""#b65"">66,</ref><ref type=""bibr"" target=""#b34"">35,</ref><ref type=""bibr"" target=""#b55"">56,</ref><ref type=""bibr"" tar arget=""#b35"">36,</ref><ref type=""bibr"" target=""#b62"">63,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b34"">35]</ref>, Figure <ref type=""figure"">2a</ref>). It uses samples in th dual stage removed <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b45"">46,</ref><ref type=""bibr"" target=""#b34"">35]</ref>, and R170 is made wider <ref type=""bibr"" target=""#b34"">[35] is supervised by ImageNet labels.</p><p>patchified inputs <ref type=""bibr"" target=""#b45"">[46,</ref><ref type=""bibr"" target=""#b34"">35]</ref>, carefully tailored receptive fields <ref type=""bibr"" targe found that using BN prevents the model from learning good representations, as similarly reported in <ref type=""bibr"" target=""#b34"">[35]</ref> (which avoids using BN). The model appears to ""cheat"" the type=""bibr"" target=""#b45"">46,</ref><ref type=""bibr"" target=""#b34"">35]</ref>, and R170 is made wider <ref type=""bibr"" target=""#b34"">[35]</ref>; Rv50 is a reversible net <ref type=""bibr"" target=""#b22"">[",0
"pe=""bibr"" target=""#b41"">[42]</ref>, etc. As prerequisites, we discuss two important issues involved <ref type=""bibr"" target=""#b30"">[31]</ref>: normalization and schedules.</p><p>Normalization. As note m initialization can be strong baselines, and can match the ImageNet supervised counterpart on COCO <ref type=""bibr"" target=""#b30"">[31]</ref>. Our goal is to investigate transferabil-AP 50</p></div> < epochs) or 2× schedules <ref type=""bibr"" target=""#b21"">[22]</ref> for COCO, in contrast to 6×∼9× in <ref type=""bibr"" target=""#b30"">[31]</ref>. On smaller datasets like VOC, training longer may not cat ype=""bibr"" target=""#b30"">[31]</ref>. On smaller datasets like VOC, training longer may not catch up <ref type=""bibr"" target=""#b30"">[31]</ref>. Nonetheless, in our fine-tuning, MoCo uses the same sched ield. In Table <ref type=""table"">A</ref>.1, we supplement the results of a 6× schedule (∼72 epochs) <ref type=""bibr"" target=""#b30"">[31]</ref> and compare with those of the 2× schedule.</p><p>We observ",0
"sful in natural language processing, e.g., as shown by GPT <ref type=""bibr"" target=""#b49"">[50,</ref><ref type=""bibr"" target=""#b50"">51]</ref> and BERT <ref type=""bibr"" target=""#b11"">[12]</ref>. But sup",0
"ral recent studies <ref type=""bibr"" target=""#b60"">[61,</ref><ref type=""bibr"" target=""#b45"">46,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" target=""#b65"">66,</ref><ref type=""bibr"" tar upervised learning <ref type=""bibr"" target=""#b60"">[61,</ref><ref type=""bibr"" target=""#b45"">46,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" target=""#b65"">66,</ref><ref type=""bibr"" tar <ref type=""bibr"" target=""#b62"">63]</ref>, partially shared <ref type=""bibr"" target=""#b45"">[46,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" target=""#b1"">2]</ref>, or different <ref ty l mechanism (e.g., <ref type=""bibr"" target=""#b28"">[29,</ref><ref type=""bibr"" target=""#b45"">46,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" target=""#b62"">63,</ref><ref type=""bibr"" tar type=""bibr"" target=""#b24"">[25]</ref>. Some recent methods <ref type=""bibr"" target=""#b45"">[46,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" target=""#b1"">2]</ref> are based on pretext et=""#b28"">[29,</ref><ref type=""bibr"" target=""#b58"">59,</ref><ref type=""bibr"" target=""#b60"">61,</ref><ref type=""bibr"" target=""#b35"">36]</ref>, such as margin-based losses and variants of NCE losses. Th",0
"ion, and its images generally contain iconic view of objects.</p><p>Instagram-1B (IG-1B): Following <ref type=""bibr"" target=""#b43"">[44]</ref>, this is a dataset of ∼1 billion (940M) public images from is a dataset of ∼1 billion (940M) public images from Instagram. The images are from ∼1500 hashtags <ref type=""bibr"" target=""#b43"">[44]</ref> that are related to the ImageNet categories. This dataset",0
"t in this task. MoCo with IG-1B surpasses it in all metrics.</p><p>Cityscapes instance segmentation <ref type=""bibr"" target=""#b9"">[10]</ref>: MoCo with IG-1B is on par with its supervised pre-training re-training counterpart in AP mk , and is higher in AP mk 50 . Semantic segmentation: On Cityscapes <ref type=""bibr"" target=""#b9"">[10]</ref>, MoCo outperforms its supervised pre-training counterpart b",0
". Limago explores the changes needed to upgrade an existing open-source TCP/IP stack from 10 Gbit/s <ref type=""bibr"" target=""#b10"">[11]</ref> to 100 Gbit/s, but maintaining the same high-productivity . The starting point for Limago is a 10 Gbit/s TOE written by Sidler et al. in C++ using Vivado-HLS <ref type=""bibr"" target=""#b10"">[11]</ref>, <ref type=""bibr"" target=""#b28"">[29]</ref>.</p></div> <div ng data path (Rx Engine), the outgoing data path (Tx Engine), and the state-keeping data structures <ref type=""bibr"" target=""#b10"">[11]</ref>, <ref type=""bibr"" target=""#b28"">[29]</ref>. The dash boxes",1
"s. The design targets low latency applications such as high-frequency trading.</p><p>The authors in <ref type=""bibr"" target=""#b24"">[25]</ref> presented a comparison of three 10 Gbit/s alternatives: a",0
"rg/ns/1.0""><head>B. CuckooCAM</head><p>The 10 Gbit/s version of the stack is based on the smart-CAM <ref type=""bibr"" target=""#b13"">[14]</ref> module provided by Xilinx. It used a four-tuple consisting",0
"e to achieve with the Vivado-HLS version being used (2018.2). The circuit is described in detail in <ref type=""bibr"" target=""#b12"">[13]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>",0
"bibr"" target=""#b6"">[7]</ref>. Catapult is, by far, not the only possible design. In IBM's cloudFPGA <ref type=""bibr"" target=""#b7"">[8]</ref>, the FPGA is deployed as a network-attached accelerator. Sim",0
"Radford et al. (2018)</ref> demonstrate a pre-trained generative model (GPT) and its effects, while <ref type=""bibr"" target=""#b10"">Devlin et al. (2019b)</ref> release a pre-trained deep Bidirectional entation from Transformers (BERT), achieving state-of-the-arts on dozens of benchmarks.</p><p>After <ref type=""bibr"" target=""#b10"">Devlin et al. (2019b)</ref>, similar pre-trained encoders spring up r many alternatives for pre-trained language representation can be used, e.g., masked language model <ref type=""bibr"" target=""#b10"">(Devlin et al., 2019b)</ref>. Note that those two tasks only share th output at [CLS] is often used as the sentence representation.</p><p>PLM Objective Inspired by BERT <ref type=""bibr"" target=""#b10"">(Devlin et al., 2019b)</ref>, MLM randomly selects 15% of input token LS] output for sentence-level prediction and the outputs of all tokens for sequence labelling tasks <ref type=""bibr"" target=""#b10"">(Devlin et al., 2019b)</ref> </p></div> <div xmlns=""http://www.tei-c. use the transformer architecture <ref type=""bibr"" target=""#b41"">(Vaswani et al., 2017)</ref> as in <ref type=""bibr"" target=""#b10"">(Devlin et al., 2019b;</ref><ref type=""bibr"" target=""#b20"">Liu et al.",1
"tation models (PLMs) such as ELMo <ref type=""bibr"" target=""#b29"">(Peters et al., 2018a)</ref>, BERT <ref type=""bibr"" target=""#b9"">(Devlin et al., 2019a)</ref> and XLNet <ref type=""bibr"" target=""#b46"">",1
"t=""#b22"">(Madotto et al., 2018)</ref>, but also some early works use text as additional information <ref type=""bibr"" target=""#b44"">(Xie et al., 2016;</ref><ref type=""bibr"" target=""#b22"">An et al., 201 target=""#b37"">(Sun et al., 2019a)</ref> combines the advantages of both of them. Among these works, <ref type=""bibr"" target=""#b44"">Xie et al. (2016)</ref> propose to utilize entity descriptions as an",1
"ity mentions into pre-defined entity types. For this task, we evaluate all the models on OpenEntity <ref type=""bibr"" target=""#b5"">(Choi et al., 2018)</ref> following the setting from <ref type=""bibr""",0
"dge graph dataset with aligned text description for each entity. Wikidata5m is a subset of Wikidata <ref type=""bibr"" target=""#b42"">(Vrandečić and Krötzsch, 2014)</ref>, a free knowledge base with abou nowledge graph dataset with aligned text descriptions. Our dataset is built by integrating Wikidata <ref type=""bibr"" target=""#b42"">(Vrandečić and Krötzsch, 2014)</ref>, a large-scale open knowledge ba",0
"commonlyused datasets: TACRED <ref type=""bibr"" target=""#b48"">(Zhang et al., 2017)</ref> and FewRel <ref type=""bibr"" target=""#b13"">(Han et al., 2018)</ref>. TACRED covers 42 relation types and contain to"" indicates Prototypical Networks<ref type=""bibr"" target=""#b36"">(Snell et al., 2017)</ref> used in<ref type=""bibr"" target=""#b13"">Han et al. (2018)</ref>. ""PAIR"" is proposed in<ref type=""bibr"" target",0
"not scale to Wikidata5m, we benchmark these methods using the multi-GPU implementation in GraphVite <ref type=""bibr"" target=""#b51"">(Zhu et al., 2019)</ref>. The performance of link prediction is evalu",0
"get=""#b2"">(Bordes et al., 2013)</ref> treats tail entities as translations of heads, while DistMult <ref type=""bibr"" target=""#b45"">(Yang et al., 2015)</ref> use matrix multiplications as score functio E <ref type=""bibr"" target=""#b2"">(Bordes et al., 2013)</ref> 109370 0.253 0.170 0.311 0.392 DistMult <ref type=""bibr"" target=""#b45"">(Yang et al., 2015)</ref> 211030 0.253 0.208 0.278 0.334 ComplEx <ref ding models , including TransE <ref type=""bibr"" target=""#b2"">(Bordes et al., 2013)</ref>, Dist-Mult <ref type=""bibr"" target=""#b45"">(Yang et al., 2015)</ref>, ComplEx <ref type=""bibr"" target=""#b40"">(Tr",0
"""#b46"">Yang et al. (2019)</ref> propose a permutation language model (XLNet) based on TransformerXL <ref type=""bibr"" target=""#b8"">(Dai et al., 2019)</ref>. Later, <ref type=""bibr"" target=""#b20"">Liu et",0
"ypically lack of factual world knowledge <ref type=""bibr"" target=""#b32"">(Petroni et al., 2019;</ref><ref type=""bibr"" target=""#b21"">Logan et al., 2019)</ref>.</p><p>Recent works <ref type=""bibr"" target",0
"remoodi et al., 2018)</ref>, reading comprehension <ref type=""bibr"">(Mihaylov and Frank, 2018;</ref><ref type=""bibr"" target=""#b50"">Zhong et al., 2019)</ref> and dialogue system <ref type=""bibr"" target",0
"of our model. However, we still keep MLM as one of our objectives to avoid catastrophic forgetting <ref type=""bibr"" target=""#b23"">(McCloskey and Cohen, 1989)</ref> while training towards the KRL loss",0
"and Frank, 2018;</ref><ref type=""bibr"" target=""#b50"">Zhong et al., 2019)</ref> and dialogue system <ref type=""bibr"" target=""#b22"">(Madotto et al., 2018)</ref>, but also some early works use text as a rly works use text as additional information <ref type=""bibr"" target=""#b44"">(Xie et al., 2016;</ref><ref type=""bibr"" target=""#b22"">An et al., 2018)</ref> or jointly train the knowledge and text embedd",0
"t=""#b6"">(Collobert and Weston, 2008;</ref><ref type=""bibr"" target=""#b25"">Mikolov et al., 2013;</ref><ref type=""bibr"" target=""#b27"">Pennington et al., 2014)</ref>, many of which are still often adopted",0
"><p>Inspired by the success of augmentation methods in ASR <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b16"">17]</ref>, as a remedy to avoid overfitting while using lowresource t",1
"nd a machine translation (MT) model trained on bilingual text data. Recent advancements in both ASR <ref type=""bibr"" target=""#b0"">[1]</ref><ref type=""bibr"" target=""#b1"">[2]</ref><ref type=""bibr"" targe </ref><ref type=""bibr"" target=""#b12"">13]</ref>. Some appealing advantages of the direct models are: <ref type=""bibr"" target=""#b0"">(1)</ref> no error accumulation from the recognizer, (2) faster decodi",0
"#b5"">[6]</ref> and MT <ref type=""bibr"" target=""#b6"">[7]</ref><ref type=""bibr"" target=""#b7"">[8]</ref><ref type=""bibr"" target=""#b8"">[9]</ref><ref type=""bibr"" target=""#b9"">[10]</ref><ref type=""bibr"" targ > 			<note xmlns=""http://www.tei-c.org/ns/1.0"" place=""foot"" n=""9"" xml:id=""foot_5"">computed by tercom<ref type=""bibr"" target=""#b8"">9</ref> . WER is</note> 			<note xmlns=""http://www.tei-c.org/ns/1.0"" p",0
"dequate volume of training data, one remedy is generating synthetic data like back-translation (BT) <ref type=""bibr"" target=""#b13"">[14]</ref> as the most common data augmentation method to leverage mo",0
"ave inspired the end-to-end direct ST models which can be trained using a translation speech corpus <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b12"">13]</ref>. Some appealing ad",0
"target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" tar pe=""bibr"" target=""#b29"">30]</ref>. Meanwhile, recent works <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b44"">45]</ref> also suggest that t and adversarial examples is a key factor that causes the performance degradation in previous works <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" ta b17"">[18]</ref> shows the result of training with random normal perturbations) or adversarial noise <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" ta evious conclusions <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b41"">42,</ref><ref type=""bibr"" target=""#b15"">16]</ref> that the performance degradation is always observed if adve vanilla training baseline by 0.6%. However, previous works <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b15"">16]</ref> show adversarial training always degrades performance.</p>< al training always degrades performance.</p><p>Compared to <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b15"">16]</ref>, we make two changes in our reimplementation: (1) using str",1
"arget=""#b6"">7,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" tar that causes the performance degradation in previous works <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b44"">45]</ref>.</p><p>In this pape state-of-the-arts for defending against adversarial attacks <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" tar ing with random normal perturbations) or adversarial noise <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b41"">42]</ref>, fail to improve ac pe=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b29"">30]</ref>. Meanwhile, recent works <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" ta etter recognition than the vanilla training baseline. These results contradict previous conclusions <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b41"">42,</ref><ref type=""bibr"" ta -1 accuracy on ImageNet, which beats the vanilla training baseline by 0.6%. However, previous works <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b15"">16]</ref> show adversarial t r"" target=""#b15"">16]</ref> show adversarial training always degrades performance.</p><p>Compared to <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b15"">16]</ref>, we make two chang ting noise. However, all previous attempts, by augmenting either with random noise (e.g., Tab. 5 in <ref type=""bibr"" target=""#b17"">[18]</ref> shows the result of training with random normal perturbati ojection step in PGD; or (2) we skip the random noise initialization step in PGD, turn it to I-FGSM <ref type=""bibr"" target=""#b17"">[18]</ref>. Other attack hyper-parameters are unchanged: the maximum Vanilla Training 81.7 83.7 84.5 PGD <ref type=""bibr"" target=""#b22"">[23]</ref> 81.8 84.3 85.2 I-FGSM <ref type=""bibr"" target=""#b17"">[18]</ref> 81.9 84. In Sec. 5.3, we show that adversarial training ca re of adversarial examples and clean images, as suggested in <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b17"">18]</ref>,</p><formula xml:id=""formula_3"">arg min θ E (x,y)∼D L(θ, x, al and clean domains. However, as observed in former studies <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b17"">18]</ref>, directly optimizing Eq. ( <ref type=""formula"" target=""#for stronger performance than the adversarial training baseline <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b17"">18]</ref>. Besides, compared to the fine-tuning strategy in Sec. 3, A",1
"racy on ImageNet-A <ref type=""bibr"" target=""#b9"">[10]</ref> and 26.6% accuracy on Stylized-ImageNet <ref type=""bibr"" target=""#b5"">[6]</ref>, beating its vanilla counterpart by 0.7%, 6.5%, 7.0% and 4.8 "" target=""#b8"">[9]</ref>, ImageNet-A <ref type=""bibr"" target=""#b9"">[10]</ref> and Stylized-ImageNet <ref type=""bibr"" target=""#b5"">[6]</ref>, respectively.</p><p>As AdvProp effectively prevents overfit rios (e.g., occlusion and fog scene) which are difficult for recognition.</p><p>• Stylized-ImageNet <ref type=""bibr"" target=""#b5"">[6]</ref>. The Stylized-ImageNet dataset is created by removing local natural images via AdaIN style transfer <ref type=""bibr"" target=""#b12"">[13]</ref>. As suggested in <ref type=""bibr"" target=""#b5"">[6]</ref>, networks are required to learn more shape-based representat These are the best results so far if models are not allowed to train with corresponding distortions <ref type=""bibr"" target=""#b5"">[6]</ref> or extra data <ref type=""bibr"" target=""#b23"">[24,</ref><ref curacy on ImageNet-A<ref type=""bibr"" target=""#b9"">[10]</ref> and 26.6% accuracy on Stylized-ImageNet<ref type=""bibr"" target=""#b5"">[6]</ref>, beating its vanilla counterpart by 0.7%, 6.5%, 7.0% and 4.8 tortions are specifically designed for images of the size 224×224×3, so we follow the previous setup<ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b8"">9]</ref> to always fix the test",0
"oposed, e.g., applying masking out <ref type=""bibr"" target=""#b4"">[5]</ref> or adding Gaussian noise <ref type=""bibr"" target=""#b21"">[22]</ref> to regions in images, or mixing up pairs of images and the",0
"/ns/1.0""><head n=""4.2."">Disentangled Learning via An Auxiliary BN</head><p>Batch normalization (BN) <ref type=""bibr"" target=""#b13"">[14]</ref> serves as an essential component for many state-of-the-art",0
"ower is better) on ImageNet-C <ref type=""bibr"" target=""#b8"">[9]</ref>, 44.7% accuracy on ImageNet-A <ref type=""bibr"" target=""#b9"">[10]</ref> and 26.6% accuracy on Stylized-ImageNet <ref type=""bibr"" ta mprovement of 9.0%, 7.0% and 5.0% on ImageNet-C <ref type=""bibr"" target=""#b8"">[9]</ref>, ImageNet-A <ref type=""bibr"" target=""#b9"">[10]</ref> and Stylized-ImageNet <ref type=""bibr"" target=""#b5"">[6]</re of corruption has five levels of severity, resulting in 75 distinct corruptions.</p><p>• ImageNet-A <ref type=""bibr"" target=""#b9"">[10]</ref>. The ImageNet-A dataset adversarially collects 7,500 natura lower is better) on ImageNet-C<ref type=""bibr"" target=""#b8"">[9]</ref>, 44.7% accuracy on ImageNet-A<ref type=""bibr"" target=""#b9"">[10]</ref> and 26.6% accuracy on Stylized-ImageNet<ref type=""bibr"" tar",0
"small datasets (e.g., MNIST) in the fully-supervised setting <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b19"">20]</ref>, or on larger datasets but in the semi-supervised setting <",0
"ions <ref type=""bibr"" target=""#b5"">[6]</ref> or extra data <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b45"">46]</ref>.</p><p>To summarize, the results suggest that AdvProp signi",0
"sarialexamples is a form of data augmentation. We choose the standard Inception-style pre-processing<ref type=""bibr"" target=""#b37"">[38]</ref> as baseline, and compare the benefits of additionally appl",0
"CIFAR-10 [23] and ∼15% drop on Im-ageNet <ref type=""bibr"" target=""#b44"">[45]</ref>. Tsipras et al. <ref type=""bibr"" target=""#b42"">[43]</ref> argue that the performance tradeoff between adversarial ro es make network representations align better with salient data characteristics and human perception <ref type=""bibr"" target=""#b42"">[43]</ref>. Moreover, such trained models are much more robust to hig es as described in <ref type=""bibr"" target=""#b50"">[51,</ref><ref type=""bibr"" target=""#b46"">47,</ref><ref type=""bibr"" target=""#b42"">43]</ref>, they cannot generalize well to clean images <ref type=""bib",0
"g, and orange NAS components. Top: gradientbased meta-learning with fixed architecture such as MAML <ref type=""bibr"" target=""#b15"">[16]</ref> or REPTILE <ref type=""bibr"" target=""#b36"">[37]</ref>. Midd are as follows:</p><p>1. We show that model-agnostic, gradient-based metalearning methods (such as <ref type=""bibr"" target=""#b15"">[16]</ref>) can very naturally be combined with recently proposed gra rchitecture search because of conceptual similarities to gradient-based meta-learning, such as MAML <ref type=""bibr"" target=""#b15"">[16]</ref>, which will allow us to combine the two kinds of methods.< rning of Neural Architectures</head><p>Similar to MAML's meta-learning strategy in the weight space <ref type=""bibr"" target=""#b15"">[16]</ref>, our goal is to meta-learn an architecture with correspond rning algorithm ? not only for w but also for the architecture ?. As an example, one could use MAML <ref type=""bibr"" target=""#b15"">[16]</ref> as a meta-learning algorithm, which runs SGD on the meta-o head>MiniImagenet</head><p>Omniglot Method # params 1-shot, 5-way 5-shot, 5-way 1-shot, 20 way MAML <ref type=""bibr"" target=""#b15"">[16]</ref> 30k 48. which might be of independent interest. Empirical ng, and orange NAS components. Top: gradientbased meta-learning with fixed architecture such as MAML<ref type=""bibr"" target=""#b15"">[16]</ref> or REPTILE<ref type=""bibr"" target=""#b36"">[37]</ref>. Middl ew examples. Prior work has proposed meta-learning methods for this problem that are model-agnostic <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b36"">37]</ref> and allow meta-lea >In this work, we focus on a particular class of approaches denoted as model-agnostic meta-learning <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" ta task-learner, where k refers to k iterations of learning/ weight updates (e.g., by SGD). Prior work <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b36"">37,</ref><ref type=""bibr"" ta eta 3. This is in contrast to prior work where the architecture is always fixed during meta-testing <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b36"">37]</ref>. Also prior work u itting when only very little data is available. Prior work <ref type=""bibr"" target=""#b39"">[40,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" tar not improve performance due to overfitting as reported by <ref type=""bibr"" target=""#b39"">[40,</ref><ref type=""bibr"" target=""#b15"">16]</ref>.</p><p>Results are presented in Table <ref type=""table"" tar",1
"et=""#b39"">[40,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b18"">19]</ref> often approaches few-shot learning via meta-learning or lea that is shared across tasks but fixed during task learning <ref type=""bibr"" target=""#b57"">[58,</ref><ref type=""bibr"" target=""#b18"">19]</ref>.</p><p>In this work, we focus on a particular class of appr",0
"get=""#b10"">[11]</ref> in the context of relaxing discrete distributions via the Gumbel distribution <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b33"">34]</ref>. Note that this re",0
"more thorough literature overview. Researchers often frame NAS as a reinforcement learning problem <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b58"">59,</ref><ref type=""bibr"" targ",0
"lated tasks is known as meta-learning or learning to learn <ref type=""bibr"" target=""#b45"">[46,</ref><ref type=""bibr"" target=""#b49"">50,</ref><ref type=""bibr"" target=""#b20"">21]</ref>. Here, we consider s few-shot learning via meta-learning or learning to learn <ref type=""bibr"" target=""#b45"">[46,</ref><ref type=""bibr"" target=""#b49"">50,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" tar",0
"ent obfuscation based defenses have proven vulnerable. In their recent seminal work, Athalye et al. <ref type=""bibr"" target=""#b1"">[2]</ref> presented a suite of strategies for estimating network gradi range of possible attacks, including those having successfully circumvented many previous defenses <ref type=""bibr"" target=""#b1"">[2]</ref>. Under these attacks, we compare the worst-case robustness o b32"">33]</ref>.</p><p>Unfortunately, many of these methods have proven vulnerable by Athalye et al. <ref type=""bibr"" target=""#b1"">[2]</ref>, who introduced a set of attacking strategies, including a m .</p><p>Thus far, gradient obfuscation is generally considered vulnerable (and at least incomplete) <ref type=""bibr"" target=""#b1"">[2]</ref>. We revisit gradient obfuscation, and our defense demonstrat ased defense mechanisms seem plausible. Yet they are all fragile. As demonstrated by Athalye et al. <ref type=""bibr"" target=""#b1"">[2]</ref>, with random input transformation, adversarial examples can -removal transformation is also ineffective. One can use Backward Pass Differentiable Approximation <ref type=""bibr"" target=""#b1"">[2]</ref> to easily construct effective adversarial examples. In short ation (BPDA).</head><p>To circumvent the defense using non-differentiable operators, Athalye et al. <ref type=""bibr"" target=""#b1"">[2]</ref> introduced a strategy called Backward Pass Differentiable Ap attacking strategy. It causes the adversary to suffer from either exploding or vanishing gradients <ref type=""bibr"" target=""#b1"">[2]</ref>. Figure <ref type=""figure"" target=""#fig_4"">3</ref>-left show #b32"">33]</ref>. Yet those defenses have been proven vulnerable under a reparameterization strategy <ref type=""bibr"" target=""#b1"">[2]</ref>. This strategy aims to find some differentiable function h(• those defenses, the transformed image g(x) remain similar to the input x. Consequently, as shown in <ref type=""bibr"" target=""#b1"">[2]</ref>, those defenses can be easily circumvented by replacing g(•) mlns=""http://www.tei-c.org/ns/1.0""><head n=""4.1."">BPDA Attack and the Variants</head><p>BPDA attack <ref type=""bibr"" target=""#b1"">[2]</ref>, as reviewed in Sec. 3.3, is a powerful way to estimate netw ck-box attacks, including the black-box transfer attack <ref type=""bibr"" target=""#b29"">[30]</ref>   <ref type=""bibr"" target=""#b1"">[2]</ref>. We evaluate other methods using the code provided in the or er all tested attacks. The methods indicated by a star (*) are those circumvented by Athalye et al. <ref type=""bibr"" target=""#b1"">[2]</ref>. We include their results therein as a reference. The other nsformations to the input. But they have been circumvented by Expectation Over Transformation (EOT) <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b2"">3]</ref>. EOT attack first esti rsarial examples of f b directly (so that g(h(•)) = h(•)), without solving the optimization problem <ref type=""bibr"" target=""#b1"">(2)</ref>. We argue that finding such an h(•) is extremely hard. If h(",1
"pe=""bibr"" target=""#b29"">[30]</ref>. Several methods (e.g., <ref type=""bibr"" target=""#b43"">[44,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b51"">52,</ref><ref type=""bibr"" targe input transformation, adversarial examples can still be found using Expectation over Transformation <ref type=""bibr"" target=""#b2"">[3]</ref>, which estimates the network gradient by taking the average e been circumvented by Expectation Over Transformation (EOT) <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b2"">3]</ref>. EOT attack first estimates the gradient of expected f (g(x))",0
"ns, from face recognition authorization to autonomous cars <ref type=""bibr"" target=""#b35"">[36,</ref><ref type=""bibr"" target=""#b42"">43]</ref>, the vulnerability caused by adversarial examples gives ris",0
". Another type of black-box attacking methods is query-based <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b36"">37,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" targe",0
"h uses adversarial examples generated on a known model (e.g., using PGD) to attack an unknown model <ref type=""bibr"" target=""#b29"">[30]</ref>. Several methods (e.g., <ref type=""bibr"" target=""#b43"">[44 We also evaluate our defense against the black-box attacks, including the black-box transfer attack <ref type=""bibr"" target=""#b29"">[30]</ref>   <ref type=""bibr"" target=""#b1"">[2]</ref>. We evaluate oth -based attacks (such as the PGD attack), a natural idea is to obfuscate (or mask) network gradients <ref type=""bibr"" target=""#b29"">[30,</ref><ref type=""bibr"" target=""#b43"">44]</ref>. To this end, ther arial examples for the model with a small N have a different distribution from that with a larger N <ref type=""bibr"" target=""#b29"">[30,</ref><ref type=""bibr"" target=""#b39"">40]</ref>. Identity mapping",0
"the dependency path between the entities might be even more indicative of the relation, as noted by <ref type=""bibr"" target=""#b40"">Toutanova et al. (2015)</ref>. It is quite possible that using these",1
"arget=""#b35"">Sap et al., 2019)</ref>, or extract factual knowledge about relations between entities <ref type=""bibr"" target=""#b29"">(Petroni et al., 2019;</ref><ref type=""bibr"" target=""#b1"">Baldini Soa using LMs as part of a knowledge extraction system.</p><p>In particular, we focus on the setting of <ref type=""bibr"" target=""#b29"">Petroni et al. (2019)</ref> who examine extracting knowledge regardin cting many varieties of knowledge from LMs, in this paper we specifically follow the formulation of <ref type=""bibr"" target=""#b29"">Petroni et al. (2019)</ref>, where factual knowledge is in the form o pt generation and selection/ensembling, and compare them with the manually designed prompts used in <ref type=""bibr"" target=""#b29"">Petroni et al. (2019)</ref>. Majority refers to predicting the majori s to predicting the majority object for each relation, as mentioned above. Man is the baseline from <ref type=""bibr"" target=""#b29"">Petroni et al. (2019)</ref> that only uses the manually designed prom 2019)</ref>.</p><p>Different from analyses probing the representations themselves, our work follows <ref type=""bibr"" target=""#b29"">Petroni et al. (2019)</ref> in probing for factual knowledge with man cting the missing subjects. Since our focus is on improving prompts, we choose to be consistent with<ref type=""bibr"" target=""#b29"">Petroni et al. (2019)</ref> to make a fair comparison, and leave expl mbine the answers from different prompts together ( § 4).</p><p>We experiment on the LAMA benchmark <ref type=""bibr"" target=""#b29"">(Petroni et al., 2019)</ref>, which is an English-language benchmark target=""#b21"">(McCann et al., 2018;</ref><ref type=""bibr"" target=""#b32"">Radford et al., 2019;</ref><ref type=""bibr"" target=""#b29"">Petroni et al., 2019)</ref>, t r has been a single manually defined p",0
"rstanding by formulating queries in natural language and either generating textual answers directly <ref type=""bibr"" target=""#b21"">(McCann et al., 2018;</ref><ref type=""bibr"" target=""#b32"">Radford et uld trigger the LM to predict the ground-truth objects as often as possible.</p><p>In previous work <ref type=""bibr"" target=""#b21"">(McCann et al., 2018;</ref><ref type=""bibr"" target=""#b32"">Radford et",0
"e=""bibr"" target=""#b43"">Yang et al., 2017;</ref><ref type=""bibr"" target=""#b16"">IV et al., 2019;</ref><ref type=""bibr"" target=""#b13"">Hayashi et al., 2020)</ref>. Similar extensions have been applied to",0
"ch that it covers a predefined set of downsampling kernels <ref type=""bibr"" target=""#b37"">[38,</ref><ref type=""bibr"" target=""#b12"">13]</ref>; using DNNs to capture only a natural-image prior which is adation model and assumes that the downsampling kernels belong to a certain set of Gaussian filters <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b37"">38]</ref>. Another approach estimation methods <ref type=""bibr"" target=""#b26"">[27,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b1"">2]</ref>.</p><p>Finally, we wo f>.</p><p>Finally, we would like to highlight major differences be-tween this paper and the work in <ref type=""bibr"" target=""#b12"">[13]</ref>, whose ""kernel correction"" approach may be misunderstood a >[13]</ref>, whose ""kernel correction"" approach may be misunderstood as our ""correction filter"". In <ref type=""bibr"" target=""#b12"">[13]</ref>, three different DNNs (super-resolver, kernel estimator, a ned existing DNN methods (other than SRMD <ref type=""bibr"" target=""#b37"">[38]</ref>) can be used in <ref type=""bibr"" target=""#b12"">[13]</ref>. Secondly, their approach is restricted by the offline tra ur approach. Thirdly, the concepts of these works are very different: The (iterative) correction in <ref type=""bibr"" target=""#b12"">[13]</ref> modifies the estimated downsampling kernel, while our corr erformed with the official code of each method. Unfortunately, such code has not been available for <ref type=""bibr"" target=""#b12"">[13]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head",1
"struction kernel.</p><p>Several works have used the correction filter approach for image processing <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" targ e Fourier Transform and its inverse respectively. To ensure numerical stability, we slightly modify <ref type=""bibr"" target=""#b8"">(9)</ref>, and compute h using</p><formula xml:id=""formula_10"">h = IDF e downsampling kernel k is known. Therefore, the correction filter h can be computed directly using <ref type=""bibr"" target=""#b8"">(9)</ref>. We examine scenarios with scale factors of 2 and 4. For sca 3.5/ √ 2 and scale factor of 2 can be resolved by increasing the regularization parameter ǫ used in <ref type=""bibr"" target=""#b8"">(9)</ref>, which helps compensating the null space in S * R and stabil",0
"get=""#b17"">18,</ref><ref type=""bibr"" target=""#b40"">41,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" target=""#b13"">14]</ref>.</p><p>Typically, the performance of SISR approaches is eva get=""#b17"">18,</ref><ref type=""bibr"" target=""#b40"">41,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" target=""#b13"">14]</ref>, assume that the observations are obtained using the bicubi ic kernel. The modified LR can then be inserted into existing leading super-resolvers, such as DBPN <ref type=""bibr"" target=""#b13"">[14]</ref>, RCAN <ref type=""bibr"" target=""#b40"">[41]</ref>, and proSR oSR <ref type=""bibr"" target=""#b32"">[33]</ref>, RCAN <ref type=""bibr"" target=""#b40"">[41]</ref>, DBPN <ref type=""bibr"" target=""#b13"">[14]</ref>, proSR with correction, RCAN with correction, DBPN with co figure"" target=""#fig_1"">2</ref>(c) with a DNN).</p><p>For the experiments in this paper we use DBPN <ref type=""bibr"" target=""#b13"">[14]</ref>, RCAN <ref type=""bibr"" target=""#b40"">[41]</ref>, and proSR the-shelf DNN super-resolvers that serve as f (•) in <ref type=""bibr"" target=""#b9"">(10)</ref>: DBPN <ref type=""bibr"" target=""#b13"">[14]</ref>, RCAN <ref type=""bibr"" target=""#b40"">[41]</ref>, and proSR oSR <ref type=""bibr"" target=""#b32"">[33]</ref>, RCAN <ref type=""bibr"" target=""#b40"">[41]</ref>, DBPN <ref type=""bibr"" target=""#b13"">[14]</ref>, proSR with correction, RCAN with correction, DBPN with co oSR <ref type=""bibr"" target=""#b32"">[33]</ref>, RCAN <ref type=""bibr"" target=""#b40"">[41]</ref>, DBPN <ref type=""bibr"" target=""#b13"">[14]</ref>, proSR with estimated correction, RCAN with estimated corr RMD <ref type=""bibr"" target=""#b37"">[38]</ref>, ZSSR <ref type=""bibr"" target=""#b25"">[26]</ref>, DBPN <ref type=""bibr"" target=""#b13"">[14]</ref>, proSR <ref type=""bibr"" target=""#b32"">[33]</ref>, RCAN <re proSR<ref type=""bibr"" target=""#b32"">[33]</ref>, RCAN<ref type=""bibr"" target=""#b40"">[41]</ref>, DBPN<ref type=""bibr"" target=""#b13"">[14]</ref>, proSR with correction, RCAN with correction, DBPN with co proSR<ref type=""bibr"" target=""#b32"">[33]</ref>, RCAN<ref type=""bibr"" target=""#b40"">[41]</ref>, DBPN<ref type=""bibr"" target=""#b13"">[14]</ref>, proSR with estimated correction, RCAN with estimated corr ance in performance with respect to the reconstruction error <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" tar",0
"ne of the most examined inverse problems in the past decade <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b34"">35,</ref><ref type=""bibr"" tar non-local similarity <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b34"">35,</ref><ref type=""bibr"" tar",0
"tion comparison on Set14. Each cell displays PSNR [dB] (left), SSIM (middle) and perceptual distance<ref type=""bibr"" target=""#b39"">[40]</ref> (right).</figDesc><table><row><cell></cell><cell cols=""2""> ion comparison on BSD100. Each cell displays PSNR [dB] (left), SSIM (middle) and perceptual distance<ref type=""bibr"" target=""#b39"">[40]</ref> (right).</figDesc><table><row><cell></cell><cell cols=""2"">",0
"strained RL problem and to use a primal-dual algorithm as in <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b10"">11]</ref> that chooses the parameters automatically. The main advanta dual algorithms for constrained reinforcement learning, e.g. <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b10"">11]</ref>, in fact converge to the optimal solution under mild assump type=""bibr"" target=""#b20"">[21]</ref>. Primal-dual algorithms <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b10"">11]</ref>, allow us to choose dynamically the multipliers by find the ning as well, where a policy gradient -or actor critic as in <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b10"">11]</ref> -update is followed by an update of the multipliers along t =""bibr"" target=""#b0"">(1)</ref>. In particular, the proofs in <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b10"">11]</ref> rely on the fact that this different time-scale is such tha primal dual algorithm considered here and those proposed in <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b10"">11]</ref> provide a manner to solve constrained policy optimization p",1
"neural network-which are universal function approximators <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b24"">[25]</ref><ref type=""bibr"" target=""#b25"">[26]</ref><ref type=""bibr"" t",0
"active field of research. CMDP applications cover a vast number of topics, such as: electric grids <ref type=""bibr"" target=""#b13"">[14]</ref>, networking <ref type=""bibr"" target=""#b14"">[15]</ref>, rob aluated at a local maximizer of Lagrangian ? ? (?) approximate the subgradient of the dual function <ref type=""bibr"" target=""#b13"">(14)</ref>. In particular it follows that</p><formula xml:id=""formula",0
"tei-c.org/ns/1.0""><head n=""1.1"">Related Work</head><p>Constrained Markov Decision Processes (CMDPs) <ref type=""bibr"" target=""#b12"">[13]</ref> are an active field of research. CMDP applications cover a",0
"r tuning that often times are domain dependent, as showed in <ref type=""bibr"" target=""#b6"">[7]</ref><ref type=""bibr"" target=""#b7"">[8]</ref><ref type=""bibr"" target=""#b8"">[9]</ref>. Moreover, implicit i",0
"number of clusters reaches the specified K.</p><p>When K is unknown, we adopt an optimal modularity <ref type=""bibr"" target=""#b21"">[22]</ref> partitioning mechanism to determine the partition of publi",1
"es a relational graph convolutional network to link prediction task and entity classification task. <ref type=""bibr"" target=""#b35"">[36]</ref> propose a heterogeneous graph neural network model which c",1
"s. Plenty of researches have been conducted to solve the name ambiguity problem. Supervised methods <ref type=""bibr"" target=""#b0"">[1]</ref>,</p><p>The research is supported by the National Key Researc representations play a critical role to quantify distinctions and similarities between publications <ref type=""bibr"" target=""#b0"">[1]</ref>. The majority of existing solutions utilize biographical fea lustering (HAC) method works well for skewed data and is widely in many name disambiguation methods <ref type=""bibr"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b4"">[5]</ref>, <ref type=""bibr"" t ected components to generate the clustering results for each name to be disambiguated. Zhang et al. <ref type=""bibr"" target=""#b0"">[1]</ref>: This method uses a global metric learning and local linkage For example, <ref type=""bibr"" target=""#b4"">[5]</ref> need to specify the number of distinct author, <ref type=""bibr"" target=""#b0"">[1]</ref> need labeled data to estimate the number. For a fair compari d co-occurrence information of text and loss a certain amount of semantic information. Zhang et al. <ref type=""bibr"" target=""#b0"">[1]</ref> also use a graph convolutional network based encoder-decoder based framework to extract multiple types of characteristics and relations in publication database. <ref type=""bibr"" target=""#b0"">[1]</ref> use a global metric learning and local linkage graph auto-en",1
"s and construct publication networks, then use graph-based <ref type=""bibr"" target=""#b2"">[3]</ref>- <ref type=""bibr"" target=""#b4"">[5]</ref> or heuristic methods <ref type=""bibr"" target=""#b5"">[6]</ref> ns when we do not exactly know the number of distinct person for an ambiguous name. Some researches <ref type=""bibr"" target=""#b4"">[5]</ref>, <ref type=""bibr"" target=""#b10"">[11]</ref> assume that the c that may contain potentially ambiguous names. Many methods <ref type=""bibr"" target=""#b2"">[3]</ref>, <ref type=""bibr"" target=""#b4"">[5]</ref>, <ref type=""bibr"" target=""#b5"">[6]</ref>, <ref type=""bibr"" t wed data and is widely in many name disambiguation methods <ref type=""bibr"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b4"">[5]</ref>, <ref type=""bibr"" target=""#b8"">[9]</ref>, <ref type=""bibr"" t is determined adaptively based on the results of HDBSCAN and AP clustering algorithm. Zhang et al. <ref type=""bibr"" target=""#b4"">[5]</ref>: This method constructs three different networks on relation in their own paper. These comparison methods use different kinds of cluster strategy. For example, <ref type=""bibr"" target=""#b4"">[5]</ref> need to specify the number of distinct author, <ref type=""bi our HGCN based heterogenous network embedding method. Among the compared methods, both Zhang et al. <ref type=""bibr"" target=""#b4"">[5]</ref> and Xu et al. construct several graphs based on various rela use network embedding based methods to learn representations of publications. However, Zhang et al. <ref type=""bibr"" target=""#b4"">[5]</ref> ignore the text information, and Xu et al. just use the word use network embedding methods to learn publication embeddings by constructing publication networks. <ref type=""bibr"" target=""#b4"">[5]</ref> utilize a network representation learning based approach on",0
"e in some digital libraries, especially the affiliation information usually has the synonym problem <ref type=""bibr"" target=""#b13"">[14]</ref> and hard to be cleaned, so in this paper, we only use the e HAC as the clustering method of <ref type=""bibr"" target=""#b3"">[4]</ref>. We use pairwise F1-score <ref type=""bibr"" target=""#b13"">[14]</ref> to evaluate the clustering results of our method and the c",0
"</ref> use random walk strategy on network and skip-gram <ref type=""bibr"" target=""#b29"">[30]</ref>, <ref type=""bibr"" target=""#b30"">[31]</ref> model to learn the representation of each node in network.",0
"=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b4"">[5]</ref>, <ref type=""bibr"" target=""#b8"">[9]</ref>, <ref type=""bibr"" target=""#b19"">[20]</ref>, <ref type=""bibr"" target=""#b20"">[21]</ref>. However, the H",0
"iased by highly visible relation types and concentrated nodes, and generate incorporated node paths <ref type=""bibr"" target=""#b31"">[32]</ref>. Metapath2Vec <ref type=""bibr"" target=""#b16"">[17]</ref> pr",0
"node in PHNet to a high quality representation. Inspired by the network embedding methods DeepWalk <ref type=""bibr"" target=""#b15"">[16]</ref> and Metapath2Vec <ref type=""bibr"" target=""#b16"">[17]</ref> networks with same construction as PHNet, and use HAC to generate the clustering results: DeepWalk <ref type=""bibr"" target=""#b15"">[16]</ref>: DeepWalk is a network embedding method based on random wa mbedding. Recently, there has been a growing interest in the network embedding technology. DeepWalk <ref type=""bibr"" target=""#b15"">[16]</ref> and Node2Vec <ref type=""bibr"" target=""#b28"">[29]</ref> use",0
"biguation methods <ref type=""bibr"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b4"">[5]</ref>, <ref type=""bibr"" target=""#b8"">[9]</ref>, <ref type=""bibr"" target=""#b19"">[20]</ref>, <ref type=""bibr"" the representation of publications, but it requires lots of human labeled data to train the model. <ref type=""bibr"" target=""#b8"">[9]</ref> propose a hierarchical agglomerative clustering based approa",0
"ode2Vec <ref type=""bibr"" target=""#b28"">[29]</ref> use random walk strategy on network and skip-gram <ref type=""bibr"" target=""#b29"">[30]</ref>, <ref type=""bibr"" target=""#b30"">[31]</ref> model to learn",0
"od of a node and then leverages a heterogeneous skip-gram model to perform node embeddings. Hin2Vec <ref type=""bibr"" target=""#b23"">[24]</ref>: Hin2Vec is also an unweighted heterogeneous network embed",0
"parameters are independent with the size of publication set. Besides, we use the alias table method <ref type=""bibr"" target=""#b18"">[19]</ref> to sample neighbors and negative nodes, which only takes O",0
"l features u (0) i by HGCN in eq.2. Then, we adopt the popular negative sampling method proposed in <ref type=""bibr"" target=""#b17"">[18]</ref> to sample negative nodes to increase the optimization effi",0
"#b4"">[5]</ref>, <ref type=""bibr"" target=""#b8"">[9]</ref>, <ref type=""bibr"" target=""#b19"">[20]</ref>, <ref type=""bibr"" target=""#b20"">[21]</ref>. However, the HAC has the following drawbacks: its time co and use a network embedding algorithm to learn representations of publications via their neighbors, <ref type=""bibr"" target=""#b20"">[21]</ref> introduce a simple random walk strategy and a graph embedd",0
"ed for online speech recognition. There are some works trying to solve this problem. Tjandra et al. <ref type=""bibr"" target=""#b6"">[7]</ref> propose a local monotonic attention mechanism that forces th",1
"obabilities. But complex and tricky training methods make it hard to implement. Triggered attention <ref type=""bibr"" target=""#b8"">[9]</ref> utilizes the spikes produced by connectionist temporal class",1
"predict the next central position just based on limited information. Monotonic chunkwise attention <ref type=""bibr"" target=""#b7"">[8]</ref> is proposed to adaptively split the encoded state sequence i",1
"e=""foot"" target=""#foot_1"">2</ref> for data preparation. And our Sync-Transformer is built on ESPNet <ref type=""bibr"" target=""#b17"">[18]</ref> and warp-rnnt <ref type=""foot"" target=""#foot_2"">3</ref> . mer with other end-toend models. The transformer model is trained according to the recipe in ESPnet <ref type=""bibr"" target=""#b17"">[18]</ref>, which has the same settings as our Sync-Transformer. The",0
"DUCTION</head><p>Attention-based sequence-to-sequence models <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target= pe=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b5"">6]</ref>, especially transformer model <ref type=""bibr"" target=""#b1"">[2]</ref>, have shown great success in various tasks, e.g. neural mach > <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2.1."">Model</head><p>Similar to the transformer <ref type=""bibr"" target=""#b1"">[2]</ref>, a Sync-Transformer consists of an encoder and a decoder, as e dimensions from 40 to 256), time-axis down-sampling and adding sine-cosine positional information <ref type=""bibr"" target=""#b1"">[2]</ref>. Let x 1:T be the input feature sequence, the processed sequ ore, we adopt an Adam optimizer with warmup steps 25000 and the learning rate scheduler reported in <ref type=""bibr"" target=""#b1"">[2]</ref>.</p><p>During decoding, we use a beam search with a width of at success in various tasks, e.g. neural machine translation <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref>, image captioning <ref type=""bibr"" target=""#b2"">[3]</ref> and >1</ref>(a). Both encoder and decoder are composed of multi-head attentions and feed-forward layers <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target",0
":U |x 1,T ) by enumerating all possible alignment paths. Therefore, like transducer-based models in <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b9"">10]</ref>, we introduce a for og-loss function L = −lnp(y 1:U |x 1,T ). The calculation of gradients is exactly the same as RNN-T <ref type=""bibr"" target=""#b15"">[16]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head",0
"re conducted on a public Mandarin speech corpus AISHELL-1<ref type=""foot"" target=""#foot_0"">1</ref>  <ref type=""bibr"" target=""#b16"">[17]</ref>. The training set contains about 150 hours of speech (120,",0
"ww.tei-c.org/ns/1.0""><head n=""1."">INTRODUCTION</head><p>Attention-based sequence-to-sequence models <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target target=""#b1"">[2]</ref>, have shown great success in various tasks, e.g. neural machine translation <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref>, image captioning <ref",0
"of the encoder can still model long-term dependencies. On the other hand, similar to transformer-xl <ref type=""bibr"" target=""#b14"">[15]</ref>, the encoder of Sync-Transformer processes the input seque",0
"Neural Transducer <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b12"">13]</ref>, the decoder generates the output sequence chunk by chunk.",0
"ible alignment paths and calculate the negative log loss function as the same as the RNN-Transducer <ref type=""bibr"" target=""#b13"">[14]</ref> and SA-T <ref type=""bibr"" target=""#b9"">[10]</ref>. We eval",0
"r"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b5"">6]</ref>, especially transformer ef type=""bibr"" target=""#b2"">[3]</ref> and speech recognition <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b5"">6]</ref>.</p><p>For conventional",0
"ync-Transformer can achieve a comparable result with the best transformer, which is better than LAS <ref type=""bibr"" target=""#b19"">[20]</ref>, RNN-T and our previous (Chunk-Flow) SA-T <ref type=""bibr""",0
""" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target= get=""#b1"">2]</ref>, image captioning <ref type=""bibr"" target=""#b2"">[3]</ref> and speech recognition <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target",0
"dict symbols immediately. Similar to the Neural Transducer <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b12"">13]</ref>, the decoder genera",0
"application-agnostic and is implemented in the Customizable Streaming Partitioner (CuSP) framework <ref type=""bibr"" target=""#b35"">[36]</ref>. The experimental results in this paper are based on the p lemented the proposed graph partitioning policy using the Customizable Streaming Partitioner (CuSP) <ref type=""bibr"" target=""#b35"">[36]</ref> framework. We modified CuSP to create edge proxies in addi",1
"ter performance on GPUs due to coalesced memory accesses <ref type=""bibr"" target=""#b14"">[15]</ref>, <ref type=""bibr"" target=""#b15"">[16]</ref>.</p><p>Triangle counting on single GPU: Green et al. <ref intersection approach, and a matrix formulation based on sparse matrix-matrix multiplies. Hu et al. <ref type=""bibr"" target=""#b15"">[16]</ref> presented a distributed implementation of triangle countin of memory bandwidth.</p><p>Our implementation leverages the binary search based intersection method <ref type=""bibr"" target=""#b15"">[16]</ref> to improve performance on a single host which in turn impr PU memory to the GPU memory on the fly. It uses binary search to increase coalesced memory accesses <ref type=""bibr"" target=""#b15"">[16]</ref> and employs load balancing by dynamically assigning indepe",0
"gree and distribute them across hosts in a round-robin fashion. Similarly, several other techniques <ref type=""bibr"" target=""#b21"">[22]</ref>, <ref type=""bibr"" target=""#b22"">[23]</ref>, <ref type=""bib",0
"e briefly discuss this prior work below.</p><p>Triangle counting on shared memory CPUs: Shun et al. <ref type=""bibr"" target=""#b13"">[14]</ref> detail a cache-oblivious parallel triangle counting on sha",0
"gle counting. Most implementations are either sequential or shared-memory multicore implementations <ref type=""bibr"" target=""#b3"">[4]</ref>, <ref type=""bibr"" target=""#b4"">[5]</ref> so they cannot deal",0
"allenge to process massive remote sensing images. In our work, two lightweight attention mechanisms <ref type=""bibr"" target=""#b16"">[17]</ref> which contains spatial attention and channel attention are ts a convolution operation with 7 × 7 kernel size.</p><p>In this study, we follow the method of Woo <ref type=""bibr"" target=""#b16"">[17]</ref>to integrate the two attention mechanisms. First, we use ch",1
"ract features and then use clustering, classification, and threshold algorithms to segment an image <ref type=""bibr"" target=""#b0"">[1]</ref> <ref type=""bibr"" target=""#b1"">[2]</ref>. However, these meth",0
"d><p>In this study, we use two representative backbones for feature extraction: SegNet and ResNet50 <ref type=""bibr"" target=""#b17"">[18]</ref>. On this basis, we proposed two networks: SCAttNet V1 with",0
"lassification, and threshold algorithms to segment an image <ref type=""bibr"" target=""#b0"">[1]</ref> <ref type=""bibr"" target=""#b1"">[2]</ref>. However, these methods depend heavily on artificial design",0
"y natural image datasets, such as PASCAL VOC <ref type=""bibr"" target=""#b5"">[6]</ref> and Cityscapes <ref type=""bibr"" target=""#b6"">[7]</ref>.</p><p>However, remote sensing images are different from nat",0
"ired target span inside the trigger. For language modeling, triggers are prefixes that prompt GPT-2 <ref type=""bibr"" target=""#b24"">(Radford et al., 2019)</ref> to generate racist outputs, even when co x the attack to targets of similar content. particular, our trigger causes the GPT-2 language model <ref type=""bibr"" target=""#b24"">(Radford et al., 2019)</ref> to output racist content. We maximize th",1
"hts into global model behavior.</p><p>Triggers are a new form of universal adversarial perturbation <ref type=""bibr"" target=""#b18"">(Moosavi-Dezfooli et al., 2017)</ref> adapted to discrete textual inp or anyone to fool machine learning models. Moreover, universal attacks often transfer across models <ref type=""bibr"" target=""#b18"">(Moosavi-Dezfooli et al., 2017)</ref>, which further decreases attack e adversarial threat is higher if an attack is universal: using the exact same attack for any input <ref type=""bibr"" target=""#b18"">(Moosavi-Dezfooli et al., 2017;</ref><ref type=""bibr"" target=""#b4"">Br",1
"exact same attack for any input <ref type=""bibr"" target=""#b18"">(Moosavi-Dezfooli et al., 2017;</ref><ref type=""bibr"" target=""#b4"">Brown et al., 2017)</ref>. Universal attacks are advantageous as (1) n",0
"tions. For example, adversarially-modified inputs are used to evaluate reading comprehension models <ref type=""bibr"" target=""#b13"">(Jia and Liang, 2017;</ref><ref type=""bibr"" target=""#b26"">Ribeiro et ain type. We focus on why, who, when, and where questions. We use sentences of length ten following <ref type=""bibr"" target=""#b13"">Jia and Liang (2017)</ref> and sum the cross-entropy of the start and SQuAD; adversarial evaluation in-stead highlights erroneous model behaviors on a per-example basis <ref type=""bibr"" target=""#b13"">(Jia and Liang, 2017)</ref>. Here, we analyze the SQuAD triggers to s",0
"e system vulnerabilities, e.g., a spammer may use adversarial attacks to bypass a spam email filter <ref type=""bibr"" target=""#b2"">(Biggio et al., 2013)</ref>. These security concerns grow as natural l",0
"ed to any input whereas SEARs is only applicable when one its rule applies.</p><p>In parallel work, <ref type=""bibr"" target=""#b0"">Behjati et al. (2019)</ref> consider universal adversarial attacks on",0
"enerative <ref type=""bibr"" target=""#b12"">(Iyyer et al., 2018)</ref> or human-in-the-loop approaches <ref type=""bibr"" target=""#b32"">(Wallace et al., 2019)</ref>. We turn the reader to <ref type=""bibr""",0
"2017, ESIM)</ref> and Decomposable Attention (Parikh et al., 2016, DA) models with GloVe embeddings <ref type=""bibr"" target=""#b21"">(Pennington et al., 2014)</ref>. We also consider a DA model with ELM iDAF <ref type=""bibr"" target=""#b28"">(Seo et al., 2017)</ref>; we lowercase all inputs and use GloVe <ref type=""bibr"" target=""#b21"">(Pennington et al., 2014)</ref>. We pick the target answers ""to kill",0
"s</head><p>The construction of NLP datasets can lead to dataset biases or ""artifacts"". For example, <ref type=""bibr"" target=""#b9"">Gururangan et al. (2018)</ref> and <ref type=""bibr"" target=""#b23"">Poli d the labels in SNLI. We investigate whether triggers are caused by such artifacts.</p><p>Following <ref type=""bibr"" target=""#b9"">Gururangan et al. (2018)</ref>, we identify dataset artifacts by ranki",0
"wise approach.</p><p>• Combining the proposed model with densityweighted Expected Loss Optimization <ref type=""bibr"" target=""#b16"">[17]</ref>, we introduce active learning into POLAR <ref type=""bibr"" ediction the parameters under the posterior disagree about are selected. Expected Loss Optimization <ref type=""bibr"" target=""#b16"">[17]</ref> selects the instance that maximizes the expected loss base ected Loss Optimization</head><p>The active learning metric we choose is Expected Loss Optimization <ref type=""bibr"" target=""#b16"">[17]</ref>. The basic idea is to choose the instance that maximizes t",1
"pwords like of and on. Therefore, a better mechanism for local weights is needed.</p><p>Inspired by <ref type=""bibr"" target=""#b59"">[60]</ref>, we propose a local weight network based on distributed wo",1
"Bayesian active learning algorithm for deep learning in image data is proposed based on the idea in <ref type=""bibr"" target=""#b41"">[42]</ref>.</p><p>Most works that apply active learning to recommende basic equation of Variational Inference <ref type=""bibr"" target=""#b51"">[52]</ref>.</p><p>Following <ref type=""bibr"" target=""#b41"">[42]</ref>, we use the distribution of the network parameter with dro nsidered as the smoothed version of hinge loss.</p><p>As for the second term in (2), it's proved in <ref type=""bibr"" target=""#b41"">[42]</ref> that it can be approximated by L2 regularization term</p><",1
"bibr"" target=""#b44"">[45]</ref>. The active learning method is also called the Ask-To-Rate technique <ref type=""bibr"" target=""#b45"">[46]</ref>. A comprehensive survey can be found in <ref type=""bibr"" t",0
"32]</ref>, stream-based sampling <ref type=""bibr"" target=""#b32"">[33]</ref>, and pool-based sampling <ref type=""bibr"" target=""#b33"">[34]</ref>. One of the most common frameworks for active learning is et=""#b33"">[34]</ref>. One of the most common frameworks for active learning is Uncertainty Sampling <ref type=""bibr"" target=""#b33"">[34]</ref>, where the active learner selects the instance for which t for the user.</p><p>Moving from this, we define the new problem in the pool-based sampling setting <ref type=""bibr"" target=""#b33"">[34]</ref> of active learning : Definition 2 (Active One-shot Article",0
"8"">[9]</ref>, <ref type=""bibr"" target=""#b18"">[19]</ref>, <ref type=""bibr"" target=""#b19"">[20]</ref>, <ref type=""bibr"" target=""#b20"">[21]</ref>. Most approaches use plain words as features, although som n-grams <ref type=""bibr"" target=""#b19"">[20]</ref>, topics <ref type=""bibr"" target=""#b8"">[9]</ref>, <ref type=""bibr"" target=""#b20"">[21]</ref>, and citations <ref type=""bibr"" target=""#b17"">[18]</ref>.",0
"hot deep learning <ref type=""bibr"" target=""#b5"">[6]</ref>, <ref type=""bibr"" target=""#b6"">[7]</ref>, <ref type=""bibr"" target=""#b7"">[8]</ref>, we propose to learn a one-shot deep matching metric for per onal layers used before the fully-connected layers and the top-level energy function. Matching Nets <ref type=""bibr"" target=""#b7"">[8]</ref> take as input not only the new sample but also a small suppo gy between oneshot learning and our problem because S is of minimal size or even empty. Inspired by <ref type=""bibr"" target=""#b7"">[8]</ref>, our model computes f ω (d q , S, d i ) as:</p><formula xml:",0
"]</ref> selects the instance that maximizes the expected loss based on Bayesian decision theory. In <ref type=""bibr"" target=""#b40"">[41]</ref> a Bayesian active learning algorithm for deep learning in",0
"two articles <ref type=""bibr"" target=""#b10"">[11]</ref>, <ref type=""bibr"" target=""#b12"">[13]</ref>, <ref type=""bibr"" target=""#b26"">[27]</ref>. However, these models cannot identify the specific matchi",0
"ning</head><p>We formulate the training set in the setting of Pairwise Approach of Learning to Rank <ref type=""bibr"" target=""#b60"">[61]</ref>. It means that D train = {(d q and support set S (i) . We",0
"recommender systems are based on collaborative filtering <ref type=""bibr"" target=""#b42"">[43]</ref>, <ref type=""bibr"" target=""#b43"">[44]</ref>, <ref type=""bibr"" target=""#b44"">[45]</ref>. The active lea ef type=""bibr"" target=""#b48"">[49]</ref>, <ref type=""bibr"" target=""#b49"">[50]</ref>. For example, in <ref type=""bibr"" target=""#b43"">[44]</ref> a decision tree is built to model the Ask-To-Rate process",0
"type=""bibr"" target=""#b41"">[42]</ref>, we use the distribution of the network parameter with dropout <ref type=""bibr"" target=""#b52"">[53]</ref> as q(ω). Consider a neural network with only one layer, wh",0
"ology, Tsinghua University. E-mail: jietang@tsinghua.edu.cn. Jie Tang is the corresponding author.  <ref type=""bibr"" target=""#b3"">[4]</ref> or Collaborative Filtering <ref type=""bibr"" target=""#b4"">[5] as part of the CiteSeer project <ref type=""bibr"" target=""#b17"">[18]</ref>. Content-based filtering <ref type=""bibr"" target=""#b3"">[4]</ref> is one of the most widely used and researched recommendation",0
""">[12]</ref>, <ref type=""bibr"" target=""#b12"">[13]</ref>, <ref type=""bibr"" target=""#b13"">[14]</ref>, <ref type=""bibr"" target=""#b14"">[15]</ref>, but they often treat all the words in an article indiscri action of two articles, usually based on word embeddings <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" target=""#b14"">[15]</ref>, <ref type=""bibr"" target=""#b27"">[28]</ref>. These models l",0
"ion retrieval <ref type=""bibr"" target=""#b10"">[11]</ref>, <ref type=""bibr"" target=""#b24"">[25]</ref>, <ref type=""bibr"" target=""#b25"">[26]</ref> and semantic matching <ref type=""bibr"" target=""#b11"">[12]< ]</ref>. Traditional methods for measuring the similarity between two pieces of texts, such as BM25 <ref type=""bibr"" target=""#b25"">[26]</ref> and TF-IDF <ref type=""bibr"" target=""#b24"">[25]</ref>, are cument, such as the term frequency (TF) in TF-IDF <ref type=""bibr"" target=""#b24"">[25]</ref> or BM25 <ref type=""bibr"" target=""#b25"">[26]</ref> ranking function:</p><formula xml:id=""formula_11"">BM25(d,",0
"g future lightning occurrences. However, although extrapolationbased methods for weather nowcasting <ref type=""bibr"" target=""#b0"">[1]</ref>- <ref type=""bibr"" target=""#b2"">[3]</ref> can be migrated to re sensitive to different dimensions are assembled to predict mobile events in the city. Shi et al. <ref type=""bibr"" target=""#b0"">[1]</ref> proposed convolutional LSTM (ConvLSTM) for precipitation now t−1 .</formula><p>The ConvLSTM in this paper does not include peephole connections, as mentioned in <ref type=""bibr"" target=""#b0"">[1]</ref>. The data first enters the CNN modules, where sequentially a",1
"ting. As a landmark structure, ConvLSTM is used as a base module in many subsequent works. In MCnet <ref type=""bibr"" target=""#b31"">[32]</ref>, a CNN-ConvLSTM motion encoder and a CNN content encoder c",0
"re facilitates sequence-to-sequence (Seq2Seq) conversion between data series with different lengths <ref type=""bibr"" target=""#b8"">[9]</ref>- <ref type=""bibr"" target=""#b10"">[11]</ref>. Since both recen",0
"th conditions such as season, terrain, stage of thunderstorm development and type of weather system <ref type=""bibr"" target=""#b13"">[14]</ref>, <ref type=""bibr"" target=""#b15"">[16]</ref>- <ref type=""bib",0
"d to some of them <ref type=""bibr"" target=""#b4"">[5]</ref>, <ref type=""bibr"" target=""#b6"">[7]</ref>, <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" target=""#b12"">[13]</ref>. The more compl grees. Specifically, temperature layers between −10 • C to −20 • C are considered the most relevant <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" target=""#b18"">[19]</ref>, <ref type=""bib",0
"ections, background, and reflection images, respectively. Here, α and β are the mixing coefficients <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" targ generation results and clearer separation results. Moreover, we introduce the gradient constraints <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b25"">26]</ref> to make the model le ><head n=""4.1."">Framework of the Proposed Scheme</head><p>In contrast to the conventional pipelines <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b28"">29,</ref><ref type=""bibr"" targ ird columns in Figure <ref type=""figure"" target=""#fig_3"">4</ref> 1 ) than previous linear functions <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" targ n, and the background edge map (E) concurrently. Instead of one-toone framework in previous methods <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b28"">29]</ref>, our separator learn . Recently, deep learning based reflection removal methods <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b4"">5]</ref> with better generalization ability have been proposed to addr h previous methods <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b4"">5]</ref>, that heavily rely on the simplified model in Equation 1 and learn the edge features of the reflections with the light field camera. The framework introduced in <ref type=""bibr"" target=""#b4"">[5]</ref> exploited the edge information when training the whole netwo and reflection, and three discriminator networks to produce the adversarial losses. Existing method <ref type=""bibr"" target=""#b4"">[5]</ref> can be treated as a special instance of our method when the type=""bibr"" target=""#b28"">[29]</ref>, CycleGAN <ref type=""bibr"" target=""#b29"">[30]</ref>, and FY17 <ref type=""bibr"" target=""#b4"">[5]</ref>. The yellow boxes highlight some noticeable differences.</p> 5"">[26]</ref> 0.833 0.877 22.39 NR17 <ref type=""bibr"" target=""#b0"">[1]</ref> 0.832 0.882 23.70 FY17 <ref type=""bibr"" target=""#b4"">[5]</ref> 0.820 0.871 22.51 CycleGAN <ref type=""bibr"" target=""#b29"">[3 <ref type=""bibr"" target=""#b28"">[29]</ref>, CycleGAN <ref type=""bibr"" target=""#b29"">[30]</ref>, FY17 <ref type=""bibr"" target=""#b4"">[5]</ref>, NR17 <ref type=""bibr"" target=""#b0"">[1]</ref>, WS16 <ref typ able"" xml:id=""tab_3""><head>Table 3 .</head><label>3</label><figDesc>Efficiency comparisons with FY17<ref type=""bibr"" target=""#b4"">[5]</ref>, Zhang18<ref type=""bibr"" target=""#b28"">[29]</ref> and Wan18<",1
"mixing coefficients <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" target=""#b25"">26]</ref>.</p><formula xml:id=""formula_0"">M = αB + βR,<label>(1)</lab ion results. Moreover, we introduce the gradient constraints <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b25"">26]</ref> to make the model learning more effective, in which the edg In this scenario, image priors such as different blur levels between the background and reflection <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b16"">17]</ref>, ghosting effects "" target=""#fig_3"">4</ref> 1 ) than previous linear functions <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" tar <p>SSIM r SSIM PSNR(dB)</p><p>LB14 <ref type=""bibr"" target=""#b16"">[17]</ref> 0.801 0.829 21.77 WS16 <ref type=""bibr"" target=""#b25"">[26]</ref> 0.833 0.877 22.39 NR17 <ref type=""bibr"" target=""#b0"">[1]</ >, FY17 <ref type=""bibr"" target=""#b4"">[5]</ref>, NR17 <ref type=""bibr"" target=""#b0"">[1]</ref>, WS16 <ref type=""bibr"" target=""#b25"">[26]</ref>, and LB14 <ref type=""bibr"" target=""#b16"">[17]</ref>. For a",1
"prove the robustness. It is worthy to note that traditional cycle-consistent network, like CycleGAN <ref type=""bibr"" target=""#b29"">[30]</ref>, cannot be directly applied to reflection removal, as its in cross domains.</p><p>It is worthy to mention that some existing mature frameworks like CycleGAN <ref type=""bibr"" target=""#b29"">[30]</ref> and DiscoGAN <ref type=""bibr"" target=""#b9"">[10]</ref> are f type=""bibr"" target=""#b23"">[24]</ref>, Zhang18 <ref type=""bibr"" target=""#b28"">[29]</ref>, CycleGAN <ref type=""bibr"" target=""#b29"">[30]</ref>, and FY17 <ref type=""bibr"" target=""#b4"">[5]</ref>. The yel [1]</ref> 0.832 0.882 23.70 FY17 <ref type=""bibr"" target=""#b4"">[5]</ref> 0.820 0.871 22.51 CycleGAN <ref type=""bibr"" target=""#b29"">[30]</ref> 0.794 0.813 20.10 Zhang18 <ref type=""bibr"" target=""#b28"">[ f type=""bibr"" target=""#b23"">[24]</ref>, Zhang18 <ref type=""bibr"" target=""#b28"">[29]</ref>, CycleGAN <ref type=""bibr"" target=""#b29"">[30]</ref>, FY17 <ref type=""bibr"" target=""#b4"">[5]</ref>, NR17 <ref t >. To alleviate the problem of obtaining data pairs, unpaired image-to-image translation frameworks <ref type=""bibr"" target=""#b29"">[30,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" tar ure. Moreover, in contrast with the classical cycle-consistent  model with the one-to-one framework <ref type=""bibr"" target=""#b29"">[30,</ref><ref type=""bibr"" target=""#b9"">10]</ref>, we propose a joint",0
"nslation frameworks <ref type=""bibr"" target=""#b29"">[30,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b17"">18]</ref> have been proposed. UNIT <ref type=""bibr"" target=""#b17"">[18 type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b17"">18]</ref> have been proposed. UNIT <ref type=""bibr"" target=""#b17"">[18]</ref> combines variational autoencoders (VAEs) <ref type=""bibr""",0
"rea remove reflections by virtue of multiple images. By exploiting different image correlation cues <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b5"">6]</ref>, the modelling based m",0
"beyond the straightforward linear combination. For example, either non-uniform lighting conditions <ref type=""bibr"" target=""#b11"">[12]</ref> or the non-flat surface of glass <ref type=""bibr"" target=""",0
"<ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b16"">17]</ref>, ghosting effects <ref type=""bibr"" target=""#b21"">[22]</ref>, and the non-local similarity in the images <ref type=""bib the background and reflection layer <ref type=""bibr"" target=""#b16"">[17]</ref>, the ghosting effects <ref type=""bibr"" target=""#b21"">[22]</ref> and the Laplacian data fidelity term <ref type=""bibr"" targ",0
"trivial to learn this function accurately. Recently, deep learning based reflection removal methods <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b4"">5]</ref> with better generali ndling the mutual effects of two phases in training models.</p><p>In contrast with previous methods <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" ta nventional pipelines <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b28"">29,</ref><ref type=""bibr"" target=""#b23"">24]</ref> that treat the image generation and separation as two indep s in the SIR<ref type=""foot"" target=""#foot_0"">2</ref> dataset. The comparison methods include Wan18 <ref type=""bibr"" target=""#b23"">[24]</ref>, Zhang18 <ref type=""bibr"" target=""#b28"">[29]</ref>, CycleG ]</ref> 0.794 0.813 20.10 Zhang18 <ref type=""bibr"" target=""#b28"">[29]</ref> 0.842 0.885 24.01 Wan18 <ref type=""bibr"" target=""#b23"">[24]</ref> 0.854 0.891 24.08 Eq. ( <ref type=""formula"" target=""#formu SIR 2 dataset for the three best methods (Zhang18 <ref type=""bibr"" target=""#b28"">[29]</ref>, Wan18 <ref type=""bibr"" target=""#b23"">[24]</ref>, and ours) in terms of the quantitative scores in Table <r hod is compared with seven state-ofthe-art single image reflection removal methods, including Wan18 <ref type=""bibr"" target=""#b23"">[24]</ref>, Zhang18 <ref type=""bibr"" target=""#b28"">[29]</ref>, CycleG le <ref type=""table"" target=""#tab_3"">3</ref>. In particular, the SSIM-guided loss proposed by Wan18 <ref type=""bibr"" target=""#b23"">[24]</ref> performs well while our method is much more efficient (15 7<ref type=""bibr"" target=""#b4"">[5]</ref>, Zhang18<ref type=""bibr"" target=""#b28"">[29]</ref> and Wan18<ref type=""bibr"" target=""#b23"">[24]</ref> of an image with size 224 × 288 on a single Titan XP GPU.<",0
"the ghosting effects <ref type=""bibr"" target=""#b21"">[22]</ref> and the Laplacian data fidelity term <ref type=""bibr"" target=""#b0"">[1]</ref>. Other methods in this area remove reflections by virtue of >[17]</ref> 0.801 0.829 21.77 WS16 <ref type=""bibr"" target=""#b25"">[26]</ref> 0.833 0.877 22.39 NR17 <ref type=""bibr"" target=""#b0"">[1]</ref> 0.832 0.882 23.70 FY17 <ref type=""bibr"" target=""#b4"">[5]</re leGAN <ref type=""bibr"" target=""#b29"">[30]</ref>, FY17 <ref type=""bibr"" target=""#b4"">[5]</ref>, NR17 <ref type=""bibr"" target=""#b0"">[1]</ref>, WS16 <ref type=""bibr"" target=""#b25"">[26]</ref>, and LB14 <r rget=""#b4"">[5,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b0"">1]</ref> with fixed coefficients.</p><p>Separator (S). We perform a di",0
"rategy</head><p>The model is implemented with PyTorch 2 . To inject scale-invariance to the network <ref type=""bibr"" target=""#b20"">[21]</ref>, we adopt a multi-size training strategy by feeding images egional properties of the reflection <ref type=""bibr"" target=""#b22"">[23]</ref>, we also adopt SSIMr <ref type=""bibr"" target=""#b20"">[21]</ref> to assess the quality by focusing on local reflections.  < ions and evaluate the SSIM values in these regions analogously to the evaluation method proposed in <ref type=""bibr"" target=""#b20"">[21]</ref>. As a result, higher SSIM r results have been obtained as",0
"ectures, e.g., Siamese networks <ref type=""bibr"" target=""#b9"">(He et al., 2016)</ref> and attention <ref type=""bibr"" target=""#b26"">(Seo et al., 2017;</ref><ref type=""bibr"" target=""#b31"">Tay et al., 20 her than specific term matches. Context-aware representation learning, such as co-attention methods <ref type=""bibr"" target=""#b26"">(Seo et al., 2017)</ref>, has been proved effective in many benchmark ng elements in the missing dimen-sions. Softmax col is the column-wise softmax operator. Similar to <ref type=""bibr"" target=""#b26"">Seo et al. (2017)</ref>, we perform co-attention from two directions: ; (2) interaction and attention mecha-nisms <ref type=""bibr"" target=""#b31"">(Tay et al., 2019b;</ref><ref type=""bibr"" target=""#b26"">Seo et al., 2017;</ref><ref type=""bibr"" target=""#b21"">Parikh et al.,",1
"ral architectures, such as DRMM <ref type=""bibr"" target=""#b8"">(Guo et al., 2016)</ref> and Co-PACRR <ref type=""bibr"" target=""#b13"">(Hui et al., 2018)</ref>, adopt an interaction-based design. They ope",1
"esults on each dataset from published literature. We also include the current state-of-the-art BERT <ref type=""bibr"" target=""#b6"">(Devlin et al., 2019)</ref> results on each dataset.</p><p>For the twe",0
"ibr"" target=""#b8"">(Guo et al., 2016)</ref>, <ref type=""bibr"">DUET (Mitra et al., 2017)</ref>, K-NRM <ref type=""bibr"" target=""#b36"">(Xiong et al., 2017b)</ref>, and PACRR <ref type=""bibr"" target=""#b12"" m gating network aggregates weighted matching signals from different query terms. Inspired by DRMM, <ref type=""bibr"" target=""#b36"">Xiong et al. (2017b)</ref> proposed K-NRM, which introduced a differe",0
"er two sentences or phrases convey the same meaning. In question answering or reading comprehension <ref type=""bibr"" target=""#b35"">(Xiong et al., 2017a;</ref><ref type=""bibr"" target=""#b30"">Tay et al.,",0
"is to rank candidate answer sentences based on their similarity to the question. We use the TrecQA <ref type=""bibr"" target=""#b33"">(Wang et al., 2007)</ref> dataset (raw version)<ref type=""foot"" targe",0
"r"" target=""#b9"">(He and Lin, 2016;</ref><ref type=""bibr"" target=""#b29"">Sutskever et al., 2014;</ref><ref type=""bibr"" target=""#b39"">Yin et al., 2016;</ref><ref type=""bibr"" target=""#b24"">Rao et al., 201",0
"r"" target=""#b29"">Sutskever et al., 2014;</ref><ref type=""bibr"" target=""#b39"">Yin et al., 2016;</ref><ref type=""bibr"" target=""#b24"">Rao et al., 2017b)</ref>. Current neural models for IR can be divided",0
"at existing approaches for textual similarity modeling in NLP can produce poor results for IR tasks <ref type=""bibr"" target=""#b8"">(Guo et al., 2016)</ref>, and vice versa <ref type=""bibr"" target=""#b10 relevance matching is fundamentally a matching task, most recent neural architectures, such as DRMM <ref type=""bibr"" target=""#b8"">(Guo et al., 2016)</ref> and Co-PACRR <ref type=""bibr"" target=""#b13"">( ntext.</p><p>It's worth noting that term importance modeling can be important for some search tasks <ref type=""bibr"" target=""#b8"">(Guo et al., 2016)</ref>; therefore, we inject external weights as pri eel et al., 2004)</ref>, learning to rank (L2R), as well as a number of neural ranking models: DRMM <ref type=""bibr"" target=""#b8"">(Guo et al., 2016)</ref>, <ref type=""bibr"">DUET (Mitra et al., 2017)</ uery and the document, often with countbased techniques to address data sparsity. For example, DRMM <ref type=""bibr"" target=""#b8"">(Guo et al., 2016)</ref> introduced a pyramid pooling technique to con /ref>, and vice versa <ref type=""bibr"" target=""#b10"">(Htut et al., 2018)</ref>.</p><p>Specifically, <ref type=""bibr"" target=""#b8"">Guo et al. (2016)</ref> point out three distinguishing characteristics",0
"ra et al., 2017)</ref>, K-NRM <ref type=""bibr"" target=""#b36"">(Xiong et al., 2017b)</ref>, and PACRR <ref type=""bibr"" target=""#b12"">(Hui et al., 2017)</ref>. For the neural baselines, we used implement",0
"GCN model for node classification and showed how performance degrades when using more than 3 layers <ref type=""bibr"" target=""#b17"">[18]</ref>. Pham et al. <ref type=""bibr"" target=""#b25"">[26]</ref> pro ent variants of those two functions. For example, the aggregation function can be a mean aggregator <ref type=""bibr"" target=""#b17"">[18]</ref>, a max-pooling aggregator <ref type=""bibr"" target=""#b26"">[",1
"ar how to properly train deep GCN architectures, where several works have studied their limitations <ref type=""bibr"" target=""#b18"">[19,</ref><ref type=""bibr"" target=""#b42"">43,</ref><ref type=""bibr"" ta ref type=""bibr"" target=""#b52"">53]</ref> is an open problem in the graph learning space. Recent work <ref type=""bibr"" target=""#b18"">[19,</ref><ref type=""bibr"" target=""#b42"">43,</ref><ref type=""bibr"" ta causes oversmoothing, eventually leading to features of graph vertices converging to the same value <ref type=""bibr"" target=""#b18"">[19]</ref>. Due to these limitations, most state-of-the-art GCNs are is limited to a small number of layers <ref type=""bibr"" target=""#b5"">(6)</ref>. Recently, Li et al. <ref type=""bibr"" target=""#b18"">[19]</ref> studied the depth limitations of GCNs and showed that deep",1
"wed peak performance with 10 layers with the performance degrading for deeper graphs. Rahimi et al. <ref type=""bibr"" target=""#b30"">[31]</ref> developed a Highway GCN for user geo-location in social me",1
""">40]</ref>, enhance predictions of recommendation engines <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b49"">50]</ref>, efficiently segment large point clouds <ref type=""bibr"" ta Another popular use of graphs is in recommendation engines <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b49"">50]</ref>, where accurate modelling of user interactions leads to imp",0
"target=""#b34"">[35,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b21"">22]</ref>, or by voxelization <ref type=""bibr"" target=""#b4"">[5,</ref>",0
"rget=""#b26"">[27,</ref><ref type=""bibr"" target=""#b28"">29,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b48"">49]</ref>. The recent EdgeCon "">64,</ref><ref type=""bibr"">128)</ref>, and number of layers <ref type=""bibr"" target=""#b6"">(7,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"">56)",0
"=""bibr"" target=""#b17"">[18]</ref>, a max-pooling aggregator <ref type=""bibr"" target=""#b26"">[27,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b41"">42]</ref>, an attention aggreg ator <ref type=""bibr"" target=""#b24"">[25]</ref>. The update function can be a multi-layer perceptron <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b6"">7]</ref>, a gated network <ref eneral GCNs, we perform additional experiments. In particular, we build ResGCNs based on Graph-SAGE <ref type=""bibr"" target=""#b9"">[10]</ref>, Graph Isomorphism Network (GIN) <ref type=""bibr"" target=""# (d) (v l )} , h v l+1 = h res v l+1 + h v l .<label>(5)</label></formula><p>ResGraphSAGE. GraphSAGE <ref type=""bibr"" target=""#b9"">[10]</ref> proposes different types of aggregator functions including",0
"et=""#b29"">[30,</ref><ref type=""bibr"" target=""#b43"">44,</ref><ref type=""bibr"" target=""#b47"">48,</ref><ref type=""bibr"" target=""#b19"">20]</ref>. Scene graphs also facilitate the inverse process, where an",0
"ith these gates, the authors demonstrate performance degradation after 6 layers of depth. Xu et al. <ref type=""bibr"" target=""#b45"">[46]</ref> developed a Jump Knowledge Network for representation lear",0
"ation. Recent work <ref type=""bibr"" target=""#b33"">[34,</ref><ref type=""bibr"" target=""#b41"">42,</ref><ref type=""bibr"" target=""#b37"">38]</ref> demonstrates that dynamic graph convolution, where the grap In order to learn to generate point clouds, Graph-Convolution GAN (Generative Adversarial Network) <ref type=""bibr"" target=""#b37"">[38]</ref> also applies k-NN graphs to construct the neighbourhood of",0
"ctures, where several works have studied their limitations <ref type=""bibr"" target=""#b18"">[19,</ref><ref type=""bibr"" target=""#b42"">43,</ref><ref type=""bibr"" target=""#b52"">53]</ref>. Stacking more laye s an open problem in the graph learning space. Recent work <ref type=""bibr"" target=""#b18"">[19,</ref><ref type=""bibr"" target=""#b42"">43,</ref><ref type=""bibr"" target=""#b52"">53]</ref> suggests that GCNs s in features at vertices within each connected component converging to the same value. Other works <ref type=""bibr"" target=""#b42"">[43,</ref><ref type=""bibr"" target=""#b52"">53]</ref> also show the limi i-c.org/ns/1.0""><head n=""3.2."">Residual Learning for GCNs</head><p>Designing deep GCN architectures <ref type=""bibr"" target=""#b42"">[43,</ref><ref type=""bibr"" target=""#b52"">53]</ref> is an open problem",0
"sGCNs based on Graph-SAGE <ref type=""bibr"" target=""#b9"">[10]</ref>, Graph Isomorphism Network (GIN) <ref type=""bibr"" target=""#b44"">[45]</ref> and MRGCN (Max-Relative GCN) which is a new GCN operation ion h res v l+1 = h res v l+1 / h res v l+1 2</p><p>.</p><p>ResGIN. The main difference between GIN <ref type=""bibr"" target=""#b44"">[45]</ref> and other GCNs is that an is learned at each GCN layer to ver, it fails to generalize to the test set. This phenomenon is also observed in the original paper <ref type=""bibr"" target=""#b44"">[45]</ref> in which they find setting to 0 can get the best performan",0
"6"">37]</ref>. More recent work focuses on directly processing unordered point cloud representations <ref type=""bibr"" target=""#b26"">[27,</ref><ref type=""bibr"" target=""#b28"">29,</ref><ref type=""bibr"" ta nction can be a mean aggregator <ref type=""bibr"" target=""#b17"">[18]</ref>, a max-pooling aggregator <ref type=""bibr"" target=""#b26"">[27,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" tar with 4096 vertices. The fusion and MLP prediction blocks follow a similar architecture as PointNet <ref type=""bibr"" target=""#b26"">[27]</ref> and DGCNN <ref type=""bibr"" target=""#b41"">[42]</ref>. The f",0
""">[54,</ref><ref type=""bibr"" target=""#b39"">40]</ref>, enhance predictions of recommendation engines <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b49"">50]</ref>, efficiently segme ve substantial impact on drug discovery. Another popular use of graphs is in recommendation engines <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b49"">50]</ref>, where accurate mo",0
"pular modes of representation in natural language processing <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b22"">23]</ref>, where they are used to represent complex relations between",0
"et=""#b30"">[31,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b27"">28]</ref>. Algorithms that do consider object articulations <ref type ct's pose and scale relative to a category-specific canonical representation. Recently, Wang et al. <ref type=""bibr"" target=""#b27"">[28]</ref> extended the object coordinate based approach to perform c re normalized and the orientations are aligned for objects in a given category. Whereas the work by <ref type=""bibr"" target=""#b27"">[28]</ref> focuses on pose and size estimation for rigid objects, the NCSH representation is inspired by and closely related to Normalized Object Coordinate Space (NOCS) <ref type=""bibr"" target=""#b27"">[28]</ref>, which we briefly review here. NOCS is defined as a 3D spa iefly review here. NOCS is defined as a 3D space contained within a unit cube and was introduced in <ref type=""bibr"" target=""#b27"">[28]</ref> to estimate the category-level 6D pose and size of rigid o closed. In addition to normalizing the articulations, NAOCS applies the same normalization used in <ref type=""bibr"" target=""#b27"">[28]</ref> to the objects, including zero-centering, aligning orienta head><p>For each part, NPCS further zero-centers its position and uniformly scales it as is done in <ref type=""bibr"" target=""#b27"">[28]</ref>, while at the same time keeps its orientation unchanged as ime keeps its orientation unchanged as in NAOCS. In this respect, NPCS is defined similarly to NOCS <ref type=""bibr"" target=""#b27"">[28]</ref> but for individual parts instead of whole objects. NPCS pr s {p i ∈ S (j) }, we have their corresponding NPCS predictions {c i |p i ∈ S (j) }. We could follow <ref type=""bibr"" target=""#b27"">[28]</ref> to perform pose fitting, where the Umeyama algorithm <ref j) }, as is commonly done for bundle adjustment <ref type=""bibr"" target=""#b1"">[2]</ref>. Similar to <ref type=""bibr"" target=""#b27"">[28]</ref>, we also use RANSAC for outlier removal.</p><p>Finally, fo , t (j) , s (j) and the NPCS {c i |p i ∈ S (j) } to compute an amodal bounding box, the same as in <ref type=""bibr"" target=""#b27"">[28]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head",1
"end-to-end networks to predict 3D joint locations directly <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b18"">19]</ref>, using dense corres",0
"for rigid objects <ref type=""bibr"" target=""#b30"">[31,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b27"">28]</ref>. Algorithms that do",0
"east-squares solver to further optimize {R (j) , t (j) }, as is commonly done for bundle adjustment <ref type=""bibr"" target=""#b1"">[2]</ref>. Similar to <ref type=""bibr"" target=""#b27"">[28]</ref>, we al",0
"tes onto its CAD model for each observed object pixel, and then use voting to solve for object pose <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b6"">7]</ref>. These approaches are",0
"rget=""#b3"">[4,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b31"">32,</ref><ref type=""bibr"" target=""#b38"">39]</ref>. Since adversarial perturbations are usually bounded by a c get=""#b20"">21,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b37"">38,</ref><ref type=""bibr"" target=""#b38"">39]</ref> focuse on analyzing and improving adversarial machine learn et=""#b20"">[21,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" target=""#b36"">37,</ref><ref type=""bibr"" target=""#b38"">39]</ref>. PGD-k adversarial attack is the multi-step projected gradi ing the literature <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b36"">37,</ref><ref type=""bibr"" target=""#b38"">39]</ref>, we use both MNIST <ref type=""bibr"" target=""#b16"">[17]</ref ull-connected layers which is same architecture as used in <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b38"">39]</ref>. The adversarial perturbation is bounded by l ∞ ball with s ataset, we use the wide residual network  which is same as <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b38"">39]</ref>. The perturbation is bounded by l ∞ ball with size = 0.031. et=""#b20"">[21,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" target=""#b36"">37,</ref><ref type=""bibr"" target=""#b38"">39]</ref>. We set the epoch period to reset perturbation as 10 epochs ccuracy. This trade-off has been observed and explained in <ref type=""bibr"" target=""#b32"">[33,</ref><ref type=""bibr"" target=""#b38"">39]</ref>. A recent work <ref type=""bibr"" target=""#b12"">[13]</ref> po chitecture used in <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b36"">37,</ref><ref type=""bibr"" target=""#b38"">39]</ref>, which has four convolutional layers followed by three full et=""#b20"">[21,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" target=""#b36"">37,</ref><ref type=""bibr"" target=""#b38"">39]</ref>, we use Wide-Resnet- <ref type=""bibr"">34-10 [36]</ref> as t e on Madry's Adversarial Training method (MAT) <ref type=""bibr"" target=""#b20"">[21]</ref> and TRADES <ref type=""bibr"" target=""#b38"">[39]</ref> and evaluate the performance on both MNIST and CIFAR10 dat us attack loss functions are used in different adversarial training algorithms. For example, TRADES <ref type=""bibr"" target=""#b38"">[39]</ref> uses robustness loss (Equation <ref type=""formula"" target= This loss represents how much the adversarial example diverges from the natural image. Zhang et al. <ref type=""bibr"" target=""#b38"">[39]</ref> shows that this loss has a better performance in the TRADE ng methods: Madry's Adversarial Training (MAT) <ref type=""bibr"" target=""#b20"">[21]</ref> and TRADES <ref type=""bibr"" target=""#b38"">[39]</ref>. By evaluating the training time and robustness, we show t rt adversarial training methods as baselines: MAT <ref type=""bibr"" target=""#b20"">[21]</ref>, TRADES <ref type=""bibr"" target=""#b38"">[39]</ref>, YOPO <ref type=""bibr"" target=""#b36"">[37]</ref> and Free < xmlns=""http://www.tei-c.org/ns/1.0""><head>Attack</head><p>Natural PGD-20 Attack loss. Zhang et al. <ref type=""bibr"" target=""#b38"">[39]</ref> show that, for TRADES, using Equation 2 leads to a better e gradient information computed when updating model parameters to generate adversarial examples. In <ref type=""bibr"" target=""#b38"">[39]</ref>, TRADES improves the robustness of an adversarially traine ADES <ref type=""foot"" target=""#foot_4"">5</ref> [39], YOPO<ref type=""foot"" target=""#foot_5"">6</ref>  <ref type=""bibr"" target=""#b38"">[39]</ref>, and Free<ref type=""foot"" target=""#foot_6"">7</ref>  <ref t",1
"e=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b33"">34]</ref>, with adversarial training <ref type=""bibr"" target=""#b20"">[21]</ref> being one of the most effective methods. It formulates tra Typically, using more attack iterations (higher value of k) produces stronger adversarial examples <ref type=""bibr"" target=""#b20"">[21]</ref>. However, each attack iteration needs to compute the gradi ct to traditional PGD-40.</p><p>We apply our technique on Madry's Adversarial Training method (MAT) <ref type=""bibr"" target=""#b20"">[21]</ref> and TRADES <ref type=""bibr"" target=""#b38"">[39]</ref> and e arget=""#b38"">39]</ref> focuse on analyzing and improving adversarial machine learning. Madry et al. <ref type=""bibr"" target=""#b20"">[21]</ref> first formulate adversarial training as a min-max optimiza higher value of k (more attack iterations), PGDk can generate adversarial examples with higher loss <ref type=""bibr"" target=""#b20"">[21]</ref> els. This property is named as transferability. This prope rbations from previous epochs. To compare the attack strength of two attacks, we use Madry's method <ref type=""bibr"" target=""#b20"">[21]</ref> to adversarially train two models on MNIST and CIFAR10 and we integrate ATTA with two popular adversarial training methods: Madry's Adversarial Training (MAT) <ref type=""bibr"" target=""#b20"">[21]</ref> and TRADES <ref type=""bibr"" target=""#b38"">[39]</ref>. By e efficiency</head><p>We select four state-of-the-art adversarial training methods as baselines: MAT <ref type=""bibr"" target=""#b20"">[21]</ref>, TRADES <ref type=""bibr"" target=""#b38"">[39]</ref>, YOPO <r ed in <ref type=""bibr"" target=""#b15"">[16]</ref> and is formulated as a min-max optimization problem <ref type=""bibr"" target=""#b20"">[21]</ref>. As one of the most effective defense methods, lots of wor , are widely adopted in various adversarial training methods <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b31"">32,</ref><ref type=""bibr"" tar rget=""#b1"">[2,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" tar arget=""#b3"">4,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" tar jected back to S, k-step projected gradient descent method <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b20"">21]</ref> (PGDk) has been widely adopted to generate adversarial exam gradient descent <ref type=""bibr"" target=""#b15"">[16]</ref>) is adopted to conduct iterative attack <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" ta div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""5.1"">Setup</head><p>Following the literature <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b36"">37,</ref><ref type=""bibr"" ta convolutional layers followed by three full-connected layers which is same architecture as used in <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b38"">39]</ref>. The adversarial p with size = 0.3.</p><p>For the CIFAR10 dataset, we use the wide residual network  which is same as <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b38"">39]</ref>. The perturbation a new perspective (e.g., improving transferability between epochs). which is same with other works <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" ta re, and hyper-parameters used in this work.</p><p>MNIST. We use the same model architecture used in <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b36"">37,</ref><ref type=""bibr"" ta step size and set decay factor as 1 for M-PGD (momentum PGD).</p><p>CIFAR10. Following other works <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" ta",1
"rget=""#b6"">[7,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b33"">34]</ref>, with adversarial t",0
"ently, lots of works <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" tar",0
"evaluate the model robustness, we perform the PGD <ref type=""bibr"" target=""#b15"">[16]</ref>, M-PGD <ref type=""bibr"" target=""#b7"">[8]</ref> and CW <ref type=""bibr"" target=""#b2"">[3]</ref> attack with a",0
"es achieves considerable robustness. Recently, lots of works <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" tar methods, lots of works <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" tar",0
"θ ,x * ,y) ∂θ 14:</formula><p>x ← inverse aug(x, x * , T aug )</p><p>15:</p><p>end for 16: end for <ref type=""bibr"" target=""#b4"">5</ref> </p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>Eval y of adversarial examples. This property is usually used to perform black-box attack between models <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" targ",0
"with two fast adversarial training methods, YOPO <ref type=""bibr"" target=""#b36"">[37]</ref> and Free <ref type=""bibr"" target=""#b26"">[27]</ref>. Evaluation results show that our method is both more effe <ref type=""bibr"" target=""#b38"">[39]</ref>, YOPO <ref type=""bibr"" target=""#b36"">[37]</ref> and Free <ref type=""bibr"" target=""#b26"">[27]</ref>. For MNIST, the model trained with ATTA can achieve a comp first layer. It can speed up the training progress by just updating the first layer. Shafahi et al. <ref type=""bibr"" target=""#b26"">[27]</ref> improve the training efficiency by recycling the gradient ref>  <ref type=""bibr"" target=""#b38"">[39]</ref>, and Free<ref type=""foot"" target=""#foot_6"">7</ref>  <ref type=""bibr"" target=""#b26"">[27]</ref> with the hyper-parameters recommended in their works, and =""#b15"">[16]</ref>) is adopted to conduct iterative attack <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" target=""#b36"">37,</ref><ref type=""bibr"" tar get=""#b11"">12,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" tar ferability between epochs). which is same with other works <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" target=""#b36"">37,</ref><ref type=""bibr"" tar M-PGD (momentum PGD).</p><p>CIFAR10. Following other works <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" target=""#b36"">37,</ref><ref type=""bibr"" tar",0
"/p><p>Recent works <ref type=""bibr"" target=""#b19"">[20,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b23"">24]</ref> show that adversarial examples can be transferred between m et=""#b18"">[19,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b23"">24]</ref>. To attack a targeted model f t , the attacker generates tr benchmark label y, f s is trained with f t (x) which is the prediction result of the targeted model <ref type=""bibr"" target=""#b23"">[24]</ref> to achieve a higher black-box attack success rate. While o rget=""#b4"">[5,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b34"">35]</ref>. Lie et al. <ref ty",0
"t for the graph structure and do not leverage rich node features. For instance, Poincar? embeddings <ref type=""bibr"" target=""#b28"">[29]</ref> capture the hyperbolic properties of real graphs by learni aselines. For shallow methods, we consider Euclidean embeddings (EUC) and Poincar? embeddings (HYP) <ref type=""bibr"" target=""#b28"">[29]</ref>. We conjecture that HYP will outperform EUC on hierarchica target=""#b30"">31]</ref>. Shallow embedding methods have also been developed in hyperbolic geometry <ref type=""bibr"" target=""#b28"">[29,</ref><ref type=""bibr"" target=""#b29"">30]</ref> for reconstructing </p><p>For link prediction, we use the Fermi-Dirac decoder <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b28"">29]</ref>, a generalization of sigmoid, to compute probability scores",1
"urIPS 2019 announced accepted papers, we also became aware of the concurrently developed HGNN model <ref type=""bibr"" target=""#b25"">[26]</ref> for learning GNNs in hyperbolic space. The main difference s the architecture for neighborhood aggregation and uses a learnable curvature. Additionally, while <ref type=""bibr"" target=""#b25"">[26]</ref> demonstrates strong performance on graph classification ta",0
"35]</ref> and graphs <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b21"">22]</ref>, or embedding text <ref type=""bibr"" target=""#b38"">[39]</ref",0
"o problems of computer vision or natural language processing <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" tar",0
"ting GCNs, have a high distortion when embedding such graphs <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b31"">32]</ref>. In particular, scale-free graphs have tree-like structure",0
"for other network analysis tasks, such as link prediction <ref type=""bibr"" target=""#b35"">[36,</ref><ref type=""bibr"" target=""#b40"">41]</ref> and graph classification <ref type=""bibr"" target=""#b16"">[17",1
"nal GNNs can operate on the transformed homogeneous graphs <ref type=""bibr"" target=""#b36"">[37,</ref><ref type=""bibr"" target=""#b42"">43]</ref>. This is a two-stage approach and requires hand-crafted met <head n=""3.2"">Meta-Path Generation</head><p>Previous works <ref type=""bibr"" target=""#b36"">[37,</ref><ref type=""bibr"" target=""#b42"">43]</ref> require manually defined meta-paths and perform Graph Neura",1
"defined by the meta-paths. Then conventional GNNs can operate on the transformed homogeneous graphs <ref type=""bibr"" target=""#b36"">[37,</ref><ref type=""bibr"" target=""#b42"">43]</ref>. This is a two-sta <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.2"">Meta-Path Generation</head><p>Previous works <ref type=""bibr"" target=""#b36"">[37,</ref><ref type=""bibr"" target=""#b42"">43]</ref> require manually d "" target=""#b9"">[10]</ref> learns graph representations by using meta-path based random walk and HAN <ref type=""bibr"" target=""#b36"">[37]</ref> learns graph representation learning by transforming a het matrix X ∈ R N ×D meaning that the D-dimensional input feature given for each node.</p><p>Meta-Path <ref type=""bibr"" target=""#b36"">[37]</ref> denoted by p is a path on the heterogeneous graph G that i N <ref type=""bibr"" target=""#b18"">[19]</ref>, GAT <ref type=""bibr"" target=""#b32"">[33]</ref>, and HAN <ref type=""bibr"" target=""#b36"">[37]</ref> as GNN based methods. GCN is a graph convolutional network Here, we test HAN on the selected sub-graphs whose nodes are linked with meta-paths as described in <ref type=""bibr"" target=""#b36"">[37]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head",1
"ive in achieving state-ofthe-art performance in a variety of graph datasets such as social networks <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" targ type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b37"">38]</ref> and nonspectral methods <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" targ ,</ref><ref type=""bibr"" target=""#b32"">33]</ref>, subsampling <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b6"">7]</ref> and inductive representation for a large graph <ref type=""bib",0
"type=""bibr"" target=""#b41"">42]</ref> and node classification <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b32"">33]</ref>. The representation mance in a variety of graph datasets such as social networks <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b34"">35]</ref>, citation networks nonspectral methods <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" tar te convolution directly on graphs by passing node features <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b13"">14]</ref> to neighbors, or perform convolution in the spectral domain "">[33]</ref> applies different weight matrices for nodes with different degrees and Hamilton et al. <ref type=""bibr"" target=""#b13"">[14]</ref> has proposed learnable aggregator functions which summariz #b5"">[6,</ref><ref type=""bibr"" target=""#b6"">7]</ref> and inductive representation for a large graph <ref type=""bibr"" target=""#b13"">[14]</ref> have been studied. Although these methods show outstanding",0
"graph datasets have been recently studied for other network analysis tasks, such as link prediction <ref type=""bibr"" target=""#b35"">[36,</ref><ref type=""bibr"" target=""#b40"">41]</ref> and graph classifi",0
"ave been used such as simple graph statistics <ref type=""bibr"" target=""#b1"">[2]</ref>, graph kernel <ref type=""bibr"" target=""#b33"">[34]</ref>, and engineered features from a local neighbor structure <",0
"aph classification <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b39"">40]</ref>, link prediction <ref type=""bibr"" target=""#b17"">[18,</ref><",0
"r"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b25"">26]</ref>, attention mechanism on neighbors <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b32"">33]</ref>, subsampling <ref",0
"ral domain using the Fourier basis of a given graph, i.e., eigenfunctions of the Laplacian operator <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" targ of tasks. They are categorized into two approaches: spectral <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" targe",0
"arget=""#b8"">9,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b37"">38]</ref> and nonspectral met",0
"<ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b34"">35]</ref>, citation networks <ref type=""bibr"" target=""#b18"">[19,</ref><ref type=""bibr"" target=""#b32"">33]</ref>, functional struct e Laplacian operator <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b18"">19]</ref>.</p><p>However, one limitation of most GNNs is that they as target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" tar to perform convolution in the spectral domain using the Fourier basis of a given graph. Kipf et al. <ref type=""bibr"" target=""#b18"">[19]</ref> simplified GNNs using the first-order approximation of the <head>Graph Convolutional network (GCN).</head><p>In this work, a graph convolutional network (GCN) <ref type=""bibr"" target=""#b18"">[19]</ref> is used to learn useful representations for node classific /p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>GNN-based methods</head><p>We used the GCN <ref type=""bibr"" target=""#b18"">[19]</ref>, GAT <ref type=""bibr"" target=""#b32"">[33]</ref>, and HAN <r",0
"ndom walks on graphs have been proposed in DeepWalk <ref type=""bibr"" target=""#b27"">[28]</ref>, LINE <ref type=""bibr"" target=""#b31"">[32]</ref>, and node2vec <ref type=""bibr"" target=""#b12"">[13]</ref> wi",0
"d for decades. Conventionally, hand-crafted features have been used such as simple graph statistics <ref type=""bibr"" target=""#b1"">[2]</ref>, graph kernel <ref type=""bibr"" target=""#b33"">[34]</ref>, and",0
"in various tasks over graphs, such as graph classification <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b39"">40]</ref>, link prediction <r",0
"academic communities <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b14"">14,</ref><ref type=""bibr"" target=""#b18"">18,</ref><ref type=""bibr"" target=""#b20"">20,</ref><ref type=""bibr"" tar /head><p>GCNs are showing great potential in various tasks <ref type=""bibr"" target=""#b16"">[16,</ref><ref type=""bibr"" target=""#b18"">18,</ref><ref type=""bibr"" target=""#b19"">19,</ref><ref type=""bibr"" tar ted by recursively aggregating and transforming the representation vectors of its neighbor vertices <ref type=""bibr"" target=""#b18"">[18,</ref><ref type=""bibr"" target=""#b39"">39,</ref><ref type=""bibr"" ta to sample a subset from the neighbor vertices of each vertex <ref type=""bibr"" target=""#b6"">[6,</ref><ref type=""bibr"" target=""#b18"">18]</ref> as the new neighbors, specifically,</p><formula xml:id=""for ling to alleviate receptive field expansion that effectively trades off accuracy and execution time <ref type=""bibr"" target=""#b18"">[18]</ref>. It is formulated as</p><formula xml:id=""formula_4"">a k v ing preprocessing <ref type=""bibr"" target=""#b20"">[20]</ref> or with random selection during runtime <ref type=""bibr"" target=""#b18"">[18]</ref>. Aggregation aggregates the features from its 1-hop neighb he execution time breakdown of GCN (GCN) <ref type=""bibr"" target=""#b25"">[25]</ref>, GraphSage (GSC) <ref type=""bibr"" target=""#b18"">[18]</ref>, and GINConv (GIN) <ref type=""bibr"" target=""#b39"">[39]</re",1
"ccesses in the Aggregation phase since the irregularity harms the predictability of memory accesses <ref type=""bibr"" target=""#b10"">[10]</ref>. Besides, it is difficult to efficiently implement the reu",0
"Aggregation in a hardware thread.  Furthermore, the hardware-optimized libraries such as Intel MKL <ref type=""bibr"" target=""#b21"">[21]</ref> and NVIDIA cuBLAS library <ref type=""bibr"" target=""#b31"">[ also misses the advantage of hardwareoptimized operations, such as matrix multiplication operation <ref type=""bibr"" target=""#b21"">[21,</ref><ref type=""bibr"" target=""#b31"">31]</ref>. Furthermore, furt",0
"As a result, GCNs gradually become a new workload family member in data-centers, such as in Google <ref type=""bibr"" target=""#b13"">[13]</ref>, Facebook <ref type=""bibr"" target=""#b28"">[28]</ref>, and A br"" target=""#b43"">43,</ref><ref type=""bibr"" target=""#b46"">46]</ref>. Many companies, such as Google <ref type=""bibr"" target=""#b13"">[13]</ref>, Facebook <ref type=""bibr"" target=""#b28"">[28]</ref>, and A",0
"al efforts from both the industrial and academic communities <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b14"">14,</ref><ref type=""bibr"" target=""#b18"">18,</ref><ref type=""bibr"" tar e problems including node classification <ref type=""bibr"" target=""#b25"">[25]</ref>, link prediction <ref type=""bibr"" target=""#b14"">[14,</ref><ref type=""bibr"" target=""#b16"">16]</ref>, graph clustering",0
"rotect the proprietary data. However, recent work by Zhu et al., ""Deep Leakage from Gradient"" (DLG) <ref type=""bibr"" target=""#b0"">[1]</ref> showed the possibility to steal the private training data fr nables us to always extract the ground-truth labels and significantly simplify the objective of DLG <ref type=""bibr"" target=""#b0"">[1]</ref> in order to extract good-quality data. Hence, we name our ap extraction with better fidelity.</p><p>• We empirically demonstrate the advantages of iDLG over DLG <ref type=""bibr"" target=""#b0"">[1]</ref> via comparing the accuracy of extracted labels and the fidel <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2"">Methodology</head><p>Recent work by Zhu et al. <ref type=""bibr"" target=""#b0"">[1]</ref> presents an approach (DLG) to steal the proprietary data pro s</head><p>In this section, we empirically demonstrate the advantages of our (iDLG) method over DLG <ref type=""bibr"" target=""#b0"">[1]</ref>. We perform experiments on the classification task over thre "" target=""#b9"">[10]</ref> with 10, 100, and 5749 categories respectively. Following the settings in <ref type=""bibr"" target=""#b0"">[1]</ref>, we use the randomly initialized LeNet for all experiments. is used as the optimizer. For fast training, we resize all images in LFW to 32 × 32.</p><p>For DLG <ref type=""bibr"" target=""#b0"">[1]</ref>, as described by the authors, we start the procedure with th % 100.0% LFW 79.1% 100.0% Table <ref type=""table"">1</ref>: Accuracy of the extracted labels for DLG <ref type=""bibr"" target=""#b0"">[1]</ref> and iDLG. Note that iDLG always extracts the correct label a",1
"""1"">Introduction</head><p>In multi-node distributed learning systems such as Collaborative Learning <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target",0
"and Federated Learning <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b6"">7]</ref>, it is widely believed that sharing gradients between nodes w",0
"<ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b3"">4]</ref> and Federated Learning <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target",0
"arget=""#b0"">[1]</ref>. We perform experiments on the classification task over three datasets: MNIST <ref type=""bibr"" target=""#b7"">[8]</ref>, CIFAR-100 <ref type=""bibr"" target=""#b8"">[9]</ref>, and LFW",0
"NN architectures, which we refer to as Attention MPNN (AMPNN) and Edge Memory Neural Network (EMNN) <ref type=""bibr"" target=""#b36"">[34]</ref>, and evaluate them against published benchmark results wit",1
"><p>Simpler molecules nevertheless can be modelled with a much lower error around 0.3-0.4 log units <ref type=""bibr"" target=""#b66"">[65]</ref>-this same study further suggests that the limit of ca. 0.6",0
"odelling. Researchers <ref type=""bibr"" target=""#b4"">[2]</ref><ref type=""bibr"" target=""#b5"">[3]</ref><ref type=""bibr"" target=""#b6"">[4]</ref><ref type=""bibr"" target=""#b7"">[5]</ref><ref type=""bibr"" targe",0
"<ref type=""figure"">2</ref>). Charge-parent fragmentation was performed using the MolVS standardizer <ref type=""bibr"" target=""#b62"">[61]</ref>, which returned the uncharged version of the largest organ",0
"m coupled-cluster (CC2) and TD-DFT data on synthetically feasible small organic molecules. The ESOL <ref type=""bibr"" target=""#b55"">[53]</ref> dataset comprises aqueous solubility values for small mole",0
"private-attribute inference attack can be naturally formulated as a problem of adversarial learning <ref type=""bibr"" target=""#b18"">[19]</ref>. In our proposed RAP, there are two components: a Bayesian",1
"interactions history <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" target=""#b38"">39]</ref>. One main reason is ta is masked (i.e., differential privacy based techniques) <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" target=""#b44"">45]</ref>; and (2) coarsening tial privacy based <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" target=""#b44"">45]</ref> and perturbation ba hat two arbitrary records have close probabilities to generate the same noisy data. McSherry et al. <ref type=""bibr"" target=""#b32"">[33]</ref> utilize differential privacy to construct private covarian",0
"ate attributes, gender (male/female), age, and occupation. For this paper, we follow the setting of <ref type=""bibr"" target=""#b21"">[22]</ref> and categorize age attribute into three groups, over-45, u",0
"type=""bibr"" target=""#b40"">41]</ref> approaches. Some methods utilize differential privacy strategy <ref type=""bibr"" target=""#b13"">[14]</ref> to modify the answers of the recommendation algorithm so t",0
"target=""#b0"">[1]</ref><ref type=""bibr"" target=""#b1"">[2]</ref><ref type=""bibr"" target=""#b2"">[3]</ref><ref type=""bibr"" target=""#b3"">[4]</ref> and threats to users' privacy <ref type=""bibr"" target=""#b6"">",0
"arget=""#b1"">[2]</ref> based controller to update the memory, while Working Memory Network (W-MemNN) <ref type=""bibr"" target=""#b16"">[17]</ref> uses a multi-head attention <ref type=""bibr"" target=""#b25"" <ref type=""bibr"" target=""#b25"">[26]</ref>, which is similar to that used in Working Memory Network <ref type=""bibr"" target=""#b16"">[17]</ref>. Multi-head attention allows the model to jointly attend t",1
"get=""#b15"">16,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b30"">31]</ref>. The recognition process has become a necessary part of man aved first, then the recognition tasks are conducted on text <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b30"">31]</ref>. Given the plain text of an academic homepage, we aim to re ding, i.e., S ∈ R n×d e . Following state-of-the-art methods <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b30"">31]</ref>, we use GloVe <ref type=""bibr"" target=""#b18"">[19]</ref> to ods have been developed to address these problems. The state-of-the-art for publication recognition <ref type=""bibr"" target=""#b30"">[31]</ref> uses a Bi-LSTM-CRF based model to learn the page-level and l language processing methods. For example, state-of-the-art techniques for publication recognition <ref type=""bibr"" target=""#b30"">[31]</ref> and for person names recognition <ref type=""bibr"" target="" ifferent methods to capture the position patterns. The state-of-the-art for publication recognition <ref type=""bibr"" target=""#b30"">[31]</ref> trains webpage-level and line-level models together to cap nd Preprocessing. We use the same datasets used by the state-of-the-art for publication recognition <ref type=""bibr"" target=""#b30"">[31]</ref> and person name recognition <ref type=""bibr"" target=""#b0""> ref type=""table"" target=""#tab_0"">2</ref> summarises the dataset statistics.</p><p>• HomePub dataset <ref type=""bibr"" target=""#b30"">[31]</ref> contains the plain text of 2,087 homepages from different formation about the position patterns and person names. PAM also outperforms the hierarchical PubSE <ref type=""bibr"" target=""#b30"">[31]</ref> model, which can capture the positional diversity, by 3.64 a manual inspection of recognition results of state-ofthe-art models for the two tasks (i.e., PubSE <ref type=""bibr"" target=""#b30"">[31]</ref> and CogNN <ref type=""bibr"" target=""#b0"">[1]</ref>) and our ublications and 5,542 person names.</p><p>For publication string recognition, we observe that PubSE <ref type=""bibr"" target=""#b30"">[31]</ref> misrecognises strings about patents, grants, and research",1
"ies may need to solve the name disambiguation problem (i.e., different people with identical names) <ref type=""bibr"" target=""#b22"">[23]</ref> before mining the collaboration networks or research inter",0
"]</ref>. The recognition process has become a necessary part of many online systems, such as AMiner <ref type=""bibr"" target=""#b23"">[24]</ref> and CiteSeerX <ref type=""bibr"" target=""#b15"">[16]</ref>, a digital libraries <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b24"">25]</ref>. Such a recognition",0
"Dynamic Memory Network (DMN) <ref type=""bibr"" target=""#b11"">[12]</ref> uses a gated recurrent unit <ref type=""bibr"" target=""#b1"">[2]</ref> based controller to update the memory, while Working Memory",0
"king Memory Network (W-MemNN) <ref type=""bibr"" target=""#b16"">[17]</ref> uses a multi-head attention <ref type=""bibr"" target=""#b25"">[26]</ref> based controller. All these networks use a memory module f ting the memory representation M, we use a memory updating controller based on multi-head attention <ref type=""bibr"" target=""#b25"">[26]</ref>, which is similar to that used in Working Memory Network < • Joint-Att is resulted from replacing the AM and PM modules with multi-layer multi-head attention <ref type=""bibr"" target=""#b25"">[26]</ref>. This model has a pipeline architecture for two jointly tr",0
"Our proposed PAM model outperforms the baselines that use standard NER models, such as Stanford NER <ref type=""bibr"" target=""#b4"">[5]</ref> and Bi-LSTM-CRF <ref type=""bibr"" target=""#b8"">[9]</ref>, by",0
"et=""#b14"">[15,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b24"">25]</ref>. Such a recognition problem is simpler since the text in re",0
"foot_1"">2</ref> and train it on an NVIDIA GTX1080 GPU. The model is trained with the Adam optimiser <ref type=""bibr"" target=""#b10"">[11]</ref> with the learning rates tuned among {0.01, 0.005, 0.001}. chanisms reported in the corresponding papers. If these are not reported, we use the Adam optimiser <ref type=""bibr"" target=""#b10"">[11]</ref> to train the model for a maximum of 20 epochs with early s t-Att and Joint-Stack have attention heads of 5. All the models are trained with the Adam optimiser <ref type=""bibr"" target=""#b10"">[11]</ref> and the hyperparameters are selected using the same develo",0
"lleviated by recognising information from academic homepages.</p><p>Models based on memory networks <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b26"">27]</ref> are proposed for q ><head n=""3.2"">Alternatingly Updated Memory</head><p>Different from the traditional Memory Networks <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b26"">27]</ref>, which updates a m",0
"n or essential characteristics. However, there is no such wide-accepted formal definition. Paulheim <ref type=""bibr"" target=""#b5"">[6]</ref> defined four criteria for knowledge graphs. Ehrlinger and W statistical relational learning <ref type=""bibr"" target=""#b8"">[9]</ref>, knowledge graph refinement <ref type=""bibr"" target=""#b5"">[6]</ref>, Chinese knowledge graph construction <ref type=""bibr"" targe",1
"/head><p>Previous survey papers on knowledge graphs mainly focus on statistical relational learning <ref type=""bibr"" target=""#b8"">[9]</ref>, knowledge graph refinement <ref type=""bibr"" target=""#b5"">[6",1
"ns and proposed Definition 1 which emphasizes the reasoning engine of knowledge graphs. Wang et al. <ref type=""bibr"" target=""#b7"">[8]</ref> proposed a definition as a multi-relational graph in Definit ormation into an ontology and applies a reasoner to derive new knowledge. Definition 2 (Wang et al. <ref type=""bibr"" target=""#b7"">[8]</ref>). A knowledge graph is a multirelational graph composed of e ""#b5"">[6]</ref>, Chinese knowledge graph construction <ref type=""bibr"" target=""#b9"">[10]</ref>, KGE <ref type=""bibr"" target=""#b7"">[8]</ref> or KRL <ref type=""bibr"" target=""#b10"">[11]</ref>. The latter </ref> presented KRL in a linear manner, with a concentration on quantitative analysis. Wang et al. <ref type=""bibr"" target=""#b7"">[8]</ref> categorized KRL according to scoring functions, and specific s of auxiliary information for KRL such as attributes, relation path and logical rules. Wang et al. <ref type=""bibr"" target=""#b7"">[8]</ref> gave a detailed review on these information. This paper disc",1
"policy networks. Similarly, Zeng et al. <ref type=""bibr"" target=""#b127"">[128]</ref> and Feng et al. <ref type=""bibr"" target=""#b128"">[129]</ref> proposed different reward strategies. The advantage of R =""#b127"">[128]</ref> Policy gradient + CNN + +1/-1 bag-result reward position embedding Feng et al. <ref type=""bibr"" target=""#b128"">[129]</ref> Policy gradient + CNN + predictive probability reward po",0
"e models which rely on preprepared entity pairs or text corpus. Focusing on medical domain, REM-EDY <ref type=""bibr"" target=""#b67"">[68]</ref> proposes a generative model called conditional relationshi",0
"ulti-channel LSTM while utilizing the shortest dependency path between entity pair, and Miwa et al. <ref type=""bibr"" target=""#b114"">[115]</ref> stacks sequential and tree-structure LSTMs based on depe target=""#b113"">[114]</ref> Multichannel LSTM + dropout dependency tree, POS, GR, hypernyms LSTM-RNN <ref type=""bibr"" target=""#b114"">[115]</ref> Bi-LSTM + Bi-TreeLSTM POS, dependency tree BRCNN <ref ty",0
". Random walk inference has been widely investigated, for example, the Path-Ranking Algorithm (PRA) <ref type=""bibr"" target=""#b68"">[69]</ref> chooses relational path under a combination of path constr",0
">Vanilla vector-based embedding methods failed to deal with 1-to-n relations. Recently, Dong et al. <ref type=""bibr"" target=""#b88"">[89]</ref> extended the embedding space into region-based n-dimension",0
"type=""figure"" target=""#fig_3"">7a</ref> illustrates a simple first-order Horn clause inference. RUGE <ref type=""bibr"" target=""#b79"">[80]</ref> proposes an iterative model, where soft rules are utilized",0
"hips, ConvKB keeps the transitional characteristic and shows better experimental performance. HypER <ref type=""bibr"" target=""#b47"">[48]</ref> utilizes hypernetwork H for 1D relation-specific convoluti",0
"soft label prediction from unlabeled triples and labeled triples for embedding rectification. IterE <ref type=""bibr"" target=""#b80"">[81]</ref> proposes an iterative training strategy with three compone",0
"cal knowledge graph, to consider inter-slot relations in spoken language understanding. Wang et al. <ref type=""bibr"" target=""#b146"">[147]</ref> augmented short text representation learning with knowle",0
"us downstream knowledge-aware applications also come up with many datasets, for example, Wiki-Facts <ref type=""bibr"" target=""#b202"">[203]</ref> for language modeling; SimpleQuestions <ref type=""bibr""",0
"ching guided by long short-term memory (LSTM) networks to calculate the similarity scores. Meta-KGR <ref type=""bibr"" target=""#b86"">[87]</ref>, an optimization-based meta learning approach, adopts mode",0
"ion is another direction for dialog systems. By predefining a set of base actions, Dialog-to-Action <ref type=""bibr"" target=""#b187"">[188]</ref> is an encoder-decoder approach that maps executable logi",0
"consider the degrees of importance of items based on their rankings.</p><p>Recently, Tang and Wang <ref type=""bibr"" target=""#b10"">[11]</ref> proposed a KD model to address the ranking problem, called ledge distillation (KD) <ref type=""bibr"" target=""#b9"">[10]</ref> and present rank distillation (RD) <ref type=""bibr"" target=""#b10"">[11]</ref> that applies knowledge distillation to recommender models. ommendation problem because Fig. <ref type=""figure"">1</ref>. Illustration of rank distillation (RD) <ref type=""bibr"" target=""#b10"">[11]</ref>. The teacher model transfers manipulated top-k items as th t may have worse performance than the original student model. Rank distillation (RD). Tang and Wang <ref type=""bibr"" target=""#b10"">[11]</ref> proposed ranking distillation (RD) that applies KD for ran e rated by less than 5 users. Table I reports the detailed statistics of these datasets.</p><p>• RD <ref type=""bibr"" target=""#b10"">[11]</ref>: To define the KD loss in equation ( <ref type=""formula"" t een λ that appears in RD and CD. Specifically, we used the following parameter settings.</p><p>• RD <ref type=""bibr"" target=""#b10"">[11]</ref> and RD-Rank: We set ρ to be 0.5. For CDAE, the number of i er, we used the public PyTorch implementation <ref type=""foot"" target=""#foot_5"">6</ref> provided in <ref type=""bibr"" target=""#b10"">[11]</ref>. All experiments were conducted on a desktop with 128 GB m KD. Also, the gain indicates how additional accuracy achieved by the proposed model over that of RD <ref type=""bibr"" target=""#b10"">[11]</ref>.</p><p>Based on this evaluation, we found several interest rform RD over all datasets. Note that the improvement gap for RD is somewhat different from that in <ref type=""bibr"" target=""#b10"">[11]</ref>. It is because we used leave-one-out evaluation while <ref in <ref type=""bibr"" target=""#b10"">[11]</ref>. It is because we used leave-one-out evaluation while <ref type=""bibr"" target=""#b10"">[11]</ref> used cross-validation evaluation. Our models are consisten el size and efficiency. The model size is proportional to the accuracy of our model, as observed in <ref type=""bibr"" target=""#b10"">[11]</ref> as well. The same tendency consistently holds in different oot"" n=""4"" xml:id=""foot_3"">http://dawenl.github.io/data/gowalla pro.zip Competitive models. Since RD<ref type=""bibr"" target=""#b10"">[11]</ref> is the state-of-the-art KD model for top-N recommendation,",1
"balance between effectiveness and efficiency. In this paper, we employ knowledge distillation (KD) <ref type=""bibr"" target=""#b9"">[10]</ref> which is a network compression technique by transferring th ulate the top-N recommendation problem. Then, we explain the concept of knowledge distillation (KD) <ref type=""bibr"" target=""#b9"">[10]</ref> and present rank distillation (RD) <ref type=""bibr"" target= r proposed training strategies.</p><p>Temperature in the KD loss. One key factor of the original KD <ref type=""bibr"" target=""#b9"">[10]</ref> is to find a proper balance between the soft targets and ha ]</ref> is to find a proper balance between the soft targets and hard labels. To tackle this issue, <ref type=""bibr"" target=""#b9"">[10]</ref> introduces the notion of a temperature T . Although the sof",1
"ly discussed in one-class collaborative filtering (OCCF) <ref type=""bibr"" target=""#b11"">[12]</ref>- <ref type=""bibr"" target=""#b18"">[19]</ref>. Given user u, I + u = {i ∈ I|r ui = 1} is the set of item imilarity matrices between users and items to predict drug-target interaction. Moreover, Yao et al. <ref type=""bibr"" target=""#b18"">[19]</ref> proposed dual regularization by combining the weighted-and",1
"tivation boundaries, and thus suggested a hinge loss. <ref type=""bibr"" target=""#b21"">[22]</ref> and <ref type=""bibr"" target=""#b22"">[23]</ref> employed adversarial learning into the KD framework. Recen",0
"ased loss is inappropriate for transferring activation boundaries, and thus suggested a hinge loss. <ref type=""bibr"" target=""#b21"">[22]</ref> and <ref type=""bibr"" target=""#b22"">[23]</ref> employed adv",0
"to recommender models involves several challenges: (1) Implicit user feedback is extremely sparse. <ref type=""bibr"" target=""#b1"">(2)</ref> As users only provide positive feedback in implicit datasets",0
"aluation protocol. We adopted the leave-one-out evaluation <ref type=""bibr"" target=""#b3"">[4]</ref>- <ref type=""bibr"" target=""#b5"">[6]</ref>. Specifically, we held-out the last timestamp useritem inter e used for training data. Unlike sampling-based evaluation <ref type=""bibr"" target=""#b3"">[4]</ref>- <ref type=""bibr"" target=""#b5"">[6]</ref> that randomly chose 100 items from the set of unrated items, ounted cumulative gain (NDCG), as done in existing studies <ref type=""bibr"" target=""#b3"">[4]</ref>- <ref type=""bibr"" target=""#b5"">[6]</ref>. The size N of the ranked list was chosen to be 50 for HR@N",0
"e knowledge extracted from a complex teacher model to a simple student model. Many existing studies <ref type=""bibr"" target=""#b19"">[20]</ref>- <ref type=""bibr"" target=""#b27"">[28]</ref> have utilized K model parameters of the teacher model directly to initialize those of the student model. Recently, <ref type=""bibr"" target=""#b19"">[20]</ref> used the attention map as an additional matching constrain",0
"n and suggested using the output of intermediate layers as additional matching criteria. Similarly, <ref type=""bibr"" target=""#b25"">[26]</ref> utilized the gram matrix of the channel responses from the",0
"""><head>I. INTRODUCTION</head><p>Neural recommender models <ref type=""bibr"" target=""#b0"">[1]</ref>- <ref type=""bibr"" target=""#b8"">[9]</ref> have achieved better performance than conventional latent fa nt-wise preferences. We leave the evaluation for other models with pair-wise preferences, e.g., NPR <ref type=""bibr"" target=""#b8"">[9]</ref>, to future work.) Although the teacher model can be an ensem",0
"reduce the memory size and enhance efficiency. Second, the pruning and sharing method presented in <ref type=""bibr"" target=""#b31"">[32]</ref>, <ref type=""bibr"" target=""#b32"">[33]</ref> removes or bind",0
"et=""#b16"">[17]</ref> leveraged side information to construct user-item similarity, and Zheng et al. <ref type=""bibr"" target=""#b17"">[18]</ref> employed multiple similarity matrices between users and it",0
"rget, the attention map of the student model should match that of the teacher model. Most recently, <ref type=""bibr"" target=""#b23"">[24]</ref> further improved the attentionbased method by matching the",0
"eedback as optimization variables and imputed missing feedback via optimization. Besides, Li et al. <ref type=""bibr"" target=""#b16"">[17]</ref> leveraged side information to construct user-item similari",0
"ls is an fundamental issue for real-world applications. To address this problem, various techniques <ref type=""bibr"" target=""#b28"">[29]</ref> have been widely developed to compress cumbersome models i",0
"bibr"" target=""#b22"">[23]</ref> employed adversarial learning into the KD framework. Recently, KDGAN <ref type=""bibr"" target=""#b20"">[21]</ref> bypassed the convergence step of adversarial learning by e",0
"retion, (2) pruning and sharing model parameters, and (3) knowledge distillation (KD).</p><p>First, <ref type=""bibr"" target=""#b29"">[30]</ref>, <ref type=""bibr"" target=""#b30"">[31]</ref> proposed the bi",0
"ethod by considering the degree distributions of users/items in the graph. Lastly, Sindhwani et al. <ref type=""bibr"" target=""#b12"">[13]</ref> regarded unobserved feedback as optimization variables and",0
"bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b14"">15]</ref>, large Transformer based models <ref type=""bibr"" target=""#b31"">[32]</ref>, such as BERT <ref type=""bibr"" target=""#b5"">[6]</ref>, sho il: markus.zlabinger@tuwien.ac.at 3 TU Wien, Austria, email: hanbury@ifs.tuwien.ac.at former layers <ref type=""bibr"" target=""#b31"">[32]</ref> (we evaluate up to three) can effectively contextualize qu learning -a local contextualization, fixed by the n-gram size hyperparameter.</p><p>Vaswani et al. <ref type=""bibr"" target=""#b31"">[32]</ref> proposed the Transformer architecture in the context of la intensity of the contextualization. We calculate the context(t1:n) with a set of Transformer layers <ref type=""bibr"" target=""#b31"">[32]</ref>. First, the input sequence is fused with a positional enco",1
"re-ranking model's effectiveness and its efficiency. While IR-specific networks are reasonably fast <ref type=""bibr"" target=""#b35"">[36,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" targ rms in a single interaction match matrix, followed by softhistogram scoring based on kernel-pooling <ref type=""bibr"" target=""#b35"">[36]</ref>. This allows us to explain scoring reasons by probing the of a hard histogram method and the resulting lack of fine-tuned word representations. Xiong et al. <ref type=""bibr"" target=""#b35"">[36]</ref> improve on the idea and propose the kernel-pooling techniq qi, dj)<label>(4)</label></formula><p>Then, we transform each entry in M with a set of RBF-kernels <ref type=""bibr"" target=""#b35"">[36]</ref>. Each kernel focuses on a specific similarity range with c similarity range with center µ k . The size of all ranges is set by σ. In contrast to Xiong et al. <ref type=""bibr"" target=""#b35"">[36]</ref> we do not employ an exact match kernel -as contextualized alysis unfeasible.</p><p>The differences of TK to previous kernel-pooling methods are:</p><p>• KNRM <ref type=""bibr"" target=""#b35"">[36]</ref> uses only word embeddings, therefore a match does not have improves the robustness of PACRR's pooling strategy with randomization during training.</p><p>KNRM <ref type=""bibr"" target=""#b35"">[36]</ref> uses a soft-histogram (differentiable Gaussian kernel func",1
"ype=""bibr"" target=""#b29"">[30]</ref>, whereas pattern-based methods are harder to interpret post-hoc <ref type=""bibr"" target=""#b8"">[9]</ref>.</p><p>Contextualization allows neural IR models to vary the",0
"it offers many benefits in practice. Word embeddings are easy to pre-train on domain specific data <ref type=""bibr"" target=""#b13"">[14]</ref>. They require only one id per term, making the index consu",0
"ructure over more complex methods -such as FastText <ref type=""bibr"" target=""#b2"">[3]</ref> or ELMo <ref type=""bibr"" target=""#b28"">[29]</ref> -as it offers many benefits in practice. Word embeddings a",0
"nal learning-to-rank the trade-off between effectiveness and efficiency has been thoroughly studied <ref type=""bibr"" target=""#b33"">[34,</ref><ref type=""bibr"" target=""#b34"">35,</ref><ref type=""bibr"" ta ibr"" target=""#b34"">[35]</ref>, incorporating an efficiency metric in the training of linear rankers <ref type=""bibr"" target=""#b33"">[34]</ref>, and comparing the effectiveness and efficiency of various",0
"et=""#b33"">[34,</ref><ref type=""bibr"" target=""#b34"">35,</ref><ref type=""bibr"" target=""#b36"">37,</ref><ref type=""bibr"" target=""#b3"">4]</ref>. This includes applying a temporal constraint on the number o "">[34]</ref>, and comparing the effectiveness and efficiency of various learning-to-rank algorithms <ref type=""bibr"" target=""#b3"">[4]</ref>. In web search the speed of a response is crucial as determi",0
"target=""#b1"">[2]</ref>, MSMARCO-Document <ref type=""bibr"" target=""#b1"">[2]</ref>, and TREC CAR 2017 <ref type=""bibr"" target=""#b6"">[7]</ref>. We evaluate a broad range of traditional and neural ranking ) it is deemed relevant in the retrieval task as well as the document containing it.</p><p>TREC CAR <ref type=""bibr"" target=""#b6"">[7]</ref> is created as part of the TREC Complex Answer Retrieval (CAR",0
"independence <ref type=""bibr"" target=""#b22"">[23]</ref> or by approximating interaction similarities <ref type=""bibr"" target=""#b16"">[17]</ref>. When a large number of pre-trained Transformer layers is",0
"neural IR models unsuccessfully tried to match single vector representations per query and document <ref type=""bibr"" target=""#b20"">[21]</ref>. Then, interaction-focused models moved to a more fine-gra",0
"ndamental task towards effective personalized recommendation <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" tar . embedding) to represent a user and an item, and perform prediction based on the embedding vectors <ref type=""bibr"" target=""#b17"">[18]</ref>. Matrix factorization is an early such model, which direct [33]</ref> and PinSage <ref type=""bibr"" target=""#b45"">[46]</ref>, neural network-based models NeuMF <ref type=""bibr"" target=""#b17"">[18]</ref> and CMN <ref type=""bibr"" target=""#b7"">[8]</ref>, and facto e ID of a user (or an item) into an embedding vector. The recent neural recommender models like NCF <ref type=""bibr"" target=""#b17"">[18]</ref> and LRML <ref type=""bibr"" target=""#b31"">[32]</ref> use the",1
"rget=""#b7"">[8,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b38"">39]</ref>.</p><p>The most common paradigm for CF is to learn latent f et=""#b11"">[12,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b33"">34,</ref><ref type=""bibr"" target=""#b38"">39]</ref> that typically aggregate extended neighbors and need to han edding learning.</p><p>To deepen the use of subgraph structure with high-hop neighbors, Wang et al. <ref type=""bibr"" target=""#b38"">[39]</ref> recently proposes NGCF and achieves state-of-the-art perfo <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2"">PRELIMINARIES</head><p>We first introduce NGCF <ref type=""bibr"" target=""#b38"">[39]</ref>, a representative and state-of-the-art GCN model for recom the scores of NGCF are directly copied from the Table <ref type=""table"" target=""#tab_4"">3</ref> of <ref type=""bibr"" target=""#b38"">[39]</ref>. As can be seen, removing feature transformation (i.e., NG >transformation and nonlinear activation. The graph convolution operation (a.k.a., propagation rule <ref type=""bibr"" target=""#b38"">[39]</ref>) in LightGCN is defined as:</p><formula xml:id=""formula_5"" ms) that have overlap on interacted items (users), and higher-layers capture higher-order proximity <ref type=""bibr"" target=""#b38"">[39]</ref>. Thus combining them will make the representation more com TS</head><p>We first describe experimental settings, and then conduct detailed comparison with NGCF <ref type=""bibr"" target=""#b38"">[39]</ref>, the method that is most relevant with LightGCN but more c e experiment workload and keep the comparison fair, we closely follow the settings of the NGCF work <ref type=""bibr"" target=""#b38"">[39]</ref>. We request the experimented datasets (including train/tes ation is that increasing the layer number from 0 (i.e., the matrix factorization model, results see <ref type=""bibr"" target=""#b38"">[39]</ref>) to 1 leads to the largest performance gain, and  using a "" target=""#b26"">27]</ref>. Motivated by the strength of graph convolution, recent efforts like NGCF <ref type=""bibr"" target=""#b38"">[39]</ref>, GC-MC <ref type=""bibr"" target=""#b32"">[33]</ref>, and PinS ifferent layers. The scores of NGCF on Gowalla and Amazon-Book are directly copied from the Table3of<ref type=""bibr"" target=""#b38"">[39]</ref>; the scores of NGCF on Yelp2018 are re-run by us.</figDesc",1
"directly projects the single ID of a user to her embedding <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b29"">30]</ref>. Later on, several research find that augmenting user ID wi example, earlier CF models like matrix factorization (MF) <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b29"">30]</ref> project the ID of a user (or an item) into an embedding vec e as the standard matrix factorization (MF). We employ the Bayesian Personalized Ranking (BPR) loss <ref type=""bibr"" target=""#b29"">[30]</ref>, which is a pairwise loss that encourages the prediction o #b17"">[18]</ref> and CMN <ref type=""bibr"" target=""#b7"">[8]</ref>, and factorization-based models MF <ref type=""bibr"" target=""#b29"">[30]</ref> and HOP-Rec <ref type=""bibr"" target=""#b43"">[44]</ref>. As",0
"ref type=""bibr"" target=""#b2"">[3]</ref>, NAIS <ref type=""bibr"" target=""#b16"">[17]</ref>, and DeepICF <ref type=""bibr"" target=""#b42"">[43]</ref>, to automatically learn the importance of each historical",0
"layers of nonlinear feature transformationwhich is the key to the success of modern neural networks <ref type=""bibr"" target=""#b13"">[14]</ref> -will bring no benefits, but negatively increases the diff",0
"resentation for nodes by smoothing features over the graph <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b39"">40]</ref>. To achieve this, it performs graph convolution iteratively insights into GNNs <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b39"">40]</ref>, which inspire us developing LightGCN. Particularly, Wu et ehind the simple design of LightGCN. First we discuss the connection with the Simplified GCN (SGCN) <ref type=""bibr"" target=""#b39"">[40]</ref>, which is a recent linear GCN model that integrates self-c providing more insights into the working mechanism of LightGCN.</p><p>3.2.1 Relation with SGCN. In <ref type=""bibr"" target=""#b39"">[40]</ref>, the authors argue the unnecessary complexity of GCN for n type=""bibr"" target=""#b39"">40]</ref>, which inspire us developing LightGCN. Particularly, Wu et al. <ref type=""bibr"" target=""#b39"">[40]</ref> argues the unnecessary complexity of GCN, developing a sim",0
"model, they are advantageous to traditional supervised learning scheme like factorization machines <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b30"">31]</ref> that model the rel",0
"label>(15)</label></formula><p>where λ controls the L 2 regularization strength. We employ the Adam <ref type=""bibr"" target=""#b19"">[20]</ref> optimizer and use it in a mini-batch manner. We are aware zed with the Xavier method <ref type=""bibr"" target=""#b9"">[10]</ref>. We optimize LightGCN with Adam <ref type=""bibr"" target=""#b19"">[20]</ref> and use the default learning rate of 0.001 and default min",0
"ability and efficiency, it quickly becomes a prevalent formulation of GNNs and is being widely used <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b26"">27]</ref>. Motivated by the st",0
"we discuss the relation with the Approximate Personalized Propagation of Neural Predictions (APPNP) <ref type=""bibr"" target=""#b21"">[22]</ref>, which is recent GCN variant that addresses oversmoothing xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.2.2"">Relation with APPNP.</head><p>In a recent work <ref type=""bibr"" target=""#b21"">[22]</ref>, the authors connect GCN with Personalized PageRank <ref t mendation.</p><p>It is worth mentioning that several recent efforts provide deep insights into GNNs <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" ta",0
"sity model typically involves perturbing the search results to collect examples of counterfactuals, <ref type=""bibr"" target=""#b0"">[1]</ref> describes methods to construct the propensity model without",1
"l bias and dependent only on listing relevance. In essence, we added position as a control variable <ref type=""bibr"" target=""#b17"">[18]</ref> in the ranking model. </p></div> <div xmlns=""http://www.te",0
"g gap of time. For all practical purposes users are in a state of continuous cold start as noted in <ref type=""bibr"" target=""#b1"">[2]</ref>. Handling user level cold start is part of the core ranking",0
"ctures that could tackle interaction between query and listings more explicitly, like deep and wide <ref type=""bibr"" target=""#b2"">[3]</ref>, where query-listing feature crosses were added to the wide",0
"re, like applying residual learning <ref type=""bibr"" target=""#b6"">[7]</ref> and batch normalization <ref type=""bibr"" target=""#b7"">[8]</ref>. Still, NDCG refused to budge in o ine tests. Our takeaway f",0
"target=""#b14"">Dosovitskiy et al., 2014;</ref><ref type=""bibr"" target=""#b41"">Oord et al., 2018;</ref><ref type=""bibr"" target=""#b1"">Bachman et al., 2019)</ref>.</p><p>In this work, we introduce a simple rget=""#b33"">(Krizhevsky et al., 2012;</ref><ref type=""bibr"" target=""#b22"">Hénaff et al., 2019;</ref><ref type=""bibr"" target=""#b1"">Bachman et al., 2019)</ref>, it has not been considered as a systemati br"" target=""#b22"">Hénaff et al., 2019;</ref><ref type=""bibr"" target=""#b24"">Hjelm et al., 2018;</ref><ref type=""bibr"" target=""#b1"">Bachman et al., 2019)</ref>. However, it is not clear if the success o tation learning methods:</p><p>• DIM/AMDIM <ref type=""bibr"" target=""#b24"">(Hjelm et al., 2018;</ref><ref type=""bibr"" target=""#b1"">Bachman et al., 2019)</ref> achieve global-to-local/local-to-neighbor gure"" target=""#fig_0"">1</ref>), but it is also simpler, requiring neither specialized architectures <ref type=""bibr"" target=""#b1"">(Bachman et al., 2019;</ref><ref type=""bibr"" target=""#b22"">Hénaff et a ibr"" target=""#b20"">(Zhang et al., 2016;</ref><ref type=""bibr"" target=""#b41"">Oord et al., 2018;</ref><ref type=""bibr"" target=""#b1"">Bachman et al., 2019;</ref><ref type=""bibr"" target=""#b29"">Kolesnikov e n is useful for self-supervised learning <ref type=""bibr"" target=""#b11"">(Doersch et al., 2015;</ref><ref type=""bibr"" target=""#b1"">Bachman et al., 2019;</ref><ref type=""bibr"" target=""#b22"">Hénaff et al the default nonlinear projection with one additional hidden layer (and ReLU activation), similar to <ref type=""bibr"" target=""#b1"">Bachman et al. (2019)</ref>. We observe that a nonlinear projection is tch size. The best self-supervised model that reports linear evaluation result on CIFAR-10 is AMDIM <ref type=""bibr"" target=""#b1"">(Bachman et al., 2019)</ref>, which achieves 91.2% with a model 25× la",1
"et=""#b19"">(Hadsell et al., 2006;</ref><ref type=""bibr"" target=""#b14"">Dosovitskiy et al., 2014;</ref><ref type=""bibr"" target=""#b41"">Oord et al., 2018;</ref><ref type=""bibr"" target=""#b1"">Bachman et al., the widely used linear evaluation protocol <ref type=""bibr"" target=""#b20"">(Zhang et al., 2016;</ref><ref type=""bibr"" target=""#b41"">Oord et al., 2018;</ref><ref type=""bibr"" target=""#b1"">Bachman et al., ef type=""bibr"" target=""#b46"">(Sohn, 2016;</ref><ref type=""bibr"" target=""#b51"">Wu et al., 2018;</ref><ref type=""bibr"" target=""#b41"">Oord et al., 2018)</ref>; for convenience, we term it NT-Xent (the no e the success of their methods to maximization of mutual information between latent representations <ref type=""bibr"" target=""#b41"">(Oord et al., 2018;</ref><ref type=""bibr"" target=""#b22"">Hénaff et al. ta augmentation policy, while they use FastAutoAugment for their best result.</p><p>• CPC v1 and v2 <ref type=""bibr"" target=""#b41"">(Oord et al., 2018;</ref><ref type=""bibr"" target=""#b22"">Hénaff et al.",1
"ge classification datasets, SimCLR performs on par with or better than a strong supervised baseline <ref type=""bibr"" target=""#b30"">(Kornblith et al., 2019)</ref> on 10 out of 12 datasets.</p></div> <d ge datasets in both linear evaluation (fixed feature extractor) and fine-tuning settings. Following <ref type=""bibr"" target=""#b30"">Kornblith et al. (2019)</ref>, we perform hyperparameter tuning for e ntly long to achieve near-maximal accuracy, as demonstrated in Figure <ref type=""figure"">8</ref> of <ref type=""bibr"" target=""#b30"">Kornblith et al. (2019)</ref>.</p><p>On Birdsnap, there are no statis re we allow all weights to vary during training. In both cases, we follow the approach described by <ref type=""bibr"" target=""#b30"">Kornblith et al. (2019)</ref>, although our preprocessing differs sli",0
"such as logistic loss <ref type=""bibr"" target=""#b37"">(Mikolov et al., 2013)</ref>, and margin loss <ref type=""bibr"" target=""#b44"">(Schroff et al., 2015)</ref>. Table <ref type=""table"" target=""#tab_3"" igh the negatives by their relative hardness. As a result, one must apply semi-hard negative mining <ref type=""bibr"" target=""#b44"">(Schroff et al., 2015)</ref> for these loss functions: instead of com",0
"andomly chose 30 images per class and test on the remainder, for fair comparison with previous work <ref type=""bibr"" target=""#b13"">(Donahue et al., 2014;</ref><ref type=""bibr"" target=""#b45"">Simonyan &",0
"hman et al., 2019;</ref><ref type=""bibr"" target=""#b22"">Hénaff et al., 2019)</ref> nor a memory bank <ref type=""bibr"" target=""#b51"">(Wu et al., 2018;</ref><ref type=""bibr"" target=""#b49"">Tian et al., 20 lized cross-entropy-based objective. We use simpler data augmentation.</p><p>• InstDisc, MoCo, PIRL <ref type=""bibr"" target=""#b51"">(Wu et al., 2018;</ref><ref type=""bibr"" target=""#b21"">He et al., 2019 ni-batch. This loss has been used in previous work <ref type=""bibr"" target=""#b46"">(Sohn, 2016;</ref><ref type=""bibr"" target=""#b51"">Wu et al., 2018;</ref><ref type=""bibr"" target=""#b41"">Oord et al., 201 <head n=""2.2."">Training with Large Batch Size</head><p>We do not train the model with a memory bank <ref type=""bibr"" target=""#b51"">(Wu et al., 2018)</ref>. Instead, we vary the training batch size N f + − u T v − &lt; m else 0</formula><p>(2) linear projection, as used by several previous approaches <ref type=""bibr"" target=""#b51"">(Wu et al., 2018)</ref>; and (3) the default nonlinear projection wit proposes to treat each instance as a class represented by a feature vector (in a parametric form). <ref type=""bibr"" target=""#b51"">Wu et al. (2018)</ref> proposes to use a memory bank to store the ins",0
"pare the NT-Xent loss against other commonly used contrastive loss functions, such as logistic loss <ref type=""bibr"" target=""#b37"">(Mikolov et al., 2013)</ref>, and margin loss <ref type=""bibr"" target",0
"er contexts such as semi-supervised learning <ref type=""bibr"" target=""#b53"">(Xie et al., 2019;</ref><ref type=""bibr"" target=""#b4"">Berthelot et al., 2019)</ref>.</p><p>Handcrafted pretext tasks. The re",0
"=""bibr"" target=""#b51"">(Wu et al., 2018;</ref><ref type=""bibr"" target=""#b49"">Tian et al., 2019;</ref><ref type=""bibr"" target=""#b21"">He et al., 2019;</ref><ref type=""bibr"" target=""#b38"">Misra &amp; van <ref type=""bibr"">(Zhuang et al., 2019;</ref><ref type=""bibr"" target=""#b49"">Tian et al., 2019;</ref><ref type=""bibr"" target=""#b21"">He et al., 2019;</ref><ref type=""bibr"" target=""#b38"">Misra &amp; van ta augmentation.</p><p>• InstDisc, MoCo, PIRL <ref type=""bibr"" target=""#b51"">(Wu et al., 2018;</ref><ref type=""bibr"" target=""#b21"">He et al., 2019;</ref><ref type=""bibr"" target=""#b38"">Misra &amp; van and variance over all devices during the training. Other approaches include shuffling data examples <ref type=""bibr"" target=""#b21"">(He et al., 2019)</ref>, or replacing BN with layer norm <ref type=""b",0
"synthesis and physicochemical property prediction may overcome these limitations in the near future <ref type=""bibr"" target=""#b5"">(Coley et al., 2018;</ref><ref type=""bibr"" target=""#b16"">Gao et al., 2",1
"g dataset, and predicted toxicity using a deep neural network model trained on the ClinTox database <ref type=""bibr"" target=""#b17"">(Gayvert et al., 2016;</ref><ref type=""bibr"" target=""#b60"">Wu et al.,",0
"nrichment was performed using EcoCyc Pathway Tools <ref type=""bibr"" target=""#b22"">(Karp, 2001;</ref><ref type=""bibr"" target=""#b23"">Karp et al., 2016;</ref><ref type=""bibr"" target=""#b24"">Keseler et al.",0
"oli displaying resistance to the nitrofuran antibiotic nitrofurantoin via deletion of nfsA and nfsB <ref type=""bibr"" target=""#b44"">(Sandegren et al., 2008)</ref> (Figures <ref type=""figure"" target=""#f",0
"induce bacterial cell death against E. coli in a metabolically repressed, antibiotic-tolerant state <ref type=""bibr"" target=""#b1"">(Balaban et al., 2019;</ref><ref type=""bibr"">Stokes et al., 2019a</ref",0
"While previous learning simulation approaches <ref type=""bibr"" target=""#b17"">(Li et al., 2018;</ref><ref type=""bibr"" target=""#b33"">Ummenhofer et al., 2020)</ref> have been highly specialized for parti lution 3D water scenario with randomized water position, initial velocity and volume, comparable to <ref type=""bibr"" target=""#b33"">Ummenhofer et al. (2020)</ref>'s containers of water. We used SPlisHS d boundary particles, a loss function that weights slow particles with few neighbors more heavily). <ref type=""bibr"" target=""#b33"">Ummenhofer et al. (2020)</ref> reported CConv outperformed DPI, so we is to, during training, provide the model with its own predictions by rolling out short sequences. <ref type=""bibr"" target=""#b33"">Ummenhofer et al. (2020)</ref>, for example, train with two-step pred e=""bibr"" target=""#b14"">(Kipf &amp; Welling, 2016)</ref> work. The full CConv update as described in <ref type=""bibr"" target=""#b33"">Ummenhofer et al. (2020)</ref> is,</p><formula xml:id=""formula_19"">f e comparisons.</head><p>We implemented the CConv model, loss and training procedure as described by <ref type=""bibr"" target=""#b33"">Ummenhofer et al. (2020)</ref>. For simplicity, we only tested the CC appended a particle type learned embedding to the input node features.</p><p>To be consistent with <ref type=""bibr"" target=""#b33"">Ummenhofer et al. (2020)</ref>, we used their batch size of 16, learn ""><head>D. Supplementary baseline comparisons D.1. Continuous convolution (CConv)</head><p>Recently <ref type=""bibr"" target=""#b33"">Ummenhofer et al. (2020)</ref> presented Continuous Convolution (CCon eral tasks.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>Interpretation.</head><p>While <ref type=""bibr"" target=""#b33"">Ummenhofer et al. (2020)</ref> state that ""Unlike previous approaches",1
"ferent node, edge, and graph-level attributes, and can be trained to learn a form of messagepassing <ref type=""bibr"" target=""#b6"">(Gilmer et al., 2017)</ref>, where latent information is propagated be",0
"(PBD) <ref type=""bibr"" target=""#b23"">(Müller et al., 2007)</ref> and ""material point method"" (MPM) <ref type=""bibr"" target=""#b28"">(Sulsky et al., 1995)</ref>, are more suitable for interacting, defor",0
"get=""#b3"">(Han et al. 2004;</ref><ref type=""bibr"" target=""#b4"">Huang, Ertekin, and Giles 2006;</ref><ref type=""bibr"" target=""#b12"">Yoshida et al. 2010</ref>) usually leverage supervised learning algor t=""#b4"">Huang, Ertekin, and Giles 2006;</ref><ref type=""bibr"" target=""#b8"">Louppe et al. 2016;</ref><ref type=""bibr"" target=""#b12"">Yoshida et al. 2010)</ref>, which usually solve the problem in a disc name disambiguation task using content information, we construct a new dataset collected from AceKG <ref type=""bibr"" target=""#b12"">(Wang et al. 2018b</ref>). The benchmark dataset consists of 130,655",1
"thor. Thus constructing the network becomes the critical part of these methods, e.g., paper network <ref type=""bibr"" target=""#b13"">(Zhang et al. 2018)</ref>, paper-author network <ref type=""bibr"" targ periments:</p><p>• AMiner-AND<ref type=""foot"" target=""#foot_0"">1</ref> . The dataset is released by <ref type=""bibr"" target=""#b13"">(Zhang et al. 2018)</ref>, which contains 500 author names for traini e.g., paper network <ref type=""bibr"" target=""#b13"">(Zhang et al. 2018)</ref>, paper-author network <ref type=""bibr"" target=""#b13"">(Zhang and Al Hasan 2017)</ref>. However, either complicated feature (Zhang and Al Hasan 2017)</ref>. However, either complicated feature engineering or the supervision <ref type=""bibr"" target=""#b13"">(Zhang et al. 2018</ref>) is required.</p><p>The two categories of me fully designed pairwise features, including author names, titles, institute names etc.</p><p>AMiner <ref type=""bibr"" target=""#b13"">(Zhang et al. 2018</ref>): This model designs a supervised global sta D, we use 100 names for testing and compare the result with the results of other models reported in <ref type=""bibr"" target=""#b13"">(Zhang et al. 2018</ref>). In the experiment on AceKG-AND, we sample",1
"les and complicated feature engineering to some extent. Inspired by generative adversarial networks <ref type=""bibr"" target=""#b1"">(Goodfellow et al. 2014)</ref>, we may combine the two categories in a",0
"c. Methods focusing on the content information <ref type=""bibr"" target=""#b3"">(Han et al. 2004;</ref><ref type=""bibr"" target=""#b4"">Huang, Ertekin, and Giles 2006;</ref><ref type=""bibr"" target=""#b12"">Yo The first are based on the content information <ref type=""bibr"" target=""#b3"">(Han et al. 2004;</ref><ref type=""bibr"" target=""#b4"">Huang, Ertekin, and Giles 2006;</ref><ref type=""bibr"" target=""#b8"">Lou",0
"ey cannot measure the high-order connections among papers. Methods focusing on relation information <ref type=""bibr"" target=""#b5"">(Kanani, McCallum, and Pal 2007;</ref><ref type=""bibr"" target=""#b0"">Be",0
"ocusing on relation information <ref type=""bibr"" target=""#b5"">(Kanani, McCallum, and Pal 2007;</ref><ref type=""bibr"" target=""#b0"">Bekkerman and McCallum 2005)</ref> usually solve the problem on the bi",0
"ncludes title, abstract, introduction and keywords etc. Methods focusing on the content information <ref type=""bibr"" target=""#b3"">(Han et al. 2004;</ref><ref type=""bibr"" target=""#b4"">Huang, Ertekin, a tegories according to the information they focus on. The first are based on the content information <ref type=""bibr"" target=""#b3"">(Han et al. 2004;</ref><ref type=""bibr"" target=""#b4"">Huang, Ertekin, a",0
"ence evaluating and mentor recommendation, which raises the necessity of author name disambiguation <ref type=""bibr"" target=""#b9"">(Smalheiser and Torvik 2009)</ref>.</p><p>Author name disambiguation i e generator is guided to find the rules to select the homogeneous papers. To update θ G , we follow <ref type=""bibr"" target=""#b9"">(Schulman et al. 2015)</ref> to compute the gradient of V(G, D) by pol",0
"fline model lack fine-grained estimation and customized models are not general as desired. Timeloop <ref type=""bibr"" target=""#b21"">[21]</ref> and Eyeriss <ref type=""bibr"" target=""#b22"">[22]</ref> use",1
"werful performance often comes with a prohibitive complexity <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=",0
"get=""#b10"">11,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b13"">14]</ref>.</p><p>While DNN accelerators can be 1000× more efficient t is an early effort on synthesis based ASIC accelerators; Eyeriss proposes a row-stationary dataflow <ref type=""bibr"" target=""#b13"">[14]</ref> to reduce expensive DRAM accesses; and Google TPUs <ref ty cribe hardware architectures and depends on plug-ins, e.g., Timeloop, to calculate the energy as in <ref type=""bibr"" target=""#b13"">[14]</ref>. The work in <ref type=""bibr"" target=""#b24"">[24]</ref> ado ge for image processing applications, and proposes a modeling framework which is similar to that of <ref type=""bibr"" target=""#b13"">[14]</ref>. MAESTRO <ref type=""bibr"" target=""#b26"">[26]</ref> is the is easy to follow? For ease of use and better visualization, we adopt a nested for-loop description <ref type=""bibr"" target=""#b13"">[14]</ref> to describe the design space as shown in Fig. <ref type=""f roposed DNN-Chip Predictor by comparing its predicted performance with actual chip measured ones in <ref type=""bibr"" target=""#b13"">[14]</ref>, FPGA implementation results in <ref type=""bibr"" target=""# ""bibr"" target=""#b29"">[29]</ref> and our Predictor for AlexNet's CONV layers. normalized unit energy <ref type=""bibr"" target=""#b13"">[14]</ref>. First, Table <ref type=""table"" target=""#tab_0"">1</ref> co",0
"n intensive studies of DNN accelerators. For example, the first well-optimized FPGA DNN accelerator <ref type=""bibr"" target=""#b16"">[16]</ref> uses loop tiling; the DianNao series <ref type=""bibr"" targ ds have been developed for predict-ing or simulating DNN accelerators' performance. Roofline models <ref type=""bibr"" target=""#b16"">[16,</ref><ref type=""bibr"" target=""#b18"">18]</ref> and customized ana",0
"rget=""#b9"">10,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b13"">14]</ref>.</p><p>While DNN ac FPGA DNN accelerator <ref type=""bibr"" target=""#b16"">[16]</ref> uses loop tiling; the DianNao series <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b17"">17]</ref> is an early effort",0
"model with feature selection techniques to enhance the performance. The magnitude measure technique <ref type=""bibr"" target=""#b2"">[3]</ref> uses the absolute value of weights from a fully trained netw presented two techniques to reduce the number of features, namely filter method (magnitude measure) <ref type=""bibr"" target=""#b2"">[3]</ref> and embedded method ( -1 norm regularisation) <ref type=""bib ues that connects a hidden neuron j in the hidden layer into an output neuron k in the output layer <ref type=""bibr"" target=""#b2"">[3]</ref>. The following equation measures the contribution from an in data set described in Sect. 2.1, and removed two features with the lowest Q-values as suggested by <ref type=""bibr"" target=""#b2"">[3]</ref> to produce more consistent results. The network was re-train",1
"examined neural network models to perform video-based stress recognition using ANUStressDB data set <ref type=""bibr"" target=""#b5"">[6]</ref>. Recent works on video-based stress recognition <ref type=""b > <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2.1"">Data Set</head><p>The ANUStressDB data set <ref type=""bibr"" target=""#b5"">[6]</ref> consists of video samples from 24 subjects. Each video has a d 6-7% accuracy loss</p><p>The methods described in this paper are neural-based model. The paper in <ref type=""bibr"" target=""#b5"">[6]</ref> used Support Vector Machine (SVM) based model to perform the DB data set <ref type=""bibr"" target=""#b5"">[6]</ref>. Recent works on video-based stress recognition <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b10"">11]</ref> requires feature eng based stress recognition system incorporates feature engineering process before making a prediction <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b10"">11]</ref>. However, this proce",1
"ed in the model is cross-entropy error function. The model was trained using error back-propagation <ref type=""bibr"" target=""#b1"">[2]</ref> and optimised using Stochastic Gradient Descent (SGD) with m",0
"from original fully-connected layers. Instead of ReLU activation function, the model uses LeakyReLU <ref type=""bibr"" target=""#b6"">[7]</ref> with negative slope 0.1 to avoid dying ReLU problem due to z",0
"? R 7?34 transforms input vector into a vector with 7-dimension. Finally, ReLU activation function <ref type=""bibr"" target=""#b3"">[4]</ref> is applied to this 7-dimensional vector producing hidden vec",0
"sure the contribution of input features towards output values. The -1 norm regularisation technique <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b7"">8]</ref> is an embedded feature tude measure) <ref type=""bibr"" target=""#b2"">[3]</ref> and embedded method ( -1 norm regularisation) <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b7"">8]</ref>.</p><p>Magnitude Measu",0
"del using Feature Selection Technique, namely magnitude measure technique [3] and -1 regularisation <ref type=""bibr"" target=""#b8"">[9]</ref>. Subsequently, we performed extensive evaluation between tho ue used to bring weight of irrelevant inputs to 0, hence remove them from the model during training <ref type=""bibr"" target=""#b8"">[9]</ref>. Finally, we compared improved NN models with a recurrent ne ularisation using -1 norm discourages parameters with high sum of absolute values of the parameters <ref type=""bibr"" target=""#b8"">[9]</ref>, thus creating a sparse weight (parameter) matrix solution. on hence bring some parameters to zero makes this technique a good candidate for feature selection. <ref type=""bibr"" target=""#b8"">[9]</ref>. We applied -1 penalty term using following equation.</p><fo",0
"m due to zero gradient during back-propagation. To address overfitting on the model, dropout layers <ref type=""bibr"" target=""#b11"">[12]</ref> are used in the LSTM layer (p = 0.1) and both hidden layer",0
"towards output values. The -1 norm regularisation technique <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b7"">8]</ref> is an embedded feature selection technique used to bring weig #b2"">[3]</ref> and embedded method ( -1 norm regularisation) <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b7"">8]</ref>.</p><p>Magnitude Measure Technique. The magnitude measure tec",0
">[9]</ref>. Finally, we compared improved NN models with a recurrent neural network model with LSTM <ref type=""bibr"" target=""#b4"">[5]</ref> to perform video-based stress recognition task. The LSTM mod",0
"xmlns=""http://www.tei-c.org/ns/1.0""><head n=""1"">INTRODUCTION</head><p>The Transformer architecture <ref type=""bibr"" target=""#b23"">(Vaswani et al., 2017)</ref> is widely used in natural language proce oduct attention. The standard attention used in the Transformer is the scaled dot-product attention <ref type=""bibr"" target=""#b23"">(Vaswani et al., 2017)</ref>. The input consists of queries and keys ""http://www.tei-c.org/ns/1.0""><head n=""4"">RELATED WORK</head><p>The Transformer model introduced in <ref type=""bibr"" target=""#b23"">(Vaswani et al., 2017)</ref> has been used widely in natural language also evaluate on the WMT 2014 English-to-German translation task, following the hyperparameters of <ref type=""bibr"" target=""#b23"">Vaswani et al. (2017)</ref>. Training for all experiments  Effect of he right in Figure <ref type=""figure"" target=""#fig_2"">3</ref>, we compare a regular Transformer per <ref type=""bibr"" target=""#b23"">Vaswani et al. (2017)</ref> with the reversible one describe in Secti ible in the Transformer-base architecture, and  see that the resulting model performs comparably to <ref type=""bibr"" target=""#b23"">Vaswani et al. (2017)</ref> when trained for 100K steps. We also eval",1
"olves these problems using the following techniques:</p><p>• Reversible layers, first introduced in <ref type=""bibr"" target=""#b7"">Gomez et al. (2017)</ref>, enable storing only a single copy of activa term: the b • n h • l • d k ,</formula><p>RevNets. Reversible residual networks were introduced by <ref type=""bibr"" target=""#b7"">Gomez et al. (2017)</ref> where it was shown that they can replace Res mer model's self-attention mechanism <ref type=""bibr"" target=""#b21"">(Sukhbaatar et al., 2019a;</ref><ref type=""bibr"" target=""#b7"">b)</ref> have also recently been explored.</p><p>In particular, levera",1
"increasingly long sequences. Up to 11 thousand tokens of text in a single example were processed in <ref type=""bibr"" target=""#b11"">(Liu et al., 2018)</ref> and when processing other modalities, like m",0
"""#b8"">Hill et al. (2015)</ref>. The requirement that the memory be fixed before has been removed in <ref type=""bibr"" target=""#b17"">Santoro et al. (2016)</ref> at the cost of memory size and later alle",0
"here [u; v]</formula><p>denotes the concatenation of two vectors. This method is a known LSH scheme <ref type=""bibr"" target=""#b1"">(Andoni et al., 2015)</ref> and is easy to implement and apply to batc",0
"re either given as additional supervising information by the task or determined heuristically as in <ref type=""bibr"" target=""#b8"">Hill et al. (2015)</ref>. The requirement that the memory be fixed bef",0
"raining of extremely large language models <ref type=""bibr"" target=""#b6"">(Devlin et al., 2018;</ref><ref type=""bibr"" target=""#b14"">Radford et al., 2019)</ref>.</p><p>Given the enormous computational r",0
"rmer models. In addition to standard methods such as precision reduction and gradient checkpointing <ref type=""bibr"" target=""#b20"">(Sohoni et al., 2019)</ref>, more efficient versions of the Transform",0
"type=""bibr"" target=""#b11"">(Liu et al., 2018)</ref> and when processing other modalities, like music <ref type=""bibr"" target=""#b9"">(Huang et al., 2018)</ref> and images <ref type=""bibr"" target=""#b13"">( ed widely in natural language tasks and further extended to model diverse data such as music scores <ref type=""bibr"" target=""#b9"">(Huang et al., 2018)</ref>, and images <ref type=""bibr"" target=""#b13"">",0
"target=""#b13"">Gotmare et al., 2019;</ref><ref type=""bibr"" target=""#b3"">Bernstein et al., 2018;</ref><ref type=""bibr"" target=""#b30"">Xiao et al., 2017)</ref>. We notice that r t has a similar form to th",0
"ximate the distribution of the exponential moving average as the distribution of the simple average <ref type=""bibr"" target=""#b23"">(Nau, 2014)</ref>, i.e., p(ψ(.)) = p(</p><formula xml:id=""formula_5""> (EMA) can be interpreted as an approximation to the simple moving average (SMA) in real application <ref type=""bibr"" target=""#b23"">(Nau, 2014)</ref>, i.e.,</p><formula xml:id=""formula_10"">p (1 − β 2 )",0
"=""#b7"">(Chen &amp; Gu, 2018)</ref>, or adding range constraints (Luo et al., 2019)), initialization <ref type=""bibr"" target=""#b1"">(Balduzzi et al., 2017;</ref><ref type=""bibr"" target=""#b32"">Zhang et a at the beginning of the training, since weights are sampled from normal distributions with mean zero<ref type=""bibr"" target=""#b1"">(Balduzzi et al., 2017)</ref>, further analysis is conducted in Sectio",0
"hat generations of researchers have been pursuing <ref type=""bibr"" target=""#b12"">(Gauss, 1823;</ref><ref type=""bibr"" target=""#b4"">Cauchy, 1847)</ref>. Remarkably, stochastic gradient-based optimizatio",0
"<ref type=""bibr"" target=""#b28"">(Vaswani et al., 2017)</ref> implementation from the fairseq package <ref type=""bibr"" target=""#b24"">(Ott et al., 2019)</ref>. Specifically, we use word embedding with 51",0
"ei-c.org/ns/1.0""><head>B.2 IMAGEINE CLASSIFICATION</head><p>We use the default ResNet architectures <ref type=""bibr"" target=""#b15"">(He et al., 2016)</ref> in a public pytorch re-implementation<ref typ",0
"g the variance of the adaptive learning rate, and also explains why tuning is important in practice <ref type=""bibr"" target=""#b21"">(Liu et al., 2019)</ref>. Besides, similar to Adam-2k, it prevents th",0
"s on Graph Convolutional Networks (GCNs) <ref type=""bibr"" target=""#b9"">(Hamilton et al., 2017;</ref><ref type=""bibr"" target=""#b4"">Chen et al., 2018b;</ref><ref type=""bibr"" target=""#b8"">Gao et al., 201 large graphs, layer sampling techniques <ref type=""bibr"" target=""#b9"">(Hamilton et al., 2017;</ref><ref type=""bibr"" target=""#b4"">Chen et al., 2018b;</ref><ref type=""bibr"" target=""#b26"">Ying et al., 2 ly a small number of neighbors (typically from 2 to 50) are selected by one node in the next layer. <ref type=""bibr"" target=""#b4"">Chen et al. (2018b)</ref> and <ref type=""bibr"" target=""#b14"">Huang et he above edge sampler to perform layer sampling. Under the independent layer sampling assumption of <ref type=""bibr"" target=""#b4"">Chen et al. (2018b)</ref>, one would sample a connection u ( ) , v ( + b3"">Chen et al. (2018a)</ref>. Point (2) is due to the better interlayer connectivity compared with <ref type=""bibr"" target=""#b4"">Chen et al. (2018b)</ref>, and unbiased minibatch estimator compared w s to use the historical activations in the previous layer to avoid redundant re-evaluation. FastGCN <ref type=""bibr"" target=""#b4"">(Chen et al., 2018b)</ref> performs sampling from another perspective. , 2016)</ref>, 2. GraphSAGE <ref type=""bibr"" target=""#b9"">(Hamilton et al., 2017)</ref>, 3. FastGCN <ref type=""bibr"" target=""#b4"">(Chen et al., 2018b)</ref>, 4. S-GCN <ref type=""bibr"" target=""#b3"">(Ch of each layer independently. This is similar to the treatment of layers independently by prior work <ref type=""bibr"" target=""#b4"">(Chen et al., 2018b;</ref><ref type=""bibr"" target=""#b14"">Huang et al., ip connections to GraphSAINT is straightforward. On the other hand, for some layer sampling methods <ref type=""bibr"" target=""#b4"">(Chen et al., 2018b;</ref><ref type=""bibr"" target=""#b14"">Huang et al.,",1
"lar GCN architectures such as JK-net <ref type=""bibr"" target=""#b25"">(Xu et al., 2018)</ref> and GAT <ref type=""bibr"" target=""#b23"">(Veličković et al., 2017)</ref>. The resulting deep models achieve ne In Figure <ref type=""figure"" target=""#fig_3"">4</ref>, we train a 2-layer and a 4-layer model of GAT <ref type=""bibr"" target=""#b23"">(Veličković et al., 2017)</ref> and JK-net <ref type=""bibr"" target=""# b11"">(Hochreiter &amp; Schmidhuber, 1997)</ref> based aggregation.</p><p>The graph attention of GAT <ref type=""bibr"" target=""#b23"">(Veličković et al., 2017)</ref> calculates the edge weights for neigh of research focuses on improving model capacity. Applying attention on graphs, the architectures of <ref type=""bibr"" target=""#b23"">Veličković et al. (2017)</ref>; <ref type=""bibr"" target=""#b30"">Zhang x v (9)</formula><p>Note that the α calculation is slightly different from the original equation in <ref type=""bibr"" target=""#b23"">Veličković et al. (2017)</ref>. Namely, GAT-SAINT does not normalize nowledge architecture requires layersamples to be a subset of layer-( − 1) samples * . 2. Attention <ref type=""bibr"" target=""#b23"">(Veličković et al., 2017;</ref><ref type=""bibr"" target=""#b7"">Fey, 201",0
">A neural network model that extends convolution operation to the graph domain is first proposed by <ref type=""bibr"" target=""#b1"">Bruna et al. (2013)</ref>. Further, <ref type=""bibr"" target=""#b16"">Kip",0
"nstead of sampling layers, the works of <ref type=""bibr"" target=""#b29"">Zeng et al. (2018)</ref> and <ref type=""bibr"" target=""#b5"">Chiang et al. (2019)</ref> build minibatches from subgraphs. <ref type ed. Similar to GraphSAINT, the works of <ref type=""bibr"" target=""#b29"">Zeng et al. (2018)</ref> and <ref type=""bibr"" target=""#b5"">Chiang et al. (2019)</ref> do not sample the layers and thus ""neighbor type=""bibr"" target=""#b4"">Chen et al. (2018b)</ref>, and unbiased minibatch estimator compared with <ref type=""bibr"" target=""#b5"">Chiang et al. (2019)</ref>. Point (3) is due to the simple and trivial red with the sampling of <ref type=""bibr"" target=""#b14"">Huang et al. (2018)</ref> and clustering of <ref type=""bibr"" target=""#b5"">Chiang et al. (2019)</ref>.</p></div> <div xmlns=""http://www.tei-c.org techniques to scale such training on shared-memory multi-core platforms. More recently, ClusterGCN <ref type=""bibr"" target=""#b5"">(Chiang et al., 2019)</ref> proposes graph clustering based minibatch 018a)</ref>, 5. AS-GCN <ref type=""bibr"" target=""#b14"">(Huang et al., 2018)</ref>, and 6. ClusterGCN <ref type=""bibr"" target=""#b5"">(Chiang et al., 2019)</ref>. All baselines are executed with their off and hidden dimension, respectively. The four architectures are the ones used in the original paper <ref type=""bibr"" target=""#b5"">(Chiang et al., 2019)</ref>. Again, GraphSAINT achieves significant ac",0
"NN) model to extract user and item latent features from their respective nearest-neighbor networks. <ref type=""bibr"" target=""#b1"">Berg et al. (2017)</ref> propose graph convolutional matrix completion ween content and graph structures in the early graph convolution stage.</p><p>On the other hand, as <ref type=""bibr"" target=""#b1"">Berg et al. (2017)</ref> and <ref type=""bibr"">Zhang &amp; Chen (2018)< the bipartite graph. Our node labeling is also different from using the global node IDs as in GC-MC <ref type=""bibr"" target=""#b1"">(Berg et al., 2017)</ref>. Using one-hot encoding of global IDs is ess "">(Rao et al., 2015)</ref>, sRGCNN <ref type=""bibr"" target=""#b29"">(Monti et al., 2017)</ref>, GC-MC <ref type=""bibr"" target=""#b1"">(Berg et al., 2017)</ref>, F-EAE <ref type=""bibr"" target=""#b13"">(Hartf t user and item's content vectors with the final graph representation output by the GNN, similar to <ref type=""bibr"" target=""#b1"">(Berg et al., 2017)</ref>. However, this method fails to model the int feature vectors to the graph representation g before feeding into the MLP (4), which is similar to <ref type=""bibr"" target=""#b1"">(Berg et al., 2017)</ref>. The results are shown in Table <ref type=""t reflected in that previous node-based approaches mainly use only one or two message passing layers <ref type=""bibr"" target=""#b1"">(Berg et al., 2017;</ref><ref type=""bibr"" target=""#b39"">Ying et al., 2",1
"side information) of users and items <ref type=""bibr"" target=""#b14"">(Jain &amp; Dhillon, 2013;</ref><ref type=""bibr"" target=""#b38"">Xu et al., 2013)</ref>. In IMC, a rating is decomposed by r ij = x i",1
"head>GNNs for matrix completion</head><p>The matrix completion problem has been studied using GNNs. <ref type=""bibr"" target=""#b29"">Monti et al. (2017)</ref> develop a multi-graph CNN (MGCNN) model to st sets. For Flixster, Douban and YahooMusic we use the preprocessed subsets and splits provided by <ref type=""bibr"" target=""#b29"">(Monti et al., 2017)</ref>. Dataset statistics are summarized in Tabl ets, we compare our IGMC with GRALS <ref type=""bibr"" target=""#b31"">(Rao et al., 2015)</ref>, sRGCNN <ref type=""bibr"" target=""#b29"">(Monti et al., 2017)</ref>, GC-MC <ref type=""bibr"" target=""#b1"">(Berg",0
"&amp; Ester, 2010)</ref>, Douban <ref type=""bibr"" target=""#b26"">(Ma et al., 2011)</ref>, YahooMusic <ref type=""bibr"" target=""#b8"">(Dror et al., 2011)</ref>, MovieLens-100K and MovieLens-1M <ref type=""",0
"al., 2007)</ref>, NNMF <ref type=""bibr"" target=""#b10"">(Dziugaite &amp; Roy, 2015)</ref>, I-AutoRec <ref type=""bibr"" target=""#b36"">(Sedhain et al., 2015)</ref> and CF-NADE <ref type=""bibr"" target=""#b4",0
"cannot generalize to unseen users/items. A recent inductive graphbased recommender system, PinSage <ref type=""bibr"" target=""#b39"">(Ying et al., 2018a)</ref>, uses node content as initial node feature t al., 2017)</ref>, F-EAE <ref type=""bibr"" target=""#b13"">(Hartford et al., 2018)</ref>, and PinSage <ref type=""bibr"" target=""#b39"">(Ying et al., 2018a)</ref>. Among them, GRALS is a graph regularized y use only one or two message passing layers <ref type=""bibr"" target=""#b1"">(Berg et al., 2017;</ref><ref type=""bibr"" target=""#b39"">Ying et al., 2018a)</ref>.</p></div> <div xmlns=""http://www.tei-c.org",0
"ural networks for learning over graphs <ref type=""bibr"" target=""#b33"">(Scarselli et al., 2009;</ref><ref type=""bibr"" target=""#b3"">Bruna et al., 2013;</ref><ref type=""bibr"" target=""#b9"">Duvenaud et al.",0
"get=""#b19"">Kipf &amp; Welling, 2016;</ref><ref type=""bibr"" target=""#b30"">Niepert et al., 2016;</ref><ref type=""bibr"" target=""#b6"">Dai et al., 2016)</ref>. There are two types of GNNs: Nodelevel GNNs u",0
"r"" target=""#b34"">Schafer et al., 2007;</ref><ref type=""bibr"" target=""#b20"">Koren et al., 2009;</ref><ref type=""bibr"" target=""#b2"">Bobadilla et al., 2013)</ref> However, matrix factorization is intrins",0
"ill only extend the convolution range to unrelated distant nodes and oversmooth the node embeddings <ref type=""bibr"" target=""#b21"">(Li et al., 2018)</ref>. This is reflected in that previous node-base",0
""">(Adomavicius &amp; Tuzhilin, 2005;</ref><ref type=""bibr"" target=""#b34"">Schafer et al., 2007;</ref><ref type=""bibr"" target=""#b20"">Koren et al., 2009;</ref><ref type=""bibr"" target=""#b2"">Bobadilla et a",0
"The use of deep pre-trained transformers has led to remarkable progress in a number of applications <ref type=""bibr"" target=""#b5"">(Devlin et al., 2019)</ref>. For tasks that make pairwise comparisons asks have been achieved through the use of deep pre-trained language models followed by fine-tuning <ref type=""bibr"" target=""#b5"">(Devlin et al., 2019)</ref>. In this work we explore improvements to t rohibitively slow.</p><p>The current state-of-the-art focuses on using BERT models for pre-training <ref type=""bibr"" target=""#b5"">(Devlin et al., 2019)</ref>, which employ large text corpora on genera based on large pre-trained transformer models with the same architecture and dimension as BERT-base <ref type=""bibr"" target=""#b5"">(Devlin et al., 2019)</ref>, which has 12 layers, 12 attention heads, >We tried two optimizers: Adam (Kingma &amp; Ba, 2015) with weight decay of 0.01 (as recommended by <ref type=""bibr"" target=""#b5"">(Devlin et al., 2019)</ref>) and Adamax (Kingma &amp; Ba, 2015) withou "">3</ref> we show validation performance when fine-tuning various layers of the weights provided by <ref type=""bibr"" target=""#b5"">(Devlin et al., 2019)</ref>, using Adam with decay optimizer. Fine-tun and the label is the next utterance.</p><p>When pre-training on Wikipedia and Toronto Books, as in <ref type=""bibr"" target=""#b5"">Devlin et al. (2019)</ref>, the input is one sentence and the label th training strategy involves training with a masked language model (MLM) task identical to the one in <ref type=""bibr"" target=""#b5"">Devlin et al. (2019)</ref>. In the pre-training on Wikipedia and Toron stigate fine-tuning the Bi-and Cross-encoder architectures initialized with the weights provided by <ref type=""bibr"" target=""#b5"">Devlin et al. (2019)</ref>, studying the choice of other hyperparamete",1
"coders give slightly better results.</p><p>We note that since reporting our results, the authors of <ref type=""bibr"" target=""#b14"">Li et al. (2019)</ref> have conducted a human evaluation study on Con",0
"their similarity. We refer to these models as Bi-encoders. Such methods include vector space models <ref type=""bibr"" target=""#b17"">(Salton et al., 1975)</ref>, LSI <ref type=""bibr"" target=""#b4"">(Deerw",0
"l., 2009;</ref><ref type=""bibr"" target=""#b21"">Wu et al., 2018)</ref> and classical siamese networks <ref type=""bibr"" target=""#b1"">(Bromley et al., 1994)</ref>. For the next utterance prediction tasks",0
"75)</ref>, LSI <ref type=""bibr"" target=""#b4"">(Deerwester et al., 1990)</ref>, supervised embeddings <ref type=""bibr"" target=""#b0"">(Bai et al., 2009;</ref><ref type=""bibr"" target=""#b21"">Wu et al., 2018",0
"core. In particular, standard likelihood training and decoding leads to dull and repetitive outputs <ref type=""bibr"" target=""#b10"">(Holtzman et al., 2019)</ref>. While some post-hoc fixes have been pr </ref><ref type=""bibr"" target=""#b25"">Vig, 2018)</ref>, (ii) an intrinsic property of human language <ref type=""bibr"" target=""#b10"">(Holtzman et al., 2019)</ref> rather than a modeling deficiency, or t on time. Top k-sampling <ref type=""bibr"" target=""#b7"">(Fan et al., 2018)</ref> and nucleus sampling <ref type=""bibr"" target=""#b10"">(Holtzman et al., 2019)</ref> are two methods that sample sequences b often prefers semantically similar phrasing, depending on the temperature parameter of the sampling <ref type=""bibr"" target=""#b10"">(Holtzman et al., 2019)</ref>. Furthermore, this solution is less rel mass above a threshold p ∈ [0, 1]; i.e. U is the smallest subset with x∈U p θ (x|x &lt;t ) &gt;= p <ref type=""bibr"" target=""#b10"">(Holtzman et al., 2019)</ref>.</p></div> <div xmlns=""http://www.tei-c lidation corpus.</p><p>Unlike previous work which only focused on degenerate sequence-level repeats <ref type=""bibr"" target=""#b10"">(Holtzman et al., 2019)</ref>, we additionally observe that neural la ted with top-k sampling <ref type=""bibr"" target=""#b7"">(Fan et al., 2018)</ref> and nucleus sampling <ref type=""bibr"" target=""#b10"">(Holtzman et al., 2019)</ref>. Models trained with unlikelihood objec to be dull, with high frequency tokens used too often and interesting content words used too rarely <ref type=""bibr"" target=""#b10"">(Holtzman et al., 2019;</ref><ref type=""bibr"" target=""#b5"">Dinan et a ng (i) a by-product of the model architecture, e.g. the Transformer architecture preferring repeats <ref type=""bibr"" target=""#b10"">(Holtzman et al., 2019;</ref><ref type=""bibr"" target=""#b25"">Vig, 2018 etition <ref type=""bibr"" target=""#b5"">(Dinan et al., 2019)</ref>. In language modeling, the work of <ref type=""bibr"" target=""#b10"">Holtzman et al. (2019)</ref> highlighted problems with the word frequ ional neural text generators have different token distributions from human text. As demonstrated by <ref type=""bibr"" target=""#b10"">Holtzman et al. (2019)</ref>, such models with greedy or beam search",1
"m . On March 7 , 2012 , he was named one of five finalists for the Naismith Award , which is 0.064  <ref type=""bibr"" target=""#b0"">(Baevski and Auli, 2019;</ref><ref type=""bibr"" target=""#b19"">Radford e",1
"hat are unlike previously chosen ones. Separately, hard or soft beam blocking has been investigated <ref type=""bibr"" target=""#b18"">(Paulus et al., 2017;</ref><ref type=""bibr"" target=""#b11"">Klein et al",0
"algorithms with neural language models have been applied to machine translation in recent years by <ref type=""bibr"" target=""#b23"">Shen et al. (2015)</ref> and <ref type=""bibr"" target=""#b6"">Edunov et",0
"ing objective relying on fixed corpora cannot take into account the real goal of using the language <ref type=""bibr"" target=""#b1"">(Choi, 2018)</ref>. Our work shows that, while the above may be factor",0
"s often generated by the model itself. Some representative algorithms include structured perceptron <ref type=""bibr"" target=""#b2"">(Collins, 2002)</ref>, energy-based models <ref type=""bibr"" target=""#b",0
"s <ref type=""bibr"" target=""#b13"">(LeCun et al., 2006)</ref> and more recently reflective likelihood <ref type=""bibr"" target=""#b4"">(Dieng et al., 2018)</ref>. A particular variant in this family of alg",0
"plications. However, the standard approach -training a sequence to sequence model, e.g. Transformer <ref type=""bibr"" target=""#b24"">(Vaswani et al., 2017)</ref>, to maximize log-likelihood and approxim s are based on the Transformer architecture, a multi-layer feed-forward network with self-attention <ref type=""bibr"" target=""#b24"">(Vaswani et al., 2017)</ref>. We use a 16-layer Transformer with 8 at",0
"sformer architecture preferring repeats <ref type=""bibr"" target=""#b10"">(Holtzman et al., 2019;</ref><ref type=""bibr"" target=""#b25"">Vig, 2018)</ref>, (ii) an intrinsic property of human language <ref t",0
"s that operate on model-generated sequences <ref type=""bibr"" target=""#b3"">(Daumé et al., 2009;</ref><ref type=""bibr"" target=""#b21"">Ross et al., 2011;</ref><ref type=""bibr"" target=""#b20"">Ranzato et al.",0
"ing features. For example, GAT <ref type=""bibr"" target=""#b27"">(Velickovic et al., 2017)</ref>, LGCL <ref type=""bibr"" target=""#b4"">(Gao et al., 2018)</ref> and GG-NN <ref type=""bibr"" target=""#b11"">(Li",0
"ture of descriptors in manifold geometry <ref type=""bibr"" target=""#b9"">(Kokkinos et al., 2012;</ref><ref type=""bibr"" target=""#b13"">Monti et al., 2017)</ref>, thereby preserving more and richer structu",0
"topology patterns in graphs can be preserved and presented as intuitive geometry, such as subgraph <ref type=""bibr"" target=""#b16"">(Narayanan et al., 2016</ref><ref type=""bibr"">), community (Ni et al.",0
"or generation purposes, when at test time the model needs to generate from scratch.</p><p>Recently, <ref type=""bibr"" target=""#b1"">Bakhtin et al. (2019)</ref> carefully studied the problem of training iminator can generalize rather well to weaker language models when the training/test corpora match. <ref type=""bibr"" target=""#b1"">Bakhtin et al. (2019)</ref> found that the learned discriminator is no ><p>In this work, we build upon these two works. First, we formalize the residual interpretation by <ref type=""bibr"" target=""#b1"">Bakhtin et al. (2019)</ref> and use a generative model of the form:</p ur work, the pretrained locally normalized language model can be seen as a fixed generator, like in <ref type=""bibr"" target=""#b1"">Bakhtin et al. (2019)</ref>. <ref type=""bibr"" target=""#b0"">Azadi et al language modeling. Second, the language model provides a natural proposal distribution for training <ref type=""bibr"" target=""#b1"">(Bakhtin et al., 2019)</ref>, and training can be made efficient by us ""#b16"">(Zhu et al., 2015;</ref><ref type=""bibr"" target=""#b16"">Kiros et al., 2015)</ref> and CC-News <ref type=""bibr"" target=""#b1"">(Bakhtin et al., 2019)</ref>. The former dataset consists of fiction b",1
"argued that the discriminator operates in the ""residual"" space of the language model. Concurrently, <ref type=""bibr"" target=""#b9"">Grover et al. (2019)</ref> proposed a general approach to ""de-bias"" a sampling from the generator by using the discriminator for rejection sampling. Similar to our work, <ref type=""bibr"" target=""#b9"">Grover et al. (2019)</ref> propose to use the discriminator to de-bias neration via importance sampling <ref type=""bibr"" target=""#b15"">(Horvitz &amp; Thompson, 1952;</ref><ref type=""bibr"" target=""#b9"">Grover et al., 2019)</ref>.</p><p>In some sense, this last point is pe enerate efficiently, we use self-normalizing importance sampling <ref type=""bibr"">(Owen, 2013;</ref><ref type=""bibr"" target=""#b9"">Grover et al., 2019)</ref>. Under the assumptions that the model from",1
"surrogate ranking losses. This approach is also related to other sequence level training objectives <ref type=""bibr"" target=""#b6"">(Edunov et al., 2018)</ref>, with the major differ-ence that in those",0
"s have been used for sequence modeling <ref type=""bibr"" target=""#b27"">(Rosenfeld et al., 2001;</ref><ref type=""bibr"" target=""#b35"">Wang et al., 2015;</ref><ref type=""bibr"">2017;</ref><ref type=""bibr"">",0
"b35"">Wang et al., 2015;</ref><ref type=""bibr"">2017;</ref><ref type=""bibr"">Wang &amp; Ou, 2017;</ref><ref type=""bibr"" target=""#b33"">2018a;</ref><ref type=""bibr"" target=""#b23"">Parshakova et al., 2019)</",0
"The dominant approach to parametric text generation is based on large neural auto-regressive models <ref type=""bibr"" target=""#b24"">(Radford et al., 2019)</ref>. These models can be trained efficiently to-regressive language models trained on large datasets have improved significantly in recent years <ref type=""bibr"" target=""#b24"">(Radford et al., 2019)</ref>. In general however, residual learning a",0
"s, both in terms of perplexity, and through human evaluation.</p><p>Generative Adversarial Networks <ref type=""bibr"" target=""#b8"">(Goodfellow et al., 2014</ref>) also relate to EBMs, except that in EB",0
"model distribution, which is usually approximated with Monte Carlo sampling or mean field inference <ref type=""bibr"" target=""#b12"">(Hinton, 2012;</ref><ref type=""bibr"" target=""#b17"">LeCun et al., 2006",0
"imilar to discriminative reranking approaches used in the parsing and machine translation community <ref type=""bibr"" target=""#b29"">(Shen et al., 2004)</ref>. However, our approach provides a generativ",0
"words. The latter is a de-duplicated subset of the English portion of the CommonCrawl news dataset <ref type=""bibr"" target=""#b20"">(Nagel, 2016)</ref>, which totals around 16 Billion words. The book c",0
"to compute with modern hardware given the limited vocabulary size of common sub-word units like BPE <ref type=""bibr"" target=""#b28"">(Sennrich et al., 2015)</ref>.</p><p>Unfortunately, local normalizati",0
"Several variants of auto-encoders have also been investigated for representing and generating text <ref type=""bibr"" target=""#b3"">(Bowman et al., 2016;</ref><ref type=""bibr"">Zhao et al., 2018)</ref>,",0
"EBM, a transformer language model with 12 layers, h = 16, d model = 1024, d f f = 4096 (we refer to <ref type=""bibr"" target=""#b31"">Vaswani et al. (2017)</ref> for notations). This is also our first ba itecture We consider two architectures for our residual EBM, both of them are based on transformers <ref type=""bibr"" target=""#b31"">(Vaswani et al., 2017;</ref><ref type=""bibr"" target=""#b4"">Devlin et a",0
"e=""foot"" n=""3"" xml:id=""foot_1"">Adapting to other types of local constraints such as nucleus sampling<ref type=""bibr"" target=""#b13"">(Holtzman et al., 2019)</ref> is straightforward.</note> 			<note xml",0
"t=""#b16"">Phang et al., 2018)</ref>.</p><p>When finetuning a big, pretrained language model, dropout <ref type=""bibr"" target=""#b20"">(Srivastava et al., 2014)</ref> has been used as a regularization tec er of that neuron as w during training, then we use (1 − p)w for that weight parameter at test time <ref type=""bibr"" target=""#b20"">(Srivastava et al., 2014)</ref>. This ensures that the expected outpu ght decay of λ is equivalent to wdecay(0, λ).</p><p>Probability for Dropout and Dropconnect Dropout <ref type=""bibr"" target=""#b20"">(Srivastava et al., 2014</ref>) is a regularization technique selecti",1
"""#b26"">Wiese et al. (2017)</ref>,<ref type=""bibr"" target=""#b12"">Kirkpatrick et al. (2017)</ref>, and<ref type=""bibr"" target=""#b17"">Schwarz et al. (2018)</ref> used L 2 -penalty toward a pretrained mod",0
"ate the effect of L 2 -penalty toward the pretrained model parameter on the stability of finetuning.<ref type=""bibr"" target=""#b1"">Barone et al. (2017)</ref> introduced tuneout, which is a special case",0
"ian dropout <ref type=""bibr"" target=""#b24"">(Wang &amp; Manning, 2013)</ref> and variational dropout <ref type=""bibr"" target=""#b11"">(Kingma et al., 2015)</ref> use other random masks to improve dropout",0
"se approach <ref type=""bibr"" target=""#b2"">(Chen et al., 2018)</ref> and its layer-dependent variant <ref type=""bibr"" target=""#b13"">(Huang et al., 2018)</ref>. Specifically, GAT <ref type=""bibr"" target on et al., 2017)</ref>, FastGCN <ref type=""bibr"" target=""#b2"">(Chen et al., 2018)</ref>, and AS-GCN <ref type=""bibr"" target=""#b13"">(Huang et al., 2018)</ref>. We name this category of approaches as Dr e <ref type=""table"" target=""#tab_1"">2</ref>; for the SOTA methods, we reuse the results reported in <ref type=""bibr"" target=""#b13"">Huang et al. (2018)</ref>.</p><p>We have these findings: (1) Clearly, ing the testing nodes are unseen for training. We apply the full-supervised training fashion used in<ref type=""bibr"" target=""#b13"">Huang et al. (2018)</ref> and<ref type=""bibr"" target=""#b2"">Chen et al",1
"et=""#b3"">Defferrard et al. (2016)</ref>; <ref type=""bibr"" target=""#b10"">Henaff et al. (2015)</ref>; <ref type=""bibr"" target=""#b19"">Li et al. (2018b)</ref>; <ref type=""bibr"" target=""#b16"">Levie et al.",0
"mpling methods <ref type=""bibr"" target=""#b8"">(Hamilton et al., 2017)</ref>, the layer-wise approach <ref type=""bibr"" target=""#b2"">(Chen et al., 2018)</ref> and its layer-dependent variant <ref type=""b d methods, including GraphSAGE <ref type=""bibr"" target=""#b8"">(Hamilton et al., 2017)</ref>, FastGCN <ref type=""bibr"" target=""#b2"">(Chen et al., 2018)</ref>, and AS-GCN <ref type=""bibr"" target=""#b13"">( full-supervised training fashion used in<ref type=""bibr"" target=""#b13"">Huang et al. (2018)</ref> and<ref type=""bibr"" target=""#b2"">Chen et al. (2018)</ref> on all datasets in our experiments. The stati",0
"s, have boosted the state-of-the-arts for a variety of tasks on graphs, such as node classification <ref type=""bibr"" target=""#b0"">(Bhagat et al., 2011;</ref><ref type=""bibr"" target=""#b34"">Zhang et al.",0
"target=""#b14"">(Kipf &amp; Welling, 2017;</ref><ref type=""bibr"" target=""#b18"">Li et al., 2018a;</ref><ref type=""bibr"" target=""#b32"">Xu et al., 2018a;</ref><ref type=""bibr"" target=""#b17"">Li et al., 2019 t al. (2018a)</ref> and further explained in <ref type=""bibr"" target=""#b31"">Wu et al. (2019)</ref>; <ref type=""bibr"" target=""#b32"">Xu et al. (2018a)</ref>; <ref type=""bibr"" target=""#b15"">Klicpera et a information loss caused by it.</p><p>We are also aware that the dense connections employed by JKNet <ref type=""bibr"" target=""#b32"">(Xu et al., 2018a)</ref> are another kind of tools that can potential pf &amp; Welling, 2017)</ref>, ResGCN <ref type=""bibr"" target=""#b17"">(Li et al., 2019)</ref>, JKNet <ref type=""bibr"" target=""#b32"">(Xu et al., 2018a)</ref>, and GraphSAGE <ref type=""bibr"" target=""#b8"" ng loop; however, the accuracy is still observed to decrease when the depth increases from 2. JKNet <ref type=""bibr"" target=""#b32"">(Xu et al., 2018a)</ref> employs dense connections for multi-hop mess r"" target=""#b9"">(He et al., 2016;</ref><ref type=""bibr"" target=""#b17"">Li et al., 2019)</ref>, JKNet <ref type=""bibr"" target=""#b32"">(Xu et al., 2018a)</ref>, IncepGCN<ref type=""foot"" target=""#foot_2"">4",0
"i et al., 2019)</ref>, JKNet <ref type=""bibr"" target=""#b32"">(Xu et al., 2018a)</ref>, and GraphSAGE <ref type=""bibr"" target=""#b8"">(Hamilton et al., 2017)</ref>. We provide detailed evaluations in the have been proposed for fast graph representation learning, including the node-wise sampling methods <ref type=""bibr"" target=""#b8"">(Hamilton et al., 2017)</ref>, the layer-wise approach <ref type=""bibr opNode Another related vein belongs to the kind of node sampling based methods, including GraphSAGE <ref type=""bibr"" target=""#b8"">(Hamilton et al., 2017)</ref>, FastGCN <ref type=""bibr"" target=""#b2"">( et=""#b32"">(Xu et al., 2018a)</ref>, IncepGCN<ref type=""foot"" target=""#foot_2"">4</ref> and GraphSAGE <ref type=""bibr"" target=""#b8"">(Hamilton et al., 2017)</ref> with varying depth from 2 to 64. <ref ty , 2008)</ref>; (2) predicting which community different posts belong to in the Reddit social network<ref type=""bibr"" target=""#b8"">(Hamilton et al., 2017)</ref>. Note that the tasks in Cora, Citeseer a bility issue of spectral-based GCNs on large graphs, spatial-based GCNs have been rapidly developed <ref type=""bibr"" target=""#b8"">(Hamilton et al., 2017;</ref><ref type=""bibr"" target=""#b22"">Monti et a",0
"""figure"">2</ref> Implementations We consider five backbones: GCN (Kipf &amp; Welling, 2017), ResGCN <ref type=""bibr"" target=""#b9"">(He et al., 2016;</ref><ref type=""bibr"" target=""#b17"">Li et al., 2019) ment three popular backbones recasted from image classification. They are residual network (ResGCN) <ref type=""bibr"" target=""#b9"">(He et al., 2016;</ref><ref type=""bibr"" target=""#b17"">Li et al., 2019)",0
"of sharing parameters across layers has been previously explored with the Transformer architecture <ref type=""bibr"" target=""#b53"">(Vaswani et al., 2017)</ref>, but this prior work has focused on trai <p>The backbone of the ALBERT architecture is similar to BERT in that it uses a transformer encoder <ref type=""bibr"" target=""#b53"">(Vaswani et al., 2017)</ref> with GELU nonlinearities <ref type=""bibr",1
"re quite simple. Skipthought <ref type=""bibr"" target=""#b29"">(Kiros et al., 2015)</ref> and FastSent <ref type=""bibr"" target=""#b23"">(Hill et al., 2016)</ref> sentence embeddings are learned by using an",0
"t=""#b54"">(Wang et al., 2018)</ref>, two versions of the Stanford Question Answering Dataset (SQuAD; <ref type=""bibr"" target=""#b44"">Rajpurkar et al., 2016;</ref><ref type=""bibr"">2018)</ref>, and the Re",0
"w York Times (NYT) <ref type=""bibr"" target=""#b13"">(Riedel, Yao, and McCallum 2010)</ref> and WebNLG <ref type=""bibr"" target=""#b6"">(Gardent et al. 2017)</ref>. NYT comes from the distant supervised rel ramework <ref type=""bibr"" target=""#b15"">(Sutskever, Vinyals, and Le 2014)</ref> with copy mechanism <ref type=""bibr"" target=""#b6"">(Gu et al. 2016)</ref>. But it cannot predict the entire entities. In",1
"the recent progress of neural models <ref type=""bibr"" target=""#b1"">(Cai, Zhang, and Wang 2016;</ref><ref type=""bibr"" target=""#b17"">Zeng et al. 2014;</ref><ref type=""bibr"" target=""#b3"">Christopoulou, M",0
"pe=""bibr"" target=""#b10"">(Mintz et al. 2009)</ref>. In spite of the recent progress of neural models <ref type=""bibr"" target=""#b1"">(Cai, Zhang, and Wang 2016;</ref><ref type=""bibr"" target=""#b17"">Zeng e",0
"p><p>Early studies use pipeline models <ref type=""bibr"" target=""#b11"">(Nadeau and Sekine 2007;</ref><ref type=""bibr"" target=""#b2"">Chan and Roth 2011)</ref>, where they cast the relation extraction pro",0
"mission potential from the encoder output. Then, an additional Conditional Random Field (CRF) layer <ref type=""bibr"" target=""#b8"">(Lafferty, McCallum, and Pereira 2001)</ref> is employed to calculate",0
"thor. Thus constructing the network becomes the critical part of these methods, e.g., paper network <ref type=""bibr"" target=""#b21"">(Zhang et al. 2018)</ref>, paper-author network <ref type=""bibr"" targ for experiments: • AMiner-AND<ref type=""foot"" target=""#foot_0"">1</ref> . The dataset is released by <ref type=""bibr"" target=""#b21"">(Zhang et al. 2018)</ref>, which contains 500 author names for traini (Zhang and Al Hasan 2017)</ref>. However, either complicated feature engineering or the supervision <ref type=""bibr"" target=""#b21"">(Zhang et al. 2018</ref>) is required.</p><p>The two categories of me fully designed pairwise features, including author names, titles, institute names etc.</p><p>AMiner <ref type=""bibr"" target=""#b21"">(Zhang et al. 2018</ref>): This model designs a supervised global sta D, we use 100 names for testing and compare the result with the results of other models reported in <ref type=""bibr"" target=""#b21"">(Zhang et al. 2018</ref>). In the experiment on AceKG-AND, we sample",1
"Some following literature modifies the framework for the purpose of the adversarial training. IRGAN <ref type=""bibr"" target=""#b18"">(Wang et al. 2017)</ref>  </p></div> <div xmlns=""http://www.tei-c.org ive samples iteratively. And to make the generative module aware of relation information, following <ref type=""bibr"" target=""#b18"">(Wang et al. 2018a)</ref>, we design a random walk based generating s name disambiguation task using content information, we construct a new dataset collected from AceKG <ref type=""bibr"" target=""#b18"">(Wang et al. 2018b</ref>). The benchmark dataset consists of 130,655",1
"e.g., paper network <ref type=""bibr"" target=""#b21"">(Zhang et al. 2018)</ref>, paper-author network <ref type=""bibr"" target=""#b20"">(Zhang and Al Hasan 2017)</ref>. However, either complicated feature",1
"resent the relation features by preserving the connectivity information of the HIN. We use node2vec <ref type=""bibr"" target=""#b5"">(Grover and Leskovec 2016)</ref> to represent these features by v i ∈",0
"ion. Original purpose of GAN is to generate data from the underlying true distribution, e.g., image <ref type=""bibr"" target=""#b2"">(Denton et al. 2015)</ref>, sequence <ref type=""bibr"" target=""#b19"">(Y",0
"ocusing on relation information <ref type=""bibr"" target=""#b9"">(Kanani, McCallum, and Pal 2007;</ref><ref type=""bibr"" target=""#b0"">Bekkerman and McCallum 2005)</ref> usually solve the problem on the bi",0
"get=""#b6"">(Han et al. 2004;</ref><ref type=""bibr"" target=""#b8"">Huang, Ertekin, and Giles 2006;</ref><ref type=""bibr"" target=""#b12"">Louppe et al. 2016;</ref><ref type=""bibr"">Yoshida et al. 2010)</ref>, network embedding is learned with an aim to preserve the connectivity of the constructed networks. <ref type=""bibr"" target=""#b12"">Louppe et al. (2016)</ref>: This model trains a function to measure t",0
"g true distribution, e.g., image <ref type=""bibr"" target=""#b2"">(Denton et al. 2015)</ref>, sequence <ref type=""bibr"" target=""#b19"">(Yu et al. 2017)</ref>, dialogue <ref type=""bibr"" target=""#b11"">(Li e",0
"ey cannot measure the high-order connections among papers. Methods focusing on relation information <ref type=""bibr"" target=""#b9"">(Kanani, McCallum, and Pal 2007;</ref><ref type=""bibr"" target=""#b0"">Be",0
"ncludes title, abstract, introduction and keywords etc. Methods focusing on the content information <ref type=""bibr"" target=""#b6"">(Han et al. 2004;</ref><ref type=""bibr"" target=""#b8"">Huang, Ertekin, a tegories according to the information they focus on. The first are based on the content information <ref type=""bibr"" target=""#b6"">(Han et al. 2004;</ref><ref type=""bibr"" target=""#b8"">Huang, Ertekin, a",0
"ven their complementary strengths and weaknesses <ref type=""bibr"">(d'Avila Garcez et al. 2015;</ref><ref type=""bibr"" target=""#b39"">Rocktäschel and Riedel 2017;</ref><ref type=""bibr"" target=""#b47"">Yang nowledge is compiled into a neural network architecture <ref type=""bibr"">(Bošnjak et al. 2017;</ref><ref type=""bibr"" target=""#b39"">Rocktäschel and Riedel 2017;</ref><ref type=""bibr"">Evans and Grefenst retability and generalisation, thereby inheriting the best of both worlds. Among such systems, NTPs <ref type=""bibr"" target=""#b39"">(Rocktäschel and Riedel 2017;</ref><ref type=""bibr"" target=""#b32"">Min div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>End-to-end Differentiable Proving</head><p>NTPs <ref type=""bibr"" target=""#b39"">(Rocktäschel and Riedel 2017)</ref> recursively build a neural networ l atoms in the body need to be proven, and because Z is a free variable with many possible bindings <ref type=""bibr"" target=""#b39"">(Rocktäschel and Riedel 2017)</ref>. We consider two problems -given (F) log[1 − ntp K θ ( F, d)]<label>(4)</label></formula><p>NTPs can also learn interpretable rules. <ref type=""bibr"" target=""#b39"">Rocktäschel and Riedel (2017)</ref> show that it is possible to learn tes. Although NTPs can be used for learning interpretable rules from data, the solution proposed by <ref type=""bibr"" target=""#b39"">Rocktäschel and Riedel (2017)</ref> can be quite inefficient, as the <ref type=""bibr"" target=""#b21"">(Kemp et al. 2006</ref>) -following the same evaluation protocols as <ref type=""bibr"" target=""#b39"">Rocktäschel and Riedel (2017)</ref>. Furthermore, since GNTPs allows Prediction Results. We compare GNTPs and NTPs on a set of link prediction benchmarks, also used in <ref type=""bibr"" target=""#b39"">Rocktäschel and Riedel (2017)</ref>. Results, presented in Table 1, s predicates, 111 unary predicates, 14 constants and 2565 true facts. We follow the protocol used by <ref type=""bibr"" target=""#b39"">Rocktäschel and Riedel (2017)</ref> and split every dataset into trai ww.tei-c.org/ns/1.0"" place=""foot"" n=""2"" xml:id=""foot_0"">For consistency, we use the same notation as<ref type=""bibr"" target=""#b39"">Rocktäschel and Riedel (2017)</ref>.</note> 			<note xmlns=""http://ww s parallel inference to be implemented very efficiently on GPU. This optimisation is also present in<ref type=""bibr"" target=""#b39"">Rocktäschel and Riedel (2017)</ref>.</note> 			<note xmlns=""http://ww <note xmlns=""http://www.tei-c.org/ns/1.0"" place=""foot"" n=""7"" xml:id=""foot_5"">Results reported in<ref type=""bibr"" target=""#b39"">Rocktäschel and Riedel (2017)</ref> were calculated with an incorrect 158 facts about the neighbourhood of countries, and the location of countries and subregions. As in <ref type=""bibr"" target=""#b39"">Rocktäschel and Riedel (2017)</ref>, we randomly split countries into",1
"till very effective Bag of Embeddings model <ref type=""bibr"" target=""#b45"">(White et al. 2015;</ref><ref type=""bibr"" target=""#b0"">Arora, Liang, and Ma 2017)</ref> showing that, even in this case, A re",1
"""bibr"" target=""#b2"">(Bos 2008)</ref>, Natural Language Inference and Recognising Textual Entailment <ref type=""bibr"" target=""#b10"">(Fyodorov, Winter, and Francez 2000;</ref><ref type=""bibr"" target=""#b",0
"ef type=""bibr"" target=""#b3"">(Bouchard, Singh, and Trouillon 2015)</ref>, Nations, UMLS, and Kinship <ref type=""bibr"" target=""#b21"">(Kemp et al. 2006</ref>) -following the same evaluation protocols as",0
"s with a differentiable external memory<ref type=""bibr"" target=""#b40"">(Sukhbaatar et al. 2015;</ref><ref type=""bibr"" target=""#b13"">Graves, Wayne, and Danihelka 2014;</ref><ref type=""bibr"" target=""#b19",0
"raves, Wayne, and Danihelka 2014;</ref><ref type=""bibr"" target=""#b19"">Joulin and Mikolov 2015;</ref><ref type=""bibr"" target=""#b14"">Grefenstette et al. 2015;</ref><ref type=""bibr"" target=""#b20"">Kaiser",0
"architectures, for this work we opted for a simple but still very effective Bag of Embeddings model <ref type=""bibr"" target=""#b45"">(White et al. 2015;</ref><ref type=""bibr"" target=""#b0"">Arora, Liang,",0
"es of the domain are unknown or hard to formalise, all of which being the case for natural language <ref type=""bibr"" target=""#b37"">(Raedt et al. 2008;</ref><ref type=""bibr"" target=""#b12"">Garnelo and S",0
"addition, we consider DistMult <ref type=""bibr"" target=""#b46"">(Yang et al. 2015)</ref> and ComplEx <ref type=""bibr"" target=""#b42"">(Trouillon et al. 2016)</ref>, two state-of-the-art black-box neural",0
">(Etzioni, Banko, and Cafarella 2006;</ref><ref type=""bibr"" target=""#b16"">Hermann et al. 2015;</ref><ref type=""bibr"" target=""#b44"">Weston et al. 2015;</ref><ref type=""bibr"" target=""#b7"">Das et al. 201",0
"large-scale ST dataset, multitask learning <ref type=""bibr"" target=""#b32"">(Weiss et al. 2017;</ref><ref type=""bibr"" target=""#b6"">Bérard et al. 2018</ref>) and pretraining techniques <ref type=""bibr"" ficantly increases the learning difficulty.</p><p>• Non-pre-trained Attention Module: Previous work <ref type=""bibr"" target=""#b6"">(Bérard et al. 2018)</ref> trains attention modules for ASR, MT and ST",1
"target=""#b23"">(Meignier and Merlin 2010)</ref> and then perform MWER segmentation with RWTH toolkit <ref type=""bibr"" target=""#b4"">(Bender et al. 2004</ref>). Case-insensitive BLEU is used as evaluatio",0
"b24"">(Munro 2010)</ref>; or in online courses, where audiences and speakers use different languages <ref type=""bibr"" target=""#b12"">(Jan et al. 2018)</ref>. To tackle this problem, existing approaches t sequences.</p><p>We conduct comprehensive experiments on the IWSLT18 speech translation benchmark <ref type=""bibr"" target=""#b12"">(Jan et al. 2018)</ref>, demonstrating the effectiveness of each comp",0
"//www.tei-c.org/ns/1.0""><head>Introduction</head><p>Large-scale knowledge graphs (KGs) such as YAGO <ref type=""bibr"" target=""#b10"">(Suchanek, Kasneci, and Weikum 2007)</ref>, NELL <ref type=""bibr"" tar e=""bibr"" target=""#b0"">(Carlson et al. 2010), and</ref><ref type=""bibr"">Wikidata (Vrandečić and</ref><ref type=""bibr"" target=""#b10"">Krötzsch 2014)</ref> usually represent facts in the form of relations =""bibr"" target=""#b9"">Socher et al. 2013;</ref><ref type=""bibr"" target=""#b11"">Yang et al. 2015;</ref><ref type=""bibr"" target=""#b10"">Trouillon et al. 2016;</ref><ref type=""bibr"" target=""#b8"">Schlichtkru sume available, sufficient training instances for all relations.</p><p>In light of the above issue, <ref type=""bibr"" target=""#b10"">Xiong et al. (2018)</ref> proposed GMatching which introduces a local )</ref> have been proposed to learn entity embeddings by using relational information, Xiong et al. <ref type=""bibr"" target=""#b10"">(Xiong et al. 2018</ref>) demonstrated that explicitly encoding graph spectively. In order to measure the similarity between two vectors, we employ a recurrent processor <ref type=""bibr"" target=""#b10"">(Vinyals et al. 2016</ref>) f µ to perform multiple steps matching. T",1
"""#b8"">(Nickel, Tresp, and Kriegel 2011;</ref><ref type=""bibr"" target=""#b0"">Bordes et al. 2013;</ref><ref type=""bibr"" target=""#b9"">Socher et al. 2013;</ref><ref type=""bibr"" target=""#b11"">Yang et al. 20",0
"line of work has been facilitated by the release of multi-domain dialogue corpora such as MultiWOZ <ref type=""bibr"" target=""#b1"">(Budzianowski et al. 2018)</ref>, M2M <ref type=""bibr"" target=""#b15"">( Asri et al. 2017)</ref>, M2M <ref type=""bibr"" target=""#b15"">(Shah et al. 2018</ref>) and Multi-WOZ <ref type=""bibr"" target=""#b1"">(Budzianowski et al. 2018</ref>). These datasets have utilized a varie",1
"ressing these concerns, approaches utilizing a dynamic vocabulary of slot values have been proposed <ref type=""bibr"" target=""#b13"">(Rastogi, Gupta, and Hakkani-Tur 2018;</ref><ref type=""bibr"" target="" representations for potentially unseen inputs from new services. Recent pretrained models like ELMo <ref type=""bibr"" target=""#b13"">(Peters et al. 2018)</ref> and BERT <ref type=""bibr"" target=""#b3"">(De",1
"e=""bibr"" target=""#b0"">(Bapna et al. 2017;</ref><ref type=""bibr"" target=""#b20"">Xia et al. 2018;</ref><ref type=""bibr"" target=""#b16"">Shah et al. 2019)</ref>, domain adaptation and transfer learning tech",0
"type=""bibr"" target=""#b17"">Wen et al. 2017)</ref> or individually score all slot-value combinations <ref type=""bibr"" target=""#b12"">(Mrkšić et al. 2017;</ref><ref type=""bibr"" target=""#b21"">Zhong, Xiong",0
"alues have been proposed <ref type=""bibr"" target=""#b13"">(Rastogi, Gupta, and Hakkani-Tur 2018;</ref><ref type=""bibr"" target=""#b6"">Goel, Paul, and Hakkani-Tür 2019;</ref><ref type=""bibr"" target=""#b19"">",0
"that arise with scaling virtual assistants in production. These assistants need to support a large <ref type=""bibr"" target=""#b11"">(Kim et al. 2018)</ref>, constantly increasing number of services ove",0
"ecent work has focused on zero-shot modeling <ref type=""bibr"" target=""#b0"">(Bapna et al. 2017;</ref><ref type=""bibr"" target=""#b20"">Xia et al. 2018;</ref><ref type=""bibr"" target=""#b16"">Shah et al. 2019",0
"ta, and Hakkani-Tur 2018;</ref><ref type=""bibr"" target=""#b6"">Goel, Paul, and Hakkani-Tür 2019;</ref><ref type=""bibr"" target=""#b19"">Wu et al. 2019</ref>).</p></div> <div xmlns=""http://www.tei-c.org/ns/",0
"tion of dialogue datasets with increasing complexity. Other notable related datasets include WOZ2.0 <ref type=""bibr"" target=""#b17"">(Wen et al. 2017</ref><ref type=""bibr"">), FRAMES (El Asri et al. 2017 er all possible slot-values <ref type=""bibr"" target=""#b9"">(Henderson, Thomson, and Young 2014;</ref><ref type=""bibr"" target=""#b17"">Wen et al. 2017)</ref> or individually score all slot-value combinati",0
"dentification by the uniqueness of every individual is an indispensable part of human life nowadays <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref>. However, because of t",1
"<ref type=""bibr"" target=""#b58"">[59]</ref>, STDB <ref type=""bibr"" target=""#b59"">[60]</ref>, and AFDB <ref type=""bibr"" target=""#b60"">[61]</ref> are used for conducting experiments. These datasets are co",0
"rget=""#b37"">[38,</ref><ref type=""bibr"" target=""#b25"">26]</ref>, amplitude features of various peaks <ref type=""bibr"" target=""#b23"">[24]</ref> and morphological features <ref type=""bibr"" target=""#b38""> classifiers, and SVM classifiers have been widely explored <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b48"">49]</ref>.</p><p>(2) NN based",0
"get=""#b49"">50,</ref><ref type=""bibr"" target=""#b50"">51,</ref><ref type=""bibr"" target=""#b61"">62,</ref><ref type=""bibr"" target=""#b62"">63,</ref><ref type=""bibr"" target=""#b63"">64]</ref> by a large margin.",0
"stride, and z rounds down z if it is a fraction. BN(?) represents the Batch Normalization operation <ref type=""bibr"" target=""#b55"">[56]</ref>, and the nonlinear activation operation is performed by</p",0
"ssing pipeline implementations.</p><p>In recent years, the Halide image processing language [Ragan- <ref type=""bibr"" target=""#b12"">Kelley et al. 2012;</ref><ref type=""bibr"" target=""#b12"">Ragan-Kelley he Halide image processing language [Ragan- <ref type=""bibr"" target=""#b12"">Kelley et al. 2012;</ref><ref type=""bibr"" target=""#b12"">Ragan-Kelley et al. 2013</ref>] has proven to be an effective system nt efforts to automatically generate efficient image processing pipelines from high-level programs. <ref type=""bibr"" target=""#b12"">Ragan-Kelley et al. [2013]</ref> employed auto-tuning guided by genet uthoring efficient schedules. We assume familiarity with the Halide system, and refer the reader to <ref type=""bibr"" target=""#b12"">[Ragan-Kelley et al. 2012;</ref><ref type=""bibr"" target=""#b12"">Ragan- processing, and computer vision workloads. Eight of the benchmarks are drawn from public literature <ref type=""bibr"" target=""#b12"">[Ragan-Kelley et al. 2012;</ref><ref type=""bibr"" target=""#b12"">Ragan- lide system, and refer the reader to <ref type=""bibr"" target=""#b12"">[Ragan-Kelley et al. 2012;</ref><ref type=""bibr"" target=""#b12"">Ragan-Kelley et al. 2013]</ref> for a comprehensive description of th rks are drawn from public literature <ref type=""bibr"" target=""#b12"">[Ragan-Kelley et al. 2012;</ref><ref type=""bibr"" target=""#b12"">Ragan-Kelley et al. 2013;</ref><ref type=""bibr"" target=""#b13"">Ragan-K",1
"/p><p>? Dense matrix-matrix multiplication (MATMUL). Fast bilateral filter using the bilateral grid <ref type=""bibr"" target=""#b3"">[Chen et al. 2007</ref>]. Constructs the grid using a histogram reduct",0
"#b12"">[Ragan-Kelley et al. 2012;</ref><ref type=""bibr"" target=""#b12"">Ragan-Kelley et al. 2013;</ref><ref type=""bibr"" target=""#b13"">Ragan-Kelley et al. 2015]</ref> and the Halide open source community.",0
"blending, and multi-scale interpolation is five to ten times slower than hand-tuned implementations <ref type=""bibr"" target=""#b11"">[Mullapudi et al. 2015]</ref>.</p><p>While auto-tuning may seem like that stochastic auto-tuning systems struggled to converge quickly (or at all) on complex pipelines <ref type=""bibr"" target=""#b11"">[Mullapudi et al. 2015]</ref>, we implemented a simple, bruteforce au egarty et al. 2016</ref>], but in doing so it sacrifices fully automatic scheduling.</p><p>PolyMage <ref type=""bibr"" target=""#b11"">[Mullapudi et al. 2015</ref>] extends polyhedral analysis techniques or our more complex benchmarks.</p><p>PolyMage. We approximate the scheduling behavior of Poly-Mage <ref type=""bibr"" target=""#b11"">[Mullapudi et al. 2015</ref>] by restricting the auto-scheduler to ti orderings for a group, to make exploration of the optimization space tractable, our system follows <ref type=""bibr"" target=""#b11"">Mullapudi et al. [2015]</ref> and only considers a narrower space sch the loop nest of the output function. The iterative grouping process is similar to that employed by <ref type=""bibr"" target=""#b11"">Mullapudi et al. [2015]</ref>. However, while their work makes groupi",0
"get=""#b21"">22]</ref>, there are several attempts to adopt GNNs to learn with heterogeneous networks <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" ta ntly, studies have attempted to extend GNNs for modeling heterogeneous graphs. Schlichtkrull et al. <ref type=""bibr"" target=""#b13"">[14]</ref> propose the relational graph convolutional networks (RGCN) heterogeneous GNNs as baselines, including:</p><p>• Relational Graph Convolutional Networks (RGCN) <ref type=""bibr"" target=""#b13"">[14]</ref>, which keeps a different weight for each relationship, i.e refers to HGT +RT E +H e t e r .</p><p>GNN Models GCN <ref type=""bibr"" target=""#b8"">[9]</ref> RGCN <ref type=""bibr"" target=""#b13"">[14]</ref> GAT <ref type=""bibr"" target=""#b21"">[22]</ref> HetGNN <ref",1
", i.e., the ⟨τ (s), ϕ(e), τ (t)⟩ triplets.</p><p>Inspired by the architecture design of Transformer <ref type=""bibr"" target=""#b20"">[21]</ref>, we map target node t into a Query vector, and source node TE is inspired by Transformer's positional encoding method <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b20"">21]</ref>, which has been shown successful to capture the sequential",1
"ond, OAG has been consistently evolving, e.g., 1) the volume of publications doubles every 12 years <ref type=""bibr"" target=""#b3"">[4]</ref>, and 2) the KDD conference was more related to database in t",0
"r the past decade, a significant line of research has been explored for mining heterogeneous graphs <ref type=""bibr"" target=""#b16"">[17]</ref>. One of the classical paradigms is to define and use meta http://www.tei-c.org/ns/1.0""><head n=""2.1"">Heterogeneous Graph Mining</head><p>Heterogeneous graphs <ref type=""bibr"" target=""#b16"">[17]</ref> (a.k.a., heterogeneous information networks) are an import , ϕ(e), τ (t)⟩. Naturally, ϕ(e) −1 represents the inverse of ϕ(e). The classical meta path paradigm <ref type=""bibr"" target=""#b16"">[17]</ref><ref type=""bibr"" target=""#b17"">[18]</ref><ref type=""bibr"" t ssification, clustering, ranking and representation learning <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b16"">[17]</ref><ref type=""bibr"" target=""#b17"">[18]</ref><ref type=""bibr"" t",0
"phs. To address this issue, different sampling-based methods <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=",0
"ween 2016 and 2019 as testing. We choose NDCG and MRR, which are two widely adopted ranking metrics <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b10"">11]</ref>, as the evaluation",0
". Recently, in view of graph neural networks' (GNNs) success <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b21"">22]</ref>, there are several at sed the success of graph neural networks for relational data <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b21"">22]</ref>. Generally, a GNN can <p>Various (homogeneous) GNN architectures have been proposed following this framework. Kipf et al. <ref type=""bibr"" target=""#b8"">[9]</ref> propose graph convolutional network (GCN), which averages th /div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.1"">HGSampling</head><p>The full-batch GNN <ref type=""bibr"" target=""#b8"">[9]</ref> training requires the calculation of all node representation baselines is designed for homogeneous graphs, including:</p><p>• Graph Convolutional Networks (GCN) <ref type=""bibr"" target=""#b8"">[9]</ref>, which simply averages the neighbor's embedding followed by etwork are exactly ‡ Unless other stated, HGT refers to HGT +RT E +H e t e r .</p><p>GNN Models GCN <ref type=""bibr"" target=""#b8"">[9]</ref> RGCN <ref type=""bibr"" target=""#b13"">[14]</ref> GAT <ref type e <ref type=""table"">1</ref>: Open Academic Graph (OAG) Statistics.</p><p>Cora, Citeseer, and Pubmed <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b21"">22]</ref>, which only contain",0
"eta path paradigm <ref type=""bibr"" target=""#b16"">[17]</ref><ref type=""bibr"" target=""#b17"">[18]</ref><ref type=""bibr"" target=""#b18"">[19]</ref> is defined as a sequence of such meta relation.</p><p>Noti et=""#b2"">[3,</ref><ref type=""bibr"" target=""#b16"">[17]</ref><ref type=""bibr"" target=""#b17"">[18]</ref><ref type=""bibr"" target=""#b18"">[19]</ref>, while the dynamic perspective of HGs has not been extensi",0
"mogeneous) GNN sampling methods, such as GraphSage <ref type=""bibr"" target=""#b6"">[7]</ref>, FastGCN <ref type=""bibr"" target=""#b0"">[1]</ref>, and LADIES <ref type=""bibr"" target=""#b28"">[29]</ref>, resul aking it not scalable for Web-scale graphs. To address this issue, different sampling-based methods <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target",0
"et=""#b13"">[14,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b26"">27]</ref>. However, these works face several issues: First, most of t the same distribution, which is also adopted in literature <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b26"">27]</ref>.</p><p>Implementation Details. We use 256 as the hidden dim l knowledge graphs. RGCN keeps a distinct linear projection weight for each edge type. Zhang et al. <ref type=""bibr"" target=""#b26"">[27]</ref> present the heterogeneous graph neural networks (HetGNN) t triplet. We use the implementation provided in PyG. • Heterogeneous Graph Neural Networks (HetGNN) <ref type=""bibr"" target=""#b26"">[27]</ref>, which adopts different Bi-LSTMs for different node type f RGCN <ref type=""bibr"" target=""#b13"">[14]</ref> GAT <ref type=""bibr"" target=""#b21"">[22]</ref> HetGNN <ref type=""bibr"" target=""#b26"">[27]</ref> HAN <ref type=""bibr"" target=""#b22"">[23]</ref> HGT the same",0
"br"" target=""#b22"">Li et al., 2008;</ref><ref type=""bibr"" target=""#b34"">Rosenberg et al., 2005;</ref><ref type=""bibr"" target=""#b33"">Riloff and Jones, 1999)</ref>, which uses the prediction of models wi br"" target=""#b22"">Li et al., 2008;</ref><ref type=""bibr"" target=""#b34"">Rosenberg et al., 2005;</ref><ref type=""bibr"" target=""#b33"">Riloff and Jones, 1999)</ref>. However, in order to prevent the deep",1
"ref type=""bibr"" target=""#b52"">(Zhu, 2006;</ref><ref type=""bibr"" target=""#b22"">Li et al., 2008;</ref><ref type=""bibr"" target=""#b34"">Rosenberg et al., 2005;</ref><ref type=""bibr"" target=""#b33"">Riloff an ref type=""bibr"" target=""#b52"">(Zhu, 2006;</ref><ref type=""bibr"" target=""#b22"">Li et al., 2008;</ref><ref type=""bibr"" target=""#b34"">Rosenberg et al., 2005;</ref><ref type=""bibr"" target=""#b33"">Riloff an",1
"correctly labeled data and mislabeled data. Co-learning is motivated by the self-training algorithm <ref type=""bibr"" target=""#b52"">(Zhu, 2006;</ref><ref type=""bibr"" target=""#b22"">Li et al., 2008;</ref ines the labels of training samples and updates model parameters, in a way similar to self-training <ref type=""bibr"" target=""#b52"">(Zhu, 2006;</ref><ref type=""bibr"" target=""#b22"">Li et al., 2008;</ref",1
"earning is motivated by the self-training algorithm <ref type=""bibr"" target=""#b52"">(Zhu, 2006;</ref><ref type=""bibr"" target=""#b22"">Li et al., 2008;</ref><ref type=""bibr"" target=""#b34"">Rosenberg et al. model parameters, in a way similar to self-training <ref type=""bibr"" target=""#b52"">(Zhu, 2006;</ref><ref type=""bibr"" target=""#b22"">Li et al., 2008;</ref><ref type=""bibr"" target=""#b34"">Rosenberg et al.",1
"rriculums in loss functions to dynamically adjust weights of samples is also proved to be effective <ref type=""bibr"" target=""#b25"">(Lin et al., 2017;</ref><ref type=""bibr"" target=""#b13"">Jiang et al.,",0
"2019)</ref>. In contrast, obtaining coarsely labeled data, e.g., images retrieved by search engines <ref type=""bibr"" target=""#b21"">(Li et al., 2017)</ref>, web social data, Flickr tags <ref type=""bibr",0
"=""bibr"" target=""#b18"">Kumar et al., 2010;</ref><ref type=""bibr"" target=""#b20"">Li et al., 2018;</ref><ref type=""bibr"" target=""#b13"">Jiang et al., 2018;</ref><ref type=""bibr"" target=""#b51"">Zhao et al., ts of samples is also proved to be effective <ref type=""bibr"" target=""#b25"">(Lin et al., 2017;</ref><ref type=""bibr"" target=""#b13"">Jiang et al., 2018;</ref><ref type=""bibr"" target=""#b1"">Bengio et al., 2018)</ref>, and learning with meta-learning <ref type=""bibr"" target=""#b20"">(Li et al., 2018;</ref><ref type=""bibr"" target=""#b13"">Jiang et al., 2018)</ref>. However, existing approaches mainly focus ef> propose a meta-learning method to reweight training samples based on their gradient directions. <ref type=""bibr"" target=""#b13"">Jiang et al. (2018)</ref> train an auxiliary LSTM-based MentorNet to heme widely-used for them: random 32x32 cropping with 4-pixel padded and random horizontal flipping <ref type=""bibr"" target=""#b13"">(Jiang et al., 2018;</ref><ref type=""bibr"" target=""#b50"">Zhang and Sa ompared with several state-of-the-art methods to make DNN robust against label noise. b1) MentorNet <ref type=""bibr"" target=""#b13"">(Jiang et al., 2018)</ref>  </p></div> <div xmlns=""http://www.tei-c.o notes the ratio of wrong labels. For self-paced learning and MentorNet, we cite results reported in <ref type=""bibr"" target=""#b13"">(Jiang et al., 2018)</ref>. For other methods, we report results of o >(Jiang et al., 2018)</ref>. For other methods, we report results of our implementations. Following <ref type=""bibr"" target=""#b13"">(Jiang et al., 2018)</ref> For b1) and b2), we directly cite the resu target=""#b13"">(Jiang et al., 2018)</ref> For b1) and b2), we directly cite the results reported in <ref type=""bibr"" target=""#b13"">(Jiang et al., 2018)</ref>. For a fair comparison, we also use ResNet e ResNets with wide filters mentioned in their paper and apply exactly the same training details as <ref type=""bibr"" target=""#b13"">(Jiang et al., 2018)</ref>. The conv4 group of the network is duplica ivided by 10 after the 80 th and 120 th epoch, respectively.</p><p>Hyper-parameter. As MentorNet-DD <ref type=""bibr"" target=""#b13"">(Jiang et al., 2018)</ref> uses additional samples with accurate labe",0
". The two branches tend to make different mistakes with stochastic training techniques like dropout <ref type=""bibr"" target=""#b19"">(Lan et al., 2018;</ref><ref type=""bibr"" target=""#b41"">Szegedy et al. ined with stochastic techniques like dropout, two separate branches tend to make different mistakes <ref type=""bibr"" target=""#b19"">(Lan et al., 2018;</ref><ref type=""bibr"" target=""#b41"">Szegedy et al. the models, the two branches tend to converge to different local minima and make different mistakes <ref type=""bibr"" target=""#b19"">(Lan et al., 2018;</ref><ref type=""bibr"" target=""#b41"">Szegedy et al.",0
"search engines <ref type=""bibr"" target=""#b21"">(Li et al., 2017)</ref>, web social data, Flickr tags <ref type=""bibr"" target=""#b45"">(Vahdat, 2017)</ref>, is a relatively easier task. Unfortunately, DNN noise via more complicated distributions, which takes the instances distribution into consideration <ref type=""bibr"" target=""#b45"">(Vahdat, 2017)</ref>. However, estimating the label noise distributio",0
"=""bibr"" target=""#b9"">(He et al., 2016;</ref><ref type=""bibr"" target=""#b11"">Huang et al., 2017;</ref><ref type=""bibr"" target=""#b37"">Simonyan and Zisserman, 2015;</ref><ref type=""bibr"" target=""#b32"">Ren get=""#b37"">Simonyan and Zisserman, 2015;</ref><ref type=""bibr"" target=""#b32"">Ren et al., 2015;</ref><ref type=""bibr"" target=""#b37"">Silver et al., 2016;</ref><ref type=""bibr"" target=""#b36"">Schmidhuber,",0
"rget=""#b50"">Zhang and Sabuncu, 2018;</ref><ref type=""bibr"" target=""#b29"">Patrini et al., 2017;</ref><ref type=""bibr"" target=""#b42"">Tanaka et al., 2018)</ref>. The MNIST dataset consists of 32x32 image",0
"a is usually very time-consuming and costly <ref type=""bibr"" target=""#b43"">(Tao and Dai, 2019;</ref><ref type=""bibr"" target=""#b44"">Tavanaei et al., 2019)</ref>. In contrast, obtaining coarsely labeled",0
"=""bibr"" target=""#b5"">Ghosh et al., 2017;</ref><ref type=""bibr"" target=""#b31"">Ren et al., 2018;</ref><ref type=""bibr"" target=""#b18"">Kumar et al., 2010;</ref><ref type=""bibr"" target=""#b20"">Li et al., 20 and should be down-weighted or pruned <ref type=""bibr"" target=""#b50"">(Zhang and Sabuncu, 2018;</ref><ref type=""bibr"" target=""#b18"">Kumar et al., 2010;</ref><ref type=""bibr"" target=""#b28"">Northcutt et",0
"cise annotations, such as ImageNet <ref type=""bibr"" target=""#b3"">(Deng et al., 2009)</ref> and COCO <ref type=""bibr"" target=""#b23"">(Lin et al., 2014)</ref>. However, collecting high-quality labeled da",0
"target=""#b18"">Kumar et al., 2010;</ref><ref type=""bibr"" target=""#b28"">Northcutt et al., 2017;</ref><ref type=""bibr"" target=""#b2"">Chang et al., 2017)</ref>. Adding pre-defined curriculums in loss func",0
"sing and augmentation techniques on MNIST as on CIFAR. The Street View House Numbers (SVHN) dataset <ref type=""bibr"" target=""#b27"">(Netzer et al., 2011)</ref> consists of 32x32 colored images of digit",0
"bibr"" target=""#b32"">Ren et al., 2015;</ref><ref type=""bibr"" target=""#b37"">Silver et al., 2016;</ref><ref type=""bibr"" target=""#b36"">Schmidhuber, 2015)</ref>. They can learn highly generalizable represe",0
"nal networks such as ResNets <ref type=""bibr"" target=""#b9"">(He et al., 2016)</ref> and Wide ResNets <ref type=""bibr"" target=""#b48"">(Zagoruyko and Komodakis, 2016)</ref>. We simply duplicate the final recommended by <ref type=""bibr"">Han et al. (2018)</ref>.</p><p>Training details. Wide-ResNet (WRN) <ref type=""bibr"" target=""#b48"">(Zagoruyko and Komodakis, 2016)</ref> and ResNet-32 <ref type=""bibr""",0
"e. In fact, it has been proved that DNNs have the capacity to overfit even completely random labels <ref type=""bibr"" target=""#b49"">(Zhang et al., 2017)</ref>. Therefore, training DNNs reliably on data andomly labeled samples with almost 100% accuracy. This phenomenon is in line with that observed by <ref type=""bibr"" target=""#b49"">Zhang et al. (2017)</ref> that DNNs are able fit random noise. We als",0
"><p>To address the aforementioned challenge, a number of methods have been proposed in recent years <ref type=""bibr"" target=""#b4"">(Frenay and Verleysen, 2014;</ref><ref type=""bibr"" target=""#b5"">Ghosh",0
"Normalization (GN) as a simple alternative to BN. We notice that many classical features like SIFT <ref type=""bibr"" target=""#b13"">[14]</ref> and HOG <ref type=""bibr"" target=""#b14"">[15]</ref> are grou ><p>The channels of visual representations are not entirely independent. Classical features of SIFT <ref type=""bibr"" target=""#b13"">[14]</ref>, HOG <ref type=""bibr"" target=""#b14"">[15]</ref>, and GIST < more abstract and their behaviors are not as intuitive. However, in addition to orientations (SIFT <ref type=""bibr"" target=""#b13"">[14]</ref>, HOG <ref type=""bibr"" target=""#b14"">[15]</ref>, or <ref ty",1
"We notice that many classical features like SIFT <ref type=""bibr"" target=""#b13"">[14]</ref> and HOG <ref type=""bibr"" target=""#b14"">[15]</ref> are group-wise features and involve group-wise normalizati not entirely independent. Classical features of SIFT <ref type=""bibr"" target=""#b13"">[14]</ref>, HOG <ref type=""bibr"" target=""#b14"">[15]</ref>, and GIST <ref type=""bibr"" target=""#b40"">[41]</ref> are gr ntuitive. However, in addition to orientations (SIFT <ref type=""bibr"" target=""#b13"">[14]</ref>, HOG <ref type=""bibr"" target=""#b14"">[15]</ref>, or <ref type=""bibr"" target=""#b43"">[44,</ref><ref type=""bi",1
"tp://www.tei-c.org/ns/1.0""><head n=""1"">Introduction</head><p>Batch Normalization (Batch Norm or BN) <ref type=""bibr"" target=""#b0"">[1]</ref> has been established as a very effective component in deep l ref>. LRN computes the statistics in a small neighborhood for each pixel.</p><p>Batch Normalization <ref type=""bibr"" target=""#b0"">[1]</ref> performs more global normalization along the batch dimension s not legitimate at inference time, so the mean and variance are pre-computed from the training set <ref type=""bibr"" target=""#b0"">[1]</ref>, often by running average; consequently, there is no normali ined (Figure <ref type=""figure"" target=""#fig_1"">2</ref>), discussed as follows.</p><p>In Batch Norm <ref type=""bibr"" target=""#b0"">[1]</ref>, the set S i is defined as:</p><formula xml:id=""formula_4"">S elations among BN, LN, and IN are in Figure <ref type=""figure"" target=""#fig_1"">2</ref>.</p><p>As in <ref type=""bibr"" target=""#b0"">[1]</ref>, all methods of BN, LN, and IN learn a per-channel linear tr putation introduces uncertainty caused by the stochastic batch sampling, which helps regularization <ref type=""bibr"" target=""#b0"">[1]</ref>. This uncertainty is missing in GN (and LN/IN). But it is po rticular, it is required for BN to work with a sufficiently large batch size (e.g., 32 per worker 1 <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target feature normalization methods. We first experiment with a regular batch size of 32 images (per GPU) <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b2"">3]</ref>. BN works successfully",1
"alization variants <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b18"">19]</ref>. Moreover, although the batch size may change, GN can natur malization methods <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" tar type=""figure"" target=""#fig_1"">2</ref>). Instead of operating on features, Weight Normalization (WN) <ref type=""bibr"" target=""#b18"">[19]</ref> proposes to normalize the filter weights. These methods do",0
"transformations) is made into the architectures by design <ref type=""bibr"" target=""#b43"">[44,</ref><ref type=""bibr"" target=""#b44"">45]</ref>, then the corresponding channels of these filters can be no ]</ref>, HOG <ref type=""bibr"" target=""#b14"">[15]</ref>, or <ref type=""bibr"" target=""#b43"">[44,</ref><ref type=""bibr"" target=""#b44"">45]</ref>), there are many factors that could lead to grouping, e.g.,",0
"s a very effective component in deep learning, largely helping push the frontier in computer vision <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b2"">3]</ref> and beyond <ref type="" antially lower error (by 10%) than BN with a batch size of 2.</p><p>As a result, many recent models <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target with a sufficiently large batch size (e.g., 32 per worker 1 <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b2"">3]</ref>). A small batch leads t",0
"channel dependence.  Implementation. GN can be easily implemented by a few lines of code in PyTorch <ref type=""bibr"" target=""#b49"">[50]</ref> and TensorFlow <ref type=""bibr"" target=""#b50"">[51]</ref> w",0
"fact, a wellaccepted computational model in neuroscience is to normalize across the cell responses <ref type=""bibr"" target=""#b45"">[46,</ref><ref type=""bibr"" target=""#b46"">47,</ref><ref type=""bibr"" ta field centers (covering the visual field) and with various spatiotemporal frequency tunings"" (p183, <ref type=""bibr"" target=""#b45"">[46]</ref>); this can happen not only in the primary visual cortex, b",0
"lt, many recent models <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=",0
"""bibr"" target=""#b22"">23]</ref>) or generative models (GANs <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b24"">25]</ref>). But as we will show by experiments, both LN and IN have l",0
"ing along the batch dimension. These methods are effective for training sequential models (RNN/LSTM <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b22"">23]</ref>) or generative mod",0
">. As a reference, with synchronous BN <ref type=""bibr"" target=""#b35"">[36]</ref>, a concurrent work <ref type=""bibr"" target=""#b61"">[62]</ref> achieves a from-scratch result of 34.5 box AP using R50 an",0
"""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b22"">23]</ref>) or generative models (GANs <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b24"">25]</ref>). But as we will s",0
"d following models <ref type=""bibr"" target=""#b28"">[29,</ref><ref type=""bibr"" target=""#b29"">30,</ref><ref type=""bibr"" target=""#b30"">31]</ref>. LRN computes the statistics in a small neighborhood for ea ning rate by 10× at 30, 60, and 90 epochs.</p><p>During training, we adopt the data augmentation of <ref type=""bibr"" target=""#b30"">[31]</ref> as implemented by <ref type=""bibr"" target=""#b51"">[52]</ref",0
"wer error than its BN counterpart for ResNet-50 <ref type=""bibr"" target=""#b2"">[3]</ref> in ImageNet <ref type=""bibr"" target=""#b15"">[16]</ref>. With a regular batch size, GN is comparably good as BN (w",0
"get=""#tab_0"">I</ref>.</p><p>Graph neural networks (GNNs) can effectively exploit non-Euclidean data <ref type=""bibr"" target=""#b12"">[13]</ref>, e.g., CSI. In this paper, to overcome the limitations men al Networks</head><p>In this subsection, we give a brief introduction to GNNs, and one can refer to <ref type=""bibr"" target=""#b12"">[13]</ref>, <ref type=""bibr"" target=""#b16"">[17]</ref> for a more deta",1
"ction on transformed elements in the point set to approximate a general function defined on the set <ref type=""bibr"" target=""#b20"">[21]</ref>:</p><formula xml:id=""formula_12"">AGGREGATE({x 1 , • • • ,",1
"give a brief introduction to GNNs, and one can refer to <ref type=""bibr"" target=""#b12"">[13]</ref>, <ref type=""bibr"" target=""#b16"">[17]</ref> for a more detailed information. GNNs deal with learning p able.</p><p>The design of the two functions in GNNs is crucial and leads to different kinds of GNNs <ref type=""bibr"" target=""#b16"">[17]</ref>. The most popular GNNs are listed as follows.</p><p>1) Gra here u ∈ N (v), and W 1 and W 2 are the weight matrices to be learned. 3) Graph Isomorphism Network <ref type=""bibr"" target=""#b16"">[17]</ref>: It uses the MLP and sum pooling as the aggregation and co",0
"bibr"" target=""#b4"">[5]</ref>. The first attempts came from <ref type=""bibr"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b1"">[2]</ref>, which applied MLP and CNN, respectively, to approximate the cally, MLP <ref type=""bibr"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b2"">[3]</ref> and CNN <ref type=""bibr"" target=""#b1"">[2]</ref> have been used to approximate the input-output mapping of th e=""figure"" target=""#fig_3"">3</ref>.</p><p>The loss function adopted is the negative sum rate, as in <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b2"">[3]</ref>,</p><formula xml:id f>: It employs MLP and an unsupervised loss function to learn near-optimal power allocation. 4) DPC <ref type=""bibr"" target=""#b1"">[2]</ref>: CNN and the unsupervised loss function are used in this met , network realizations, to train MLP, PCNet, and DPC as in <ref type=""bibr"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b1"">[2]</ref> while the number of training samples used for IGCNet is 2000 ted in the square region [0, 100] × [0, 100] meters. The receivers are uniformly distributed within <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b9"">10]</ref> meters away from the ibr"" target=""#b3"">[4]</ref> to set up the simulation. The link distance is uniformly distributed in <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b9"">10]</ref> meters during trainin he test, the link distance is uniformly distributed in [l r , u r ] meters, where l r is uniform in <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b19"">20]</ref> meters and u r is un",0
"o solve NP-hard optimization problems in wireless networks <ref type=""bibr"" target=""#b0"">[1]</ref>- <ref type=""bibr"" target=""#b2"">[3]</ref>, <ref type=""bibr"" target=""#b8"">[9]</ref>- <ref type=""bibr"" t and accelerate the computation. Unsupervised learning and an ensembling mechanism were employed in <ref type=""bibr"" target=""#b2"">[3]</ref> to achieve better performance than the suboptimal WMMSE algo arning based methods have been proposed. Specifically, MLP <ref type=""bibr"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b2"">[3]</ref> and CNN <ref type=""bibr"" target=""#b1"">[2]</ref> have been us erioration phenomenon of existing methods using MLP or CNN <ref type=""bibr"" target=""#b0"">[1]</ref>- <ref type=""bibr"" target=""#b2"">[3]</ref>.</p><p>Fig. <ref type=""figure"">II</ref> illustrates MLP and aches for power control. From the numerical experiments in <ref type=""bibr"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b2"">[3]</ref>, we observe a performance loss when K gets larger. For examp >The loss function adopted is the negative sum rate, as in <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b2"">[3]</ref>,</p><formula xml:id=""formula_15"">= −E H K k=1 w k log 2 1 + e effectiveness of IGCNet, we follow the system setting of <ref type=""bibr"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b2"">[3]</ref> to set up simulations. Under this system setting, all the we e channel power control. It is also used as a benchmark in <ref type=""bibr"" target=""#b0"">[1]</ref>- <ref type=""bibr"" target=""#b2"">[3]</ref>. 2) MLP <ref type=""bibr"" target=""#b0"">[1]</ref>: It leverage bibr"" target=""#b0"">[1]</ref>: It leverages MLP to learn the input-output mapping of WMMSE. 3) PCNet <ref type=""bibr"" target=""#b2"">[3]</ref>: It employs MLP and an unsupervised loss function to learn n /1.0""><head>D. Time Comparison</head><p>It was reported in <ref type=""bibr"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b2"">[3]</ref> that learning-based methods have less computation time than",0
"mization in wireless networks. However, typical resource allocation problems, such as power control <ref type=""bibr"" target=""#b6"">[7]</ref>, <ref type=""bibr"" target=""#b7"">[8]</ref>, are non-convex and h i = [h 1i , • • • , h Ki ] T , i = 1, • • • , K.</formula><p>This problem is known to be NP-hard <ref type=""bibr"" target=""#b6"">[7]</ref>. Although several optimization-based methods have been propo",0
"CNN, respectively, to approximate the classic weighted minimum mean square error (WMMSE) algorithm <ref type=""bibr"" target=""#b11"">[12]</ref> and accelerate the computation. Unsupervised learning and pe=""bibr"" target=""#b6"">[7]</ref>. Although several optimization-based methods have been proposed in <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" target=""#b13"">[14]</ref>, they are compu onsidered. We mainly compare the proposed IGCNet with the following five benchmarks:</p><p>1) WMMSE <ref type=""bibr"" target=""#b11"">[12]</ref>: This is the most popular optimizationbased algorithm for",0
"or non-Euclidean data. There are many sucessful applications of GNNs such as recommendation systems <ref type=""bibr"" target=""#b17"">[18]</ref> and solving combinatorial problems <ref type=""bibr"" target",0
"#b16"">[17]</ref>. The most popular GNNs are listed as follows.</p><p>1) Graph Convolutional Network <ref type=""bibr"" target=""#b19"">[20]</ref>: It uses the mean pooling and relu as the aggregation and distributed in [l r , u r ] meters, where l r is uniform in <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b19"">20]</ref> meters and u r is uniform in [l r , 20] meters. The perform",0
"several optimization-based methods have been proposed in <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" target=""#b13"">[14]</ref>, they are computationally demanding, and thus cannot be ap",0
"t=""#b54"">[53]</ref>. (iii) Designing a hardware for a fixed modest-sized parameter, e.g., n = 2 12  <ref type=""bibr"" target=""#b55"">[54]</ref>. However, encryption parameters determine the security-lev le-threaded Intel Xeon(R) Silver 4108 running at 1.80 GHz; which is a similar CPU used in prior art <ref type=""bibr"" target=""#b55"">[54]</ref>. The single-thread baseline is used by prior art for measu >. The single-thread baseline is used by prior art for measuring the performance (non-CKKS schemes) <ref type=""bibr"" target=""#b55"">[54]</ref>. In addition, SEAL is thread-safe but not multithreaded du ior performance compared to CPU execution.</p><p>Perhaps, the closest work to ours is by Roy et al. <ref type=""bibr"" target=""#b55"">[54]</ref> in which authors propose an architecture for BFV scheme an EAL running on Intel Xeon Silver 4108 at 1.8 GHz (note that similar processor is used compared with <ref type=""bibr"" target=""#b55"">[54]</ref> running at identical frequency).</p><p>FPGA-based Co-Proce and high throughput.</p><p>As has been shown by prior art <ref type=""bibr"" target=""#b54"">[53,</ref><ref type=""bibr"" target=""#b55"">54]</ref>, leveraging off-chip memory to store intermediate results s",1
"/ref> scheme. Subsequent improvements are reported in <ref type=""bibr"" target=""#b60"">[60]</ref>. In <ref type=""bibr"" target=""#b58"">[58]</ref>, a GPU-based implementation of BGV scheme <ref type=""bibr""",0
"ts efficient truncation of encrypted values. Several works <ref type=""bibr"" target=""#b41"">[40,</ref><ref type=""bibr"" target=""#b44"">43]</ref> have shown the benefits of choosing the CKKS scheme over ot",0
"e last decade (see <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b39"">38,</ref><ref type=""bibr"" target=""#b57"">57]</ref>). Specifically, many of these risks revolve around data sec",0
"capabilities of trace-based dataflow analysis to explore timeliness and load classification. Zhang <ref type=""bibr"" target=""#b52"">[53]</ref> performs dynamic prefetch optimization based on profiling,",1
"ializing jump pointer references whenever an element is inserted into a data structure. Prior works <ref type=""bibr"" target=""#b30"">[31,</ref><ref type=""bibr"" target=""#b33"">34]</ref> leverage dynamic p",0
"we can expect from them.</p><p>We applied our methodology to a suite of memory intensive SPEC 2006 <ref type=""bibr"" target=""#b21"">[22]</ref>, PARSEC <ref type=""bibr"" target=""#b5"">[6]</ref> and WSC ap analyze eight memory-bound applications. We chose 471.omnetpp and 462.libquantum from SPEC CPU2006 <ref type=""bibr"" target=""#b21"">[22]</ref>, Canneal from PARSEC <ref type=""bibr"" target=""#b5"">[6]</re ications as used in the study performed in Section 5: 462.libquantum and 471.omnetpp from SPEC 2006 <ref type=""bibr"" target=""#b21"">[22]</ref>, Canneal from PARSEC <ref type=""bibr"" target=""#b5"">[6]</re",0
"f type=""bibr"" target=""#b46"">[47]</ref>, and the high-performance conjugate gradient benchmark XHPCG <ref type=""bibr"" target=""#b13"">[14]</ref>. Furthermore, we evaluate three WSC applications <ref type C <ref type=""bibr"" target=""#b5"">[6]</ref>, XSBench <ref type=""bibr"" target=""#b46"">[47]</ref>, XHPCG <ref type=""bibr"" target=""#b13"">[14]</ref> and the WSC applications Web Search, Knowledge Graph, and",0
"target=""#b7"">8,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" target=""#b34"">35,</ref><ref type=""bibr"" tar ch as linked-lists at compile time to insert jump pointers <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" target=""#b39"">40,</ref><ref type=""bibr"" tar",0
"ow-resolution (LR) images. Based on DNNs, many methods have been proposed to improve SR performance <ref type=""bibr"" target=""#b53"">[51,</ref><ref type=""bibr"" target=""#b28"">26,</ref><ref type=""bibr"" ta <ref type=""bibr"" target=""#b28"">[26]</ref>, DBPN <ref type=""bibr"" target=""#b18"">[16]</ref>, and RCAN <ref type=""bibr"" target=""#b53"">[51]</ref>. However, these methods still suffer from the large space nsists of several up-and down-sampling layers to iteratively produce LR and HR images. Zhang et al. <ref type=""bibr"" target=""#b53"">[51]</ref> propose the channel attention mechanism to build a deep mo nlike the baseline U-Net, we build each basic block using B residual channel attention block (RCAB) <ref type=""bibr"" target=""#b53"">[51]</ref> to improve the model capacity. Following <ref type=""bibr"" are 2 dual models for 4× SR and 3 dual models for 8× SR, respectively. Let B be the number of RCABs <ref type=""bibr"" target=""#b53"">[51]</ref> and F be the number of base feature channels. For 4× SR, w ctionbased methods <ref type=""bibr"" target=""#b18"">[16,</ref><ref type=""bibr"" target=""#b27"">25,</ref><ref type=""bibr"" target=""#b53"">51]</ref>. Haris et al. <ref type=""bibr"" target=""#b18"">[16]</ref> pro ata, and augment the training data following the method in <ref type=""bibr"" target=""#b28"">[26,</ref><ref type=""bibr"" target=""#b53"">51]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>A",1
"ef type=""bibr"" target=""#b45"">[43,</ref><ref type=""bibr"" target=""#b56"">54]</ref>. Based on Cycle-GAN <ref type=""bibr"" target=""#b58"">[56]</ref>, Yuan et al. <ref type=""bibr"" target=""#b45"">[43]</ref> pro scheme has also been used to perform image translation without paired training data, e.g., CycleGAN <ref type=""bibr"" target=""#b58"">[56]</ref> and DualGAN <ref type=""bibr"" target=""#b44"">[42]</ref>. Spe avoid the possible mode collapse issue when solving the under-constrained image translation problem <ref type=""bibr"" target=""#b58"">[56]</ref>. Unlike these methods, we seek to improve the performance es collected from YouTube. Thus, there are 3 DRN-adapt models in total. And We also train a CinCGAN <ref type=""bibr"" target=""#b58"">[56]</ref> model for each kind of unpaired data for comparison. Based Specifically, a cycle consistency loss is proposed to avoid the mode collapse issue of GAN methods <ref type=""bibr"" target=""#b58"">[56,</ref><ref type=""bibr"" target=""#b6"">4,</ref><ref type=""bibr"" targ CycleGAN based SR methods. First, Cycle-GAN based methods <ref type=""bibr"" target=""#b45"">[43,</ref><ref type=""bibr"" target=""#b58"">56]</ref> use a cycle consistency loss to avoid the possible mode col",1
"get=""#b52"">50,</ref><ref type=""bibr"" target=""#b54"">52,</ref><ref type=""bibr"" target=""#b13"">11,</ref><ref type=""bibr"" target=""#b22"">20]</ref>. Recently, image super-resolution (SR) has become an import",0
"ef type=""bibr"" target=""#b49"">[47]</ref>, BSDS100 <ref type=""bibr"" target=""#b3"">[1]</ref>, UR-BAN100 <ref type=""bibr"" target=""#b23"">[21]</ref> and MANGA109 <ref type=""bibr"" target=""#b31"">[29]</ref>. Tw ref type=""bibr"" target=""#b49"">[47]</ref>, BSDS100 <ref type=""bibr"" target=""#b3"">[1]</ref>, URBAN100 <ref type=""bibr"" target=""#b23"">[21]</ref> and MANGA109 <ref type=""bibr"" target=""#b31"">[29]</ref>. Im",0
"oblem.</p><p>Second, it is hard to obtain a promising SR model when the paired data are unavailable <ref type=""bibr"" target=""#b45"">[43,</ref><ref type=""bibr"" target=""#b56"">54]</ref>. Note that most SR models to real-world data, they often incur a severe adaptation problem and yield poor performance <ref type=""bibr"" target=""#b45"">[43,</ref><ref type=""bibr"" target=""#b56"">54]</ref>. Therefore, how to easing interest in learning super-resolution models without paired data in the unsupervised setting <ref type=""bibr"" target=""#b45"">[43,</ref><ref type=""bibr"" target=""#b56"">54]</ref>. Based on Cycle-GA roblem very challenging. In this case, existing SR models often incur the severe adaptation problem <ref type=""bibr"" target=""#b45"">[43,</ref><ref type=""bibr"" target=""#b56"">54]</ref>. To alleviate this erences and advantages of DRN compared to CycleGAN based SR methods. First, Cycle-GAN based methods <ref type=""bibr"" target=""#b45"">[43,</ref><ref type=""bibr"" target=""#b58"">56]</ref> use a cycle consis target=""#b56"">54]</ref>. Based on Cycle-GAN <ref type=""bibr"" target=""#b58"">[56]</ref>, Yuan et al. <ref type=""bibr"" target=""#b45"">[43]</ref> propose a CinCGAN model to generate HR images without pair",0
"propose a CinCGAN model to generate HR images without paired data. Recently, some blind SR methods <ref type=""bibr"" target=""#b4"">[2,</ref><ref type=""bibr"" target=""#b57"">55]</ref> were proposed to lea ta from S P , respectively. Then, we train our model end-to-end by minimizing the objective in Eqn. <ref type=""bibr"" target=""#b4"">(2)</ref>. For convenience, we define the data ratio of unpaired data",0
""">Architecture Design of DRN</head><p>We build our DRN upon the design of U-Net for superresolution <ref type=""bibr"" target=""#b24"">[22,</ref><ref type=""bibr"" target=""#b33"">31]</ref> (See Figure <ref t",0
"without paired training data, e.g., CycleGAN <ref type=""bibr"" target=""#b58"">[56]</ref> and DualGAN <ref type=""bibr"" target=""#b44"">[42]</ref>. Specifically, a cycle consistency loss is proposed to avo",0
"cation hyperplane <ref type=""bibr"" target=""#b60"">[61]</ref><ref type=""bibr"" target=""#b61"">[62]</ref><ref type=""bibr"" target=""#b62"">[63]</ref><ref type=""bibr"" target=""#b63"">[64]</ref><ref type=""bibr"" t",1
"ased on fuzzy theory to reduce the influence of noises or outliers on the classification hyperplane <ref type=""bibr"" target=""#b60"">[61]</ref><ref type=""bibr"" target=""#b61"">[62]</ref><ref type=""bibr"" t",0
">. The Murmurhash <ref type=""bibr"" target=""#b21"">[22]</ref><ref type=""bibr"" target=""#b22"">[23]</ref><ref type=""bibr"" target=""#b23"">[24]</ref> is utilized with the Bloom-filter-based method to ensure t",0
"=""#b41"">[42]</ref><ref type=""bibr"" target=""#b42"">[43]</ref><ref type=""bibr"" target=""#b43"">[44]</ref><ref type=""bibr"" target=""#b44"">[45]</ref><ref type=""bibr"" target=""#b45"">[46]</ref>. AnRAD <ref type=",0
"sed on prior knowledge and judge the deviation between the current behavior and the normal behavior <ref type=""bibr"" target=""#b1"">[2]</ref>. The advantage of the anomaly-based method is the ability to",0
"get=""#b15"">[16]</ref>. The deliberation model has been used in state-of-the-art machine translation <ref type=""bibr"" target=""#b16"">[17]</ref>, or generating intermediate representation in speech-to-te",1
"acoustics and first-pass text hypotheses for second-pass decoding based on the deliberation network <ref type=""bibr"" target=""#b15"">[16]</ref>. The deliberation model has been used in state-of-the-art lation <ref type=""bibr"" target=""#b17"">[18]</ref>. Our deliberation model has a similar structure as <ref type=""bibr"" target=""#b15"">[16]</ref>: An RNN-T model generates the first-pass hypotheses, and d head><p>A deliberation model is typically trained from scratch by jointly optimizing all components <ref type=""bibr"" target=""#b15"">[16]</ref>. However, we find training a two-pass model from scratch t get=""#b0"">[1]</ref>, and a deliberation decoder, similar to <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b15"">16]</ref>. The shared encoder takes log-mel filterbank energies, x =",1
""" target=""#b16"">[17]</ref>, or generating intermediate representation in speech-to-text translation <ref type=""bibr"" target=""#b17"">[18]</ref>. Our deliberation model has a similar structure as <ref ty",1
"i.e., acoustic, pronunciation, and language models), and directly outputs subword (or word) symbols <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target f>, our deliberation network consists of three major components: A shared encoder, an RNN-T decoder <ref type=""bibr"" target=""#b0"">[1]</ref>, and a deliberation decoder, similar to <ref type=""bibr"" tar",0
"periments are conducted using the same training data as in <ref type=""bibr"" target=""#b19"">[20,</ref><ref type=""bibr"" target=""#b20"">21]</ref>, which is from multiple domains such as Voice Search, YouTu p>For training, we use the same multidomain datasets as in <ref type=""bibr"" target=""#b19"">[20,</ref><ref type=""bibr"" target=""#b20"">21]</ref> which include anonymized and hand-transcribed English utter",0
"uage models), and directly outputs subword (or word) symbols <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=",0
"challenges compared to state-of-the-art conventional models <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b8"">9]</ref>. To bridge the quality gap between a streaming recurrent neur",0
"e context information for decoding. Note that the first-pass hypotheses are sequences of wordpieces <ref type=""bibr"" target=""#b18"">[19]</ref> and are usually short in VS, and thus the encoding should r). The LAS decoder has a 4,096-dimensional softmax layer to predict the same mixed-case wordpieces <ref type=""bibr"" target=""#b18"">[19]</ref> as the RNN-T.</p><p>For feature extraction, we use 128-dim",0
"t information, and can be considered as second-pass models <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b12"">13]</ref>. The models typical type=""bibr"" target=""#b14"">[15]</ref>. A transformer-based spelling correction model is proposed in <ref type=""bibr"" target=""#b11"">[12]</ref> to correct the outputs of a connectionist temporal classif",0
"ollowed by 320-dimensional projection. Each of the two attention models is a multi-headed attention <ref type=""bibr"" target=""#b26"">[27]</ref> with four attention heads. The two output context vectors",0
"ess made by E2E models, they still face challenges compared to state-of-the-art conventional models <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b8"">9]</ref>. To bridge the quality l network transducer (RNN-T) <ref type=""bibr"" target=""#b5"">[6]</ref> and a large conventional model <ref type=""bibr"" target=""#b7"">[8]</ref>, a two-pass framework has been proposed in <ref type=""bibr"" o RNN-T <ref type=""bibr"" target=""#b5"">[6]</ref> and has a similar WER to a large conventional model <ref type=""bibr"" target=""#b7"">[8]</ref>.</p><p>A class of neural correction models post-process hypo odel achieves a WER of 5.0% on VS, which is 21% relatively better than the large conventional model <ref type=""bibr"" target=""#b7"">[8]</ref> (6.3% VS WER). Lastly, we analyze the computational complexi <ref type=""bibr"" target=""#b9"">[10]</ref> performs inferior to a state-of-the-art conventional model <ref type=""bibr"" target=""#b7"">[8]</ref>, and one reason is due to proper nouns. The voice command te",0
"ost-process hypotheses using only the text information, and can be considered as second-pass models <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" ta e text corpora <ref type=""bibr"" target=""#b13"">[14]</ref>. For example, a neural correction model in <ref type=""bibr"" target=""#b10"">[11]</ref> takes first-pass text hypotheses and generates new sequenc",0
"second-pass models <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b12"">13]</ref>. The models typically use beam search to generate new hypot correct the outputs of a connectionist temporal classification model in Mandarin ASR. In addition, <ref type=""bibr"" target=""#b12"">[13]</ref> leverages text-to-speech (TTS) audio to train an attention",0
"models perform competitively compared to more sophisticated conventional systems on Google traffic <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b6"">7]</ref>. Given its all-neural ef>. Given its all-neural nature, an E2E model can be reasonably downsized to fit on mobile devices <ref type=""bibr"" target=""#b5"">[6]</ref>.</p><p>Despite the rapid progress made by E2E models, they s 9]</ref>. To bridge the quality gap between a streaming recurrent neural network transducer (RNN-T) <ref type=""bibr"" target=""#b5"">[6]</ref> and a large conventional model <ref type=""bibr"" target=""#b7"" ass hypotheses. The two-pass model achieves 17%-22% relative WER reduction (WERR) compared to RNN-T <ref type=""bibr"" target=""#b5"">[6]</ref> and has a similar WER to a large conventional model <ref typ ype=""bibr"" target=""#b9"">[10]</ref>, and thus use a two-step training process: Train the RNN-T as in <ref type=""bibr"" target=""#b5"">[6]</ref>, and then fix the RNN-T parameters and only train the delibe recognition, we report performance on a side-by-side (SxS) test set, and 4 voice command test sets <ref type=""bibr"" target=""#b5"">[6]</ref>. The SxS set contains utterances where the LAS rescoring mod >Architecture Details and Training</head><p>Our first-pass RNN-T model has the same architecture as <ref type=""bibr"" target=""#b5"">[6]</ref>. The encoder of the RNN-T consists of an 8-layer Long Short-",0
"dels are trained in Tensorflow <ref type=""bibr"" target=""#b27"">[28]</ref> using the Lingvo framework <ref type=""bibr"" target=""#b28"">[29]</ref> on 8×8 Tensor Processing Units (TPU) slices with a global",0
", and deep contextual language models from BERT <ref type=""bibr"" target=""#b14"">[15]</ref> and XLNet <ref type=""bibr"" target=""#b40"">[41]</ref> in a vanilla and Siamese architecture <ref type=""bibr"" tar contextual embeddings as the ones used in BERT <ref type=""bibr"" target=""#b14"">[15]</ref> and XL-Net <ref type=""bibr"" target=""#b40"">[41]</ref>. The Transformer architecture allowed the efficient unsupe ype=""bibr"" target=""#b36"">[37]</ref>, named BERT <ref type=""bibr"" target=""#b14"">[15]</ref> and XLNet <ref type=""bibr"" target=""#b40"">[41]</ref>. The two Transformer models are originally designed to sol <ref type=""bibr"" target=""#b42"">[43]</ref> alone, XLNet uses additional Web corpora for pretraining <ref type=""bibr"" target=""#b40"">[41]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head pected is that BERT generally achieves slightly better results than XLNet. According to Yang et al. <ref type=""bibr"" target=""#b40"">[41]</ref>, XLNet surpasses BERT on the related GLUE benchmark <ref t may be attributed to two reasons, pretraining on different corpora, and smaller models compared to <ref type=""bibr"" target=""#b40"">[41]</ref>. We use the BASE, not the LARGE versions of the pretrained 0"">[41]</ref>. We use the BASE, not the LARGE versions of the pretrained models used by Yang et al. <ref type=""bibr"" target=""#b40"">[41]</ref>. Furthermore, the published XLNet BASE model we considered ublished XLNet BASE model we considered is pretrained on different data than the one in Yang et al. <ref type=""bibr"" target=""#b40"">[41]</ref>  <ref type=""foot"" target=""#foot_13"">15</ref> . In contrast",1
"mentation <ref type=""bibr"" target=""#b32"">[33]</ref>), and deep contextual language models from BERT <ref type=""bibr"" target=""#b14"">[15]</ref> and XLNet <ref type=""bibr"" target=""#b40"">[41]</ref> in a v GloVe <ref type=""bibr"" target=""#b30"">[31]</ref>, to contextual embeddings as the ones used in BERT <ref type=""bibr"" target=""#b14"">[15]</ref> and XL-Net <ref type=""bibr"" target=""#b40"">[41]</ref>. The tations based on the Transformer architecture <ref type=""bibr"" target=""#b36"">[37]</ref>, named BERT <ref type=""bibr"" target=""#b14"">[15]</ref> and XLNet <ref type=""bibr"" target=""#b40"">[41]</ref>. The t r Siamese</figDesc><table /><note>XLNet-512 (most complex Transformer architecture). As suggested in<ref type=""bibr"" target=""#b14"">[15]</ref>, the Transformer training is performed with batch size b =",1
"ntations of sentences and their similarity <ref type=""bibr"" target=""#b25"">[26]</ref>. In prior work <ref type=""bibr"" target=""#b31"">[32]</ref>, we also utilized a Siamese BERT model to determine the di",1
"ased document embeddings from GloVe <ref type=""bibr"" target=""#b30"">[31]</ref> and Paragraph Vectors <ref type=""bibr"" target=""#b22"">[23]</ref> (as Doc2vec implementation <ref type=""bibr"" target=""#b32""> they encode the segments with GloVe <ref type=""bibr"" target=""#b30"">[31]</ref> and Paragraph Vectors <ref type=""bibr"" target=""#b22"">[23]</ref> and compute their similarity to determine whether papers a ref type=""bibr"" target=""#b34"">35]</ref> but unable to represent entire documents. Paragraph Vectors <ref type=""bibr"" target=""#b22"">[23]</ref> (also known as Doc2vec), extends word2vec to learn embeddi",1
"any NLP benchmarks <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b38"">39,</ref><ref type=""bibr"" target=""#b42"">43]</ref>. Reimers and Gurevych <ref type=""bibr"" target=""#b33"">[34]</ are pretrained with different data. While BERT is trained on English Wikipedia and the BooksCorpus <ref type=""bibr"" target=""#b42"">[43]</ref> alone, XLNet uses additional Web corpora for pretraining < n contrast to BERT, XLNet is pretrained on Web corpora in addition to Wikipedia and the BooksCorpus <ref type=""bibr"" target=""#b42"">[43]</ref>. The almost exclusive pretraining on Wikipedia most likely",0
"beddings. However, XLNet integrates the relative positional encoding, as proposed in Transformer-XL <ref type=""bibr"" target=""#b13"">[14]</ref>. Therefore, XLNet's architecture is, in theory, not bound",0
"l. The Transformer models require a GPU as hardware. We rely on HuggingFace's PyTorch implementation<ref type=""bibr"" target=""#b39"">[40]</ref> of BERT and XLNet. The training time for a single epoch on",0
"gGloVe), whereby the number of occurrences of the word i in d defines the weight c i . Arora et al. <ref type=""bibr"" target=""#b3"">[4]</ref> showed the weighted average of word vectors is effective and e performance of the method ultimately depends on its ability to encode the documents. Arora et al. <ref type=""bibr"" target=""#b3"">[4]</ref> have shown that the weighted average of word vectors can out",0
"15]</ref> and XLNet <ref type=""bibr"" target=""#b40"">[41]</ref> in a vanilla and Siamese architecture <ref type=""bibr"" target=""#b8"">[9]</ref>. Each system is evaluated under specific configurations rega vych <ref type=""bibr"" target=""#b33"">[34]</ref> proposed to combine BERT with a Siamese architecture <ref type=""bibr"" target=""#b8"">[9]</ref> for semantic representations of sentences and their similari ese Transformer. We combine the two Transformers (BERT and XLNet) in a Siamese network architecture <ref type=""bibr"" target=""#b8"">[9]</ref>. In Siamese networks, two inputs are fed through identical s",0
"dbow over the distributed memory training model is due to its results in semantic similarity tasks <ref type=""bibr"" target=""#b21"">[22]</ref>. It is important to mention that even though the embedding",0
". Especially for complex information needs, the formulation of analogical queries is more intuitive <ref type=""bibr"" target=""#b23"">[24]</ref>. A system that supports analogical queries would be partic s C is to ?"" is a fundamental aspect of human intelligence <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b23"">24]</ref>. Chan et al. <ref type=""bibr"" target=""#b9"">[10]</ref> empha",0
"ry in which recommender system methodologies can be tested <ref type=""bibr"" target=""#b27"">[28,</ref><ref type=""bibr"" target=""#b35"">36]</ref>. In <ref type=""bibr"" target=""#b35"">[36]</ref>, we compared r Wikipedia articles have been addressed in the literature <ref type=""bibr"" target=""#b27"">[28,</ref><ref type=""bibr"" target=""#b35"">36]</ref>  <ref type=""foot"" target=""#foot_2"">3</ref> is connected to can be tested <ref type=""bibr"" target=""#b27"">[28,</ref><ref type=""bibr"" target=""#b35"">36]</ref>. In <ref type=""bibr"" target=""#b35"">[36]</ref>, we compared text-and link-based document similarity measu . It remains unknown how the length of the processed sequence affects the classification task. From <ref type=""bibr"" target=""#b35"">[36]</ref>, we know that the performance of similarity measures peaks and the second-highest with 256 tokens. One could think this outcome is to be expected. However, in <ref type=""bibr"" target=""#b35"">[36]</ref>, the performance of text-and linkbased document similarity",0
"c in our task requires considerably larger dataset than <ref type=""bibr"" target=""#b9"">[10]</ref> or <ref type=""bibr"" target=""#b19"">[20]</ref>. To the best of our knowledge, no established dataset fulf th programmatic and SPARQLlike queries. To develop such a system, the Open Research Knowledge Graph <ref type=""bibr"" target=""#b19"">[20]</ref> could be utilized as the scientific equivalence of Wikidat",0
"l for scientific literature since the discovery of the analogies is crucial for scientific progress <ref type=""bibr"" target=""#b9"">[10]</ref>.</p><p>Nonetheless, document similarity measures do not tak ence <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b23"">24]</ref>. Chan et al. <ref type=""bibr"" target=""#b9"">[10]</ref> emphasize the importance of analogical query solving for sc tences. Moreover, the learning characteristic in our task requires considerably larger dataset than <ref type=""bibr"" target=""#b9"">[10]</ref> or <ref type=""bibr"" target=""#b19"">[20]</ref>. To the best o ll, even annotations can be solved efficiently as Chan et al.'s crowdsourcing approach demonstrates <ref type=""bibr"" target=""#b9"">[10]</ref>. We are confident that our results are transferable to othe",0
"ype=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b28"">29]</ref>. Akkalyoncu Yilmaz et al. <ref type=""bibr"" target=""#b1"">[2]</ref> apply BERT to an information retrieval system for an end-to-",0
"xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2.3"">Transformers</head><p>Recently, Transformer-based <ref type=""bibr"" target=""#b36"">[37]</ref> neural language models introduced a shift from context-fre two language models for deep contextual text representations based on the Transformer architecture <ref type=""bibr"" target=""#b36"">[37]</ref>, named BERT <ref type=""bibr"" target=""#b14"">[15]</ref> and",0
"upervised pretraining of language models and led to significant improvements in many NLP benchmarks <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b38"">39,</ref><ref type=""bibr"" ta f type=""bibr"" target=""#b8"">[9]</ref> for semantic representations of sentences and their similarity <ref type=""bibr"" target=""#b25"">[26]</ref>. In prior work <ref type=""bibr"" target=""#b31"">[32]</ref>,",0
"tory for the segments. Moreover, BERT has successfully solved various document classification tasks <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b28"">29]</ref>. Akkalyoncu Yilmaz e",0
"f type=""bibr"" target=""#b38"">39,</ref><ref type=""bibr"" target=""#b42"">43]</ref>. Reimers and Gurevych <ref type=""bibr"" target=""#b33"">[34]</ref> proposed to combine BERT with a Siamese architecture <ref , the Transformers), and then passed to a classifier or a similarity function. Reimers and Gurevych <ref type=""bibr"" target=""#b33"">[34]</ref> have shown that Siamese BERT networks are suitable for tex her fixed nor frozen, but continually learned during the training of the classifier. Different than <ref type=""bibr"" target=""#b33"">[34]</ref>, our implemented Siamese architecture is applied to a mult [u; v; |u −v |; u * v]</formula><p>). Furthermore, we confirmed the results of Reimers and Gurevych <ref type=""bibr"" target=""#b33"">[34]</ref>, i.e., the most crucial component is the element-wise diff",0
"veloped Transformers, a new neural architecture for even more effective natural language processing <ref type=""bibr"" target=""#b42"">(Vaswani et al., 2017)</ref>. Transformers overcome a major drawback al., 2019)</ref>.</p><p>A.3 Why not Positional Encoding? Some Transformers uses positional encoding <ref type=""bibr"" target=""#b42"">(Vaswani et al., 2017)</ref> or positional embedding <ref type=""bibr"" presented in Sec 2.3.1. For other details (especially on the multi-head attention), please refer to <ref type=""bibr"" target=""#b42"">Vaswani et al. (2017)</ref> and in particular, <ref type=""bibr"">GPT-2",1
". 2017b) by 37.0%, the Deep3 system (Raychev et al., 2016a) by 29.7%, and an adaptation of Code2Seq <ref type=""bibr"" target=""#b7"">(Alon et al., 2019a)</ref> for code prediction by 30.0%. These are sig ""bibr"" target=""#b45"">, Yang and Xiang, 2019)</ref>. We include an adaptation of path-based Code2Seq <ref type=""bibr"" target=""#b7"">(Alon et al., 2019a)</ref> in our evaluations and show that our models Devanbu, 2017b)</ref>, Deep3 <ref type=""bibr"" target=""#b35"">(Raychev et al., 2016a)</ref>, Code2Seq <ref type=""bibr"" target=""#b7"">(Alon et al., 2019a)</ref>).</p><p>Fig 3 puts these models in perspect #b35"">(Raychev et al., 2016a</ref>)) vs. TravTrans+ ; • from 43.6% to 73.6% when comparing Code2Seq <ref type=""bibr"" target=""#b7"">(Alon et al., 2019a)</ref>  Thus, we argue that our proposal of using "">, Svyatkovskiy et al., 2019)</ref>; this model works on token sequences. We also include Code2Seq <ref type=""bibr"" target=""#b7"">(Alon et al., 2019a)</ref> to compare our efforts against a popular co ven a method body, how well can Code2Seq generate the correct method name? The training proposed in <ref type=""bibr"" target=""#b7"">(Alon et al., 2019a)</ref> is not well suited for next token predictio tsis et al., 2020</ref><ref type=""bibr"" target=""#b28"">, Li et al., 2018)</ref>), to paths in an AST <ref type=""bibr"" target=""#b7"">(Alon et al., 2019a</ref><ref type=""bibr"">(Alon et al., ,b, 2020))</re iv> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.3"">Code2Seq</head><p>Code2Seq is a model by <ref type=""bibr"" target=""#b7"">Alon et al. 2019a</ref> that embeds code snippets by embedding AST pat",1
"al., 2019)</ref> has used RNNs. Attempts has been made on feeding RNNs (LSTMs) with serialized ASTs <ref type=""bibr"" target=""#b29"">(Liu et al., 2016)</ref>; accuracy is further improved by using more",0
"architecture. These representations have ranged from linear token sequence (as for code prediction <ref type=""bibr"" target=""#b21"">(Hellendoorn and Devanbu, 2017a</ref><ref type=""bibr"" target=""#b27"">,",0
"ref type=""bibr"" target=""#b5"">(Allamanis et al., 2018a)</ref>, and in particular, to code prediction <ref type=""bibr"" target=""#b13"">(Brockschmidt et al., 2019</ref><ref type=""bibr"" target=""#b24"">, Hind",0
"target=""#b10"">(Aye and Kaiser, 2020</ref><ref type=""bibr"" target=""#b34"">, Raychev et al., 2014</ref><ref type=""bibr"" target=""#b39"">, Svyatkovskiy et al., 2019)</ref> has used RNNs. Attempts has been m get=""#b27"">Karampatsis et al., 2020</ref><ref type=""bibr"" target=""#b34"">, Raychev et al., 2014</ref><ref type=""bibr"" target=""#b39"">, Svyatkovskiy et al., 2019)</ref>; this model works on token sequenc ser, 2020, Hellendoorn and Devanbu, 2017b, <ref type=""bibr"" target=""#b34"">Raychev et al., 2014</ref><ref type=""bibr"" target=""#b39"">, Svyatkovskiy et al., 2019)</ref>. In this way, we create a language",0
"2019</ref><ref type=""bibr"" target=""#b19"">, Fernandes et al., 2019)</ref> and open-vocabulary models <ref type=""bibr"" target=""#b14"">(Cvitkovic et al., 2019</ref><ref type=""bibr"" target=""#b27"">, Karampa",0
"=""bibr"" target=""#b4"">(Allamanis et al., 2016</ref><ref type=""bibr"">, Brockschmidt et al., 2019</ref><ref type=""bibr"" target=""#b19"">, Fernandes et al., 2019)</ref> and open-vocabulary models <ref type=",0
"arning techniques for code, beyond code completion. These include techniques for code summarization <ref type=""bibr"" target=""#b8"">(Alon et al., 2019b)</ref>, bug finding <ref type=""bibr"" target=""#b6""> n problem-a non-neural but tree aware engine could outperform RNNs. In the same spirit, Alon et al. <ref type=""bibr"" target=""#b8"">(Alon et al., 2019b)</ref> had found-for code summarization problem (t",0
"res <ref type=""bibr"" target=""#b31"">[32,</ref><ref type=""bibr"" target=""#b7"">8]</ref> and descriptors <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" targ",1
"et=""#b20"">[21]</ref> or the 3D structure of target/protein <ref type=""bibr"" target=""#b29"">[30,</ref><ref type=""bibr"" target=""#b8"">9]</ref>, which are often difficult to obtain. Meanwhile, traditional",0
", DTA prediction has received much attention in recent years <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b37"">38]</ref>.</p><p>Early approa",0
"onds as edges, and a string obtained from the Simplified Molecular Input Line Entry System (SMILES) <ref type=""bibr"" target=""#b35"">[36]</ref>. Targets (or proteins) are sequences of amino acids. Bindi ), a specification in the form of a line notation for describing the structure of chemical compound <ref type=""bibr"" target=""#b35"">[36]</ref>. For example, the SMILES string of the drug in Figure <ref",0
"Boltzmann Machines (RBMs). Instead of using DBN, a nonlinear end-to-end learning model named NeoDTI <ref type=""bibr"" target=""#b33"">[34]</ref> was proposed. NeoDTI integrates variety of information fro",0
"preliminary work of a neural influence Diff usion Network (i.e., DiffNet) for social recommendation <ref type=""bibr"" target=""#b42"">[43]</ref>. DiffNet models the recursive social diffusion process for "">[19]</ref>, <ref type=""bibr"" target=""#b19"">[20]</ref>, <ref type=""bibr"" target=""#b13"">[14]</ref>, <ref type=""bibr"" target=""#b42"">[43]</ref>.</p><p>In fact, as users play a central role in social pla "">[51]</ref>, <ref type=""bibr"" target=""#b40"">[41]</ref>, <ref type=""bibr"" target=""#b47"">[48]</ref>, <ref type=""bibr"" target=""#b42"">[43]</ref>. On one hand, given the useritem interest graph, NGCF is p that the higher-order social structure is directly modeled in the recursive user embedding process <ref type=""bibr"" target=""#b42"">[43]</ref>. These graph based models showed superior performance comp ary, our main contributions are listed as follows:</p><p>? Compared to our previous work of DiffNet <ref type=""bibr"" target=""#b42"">[43]</ref>, we revisit the social recommendation problem as predictin d social recommendation model, DiffNet, for modeling the social diffusion process in recommendation <ref type=""bibr"" target=""#b42"">[43]</ref>. DiffNet advances classical embedding based models with ca at the up to K-th order social network structure is injected into the social recommendation process <ref type=""bibr"" target=""#b42"">[43]</ref>. In this part, we propose DiffNet++, an enhanced model of with user and item attributes, and are adopted as datasets of our previously proposed DiffNet model <ref type=""bibr"" target=""#b42"">[43]</ref>. The remaining two datasets of Epinions and Dianping do no t, in order to transform this model for the recommendation task. For our proposed models of DiffNet <ref type=""bibr"" target=""#b42"">[43]</ref> and DiffNet++, since both models are flexible and could be 9]</ref> and Normalized Discounted Cummulative Gain (NDCG) <ref type=""bibr"" target=""#b8"">[9]</ref>, <ref type=""bibr"" target=""#b42"">[43]</ref>. Specifically, HR measures the percentage of hit items in formance with large itemset, similar as many other works <ref type=""bibr"" target=""#b16"">[17]</ref>, <ref type=""bibr"" target=""#b42"">[43]</ref>, to evaluate the performance, for each user, we randomly s",1
"<ref type=""bibr"" target=""#b38"">[39]</ref>, and efficient training models without negative sampling <ref type=""bibr"" target=""#b5"">[6]</ref>. All these previous works focused on how to explore the soci",0
"high performance <ref type=""bibr"" target=""#b4"">[5]</ref>, <ref type=""bibr"" target=""#b7"">[8]</ref>, <ref type=""bibr"" target=""#b21"">[22]</ref>. GCNs perform node feature propagation in the graph, which entation learning <ref type=""bibr"" target=""#b4"">[5]</ref>, <ref type=""bibr"" target=""#b7"">[8]</ref>, <ref type=""bibr"" target=""#b21"">[22]</ref>. Specifically, GCNs recursively perform message passing by he neighborhood information, such that the K-th order graph structure is captured with K iterations <ref type=""bibr"" target=""#b21"">[22]</ref>. By treating the user-item interaction as a graph structur , leading to performance decrease. Other related studies have also empirically found similar trends <ref type=""bibr"" target=""#b21"">[22]</ref>, <ref type=""bibr"" target=""#b47"">[48]</ref>.</p></div> <div",0
"n performance <ref type=""bibr"" target=""#b18"">[19]</ref>, <ref type=""bibr"" target=""#b19"">[20]</ref>, <ref type=""bibr"" target=""#b13"">[14]</ref>, <ref type=""bibr"" target=""#b42"">[43]</ref>.</p><p>In fact, /ref>, <ref type=""bibr"" target=""#b43"">[44]</ref> and the user behavior enhancement based approaches <ref type=""bibr"" target=""#b13"">[14]</ref>, <ref type=""bibr"" target=""#b14"">[15]</ref>. Specifically, t=""#b14"">[15]</ref>. TrustSVD is such a representative model that shows stateof-the-art performance <ref type=""bibr"" target=""#b13"">[14]</ref>, <ref type=""bibr"" target=""#b14"">[15]</ref>. By assuming th >), social based recommendation model (SocialMF <ref type=""bibr"" target=""#b18"">[19]</ref>, TrustSVD <ref type=""bibr"" target=""#b13"">[14]</ref>, ContextMF <ref type=""bibr"" target=""#b20"">[21]</ref>, CNSR",0
"here are some recently works that also leverage the graph neural networks for social recommendation <ref type=""bibr"" target=""#b9"">[10]</ref>, <ref type=""bibr"" target=""#b44"">[45]</ref>. Specifically, G tations by fusing first order social and first-order item neighbors with non-linear neural networks <ref type=""bibr"" target=""#b9"">[10]</ref>. Researchers also proposed deep learning techniques to mode "">[35]</ref>, <ref type=""bibr"" target=""#b37"">[38]</ref>, <ref type=""bibr"" target=""#b12"">[13]</ref>, <ref type=""bibr"" target=""#b9"">[10]</ref>. E.g., with each user's direct item neighbors and social ne eling to learn the attentive weights for each social neighbor and each rated item for user modeling <ref type=""bibr"" target=""#b9"">[10]</ref>. In social contextual recommender systems, users' preferenc type=""bibr"" target=""#b43"">[44]</ref>), as well as the graph based recommendation models of GraphRec <ref type=""bibr"" target=""#b9"">[10]</ref>, PinSage <ref type=""bibr"" target=""#b47"">[48]</ref>, NGCF <r",0
"udy of the distinguishing power of some GNN variants has been initiated. In two independent studies <ref type=""bibr"" target=""#b15"">[Xu et al., 2019</ref><ref type=""bibr"" target=""#b11"">, Morris et al., ee-aware MPNNs that do use degree information. The former class of MPNNs covers the GNNs studied in <ref type=""bibr"" target=""#b15"">[Xu et al., 2019</ref><ref type=""bibr"" target=""#b11"">, Morris et al., s bounded by the WL algorithm. This result can be seen as a slight generalisation of the results in <ref type=""bibr"" target=""#b15"">[Xu et al., 2019</ref><ref type=""bibr"" target=""#b11"">, Morris et al., and degree-aware MPNNs matches that of the WL algorithm.</p><p>For anonymous MPNNs related to GNNs <ref type=""bibr"" target=""#b15"">[Xu et al., 2019</ref><ref type=""bibr"" target=""#b11"">, Morris et al., or short) is well understood. Indeed, as we will shortly see, it follows from two independent works <ref type=""bibr"" target=""#b15"">[Xu et al., 2019</ref><ref type=""bibr"" target=""#b11"">, Morris et al., L , as is indicated in Figure <ref type=""figure"" target=""#fig_1"">1</ref>. Proposition 5.2 (Based on <ref type=""bibr"" target=""#b15"">[Xu et al., 2019</ref><ref type=""bibr"" target=""#b11"">, Morris et al., ymous MPNN by using an injection h : A s → Q. What follows is in fact an adaptation of Lemma 5 from <ref type=""bibr"" target=""#b15"">[Xu et al., 2019]</ref> itself based on <ref type=""bibr"">[Zaheer et a gue that M anon is weaker than M WL . The proof is a trivial adaptation of the proofs of Lemma 2 in <ref type=""bibr"" target=""#b15"">[Xu et al., 2019]</ref> and Theorem 5 in <ref type=""bibr"" target=""#b1 t) w = (ℓ ℓ ℓ (t) M ) w ,</formula><p>as desired.</p><p>We remark that we cannot use the results in <ref type=""bibr"" target=""#b15"">[Xu et al., 2019]</ref> and <ref type=""bibr"" target=""#b11"">[Morris et x because the class M anon is more general than the class considered in those papers. The proofs in <ref type=""bibr"" target=""#b15"">[Xu et al., 2019]</ref> and <ref type=""bibr"" target=""#b11"">[Morris et can be written in the form g (t) u∈NG(v) h (t) (ℓ ℓ ℓ (t−1) u</formula><p>) , based on Lemma 5 from <ref type=""bibr"" target=""#b15"">[Xu et al., 2019]</ref>.</p><p>Suppose that ν ν ν : V → A s0 . It now",1
"has been initiated. In two independent studies <ref type=""bibr"" target=""#b15"">[Xu et al., 2019</ref><ref type=""bibr"" target=""#b11"">, Morris et al., 2019]</ref> the distinguishing power of GNNs is link rmer class of MPNNs covers the GNNs studied in <ref type=""bibr"" target=""#b15"">[Xu et al., 2019</ref><ref type=""bibr"" target=""#b11"">, Morris et al., 2019]</ref>, the latter class covers the GCNs <ref t thm.</p><p>For anonymous MPNNs related to GNNs <ref type=""bibr"" target=""#b15"">[Xu et al., 2019</ref><ref type=""bibr"" target=""#b11"">, Morris et al., 2019]</ref> and degree-aware MPNNs related to GCNs < er the graph neural network architectures <ref type=""bibr"" target=""#b4"">[Hamilton et al., 2017</ref><ref type=""bibr"" target=""#b11"">, Morris et al., 2019]</ref> defined by:</p><formula xml:id=""formula_ tly see, it follows from two independent works <ref type=""bibr"" target=""#b15"">[Xu et al., 2019</ref><ref type=""bibr"" target=""#b11"">, Morris et al., 2019]</ref> that the distinguishing power of aMPNNs et=""#fig_1"">1</ref>. Proposition 5.2 (Based on <ref type=""bibr"" target=""#b15"">[Xu et al., 2019</ref><ref type=""bibr"" target=""#b11"">, Morris et al., 2019]</ref>). The classes M anon and M WL are equall h between anonymous graph neural networks <ref type=""bibr"" target=""#b4"">[Hamilton et al., 2017</ref><ref type=""bibr"" target=""#b11"">, Morris et al., 2019]</ref> and degree-aware graph neural networks < n as a slight generalisation of the results in <ref type=""bibr"" target=""#b15"">[Xu et al., 2019</ref><ref type=""bibr"" target=""#b11"">, Morris et al., 2019</ref>]. (ii) The distinguishing power of degree ep-by-step, by GNNs that use ReLU or sign as activation function. This result refines the result in <ref type=""bibr"" target=""#b11"">[Morris et al., 2019]</ref> in that their simulation using the ReLU f of aMPNNs which are of special interest: those arising from the graph neural networks considered in <ref type=""bibr"" target=""#b11"">[Morris et al., 2019]</ref>. In Example 3.1 we established that such of the proofs of Lemma 2 in <ref type=""bibr"" target=""#b15"">[Xu et al., 2019]</ref> and Theorem 5 in <ref type=""bibr"" target=""#b11"">[Morris et al., 2019]</ref>. We show, by induction on the number of r remark that we cannot use the results in <ref type=""bibr"" target=""#b15"">[Xu et al., 2019]</ref> and <ref type=""bibr"" target=""#b11"">[Morris et al., 2019]</ref> as a black box because the class M anon i onsidered in those papers. The proofs in <ref type=""bibr"" target=""#b15"">[Xu et al., 2019]</ref> and <ref type=""bibr"" target=""#b11"">[Morris et al., 2019]</ref> relate to graph neural networks which, in nd M WL , and thus also M anon , are equally strong. The following results are known. Theorem 5.5 ( <ref type=""bibr"" target=""#b11"">[Morris et al., 2019]</ref>). (i) The classes M sign GNN and M WL are side effect, we obtain a simpler aMPNN M in M GNN , satisfying M WL M , than the one constructed in <ref type=""bibr"" target=""#b11"">[Morris et al., 2019]</ref>. The proof strategy is inspired by that o ef type=""bibr"" target=""#b11"">[Morris et al., 2019]</ref>. The proof strategy is inspired by that of <ref type=""bibr"" target=""#b11"">[Morris et al., 2019]</ref>. Crucial in the proof is the notion of ro of two, at the cost of introducing an extra parameter p ∈ A. Furthermore, the aMPNN constructed in <ref type=""bibr"" target=""#b11"">[Morris et al., 2019]</ref> uses two distinct weight matrices in A (s By the induction hypothesis, these rows are linearly independent. Following the same argument as in <ref type=""bibr"" target=""#b11"">[Morris et al., 2019]</ref>, this implies that there exists an (s t−1 ure the labelling ""refines"" ℓ ℓ ℓ (t) MWL . To do so, we again follow closely the proof strategy of <ref type=""bibr"" target=""#b11"">[Morris et al., 2019]</ref>. More specifically, we will need an analo entries having value 1 and whose size will be determined from the context. Lemma 5.9 (Lemma 9 from <ref type=""bibr"" target=""#b11"">[Morris et al., 2019]</ref>). Let C ∈ A m×w be a matrix in which all ns of the non-zero entries in µ µ µ Regarding future work, we point out that, following the work of <ref type=""bibr"" target=""#b11"">[Morris et al., 2019]</ref>, we fix the input graph in our analysis.",1
"tion-aware MPNNs using close connections with the LOCAL model for distributed graph computations of <ref type=""bibr"" target=""#b0"">[Angluin, 1980]</ref>. As such, MPNNs from <ref type=""bibr"" target=""#b",0
"""#b6"">[Jaume et al., 2019]</ref>.</p><p>We also want to compare our formalisation to the MPNNs from <ref type=""bibr"" target=""#b9"">[Loukas, 2019]</ref>. In that paper, the message functions can depend spond to MPNNs in our setting in which f assigns to each vertex a unique identifier. We remark that <ref type=""bibr"" target=""#b9"">[Loukas, 2019]</ref> shows Turing universality of position-aware MPNNs uted graph computations of <ref type=""bibr"" target=""#b0"">[Angluin, 1980]</ref>. As such, MPNNs from <ref type=""bibr"" target=""#b9"">[Loukas, 2019]</ref> can simulate our MPNNs as one could add a few ini d add a few initialisation rounds to compute f (v) and f (u). We also remark that in the MPNNs from <ref type=""bibr"" target=""#b9"">[Loukas, 2019]</ref> every vertex can also send itself a message. We p",0
"network formalisms, as reported in e.g., <ref type=""bibr"" target=""#b8"">[Kipf and Welling, 2017</ref><ref type=""bibr"" target=""#b13"">, Wu et al., 2019a</ref><ref type=""bibr"" target=""#b10"">, Meltzer et a",0
"ll need are indeed computable for algebraic numbers encoded using such a representation (see, e.g., <ref type=""bibr"" target=""#b12"">[Ouaknine and Worrell, 2014]</ref>).</p><p>Labelled graphs. Let G = (",0
"annot embed long documents, due to the sentence length limit in BERT. Concretely, we extend ProdLDA <ref type=""bibr"" target=""#b24"">(Srivastava and Sutton, 2017)</ref>, a state-of-the-art topic model t p><p>Baselines We compare our approach with the following baselines: (i) Neural-ProdLDA (N-ProdLDA) <ref type=""bibr"" target=""#b24"">(Srivastava and Sutton, 2017)</ref>  <ref type=""foot"" target=""#foot_5 entation is again passed through a hidden layer before the variational inference process. We follow <ref type=""bibr"" target=""#b24"">(Srivastava and Sutton, 2017)</ref> for the choice of the parameters. ling with BERT embeddings. More details of the architecture we extend are found in the original work<ref type=""bibr"" target=""#b24"">(Srivastava and Sutton, 2017)</ref>.</figDesc></figure> <figure xmlns nce. In general, our model provides the most coherent topics across all corpora and topic settings. <ref type=""bibr"" target=""#b24"">Srivastava and Sutton (2017)</ref> reported that NVDM obtains low coh "" target=""#b13"">(Miao et al., 2016;</ref><ref type=""bibr"" target=""#b15"">Mnih and Gregor, 2014;</ref><ref type=""bibr"" target=""#b24"">Srivastava and Sutton, 2017;</ref><ref type=""bibr"" target=""#b12"">Miao ed.</note> 			<note xmlns=""http://www.tei-c.org/ns/1.0"" place=""foot"" n=""5"" xml:id=""foot_5"">Note that<ref type=""bibr"" target=""#b24"">(Srivastava and Sutton, 2017</ref>) also propose Neural-LDA, which ha DA, which has been found to be scarcely effective in topic modeling by different researchers, though<ref type=""bibr"" target=""#b24"">(Srivastava and Sutton, 2017;</ref><ref type=""bibr"" target=""#b25"">Wan",1
"Sutton, 2017)</ref>, a state-of-the-art topic model that implements black-box variational inference <ref type=""bibr"" target=""#b20"">(Ranganath et al., 2014)</ref>, to include BERT representations. Our",1
"=""#b18"">Petterson et al., 2010)</ref>, use word relationships derived from external knowledge bases <ref type=""bibr"" target=""#b3"">(Chen et al., 2013;</ref><ref type=""bibr"" target=""#b28"">Yang et al., 2",0
"y tasks. Consequentially, researchers use BERT representations in a diverse set of NLP applications <ref type=""bibr"" target=""#b17"">(Nozza et al., 2020;</ref><ref type=""bibr"" target=""#b22"">Rogers et al",0
"xmlns=""http://www.tei-c.org/ns/1.0"" place=""foot"" n=""6"" xml:id=""foot_6"">We use the implementation of<ref type=""bibr"" target=""#b1"">(Carrow, 2018)</ref>.</note> 		</body> 		<back>  			<div type=""acknowl",0
"places the multinomial distribution over individual words in standard LDA with a product of experts <ref type=""bibr"" target=""#b9"">(Hinton, 2002)</ref> (hence the name ProdLDA).</p><p>We extend this mo",0
"""bibr"" target=""#b4"">Das et al., 2015;</ref><ref type=""bibr"" target=""#b16"">Nguyen et al., 2015;</ref><ref type=""bibr"" target=""#b18"">Petterson et al., 2010)</ref>, use word relationships derived from ex",0
"g et al., 2009)</ref> to approximated scores <ref type=""bibr"" target=""#b11"">(Lau et al., 2014;</ref><ref type=""bibr"" target=""#b21"">Röder et al., 2015)</ref>.</p><p>Topic models have inspired many exte",0
"ional Document Model (NVDM) <ref type=""bibr"" target=""#b13"">(Miao et al., 2016)</ref>; and (iii) LDA <ref type=""bibr"" target=""#b0"">(Blei et al., 2003)</ref>.</p><p>Configurations We train all models wi",0
"e=""bibr"" target=""#b27"">(Xun et al., 2017;</ref><ref type=""bibr"" target=""#b4"">Das et al., 2015;</ref><ref type=""bibr"" target=""#b16"">Nguyen et al., 2015;</ref><ref type=""bibr"" target=""#b18"">Petterson et =""bibr"" target=""#b4"">(Das et al., 2015;</ref><ref type=""bibr"" target=""#b6"">Dieng et al., 2019;</ref><ref type=""bibr"" target=""#b16"">Nguyen et al., 2015)</ref>. Even for neural topic models, there exist",0
"s selected using Google Suggest API, where the answers are entities in Freebase. CuratedTREC (TREC) <ref type=""bibr"" target=""#b1"">(Baudi? and ?ediv?, 2015)</ref>  </p></div> <div xmlns=""http://www.tei",1
"and Wikipedia hyperlinks, has also been explored recently <ref type=""bibr"">(Min et al., 2019b;</ref><ref type=""bibr"" target=""#b0"">Asai et al., 2020)</ref>. The use of dense vector representations for",1
"/ref> and more recently for mini-batch <ref type=""bibr"" target=""#b13"">(Henderson et al., 2017;</ref><ref type=""bibr"" target=""#b10"">Gillick et al., 2019)</ref>. It has been shown to be an effective str bibr"" target=""#b47"">(Yih et al., 2011;</ref><ref type=""bibr"" target=""#b14"">Huang et al., 2013;</ref><ref type=""bibr"" target=""#b10"">Gillick et al., 2019)</ref>, with applications to cross-lingual docum",0
"/ref><ref type=""bibr"" target=""#b30"">Nie et al., 2019;</ref><ref type=""bibr"">Min et al., 2019a;</ref><ref type=""bibr"" target=""#b42"">Wolfson et al., 2020)</ref>. Augmenting text-based retrieval with ext",0
"improving retrieval.</p><p>Retrieval in open-domain QA is usually implemented using TF-IDF or BM25 <ref type=""bibr"" target=""#b36"">(Robertson and Zaragoza, 2009)</ref>, which matches keywords efficien",0
"ned model <ref type=""bibr"" target=""#b8"">(Devlin et al., 2019)</ref> and a dual-encoder architecture <ref type=""bibr"" target=""#b3"">(Bromley et al., 1994)</ref>, we focus on developing the right trainin",0
"o, 2019;</ref><ref type=""bibr"" target=""#b15"">Humeau et al., 2020)</ref>. Finally, a concurrent work <ref type=""bibr"" target=""#b19"">(Khattab and Zaharia, 2020)</ref> demonstrates the feasibility of ful",0
"(or answers), without additional pretraining? By leveraging the now standard BERT pretrained model <ref type=""bibr"" target=""#b8"">(Devlin et al., 2019)</ref> and a dual-encoder architecture <ref type= assage encoders can be implemented by any neural networks, in this work we use two independent BERT <ref type=""bibr"" target=""#b8"">(Devlin et al., 2019)</ref> networks (base, uncased) and take the repr",0
"</ref> in the open Natural Questions setting <ref type=""bibr"" target=""#b22"">(Lee et al., 2019;</ref><ref type=""bibr"" target=""#b21"">Kwiatkowski et al., 2019)</ref>.</p><p>Our contributions are twofold. ataset and refer readers to their paper for the details of data preparation. Natural Questions (NQ) <ref type=""bibr"" target=""#b21"">(Kwiatkowski et al., 2019)</ref> was designed for end-to-end question",0
"uage understanding tasks for the existing VLP models is VQA. The SoTA result for VQA is from UNITER <ref type=""bibr"" target=""#b6"">[6]</ref> large model. Table <ref type=""table"" target=""#tab_3"">6</ref> Oscar B is the best among the models with equivalent size, even slightly better (0.04%) than UNITER <ref type=""bibr"" target=""#b6"">[6]</ref> large. And the Oscar L improves the SoTA overall accuracy wi other major task for the existing VLP models is NLVR2. Similarly, the SoTA model on NLVR2 is UNITER <ref type=""bibr"" target=""#b6"">[6]</ref> large. As reported in Table <ref type=""table"" target=""#tab_4 =""bibr"" target=""#b19"">19,</ref><ref type=""bibr"" target=""#b10"">10]</ref> employ BERT-like objectives <ref type=""bibr"" target=""#b6"">[6]</ref> to learn crossmodal representations from a concatenated-sequ",1
"get=""#b47"">47,</ref><ref type=""bibr"" target=""#b36"">36,</ref><ref type=""bibr"" target=""#b19"">19,</ref><ref type=""bibr"" target=""#b10"">10]</ref> employ BERT-like objectives <ref type=""bibr"" target=""#b6"">[",0
"and that these objects are often mentioned in the paired text. For example, on the MS COCO dataset <ref type=""bibr"" target=""#b21"">[21]</ref>, the percentages that an image and its paired text share a s</head><p>We have built the pre-training corpus based on the existing V+L datasets, including COCO <ref type=""bibr"" target=""#b21"">[21]</ref>, Conceptual Captions (CC) <ref type=""bibr"" target=""#b32"">[ y-used VQA v2.0 dataset <ref type=""bibr"" target=""#b9"">[9]</ref>, which is built based on the MSCOCO <ref type=""bibr"" target=""#b21"">[21]</ref> image corpus. The dataset is split into training (83k imag he widely used Karpathy split <ref type=""bibr"" target=""#b14"">[14]</ref> on the COCO caption dataset <ref type=""bibr"" target=""#b21"">[21]</ref> to conduct our experiments. Specifically, the dataset cons",0
"ref type=""bibr"" target=""#b27"">[27]</ref>, flicker30k <ref type=""bibr"" target=""#b45"">[45]</ref>, GQA <ref type=""bibr"" target=""#b13"">[13]</ref> etc. As shown in Table <ref type=""table"" target=""#tab_0"">1 nd the soft target scores. At inference, we simply use a Softmax function for prediction.</p><p>GQA <ref type=""bibr"" target=""#b13"">[13]</ref> is similar to VQA, except that GQA requires to answer an a n additional question about the reasoning process. We conduct experiments on the public GQA dataset <ref type=""bibr"" target=""#b13"">[13]</ref>. For each question, the model chooses an answer from a sha acy, which still demonstrates the superiority of Oscar pretraining. The GQA SoTA result is from NSM <ref type=""bibr"" target=""#b13"">[13]</ref>, which equips the model with more complicated reasoning. W",0
"use of anchor points for alignment modeling has been explored in natural language processing e.g., <ref type=""bibr"" target=""#b3"">[3]</ref>, to the best of our knowledge, this work is the first that e",0
"rogeneous textrich network. Meta-paths <ref type=""bibr"" target=""#b32"">[31]</ref> and motif patterns <ref type=""bibr"" target=""#b6"">[5,</ref><ref type=""bibr"" target=""#b19"">18]</ref> have been widely ado b31"">[30]</ref>, bioinformatics <ref type=""bibr"" target=""#b19"">[18]</ref>, and information networks <ref type=""bibr"" target=""#b6"">[5]</ref>. In the context of heterogeneous information networks, netwo",1
"rns in our framework to extract useful features from the heterogeneous textrich network. Meta-paths <ref type=""bibr"" target=""#b32"">[31]</ref> and motif patterns <ref type=""bibr"" target=""#b6"">[5,</ref> phs, can offer more flexibility and capture richer network semantics than the widely used meta-path <ref type=""bibr"" target=""#b32"">[31]</ref> patterns. Recent studies have shown that incorporating mot of two authors (i.e., ""Jure Leskovec"" and ""Jon Kleinberg"").</p><p>It is worth noting that meta-path <ref type=""bibr"" target=""#b32"">[31]</ref> can be viewed as a special case of motif patterns when the",1
"<ref type=""bibr"" target=""#b32"">[31]</ref> and motif patterns <ref type=""bibr"" target=""#b6"">[5,</ref><ref type=""bibr"" target=""#b19"">18]</ref> have been widely adopted to extract useful structural infor oss various domains, such as neuroscience <ref type=""bibr"" target=""#b31"">[30]</ref>, bioinformatics <ref type=""bibr"" target=""#b19"">[18]</ref>, and information networks <ref type=""bibr"" target=""#b6"">[5",1
"y applying algorithms like maximum spanning tree. The lexical patterns are either manually designed <ref type=""bibr"" target=""#b15"">[14,</ref><ref type=""bibr"" target=""#b22"">21,</ref><ref type=""bibr"" ta",0
"target=""#b2"">[1,</ref><ref type=""bibr"" target=""#b7"">6,</ref><ref type=""bibr"" target=""#b16"">15,</ref><ref type=""bibr"" target=""#b21"">20,</ref><ref type=""bibr"" target=""#b29"">28,</ref><ref type=""bibr"" tar",0
"t corpora. In pioneer studies, hierarchical topic modeling <ref type=""bibr"" target=""#b11"">[10,</ref><ref type=""bibr"" target=""#b13"">12,</ref><ref type=""bibr"" target=""#b20"">19,</ref><ref type=""bibr"" tar s many strong baselines, such as hierarchical topic models <ref type=""bibr"" target=""#b11"">[10,</ref><ref type=""bibr"" target=""#b13"">12,</ref><ref type=""bibr"" target=""#b20"">19,</ref><ref type=""bibr"" tar sters in NetTaxo. We have tested our enhanced Hierarchical Latent Dirichlet Allocation (HLDA) model <ref type=""bibr"" target=""#b13"">[12]</ref> and its performance is quite similar to HPAM++. Therefore,",0
"get=""#b13"">12,</ref><ref type=""bibr"" target=""#b20"">19,</ref><ref type=""bibr"" target=""#b38"">37,</ref><ref type=""bibr"" target=""#b39"">38]</ref> and bottom-up agglomerative clusteringbased <ref type=""bibr get=""#b13"">12,</ref><ref type=""bibr"" target=""#b20"">19,</ref><ref type=""bibr"" target=""#b38"">37,</ref><ref type=""bibr"" target=""#b39"">38]</ref>. It utilizes the same local embedding idea as our model, bu o significantly improve the quality of clustering.</p><p>Network Clustering-based Methods. CATHYHIN <ref type=""bibr"" target=""#b39"">[38]</ref> is arguably the state-of-the-art method solely based on ne but ignores network structures. • CATHYHIN++ is a method enhanced by us from the original CATHYHIN <ref type=""bibr"" target=""#b39"">[38]</ref> method. CATHYHIN <ref type=""bibr"" target=""#b39"">[38]</ref> nhanced by us from the original CATHYHIN <ref type=""bibr"" target=""#b39"">[38]</ref> method. CATHYHIN <ref type=""bibr"" target=""#b39"">[38]</ref> is a topic taxonomy construction method using network data r, venue-paper, year-paper, year range-paper, and term-paper relations. Note that, previous methods <ref type=""bibr"" target=""#b39"">[38,</ref><ref type=""bibr"" target=""#b45"">44]</ref> choose five areas been a very challenging task. Inspired by the state-of-the-art work on topic taxonomy construction <ref type=""bibr"" target=""#b39"">[38,</ref><ref type=""bibr"" target=""#b45"">44]</ref> and recent work on should be distinguishable from its sibling nodes. Following previous taxonomy construction methods <ref type=""bibr"" target=""#b39"">[38,</ref><ref type=""bibr"" target=""#b45"">44]</ref>, we perform the te",0
"ollow previous work <ref type=""bibr"" target=""#b45"">[44]</ref> and adopt the idea of local embedding <ref type=""bibr"" target=""#b14"">[13]</ref> to learn term embedding from text data. The basic idea of revious work <ref type=""bibr"" target=""#b45"">[44]</ref> as well as the original local embedding work <ref type=""bibr"" target=""#b14"">[13]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head",0
"rget=""#b3"">[2,</ref><ref type=""bibr"" target=""#b12"">11,</ref><ref type=""bibr"" target=""#b17"">16,</ref><ref type=""bibr"" target=""#b45"">44]</ref>, incorporating network structures can bring additional, val ng term embedding, topdown hierarchical clustering methods <ref type=""bibr"" target=""#b17"">[16,</ref><ref type=""bibr"" target=""#b45"">44]</ref> achieve the state-ofthe-art. For example, TaxoGen <ref type per, and term-paper relations. Note that, previous methods <ref type=""bibr"" target=""#b39"">[38,</ref><ref type=""bibr"" target=""#b45"">44]</ref> choose five areas from this dataset too, for example in <re y the state-of-the-art work on topic taxonomy construction <ref type=""bibr"" target=""#b39"">[38,</ref><ref type=""bibr"" target=""#b45"">44]</ref> and recent work on topic modeling <ref type=""bibr"" target="" ng nodes. Following previous taxonomy construction methods <ref type=""bibr"" target=""#b39"">[38,</ref><ref type=""bibr"" target=""#b45"">44]</ref>, we perform the term intrusion test. Specifically, for each 16,</ref><ref type=""bibr"" target=""#b45"">44]</ref> achieve the state-ofthe-art. For example, TaxoGen <ref type=""bibr"" target=""#b45"">[44]</ref> learns local term embedding from the documents associated leveraging both text data and network structures.</p><p>Based on our observations and previous work <ref type=""bibr"" target=""#b45"">[44]</ref>, using term embedding learned from textual contexts alone zation to our subsequent motif instance selection step. Specifically, we first follow previous work <ref type=""bibr"" target=""#b45"">[44]</ref> to learn local term embedding and obtain initial term clus dition the term embeddings to the current taxonomy node.</p><p>To this end, we follow previous work <ref type=""bibr"" target=""#b45"">[44]</ref> and adopt the idea of local embedding <ref type=""bibr"" tar h node according to its own associated (weighted) documents. Its effectiveness has been verified in <ref type=""bibr"" target=""#b45"">[44]</ref> through ablation tests.</p><p>We use skip-gram with negati ent taxonomy node. Therefore, our loss function slightly differs from the ones in the previous work <ref type=""bibr"" target=""#b45"">[44]</ref> as well as the original local embedding work <ref type=""bi aggregating clustering probability from their connected terms. This process is the same as that in <ref type=""bibr"" target=""#b45"">[44]</ref>. The aggregated probabilities of a document, multiplied by ef><ref type=""bibr"" target=""#b45"">44]</ref> choose five areas from this dataset too, for example in <ref type=""bibr"" target=""#b45"">[44]</ref>, information retrieval, computer vision, robotics, securit rmance is quite similar to HPAM++. Therefore, we only present the results of HPAM++ here. • TaxoGen <ref type=""bibr"" target=""#b45"">[44]</ref> is the state-of-the-art topic taxonomy construction method level and k = 4 for the second level of the taxonomy in both the DBLP and Yelp dataset. In TaxoGen <ref type=""bibr"" target=""#b45"">[44]</ref>, this number is set to 5 for all levels, which is not far",0
"ng motifs for node embedding leads to superior performance <ref type=""bibr"" target=""#b25"">[24,</ref><ref type=""bibr"" target=""#b42"">41,</ref><ref type=""bibr"" target=""#b46"">45]</ref> compared to convent",0
"A high-quality topic taxonomy benefits various downstream applications, such as search and indexing <ref type=""bibr"" target=""#b44"">[43]</ref>, personalized content recommendation <ref type=""bibr"" targ",0
"ng tree. The lexical patterns are either manually designed <ref type=""bibr"" target=""#b15"">[14,</ref><ref type=""bibr"" target=""#b22"">21,</ref><ref type=""bibr"" target=""#b24"">23,</ref><ref type=""bibr"" tar",0
"g is typically conducted on the entire document collection <ref type=""bibr"" target=""#b18"">[17,</ref><ref type=""bibr"" target=""#b23"">22]</ref>. However, such learning paradigm faces a major drawback in",0
"ly rely on text data <ref type=""bibr"" target=""#b3"">[2,</ref><ref type=""bibr"" target=""#b12"">11,</ref><ref type=""bibr"" target=""#b17"">16,</ref><ref type=""bibr"" target=""#b45"">44]</ref>, incorporating netw .</p><p>Among unsupervised frameworks using term embedding, topdown hierarchical clustering methods <ref type=""bibr"" target=""#b17"">[16,</ref><ref type=""bibr"" target=""#b45"">44]</ref> achieve the state-",0
"e field, and then they can quickly identify their interests, and easily acquire desired information <ref type=""bibr"" target=""#b36"">[35]</ref>. A high-quality taxonomy for business reviews on Yelp<ref",0
"ures</head><p>There have been a large number of works and debates on NIC offloading of TCP features <ref type=""bibr"" target=""#b33"">[35,</ref><ref type=""bibr"" target=""#b45"">47,</ref><ref type=""bibr"" ta",1
"ormance TCP stacks <ref type=""bibr"" target=""#b28"">[30,</ref><ref type=""bibr"" target=""#b59"">61,</ref><ref type=""bibr"" target=""#b66"">68]</ref> bypass the socket buffer to avoid extra memory copying. How",0
"pe=""bibr"" target=""#b35"">[37,</ref><ref type=""bibr"" target=""#b41"">43]</ref> or a custom RPC protocol <ref type=""bibr"" target=""#b42"">[44]</ref>, but the former requires an extra in-network support <ref P packet translation runs on NIC hardware, promising better performance. Finally, we note that eRPC <ref type=""bibr"" target=""#b42"">[44]</ref> achieves 5.0 Mtps RPC performance (vs. 3.4 Mtps of AccelTC",0
"firewalling or host networking) must be offloaded to NIC accordingly. Such NFs can be written in P4 <ref type=""bibr"" target=""#b38"">[40,</ref><ref type=""bibr"" target=""#b43"">45,</ref><ref type=""bibr"" ta",0
"label field is one of the most effective way to construct the connection between neighboring pixels <ref type=""bibr"" target=""#b4"">[5]</ref> . It decreases with the number of pixels having the same lab",1
"image as points in it. Pixels representing the same object naturally cluster in the spectral space <ref type=""bibr"" target=""#b3"">[4]</ref> . This property provides us an opportunity to segment pixels",1
") to define the class conditional probability and combined it with the MRF-based prior distribution <ref type=""bibr"" target=""#b14"">[15]</ref> . The proposed segmentation model is solved by maximum a p",0
"."">Riemannian manifold space</head><p>Image representation is very important for image segmentation <ref type=""bibr"" target=""#b17"">[18]</ref> . Riemannian manifold space is based on the characteristic",0
"f image segmentation has an essential influence on the subsequent image analysis and interpretation <ref type=""bibr"" target=""#b1"">[2]</ref> . Convolutional Neural Network (CNN) which can extract conte",0
"(η 1 ) 2 − η 2 θ 2 = 1 2(η 1 ) 2 − 2 η 2 (7)</formula><p>According to the Legendre transformations <ref type=""bibr"" target=""#b21"">[22]</ref> , potential function</p><formula xml:id=""formula_7"">under",0
"to overcome the sensitivity to cluster shapes and scales of Euclidean distance in the feature space <ref type=""bibr"" target=""#b23"">[24]</ref> . FCM_S algorithm introduces a constraint in image domain,",0
"composes an image into homogeneous regions, is an important task in remote sensing image processing <ref type=""bibr"" target=""#b0"">[1]</ref> . The accuracy of image segmentation has an essential influe",0
"</formula><p>In this paper, the Einstein summation convention r ik θ k i = k r ik θ k i is employed <ref type=""bibr"" target=""#b20"">[21]</ref> . Finally, the detected image is mapped to Riemannian mani",0
"nters, and assigns pixels to their nearest cluster when the centers of the clusters are appropriate <ref type=""bibr"" target=""#b7"">[8]</ref> . It is widely used in image segmentation due to its simplic lustering-based algorithm which segments objects according to their positions in the spectral space <ref type=""bibr"" target=""#b7"">[8]</ref> . Mahalanobis distance based FCM (MFCM) uses the Mahalanobis",0
"ain model, which can help to improve the accuracy of the classifier when there are few labeled data <ref type=""bibr"" target=""#b24"">[25]</ref>. Co-training needs to analyze data from two different ""per",1
"on their tweets and use classifiers to understand the stress levels of teen's pressure. Jin et al. <ref type=""bibr"" target=""#b14"">[15]</ref> propose an approach based on co-training, which combines W",0
"avioral habits and people social anxiety through college students' GPS and POI data. Canzian et al. <ref type=""bibr"" target=""#b17"">[18]</ref> use GPS data to predict people's depression degree. They e",0
"when they feel a lot of pressure, and in some serious cases, leading to mental or physical diseases <ref type=""bibr"" target=""#b1"">[2]</ref>. This kind of mental illness brought about by stress are oft about by stress are often neglected at the initial stage, but it may develop into a serious problem <ref type=""bibr"" target=""#b1"">[2]</ref>. Therefore, the timely detection of psychological pressure b end to exhibit reduced activity positivity, frequent use of mobile phones, and low sleeping quality <ref type=""bibr"" target=""#b1"">[2]</ref>. The sensing data provided by the mobile phone can reflect b",0
"ional instruments <ref type=""bibr"" target=""#b3"">[4]</ref>, <ref type=""bibr"" target=""#b7"">[8]</ref>- <ref type=""bibr"" target=""#b9"">[10]</ref>. For instance, the electric resistance of human skin relate e devices to monitor people's daily psychological pressure <ref type=""bibr"" target=""#b7"">[8]</ref>- <ref type=""bibr"" target=""#b9"">[10]</ref>. Typically, these devices integrate specialized sensors tha",0
"ing (AT) procedure <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b34"">35,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b39"">40]</ref> shows promising res al training (e.g., <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b39"">40,</ref><ref type=""bibr"" tar get=""#b37"">38,</ref><ref type=""bibr"" target=""#b31"">32,</ref><ref type=""bibr"" target=""#b30"">31,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" tar unreliable for generating adversarial samples during single-step adversarial training. Madry et al. <ref type=""bibr"" target=""#b21"">[22]</ref> demonstrated that models trained using adversarial samples =""bibr"" target=""#b8"">9]</ref> accepted to ICLR 2018. In this direction, adversarial training method <ref type=""bibr"" target=""#b21"">[22]</ref>, shows promising results for learning robust deep learning ls trained using EAT are still susceptible to multi-step attacks in white-box setting. Madry et al. <ref type=""bibr"" target=""#b21"">[22]</ref> demonstrated that adversarially trained model can be made show that over-fitting effect is the reason for failure to satisfy the criteria.</p><p>Madry et al. <ref type=""bibr"" target=""#b21"">[22]</ref> demonstrated that it is possible to learn robust models us rameters (θ) should be updated so as to decrease the loss on such adversarial samples. Madry et al. <ref type=""bibr"" target=""#b21"">[22]</ref> solves the maximization step by generating adversarial sam raining method <ref type=""bibr"" target=""#b12"">[13]</ref> and multi-step adversarial training method <ref type=""bibr"" target=""#b21"">[22]</ref>. Column-1 of Fig. <ref type=""figure"" target=""#fig_0"">1</re for EAT method. PGD Adversarial Training (PAT): Multi-step adversarial training method proposed by <ref type=""bibr"" target=""#b21"">[22]</ref>. At each iteration all the clean samples in the mini-batch d for EAT method. PGD Adversarial Training (PAT): Multi-step adversarial training method proposed by<ref type=""bibr"" target=""#b21"">[22]</ref>. At each iteration all the clean samples in the mini-batch s added to the image. In our experiments, we set α = /steps.</p><p>Projected Gradient Descent (PGD) <ref type=""bibr"" target=""#b21"">[22]</ref>: Initially, a small random noise sampled from Uniform dist",1
"raining method (SADS) in white-box and black-box settings. We perform the sanity tests described in <ref type=""bibr"" target=""#b6"">[7]</ref>, in order to verify that models trained using SADS are robus p://www.tei-c.org/ns/1.0""><head n=""5.4."">Sanity tests</head><p>We perform sanity tests described in <ref type=""bibr"" target=""#b6"">[7]</ref> to verify whether models trained using SADS are adversariall",0
"""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b3"">4]</ref>) and input pre-processing (e.g., <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b30"">31]</ref>) have been propose nine defense papers <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b37"">38,</ref><ref type=""bibr"" tar",0
"etwork to be smooth. On the other hand, works such as <ref type=""bibr"" target=""#b29"">[30]</ref> and <ref type=""bibr"" target=""#b35"">[36]</ref> propose a method to learn models that are provably robust",0
"ulate model's output <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b33"">34,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" targe ype=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b26"">27]</ref>. Further, Szegedy et al. <ref type=""bibr"" target=""#b33"">[34]</ref> observed that these adversarial samples are transferable a /www.tei-c.org/ns/1.0""><head n=""3."">Related Works</head><p>Following the findings of Szegedy et al. <ref type=""bibr"" target=""#b33"">[34]</ref>, various attacks (e.g., <ref type=""bibr"" target=""#b12"">[13",0
"ary in PyTorch for performing micro-batch pipeline parallelism with checkpointing proposed by GPipe <ref type=""bibr"" target=""#b10"">[11]</ref>. In particular, we develop a set of design components to e e performance. For example, AmoebaNet-B <ref type=""bibr"" target=""#b22"">[23]</ref> scaled with GPipe <ref type=""bibr"" target=""#b10"">[11]</ref> has 557 million parameters and has achieved top-1 accuracy 28]</ref> language model which has 1.5 billion parameters (see Figure <ref type=""figure"">1</ref> of <ref type=""bibr"" target=""#b10"">[11]</ref> for the effect of model scaling). However, training such a training by combining model parallelism with data pipelining, either in synchronous way as in GPipe <ref type=""bibr"" target=""#b10"">[11]</ref> or in asynchronous way as in <ref type=""bibr"" target=""#b11 must be completed before executing B i−1,j .</p><p>In addition to the micro-batch pipelining, GPipe <ref type=""bibr"" target=""#b10"">[11]</ref> further reduces the memory requirement by utilizing gradie e the efficiency of torchgpipe, we report performance benchmarks similar to that conducted by GPipe <ref type=""bibr"" target=""#b10"">[11]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head ef type=""foot"" target=""#foot_2"">5</ref> to make a fair comparison of loss due to checkpointing with <ref type=""bibr"" target=""#b10"">[11]</ref>. The model we used is our implementation of a sequential v hmark on AmoebaNet-D <ref type=""bibr"" target=""#b17"">(18,</ref><ref type=""bibr"">256)</ref>.</p><p>In <ref type=""bibr"" target=""#b10"">[11]</ref>, Cloud TPUv3s were used while we used NVIDIA Tesla P40 GPU -touse library in PyTorch for micro-batch pipeline parallelism with checkpointing proposed by GPipe <ref type=""bibr"" target=""#b10"">[11]</ref>. This library is designed and implemented in PyTorch's def arget=""#b25"">26,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b6"">7]</ref>, and recent lines of",1
"bibr"" target=""#b6"">7]</ref>, and recent lines of research questions how to find an optimal strategy <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" ta",0
"n optimal strategy <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b28"">29]</ref>. Among them, pipeli or checkpointing, and Pipeline-1, - Table <ref type=""table"">2</ref>: Speed benchmark on AmoebaNet-D <ref type=""bibr"" target=""#b17"">(18,</ref><ref type=""bibr"">256)</ref>.</p><p>In <ref type=""bibr"" targ",0
"mall, where resource consumption is computed by profiling. Specifically, we used the algorithm from <ref type=""bibr"" target=""#b1"">[2]</ref>.</p><p>As torchgpipe is built on PyTorch equipped with CUDA",0
"et=""#b15"">[16,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" targe synchronous way as in GPipe <ref type=""bibr"" target=""#b10"">[11]</ref> or in asynchronous way as in <ref type=""bibr"" target=""#b11"">[12]</ref>, PipeDream <ref type=""bibr"" target=""#b8"">[9]</ref>, and XP",0
"zing GCN, S-GCN, ChebNet and related methods. Our architecture is analogous to the inception module <ref type=""bibr"" target=""#b43"">Szegedy et al. (2015)</ref>; <ref type=""bibr"" target=""#b22"">Kazi et a ion ( <ref type=""formula"" target=""#formula_6"">4</ref>) is analogous to the popular Inception module <ref type=""bibr"" target=""#b43"">Szegedy et al. (2015)</ref> for classic CNN architectures (Figure <re",1
", human-object interaction <ref type=""bibr"" target=""#b36"">Qi et al. (2018)</ref>, computer graphics <ref type=""bibr"" target=""#b30"">Monti et al. (2016)</ref>, particle physics <ref type=""bibr"" target="" s <ref type=""bibr"" target=""#b37"">Rossi et al. (2019)</ref>, and fake news detection on social media <ref type=""bibr"" target=""#b30"">Monti et al. (2019)</ref> to mention a few. Somewhat surprisingly, of Graph attention <ref type=""bibr"" target=""#b45"">Veselkov et al. (2019)</ref> and similar mechanisms <ref type=""bibr"" target=""#b30"">Monti et al. (2017)</ref> require a more elaborate parametric aggrega",0
"get=""#b20"">21,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b45"">46,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b42"">43]</ref> and have connections get=""#b24"">25,</ref><ref type=""bibr"" target=""#b45"">46,</ref><ref type=""bibr"" target=""#b40"">41,</ref><ref type=""bibr"" target=""#b5"">6]</ref>. In these works, the losses are inspired by noise contrastive et=""#b35"">[36,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b45"">46,</ref><ref type=""bibr"" target=""#b5"">6]</ref>. Then we show how we can modify this loss to be suitable for learning framework is structurally similar to that used in <ref type=""bibr"" target=""#b45"">[46,</ref><ref type=""bibr"" target=""#b5"">6]</ref> for self-supervised contrastive learning and consists of the ar to the results for self-supervised contrastive learning <ref type=""bibr"" target=""#b45"">[46,</ref><ref type=""bibr"" target=""#b5"">6]</ref>, we found representations from the encoder to give improved p et=""#b20"">[21,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b45"">46,</ref><ref type=""bibr"" target=""#b5"">6]</ref>. The supervised contrastive loss in Eq. 4 preserves this stru e=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b45"">46]</ref> or using data augmentation <ref type=""bibr"" target=""#b5"">[6]</ref>. The major difference is that many negative pairs are used f the image and then resizing that back to the image's native resolution. In light of the findings of <ref type=""bibr"" target=""#b5"">[6]</ref> that self-supervised contrastive loss requires significantly f> -RandAugment: <ref type=""bibr"" target=""#b9"">[10]</ref> -SimAugment: A variant of the strategy of <ref type=""bibr"" target=""#b5"">[6]</ref> to sequentially apply random color distortion and Gaussian b e same form as a triplet loss with margin α = 2τ . This result is consistent with empirical results <ref type=""bibr"" target=""#b5"">[6]</ref> which show that contrastive loss performs better in general enefits the final Top-1 accuracy. We compare against previous state of the art self-supervised work <ref type=""bibr"" target=""#b5"">[6]</ref> which has used one positive which is another data augmentati the best results to train the embedding network, confirming what has been reported by previous work <ref type=""bibr"" target=""#b5"">[6]</ref>. With LARS we use a cosine learning rate decay. On the other such as RandAugment <ref type=""bibr"" target=""#b9"">[10]</ref> or the data augmentations proposed in <ref type=""bibr"" target=""#b5"">[6]</ref>, which we denote SimAugment. As we show in Table <ref type="" mented image originating from the same source image. In self-supervised contrastive learning (e.g., <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b45"">46,</ref><ref type=""bibr"" targ .0""><head>Supplementary 6. Effect of Temperature in Loss Function</head><p>Similar to previous work <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b45"">46]</ref>, we find that the te",1
"type=""bibr"" target=""#b42"">43]</ref> and have connections to the large literature on metric learning <ref type=""bibr"" target=""#b47"">[48,</ref><ref type=""bibr"" target=""#b4"">5]</ref>.</p><p>As the name s function encourages learning from hard positives and hard negatives. We also show that triplet loss <ref type=""bibr"" target=""#b47"">[48]</ref> is a special case of our loss when only a single positive ss can thus be seen to be efficient in its training. Other contrastive losses, such as triplet loss <ref type=""bibr"" target=""#b47"">[48]</ref>, often use the computationally expensive technique of hard ."">Connections to Triplet Loss</head><p>Contrastive learning is closely related to the triplet loss <ref type=""bibr"" target=""#b47"">[48]</ref>, which is one of the widely-used alternatives to cross-ent contrastive learning are metric learning and triplet losses <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b47"">48,</ref><ref type=""bibr"" target=""#b39"">40]</ref>. These losses have",1
"arget=""#b18"">19,</ref><ref type=""bibr"" target=""#b45"">46,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b42"">43]</ref> and have connections to the large literature on metric lear otivated by noise contrastive estimation and N-pair losses <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b42"">43]</ref>, wherein the ability to discriminate between signal and noi <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b31"">32]</ref> or N-pair losses <ref type=""bibr"" target=""#b42"">[43]</ref>. Typically, the loss is applied at the last layer of a dee",1
"perimented with standard optimizers such as LARS <ref type=""bibr"" target=""#b53"">[54]</ref>, RMSProp <ref type=""bibr"" target=""#b22"">[23]</ref> and SGD with momentum <ref type=""bibr"" target=""#b36"">[37]<",0
"nificant boost in many applications <ref type=""bibr"" target=""#b32"">[33]</ref>. In self-distillation <ref type=""bibr"" target=""#b23"">[24]</ref>, multiple rounds of cross-entropy training are performed b =""#b55"">[56]</ref> and CutMix <ref type=""bibr"" target=""#b54"">[55]</ref>, and knowledge distillation <ref type=""bibr"" target=""#b23"">[24]</ref>.</p><p>Recent years have seen significantly more powerful",0
"nd a large body of literature is devoted to finding efficient ways to perform hyperparameter tuning <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target",0
"n data or natural corruptions. This has been shown not only with adversarially constructed examples <ref type=""bibr"" target=""#b15"">[16]</ref>, but also with naturally occurring variations such as nois",0
"eves very good top-1 accuracy on the ImageNet dataset on the ResNet-50 and ResNet-200 architectures <ref type=""bibr"" target=""#b19"">[20]</ref>. On ResNet-50 with Auto-Augment <ref type=""bibr"" target=""# chieve state-of-the-art top-1 accuracies for ImageNet on the ResNet-50 and ResNet-200 architectures <ref type=""bibr"" target=""#b19"">[20]</ref>. In <ref type=""bibr"" target=""#b48"">[49]</ref> introduces t ation vectors. We experiment with two commonly used encoder architectures, ResNet-50 and ResNet-200 <ref type=""bibr"" target=""#b19"">[20]</ref>, where the activations of the final pooling layer (D E = 2 f type=""table"" target=""#tab_0"">1</ref> shows results for ResNet-50 and ResNet-200 (we use ResNet-v1 <ref type=""bibr"" target=""#b19"">[20]</ref>). The supervised contrastive loss performs better than cro",0
"get=""#b24"">25,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b45"">46,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" targe et=""#b49"">[50,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b45"">46,</ref><ref type=""bibr"" target=""#b40"">41,</ref><ref type=""bibr"" tar of the art results <ref type=""bibr"" target=""#b35"">[36,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b45"">46,</ref><ref type=""bibr"" target=""#b5"">6]</ref>. Then we show how we source image. In self-supervised contrastive learning (e.g., <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b45"">46,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" tar umber of negatives <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b45"">46,</ref><ref type=""bibr"" target=""#b5"">6]</ref>. The supervised contr ither co-occurence <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b45"">46]</ref> or using data augmentation <ref type=""bibr"" target=""#b5"">[6 mperature in Loss Function</head><p>Similar to previous work <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b45"">46]</ref>, we find that the temperature used in the loss function (fo ng Framework</head><p>Our representation learning framework is structurally similar to that used in <ref type=""bibr"" target=""#b45"">[46,</ref><ref type=""bibr"" target=""#b5"">6]</ref> for self-supervised ayer (for more details see Sec. 4). Similar to the results for self-supervised contrastive learning <ref type=""bibr"" target=""#b45"">[46,</ref><ref type=""bibr"" target=""#b5"">6]</ref>, we found representa",0
"nt papers. The cross-entropy loss was introduced as a powerful loss function to train deep networks <ref type=""bibr"" target=""#b37"">[38,</ref><ref type=""bibr"" target=""#b0"">1,</ref><ref type=""bibr"" targ",0
"hich may not be mapped correctly, resulting in a worse representation. ferent ways. Label smoothing <ref type=""bibr"" target=""#b44"">[45]</ref> makes a fuzzy distinction between correct and incorrect la practice have been approaches that change the reference label distribution, such as label smoothing <ref type=""bibr"" target=""#b44"">[45,</ref><ref type=""bibr"" target=""#b32"">33]</ref>, data augmentation",0
"dard data augmentations such as AutoAugment <ref type=""bibr"" target=""#b8"">[9]</ref> and RandAugment <ref type=""bibr"" target=""#b9"">[10]</ref>; we also compare to CutMix <ref type=""bibr"" target=""#b54"">[ three different options:</p><p>-AutoAugment: <ref type=""bibr"" target=""#b8"">[9]</ref> -RandAugment: <ref type=""bibr"" target=""#b9"">[10]</ref> -SimAugment: A variant of the strategy of <ref type=""bibr"" ervised contrastive loss to changes in hyperparameters. We compare three augmentations (RandAugment <ref type=""bibr"" target=""#b9"">[10]</ref>, AutoAugment <ref type=""bibr"" target=""#b8"">[9]</ref> and Si so note that AutoAugment is faster to implement than other augmentation schemes such as RandAugment <ref type=""bibr"" target=""#b9"">[10]</ref> or the data augmentations proposed in <ref type=""bibr"" targ RandAugment since that has shown to affect performance when training models with cross entropy loss <ref type=""bibr"" target=""#b9"">[10]</ref>. Fig. <ref type=""figure"" target=""#fig_2"">7</ref> show that ued use of cross-entropy to achieve state of the art results <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b50"">51,</ref><ref type=""bibr"" targ",0
"e performed by using the ""soft"" labels from previous rounds as reference class distributions. Mixup <ref type=""bibr"" target=""#b55"">[56]</ref> and related data augmentation strategies create explicit n target=""#b44"">[45,</ref><ref type=""bibr"" target=""#b32"">33]</ref>, data augmentations such as Mixup <ref type=""bibr"" target=""#b55"">[56]</ref> and CutMix <ref type=""bibr"" target=""#b54"">[55]</ref>, and ing data augmentation strategies such as CutMix <ref type=""bibr"" target=""#b54"">[55]</ref> and MixUp <ref type=""bibr"" target=""#b55"">[56]</ref> into supervised contrastive learning could potentially imp both pre-training and training the linear classifier is optimal. We leave experimenting with MixUp <ref type=""bibr"" target=""#b55"">[56]</ref> or CutMix <ref type=""bibr"" target=""#b54"">[55]</ref>   Furt",0
"o learn embeddings <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b56"">57,</ref><ref type=""bibr"" target=""#b57"">58,</ref><ref type=""bibr"" target=""#b34"">35]</ref>: the typical setup",0
"ibr"" target=""#b5"">6]</ref>. In these works, the losses are inspired by noise contrastive estimation <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b31"">32]</ref> or N-pair losses < sed contrastive loss (Eq. 4) is largely motivated by noise contrastive estimation and N-pair losses <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b42"">43]</ref>, wherein the abili",0
"Transformer also uses absolute position encoding to capture the sequential information. Inspired by <ref type=""bibr"" target=""#b22"">Yan et al. (2019)</ref>, we think commutativity of the vector inner d e train, dev, test split as <ref type=""bibr"">Gui et al. (2019b)</ref>. We take BiLSTM-CRF and TENER <ref type=""bibr"" target=""#b22"">(Yan et al., 2019)</ref> as baseline models. TENER is a Transformer u ine models and other lexicon-based models on four Chinese NER datasets. Our model outperforms TENER <ref type=""bibr"" target=""#b22"">(Yan et al., 2019</ref>) by 1.72 in average F1 score. For lattice LST",1
"words at different window sizes. However, RNN and CNN are hard to model long-distance dependencies <ref type=""bibr"" target=""#b19"">(Vaswani et al., 2017)</ref>, which may be useful in NER, such as cor icated.</p><p>In this paper, we propose FLAT: Flat LAttice Transformer for Chinese NER. Transformer <ref type=""bibr"" target=""#b19"">(Vaswani et al., 2017)</ref> adopts fully-connected selfattention to ere W r is a learnable parameter, ? denotes the concatenation operator, and p d is calculated as in <ref type=""bibr"" target=""#b19"">Vaswani et al. (2017)</ref>,</p><formula xml:id=""formula_11"">p (2k) d",1
"ref type=""bibr"" target=""#b25"">Zhang and Yang (2018)</ref>, and 'LS' denotes the lexicon released by <ref type=""bibr"" target=""#b11"">Li et al. (2018)</ref>. The result of other models are from their ori are the same as <ref type=""bibr"" target=""#b25"">Zhang and Yang (2018)</ref>. When comparing with CGN <ref type=""bibr"" target=""#b11"">(Li et al., 2018)</ref>, we use the same lexicon as CGN. The way to s r lattice LSTM, our model has an average F1 improvement of 1.51 over it. When using another lexicon <ref type=""bibr"" target=""#b11"">(Li et al., 2018)</ref>, our model also outperforms CGN by 0.73 in av",0
"raph Network (LGN) <ref type=""bibr"">(Gui et al., 2019b)</ref> and Collaborative Graph Network (CGN) <ref type=""bibr"" target=""#b17"">(Sui et al., 2019)</ref>. While sequential structure is still importa igure"">1</ref>(a), self-matched words of ""? (Drug)"" are ""??? ?(Renhe Pharmacy)"" and ""?? (Pharmacy)"" <ref type=""bibr"" target=""#b17"">(Sui et al., 2019)</ref>. Experimental results show our model outperf ncy and are hard to model long-distance dependencies. <ref type=""bibr"">Gui et al. (2019b)</ref> and <ref type=""bibr"" target=""#b17"">Sui et al. (2019)</ref> leveraged a lexicon and character sequence to",0
"nition (NER) plays an indispensable role in many downstream natural language processing (NLP) tasks <ref type=""bibr"" target=""#b1"">(Chen et al., 2015;</ref><ref type=""bibr"" target=""#b4"">Diefenbach et a",0
"=""bibr"" target=""#b23"">Yang et al., 2017;</ref><ref type=""bibr"" target=""#b12"">Liu et al., 2017;</ref><ref type=""bibr"" target=""#b18"">Sun et al., 2020)</ref>, Chinese NER is more difficult since it usual",0
"to 1 https://github.com/fastnlp/fastNLP indicate lattice structure. In Chinese-source translation, <ref type=""bibr"" target=""#b21"">Xiao et al. (2019)</ref> take the absolute position of nodes' first c",0
"and k denotes the index of dimension of position encoding. Then we use a variant of self-attention <ref type=""bibr"" target=""#b3"">(Dai et al., 2019)</ref> to leverage the relative span position encodi",0
"by a Condiftional Random Field (CRF) <ref type=""bibr"" target=""#b8"">(Lafferty et al., 2001)</ref>   <ref type=""bibr"" target=""#b7"">Sun, 2016)</ref>. We show statistics of these datasets in Table <ref t",0
"the rest of the application using multiple parallel IQs <ref type=""bibr"" target=""#b14"">[15]</ref>, <ref type=""bibr"" target=""#b15"">[16]</ref>. This is effectively a limited form of OoO scheduling. How ependence chains may impede the exploitation of ILP and MLP in such parallel InO scheduling windows <ref type=""bibr"" target=""#b15"">[16]</ref>, <ref type=""bibr"" target=""#b16"">[17]</ref>.</p><p>To sum u thereby eliminating the need for associative LQ searches <ref type=""bibr"" target=""#b14"">[15]</ref>, <ref type=""bibr"" target=""#b15"">[16]</ref>. However, this approach could significantly degrade perfor ws the performance of Load Slice Core (LSC) <ref type=""bibr"" target=""#b14"">[15]</ref>, Freeway core <ref type=""bibr"" target=""#b15"">[16]</ref>, CASINO core, and OoO core, normalized to that of an InO c ependent slice blocks the issue of younger, independent slices. To address this limitation, Freeway <ref type=""bibr"" target=""#b15"">[16]</ref> introduces a dependence-aware slice scheduling policy. In ues built upon an energy-efficient stall-on-use InO core <ref type=""bibr"" target=""#b14"">[15]</ref>, <ref type=""bibr"" target=""#b15"">[16]</ref>. However, the various shapes and sizes of dependence chain ever, the various shapes and sizes of dependence chains could restrict their ability to exploit ILP <ref type=""bibr"" target=""#b15"">[16]</ref>, <ref type=""bibr"" target=""#b16"">[17]</ref>.</p><p>2) Energ",1
"target=""#b51"">[51]</ref>, <ref type=""bibr"" target=""#b52"">[52]</ref>, and speculative pre-execution <ref type=""bibr"" target=""#b53"">[53]</ref>, <ref type=""bibr"" target=""#b54"">[54]</ref>, <ref type=""bib",0
""">[61]</ref>, <ref type=""bibr"" target=""#b62"">[62]</ref>, <ref type=""bibr"" target=""#b63"">[63]</ref>, <ref type=""bibr"" target=""#b64"">[64]</ref>, <ref type=""bibr"" target=""#b65"">[65]</ref> or to reduce th",0
"es (RAMs), and accessed multiple times by each instruction <ref type=""bibr"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b2"">[3]</ref>.</p><p>For decades, ried to make an OoO core more energy efficient by addressing the complexity of the scheduling logic <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b3"">[4]</ref> or reducing the acc t, SpecInO[2, 2] models show some performance improvements, but they are less than those of SpecInO <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b0"">1]</ref> models. This is becaus ""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b0"">1]</ref> models. This is because SpecInO <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b0"">1]</ref> has more opportunities ,</ref><ref type=""bibr"" target=""#b0"">1]</ref> has more opportunities for speculative issue. SpecInO <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b1"">2]</ref> slides too fast when i structions, even if the younger one becomes ready in the next cycle, which can be caught by SpecInO <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b0"">1]</ref>. Instructions exiting e=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b0"">1]</ref>. Instructions exiting SpecInO <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b1"">2]</ref> should wait until they f type=""figure"">4</ref> illustrates how instructions are renamed at the S-IQ head, assuming SpecInO <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b0"">1]</ref>. To provide a better u either the S-IQ (S-Issue) or the IQ (Issue), and performance with various IQ sizes assuming SpecInO <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b0"">1]</ref> with unlimited other r ecreases. Based on these results, we conclude that the optimal configuration of the S-IQ is SpecInO <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b0"">1]</ref>.</p></div> <div xmlns= nstructions can be issued at the head of any IQ. Each of the S-IQ and IQ are configured with SpecInO<ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b0"">1]</ref>. Conditional register </ref> has more opportunities for speculative issue. SpecInO <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b1"">2]</ref> slides too fast when it cannot find ready-to-execute instruct e=""bibr"" target=""#b0"">1]</ref>. Instructions exiting SpecInO <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b1"">2]</ref> should wait until they reach the IQ head, which significantly",0
"posed either to improve the accuracy of load speculation <ref type=""bibr"" target=""#b60"">[60]</ref>, <ref type=""bibr"" target=""#b61"">[61]</ref>, <ref type=""bibr"" target=""#b62"">[62]</ref>, <ref type=""bib",0
"tional LSTM + CRF), respectively, using public word embedding, character features and word features <ref type=""bibr"" target=""#b8"">[6,</ref><ref type=""bibr"">4]</ref>. We explore existing word embedding",1
"Third, a hybrid NER implements both the rule-based machine and deep learning, for instance, Bio-Ner <ref type=""bibr"" target=""#b17"">[15]</ref> that was experimented with a rule-based and classification",0
"state-of-the-art performance on many tasks, including NER <ref type=""bibr"" target=""#b15"">[13,</ref><ref type=""bibr"" target=""#b16"">14]</ref>. Third, a hybrid NER implements both the rule-based machine",0
"-based algorithm applies a set of rules in order to extract patterns, i.e., rule base for Malay NER <ref type=""bibr"" target=""#b12"">[10]</ref>.</p><p>With the emergence of the machine and deep learning",0
"s the usage of structured and unstructured techniques, such as CRF that was implemented for DrugNER <ref type=""bibr"" target=""#b13"">[11]</ref>. While in deep learning, various kind of neural network wo",0
"attack-agnostic manner, except a few touches on denoising <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b28"">29]</ref> and obfuscating gradients <ref type=""bibr"" target=""#b10"">[1 le FPD can circumvent the structure-replaced white-box attack. Our proposal is partially related to <ref type=""bibr"" target=""#b28"">[29]</ref>, as the denoising layers in our FPD are inspired by their ng layers in our FPD are inspired by their feature denoising approach. Nevertheless, different from <ref type=""bibr"" target=""#b28"">[29]</ref>, the principle behind our FPD is to improve the intrinsic antic information. We will compare the performance between FPD-enhanced CNN and the CNN enhanced by <ref type=""bibr"" target=""#b28"">[29]</ref> in Section 4.1.</p></div> <div xmlns=""http://www.tei-c.org the Gaussian filtering operator, the dot product operator helps improve the adversarial robustness <ref type=""bibr"" target=""#b28"">[29]</ref>. Meanwhile, as the dot product operator does not involve e framework structure through the exploration study. Moreover, we compare with the most related work <ref type=""bibr"" target=""#b28"">[29]</ref> as well. In the comparison experiments, we focus on compar a><p>Comparison with the Related Work As mentioned in Section 2, the denoising approach proposed in <ref type=""bibr"" target=""#b28"">[29]</ref> is similar to our denoising layers in FPD. Therefore, we c /ref> is similar to our denoising layers in FPD. Therefore, we conduct a comparison experiment with <ref type=""bibr"" target=""#b28"">[29]</ref> as well. In Table <ref type=""table"" target=""#tab_5"">1</ref /ref> as well. In Table <ref type=""table"" target=""#tab_5"">1</ref>, X represents the enhanced CNN by <ref type=""bibr"" target=""#b28"">[29]</ref>. We observe that our F 2I−Mid outperforms X . Especially,",1
"dition, we separately train a simple three layers fully-connected network as the substitute network <ref type=""bibr"" target=""#b22"">[23]</ref> for each network.</p></div> <div xmlns=""http://www.tei-c.o",0
"image quilting, total variance minimization and quantization <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b10"">11]</ref>. Pixel denoising ap",0
"pe=""bibr"" target=""#b19"">20]</ref>, while others utilize the decision boundary to attack the network <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b6"">7]</ref>. Black-box attacks mai",0
"most convolutional layers are very sensitive to perturbations brought by adversarial samples (e.g., <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b30"">31]</ref>), resulting in miscl",0
"le neural networks <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b28"">29,</ref><ref type=""bibr"" target=""#b3"">4]</ref> could achieve our goa ural network (INN) <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b28"">29,</ref><ref type=""bibr"" target=""#b31"">32,</ref><ref type=""bibr"" tar generative models <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b28"">29,</ref><ref type=""bibr"" target=""#b3"">4]</ref>. It is equivalent to",1
"tional image downscaling approaches employ frequency-based kernels, such as Bilinear, Bicubic, etc. <ref type=""bibr"" target=""#b40"">[41]</ref>, as a low-pass filter to sub-sample the input HR images in lid visually pleasing LR images. To achieve this, we utilize the widely acknowledged Bicubic method <ref type=""bibr"" target=""#b40"">[41]</ref> to guide the downscaling process of our model. Let y (n) g",0
"ing in the same downscaled LR image. Hence, this inverse task is usually considered to be ill-posed <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b54"">55,</ref><ref type=""bibr"" ta",0
"target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b44"">45,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b39"">40]</ref> show promising result",0
"> and easily fit for screens with different resolution while maintaining visually valid information <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b48"">49]</ref>. Meanwhile, many o scaling and upscaling as two separate and independent tasks, most recently, there have been efforts <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b33"">34,</ref><ref type=""bibr"" ta or L 2 loss. We call it the LR guidance loss. This practice has also been adopted in the literature <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b48"">49]</ref>. HR Reconstruction 9"">50,</ref><ref type=""bibr"" target=""#b13"">14]</ref>; (2) downscaling with upscaling-optimal models <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b33"">34,</ref><ref type=""bibr"" ta scaling-optimal downscaling method as an encoder which is jointly trained with an upscaling decoder <ref type=""bibr"" target=""#b25"">[26]</ref> or existing SR modules <ref type=""bibr"" target=""#b33"">[34, learn a downscaling model that is optimal to the post-upscaling operation. For instance, Kim et al. <ref type=""bibr"" target=""#b25"">[26]</ref> proposed a task-aware downscaling model based on an auto-e",0
"vision <ref type=""bibr"" target=""#b51"">(Zheng et al., 2016)</ref>, Neural Machine Translation (NMT; <ref type=""bibr"" target=""#b12"">Cheng et al., 2018)</ref>, and ASR <ref type=""bibr"" target=""#b45"">(Sp 6)</ref> presented a general method to stabilize model predictions against small input distortions. <ref type=""bibr"" target=""#b12"">Cheng et al. (2018)</ref> continued their work and developed the adve",1
"rable to different noise distributions at test time? Inspired by recent research in computer vision <ref type=""bibr"" target=""#b51"">(Zheng et al., 2016)</ref>, Neural Machine Translation (NMT; <ref typ model using a mixture of noisy and clean samples.</p><p>• We implement a stability training method <ref type=""bibr"" target=""#b51"">(Zheng et al., 2016)</ref>, adapted to the sequence labeling scenario r method to improve robustness is to design a representation that is less sensitive to noisy input. <ref type=""bibr"" target=""#b51"">Zheng et al. (2016)</ref> presented a general method to stabilize mod",1
"Neural Machine Translation (NMT; <ref type=""bibr"" target=""#b12"">Cheng et al., 2018)</ref>, and ASR <ref type=""bibr"" target=""#b45"">(Sperber et al., 2017)</ref>, we propose two Noise-Aware Training (NA mputer vision <ref type=""bibr"" target=""#b27"">(Krizhevsky et al., 2012)</ref> and speech recognition <ref type=""bibr"" target=""#b45"">(Sperber et al., 2017)</ref>.</p><p>During training, we artificially",1
"only a few works addressed it explicitly <ref type=""bibr"" target=""#b38"">(Piktus et al., 2019;</ref><ref type=""bibr"" target=""#b25"">Karpukhin et al., 2019)</ref>. Other methods must rely on the noise t",0
"s, in contrast to the black-box scenario <ref type=""bibr"" target=""#b4"">(Alzantot et al., 2018;</ref><ref type=""bibr"" target=""#b21"">Gao et al., 2018)</ref>, where the attacker can only sample model pre",0
"out relying on prior error correction, which, in case of OCR errors, is still far from being solved <ref type=""bibr"" target=""#b13"">(Chiron et al., 2017;</ref><ref type=""bibr"" target=""#b41"">Rigaud et a",0
"in an upstream task are propagated downstream, diminishing the performance of the end-to-end system <ref type=""bibr"" target=""#b3"">(Alex and Burns, 2014)</ref>. While humans can easily cope with typos, =""#b15"">(Derczynski et al., 2013)</ref>. Noise can also be introduced in an upstream task, like OCR <ref type=""bibr"" target=""#b3"">(Alex and Burns, 2014)</ref> or ASR <ref type=""bibr"" target=""#b10"">(Ch ten OOV tokens, and therefore they cause more recognition errors. In the document processing field, <ref type=""bibr"" target=""#b3"">Alex and Burns (2014)</ref> studied NER performed on several digitized",0
"the downstream task. Most popular post-correction techniques include correction candidates ranking <ref type=""bibr"" target=""#b19"">(Fivez et al., 2017;</ref><ref type=""bibr"" target=""#b20"">Flor et al.,",0
"e evaluate our methods on real OCR errors and misspellings against state-of-the-art baseline models <ref type=""bibr"" target=""#b37"">(Peters et al., 2018;</ref><ref type=""bibr"" target=""#b2"">Akbik et al. word units to represent textual tokens. We use the BERT BASE model in our experiments.</p><p>• ELMo <ref type=""bibr"" target=""#b37"">(Peters et al., 2018)</ref> utilizes a linear combination of hidden s",0
"l., 2013)</ref>, sequence to sequence models <ref type=""bibr"" target=""#b0"">(Afli et al., 2016;</ref><ref type=""bibr"" target=""#b42"">Schmaltz et al., 2017)</ref> and hybrid systems <ref type=""bibr"" targ",0
"corpus <ref type=""bibr"" target=""#b33"">(Namysl and Konya, 2019)</ref> using the Tesseract OCR engine <ref type=""bibr"" target=""#b44"">(Smith, 2007)</ref>. Moreover, we employed two sets of misspellings r",0
"r η train from 10 to 30%, which roughly corresponds to the label-preserving noise range. Similar to <ref type=""bibr"" target=""#b23"">Heigold et al. (2018)</ref> and <ref type=""bibr"" target=""#b11"">Cheng stness to noise is to augment the training data with samples perturbed using a similar noise model. <ref type=""bibr"" target=""#b23"">Heigold et al. (2018)</ref>  </p><formula xml:id=""formula_5"">LD = 0 L",0
"against state-of-the-art baseline models <ref type=""bibr"" target=""#b37"">(Peters et al., 2018;</ref><ref type=""bibr"" target=""#b2"">Akbik et al., 2018;</ref><ref type=""bibr"" target=""#b16"">Devlin et al., and should also be able to handle misspelled text and out-of-vocabulary (OOV) tokens:</p><p>• FLAIR <ref type=""bibr"" target=""#b2"">(Akbik et al., 2018</ref>) learns a Bidi-rectional Language Model (BiL",0
"posed a local classification-based method based on structure similarity and side effect similarity. <ref type=""bibr"" target=""#b4"">Ferdousi et al. (2017)</ref> predicted DDIs based on drug functional s",1
"target=""#b32"">(Wishart et al., , 2006;;</ref><ref type=""bibr"" target=""#b31"">Wang et al., 2009;</ref><ref type=""bibr"" target=""#b8"">Kanehisa et al., 2010;</ref><ref type=""bibr"" target=""#b12"">Kuhn et al.",0
"e=""bibr"" target=""#b15"">Li et al., 2010;</ref><ref type=""bibr"" target=""#b11"">Knox et al., 2011;</ref><ref type=""bibr"" target=""#b13"">Law et al., 2014)</ref>.</p><p>Many machine learning-based DDI predic",0
"et=""#b25"">Shi et al. (2016)</ref> presented a triple matrix factorization-based method named TMFUF. <ref type=""bibr"" target=""#b35"">Yu et al. (2018)</ref> developed a novel method DDINMF based on the s",0
"ods. Therefore, machine learning methods can be applied for predicting DDIs to reduce time and cost <ref type=""bibr"" target=""#b33"">(Wishart et al., 2008</ref><ref type=""bibr"" target=""#b32"">(Wishart et",0
"ER score, significantly outperforming previous BERT and Graph Neural Network (GNN) based approaches <ref type=""bibr"" target=""#b34"">(Zhou et al., 2019)</ref>. Our experiments demonstrate KGAT's strong 2018)</ref> further incorporates evidence identification to improve claim verification.</p><p>GEAR <ref type=""bibr"" target=""#b34"">(Zhou et al., 2019)</ref> formulates claim verification as a graph re tei-c.org/ns/1.0""><head n=""3.1"">Reasoning with Evidence Graph</head><p>Similar to previous research <ref type=""bibr"" target=""#b34"">(Zhou et al., 2019)</ref>, KGAT constructs the evidence graph G by us narios and produces a probability P (y|c, D) to predict claim label y. Different from previous work <ref type=""bibr"" target=""#b34"">(Zhou et al., 2019)</ref>, we follow the standard graph label predict sentation v p . The aggregation is done by a graph attention mechanism, the same with previous work <ref type=""bibr"" target=""#b34"">(Zhou et al., 2019)</ref>.</p><p>It first calculate the attention wei n</head><p>The per-node predictions are combined by the ""readout"" function in graph neural networks <ref type=""bibr"" target=""#b34"">(Zhou et al., 2019)</ref>, where KGAT uses node kernels to learn the ds without pre-training. BERT-pair, BERT-concat and GEAR are three baselines from the previous work <ref type=""bibr"" target=""#b34"">(Zhou et al., 2019)</ref>. BERT-pair and BERTconcat regard claim-evid eriments are all based on ESIM sentence retrieval, which is the one used by GEAR, our main baseline <ref type=""bibr"" target=""#b34"">(Zhou et al., 2019)</ref>.</p></div> <div xmlns=""http://www.tei-c.org head n=""6"">Case Study</head><p>Table <ref type=""table"">5</ref> shows the example claim used in GEAR <ref type=""bibr"" target=""#b34"">(Zhou et al., 2019)</ref> and the evidence sentences retrieved by ESI htweight backpacker, inventor, author and global adventurer. Label: SUPPORT Table5: An example claim<ref type=""bibr"" target=""#b34"">(Zhou et al., 2019)</ref> whose verification requires multiple pieces ""bibr"" target=""#b5"">(Devlin et al., 2019;</ref><ref type=""bibr"" target=""#b13"">Li et al., 2019;</ref><ref type=""bibr"" target=""#b34"">Zhou et al., 2019;</ref><ref type=""bibr"" target=""#b24"">Soleimani et a d is kept the same with previous work <ref type=""bibr"" target=""#b9"">(Hanselowski et al., 2018;</ref><ref type=""bibr"" target=""#b34"">Zhou et al., 2019;</ref><ref type=""bibr"" target=""#b24"">Soleimani et a l keeps the same as the previous work <ref type=""bibr"" target=""#b9"">(Hanselowski et al., 2018;</ref><ref type=""bibr"" target=""#b34"">Zhou et al., 2019)</ref>. The base version of BERT is used to impleme KGAT is the best on all testing scenarios. With ESIM sentence retrieval, same as the previous work <ref type=""bibr"" target=""#b34"">(Zhou et al., 2019;</ref><ref type=""bibr"" target=""#b9"">Hanselowski et",1
"variety of relevance match signals and shows strong performance in various ad-hoc retrieval dataset <ref type=""bibr"" target=""#b3"">(Dai and Callan, 2019)</ref>. Recent research also has shown kernels c",1
"e=""bibr"" target=""#b13"">Li et al., 2019;</ref><ref type=""bibr"" target=""#b34"">Zhou et al., 2019;</ref><ref type=""bibr"" target=""#b24"">Soleimani et al., 2019)</ref>.</p><p>The recent development of neural target=""#b9"">(Hanselowski et al., 2018;</ref><ref type=""bibr"" target=""#b34"">Zhou et al., 2019;</ref><ref type=""bibr"" target=""#b24"">Soleimani et al., 2019)</ref>. For a given claim, it first utilizes t h based reasoning models. With BERT based sentence retrieval, our KGAT also outperforms BERT (Base) <ref type=""bibr"" target=""#b24"">(Soleimani et al., 2019)</ref> by almost 1% FEVER score, showing cons type=""bibr"" target=""#b25"">(Thorne et al., 2018a)</ref>.</p><p>performs the corresponding version of <ref type=""bibr"" target=""#b24"">Soleimani et al. (2019)</ref>. KGAT with RoBERTa performs the best co ://www.tei-c.org/ns/1.0"" type=""table"" xml:id=""tab_0""><head>Table 1 :</head><label>1</label><figDesc><ref type=""bibr"" target=""#b24"">Soleimani et al. (2019)</ref>;Nie et al.  Statistics of FEVER Dataset",0
", 2016;</ref><ref type=""bibr"">Radford et al., 2018;</ref><ref type=""bibr"">Peters et al., 2018;</ref><ref type=""bibr"" target=""#b13"">Li et al., 2019)</ref> to verify the claim. The NLI task aims to clas n in FEVER and achieved better performance <ref type=""bibr"" target=""#b5"">(Devlin et al., 2019;</ref><ref type=""bibr"" target=""#b13"">Li et al., 2019;</ref><ref type=""bibr"" target=""#b34"">Zhou et al., 201",0
"words. This observation of the scattered dotproduct attention is consistent with previous research <ref type=""bibr"" target=""#b2"">(Clark et al., 2019)</ref>. As shown in the next case study, the edge ning is conducted. This seems to be a common challenge of the dot-product attention in Transformers <ref type=""bibr"" target=""#b2"">(Clark et al., 2019)</ref>.</p></div> <div xmlns=""http://www.tei-c.org",0
"=""bibr"" target=""#b8"">Guo et al., 2016;</ref><ref type=""bibr"" target=""#b27"">Xiong et al., 2017;</ref><ref type=""bibr"" target=""#b4"">Dai et al., 2018)</ref>. One of the effective ways to model text match xt matches is to leverage matching kernels <ref type=""bibr"" target=""#b27"">(Xiong et al., 2017;</ref><ref type=""bibr"" target=""#b4"">Dai et al., 2018)</ref>, which summarize word or phrase interactions i </p><p>) from the translation matrix M q→p <ref type=""bibr"" target=""#b27"">(Xiong et al., 2017;</ref><ref type=""bibr"" target=""#b4"">Dai et al., 2018;</ref><ref type=""bibr"" target=""#b22"">Qiao et al., 201",0
"eline <ref type=""bibr"" target=""#b25"">(Thorne et al., 2018a)</ref> with a three-step pipeline system <ref type=""bibr"" target=""#b0"">(Chen et al., 2017a)</ref>: document retrieval, sentence retrieval and",0
"CLS]"" hidden state to ranking score. Pairwise loss is used to optimize the ranking model. Some work <ref type=""bibr"" target=""#b32"">(Zhao et al., 2020;</ref><ref type=""bibr"" target=""#b29"">Ye et al., 20",0
"r"" target=""#b15"">(Luken et al., 2018;</ref><ref type=""bibr"" target=""#b31"">Yoneda et al., 2018;</ref><ref type=""bibr"" target=""#b9"">Hanselowski et al., 2018)</ref>. TwoWingOS <ref type=""bibr"" target=""#b ntence retrieval, same as the previous work <ref type=""bibr"" target=""#b34"">(Zhou et al., 2019;</ref><ref type=""bibr"" target=""#b9"">Hanselowski et al., 2018)</ref>, KGAT outperforms the graph attention .0 task and BERT based models.</p><p>Three top models in FEVER 1.0 shared task are compared. Athene <ref type=""bibr"" target=""#b9"">(Hanselowski et al., 2018)</ref> and UNC NLP <ref type=""bibr"" target="" e Me-diaWiki API<ref type=""foot"" target=""#foot_1"">4</ref> . Then the convinced article are reserved <ref type=""bibr"" target=""#b9"">(Hanselowski et al., 2018)</ref>.</p><p>Sentence retrieval. The senten ad></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>Dev</head><p>Test LA FEVER LA FEVER Athene <ref type=""bibr"" target=""#b9"">(Hanselowski et al., 2018)</ref> 68.49 64.74 65.46 61.58 UCL MRG <ref e document retrieval step retrieves related Wikipedia pages and is kept the same with previous work <ref type=""bibr"" target=""#b9"">(Hanselowski et al., 2018;</ref><ref type=""bibr"" target=""#b34"">Zhou et ERT based sentence retrieval. The ESIM based sentence retrieval keeps the same as the previous work <ref type=""bibr"" target=""#b9"">(Hanselowski et al., 2018;</ref><ref type=""bibr"" target=""#b34"">Zhou et",0
"th different weights. However, these methods <ref type=""bibr"" target=""#b27"">(Wu et al., 2019b;</ref><ref type=""bibr"" target=""#b31"">Zhu et al., 2019;</ref><ref type=""bibr"" target=""#b0"">An et al., 2019) set of documents given a query. Some works <ref type=""bibr"" target=""#b24"">(Wang et al., 2018;</ref><ref type=""bibr"" target=""#b31"">Zhu et al., 2019)</ref> propose to improve news representations via e eddings. These embeddings can be pre-trained from a large corpus or randomly initialized. Following <ref type=""bibr"" target=""#b31"">(Zhu et al., 2019)</ref>, we define the profile embedding</p><formula ed news representations would be taken as initial input embeddings of our model GNUD. Following DAN <ref type=""bibr"" target=""#b31"">(Zhu et al., 2019)</ref>, we use two parallel convolutional neural ne sa-10week, which respectively collect news click logs as long as 1 week and 10 weeks. Following DAN <ref type=""bibr"" target=""#b31"">(Zhu et al., 2019)</ref>, we just select user id, news id, time-stamp ws title and profile as semantic-level and knowledge-level representations, respectively.</p><p>DAN <ref type=""bibr"" target=""#b31"">(Zhu et al., 2019)</ref>, a deep attention neural network for news re",1
"the relationship between the preference factors and the disentangled embeddings.</p><p>According to <ref type=""bibr"" target=""#b30"">(Yang et al., 2018)</ref>, the mutual information maximization can be",1
"ence factor k instead of all the neighbors. In this work, we apply a neighborhood routing algorithm <ref type=""bibr"" target=""#b18"">(Ma et al., 2019a)</ref> to identify the subset of neighboring news t",0
"s is a central task for news recommendation. Traditional collaborative filtering (CF) based methods <ref type=""bibr"" target=""#b23"">(Wang and Blei, 2011)</ref> often utilize historical interactions bet",0
"g aims to identify and disentangle different latent explanatory factors hidden in the observed data <ref type=""bibr"" target=""#b1"">(Bengio et al., 2013)</ref>, which has been successfully applied in th",0
"twork. We also use the concatenation of news title and profile embeddings as features.</p><p>DeepFM <ref type=""bibr"" target=""#b5"">(Guo et al., 2017)</ref>, a general model that combines factorization",0
"al Settings</head><p>Datasets. We conduct experiments on the realworld online news datasets Adressa <ref type=""bibr"" target=""#b4"">(Gulla et al., 2017)</ref> 2 from a Norwegian news portal to evaluate",0
"informative user and news representations <ref type=""bibr"" target=""#b20"">(Okura et al., 2017;</ref><ref type=""bibr"" target=""#b24"">Wang et al., 2018)</ref>. For instance, DKN <ref type=""bibr"" target="" Okura et al., 2017;</ref><ref type=""bibr"" target=""#b24"">Wang et al., 2018)</ref>. For instance, DKN <ref type=""bibr"" target=""#b24"">(Wang et al., 2018)</ref> learns knowledge-aware news representation )</ref>, a CF based deep matrix factorization model without considering the news content.</p><p>DKN <ref type=""bibr"" target=""#b24"">(Wang et al., 2018)</ref>, a deep content based news recommendation f </ref>) is a content-based deep neural network to rank a set of documents given a query. Some works <ref type=""bibr"" target=""#b24"">(Wang et al., 2018;</ref><ref type=""bibr"" target=""#b31"">Zhu et al., 2",0
"tructure characteristics and alleviate the sparsity, thus improving the rec-ommendation performance <ref type=""bibr"" target=""#b25"">(Wang et al., 2019)</ref>. For example, as shown in Figure <ref type= bedding because of its powerful representation learning based on node features and graph structure. <ref type=""bibr"" target=""#b25"">Wang et al. (2019)</ref> explored the GNN to capture high-order conne",0
"urrent work further shows the success of search-based unsupervised text generation for paraphrasing <ref type=""bibr"" target=""#b15"">(Liu et al., 2020)</ref> and summa-rization <ref type=""bibr"" target=""",1
"eration for paraphrasing <ref type=""bibr"" target=""#b15"">(Liu et al., 2020)</ref> and summa-rization <ref type=""bibr"" target=""#b30"">(Schumann et al., 2020)</ref>.</p></div> <div xmlns=""http://www.tei-c",1
"o obtain.</p><p>In previous work, researchers have addressed some of the above issues. For example, <ref type=""bibr"" target=""#b0"">Alva-Manchego et al. (2017)</ref> and <ref type=""bibr"" target=""#b5"">Do",0
"r, syntactic information was also considered in the PBMT framework, for example, constituency trees <ref type=""bibr"" target=""#b42"">(Zhu et al., 2010)</ref> and dependency trees <ref type=""bibr"" target",0
"#b25"">Pauls and Klein, 2012)</ref>, to better measure fluency and human acceptability. According to <ref type=""bibr"" target=""#b13"">Lau et al. (2017)</ref>, SLOR shows the best correlation to human acc",0
"ill require large volumes of aligned data to learn these operations. To deal with the second issue, <ref type=""bibr"" target=""#b33"">Surya et al. (2019)</ref> recently proposed an unsupervised neural te h word and phrase levels in an unsupervised manner.</p><p>For unsupervised sentence simplification, <ref type=""bibr"" target=""#b33"">Surya et al. (2019)</ref> adopted style-transfer techniques, using ad g different automatic evaluation metrics.</p><p>For unsupervised competing methods, we compare with <ref type=""bibr"" target=""#b33"">Surya et al. (2019)</ref>, which is inspired by unsupervised neural m",0
"=""#b35"">(Watanabe et al., 2009)</ref>. It can also serve as a preprocessing step to improve parsers <ref type=""bibr"" target=""#b3"">(Chandrasekar et al., 1996)</ref> and summarization systems <ref type=",0
"ral language generation tasks, such as style transfer, paraphrasing, and sentence error correction. <ref type=""bibr"" target=""#b14"">Li et al. (2018)</ref> proposed edit-based style transfer without par",0
"ious aspects of a candidate simplified sentence. This is also known as the product-of-experts model <ref type=""bibr"" target=""#b8"">(Hinton, 2002)</ref>.</p><p>SLOR score from a syntax-aware language mo",0
"ed way by either phrase-based machine translation <ref type=""bibr"">(PBMT, Wubben et al., 2012;</ref><ref type=""bibr"" target=""#b21"">Narayan and Gardent, 2014;</ref><ref type=""bibr"" target=""#b38"">Xu et ., 2010)</ref> and dependency trees <ref type=""bibr"" target=""#b1"">(Bingel and Søgaard, 2016)</ref>. <ref type=""bibr"" target=""#b21"">Narayan and Gardent (2014)</ref> performed probabilistic sentence spl br"" target=""#b38"">(Xu et al., 2016)</ref>, which uses an external paraphrasing database; and Hybrid <ref type=""bibr"" target=""#b21"">(Narayan and Gardent, 2014)</ref>, which uses a combination of PBMT a",0
"emic paper analysis.</p><p>The pipeline for creating S2ORC was used to construct the CORD-19 corpus <ref type=""bibr"" target=""#b2"">(Wang et al., 2020)</ref>, which saw fervent adoption as the canonical",1
"indicators and PDF references. Following<ref type=""bibr"" target=""#b40"">Peters et al. (2018b)</ref>;<ref type=""bibr"" target=""#b30"">Liu et al. (2019a)</ref>, we observe that the final 2-3 BERT layers p",0
"al., 2009)</ref> <ref type=""foot"" target=""#foot_3"">6</ref> derived from the ACL Anthology, RefSeer <ref type=""bibr"" target=""#b21"">(Huang et al., 2015)</ref> derived from CiteSeerX, and Saier and Färb abling new forms of cross-paper discourse analysis (e.g., studying how or why papers are related).  <ref type=""bibr"" target=""#b21"">(Huang et al., 2015)</ref> 1.0M snippets no CiteSeerX multi</p><p>Tab the PDF. Although citation contexts are no longer available through CiteSeerX, the Ref-Seer dataset <ref type=""bibr"" target=""#b21"">(Huang et al., 2015)</ref> <ref type=""foot"" target=""#foot_13"">18</ref",0
"/head><p>To demonstrate the suitability of S2ORC for language model pretraining, we train BERT-Base <ref type=""bibr"" target=""#b13"">(Devlin et al., 2019)</ref> on the parsed full text of S2ORC and show r"" target=""#b39"">(Peters et al., 2018a)</ref> 1BW (800M) Wikipedia (1.9B) WMT 2008-2012 (3.6B) BERT <ref type=""bibr"" target=""#b13"">(Devlin et al., 2019)</ref> BooksCorpus (800M) Wikipedia (2.5B) ROBER",0
"a and Klein, 2014;</ref><ref type=""bibr"" target=""#b22"">Jeong et al., 2019)</ref> and document-level <ref type=""bibr"" target=""#b54"">(Yu et al., 2012;</ref><ref type=""bibr"" target=""#b29"">Liu et al., 201",0
"d Bowman (2019)</ref> only study word-level language models and also requires re-training. Finally, <ref type=""bibr"" target=""#b16"">Kurita et al. (2019)</ref> only measure bias on BERT by extending the",1
"uch as gender, race, and religion.</p><p>More recently, sentence-level representations such as ELMo <ref type=""bibr"" target=""#b31"">(Peters et al., 2018)</ref>, BERT <ref type=""bibr"" target=""#b10"">(Dev popular sentence encoders BERT <ref type=""bibr"" target=""#b10"">(Devlin et al., 2019)</ref> and ELMo <ref type=""bibr"" target=""#b31"">(Peters et al., 2018)</ref>, showing that our approach reduces the bi foot"" target=""#foot_2"">2</ref>  <ref type=""bibr"" target=""#b10"">(Devlin et al., 2019)</ref> and ELMo <ref type=""bibr"" target=""#b31"">(Peters et al., 2018)</ref>. Note that the pre-trained BERT encoder m",1
"biasing and require changing the data or underlying word embeddings and retraining which is costly. <ref type=""bibr"" target=""#b7"">Bordia and Bowman (2019)</ref> only study word-level language models a",1
"e subject of recent research in language understanding <ref type=""bibr"">(Merity et al., 2017b;</ref><ref type=""bibr"" target=""#b21"">Liu et al., 2019;</ref><ref type=""bibr"" target=""#b42"">Wang et al., 20",0
"biases in common stereotypes surrounding gendered names with respect to careers, math, and science <ref type=""bibr"" target=""#b13"">(Greenwald et al., 2009)</ref>. To evaluate biases in the multiclass",0
"9"">(Urbanek et al., 2019)</ref>. As their usage proliferates across various real-world applications <ref type=""bibr"" target=""#b14"">(Huang et al., 2019;</ref><ref type=""bibr"" target=""#b1"">Alsentzer et",0
"ltiple tasks in NLP <ref type=""bibr"" target=""#b44"">(Wu and Dredze, 2019)</ref>, multimodal learning <ref type=""bibr"" target=""#b45"">(Zellers et al., 2019;</ref><ref type=""bibr"">Sun et al., 2019a)</ref>",0
"<ref type=""bibr"">(Merity et al., 2017b;</ref><ref type=""bibr"" target=""#b21"">Liu et al., 2019;</ref><ref type=""bibr"" target=""#b42"">Wang et al., 2019)</ref> and multimodal human language <ref type=""bib",0
"word-level representations, these models have achieved better performance on multiple tasks in NLP <ref type=""bibr"" target=""#b44"">(Wu and Dredze, 2019)</ref>, multimodal learning <ref type=""bibr"" tar",0
"ols for learning from language are increasingly deployed in real-world scenarios such as healthcare <ref type=""bibr"" target=""#b40"">(Velupillai et al., 2018)</ref>, legal systems <ref type=""bibr"" targe",0
"tations for both binary <ref type=""bibr"" target=""#b6"">(Bolukbasi et al., 2016)</ref> and multiclass <ref type=""bibr"" target=""#b23"">(Manzini et al., 2019)</ref> bias attributes such as gender, race, an , we use the Mean Average Cosine similarity (MAC) metric which extends SEAT to a multiclass setting <ref type=""bibr"" target=""#b23"">(Manzini et al., 2019)</ref>. For the binary gender setting, we use w ""formula"">2017</ref>) row N . The last row measures bias in a multiclass religion setting using MAC <ref type=""bibr"" target=""#b23"">(Manzini et al., 2019)</ref> before and after debiasing. MAC score ra efore they are used in downstream tasks <ref type=""bibr"" target=""#b6"">(Bolukbasi et al., 2016;</ref><ref type=""bibr"" target=""#b23"">Manzini et al., 2019)</ref>. Secondly, sentences display large variet Caliskan Tests used in <ref type=""bibr"" target=""#b24"">May et al. (2019)</ref> with lexicons used by <ref type=""bibr"" target=""#b23"">Manzini et al. (2019)</ref>.</p></div> <div xmlns=""http://www.tei-c.o",0
">, legal systems <ref type=""bibr"" target=""#b9"">(Dale, 2019)</ref>, and computational social science <ref type=""bibr"" target=""#b2"">(Bamman et al., 2016)</ref>. Key to the success of these models are po",0
"ositive predictive ability: they can be used to detect the presence of biases but not their absence <ref type=""bibr"" target=""#b12"">(Gonen and Goldberg, 2019)</ref>. and paired sentence (BERT on QNLI)",0
"hitecture of TransformerCPI</head><p>The model we proposed is based on the transformer architecture <ref type=""bibr"" target=""#b41"">(Vaswani et al., 2017)</ref>, which was originally devised for neural",1
"<p>Identifying compound-protein interaction (CPI) plays an import role in discovering hit compounds <ref type=""bibr"" target=""#b39"">(Vamathevan et al., 2019)</ref>. Conventional methods, such as struct",1
"019)</ref>, GPT-2 , Transformer-XL <ref type=""bibr"" target=""#b6"">(Dai et al., 2019)</ref> and XLnet <ref type=""bibr"" target=""#b49"">(Yang et al., 2019)</ref>. Transformer is also applied in chemical re",0
"e LookAhead <ref type=""bibr"" target=""#b50"">(Zhang et al., 2019)</ref> optimizer combined with RAdam <ref type=""bibr"" target=""#b21"">(Liu et al., 2019)</ref> optimizer, which solved the most serious con",0
"ich are fixed during training process and contain less information than that of end-to-end learning <ref type=""bibr"" target=""#b13"">(Hamanaka et al., 2017;</ref><ref type=""bibr"" target=""#b37"">Tian et a",0
"o calculate the final output. WideDTA <ref type=""bibr"">(O ¨ztu ¨rk et al., 2019)</ref> and Conv-DTI <ref type=""bibr"" target=""#b19"">(Lee et al., 2019)</ref> followed the similar idea, and WideDTA utili",0
"rget=""#b40"">van Laarhoven et al., 2011;</ref><ref type=""bibr"" target=""#b44"">Wang et al., 2011;</ref><ref type=""bibr"" target=""#b45"">Wang and Zeng, 2013;</ref><ref type=""bibr"" target=""#b47"">Yamanishi et",0
"mation than that of end-to-end learning <ref type=""bibr"" target=""#b13"">(Hamanaka et al., 2017;</ref><ref type=""bibr"" target=""#b37"">Tian et al., 2016;</ref><ref type=""bibr"" target=""#b42"">Wan and Zeng,",0
""" target=""#b13"">(Hamanaka et al., 2017;</ref><ref type=""bibr"" target=""#b37"">Tian et al., 2016;</ref><ref type=""bibr"" target=""#b42"">Wan and Zeng, 2016)</ref>. Regarding the CPI problem as binary classi",0
"t=""#b14"">Jacob and Vert, 2008;</ref><ref type=""bibr"" target=""#b40"">van Laarhoven et al., 2011;</ref><ref type=""bibr"" target=""#b44"">Wang et al., 2011;</ref><ref type=""bibr"" target=""#b45"">Wang and Zeng,",0
"end-to-end ASR under cross-lingual transfer learning setting. To this end, we extend our prior work <ref type=""bibr"" target=""#b0"">[1]</ref>, and propose a hybrid Transformer-LSTM based architecture. T ot only require external language models but also lead to a slow inference. To tackle this problem, <ref type=""bibr"" target=""#b0"">[1]</ref> has proposed long short term memory (LSTM)-based encoderdeco <p>In this work, we propose a hybrid Transformer-LSTM architecture which combines the advantages of <ref type=""bibr"" target=""#b0"">[1]</ref> and <ref type=""bibr"" target=""#b5"">[6]</ref>. It not only has l.</p><p>The paper is organized as follows. Section 2 describes baseline architectures mentioned in <ref type=""bibr"" target=""#b0"">[1]</ref> and <ref type=""bibr"" target=""#b5"">[6]</ref>. Then, the propo res 2.1. LSTM-based encoder-decoder architecture</head><p>A LSTM-based encoder-decoder architecture <ref type=""bibr"" target=""#b0"">[1]</ref>, denoted as A1 in the rest of this paper, consists of a Bidi ayers respectively. Figure <ref type=""figure"">1</ref>: LSTM-based encoder-decoder architecture (A1) <ref type=""bibr"" target=""#b0"">[1]</ref>, where the decoder acts as an independent language model.</p ords, the LSTM acts as an independent language model that can be easily updated with text-only data <ref type=""bibr"" target=""#b0"">[1]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n= tune the transferred model. This avoids a so-called catastrophic forgetting problem as mentioned in <ref type=""bibr"" target=""#b0"">[1]</ref>. Specifically, at each training iteration, we mix a batch of cond step, the model is further fine-tuned with the labeled data of the target language. Similar to <ref type=""bibr"" target=""#b0"">[1]</ref>, we empirically found that the second step is necessary to i",1
"ular approach to address the limited resource problem in ASR <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=",0
"n be easily boosted using the text data. Another issue of the Transformer decoder is slow inference <ref type=""bibr"" target=""#b17"">[18]</ref>. Specifically, to generate an output yi, the decoder needs",0
"ons.</p><p>The extra text is usually employed to train language models (LM) applied during decoding <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target",0
"esource problem in ASR <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target",0
"models by fine-tuning pre-trained LMs in a simpler architecture.</p><p>Pre-trained LMs such as BERT <ref type=""bibr"" target=""#b12"">[13]</ref> and GPT-2 <ref type=""bibr"" target=""#b40"">[41]</ref> have d task. See Appendix A for the model architecture. In D , we fine-tune the popular 12layer BERT model <ref type=""bibr"" target=""#b12"">[13]</ref>, RoBERTa <ref type=""bibr"" target=""#b28"">[29]</ref>, and a currently support 4 pre-trained models: Distil-BERT <ref type=""bibr"" target=""#b44"">[45]</ref>, BERT <ref type=""bibr"" target=""#b12"">[13]</ref>, RoBERTa <ref type=""bibr"" target=""#b28"">[29]</ref>, and XL Figure <ref type=""figure"">6</ref> shows the model architecture of D 's language models such as BERT <ref type=""bibr"" target=""#b12"">[13]</ref>, DistilBERT <ref type=""bibr"" target=""#b44"">[45]</ref>, and best performing pre-trained model among DistilBERT <ref type=""bibr"" target=""#b44"">[45]</ref>, BERT <ref type=""bibr"" target=""#b12"">[13]</ref>, XLNet <ref type=""bibr"" target=""#b60"">[61]</ref>, and RoBE",1
"t information for making matching decisions. The last technique, data augmentation, is adapted from <ref type=""bibr"" target=""#b30"">[31]</ref> for EM to help D learn ""harder"" to understand the data inv address this issue, D applies MixDA, a recently proposed data augmentation technique for NLP tasks <ref type=""bibr"" target=""#b30"">[31]</ref> illustrated in Figure <ref type=""figure"" target=""#fig_3"">3 h the entry_swap operator. We compare the different combinations and report the best one. Following <ref type=""bibr"" target=""#b30"">[31]</ref>, we apply MixDA with the interpolation parameter λ sampled arget=""#b58"">59</ref>]. We designed a set of DA operators suitable for EM and apply them with MixDA <ref type=""bibr"" target=""#b30"">[31]</ref>, a recently proposed DA strategy based on convex interpola nd span_shuffle. These two operators are used in NLP tasks <ref type=""bibr"" target=""#b56"">[57,</ref><ref type=""bibr"" target=""#b30"">31]</ref> and shown to be effective for text classification. For span DA) has been extensively studied in computer vision and has recently received more attention in NLP <ref type=""bibr"" target=""#b30"">[31,</ref><ref type=""bibr"" target=""#b56"">57,</ref><ref type=""bibr"" ta",1
"spectively. For numeric data, a good candidate solution would be a hybrid neural network similar to <ref type=""bibr"" target=""#b19"">[20,</ref><ref type=""bibr"" target=""#b61"">62]</ref> that combines the",0
"form summarization <ref type=""bibr"" target=""#b31"">[32,</ref><ref type=""bibr"" target=""#b41"">42,</ref><ref type=""bibr"" target=""#b43"">44]</ref>. In D 's current implementation, we use a TF-IDF-based summ",0
"b63"">64]</ref>. DeepER <ref type=""bibr"" target=""#b13"">[14]</ref> trains EM models based on the LSTM <ref type=""bibr"" target=""#b20"">[21]</ref> neural network architecture with word embeddings such as G",0
"ratio of 3:1:1. The same split of the datasets is also used in the evaluation of other EM solutions <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" ta "">[25]</ref>, DeepER <ref type=""bibr"" target=""#b13"">[14]</ref>, and follow-up works of Deep-Matcher <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b22"">23]</ref>. We also compare w n the ER-Magellan EM datasets. The numbers of DeepMatcher+ (DM+) are the highest available found in <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" ta type=""bibr"" target=""#b22"">[23]</ref> slightly outperforms Deep-Matcher in the DBLP-ACM dataset and <ref type=""bibr"" target=""#b16"">[17]</ref> achieves better F1 in the Walmart-Amazon and Amazon-Google [25]</ref>, DeepMatcher <ref type=""bibr"" target=""#b33"">[34]</ref>, and DeepMatcher's follow-up work <ref type=""bibr"" target=""#b16"">[17]</ref> and <ref type=""bibr"" target=""#b22"">[23]</ref>.</p><p>We su olutions used deep learning and achieved promising results <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" tar bibr"" target=""#b33"">[34]</ref> and the two follow-up works <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b16"">17]</ref>   . We also plot the score of DeepMatcher+ on the full data",0
"ain. For domain-specific tasks, a potential solution is to leverage specialized LMs such as SciBERT <ref type=""bibr"" target=""#b2"">[3]</ref> or BioBERT <ref type=""bibr"" target=""#b26"">[27]</ref> trained",0
"a wide range of NLP tasks. They are typically deep neural networks with multiple Transformer layers <ref type=""bibr"" target=""#b50"">[51]</ref>, typically 12 or 24 layers, pre-trained on large text corp",0
"n is to leverage specialized LMs such as SciBERT <ref type=""bibr"" target=""#b2"">[3]</ref> or BioBERT <ref type=""bibr"" target=""#b26"">[27]</ref> trained on scientific and biology corpus respectively. For",0
"he ER Benchmark datasets <ref type=""bibr"" target=""#b25"">[26]</ref> and the Magellan data repository <ref type=""bibr"" target=""#b11"">[12]</ref>. We summarize the datasets in Table <ref type=""table"" targ",0
"intent-labeled speech data, and such data is usually scarce. <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b10"">11]</ref> address this problem using a curriculum and transfer learni",1
"target=""#b1"">[2]</ref><ref type=""bibr"" target=""#b2"">[3]</ref><ref type=""bibr"" target=""#b3"">[4]</ref><ref type=""bibr"" target=""#b4"">[5]</ref><ref type=""bibr"" target=""#b5"">[6]</ref><ref type=""bibr"" targe going through an intermediate text transcript. There are many advantages of end-to-end SLU systems <ref type=""bibr"" target=""#b4"">[5]</ref>, the most significant of which is that E2E systems can direc trained on increasingly relevant data until it is fine-tuned on the actual domain data. Similarly, <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b11"">12]</ref> advocate pre-trainin",1
"target=""#b0"">[1]</ref><ref type=""bibr"" target=""#b1"">[2]</ref><ref type=""bibr"" target=""#b2"">[3]</ref><ref type=""bibr"" target=""#b3"">[4]</ref><ref type=""bibr"" target=""#b4"">[5]</ref><ref type=""bibr"" targe",0
"etween modalities <ref type=""bibr"" target=""#b15"">[16]</ref><ref type=""bibr"" target=""#b16"">[17]</ref><ref type=""bibr"" target=""#b17"">[18]</ref><ref type=""bibr"" target=""#b18"">[19]</ref>. We employ the fo",0
"nt, of the text. In contrast, an end-to-end (E2E) SLU system <ref type=""bibr"" target=""#b0"">[1]</ref><ref type=""bibr"" target=""#b1"">[2]</ref><ref type=""bibr"" target=""#b2"">[3]</ref><ref type=""bibr"" targe",0
"pochs of guided training <ref type=""bibr"" target=""#b20"">[21]</ref>. We use sequence noise injection <ref type=""bibr"" target=""#b21"">[22]</ref> and SpecAugment <ref type=""bibr"" target=""#b22"">[23]</ref>",0
"BERTbase model of <ref type=""bibr"" target=""#b13"">[14]</ref>. Using the implementation introduced in <ref type=""bibr"" target=""#b23"">[24]</ref>, we first pre-train using a masked LM target with learning",0
"em that interprets the meaning, or intent, of the text. In contrast, an end-to-end (E2E) SLU system <ref type=""bibr"" target=""#b0"">[1]</ref><ref type=""bibr"" target=""#b1"">[2]</ref><ref type=""bibr"" targe",0
"etting training <ref type=""bibr"" target=""#b19"">[20]</ref>, followed by 20 epochs of guided training <ref type=""bibr"" target=""#b20"">[21]</ref>. We use sequence noise injection <ref type=""bibr"" target=""",0
"timized separately or jointly (also with end-to-end criteria <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b8"">9]</ref>). One key advantage of n-ended first utterances by customers describing the reasons for their calls, which is described in <ref type=""bibr"" target=""#b7"">[8]</ref>. The 8kHz telephony speech data was manually transcribed and",0
"ay impact the ability of language model to find good solutions and to recover from errors.</p><p>In <ref type=""bibr"" target=""#b39"">[40]</ref>, the authors consider a simple technique of adding time-de ding Gaussian noise to the gradient can not solve the problem of over-confidence.</p><p>Inspired by <ref type=""bibr"" target=""#b39"">[40]</ref>, we investigated a new label encoding method named ''Soft",1
". . . , 0.003] (6)</p><p>Feature-space Maximum Likelihood Linear Regression (FMLLR) was explored in <ref type=""bibr"" target=""#b31"">[32]</ref>, <ref type=""bibr"" target=""#b32"">[33]</ref> for speaker ada",1
"the goal of humancomputer interaction popular, and it has been a research hotspot in recent decades <ref type=""bibr"" target=""#b0"">[1]</ref>. Automatic Speech Recognition (ASR) refers to the task of an",1
"rain a better initial model. It has shown promising results in many tasks such as image recognition <ref type=""bibr"" target=""#b9"">[10]</ref>, speech recognition <ref type=""bibr"" target=""#b10"">[11]</re",0
"get=""#b43"">[44]</ref>, Natural Language Processing (NLP) <ref type=""bibr"" target=""#b12"">[13]</ref>, <ref type=""bibr"" target=""#b13"">[14]</ref> and so on <ref type=""bibr"" target=""#b14"">[15]</ref>- <ref",0
"r Vision (CV) <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" target=""#b40"">[41]</ref>- <ref type=""bibr"" target=""#b43"">[44]</ref>, Natural Language Processing (NLP) <ref type=""bibr"" target",0
"is the End-toend models, such as Encoder-decoder structure <ref type=""bibr"" target=""#b1"">[2]</ref>- <ref type=""bibr"" target=""#b3"">[4]</ref> and Neural Network structure with Connectionist temporal cla",0
"NLP) <ref type=""bibr"" target=""#b12"">[13]</ref>, <ref type=""bibr"" target=""#b13"">[14]</ref> and so on <ref type=""bibr"" target=""#b14"">[15]</ref>- <ref type=""bibr"" target=""#b16"">[17]</ref>.</p><p>Data aug",0
"ransfer learning has been proposed for a long time, and SJ Pan et al. made a complete summary of it <ref type=""bibr"" target=""#b8"">[9]</ref>. It can make full use of the data in the non-target domain t",0
"rks of different depths, the Phone Error Rate (PER) improved by an average of 0.65% on the test set <ref type=""bibr"" target=""#b21"">[22]</ref>. Ko et al. proposed a method that changing the speed of th",0
"entation cost and achieve stateof-the-art performance <ref type=""bibr"" target=""#b22"">[23]</ref>. In <ref type=""bibr"" target=""#b23"">[24]</ref>, A new method called SpecAugment is proposed and it consis",0
"ef>, <ref type=""bibr"" target=""#b13"">[14]</ref> and so on <ref type=""bibr"" target=""#b14"">[15]</ref>- <ref type=""bibr"" target=""#b16"">[17]</ref>.</p><p>Data augmentation <ref type=""bibr"" target=""#b17"">[1",0
"mum Likelihood Linear Regression (FMLLR) was explored in <ref type=""bibr"" target=""#b31"">[32]</ref>, <ref type=""bibr"" target=""#b32"">[33]</ref> for speaker adaptive training and it is a feature space tr PERIMENT A. BASELINE NN-HMM SYSTEM</head><p>All experiments are conducted on Pytorch-Kaldi platform <ref type=""bibr"" target=""#b32"">[33]</ref>. We use a single Nvidia TITAN Xp GPU to do single running. the objective function we used in the training is LF-MMI (Lattice-Free Maximum Mutual Information) <ref type=""bibr"" target=""#b32"">[33]</ref>, <ref type=""bibr"" target=""#b33"">[34]</ref>, which aims to",0
"n Markov Models (NN-HMMs), and the other is the End-toend models, such as Encoder-decoder structure <ref type=""bibr"" target=""#b1"">[2]</ref>- <ref type=""bibr"" target=""#b3"">[4]</ref> and Neural Network",0
"ormally, given the reference frame I t and each support frame I t+τ , Feature Pyramid Network (FPN) <ref type=""bibr"" target=""#b55"">[56]</ref> is leveraged to extract multi-scale pyramidal feature maps ad><p>Feature Pyramid Network. FPN is built at the top of ResNet-101 pre-trained on ImageNet. As in <ref type=""bibr"" target=""#b55"">[56]</ref>, P3, P4, Algorithm 2 Inference Algorithm of our SSVD </p><",1
"=""#b6"">[7]</ref>, <ref type=""bibr"" target=""#b7"">[8]</ref>, <ref type=""bibr"" target=""#b8"">[9]</ref>, <ref type=""bibr"" target=""#b9"">[10]</ref>, <ref type=""bibr"" target=""#b10"">[11]</ref>, <ref type=""bibr rkable progresses have been witnessed for object detection <ref type=""bibr"" target=""#b7"">[8]</ref>, <ref type=""bibr"" target=""#b9"">[10]</ref>, <ref type=""bibr"" target=""#b10"">[11]</ref>, <ref type=""bibr",1
"capitalize on FlowNet-s <ref type=""bibr"" target=""#b58"">[59]</ref> to produce optical flow, PWC-Net <ref type=""bibr"" target=""#b59"">[60]</ref> is particularly remould in our motion stream. Compared to 4 2 , and 7 2 , respectively. Two-stream Feature Aggregation. For motion stream, we utilize PWC-Net <ref type=""bibr"" target=""#b59"">[60]</ref> pre-trained on Flying Chairs dataset for optical flow esti t that the receptive field in sampling stream for offset prediction is smaller than that in PWC-Net <ref type=""bibr"" target=""#b59"">[60]</ref> for optical flow generation. As such, the range of estimat",1
"gure"" target=""#fig_3"">4</ref>. The philosophy is originated from the idea of deformable convolution <ref type=""bibr"" target=""#b60"">[61]</ref> which performs non-rigid spatial sampling with self-learnt case, we extend the augmentation of spatial sampling locations in standard deformation convolution <ref type=""bibr"" target=""#b60"">[61]</ref> which is only conditioned on one feature map to the measur achieves comparable performance with FGFA under the same backbone of Deformable Convolution Network <ref type=""bibr"" target=""#b60"">[61]</ref> (DCN). Note that as reported in <ref type=""bibr"" target=""#",0
""">[21]</ref>, <ref type=""bibr"" target=""#b21"">[22]</ref>, <ref type=""bibr"" target=""#b22"">[23]</ref>, <ref type=""bibr"" target=""#b23"">[24]</ref>. The former often applies a tracker to per-frame bounding l aggregation <ref type=""bibr"" target=""#b18"">[19]</ref>, <ref type=""bibr"" target=""#b21"">[22]</ref>, <ref type=""bibr"" target=""#b23"">[24]</ref>. Box-level tracking employs boxlevel operations and post-p ances per-frame features via spatio-temporal aggregation, enabling an end-toend detection paradigm. <ref type=""bibr"" target=""#b23"">[24]</ref>, <ref type=""bibr"" target=""#b44"">[45]</ref> calibrate a seq g motion paths to enhance object detection. Next, <ref type=""bibr"" target=""#b20"">[21]</ref> extends <ref type=""bibr"" target=""#b23"">[24]</ref> by exploiting additional motion path with box-level calibr =""bibr"" target=""#b56"">[57]</ref>, video object detection <ref type=""bibr"" target=""#b20"">[21]</ref>, <ref type=""bibr"" target=""#b23"">[24]</ref>, and video translation <ref type=""bibr"" target=""#b57"">[58] tween them in the form of optical flow. Unlike the works <ref type=""bibr"" target=""#b20"">[21]</ref>, <ref type=""bibr"" target=""#b23"">[24]</ref>, <ref type=""bibr"" target=""#b44"">[45]</ref> which capitaliz protocols in <ref type=""bibr"" target=""#b14"">[15]</ref>, <ref type=""bibr"" target=""#b41"">[42]</ref>, <ref type=""bibr"" target=""#b23"">[24]</ref> to report the results on validation set in terms of the ev Net object detection (DET) dataset. Therefore, we follow <ref type=""bibr"" target=""#b14"">[15]</ref>, <ref type=""bibr"" target=""#b23"">[24]</ref> and train SSVD on the intersection of ImageNet VID and Ima decay of 0.0001. The batch size is set as 16. Following <ref type=""bibr"" target=""#b20"">[21]</ref>, <ref type=""bibr"" target=""#b23"">[24]</ref>, the two-phase training strategy is adopted. In the first bibr"" target=""#b11"">[12]</ref>. The aggregation range K is set as 12 in all experiments. As in FGFA <ref type=""bibr"" target=""#b23"">[24]</ref>, we utilize temporal dropout in training stage by randomly input adjacent frames {I t+τ } K τ =−K of reference frame I t . Inference. At inference, we follow <ref type=""bibr"" target=""#b23"">[24]</ref> and sequentially process each frame with a sliding feature andard mean Average Precision (mAP) over all classes, we additionally adopt the metric protocols in <ref type=""bibr"" target=""#b23"">[24]</ref> by categorizing ground-truth objects with respect to their ef> ResNet-101 75.4 D (&amp; T loss) <ref type=""bibr"" target=""#b14"">[15]</ref> ResNet-101 75.8 FGFA <ref type=""bibr"" target=""#b23"">[24]</ref> ResNet-101 76.3 LWDN <ref type=""bibr"" target=""#b63"">[64]</ ral coherence among adjacent frames via RoI tracking module. The latter three baselines (i.e., FGFA <ref type=""bibr"" target=""#b23"">[24]</ref>, MANet <ref type=""bibr"" target=""#b20"">[21]</ref>, and STSN 16"">[17]</ref> GoogLeNet 68.4 TCNN <ref type=""bibr"" target=""#b40"">[41]</ref> DeepID+Craft 73.8 FGFA <ref type=""bibr"" target=""#b23"">[24]</ref> ResNet-101 78.4 D&amp;T (τ = 1) <ref type=""bibr"" target=""#",0
""">[26]</ref>, <ref type=""bibr"" target=""#b26"">[27]</ref>, <ref type=""bibr"" target=""#b27"">[28]</ref>, <ref type=""bibr"" target=""#b28"">[29]</ref>, <ref type=""bibr"" target=""#b29"">[30]</ref>, <ref type=""bib",0
""">[48]</ref>, <ref type=""bibr"" target=""#b48"">[49]</ref>, <ref type=""bibr"" target=""#b49"">[50]</ref>, <ref type=""bibr"" target=""#b50"">[51]</ref>, <ref type=""bibr"" target=""#b51"">[52]</ref>, <ref type=""bib",0
"tion boxes between each two adjacent frames by predicted tracking boxes and Viterbi Algorithm as in <ref type=""bibr"" target=""#b66"">[67]</ref>, which boosts the performance from 75.8% to 79.8%. STMN fi m to link the per-frame detection boxes of our SSVD into tracklets. The mAP track of SSVD + Viterbi <ref type=""bibr"" target=""#b66"">[67]</ref> is 60.2%, which is higher than 57.0% of DorT <ref type=""bi",0
"ef type=""bibr"" target=""#b33"">[34]</ref>, <ref type=""bibr"" target=""#b34"">[35]</ref> for recognition, <ref type=""bibr"" target=""#b35"">[36]</ref>, <ref type=""bibr"" target=""#b36"">[37]</ref> focus on learni",0
"lts. on a scheduler network to switch between detection and tracking networks. Furthermore, HQ-link <ref type=""bibr"" target=""#b43"">[44]</ref> devises the cuboid proposal networks and Tubelet NMS to en 9]</ref> ResNet-101+DCN 80.4 STMN <ref type=""bibr"" target=""#b21"">[22]</ref> ResNet-101 80.5 HQ-link <ref type=""bibr"" target=""#b43"">[44]</ref> ResNet-101 80.  consistently demonstrate that our proposed",0
""">[50]</ref>, <ref type=""bibr"" target=""#b50"">[51]</ref>, <ref type=""bibr"" target=""#b51"">[52]</ref>, <ref type=""bibr"" target=""#b52"">[53]</ref>, <ref type=""bibr"" target=""#b53"">[54]</ref>, <ref type=""bib",0
"=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b2"">[3]</ref>, <ref type=""bibr"" target=""#b3"">[4]</ref>, <ref type=""bibr"" target=""#b4"">[5]</ref> have successfully a al Networks (CNN) <ref type=""bibr"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b2"">[3]</ref>, <ref type=""bibr"" target=""#b3"">[4]</ref>, <ref type=""bibr"" target=""#b4"">[5]</ref>, remarkable progres",0
"18]</ref> and another branch is featurelevel aggregation <ref type=""bibr"" target=""#b18"">[19]</ref>, <ref type=""bibr"" target=""#b19"">[20]</ref>, <ref type=""bibr"" target=""#b20"">[21]</ref>, <ref type=""bib",0
""">[12]</ref>, <ref type=""bibr"" target=""#b12"">[13]</ref>, <ref type=""bibr"" target=""#b24"">[25]</ref>, <ref type=""bibr"" target=""#b25"">[26]</ref>, <ref type=""bibr"" target=""#b26"">[27]</ref>, <ref type=""bib ased on a two-stage paradigm, i.e., first perform region proposal and then do classification. R-CNN <ref type=""bibr"" target=""#b25"">[26]</ref> is one of first attempts that tackles object detection pro ype=""bibr"" target=""#b32"">[33]</ref> and Fast R-CNN <ref type=""bibr"" target=""#b24"">[25]</ref> extend <ref type=""bibr"" target=""#b25"">[26]</ref> by devising SPP pooling or ROI pooling to enable the shari",0
""">[19]</ref>, <ref type=""bibr"" target=""#b19"">[20]</ref>, <ref type=""bibr"" target=""#b20"">[21]</ref>, <ref type=""bibr"" target=""#b21"">[22]</ref>, <ref type=""bibr"" target=""#b22"">[23]</ref>, <ref type=""bib "" target=""#b16"">[17]</ref> and feature-level aggregation <ref type=""bibr"" target=""#b18"">[19]</ref>, <ref type=""bibr"" target=""#b21"">[22]</ref>, <ref type=""bibr"" target=""#b23"">[24]</ref>. Box-level trac tion in a frame by learning to spatially sample features from adjacent frames for aggregation. STMN <ref type=""bibr"" target=""#b21"">[22]</ref> adopts spatiotemporal memory module with spatial alignment >[15]</ref> ResNet-101 79.8 STSN <ref type=""bibr"" target=""#b18"">[19]</ref> ResNet-101+DCN 80.4 STMN <ref type=""bibr"" target=""#b21"">[22]</ref> ResNet-101 80.5 HQ-link <ref type=""bibr"" target=""#b43"">[44",0
"y are unfortunately not the only weak spot in machine learning systems.</p><p>Recently, Xiao et al. <ref type=""bibr"" target=""#b34"">[35]</ref> have demonstrated that data preprocessing used in machine ns=""http://www.tei-c.org/ns/1.0""><head n=""2.2"">Image-Scaling Attacks</head><p>Recently, Xiao et al. <ref type=""bibr"" target=""#b34"">[35]</ref> have shown that scaling algorithms are vulnerable to attac aling algorithm. Both matrices can be computed in advance and are reusable. We refer to Xiao et al. <ref type=""bibr"" target=""#b34"">[35]</ref> for a description how to calculate L and R.</p><p>Based on assignment.</p><p>We implement image-scaling attacks in the strong variant proposed by Xiao et al. <ref type=""bibr"" target=""#b34"">[35]</ref>. We make a slight improvement to the original attacks: Ins on rectangular blocks instead of columns and rows. As a result, the original attack by Xiao et al. <ref type=""bibr"" target=""#b34"">[35]</ref> is not applicable to this scaling algorithm. To attack are",1
"experiments along with these two objectives.</p><p>Dataset &amp; Setup. We use the ImageNet dataset <ref type=""bibr"" target=""#b24"">[25]</ref> with a pre-trained VGG19 model <ref type=""bibr"" target=""#b",0
"puter vision [e.g., <ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b12"">13]</ref> and natural language processing [e.g., <ref type=""bibr"" tar",0
"ent, we additionally use the Peak Signal to Noise Ratio (PSNR), a common metric in image processing <ref type=""bibr"" target=""#b7"">[8]</ref>, to measure the difference between the unmodified source ima",0
"ntroduction</head><p>Pre-training models <ref type=""bibr"" target=""#b14"">(Radford et al., 2018;</ref><ref type=""bibr"" target=""#b4"">Devlin et al., 2019;</ref><ref type=""bibr"">Radford et al., 2019b;</ref /head><p>The key of pre-training methods <ref type=""bibr"" target=""#b14"">(Radford et al., 2018;</ref><ref type=""bibr"" target=""#b4"">Devlin et al., 2019;</ref><ref type=""bibr"" target=""#b23"">Song et al., rosoft/MPNet the accuracy of NLP tasks in the past years. One of the most successful models is BERT <ref type=""bibr"" target=""#b4"">(Devlin et al., 2019)</ref>, which mainly adopts masked language model ge understanding and generation. For language understanding, masked language modeling (MLM) in BERT <ref type=""bibr"" target=""#b4"">(Devlin et al., 2019)</ref> and permuted language modeling (PLM) in XL his section, we briefly review MLM and PLM, and discuss their pros and cons.</p><p>MLM in BERT BERT <ref type=""bibr"" target=""#b4"">(Devlin et al., 2019)</ref> is one of the most successful pre-training e predicted part. For the non-predicted part (x z&lt;=c , M z&gt;c ), we use bidirectional modeling <ref type=""bibr"" target=""#b4"">(Devlin et al., 2019)</ref>  Modeling Output Dependency with Two-Strea objectives mask and predict the same amount of tokens (15%), following the common practice in BERT <ref type=""bibr"" target=""#b4"">(Devlin et al., 2019)</ref> and XLNet <ref type=""bibr"" target=""#b30"">( n=""3.1"">Experimental Setup</head><p>We conduct experiments under the BERT base setting (BERT BASE ) <ref type=""bibr"" target=""#b4"">(Devlin et al., 2019)</ref>, where the model consists of 12 transforme the predicted tokens, and prepare mask tokens following the same 8:1:1 replacement strategy in BERT <ref type=""bibr"" target=""#b4"">(Devlin et al., 2019)</ref>. Additionally, we also apply whole word ma 2018)</ref>, with 160GB data size in total. We use a subword dictionary with 30K BPE codes in BERT <ref type=""bibr"" target=""#b4"">(Devlin et al., 2019)</ref> to tokenize the sentences. We limit the le in BERT BASE setting and from MNLI QNLI QQP RTE SST MRPC CoLA STS Avg Single model on dev set BERT <ref type=""bibr"" target=""#b4"">(Devlin et al., 2019)</ref> 84.5 91.7 91.3 68.6 93.2 87.3 58.9 89.5 83 )</ref> 87 </p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>SQuAD v1.1 EM F1</head><p>BERT <ref type=""bibr"" target=""#b4"">(Devlin et al., 2019)</ref> 80.8 88.5 RoBERTa <ref type=""bibr"">(Liu et ut any data augmentation for fair comparisons. On the dev set of GLUE tasks, MPNet outperforms BERT <ref type=""bibr"" target=""#b4"">(Devlin et al., 2019)</ref>, XLNet <ref type=""bibr"" target=""#b30"">(Yan",1
"<ref type=""bibr"">Radford et al., 2019b;</ref><ref type=""bibr"" target=""#b23"">Song et al., 2019;</ref><ref type=""bibr"" target=""#b30"">Yang et al., 2019;</ref><ref type=""bibr"" target=""#b6"">Dong et al., 20 bibr"" target=""#b4"">Devlin et al., 2019;</ref><ref type=""bibr"" target=""#b23"">Song et al., 2019;</ref><ref type=""bibr"" target=""#b30"">Yang et al., 2019;</ref><ref type=""bibr"" target=""#b1"">Clark et al., 2 masked tokens efficiently, but ignores the dependency among the masked (and to be predicted) tokens <ref type=""bibr"" target=""#b30"">(Yang et al., 2019)</ref>.</p><p>To improve BERT, XLNet <ref type=""bi type=""bibr"" target=""#b4"">(Devlin et al., 2019)</ref> and permuted language modeling (PLM) in XLNet <ref type=""bibr"" target=""#b30"">(Yang et al., 2019)</ref> are two representative objectives. In this P (x k |x \K ; ?). (1)</formula><p>PLM in XLNet Permuted language model (PLM) is proposed in XLNet <ref type=""bibr"" target=""#b30"">(Yang et al., 2019)</ref> to retain the benefits of autoregressive mo edict and the remaining tokens are used as condition in order to reduce the optimization difficulty <ref type=""bibr"" target=""#b30"">(Yang et al., 2019)</ref>.</p><p>Pros and Cons of MLM and PLM We comp separately, which is not sufficient to model the complicated context dependency in natural language <ref type=""bibr"" target=""#b30"">(Yang et al., 2019)</ref>. In contrast, PLM factorizes the predicted for normal autoregressive prediction. To this end, we follow PLM to adopt two-stream self-attention <ref type=""bibr"" target=""#b30"">(Yang et al., 2019)</ref> to autoregressively predict the tokens, whi example, when predicting token x z 5 = x 6 , the query stream in the original two-stream attention <ref type=""bibr"" target=""#b30"">(Yang et al., 2019)</ref> takes mask token M z 5 = [M ] and position ing the common practice in BERT <ref type=""bibr"" target=""#b4"">(Devlin et al., 2019)</ref> and XLNet <ref type=""bibr"" target=""#b30"">(Yang et al., 2019)</ref> <ref type=""foot"" target=""#foot_2"">5</ref> . rs in total. For the pretraining objective of MPNet, we randomly permute the sentence following PLM <ref type=""bibr"" target=""#b30"">(Yang et al., 2019)</ref>  <ref type=""foot"" target=""#foot_4"">7</ref> -6. We pre-train our model for 500K steps to be comparable with state-of-the-art models like XLNet <ref type=""bibr"" target=""#b30"">(Yang et al., 2019)</ref>, RoBERTa <ref type=""bibr"">(Liu et al., 2019 =""bibr"" target=""#b4"">(Devlin et al., 2019)</ref> 84.5 91.7 91.3 68.6 93.2 87.3 58.9 89.5 83.1 XLNet <ref type=""bibr"" target=""#b30"">(Yang et al., 2019)</ref> 86.8 91.7 91.4 74.0 94.7 88.2 60.2 89.5 84. GLUE tasks, MPNet outperforms BERT <ref type=""bibr"" target=""#b4"">(Devlin et al., 2019)</ref>, XLNet <ref type=""bibr"" target=""#b30"">(Yang et al., 2019)</ref> and RoBERTa <ref type=""bibr"">(Liu et al., 2 BERT are from the RACE leaderboard 10 and the results of XL-Net are obtained from the original paper<ref type=""bibr"" target=""#b30"">(Yang et al., 2019)</ref>. ""Middle"" and ""High"" denote the accuracy on result of XLNet is ran by ourselves with only PLM pre-training objective but no long context memory<ref type=""bibr"" target=""#b30"">(Yang et al., 2019)</ref>. ""*"" represents pre-training only on Wikipe cted) tokens <ref type=""bibr"" target=""#b30"">(Yang et al., 2019)</ref>.</p><p>To improve BERT, XLNet <ref type=""bibr"" target=""#b30"">(Yang et al., 2019</ref>) introduces permuted language modeling (PLM) </p><p>We pre-train MPNet on a large-scale text corpora (over 160GB data) following the practice in <ref type=""bibr"" target=""#b30"">Yang et al. (2019)</ref>; <ref type=""bibr"">Liu et al. (2019a)</ref>, figure"" target=""#fig_2"">2a</ref>. For more details about two-stream self-attention, please refer to <ref type=""bibr"" target=""#b30"">Yang et al. (2019)</ref>. One drawback of two-stream self-attention i two-stream self-attention and use the original hiddens to extract context representations following <ref type=""bibr"" target=""#b30"">Yang et al. (2019)</ref>. The fine-tuning experiments on each downstr ref> in our model pretraining since these tricks have been successfully validated in previous works <ref type=""bibr"" target=""#b30"">(Yang et al., 2019;</ref><ref type=""bibr"">Raffel et al., 2019b)</ref> foot"" n=""2"" xml:id=""foot_0""><p>We do not consider next sentence prediction here since previous works<ref type=""bibr"" target=""#b30"">(Yang et al., 2019;</ref> Liu et al., 2019a;<ref type=""bibr"" target=""",1
"two single-sentence tasks (CoLA <ref type=""bibr"" target=""#b28"">(Warstadt et al., 2018)</ref>, SST-2 <ref type=""bibr"" target=""#b22"">(Socher et al., 2013)</ref>), three similarity and paraphrase tasks (",0
"ref type=""bibr"" target=""#b4"">Devlin et al., 2019;</ref><ref type=""bibr"">Radford et al., 2019b;</ref><ref type=""bibr"" target=""#b23"">Song et al., 2019;</ref><ref type=""bibr"" target=""#b30"">Yang et al., 2 "" target=""#b14"">(Radford et al., 2018;</ref><ref type=""bibr"" target=""#b4"">Devlin et al., 2019;</ref><ref type=""bibr"" target=""#b23"">Song et al., 2019;</ref><ref type=""bibr"" target=""#b30"">Yang et al., 2",0
"""bibr"" target=""#b23"">Song et al., 2019;</ref><ref type=""bibr"" target=""#b30"">Yang et al., 2019;</ref><ref type=""bibr"" target=""#b1"">Clark et al., 2020)</ref> is the design of self-supervised tasks/objec et=""#b30"">(Yang et al., 2019)</ref>, RoBERTa <ref type=""bibr"">(Liu et al., 2019a)</ref> and ELECTRA <ref type=""bibr"" target=""#b1"">(Clark et al., 2020)</ref>. We use 32 NVIDIA Tesla V100 GPUs, with 32G </ref> by 4.6, 3.2, 1.3 points on average. On the test set of GLEU tasks, MPNet outperforms ELECTRA <ref type=""bibr"" target=""#b1"">(Clark et al., 2020)</ref>, which achieved previous state-of-the-art a",0
"ur model on SQuAD v1.1 <ref type=""bibr"" target=""#b20"">(Rajpurkar et al., 2016)</ref> and SQuAD v2.0 <ref type=""bibr"" target=""#b19"">(Rajpurkar et al., 2018)</ref>. SQuAD v1.1 always exists the correspo",0
"accuracy on the middle school set and high school set in RACE. For IMDB, the result of BERT is from<ref type=""bibr"" target=""#b24"">Sun et al. (2019)</ref> and the result of XLNet is ran by ourselves w",0
">(Rajpurkar et al., 2016)</ref>, RTE <ref type=""bibr"" target=""#b3"">(Dagan et al., 2006)</ref>, WNLI <ref type=""bibr"" target=""#b10"">(Levesque et al., 2012)</ref>). We follow RoBERTa hyper-parameters fo",0
"word mask <ref type=""bibr"" target=""#b2"">(Cui et al., 2019)</ref> and relative positional embedding <ref type=""bibr"" target=""#b21"">(Shaw et al., 2018)</ref> <ref type=""foot"" target=""#foot_5"">8</ref> i",0
"s) and directly harness the resultant example to fool the remote target model (i.e., victim models) <ref type=""bibr"" target=""#b40"">[41,</ref><ref type=""bibr"" target=""#b7"">8]</ref>.</p><p>Among these t "" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b9"">10]</ref>, and the other one is transfer-based <ref type=""bibr"" target=""#b40"">[41,</ref><ref type=""bibr"" target=""#b38"">39,</ref><ref type=""bibr"" ta or fair comparisons, we adopt default parameters as recommended in benchmark approaches and Foolbox <ref type=""bibr"" target=""#b40"">[41,</ref><ref type=""bibr"" target=""#b27"">28]</ref>. The random noise of the source model <ref type=""bibr"" target=""#b38"">[39,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b40"">41,</ref><ref type=""bibr"" target=""#b6"">7]</ref>. Specifically, althou ork is the regularization-based approach: transferable adversarial perturbation (TAP) introduced by <ref type=""bibr"" target=""#b40"">[41]</ref>. TAP injects two regularization terms into the vanilla tra 19"">[20,</ref><ref type=""bibr"" target=""#b3"">4]</ref>. We follow the protocol of the baseline method <ref type=""bibr"" target=""#b40"">[41]</ref> to curate experimental datasets and target models for fair cannot strictly meet the l ∞ budget, we employ the modified l ∞ version of C&amp;W as introduced by <ref type=""bibr"" target=""#b40"">[41]</ref>, which can explicitly satisfy the l ∞ norm constraint. Sim b40"">[41]</ref>, which can explicitly satisfy the l ∞ norm constraint. Similar to our strategy, TAP <ref type=""bibr"" target=""#b40"">[41]</ref> boosts adversarial transferability through two regularizat e random noise is sampled from a clipped normal distribution with mean 0 and variance 1.  Following <ref type=""bibr"" target=""#b40"">[41]</ref>, we fix the perturbation budget ǫ to 16 for all methods. W ext attack models defended by adversarial training. For fair comparisons with the baseline approach <ref type=""bibr"" target=""#b40"">[41]</ref>, we stick to employing undefended models as local source m d the other one is the regularization-based transferable adversarial perturbation (TAP) proposed by <ref type=""bibr"" target=""#b40"">[41]</ref>. With the integrated attacks, we conduct experiments simil",1
"arget=""#b38"">39,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b22"">23]</ref>.</p><p>Query-based black-box attacks can settle the suscept >32]</ref> or images <ref type=""bibr"" target=""#b38"">[39,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b22"">23]</ref>. More related to our work is the regularization-based appro",1
"formation of the victim model to craft malicious instances <ref type=""bibr"" target=""#b35"">[36,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b4"">5]</ref>, while black-box attack enjoys great popularity among early work on attacking DNNs <ref type=""bibr"" target=""#b35"">[36,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" targe clean seed images by taking one step along with the sign of the gradient of the model loss function <ref type=""bibr"" target=""#b8"">[9]</ref>. Its successor, basic iterative method (BIM), iteratively ap y small number. We adopt l ∞ norm in this work, as it is the most widely advocated in the community <ref type=""bibr"" target=""#b8"">[9]</ref>. We also note that our method is generally applicable to oth at https://github. com/tensorflow/models/tree/master/research/adv_ imagenet_models.</p><p>ing FGSM <ref type=""bibr"" target=""#b8"">[9]</ref>, BIM <ref type=""bibr"" target=""#b17"">[18]</ref>, C&amp;W <ref e images for their model and augment the clean training data with such instances to train the model <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" targ",0
"/ref>, and feature squeezing to detect adversarial samples <ref type=""bibr"" target=""#b39"">[40,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b37"">38]</ref>. Adversarial traini",0
"eatmaps of three representative models (VGG 16 <ref type=""bibr"" target=""#b32"">[33]</ref>, ResNet V2 <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b12"">13]</ref>, and Inception V3 d models, we employ numerous top-performance models with diverse architectures, including ResNet V2 <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b12"">13]</ref>, Inception V3 <ref",0
"6,</ref><ref type=""bibr"" target=""#b5"">6]</ref>, and feature squeezing to detect adversarial samples <ref type=""bibr"" target=""#b39"">[40,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" ta",0
"gnized benchmark task for transfer-based black-box attacks <ref type=""bibr"" target=""#b19"">[20,</ref><ref type=""bibr"" target=""#b3"">4]</ref>. We follow the protocol of the baseline method <ref type=""bib",0
"rget=""#b38"">[39,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b40"">41,</ref><ref type=""bibr"" target=""#b6"">7]</ref>. Specifically, although the crafted adversarial samples can a",0
"ient descent (PGD) extends BIM with random start to diversify the synthesized adversarial instances <ref type=""bibr"" target=""#b21"">[22]</ref>. Carlini and Wagner attacks (C&amp;W) devise a novel attac >19]</ref>, since adversarial training is arguably the most promising and effective defense to date <ref type=""bibr"" target=""#b21"">[22]</ref>. These adversarially trained models include adversarially e clean training data with such instances to train the model <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b19"">20]</ref>. Moreover, exploiti",0
"re roughly two sorts of black-box attacks according to the mechanism they adopt. One is query-based <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" targ y, attackers can approximate the loss gradient of the target model through training a local replica <ref type=""bibr"" target=""#b23"">[24]</ref> or finite difference techniques <ref type=""bibr"" target=""#",0
"een proposed and achieved state-of-the-art results in SISR <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b39"">40,</ref><ref type=""bibr"" tar have been proposed for a better performance. Lim et al. proposed a very deep and wide network EDSR <ref type=""bibr"" target=""#b17"">[18]</ref> by stacking modified residual blocks in which the batch no the performance. Fig. <ref type=""figure"">3</ref>(Left) depicts a basic residual module used in EDSR <ref type=""bibr"" target=""#b17"">[18]</ref> and ESRGAN <ref type=""bibr"" target=""#b30"">[31]</ref>. The ion, we investigate the combination of our RFA framework with the basic residual block used in EDSR <ref type=""bibr"" target=""#b17"">[18]</ref>. Different from the original residual block used in image N <ref type=""bibr"" target=""#b14"">[15]</ref>, MemNet <ref type=""bibr"" target=""#b24"">[25]</ref>, EDSR <ref type=""bibr"" target=""#b17"">[18]</ref>, SRMD <ref type=""bibr"" target=""#b35"">[36]</ref>, NLRN <ref n the top row, which can ease the training difficulty to some extent (e.g. residual scaling in EDSR <ref type=""bibr"" target=""#b17"">[18]</ref>).</p><p>(2) Feature maps after the attention mechanism ten rchitectures. Here we introduce one of the basic architecture used by some state-of-the-art methods <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b39"">40,</ref><ref type=""bibr"" ta",1
"target=""#b2"">3,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b38"">39]</ref>.</p><p>Although consi",0
"et al. further increased the depth to 20 in VDSR <ref type=""bibr"" target=""#b12"">[13]</ref> and DRCN <ref type=""bibr"" target=""#b13"">[14]</ref> by introducing residual learning to ease the training diff pioneering work, Kim et al. designed deeper VDSR <ref type=""bibr"" target=""#b12"">[13]</ref> and DRCN <ref type=""bibr"" target=""#b13"">[14]</ref> with 20 layers based on residual learning. Later, Tai et a",0
"arget=""#b27"">28,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" tar <ref type=""bibr"" target=""#b32"">[33]</ref>, B100 <ref type=""bibr"" target=""#b19"">[20]</ref>, Urban100 <ref type=""bibr"" target=""#b11"">[12]</ref>, and Manga109 <ref type=""bibr"" target=""#b20"">[21]</ref>. B",0
"rt results in SISR <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b39"">40,</ref><ref type=""bibr"" target=""#b37"">38,</ref><ref type=""bibr"" tar e basic architecture used by some state-of-the-art methods <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b39"">40,</ref><ref type=""bibr"" target=""#b37"">38,</ref><ref type=""bibr"" tar 0"">[31]</ref> by introducing the dense connections. Zhang et al. also used dense connections in RDN <ref type=""bibr"" target=""#b39"">[40]</ref> to utilize all the hierarchical features from all the conv odology is a general framework that can be easily applied with existing SR blocks (e.g. dense block <ref type=""bibr"" target=""#b39"">[40]</ref>). We will investigate the effects in detail when our RFA f eatures would be more diverse and discriminative. Difference to RDN. The main building block of RDN <ref type=""bibr"" target=""#b39"">[40]</ref> is called residual dense block (RDB). RDB combines residua .org/ns/1.0""><head n=""4.3."">Combination with Dense Block</head><p>The motivation behind dense block <ref type=""bibr"" target=""#b39"">[40]</ref> is also to combine hierarchical cues available along the n , NLRN <ref type=""bibr"" target=""#b18"">[19]</ref>, DBPN <ref type=""bibr"" target=""#b5"">[6]</ref>, RDN <ref type=""bibr"" target=""#b39"">[40]</ref>, RCAN <ref type=""bibr"" target=""#b37"">[38]</ref> and SAN <r RCNN <ref type=""bibr"" target=""#b34"">[35]</ref>, SRMD <ref type=""bibr"" target=""#b35"">[36]</ref>, RDN <ref type=""bibr"" target=""#b39"">[40]</ref>, SRFBN <ref type=""bibr"" target=""#b16"">[17]</ref>, RCAN <re , SRMD <ref type=""bibr"" target=""#b35"">[36]</ref>, DBPN <ref type=""bibr"" target=""#b5"">[6]</ref>, RDN <ref type=""bibr"" target=""#b39"">[40]</ref>, RCAN <ref type=""bibr"" target=""#b37"">[38]</ref> and SAN <r <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.1."">Settings</head><p>Following previous works <ref type=""bibr"" target=""#b39"">[40,</ref><ref type=""bibr"" target=""#b37"">38,</ref><ref type=""bibr"" ta",0
"limitations of deep SISR. SISR performance was boosted right after the non-local attention modules <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" targ <ref type=""bibr"" target=""#b19"">[20]</ref>, RNAN <ref type=""bibr"" target=""#b36"">[37]</ref> and SAN <ref type=""bibr"" target=""#b1"">[2]</ref>, incorporate non-local operation into their networks in orde o the fusion structure. For the IS-NL branch, it contains a non-local attention module adopted from <ref type=""bibr"" target=""#b1"">[2]</ref> and a deconvolution layer for upscaling the module outputs. ution layer for upscaling the module outputs. The IS-NL module is region-based in this paper. As in <ref type=""bibr"" target=""#b1"">[2]</ref>, we divide the feature maps into region grids, where the int N <ref type=""bibr"" target=""#b17"">[18]</ref>, OISR <ref type=""bibr"" target=""#b11"">[12]</ref> and SAN <ref type=""bibr"" target=""#b1"">[2]</ref>.</p><p>Quantitative Evaluations In Table <ref type=""table"" t f> RDN <ref type=""bibr"" target=""#b38"">[39]</ref> RCAN <ref type=""bibr"" target=""#b36"">[37]</ref> SAN <ref type=""bibr"" target=""#b1"">[2]</ref> Ours Urban100 (4×): img 078 HR Bicubic LapSRN <ref type=""bib f> RDN <ref type=""bibr"" target=""#b38"">[39]</ref> RCAN <ref type=""bibr"" target=""#b36"">[37]</ref> SAN <ref type=""bibr"" target=""#b1"">[2]</ref> Ours Urban100 (4×): img 047 HR Bicubic LapSRN <ref type=""bib f> RDN <ref type=""bibr"" target=""#b38"">[39]</ref> RCAN <ref type=""bibr"" target=""#b36"">[37]</ref> SAN <ref type=""bibr"" target=""#b1"">[2]</ref> Ours which only needs 20% parameters of RCAN and SAN, but ac",1
"lance monitoring and high-definition display and imaging etc <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b31"">32,</ref><ref type=""bibr"" target=""#b39"">40,</ref><ref type=""bibr"" tar",0
"-scale cues. It has been verified that cross-scale patch similarity widely exists in natural images <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b41"">42]</ref>. Intuitively, in add hes tend to recur within and across scale of a same image has been verified for most natural images <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b41"">42]</ref>. Since then, a categ any external examples <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" targe et=""#b25"">26,</ref><ref type=""bibr"" target=""#b30"">31]</ref>. In the pioneering work, Glasner et al. <ref type=""bibr"" target=""#b8"">[9]</ref> proposed to jointly exploit repeating patches within and acr",0
"rget=""#b4"">[5,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b36"">37,</ref><ref type=""bibr"" target=""#b38"">39,</ref><ref type=""bibr"" tar et al. <ref type=""bibr"" target=""#b18"">[19]</ref>   <ref type=""bibr"" target=""#b19"">[20]</ref>, RNAN <ref type=""bibr"" target=""#b36"">[37]</ref> and SAN <ref type=""bibr"" target=""#b1"">[2]</ref>, incorpora DBPN <ref type=""bibr"" target=""#b10"">[11]</ref>, RDN <ref type=""bibr"" target=""#b38"">[39]</ref>, RCAN <ref type=""bibr"" target=""#b36"">[37]</ref>, NLRN <ref type=""bibr"" target=""#b19"">[20]</ref>, SRFBN <re > OISR <ref type=""bibr"" target=""#b11"">[12]</ref> RDN <ref type=""bibr"" target=""#b38"">[39]</ref> RCAN <ref type=""bibr"" target=""#b36"">[37]</ref> SAN <ref type=""bibr"" target=""#b1"">[2]</ref> Ours Urban100 > OISR <ref type=""bibr"" target=""#b11"">[12]</ref> RDN <ref type=""bibr"" target=""#b38"">[39]</ref> RCAN <ref type=""bibr"" target=""#b36"">[37]</ref> SAN <ref type=""bibr"" target=""#b1"">[2]</ref> Ours Urban100 > OISR <ref type=""bibr"" target=""#b11"">[12]</ref> RDN <ref type=""bibr"" target=""#b38"">[39]</ref> RCAN <ref type=""bibr"" target=""#b36"">[37]</ref> SAN <ref type=""bibr"" target=""#b1"">[2]</ref> Ours which onl",0
"RN <ref type=""bibr"" target=""#b19"">[20]</ref>, SRFBN <ref type=""bibr"" target=""#b17"">[18]</ref>, OISR <ref type=""bibr"" target=""#b11"">[12]</ref> and SAN <ref type=""bibr"" target=""#b1"">[2]</ref>.</p><p>Qua EDSR <ref type=""bibr"" target=""#b18"">[19]</ref> DBPN <ref type=""bibr"" target=""#b10"">[11]</ref> OISR <ref type=""bibr"" target=""#b11"">[12]</ref> RDN <ref type=""bibr"" target=""#b38"">[39]</ref> RCAN <ref ty EDSR <ref type=""bibr"" target=""#b18"">[19]</ref> DBPN <ref type=""bibr"" target=""#b10"">[11]</ref> OISR <ref type=""bibr"" target=""#b11"">[12]</ref> RDN <ref type=""bibr"" target=""#b38"">[39]</ref> RCAN <ref ty EDSR <ref type=""bibr"" target=""#b18"">[19]</ref> DBPN <ref type=""bibr"" target=""#b10"">[11]</ref> OISR <ref type=""bibr"" target=""#b11"">[12]</ref> RDN <ref type=""bibr"" target=""#b38"">[39]</ref> RCAN <ref ty",0
"nitial design space and the output is a refined design space of simpler or better models. Following <ref type=""bibr"" target=""#b20"">[21]</ref>, we characterize the quality of a design space by sampling gn, elevated to the population level and guided via distribution estimates of network design spaces <ref type=""bibr"" target=""#b20"">[21]</ref>.</p><p>As a testbed for this paradigm, our focus is on exp essential to use a reliable comparison metric to guide our design process. Recently, the authors of <ref type=""bibr"" target=""#b20"">[21]</ref> proposed a methodology for comparing and analyzing populat c scenario).</p><p>We rely on the concept of network design spaces introduced by Radosavovic et al. <ref type=""bibr"" target=""#b20"">[21]</ref>. A design space is a large, possibly infinite, population esign space is a large, possibly infinite, population of model architectures. The core insight from <ref type=""bibr"" target=""#b20"">[21]</ref> is that we can sample models from a design space, giving r ce design. To evaluate and compare design spaces, we use the tools introduced by Radosavovic et al. <ref type=""bibr"" target=""#b20"">[21]</ref>, who propose to quantify the quality of a design space by a single ResNet-50 <ref type=""bibr"" target=""#b7"">[8]</ref> model at 4GF for 100 epochs.</p><p>As in <ref type=""bibr"" target=""#b20"">[21]</ref>, our primary tool for analyzing design space quality is th i-c.org/ns/1.0""><head>Appendix C: Optimization Settings</head><p>Our basic training settings follow <ref type=""bibr"" target=""#b20"">[21]</ref> as discussed in §3. To tune the learning rate lr and weigh initial design space and the output is a refined design space of simpler or better models. Following<ref type=""bibr"" target=""#b20"">[21]</ref>, we characterize the quality of a design space by sampling tp://www.tei-c.org/ns/1.0"" place=""foot"" n=""1"" xml:id=""foot_0"">We use the term design space following<ref type=""bibr"" target=""#b20"">[21]</ref>, rather than search space, to emphasize that we are not se ://www.tei-c.org/ns/1.0"" place=""foot"" n=""5"" xml:id=""foot_4"">Our training setup in §3 exactly follows<ref type=""bibr"" target=""#b20"">[21]</ref>. We use SGD with momentum of 0.9, mini-batch size of 128 o",1
"space. For these plots, we employ an empirical bootstrap<ref type=""foot"" target=""#foot_3"">4</ref>  <ref type=""bibr"" target=""#b4"">[5]</ref> to estimate the likely range in which the best models fall.<",0
"<ref type=""bibr"" target=""#b15"">[16]</ref>, Cutout <ref type=""bibr"" target=""#b3"">[4]</ref>, DropPath <ref type=""bibr"" target=""#b13"">[14]</ref>, AutoAugment <ref type=""bibr"" target=""#b1"">[2]</ref>, and el><figDesc>Training enhancements to EFFICIENTNET-B0. Our EFFICIENTNET-B0 reproduction with DropPath<ref type=""bibr"" target=""#b13"">[14]</ref> and a 250 epoch training schedule (third row), achieves re",0
"as received a lot of attention and shown excellent results <ref type=""bibr"" target=""#b33"">[34,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b28"">29]</ref>. Despite the effect get=""#b22"">23,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b28"">29]</ref>. The majority of wo et=""#b34"">[35,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b17"">18]</ref>.</p><p>We emphasize that REGNET models use our basic 100 ep",0
"e VGG <ref type=""bibr"" target=""#b25"">[26]</ref>, Inception <ref type=""bibr"" target=""#b26"">[27,</ref><ref type=""bibr"" target=""#b27"">28]</ref>, ResNet <ref type=""bibr"" target=""#b7"">[8]</ref>, ResNeXt <r",0
"t-flips in <ref type=""bibr"" target=""#b7"">[8]</ref>. However, a newly proposed Bit-Flip Attack (BFA) <ref type=""bibr"" target=""#b16"">[17]</ref> whose progressive bit searching algorithm can successfully ial Weight Attack</head><p>The bit-flip based adversarial weight attack, aka. Bit-Flip Attack (BFA) <ref type=""bibr"" target=""#b16"">[17]</ref>, is an adversarial attack variant which performs weight fa loss increment. Thus, the bit searching in iteration i can be formulated as an optimization process <ref type=""bibr"" target=""#b16"">[17]</ref>:</p><formula xml:id=""formula_0"">max { Bi l } L f x; { Bi l e acceleration of modern AI applications. To clarify, we use the same threat model as in prior work <ref type=""bibr"" target=""#b16"">[17]</ref>, which is listed in Table <ref type=""table"">1</ref>.</p></ cted in Fig. <ref type=""figure"" target=""#fig_2"">2</ref>, the progressive bit search proposed in BFA <ref type=""bibr"" target=""#b16"">[17]</ref> is prone to identify vulnerable bit in the weight whose ab .   BFA Configuration. To evaluate the effectiveness of the proposed defense methods, the code from <ref type=""bibr"" target=""#b16"">[17]</ref> is utilized with further modification. The number of bit-f trials. Note that, all the quantized DNN reported hereafter still uses the uniform quantizer as in <ref type=""bibr"" target=""#b16"">[17]</ref>, but with quantization-aware training instead of post-trai ns/1.0""><head n=""5.3."">Comparison of Alternative Defense Methods</head><p>Adversarial weight attack <ref type=""bibr"" target=""#b16"">[17]</ref> is a recently developed security threat model for modern D Trained adversarial defense <ref type=""bibr"" target=""#b14"">[15]</ref> with strong weight attack BFA <ref type=""bibr"" target=""#b16"">[17]</ref>. Again, adversarial input defense fails to defend BFA, req",1
""">[2]</ref> is adopted to address the non-differential problem for the sign function as prior works <ref type=""bibr"" target=""#b9"">[10]</ref>. Nevertheless, different from STE in <ref type=""bibr"" targe nction as prior works <ref type=""bibr"" target=""#b9"">[10]</ref>. Nevertheless, different from STE in <ref type=""bibr"" target=""#b9"">[10]</ref>, the gradient clipping constraint is omitted from the backw",0
"ue, which converts the weights from 32-bit floating-point to {-1,+1} binary format encoded by 1-bit <ref type=""bibr"" target=""#b17"">[18]</ref>. Here, the binarizationaware training is leveraged as a de",0
"as in Eq. ( <ref type=""formula"" target=""#formula_2"">3</ref>). The Straight Through Estimator (STE) <ref type=""bibr"" target=""#b1"">[2]</ref> is adopted to address the non-differential problem for the s",0
"osed approach is more practical because the training data is generally inaccessible to the attacker <ref type=""bibr"" target=""#b31"">[32]</ref>. Our contributions can be summarized as follows:</p><p>• W original training data. However, in practice the attacker often has no access to the training data <ref type=""bibr"" target=""#b31"">[32]</ref>. To overcome this limitation, Mopuri et al. propose to gen ome this limitation, Mopuri et al. propose to generate universal perturbation without training data <ref type=""bibr"" target=""#b31"">[32]</ref>. However, their approach is specifically designed for non- -agnostic) attacks <ref type=""bibr"" target=""#b26"">[27,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b31"">32,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" tar",1
"C <ref type=""bibr"" target=""#b8"">[9]</ref>, two widely used object detection datasets, and Places365 <ref type=""bibr"" target=""#b49"">[50]</ref>, a large-scale scene recognition dataset. We generated tar",0
"-dependent attacks <ref type=""bibr"" target=""#b41"">[42,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b29"">30,</ref><ref type=""bibr"" tar pe=""bibr"" target=""#b4"">5]</ref> to FGSM related techniques <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" targe",0
"target=""#b41"">[42,</ref><ref type=""bibr"" target=""#b36"">37]</ref>. A wide variety of previous works <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b42"">43,</ref><ref type=""bibr"" ta pe=""bibr"" target=""#b41"">[42,</ref><ref type=""bibr"" target=""#b4"">5]</ref> to FGSM related techniques <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" ta to the local linearity of DNNs, and support their claim by their proposed simple yet effective FGSM <ref type=""bibr"" target=""#b13"">[14]</ref>. However, this linearity hypothesis is not fully compatibl cks are commonly categorized under image-dependent attacks <ref type=""bibr"" target=""#b41"">[42,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" tar",0
"""Wood rabbit"" is predicted for c and b. Such combination of images has also been explored in Mixup <ref type=""bibr"" target=""#b48"">[49]</ref> for training classifiers. To establish the reliability of",0
"]</ref> have exhibited tremendous progress in representation learning for generic graphs (GraphSAGE <ref type=""bibr"" target=""#b5"">[6]</ref>, AS-GCN <ref type=""bibr"" target=""#b7"">[8]</ref>). In general f computational inefficiency, scaling up is difficult. Although sampling methods, such as GraphSAGE <ref type=""bibr"" target=""#b5"">[6]</ref>, AS-GCN <ref type=""bibr"" target=""#b7"">[8]</ref> and FastGCN c <ref type=""bibr"" target=""#b4"">[5]</ref>, VGAE <ref type=""bibr"" target=""#b9"">[10]</ref>, GraphSAGE <ref type=""bibr"" target=""#b5"">[6]</ref>, and AS-GCN <ref type=""bibr"" target=""#b7"">[8]</ref>. We use ures).</p><p>As we can see from Eq.7, there are two distinctions between IDMP and conventional GCNs <ref type=""bibr"" target=""#b5"">[6]</ref>: 1. IDMP only performs aggregation on each node's neighbor n r by layer leads to a low computational speed and high memory cost. Sampling methods like GraphSAGE <ref type=""bibr"" target=""#b5"">[6]</ref> and AS-GCN <ref type=""bibr"" target=""#b7"">[8]</ref> have been n Results</head><p>We evaluated our BGNN results on a classification downstream task with 𝐹 1 score <ref type=""bibr"" target=""#b5"">[6]</ref> which is a popular metric for classification. For binary cla f 𝐵 𝑢 (resp. 𝐵 𝑣 ) as well as feature information from 𝑋 𝑢 and 𝑋 𝑣 . As presented in previous works <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b10"">11]</ref>, the onehop aggregat s a decoder to embed the nodes into a lowdimensional feature space. • GraphSAGE-MEAN, GraphSAGE-GCN <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b10"">11]</ref>: We implement two ty cific: in another word, they require labels in downstream tasks to supervise the models. Some works <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target",1
"presentation learning for generic graphs (GraphSAGE <ref type=""bibr"" target=""#b5"">[6]</ref>, AS-GCN <ref type=""bibr"" target=""#b7"">[8]</ref>). In general, GNNs recursively update each node's feature by icult. Although sampling methods, such as GraphSAGE <ref type=""bibr"" target=""#b5"">[6]</ref>, AS-GCN <ref type=""bibr"" target=""#b7"">[8]</ref> and FastGCN <ref type=""bibr"" target=""#b0"">[1]</ref>), have b type=""bibr"" target=""#b9"">[10]</ref>, GraphSAGE <ref type=""bibr"" target=""#b5"">[6]</ref>, and AS-GCN <ref type=""bibr"" target=""#b7"">[8]</ref>. We use a large-scale bipartite graph dataset from the Tence igh memory cost. Sampling methods like GraphSAGE <ref type=""bibr"" target=""#b5"">[6]</ref> and AS-GCN <ref type=""bibr"" target=""#b7"">[8]</ref> have been proposed to deal with this issue by reducing the n ons: GCN and MEAN aggregator. Node-wise sampling is used to address the scalability issue. • AS-GCN <ref type=""bibr"" target=""#b7"">[8]</ref>: This method uses adaptive sampling between each layer to de",1
"ated. • Node2Vec <ref type=""bibr"" target=""#b4"">[5]</ref>: This approach is an extension of Word2Vec <ref type=""bibr"" target=""#b14"">[15]</ref> on graph, which learns a feature representation by simulat",0
"ype=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b26"">27]</ref>, social networks analysis <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b23"">24]</ref>, and visual unders",0
"t bipartite graphs as heterogeneous networks and use random walk-based methods such as Metapath2Vec <ref type=""bibr"" target=""#b1"">[2]</ref>. However, Metapath2Vec does not integrate node features into o heterogeneous graphs, where different nodes are in distinct feature domains, such as MethPath2Vec <ref type=""bibr"" target=""#b1"">[2]</ref> and PTE <ref type=""bibr"" target=""#b21"">[22]</ref>. Although",0
"e in distinct feature domains, such as MethPath2Vec <ref type=""bibr"" target=""#b1"">[2]</ref> and PTE <ref type=""bibr"" target=""#b21"">[22]</ref>. Although all these methods do not require node labels in",0
"e=""bibr"" target=""#b26"">27]</ref>, social networks analysis <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b23"">24]</ref>, and visual understanding <ref type=""bibr"" target=""#b22"">[2",0
"<ref type=""bibr"" target=""#b5"">[6]</ref>, AS-GCN <ref type=""bibr"" target=""#b7"">[8]</ref> and FastGCN <ref type=""bibr"" target=""#b0"">[1]</ref>), have been proposed to deal with the scalability issue (unc",0
"also construct three synthesized datasets based on the citation networks Cora, Citeseer, and PubMed <ref type=""bibr"" target=""#b20"">[21]</ref>. For all benchmarks, BGNN outperforms other competitive ba",0
"upervised representation learning baselines: Node2Vec <ref type=""bibr"" target=""#b4"">[5]</ref>, VGAE <ref type=""bibr"" target=""#b9"">[10]</ref>, GraphSAGE <ref type=""bibr"" target=""#b5"">[6]</ref>, and AS- Vec on the bipartite graph and then concatenate the node embeddings with their own features. • VGAE <ref type=""bibr"" target=""#b9"">[10]</ref>: This method is based on a variational autoencoder, where G the models. Some works <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b13"">14]</ref> try to utilize GCN t",0
"xt to speech, TTS) <ref type=""bibr"" target=""#b27"">[28,</ref><ref type=""bibr"" target=""#b29"">30,</ref><ref type=""bibr"" target=""#b34"">35,</ref><ref type=""bibr"" target=""#b40"">41]</ref> and speech recognit et=""#b21"">[22,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b29"">30,</ref><ref type=""bibr"" target=""#b34"">35]</ref> and ASR <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""b , we re-sample it to 16kHZ and convert the raw waveform into mel-spectrograms following Shen et al. <ref type=""bibr"" target=""#b34"">[35]</ref> with 50ms frame size, 12.5ms hop size. For the text, we us",1
"ns of hours in ASR <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b31"">32,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" target=""#b38"">39]</ref>, compared to that i",0
"e, extremely low-resource and unsupervised settings.</p><p>• In the rich-resource setting, both TTS <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" ta",0
"promising accuracy achieved by LRSpeech.</p><p>According to <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" tar",0
"oduce the extremely low data cost while promising accuracy achieved by LRSpeech.</p><p>According to <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" targ",0
"get=""#b40"">[41]</ref>. Using such noisy query entities in ranking often requires manual annotations <ref type=""bibr"" target=""#b11"">[12]</ref> or soft linking/diversification <ref type=""bibr"" target=""# h as term weight in queries according to entity descriptions <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b11"">12]</ref>. There are also some researches using entities as connectio",1
"get=""#b20"">21,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b39"">40,</ref><ref type=""bibr"" target=""#b41"">42,</ref><ref type=""bibr"" target=""#b42"">43]</ref>.</p><p>Both persona ty-oriented search is the difficulty of query entity linking. Queries are often short and ambiguous <ref type=""bibr"" target=""#b41"">[42]</ref>, making query entity linking a challenging task: A recent quires manual annotations <ref type=""bibr"" target=""#b11"">[12]</ref> or soft linking/diversification <ref type=""bibr"" target=""#b41"">[42]</ref>. Personalization provides a natural way to help resolve th mory network that represents user's search preferences in the word-entity duet representation space <ref type=""bibr"" target=""#b41"">[42]</ref>. KEPS then conducts personalized ranking to adapt document eraction between bags of word representations and bags of entity representations is also studied in <ref type=""bibr"" target=""#b41"">[42]</ref>. Neuralbased search model EDRM <ref type=""bibr"" target=""#b",1
"king quality for future queries.</p><p>In our experiments on a recent contextualized search dataset <ref type=""bibr"" target=""#b0"">[1]</ref>, KEPS provides significantly more accurate search results th search versus session-based search; the comparisons with the state-of-the-art session-based ranker <ref type=""bibr"" target=""#b0"">[1]</ref> and KEPS's different variants demonstrate the advantage of K period to construct user profiles. Recently, the context search dataset constructed by Wasi et al. <ref type=""bibr"" target=""#b0"">[1]</ref>, which is based on the public AOL search log <ref type=""bibr possible to study personalized search in the public domain. We conduct experiments on this dataset <ref type=""bibr"" target=""#b0"">[1]</ref>.</p><p>Entity-Oriented Search. There have been many attempts e the detailed experimental setups.</p><p>Dataset. The dataset we use is constructed by Wasi et al. <ref type=""bibr"" target=""#b0"">[1]</ref> using the AOL search log, in which the candidate documents f aset that can be used for knowledge enhanced personalized search. We split the search log following <ref type=""bibr"" target=""#b0"">[1]</ref> to get background set, training set, test and valid set. Not >[1]</ref> to get background set, training set, test and valid set. Note the setting different from <ref type=""bibr"" target=""#b0"">[1]</ref> is that we keep the background set to provide user's basic h ion boundaries are decided based on the differences between query vectors as the task boundaries in <ref type=""bibr"" target=""#b0"">[1]</ref>.</p><p>We use the entity titles in Wikipedia. The candidate t-aware search also uses part of user's history, we take the state-of-art context search model CARS <ref type=""bibr"" target=""#b0"">[1]</ref> as a baseline to show the effect of personalization.</p><p>I ts with the same ranking scores randomly. We take the right clicked documents as relevant following <ref type=""bibr"" target=""#b0"">[1]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n= sed is helpful to rank the documents.</p><p>(4) The results of CARS are lower than that reported in <ref type=""bibr"" target=""#b0"">[1]</ref> is because we follow official TREC_ Eval and rank documents ://www.tei-c.org/ns/1.0""><p>the same ranking scores randomly. If we use the evaluation script as in <ref type=""bibr"" target=""#b0"">[1]</ref>, the gain of KEPS is consistent, which reaches 0.78 on MAP. ession-based search. So we also compare SKEPS with the state-of-art session-based search model CARS <ref type=""bibr"" target=""#b0"">[1]</ref> to show our model effect in Tab. 8.</p><p>From Tab. 8 we can ? is a parameter vector. We obtain the long-term interests ? ? by replacing ? ? ? and ? ? ? in Eqn. <ref type=""bibr"" target=""#b0"">(1)</ref> with the query text embedding ? ? ? in long-term history Q ?",0
"get=""#b27"">28,</ref><ref type=""bibr"" target=""#b39"">40,</ref><ref type=""bibr"" target=""#b41"">42,</ref><ref type=""bibr"" target=""#b42"">43]</ref>.</p><p>Both personalized search and entity-oriented search",0
"dge graphs, in search systems and effectively improves the text representation and ranking accuracy <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" ta s a latent space and learn query-document matching relevance through the latent space. Ensan et al. <ref type=""bibr"" target=""#b13"">[14]</ref> used a probability model to model the semantic entity link",0
"get=""#b16"">17,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b28"">29,</ref><ref type=""bibr"" target=""#b33"">34,</ref><ref type=""bibr"" target=""#b34"">35,</ref><ref type=""bibr"" tar get=""#b15"">16,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b33"">34]</ref>. In the released dataset from Yandex <ref type=""foot"" targe",0
"d document relevance. Click features and topic features are combined and studied in some researches <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target </p><p>Another challenge in personalized search is that many search logs are not publicly available <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" targ ef><ref type=""bibr"" target=""#b37"">38,</ref><ref type=""bibr"" target=""#b38"">39]</ref>. Bennett et al. <ref type=""bibr"" target=""#b3"">[4]</ref> proposed SLTB to combine the two types of features using lea aditional features: P-Click <ref type=""bibr"" target=""#b12"">[13]</ref> using click features and SLTB <ref type=""bibr"" target=""#b3"">[4]</ref> using click features and topic features, which is the state-",0
"cal features including the popularity of the candidate entity and the linking scores given by TAGME <ref type=""bibr"" target=""#b14"">[15]</ref>. Since user's historical search behavior reflects user's i s are done in Sec. 3.2). Since there is no entity annotation in documents in our data, we use TAGME <ref type=""bibr"" target=""#b14"">[15]</ref> to link entities in Wikipedia to the document titles. And improvement of our model. We also evaluate the effect of PEDRM we proposed in Sec 3.4, using TAGME <ref type=""bibr"" target=""#b14"">[15]</ref> to conduct entity linking for queries and removing the pre s and documents are calculated by the average sum of embedding of associated entity linked by TAGME <ref type=""bibr"" target=""#b14"">[15]</ref>. Since context-aware search also uses part of user's histo corresponding model KEPS-noPSLink, KEPS-noMN and KEPS-noAdjust. KEPS-noPSLink, directly using TAGME <ref type=""bibr"" target=""#b14"">[15]</ref> to link entities on the queries.</p><p>We also conduct abl",0
"features to improve personalized ranking effect. Other works <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" targe",0
"ersonalized search <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" tar t publicly available <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" tar e=""bibr"" target=""#b15"">[16]</ref> proposed to use a hierarchical RNN to model user's profile. PSGAN <ref type=""bibr"" target=""#b21"">[22]</ref> proposed a generative adversarial network framework to pro t=""#b15"">[16]</ref>) using hierarchical RNN and PSGAN (we choose the document-selection based model <ref type=""bibr"" target=""#b21"">[22]</ref>) using adversarial training. To make a fair comparison, we e the previous history may reflect user's global interests <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b21"">22]</ref>.</p><p>Suppose a query ? has ? entity mentions (text string ef>.</p><p>Evaluation Metrics. Following the previous work <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b21"">22]</ref>, we use MAP, MRR, P@K (precision in the top k positions) an of queries. However we find that different from stated in <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b21"">22]</ref>, the personalization models perform better on queries with less efficient than the ranking from search engine used in <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b21"">22]</ref>. Compared with SLTB, we can still see HRNN, HRNN-Entity and o less than 1, which is consistent with the previous works <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b21"">22]</ref>. KEPS has significant improvement over the baselines on bot",0
"-lasting tradition started by social scientists. Some others <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" tar widely used for such kinds of analysis since the early 1980s <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b28"">28]</ref>. On the other hand, above challenges well. Even though some realized the importance of links <ref type=""bibr"">[9,</ref><ref type=""bibr"" target=""#b12"">13]</ref>, they failed to provide an embedding. Most people learn an type=""bibr"" target=""#b16"">[17]</ref>, only a few paid attention to links <ref type=""bibr"">[9,</ref><ref type=""bibr"" target=""#b12"">13]</ref>. Our work differs from them all, since: (1) unlike probabil ollected such as from news articles <ref type=""bibr"" target=""#b1"">[2]</ref> or from social networks <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" ta",1
"should benefit from leveraging auxiliary knowledge from each other. As is concluded in an overview <ref type=""bibr"" target=""#b31"">[31]</ref>, MTL could be applied with or without neural network struc",0
"to its full potential, modeling the relations without allowing complex interactions among them. GTN <ref type=""bibr"" target=""#b42"">[42]</ref> is similar with SHINE in splitting the graph into separate NN <ref type=""bibr"" target=""#b43"">[43]</ref>, GATNE <ref type=""bibr"" target=""#b2"">[3]</ref> and GTN <ref type=""bibr"" target=""#b42"">[42]</ref> generally converge ? 10 ? 100 times slower than our model",0
"tter dataset is large and the links are relatively dense (Section 4). Some models such as GraphSAGE <ref type=""bibr"" target=""#b13"">[14]</ref> will be super slow sampling our graph. Second, labels are ould also be viewed as sampling and aggregating of the neighborhood information, such as Graph-SAGE <ref type=""bibr"" target=""#b13"">[14]</ref> and FastGCN <ref type=""bibr"" target=""#b3"">[4]</ref>, enabl =""#b42"">[42]</ref> generally converge ? 10 ? 100 times slower than our model on any task. GraphSAGE <ref type=""bibr"" target=""#b13"">[14]</ref> is not very suitable on our dataset. Moreover, other well-",0
"igh density, GIN, GraphSAGE and its extension onto heterogeneous information network such as HetGNN <ref type=""bibr"" target=""#b43"">[43]</ref> and GATNE <ref type=""bibr"" target=""#b2"">[3]</ref> are not We have explored a lot of possible baseline models. Some methods we mentioned in section 2, HetGNN <ref type=""bibr"" target=""#b43"">[43]</ref>, GATNE <ref type=""bibr"" target=""#b2"">[3]</ref> and GTN <re",0
"xtension of GAT on heterogeneous information networks is Heterogeneous Graph Attention Network, HAN <ref type=""bibr"" target=""#b39"">[39]</ref>. Beside inheriting the node-level attention from GAT, it c",0
"neighborhood information, such as Graph-SAGE <ref type=""bibr"" target=""#b13"">[14]</ref> and FastGCN <ref type=""bibr"" target=""#b3"">[4]</ref>, enabling training in batches. To improve GraphSAGE's expres",0
"could be naturally divided into two directions, based on the targets to predict: of the politicians <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b24"">24,</ref><ref type=""bibr"" targ ng those groups into the 4 subsets, as is shown in Table <ref type=""table"" target=""#tab_2"">1</ref>. <ref type=""bibr"" target=""#b6"">(7)</ref> We filter the relations within any selected group so that if",0
"e regarded as an approximation of spectraldomain convolution of the graph signals. A deeper insight <ref type=""bibr"" target=""#b21"">[21]</ref> shows that the key reason why GCN works so well on classif",0
"tering transforms or certain CNN architectures have been shown to be stable to spatial deformations <ref type=""bibr"" target=""#b31"">[32,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" targ ze the stability of GCNs to small deformation of the underlying random graph model. Similar to CNNs <ref type=""bibr"" target=""#b31"">[32,</ref><ref type=""bibr"" target=""#b3"">4]</ref>, studying GCNs in th es of stability are often balanced by discussions on how the representation preserves signal (e.g., <ref type=""bibr"" target=""#b31"">[32,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" targ ine intuitive notions of deformations and stability in the continuous world like the Euclidean case <ref type=""bibr"" target=""#b31"">[32,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" targ p><p>Related work on stability. The study of stability to deformations has been pioneered by Mallat <ref type=""bibr"" target=""#b31"">[32]</ref> in the context of the scattering transform for signals on ph models and to obtain deformation stability bounds that are similar to those on Euclidean domains <ref type=""bibr"" target=""#b31"">[32]</ref>. We note that <ref type=""bibr"" target=""#b28"">[29]</ref> al eformations is an essential feature for the generalization properties of deep architectures. Mallat <ref type=""bibr"" target=""#b31"">[32]</ref> studied the stability to small deformations of the wavelet for small enough ∇τ ∞ , we obtain N P (τ ) d ∇τ ∞ , recovering the more standard quantity of Mallat <ref type=""bibr"" target=""#b31"">[32]</ref>. In this case, we also have the bound</p><formula xml:id="" ). Once again we focus on invariant c-GCNs with pooling, similar to classical scattering transform <ref type=""bibr"" target=""#b31"">[32]</ref>.</p><p>Proposition 4 (Signal deformation). Consider a GCN τ ∞ , the GCN is invariant to translations and stable to deformations, similar to Euclidean domains <ref type=""bibr"" target=""#b31"">[32]</ref>. We note that studies of stability are often balanced by d",1
"to their power to distinguish (or not) graph isomorphisms <ref type=""bibr"" target=""#b45"">[46,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b32"">33]</ref> or compute combinat",0
"[10]</ref>, semi-supervised learning <ref type=""bibr"" target=""#b24"">[25]</ref>, or graph regression <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b18"">19]</ref>, and remain one of 1 + log(1/ρ) log(n) ∼ 1, then use a union bound to conclude.</p><p>We now bound the second term in <ref type=""bibr"" target=""#b21"">(22)</ref>. Define ρ k = Cρ (k+1) 2 ℓ d ℓ with C such that kℓ d ℓ ρ k",0
"""1"">Introduction</head><p>Graph Convolutional Networks (GCNs <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b24"">25]</ref>) are deep architect models of the literature, including all spectral-based GCNs <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b13"">14]</ref>, or GCNs with order-1 filters <ref type=""bibr"" target=""#b24 mula><p>We do not assume that the filters are of finite order (even if they usually are in practice <ref type=""bibr"" target=""#b13"">[14]</ref>), however we will always assume that</p><formula xml:id=""f",0
"tures have been shown to be stable to spatial deformations <ref type=""bibr"" target=""#b31"">[32,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b37"">38]</ref>. However the notion o cussions on how the representation preserves signal (e.g., <ref type=""bibr"" target=""#b31"">[32,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b16"">17]</ref>). In our context, the stability in the continuous world like the Euclidean case <ref type=""bibr"" target=""#b31"">[32,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b37"">38]</ref>, with direct applicat tion of the underlying random graph model. Similar to CNNs <ref type=""bibr"" target=""#b31"">[32,</ref><ref type=""bibr"" target=""#b3"">4]</ref>, studying GCNs in the continuous world allows us to define in ef><ref type=""bibr"" target=""#b1"">2]</ref>, and was later extended to more generic CNN architectures <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b37"">38]</ref>. A more recent line based scattering transform, which was extended to more generic learned convolutional network, e.g., <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b37"">38]</ref>, and tries to establ",0
"NP, showing more efficiency on graphs with few labeled nodes. Inspired by the Layer Effect on graphs<ref type=""bibr"" target=""#b31"">(Sun et al., 2019)</ref>, we argue that the increase of layers in Ada",1
"rks <ref type=""bibr"">(Xu et al., 2018b;</ref><ref type=""bibr"" target=""#b23"">Liao et al., 2019;</ref><ref type=""bibr"" target=""#b20"">Klicpera et al., 2018;</ref><ref type=""bibr"" target=""#b21"">Li et al., empirically demonstrated in many recent works <ref type=""bibr"" target=""#b34"">(Wu et al., 2019;</ref><ref type=""bibr"" target=""#b20"">Klicpera et al., 2018;</ref><ref type=""bibr"">Xu et al., 2018a)</ref>, We also established a strong connection between AdaGCN and previous state-of-the-art PPNP and APPNP <ref type=""bibr"" target=""#b20"">(Klicpera et al., 2018)</ref> method that leverages personalized page al networks suffer from overfitting to a single splitting of training, validation and test datasets <ref type=""bibr"" target=""#b20"">(Klicpera et al., 2018)</ref>. To address this problem, inspired by < <ref type=""bibr"" target=""#b20"">(Klicpera et al., 2018)</ref>. To address this problem, inspired by <ref type=""bibr"" target=""#b20"">(Klicpera et al., 2018)</ref>, we test all approaches on multiple ran igure"" target=""#fig_5"">3</ref>. In Table <ref type=""table"">2</ref>, we employ the same baselines as <ref type=""bibr"" target=""#b20"">(Klicpera et al., 2018)</ref>: V.GCN (vanilla GCN) (Kipf &amp; Wellin f type=""bibr"" target=""#b13"">(Hamilton et al., 2017)</ref>. We refer to the result of baselines from <ref type=""bibr"" target=""#b20"">(Klicpera et al., 2018)</ref> / and the implementation of AdaGCN is a",1
"e learning community. Graph neural networks <ref type=""bibr"" target=""#b12"">(Gori et al., 2005;</ref><ref type=""bibr"" target=""#b13"">Hamilton et al., 2017;</ref><ref type=""bibr"" target=""#b33"">Veli?kovi? y compare AdaGCN with FastGCN <ref type=""bibr"" target=""#b7"">(Chen et al., 2018)</ref> and GraphSAGE <ref type=""bibr"" target=""#b13"">(Hamilton et al., 2017)</ref>. We refer to the result of baselines fr",0
"ve commonly used graphs: CiteSeer, Cora-ML <ref type=""bibr"">(Bojchevski &amp; G?nnemann, 2018;</ref><ref type=""bibr"" target=""#b28"">McCallum et al., 2000)</ref>, PubMed <ref type=""bibr"" target=""#b29"">(",0
"al., 2017)</ref> has already successfully incorporated boosting algorithm into the training of GAN <ref type=""bibr"" target=""#b11"">(Goodfellow et al., 2014)</ref>.</p><p>In this work, we focus on inco",0
""" target=""#b12"">(Gori et al., 2005;</ref><ref type=""bibr"" target=""#b13"">Hamilton et al., 2017;</ref><ref type=""bibr"" target=""#b33"">Veli?kovi? et al., 2018)</ref>, particularly graph convolutional netw network of GCN) <ref type=""bibr"">(Abu-El-Haija et al., 2018a)</ref>, GAT (Graph Attention Networks) <ref type=""bibr"" target=""#b33"">(Veli?kovi? et al., 2018)</ref>, BT.FP (bootstrapped feature propagat",0
"rget=""#b27"">Mannor et al., 2003)</ref> can still be preserved when the samples are weakly dependent <ref type=""bibr"" target=""#b24"">(Lozano et al., 2013)</ref>. More discussion can refer to Appendix A.",0
"/ref>. Moreover, boosting theory has been used to analyze the success of ResNets in computer vision <ref type=""bibr"" target=""#b16"">(Huang et al., 2018)</ref> and <ref type=""bibr"">AdaGAN (Tolstikhin et",0
"ode classification <ref type=""bibr"" target=""#b19"">(Kipf &amp; Welling, 2017)</ref>, link prediction <ref type=""bibr"" target=""#b38"">(Zhu et al., 2016)</ref> and clustering tasks <ref type=""bibr"" target",0
"ed learning for computing meaningful and interpretable clusters on input graphs. On the other hand, <ref type=""bibr"" target=""#b21"">[22]</ref> proposes an approach that automatically constructs an easy such as link prediction, e-commerce recommendation, etc, <ref type=""bibr"" target=""#b18"">[19]</ref>, <ref type=""bibr"" target=""#b21"">[22]</ref>. There are some recent works that learn hierarchical graph raph representation is e-commerce taxonomy for offering a personalized dynamic shopping navigation. <ref type=""bibr"" target=""#b21"">[22]</ref> illstrates a topic-driven hierarchical taxonomy based on u </p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>D. Experiments and Results</head><p>SHOAL <ref type=""bibr"" target=""#b21"">[22]</ref> is Alibaba's current topic-driven taxonomy solution deploy iveness, we compare our proposed method with Alibaba's current topic-driven taxonomy solution SHOAL <ref type=""bibr"" target=""#b21"">[22]</ref>. In the parameter setting, we set the level number of the",1
"rchy which have shown an advantageous performance over paradigms using user-item interactions alone <ref type=""bibr"" target=""#b18"">[19]</ref>- <ref type=""bibr"" target=""#b20"">[21]</ref>.</p><p>Generall representation involves extensive and unscalable computation with the adjacent matrix of the graph. <ref type=""bibr"" target=""#b18"">[19]</ref> learns a hierarchical representation of graphs by decompos nd becomes prevailing in several scenarios such as link prediction, e-commerce recommendation, etc, <ref type=""bibr"" target=""#b18"">[19]</ref>, <ref type=""bibr"" target=""#b21"">[22]</ref>. There are some of our proposed method, which fixes the number of user levels to 2. The parameter of CGNN refers to <ref type=""bibr"" target=""#b18"">[19]</ref>. • DIN: A popular deep neural network method without graph",1
"ently flat and do not learn hierarchical representations of graphs. On one hand, it demonstrates in <ref type=""bibr"" target=""#b19"">[20]</ref> that hierarchical representations of graphs can be combine ng GNNs with different clustering processes. In particular, the recently proposed approach DIFFPOOL <ref type=""bibr"" target=""#b19"">[20]</ref>, a differentiable graph pooling module that can generate h ing to be effective in graph classification tasks, in addition to a user's individual embedding. In <ref type=""bibr"" target=""#b19"">[20]</ref>, authors make some efforts in effectively co-training two world e-commerce tasks of such large scale, including <ref type=""bibr"" target=""#b29"">[30]</ref> and <ref type=""bibr"" target=""#b19"">[20]</ref>. Our baseline algorithms are as follows:</p><p>• CGNN: A g",1
"tiveness of a query q for a topic t k can be derived from r(q, t k ) = pop(q, t k ) • con(q, t k ), <ref type=""bibr"" target=""#b13"">(14)</ref> in which pop(q, t k ) and con(q, t k ) are the popularity",0
"luding Click Through Rate (CTR), Conversion Rate (CVR), personalized recommendation list, and so on <ref type=""bibr"" target=""#b0"">[1]</ref>- <ref type=""bibr"" target=""#b4"">[5]</ref>. Precisely predicti egarded as a special case of our proposed method at level 0 (L = 0). The parameter of DIN refers to <ref type=""bibr"" target=""#b0"">[1]</ref>.</p><p>• GE: Single level Graph Embedding-based (GE) method,",0
"s matrix factorization or matrix multiplication, which makes it less scalable on large-scale graphs <ref type=""bibr"" target=""#b29"">[30]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head> sequently, it is computationally expensive that make it less popular in handling large-scale graphs <ref type=""bibr"" target=""#b29"">[30]</ref>. On the other hand, some researchers <ref type=""bibr"" targ rarchical item attractiveness to predict real-world e-commerce tasks of such large scale, including <ref type=""bibr"" target=""#b29"">[30]</ref> and <ref type=""bibr"" target=""#b19"">[20]</ref>. Our baselin",0
"g large-scale graphs <ref type=""bibr"" target=""#b29"">[30]</ref>. On the other hand, some researchers <ref type=""bibr"" target=""#b30"">[31]</ref>, <ref type=""bibr"" target=""#b31"">[32]</ref> illustrate a us",0
"rmance over paradigms using user-item interactions alone <ref type=""bibr"" target=""#b18"">[19]</ref>- <ref type=""bibr"" target=""#b20"">[21]</ref>.</p><p>Generally speaking, GNN methods are inherently flat",0
"ww.tei-c.org/ns/1.0""><head>B. Graph-based Collaborative Filtering</head><p>Another line of research <ref type=""bibr"" target=""#b26"">[27]</ref>, <ref type=""bibr"" target=""#b27"">[28]</ref> exploits the us",0
"number of queries. To make the topic more interpretive, we follow the similar strategy described in <ref type=""bibr"" target=""#b36"">[37]</ref> to find the most representative query as the description f",0
"nodes in GCN are inclined to converge to a certain value and thus become indistinguishable. ResNet <ref type=""bibr"" target=""#b13"">(He et al., 2016)</ref> solves a similar problem in computer vision w he -th weight matrix W ( ) . Initial residual connection. To simulate the skip connection in ResNet <ref type=""bibr"" target=""#b13"">(He et al., 2016)</ref>, <ref type=""bibr"" target=""#b16"">(Kipf &amp; W ations for introducing identity mapping into our model.</p><p>• Similar to the motivation of ResNet <ref type=""bibr"" target=""#b13"">(He et al., 2016)</ref>, identity mapping ensures that a deep GCNII m",1
"ing the K-th power of the graph convolution matrix in a single neural network layer. PPNP and APPNP <ref type=""bibr"" target=""#b17"">(Klicpera et al., 2019a)</ref> replace the power of the graph convolu m.</p><formula xml:id=""formula_3"">sponds to D−1/2 Ã D−1/2 K x = I n − L K x. (Wu</formula><p>APPNP. <ref type=""bibr"" target=""#b17"">(Klicpera et al., 2019a)</ref> uses Personalized PageRank to derive a sonalized PageRank, such a filter preserves locality and thus is suitable for classification tasks. <ref type=""bibr"" target=""#b17"">(Klicpera et al., 2019a)</ref> also proposes APPNP, which</p><formula al initial representation H (0) before the forward propagation.</p><p>Finally, we recall that APPNP <ref type=""bibr"" target=""#b17"">(Klicpera et al., 2019a)</ref> employs a similar approach to the init • It has been observed that frequent interaction between different dimensions of the feature matrix <ref type=""bibr"" target=""#b17"">(Klicpera et al., 2019a)</ref> degrades the performance of the model om both the input feature and the graph structure even with K going to infinity. For example, APPNP <ref type=""bibr"" target=""#b17"">(Klicpera et al., 2019a)</ref> and GDC <ref type=""bibr"" target=""#b18"" Welling, 2017)</ref>, GAT <ref type=""bibr"" target=""#b38"">(Veličković et al., 2018)</ref> and APPNP <ref type=""bibr"" target=""#b17"">(Klicpera et al., 2019a)</ref>.</p><p>We use the Adam SGD optimizer < milar approach to the initial residual connection in the context of Personalized PageRank. However, <ref type=""bibr"" target=""#b17"">(Klicpera et al., 2019a</ref>) also shows that performing multiple no",1
"18)</ref> have been successfully applied to a wide range of applications, including social analysis <ref type=""bibr"" target=""#b32"">(Qiu et al., 2018;</ref><ref type=""bibr"" target=""#b21"">Li &amp; Goldw",0
"> improves flexibility by learning a task-driven adaptive graph for each graph data while training. <ref type=""bibr"" target=""#b42"">(Xu et al., 2019)</ref> uses the graph wavelet basis instead of the F",0
"type=""bibr"">Li et al., 2019)</ref>, biology <ref type=""bibr"" target=""#b8"">(Fout et al., 2017;</ref><ref type=""bibr"" target=""#b36"">Shang et al., 2019)</ref>, recommender systems <ref type=""bibr"" targe",0
"2018)</ref> 97.6 GeniePath <ref type=""bibr"" target=""#b26"">(Liu et al., 2019)</ref> 98.5 Cluster-GCN <ref type=""bibr"" target=""#b3"">(Chiang et al., 2019)</ref> 99.36 GCNII 99.53 ± 0.01 GCNII* 99.56 ± 0. t (Xu et al., 2018), GeniePath <ref type=""bibr"" target=""#b26"">(Liu et al., 2019)</ref>, Cluster-GCN <ref type=""bibr"" target=""#b3"">(Chiang et al., 2019)</ref>. The metrics are summarized in Table <ref",0
"ut et al., 2017;</ref><ref type=""bibr"" target=""#b36"">Shang et al., 2019)</ref>, recommender systems <ref type=""bibr"" target=""#b45"">(Ying et al., 2018)</ref>, and com-1 School of Information, Renmin Un",0
"type=""bibr"" target=""#b10"">(Guo et al., 2019;</ref><ref type=""bibr"">Li et al., 2019)</ref>, biology <ref type=""bibr"" target=""#b8"">(Fout et al., 2017;</ref><ref type=""bibr"" target=""#b36"">Shang et al.,",0
"on-inspired network structure design <ref type=""bibr"" target=""#b46"">(Zhang &amp; Ghanem, 2018;</ref><ref type=""bibr"" target=""#b30"">Papyan et al., 2017)</ref>. The idea is that a feedforward neural net in the same spirit as the sparse coding problem, which has been used to design and to analyze CNNs <ref type=""bibr"" target=""#b30"">(Papyan et al., 2017)</ref>. Iterative shrinkage-thresholding algorit",0
"-supervised node classification task, we apply the standard fixed training/validation/testing split <ref type=""bibr"" target=""#b44"">(Yang et al., 2016)</ref> on three datasets Cora, Citeseer, and Pubme",0
"distances but still uses a two-layer model. <ref type=""bibr"" target=""#b9"">(Gao &amp; Ji, 2019;</ref><ref type=""bibr"" target=""#b20"">Lee et al., 2019)</ref> devote to extend pooling operations to graph",0
"i-c.org/ns/1.0""><head n=""1."">Introduction</head><p>Transformer models were originally introduced by <ref type=""bibr"" target=""#b36"">Vaswani et al. (2017)</ref> in the context of neural machine translat p><p>Initially, in § 3.1, we introduce a formulation for the transformer architecture introduced in <ref type=""bibr"" target=""#b36"">(Vaswani et al., 2017)</ref>. Subsequently, in § 3.2 and § 3.3 we pre the memory consumption with respect to the self attention layer. In all experiments, we use softmax <ref type=""bibr"" target=""#b36"">(Vaswani et al., 2017)</ref> to refer to the standard transformer arc",1
"t is limited. This disrupts temporal coherence and hinders the capturing of long-term dependencies. <ref type=""bibr"" target=""#b6"">Dai et al. (2019)</ref> addressed the latter by attending to memories rmers. Context refers to the maximum part of the sequence that is used for computing selfattention. <ref type=""bibr"" target=""#b6"">Dai et al. (2019)</ref> introduced Transformer-XL which achieves state",0
"d sparse factorizations of the attention matrix to reduce the selfattention complexity to O N √ N . <ref type=""bibr"" target=""#b13"">Kitaev et al. (2020)</ref> further reduced the complexity to O (N log e related to our model are the works of <ref type=""bibr"" target=""#b2"">Child et al. (2019)</ref> and <ref type=""bibr"" target=""#b13"">Kitaev et al. (2020)</ref>. The former <ref type=""bibr"" target=""#b2""> rall complexity from quadratic to O N √ N for generative modeling of long sequences. More recently, <ref type=""bibr"" target=""#b13"">Kitaev et al. (2020)</ref> proposed Reformer. This method further red mely, the transformers have to copy a series of symbols similar to the sequence duplication task of <ref type=""bibr"" target=""#b13"">Kitaev et al. (2020)</ref>. We use a sequence of maximum length 128 w r 250 epochs. For the reformer baseline, we use 1 and 4 hashing rounds. Furthermore, as suggested in<ref type=""bibr"" target=""#b13"">Kitaev et al. (2020)</ref>, we use 64 buckets and chunks with approxi compare our model with two baselines, the full transformer with softmax attention and the Reformer <ref type=""bibr"" target=""#b13"">(Kitaev et al., 2020)</ref>, the latter being a state-of-the-art acce andard transformer architecture, linear for our proposed linear transformers and lsh-X for Reformer <ref type=""bibr"" target=""#b13"">(Kitaev et al., 2020)</ref>, where X denotes the hashing rounds.</p><",0
"mp; Hinton, 2009)</ref>. Recent works <ref type=""bibr"" target=""#b1"">(Blanc &amp; Rendle, 2017;</ref><ref type=""bibr"" target=""#b28"">Rawat et al., 2019)</ref>, have approximated softmax with a linear do",0
"MI alone, and the choice of encoder and MI estimators have a significant impact on the performance <ref type=""bibr"" target=""#b52"">(Tschannen et al., 2020)</ref>.</p><p>Figure <ref type=""figure"">1</re",1
"s, Θ ∈ R dx×d h is</formula><p>network parameters, and σ is a parametric ReLU (PReLU) non-linearity <ref type=""bibr"" target=""#b17"">(He et al., 2015)</ref>. The learned representations are then fed int",0
"n node and graph representations have achieved state-of-the-art results on both node classification <ref type=""bibr"" target=""#b55"">(Veličković et al., 2019)</ref> and graph classification <ref type=""b proach achieves 86.8% accuracy, which is a 5.5% relative improvement over previous state-of-the-art <ref type=""bibr"" target=""#b55"">(Veličković et al., 2019)</ref>, and on Reddit-Binary graph classific the latent space by predicting the first-order neighbors. GAEs overemphasize proximity information <ref type=""bibr"" target=""#b55"">(Veličković et al., 2019)</ref> and suffer from unstructured predicti rent state-of-the-art in unsupervised node and graph classification tasks. Deep graph Infomax (DGI) <ref type=""bibr"" target=""#b55"">(Veličković et al., 2019)</ref> extends deep InfoMax <ref type=""bibr"" /formula><p>). To generate negative samples in transductive tasks, we randomly shuffle the features <ref type=""bibr"" target=""#b55"">(Veličković et al., 2019)</ref>. Finally, we optimize the model param ns. They are shown to over-emphasize proximity information at the expense of structural information <ref type=""bibr"" target=""#b55"">(Veličković et al., 2019;</ref><ref type=""bibr"" target=""#b43"">Ribeiro nd graph classification benchmarks using a unified approach and unlike previous unsupervised models <ref type=""bibr"" target=""#b55"">(Veličković et al., 2019;</ref><ref type=""bibr"" target=""#b49"">Sun et t al., 2019)</ref>.</p><p>Contrastive methods <ref type=""bibr"" target=""#b32"">(Li et al., 2019;</ref><ref type=""bibr"" target=""#b55"">Veličković et al., 2019;</ref><ref type=""bibr"" target=""#b49"">Sun et a",0
", 2003)</ref>, node2vec <ref type=""bibr"" target=""#b13"">(Grover &amp; Leskovec, 2016)</ref>, sub2vec <ref type=""bibr"" target=""#b0"">(Adhikari et al., 2018)</ref>, graph2vec <ref type=""bibr"" target=""#b36",0
"ph signal processing literature to measure the smoothness of a signal defined over nodes of a graph <ref type=""bibr"" target=""#b4"">(Chen et al., 2015)</ref>. More specifically, given a graph with the a",1
"pable of enforcing sparsity in random masks <ref type=""bibr"" target=""#b41"">(Zhou et al., 2009;</ref><ref type=""bibr"" target=""#b13"">Hajiramezanali et al., 2018)</ref>, which has been shown to be necess",0
"uring training. In GNNs, DropOut is realized by randomly removing the node features during training <ref type=""bibr"" target=""#b35"">(Rong et al., 2019)</ref>. Often, the procedure is independent of the domly removed from the graph, with DropOut has recently shown potential to alleviate these problems <ref type=""bibr"" target=""#b35"">(Rong et al., 2019)</ref>.</p><p>On the other hand, with the developm techniques such as DropOut <ref type=""bibr"" target=""#b36"">(Srivastava et al., 2014)</ref>, DropEdge <ref type=""bibr"" target=""#b35"">(Rong et al., 2019)</ref>, and node sampling <ref type=""bibr"" target= niques, such as DropOut <ref type=""bibr"" target=""#b36"">(Srivastava et al., 2014)</ref> and DropEdge <ref type=""bibr"" target=""#b35"">(Rong et al., 2019)</ref>, have been used to prevent over-fitting and (1), (2), and (3), DropOut <ref type=""bibr"" target=""#b36"">(Srivastava et al., 2014)</ref>, DropEdge <ref type=""bibr"" target=""#b35"">(Rong et al., 2019)</ref>, and Node Sampling <ref type=""bibr"" target= he ones adopted in DropOut <ref type=""bibr"" target=""#b36"">(Srivastava et al., 2014)</ref>, DropEdge <ref type=""bibr"" target=""#b35"">(Rong et al., 2019)</ref>, and Node Sampling <ref type=""bibr"" target= 18)</ref>, which has been shown to be necessary for regularizing deep GNNs as discussed in DropEdge <ref type=""bibr"" target=""#b35"">(Rong et al., 2019)</ref>.</p><p>With this hierarchical beta-Bernoull yer is depicted in 3. The number of input and output features are both two in this toy example.     <ref type=""bibr"" target=""#b35"">(Rong et al., 2019)</ref>. Each circle is a feature and each square r s/1.0"" xml:id=""fig_5""><head>Figure 6 .</head><label>6</label><figDesc>Figure6. Schematic of DropEdge<ref type=""bibr"" target=""#b35"">(Rong et al., 2019)</ref>. Each circle is a feature and each square r f limited performance from deep GNNs <ref type=""bibr"" target=""#b24"">(Kipf &amp; Welling, 2017;</ref><ref type=""bibr"" target=""#b35"">Rong et al., 2019)</ref>. Several stochastic regularization and reduc",0
"><head>B. Datasets and Implementation Details</head><p>All of the models are implemented in PyTorch <ref type=""bibr"" target=""#b33"">(Paszke et al., 2017)</ref>. All of the simulations are conducted on",0
"en used in BNNs <ref type=""bibr"" target=""#b1"">(Boluki et al., 2020)</ref> and information retrieval <ref type=""bibr"" target=""#b6"">(Dadaneh et al., 2020b;</ref><ref type=""bibr"">a)</ref>. In the next se",0
"l language understanding and generation tasks <ref type=""bibr"" target=""#b9"">(Dai et al., 2019;</ref><ref type=""bibr"" target=""#b38"">Shaw et al., 2018)</ref>. The proposed Disentangled Attention mechani tent, and position-to-position<ref type=""foot"" target=""#foot_0"">1</ref> .</p><p>Existing approaches <ref type=""bibr"" target=""#b38"">(Shaw et al., 2018;</ref><ref type=""bibr"" target=""#b18"">Huang et al., LEMENTATION</head><p>For an input sequence of length N , it requires a space complexity of OpN 2 dq <ref type=""bibr"" target=""#b38"">(Shaw et al., 2018;</ref><ref type=""bibr"" target=""#b18"">Huang et al.,",1
",</ref> </note> 			<note xmlns=""http://www.tei-c.org/ns/1.0"" place=""foot"" n=""2019"" xml:id=""foot_1"">;<ref type=""bibr"" target=""#b5"">Chen et al., 2019)</ref> where a word is represented using a tensor pr",0
"et al., 2020)</ref>, StructBERT <ref type=""bibr"" target=""#b46"">(Wang et al., 2019)</ref> and ERINE <ref type=""bibr"" target=""#b42"">(Sun et al., 2019)</ref> . These PLMs have been fine-tuned using task",0
"entangled attention thoroughly in both settings. To enable the autoregressive generation, we follow <ref type=""bibr"" target=""#b13"">(Dong et al., 2019b</ref>) by using a triangular matrix for self-atte",0
"use Wikipedia (English Wikipedia dump<ref type=""foot"" target=""#foot_2"">2</ref> ; 12GB), BookCorpus <ref type=""bibr"" target=""#b52"">(Zhu et al., 2015)</ref>  We evaluate DeBERTa on additional benchmark NING DATASETFor DeBERTa pre-training, we use Wikipedia (English Wikipedia dump 6 ; 12GB), BookCorpus<ref type=""bibr"" target=""#b52"">(Zhu et al., 2015)</ref> 7 (6GB), OPENWEBTEXT (public Reddit content<",0
"creasing with the increase of training iterations. Inspired by the dynamic gradient descent methods <ref type=""bibr"" target=""#b21"">[22]</ref>, we replace the fixed threshold with a dynamic threshold f to CDAE, the hidden size of MLP is set as 200. In addition, the batch size is always 1,024 and Adam <ref type=""bibr"" target=""#b21"">[22]</ref> is applied to optimize all the parameters with the learnin",1
"usted so that it can fit different models and datasets.</p><p>Inspired by the success of Focal Loss <ref type=""bibr"" target=""#b29"">[30]</ref>, we estimate 𝜔 (𝑢, 𝑖) with a function of 𝑓 ( ŷ𝑢𝑖 ) that ta",1
"such as caption bias <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b32"">33]</ref> and position bias <ref type=""bibr"" target=""#b18"">[19]</ref> ive interactions by: 1) negative experience identification <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b32"">33]</ref> (illustrated in Figure <ref type=""figure"" target=""#fig_1"">1 bibr"" target=""#b6"">[7]</ref>) and the item characteristics <ref type=""bibr"" target=""#b31"">[32,</ref><ref type=""bibr"" target=""#b32"">33]</ref> to predict the user's satisfaction. Lu et al. <ref type=""bi manually feature design and the labeling of domain experts <ref type=""bibr"" target=""#b31"">[32,</ref><ref type=""bibr"" target=""#b32"">33]</ref>. The unaffordable labor cost hinders the practical usage of "">33]</ref> and position bias <ref type=""bibr"" target=""#b18"">[19]</ref>. Moreover, existing studies <ref type=""bibr"" target=""#b32"">[33,</ref><ref type=""bibr"" target=""#b38"">39]</ref> have demonstrated reliable implicit feedback into the task of denoising, despite their success in a few applications <ref type=""bibr"" target=""#b32"">[33,</ref><ref type=""bibr"" target=""#b38"">39]</ref>. This is because s r reliable implicit feedback into the task of denoising, despite their success in a few applications<ref type=""bibr"" target=""#b32"">[33,</ref><ref type=""bibr"" target=""#b38"">39]</ref>. This is because s g., dwell time and gaze pattern) and auxiliary item features (e.g., length of the item description) <ref type=""bibr"" target=""#b32"">[33]</ref>. The latter incorporates extra feedback (e.g., favorite an higher risk on leading to further false-positive interactions, which would hurt the user experience <ref type=""bibr"" target=""#b32"">[33]</ref>. Despite the success of clean training in the pilot study,",0
">1(b)</ref>); and 2) the incorporation of various feedback <ref type=""bibr"" target=""#b40"">[41,</ref><ref type=""bibr"" target=""#b42"">43]</ref> (shown in Figure <ref type=""figure"" target=""#fig_1"">1(c)</r dwell time &lt; 10s are thought of as false-positive ones <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b42"">43]</ref>. • Amazon-book: It is from the Amazon-review datasets<ref t ., favorite and skip) into recommender training to prune the effects of false-positive interactions <ref type=""bibr"" target=""#b42"">[43]</ref>. A key limitation with these methods is that they require ref type=""bibr"" target=""#b43"">44]</ref> also consider incorporating more feedback (e.g., dwell time <ref type=""bibr"" target=""#b42"">[43]</ref>, skip <ref type=""bibr"" target=""#b26"">[27,</ref><ref type=""",0
"ef>. • Amazon-book: It is from the Amazon-review datasets<ref type=""foot"" target=""#foot_3"">3</ref>  <ref type=""bibr"" target=""#b13"">[14]</ref>. It covers users' purchases over books with rating scores.",0
">, NeuMF <ref type=""bibr"" target=""#b15"">[16]</ref>, and Collaborative Denoising Auto-Encoder (CDAE) <ref type=""bibr"" target=""#b39"">[40]</ref>. The results show significant performance improvements of ype=""bibr"" target=""#b39"">40]</ref> introduce the denoising techniques. These approaches (e.g., CDAE <ref type=""bibr"" target=""#b39"">[40]</ref>) first corrupt the interactions of user by random noises, CF models, GMF and NeuMF <ref type=""bibr"" target=""#b15"">[16]</ref>, and one item-based model, CDAE <ref type=""bibr"" target=""#b39"">[40]</ref>. Note that CDAE is also a representative model of robust r ationship between users and items by combining GMF and a Multi-Layer Perceptron (MLP).</p><p>• CDAE <ref type=""bibr"" target=""#b39"">[40]</ref>: CDAE corrupts the interactions with random noises, and th coder based models <ref type=""bibr"" target=""#b27"">[28,</ref><ref type=""bibr"" target=""#b36"">37,</ref><ref type=""bibr"" target=""#b39"">40]</ref> introduce the denoising techniques. These approaches (e.g., target=""#b22"">[23]</ref> due to their inferior performance <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b39"">40]</ref>.</p><p>Parameter Settings. For the three testing recommende",0
"plicit feedback although explicit feedback also exists in each interaction. We followed former work <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" ta",0
"different users and items. Prior study on robust learning <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b19"">20]</ref> and curriculum learning <ref type=""bibr"" target=""#b1"">[2]</ tional machine learning tasks such as image classification <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b19"">20]</ref>. However, little attention has been paid to such effect on are harder to fit in the early stages. In robust learning <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b19"">20]</ref> and curriculum learning <ref type=""bibr"" target=""#b1"">[2]</ ht be whether it is reliable. Actually, many existing work <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b19"">20]</ref> has pointed out the connection between the large loss and n",0
"they are usually sparse. Existing work either adopts the additional feedback by multi-task learning <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b9"">10]</ref>, or leverages it to i",0
"dwell time <ref type=""bibr"" target=""#b42"">[43]</ref>, skip <ref type=""bibr"" target=""#b26"">[27,</ref><ref type=""bibr"" target=""#b44"">45]</ref>, and adding to favorites) into training directly. For insta",0
"ss in entity type and new fact predictions <ref type=""bibr"" target=""#b16"">[Nickel et al., 2012</ref><ref type=""bibr"" target=""#b23"">, Trouillon et al., 2016</ref><ref type=""bibr"" target=""#b3"">, Dettmer bedding methods,viz., ConvE <ref type=""bibr"" target=""#b3"">[Dettmers et al., 2018]</ref> and ComplEx <ref type=""bibr"" target=""#b23"">[Trouillon et al., 2016]</ref>, which do not make use of any ontologi raining can be done for the refinement task with a negative log-likelihood loss function as follows <ref type=""bibr"" target=""#b23"">[Trouillon et al., 2016]</ref>.</p><formula xml:id=""formula_0"">L(G) = ods can also be used to predict type labels of entities (the typeOf relation). We work with ComplEx <ref type=""bibr"" target=""#b23"">[Trouillon et al., 2016]</ref> and ConvE <ref type=""bibr"" target=""#b3 ., subtype and subproperty information-and also shows that state-of-the-art embeddings like ComplEx <ref type=""bibr"" target=""#b23"">[Trouillon et al., 2016]</ref>, <ref type=""bibr"">SimplE [Kazemi and P valuate the performance of TypeE-X models in the KG refinement task, and compare them with Com-plEx <ref type=""bibr"" target=""#b23"">[Trouillon et al., 2016]</ref> and ConvE <ref type=""bibr"" target=""#b3 "" target=""#b15"">[Nickel et al., 2011</ref><ref type=""bibr"" target=""#b21"">, Socher et al., 2013</ref><ref type=""bibr"" target=""#b23"">, Trouillon et al., 2016]</ref>.</p><p>An important step in learning",1
"ort, baseball is false since Matt Flynn is an NFL player), incompatible entity types, and many more <ref type=""bibr"" target=""#b18"">[Pujara et al., 2013]</ref>. It has also been observed that such nois fectively, and specifically, the PSL-KGI implementation uses rules defined on schema-level features <ref type=""bibr"" target=""#b18"">[Pujara et al., 2013]</ref>.</p></div> <div xmlns=""http://www.tei-c.o exclusive (MUT and RMUT); and inverse relations (INV). We reproduce the list of information used in <ref type=""bibr"" target=""#b18"">[Pujara et al., 2013]</ref> in tabular form in Table <ref type=""table mal distributions: N (0.7, 0.2) for facts in the original KG and N (0.3, 0.2) for added noisy facts <ref type=""bibr"" target=""#b18"">[Pujara et al., 2013]</ref>. The SAMEENT facts between entities are g hyper-parameter threshold as the cutoff for classifying a test triple based on the prediction score <ref type=""bibr"" target=""#b18"">[Pujara et al., 2013]</ref>. Our experiments were run on Intel(R) Xeo the KG refinement task and methods for the same, from probabilistic rule based methods like PSL-KGI <ref type=""bibr"" target=""#b18"">[Pujara et al., 2013]</ref> to KG embedding methods like type-ComplEx e the probabilistic sources of information such as the confidence scores obtained during extraction <ref type=""bibr"" target=""#b18"">[Pujara et al., 2013</ref><ref type=""bibr"" target=""#b8"">, Jiang et al pe=""bibr"" target=""#b8"">, Jiang et al., 2012]</ref> from multiple sources. Of these methods, PSL-KGI <ref type=""bibr"" target=""#b18"">[Pujara et al., 2013</ref><ref type=""bibr"" target=""#b19"">[Pujara et a ref type=""bibr"" target=""#b1"">[Carlson et al., 2010]</ref>) has been used for the KG refinement task <ref type=""bibr"" target=""#b18"">[Pujara et al., 2013</ref><ref type=""bibr"" target=""#b8"">, Jiang et al",1
"arget=""#b16"">[Nickel et al., 2012</ref><ref type=""bibr"" target=""#b23"">, Trouillon et al., 2016</ref><ref type=""bibr"" target=""#b3"">, Dettmers et al., 2018]</ref>. It is worth noting that embeddings, wi porate inference rules and ontologies, along with state-of-the-art KG embedding methods,viz., ConvE <ref type=""bibr"" target=""#b3"">[Dettmers et al., 2018]</ref> and ComplEx <ref type=""bibr"" target=""#b2 tion). We work with ComplEx <ref type=""bibr"" target=""#b23"">[Trouillon et al., 2016]</ref> and ConvE <ref type=""bibr"" target=""#b3"">[Dettmers et al., 2018]</ref> embeddings which have shown state of the #b23"">[Trouillon et al., 2016]</ref>, <ref type=""bibr"">SimplE [Kazemi and Poole, 2018]</ref>, ConvE <ref type=""bibr"" target=""#b3"">[Dettmers et al., 2018]</ref> cannot enforce subsumption. Taking a dif Then all facts upto length 3 in the hierarchy of taxonomy were included.</p><p>FB15K-237: FB15K-237 <ref type=""bibr"" target=""#b3"">[Dettmers et al., 2018]</ref>, another popular benchmark does not have compare them with Com-plEx <ref type=""bibr"" target=""#b23"">[Trouillon et al., 2016]</ref> and ConvE <ref type=""bibr"" target=""#b3"">[Dettmers et al., 2018]</ref>, two state-of-the-art KG embeddings meth g the same class balance, and use them as our validation and test split.</p><p>YAGO3-10: YAGO3-10 [ <ref type=""bibr"" target=""#b3"">Dettmers et al., 2018]</ref> is a subset of the YAGO3 <ref type=""bibr""",1
"the entities due to their typeagnostic nature <ref type=""bibr"" target=""#b25"">[Xie et al., 2016</ref><ref type=""bibr"" target=""#b7"">, Jain et al., 2018]</ref>. Since PSL-KGI is able to predict entity ty sion.</p><p>(ii) Explicit type supervised models also outperform the implict type supervised models <ref type=""bibr"" target=""#b7"">[Jain et al., 2018]</ref>.</p><p>The margin of improvement is large wh on the use of ontological rules (exemplified by PSL-KGI) and embeddings (we use ComplEx, ConvE and <ref type=""bibr"" target=""#b7"">[Jain et al., 2018]</ref>). Rule induction methods are orthogonal to o target=""#b3"">[Dettmers et al., 2018]</ref> cannot enforce subsumption. Taking a different approach <ref type=""bibr"" target=""#b7"">[Jain et al., 2018]</ref> propose extending standard KG embeddings wit es for entities generated by PSL-KGI in KG embeddings (the second stage), we modify the typed model <ref type=""bibr"" target=""#b7"">[Jain et al., 2018]</ref> as follows:</p><p>Instead of just using the are our explicitly supervised TypeE-X methods with the implicitly supervised embeddings proposed by <ref type=""bibr"" target=""#b7"">[Jain et al., 2018]</ref>.</p><p>• In Section 6.3, we analyse how our e supervision with the unsupervised type-compatible embeddings-based method proposed by Jain et al. <ref type=""bibr"" target=""#b7"">[Jain et al., 2018]</ref>. As these results indicate, while explicitly nificantly improves the relation scores, improving weighted F1 up to 18% (over NELL).</p><p>Dataset <ref type=""bibr"" target=""#b7"">[Jain et al., 2018]</ref>  </p></div> <div xmlns=""http://www.tei-c.org ref type=""bibr"" target=""#b18"">[Pujara et al., 2013]</ref> to KG embedding methods like type-ComplEx <ref type=""bibr"" target=""#b7"">[Jain et al., 2018]</ref>. We showed their performance on existing dat d>Table 7 :</head><label>7</label><figDesc>Weighted F1 scores on relation triples in the test set by<ref type=""bibr"" target=""#b7"">[Jain et al., 2018]</ref> and TypeE-ComplEx.Anecdotes. Looking at the",1
"ns=""http://www.tei-c.org/ns/1.0""><head>Class</head><p>Ontological Rule</p><p>Uncertain Extractions  <ref type=""bibr"" target=""#b0"">[Blum and Mitchell, 1998]</ref>, to combine the strengths of PSL-KGI a",1
"ciation rule mining over the noisy KG to induce rules which can help in eliminating incorrect facts <ref type=""bibr"" target=""#b11"">[Ma et al., 2014]</ref>; reconciling diverse evidence from multiple e other research directions for (partially) solving the KG refinement problem such as rule induction <ref type=""bibr"" target=""#b11"">[Ma et al., 2014]</ref>, classification with diverse extractors <ref",0
"t al., 2011</ref>] and many more. A detailed survey of approaches for KG refinement is available in <ref type=""bibr"" target=""#b17"">[Paulheim, 2017]</ref>. On the other hand, neural and tensor-based em verse extractors <ref type=""bibr"" target=""#b4"">[Dong et al., 2014]</ref>, crowdsourcing, etc., (see <ref type=""bibr"" target=""#b17"">[Paulheim, 2017]</ref> for an overview). While these works have their",0
"e presence of noise from the automatic extraction methods used to populate them. For instance, NELL <ref type=""bibr"" target=""#b1"">[Carlson et al., 2010]</ref> is known to contain various kinds of erro e part of the original benchmark test collection.   The NELL subset taken from its 165 th iteration <ref type=""bibr"" target=""#b1"">[Carlson et al., 2010]</ref>) has been used for the KG refinement task",0
"nfidence scores obtained during extraction <ref type=""bibr"" target=""#b18"">[Pujara et al., 2013</ref><ref type=""bibr"" target=""#b8"">, Jiang et al., 2012]</ref> from multiple sources. Of these methods, P ) has been used for the KG refinement task <ref type=""bibr"" target=""#b18"">[Pujara et al., 2013</ref><ref type=""bibr"" target=""#b8"">, Jiang et al., 2012]</ref>. It comes with a rich ontology from the NE onstraints that were first introduced in the earlier work on Markov Logic Networks (MLN) based work <ref type=""bibr"" target=""#b8"">[Jiang et al., 2012]</ref>. These rules are listed in Appendix A in Ta",0
"triples that are already present in the KG <ref type=""bibr"" target=""#b15"">[Nickel et al., 2011</ref><ref type=""bibr"" target=""#b21"">, Socher et al., 2013</ref><ref type=""bibr"" target=""#b23"">, Trouillon",0
"ion in KG embeddings -e.g., TKRL <ref type=""bibr"" target=""#b25"">[Xie et al., 2016]</ref> and TransC <ref type=""bibr"" target=""#b10"">[Lv et al., 2018]</ref>. Recently, SimplE + <ref type=""bibr"" target=""",0
"n contrast to the two stages with iterations of our method). We use the setting introduced in R-GCN <ref type=""bibr"" target=""#b20"">(Schlichtkrull et al. [2018]</ref>) to combine scores of KG embedding",0
"work in modeling structural as well as uncertainty information of relations in the embedding space. <ref type=""bibr"" target=""#b2"">[Chen et al., 2019]</ref> uses Probabilistic Soft Logic to come up wit",0
"3"">, Dettmers et al., 2018]</ref>. It is worth noting that embeddings, with a few recent exceptions <ref type=""bibr"" target=""#b6"">[Guo et al., 2016</ref><ref type=""bibr"" target=""#b12"">, Minervini et a",0
"""bibr"" target=""#b11"">[Ma et al., 2014]</ref>; reconciling diverse evidence from multiple extractors <ref type=""bibr"" target=""#b4"">[Dong et al., 2014]</ref>; the use of ontology reasoners <ref type=""bi tion <ref type=""bibr"" target=""#b11"">[Ma et al., 2014]</ref>, classification with diverse extractors <ref type=""bibr"" target=""#b4"">[Dong et al., 2014]</ref>, crowdsourcing, etc., (see <ref type=""bibr"" ent or replace the set of rules we use. Further, evidence from diverse extractors as in the case of <ref type=""bibr"" target=""#b4"">[Dong et al., 2014]</ref> can be incorporated into the PSL-KGI framewo",0
"ngs in such a way as to maximise the plausibility of the triples that are already present in the KG <ref type=""bibr"" target=""#b15"">[Nickel et al., 2011</ref><ref type=""bibr"" target=""#b21"">, Socher et",0
"2016</ref><ref type=""bibr"" target=""#b12"">, Minervini et al., 2017</ref><ref type=""bibr"">, 2018</ref><ref type=""bibr"" target=""#b5"">, Fatemi et al., 2019]</ref>, do not make use of rich taxonomic/ontolo , 2016]</ref> and TransC <ref type=""bibr"" target=""#b10"">[Lv et al., 2018]</ref>. Recently, SimplE + <ref type=""bibr"" target=""#b5"">[Fatemi et al., 2019]</ref> includes taxonomic information -i.e., subt",0
"en some research in using rule-based reasoning and KG embeddings together in an iterative manner in <ref type=""bibr"" target=""#b26"">[Zhang et al., 2019]</ref>.</p><p>They achieve improvements in the pe",0
"e extractors <ref type=""bibr"" target=""#b4"">[Dong et al., 2014]</ref>; the use of ontology reasoners <ref type=""bibr"" target=""#b14"">[Nakashole et al., 2011</ref>] and many more. A detailed survey of ap",0
"e often due to the lack of type compatibility between the entities due to their typeagnostic nature <ref type=""bibr"" target=""#b25"">[Xie et al., 2016</ref><ref type=""bibr"" target=""#b7"">, Jain et al., 2 here are some recent efforts to incorporate type hierarchy information in KG embeddings -e.g., TKRL <ref type=""bibr"" target=""#b25"">[Xie et al., 2016]</ref> and TransC <ref type=""bibr"" target=""#b10"">[L ot have ontological and type label information. Therefore, we use the type labels for entities from <ref type=""bibr"" target=""#b25"">[Xie et al., 2016]</ref> which also provides the domain and range inf",0
"-10: YAGO3-10 [ <ref type=""bibr"" target=""#b3"">Dettmers et al., 2018]</ref> is a subset of the YAGO3 <ref type=""bibr"" target=""#b22"">[Suchanek et al., 2007]</ref> knowledge graph. It is often used for e",0
"t has also been observed that such noise can significantly degrade the performance of KG embeddings <ref type=""bibr"" target=""#b19"">[Pujara et al., 2017]</ref>.</p><p>The KG refinement task aims to red r subject, relation label or object. Note that this was the same model followed in an earlier study <ref type=""bibr"" target=""#b19"">[Pujara et al., 2017]</ref>.</p><p>• We further refine the noise mode ultiple sources. Of these methods, PSL-KGI <ref type=""bibr"" target=""#b18"">[Pujara et al., 2013</ref><ref type=""bibr"" target=""#b19"">[Pujara et al., , 2017] ]</ref> is shown not only to perform better w",0
"that embeddings, with a few recent exceptions <ref type=""bibr"" target=""#b6"">[Guo et al., 2016</ref><ref type=""bibr"" target=""#b12"">, Minervini et al., 2017</ref><ref type=""bibr"">, 2018</ref><ref type=",0
"body> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""1"">Introduction</head><p>Transformer models <ref type=""bibr"" target=""#b28"">(Vaswani et al., 2017)</ref> have become ubiquitous for wide variety //www.tei-c.org/ns/1.0""><head>Model Architecture</head><p>Complexity per Layer Sequential Operation <ref type=""bibr"" target=""#b28"">(Vaswani et al., 2017)</ref> O(n 2 ) O(1) Sparse Tansformer, <ref typ",1
"sformer when sequence length is extremely long.</p><p>Improving Optimizer Efficiency: Microbatching <ref type=""bibr"" target=""#b10"">(Huang et al., 2019)</ref> splits a batch into small microbatches (wh",0
"bibr"" target=""#b15"">(Liu et al., 2019)</ref> on two tasks: masked-language-modeling task on Wiki103 <ref type=""bibr"" target=""#b17"">(Merity et al., 2016)</ref> and classification task on IMDB <ref type",0
"""bibr"" target=""#b5"">(Child et al., 2019;</ref><ref type=""bibr"" target=""#b22"">Qiu et al., 2019;</ref><ref type=""bibr"" target=""#b1"">Beltagy et al., 2020)</ref> by having each token attend to only a subs",0
"to the standard Transformer model. We then finetune our pretrained models on three tasks from GLUE <ref type=""bibr"" target=""#b29"">(Wang et al., 2018)</ref> and one sentiment analysis task, IMDB revie",0
"ly work mostly deals with simple graphs with unlabeled edges, recently proposed relation-aware GNNs <ref type=""bibr"" target=""#b37"">[38,</ref><ref type=""bibr"" target=""#b38"">39]</ref> consider multi-rel pe=""bibr"" target=""#b38"">[39]</ref> consider direction and relation types, respectively. Also, R-GCN <ref type=""bibr"" target=""#b37"">[38]</ref> considers direction and relation types simultaneously. Rec e=""bibr"" target=""#b44"">45]</ref>. 5) R-GCN. This is a GNN-based method for modeling relational data <ref type=""bibr"" target=""#b37"">[38]</ref>. 6) MEAN. 7) LAN. These are GNN models for a out-of-knowle get=""#b14"">[15]</ref>. 3) R-GCN. The same model used in the entity prediction on KG completion task <ref type=""bibr"" target=""#b37"">[38]</ref>. 4) I-GEN. Inductive GEN, which only uses feature represen nds the graph convolutional network to consider multi-relational structure, by Schlichtkrull et al. <ref type=""bibr"" target=""#b37"">[38]</ref>.</p><p>6) MEAN. This model computes the embedding of entit s W r and W r to prevent the excessive increase in the model size, proposed in Schlichtkrull et al. <ref type=""bibr"" target=""#b37"">[38]</ref>: W r = B b=1 a r b V b , where B is the number of basis, a twork based methods <ref type=""bibr"" target=""#b30"">[31,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b37"">38,</ref><ref type=""bibr"" target=""#b29"">30]</ref>. While they require ggested by several recent works on multi-relational graphs <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b37"">38,</ref><ref type=""bibr"" target=""#b45"">46]</ref>, where directed rel ne in previous works <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b30"">31,</ref><ref type=""bibr"" target=""#b37"">38]</ref>, we measure the ranks in a filtered setting where we do not",1
"training nodes. To mention a few, metric-based approaches <ref type=""bibr"" target=""#b47"">[48,</ref><ref type=""bibr"" target=""#b40"">41]</ref> learn a shared metric space to minimize the distance betwee",0
"ions between drugs. Recently, Zitnik et al. <ref type=""bibr"" target=""#b59"">[60]</ref> and Ma et al. <ref type=""bibr"" target=""#b23"">[24]</ref> propose end-to-end GNNs to tackle this problem, which demo constructing In-Graph.</p><p>4) BIOSNAP-sub. This dataset <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b23"">24]</ref> consists of 637 entities, which is used for the drug-todrug tion types are used as labels. 2) BIOSNAP-sub. This dataset<ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b23"">24]</ref> consists of 645 drugs (entities) and 46,221 drug-drug pairs",0
"NNs to tackle this problem, which demonstrate comparatively better performance over non-GNN methods <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b33"">34,</ref><ref type=""bibr"" targ",0
"rotein structure <ref type=""bibr"" target=""#b12"">[13]</ref>, and drug-to-drug interaction prediction <ref type=""bibr"" target=""#b59"">[60]</ref>.</p><p>Among multi-relational graphs, Knowledge Graphs (KG ational graphs, where the problem is to predict interactions between drugs. Recently, Zitnik et al. <ref type=""bibr"" target=""#b59"">[60]</ref> and Ma et al. <ref type=""bibr"" target=""#b23"">[24]</ref> pr",0
"c. We validated the performance statistics of MAESTRO against cycle-accurate RTL simulation results <ref type=""bibr"" target=""#b4"">5</ref> and reported performance in a previous work <ref type=""bibr"" t 's performance model against RTL simulation and reported processing delay of two accelerators-MAERI <ref type=""bibr"" target=""#b4"">5</ref> and Eyeriss 6 when running VGG16 and AlexNet, respectively. Th ch an optimization opportunity can be exploited by flexible accelerators like Flexflow 15 and MAERI <ref type=""bibr"" target=""#b4"">5</ref> or via heterogeneous accelerators that employ multiple subacce",1
"by maximizing data reuse within PEs and on-chip scratchpads. <ref type=""bibr"" target=""#b0"">[1]</ref><ref type=""bibr"" target=""#b1"">[2]</ref><ref type=""bibr"" target=""#b2"">[3]</ref><ref type=""bibr"" targe",0
"usal model which could measure this causal relationship i.e. Linear Structual Equation Models (SEM) <ref type=""bibr"" target=""#b20"">(Shimizu et al., 2006)</ref>. Existing methods for disentangled repre et=""#b6"">(Hoyer et al., 2009;</ref><ref type=""bibr"" target=""#b24"">Zhang &amp; Hyvarinen, 2012;</ref><ref type=""bibr"" target=""#b20"">Shimizu et al., 2006)</ref>. <ref type=""bibr"" target=""#b19"">Pearl (20 2009)</ref> introduce a probabilistic graphical model based framework to learn causality from data. <ref type=""bibr"" target=""#b20"">Shimizu et al. (2006)</ref> proposed an effective method called LiNGA",1
"f methods consider minimizing the mutual information between different latent factors. For example, <ref type=""bibr"" target=""#b4"">Higgins et al. (2017)</ref>; <ref type=""bibr"" target=""#b1"">Burgess et >(Shimizu et al., 2006)</ref>. Existing methods for disentangled representation learning like β-VAE <ref type=""bibr"" target=""#b4"">(Higgins et al., 2017)</ref> might not work as they forces the learned pendence constraints on the original loss function, leading to various disentangling metrics. β-VAE <ref type=""bibr"" target=""#b4"">(Higgins et al., 2017)</ref> proposes an adaptation framework which ad",0
"et=""#b17"">(Mathieu et al., 2018;</ref><ref type=""bibr"" target=""#b14"">Locatello et al., 2018)</ref>. <ref type=""bibr"" target=""#b13"">Kulkarni et al. (2015)</ref>; <ref type=""bibr"" target=""#b15"">Locatell",0
"pe=""bibr"" target=""#b8"">(Hsu et al., 2017;</ref><ref type=""bibr"" target=""#b16"">Ma et al., 2019;</ref><ref type=""bibr"" target=""#b7"">Hsieh et al., 2018)</ref>. The reason is that it would help enhancing",0
"dustry practitioners have turned to search-based compilation <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b27"">29,</ref><ref type=""bibr"" targ nually-written assembly code on large matrix multiplications <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b43"">45]</ref>, as the code has bee tation definition of matrix multiplication. ple compiler techniques have been introduced (e.g., TVM <ref type=""bibr"" target=""#b9"">[10]</ref>, Halide <ref type=""bibr"" target=""#b36"">[38]</ref>, Tensor C ically, the compiler partitions the large computational graph of a DNN into several small subgraphs <ref type=""bibr"" target=""#b9"">[10]</ref>. This partition has a negligible effect on the performance arch and learned cost model performs the best among them, which is also used in our evaluation. TVM <ref type=""bibr"" target=""#b9"">[10]</ref> utilizes a similar scheduling language and includes a templ search for GPU code automatically, but it is not yet meant to be used for compute-bounded problems <ref type=""bibr"" target=""#b9"">[10]</ref>. It cannot outperform TVM on operators like conv2d and matm f type=""bibr"" target=""#b9"">[10]</ref>. It cannot outperform TVM on operators like conv2d and matmul <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b42"">44]</ref>. This is because of graph level include layout optimizations <ref type=""bibr"" target=""#b27"">[29]</ref>, operator fusion <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b33"">35]</ref>, constant folding <",1
"a critical role in autonomous driving <ref type=""bibr"" target=""#b12"">[13]</ref>, augmented reality <ref type=""bibr"" target=""#b2"">[3]</ref>, language translation <ref type=""bibr"" target=""#b13"">[14]</r",0
"er <ref type=""bibr"" target=""#b16"">[17]</ref> and AutoPhase <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b21"">23]</ref> use deep reinforcement learning to automatically vectorize",0
"=""#b12"">[13]</ref>, augmented reality <ref type=""bibr"" target=""#b2"">[3]</ref>, language translation <ref type=""bibr"" target=""#b13"">[14]</ref>, and other applications of AI. DNNs can be expressed as a cognition, DCGAN <ref type=""bibr"" target=""#b35"">[37]</ref> generator for image generation, and BERT <ref type=""bibr"" target=""#b13"">[14]</ref> for language understanding. We benchmark these DNNs on thr",0
"ruction into a fixed sequence of decisions. The compiler then uses an algorithm such as beam search <ref type=""bibr"" target=""#b29"">[31]</ref> to search for good decisions (e.g., Halide auto-scheduler",0
"methods use fixed target networks, BYOL uses a weighted moving average of previous networks (as in <ref type=""bibr"" target=""#b53"">[54]</ref>) in order to provide smoother changes in the target repres e online network, and its parameters ξ are an exponential moving average of the online parameters θ <ref type=""bibr"" target=""#b53"">[54]</ref>. More precisely, given a target decay rate τ ∈ [0, 1], aft",1
"text tasks. Among them, state-of-the-art contrastive methods <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target sizes <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b11"">12]</ref>, memory banks <ref type=""bibr"" target=""#b8"">[9]</ref> or customized mining strategies <ref type=""bibr"" target=""#b1 op of the online network, which prevents collapse.</p><p>Finally, in self-supervised learning, MoCo <ref type=""bibr"" target=""#b8"">[9]</ref> uses a slow-moving average network (momentum encoder) to mai an optimizer and η is a learning rate. At the end of training, we only keep the encoder f θ ; as in <ref type=""bibr"" target=""#b8"">[9]</ref>. When comparing to other methods, we consider the number of nd SimCLR (+1.1 mIoU).</p><p>Similarly, we evaluate on object detection by reproducing the setup in <ref type=""bibr"" target=""#b8"">[9]</ref> using a Faster R-CNN architecture <ref type=""bibr"" target=""# y improves performance (+1.6 points). This sheds new light on the use of the target network in MoCo <ref type=""bibr"" target=""#b8"">[9]</ref>, where the target network is used to provide more negative e t be necessary for representation learning.</p><p>Among discriminative methods, contrastive methods <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" targe ef>. Contrastive methods often require comparing each example with many other examples to work well <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b7"">8]</ref> prompting the question",1
">12]</ref>.</p><p>Our approach has some similarities with Predictions of Bootstrapped Latents (PBL, <ref type=""bibr"" target=""#b48"">[49]</ref>), a selfsupervised representation learning technique for r",1
"r"" target=""#b44"">[45]</ref>, and geometric transformations <ref type=""bibr"" target=""#b45"">[46,</ref><ref type=""bibr"" target=""#b46"">47]</ref> have been shown to be useful. Yet, even with suitable archi",0
"d python pseudo-code based on the libraries JAX <ref type=""bibr"" target=""#b63"">[64]</ref> and Haiku <ref type=""bibr"" target=""#b64"">[65]</ref> is provided in in Appendix J.</p></div> <div xmlns=""http:/",0
"et=""#b49"">[50,</ref><ref type=""bibr"" target=""#b50"">51,</ref><ref type=""bibr"" target=""#b51"">52,</ref><ref type=""bibr"" target=""#b52"">53]</ref>. Target networks stabilize the bootstrapping updates provid",0
"target=""#b7"">[8]</ref>, 76.5%, but is still significantly below the stronger supervised baseline of <ref type=""bibr"" target=""#b74"">[75]</ref>, 78.9%. With deeper and wider architectures, BYOL consiste",0
"ing average target network to produce stable targets for the online network was inspired by deep RL <ref type=""bibr"" target=""#b49"">[50,</ref><ref type=""bibr"" target=""#b50"">51,</ref><ref type=""bibr"" ta",0
"r the following objective that extends the InfoNCE objective<ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b83"">84]</ref> (see Appendix G.4), InfoNCE α,β θ</note></figure> 			<note",0
"get=""#b57"">58,</ref><ref type=""bibr"" target=""#b58"">59,</ref><ref type=""bibr"" target=""#b59"">60,</ref><ref type=""bibr"" target=""#b60"">61,</ref><ref type=""bibr"" target=""#b61"">62]</ref>. Among these method",0
"bel information. We follow the semi-supervised protocol of <ref type=""bibr"" target=""#b73"">[74,</ref><ref type=""bibr"" target=""#b75"">76,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" targe",0
"et=""#b18"">[19,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" target=""#b56"">57,</ref><ref type=""bibr"" target=""#b57"">58,</ref><ref type=""bibr"" target=""#b58"">59,</ref><ref type=""bibr"" tar",0
"t=""#b41"">42]</ref>, image inpainting <ref type=""bibr"" target=""#b42"">[43]</ref>, image jigsaw puzzle <ref type=""bibr"" target=""#b43"">[44]</ref>, image super-resolution <ref type=""bibr"" target=""#b44"">[45",0
"nd efficiently segmenting large point clouds <ref type=""bibr"" target=""#b45"">[Wang et al., 2018</ref><ref type=""bibr"" target=""#b29"">, Li et al., 2019b]</ref>. Recent works have looked at frameworks to 20"">, Huang et al., 2017</ref><ref type=""bibr"" target=""#b53"">, Yu and Koltun, 2016]</ref>, DeepGCNs <ref type=""bibr"" target=""#b29"">[Li et al., 2019b]</ref> propose to train very deep GCNs (56 layers) to be either SoftMax_Agg β (•) or PowerMean_Agg p (•).</p><p>Better Residual Connections. DeepGCNs <ref type=""bibr"" target=""#b29"">[Li et al., 2019b]</ref> show residual connections <ref type=""bibr"" t ., 2016]</ref> is used in every layer before the activation function ReLU.</p><p>ResGCN. Similar to <ref type=""bibr"" target=""#b29"">Li et al. [2019b]</ref>, we construct ResGCN by adding residual conne",1
"target=""#b14"">[Hamilton et al., 2017</ref><ref type=""bibr"" target=""#b0"">, Armeni et al., 2017</ref><ref type=""bibr"" target=""#b40"">, Rahimi et al., 2018</ref><ref type=""bibr"" target=""#b49"">, Xu et al.",0
"br"" target=""#b50"">, Xu et al., 2019b]</ref>. Popular choices for aggregation functions include mean <ref type=""bibr"" target=""#b23"">[Kipf and Welling, 2016]</ref>, max <ref type=""bibr"" target=""#b14"">[H 2016]</ref>, Chebyshev graph CNN <ref type=""bibr"" target=""#b8"">[Defferrard et al., 2016]</ref>, GCN <ref type=""bibr"" target=""#b23"">[Kipf and Welling, 2016]</ref>, MPNN <ref type=""bibr"" target=""#b12"">[ target=""#b2"">[Battaglia et al., 2018]</ref>. ζ (l) can be a simply symmetric function such as mean <ref type=""bibr"" target=""#b23"">[Kipf and Welling, 2016]</ref>, max <ref type=""bibr"" target=""#b14"">[H many works in the graph learning field tend to use simple permutation invariant functions like mean <ref type=""bibr"" target=""#b23"">[Kipf and Welling, 2016]</ref>, max <ref type=""bibr"" target=""#b14"">[H esults with SOTA posted on OGB Learderboard at the time of this submission. The methods include GCN <ref type=""bibr"" target=""#b23"">[Kipf and Welling, 2016]</ref>, GraphSAGE <ref type=""bibr"" target=""#b ""#b50"">[Xu et al., 2019b]</ref>, they are found to be effective on the tasks of node classification <ref type=""bibr"" target=""#b23"">[Kipf and</ref><ref type=""bibr"">Welling, 2016, Hamilton et al., 2017]",0
"GATs) <ref type=""bibr"" target=""#b43"">[Veličković et al., 2018]</ref> employ the attention mechanism <ref type=""bibr"" target=""#b1"">[Bahdanau et al., 2015]</ref> to obtain different and trainable weight thods such as BatchNorm <ref type=""bibr"" target=""#b21"">[Ioffe and Szegedy, 2015]</ref> or LayerNorm <ref type=""bibr"" target=""#b1"">[Ba et al., 2016]</ref> to normalize vertex features. In addition to t the aggregation function is replaced by Sum(•), Mean(•) or Max(•) aggregation. Layer normalization <ref type=""bibr"" target=""#b1"">[Ba et al., 2016]</ref> is used in every layer before the activation f",0
"><p>Here, p is a non-zero, continuous variable denoting the q-th power.</p><p>Quasi-arithmetic mean <ref type=""bibr"" target=""#b25"">[Kolmogorov and Castelnuovo, 1930]</ref> was proposed to unify the fa",0
"type=""bibr"" target=""#b52"">, Ying et al., 2018]</ref>, and efficiently segmenting large point clouds <ref type=""bibr"" target=""#b45"">[Wang et al., 2018</ref><ref type=""bibr"" target=""#b29"">, Li et al., 2",0
"ed out by <ref type=""bibr"">Li et al. [2018]</ref>. Recent works focus on addressing this phenomenon <ref type=""bibr"" target=""#b24"">[Klicpera et al., 2019</ref><ref type=""bibr"" target=""#b41"">, Rong et rget=""#b41"">, Rong et al., 2020</ref><ref type=""bibr"" target=""#b57"">, Zhao and Akoglu, 2020]</ref>. <ref type=""bibr"" target=""#b24"">Klicpera et al. [2019]</ref> proposes a PageRank-based message passin",0
"i to see how well the model performs on that task. The goal of Model-Agnostic Meta-Learning (MAML) <ref type=""bibr"" target=""#b9"">[9]</ref> is to obtain a parameter initialization θ * that can adapt t nowledge across meta-training tasks and is the optimal parameter to adapt to unseen tasks quickly.  <ref type=""bibr"" target=""#b9"">[9]</ref> switches ProtoNet to MAML as the meta-learner. All experimen",1
"derstand how nodes influence each other during neural message passing. The assumptions are based on <ref type=""bibr"" target=""#b40"">[39]</ref>  </p><formula xml:id=""formula_8"">I u,v = ∂x (∞) u /∂x (∞) orm is any subordinate norm and the Jacobian measures how a change in v translates to a change in u <ref type=""bibr"" target=""#b40"">[39]</ref>. ( <ref type=""formula"" target=""#formula_6"">2</ref> </p><fo ght matrices at layer l, respectively, and Â = D −1 A is the normalized adjacency matrix. Following <ref type=""bibr"" target=""#b40"">[39]</ref>, throughout these derivations, we assume that σ is an iden orm is any subordinate norm and the Jacobian measures how a change in v translates to a change in u <ref type=""bibr"" target=""#b40"">[39]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>",0
"ype=""bibr"" target=""#b47"">46,</ref><ref type=""bibr"" target=""#b33"">32]</ref>. For example, Patchy-San <ref type=""bibr"" target=""#b23"">[23]</ref> uses local receptive fields to extract useful features fro",0
"likely fake edges, and assigns less weight to suspicious edges based on network theory of homophily <ref type=""bibr"" target=""#b13"">[14]</ref>. The latter components stabilizes the evolution of graph s target=""#b39"">40]</ref>), GNNGUARD determines importance weights using theory of network homophily <ref type=""bibr"" target=""#b13"">[14]</ref>, positing that similar nodes (i.e., nodes with similar fea",1
"ience and development of technology but can also substantially interfere with human decision making <ref type=""bibr"" target=""#b10"">[11]</ref>. For this reason, it is vital to develop GNNs that are rob 2]</ref> and the development of effective defense techniques <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b12"">13]</ref>, adversarial attack",0
"de features and graph structure and does not need any external data. Further, recent studies (e.g., <ref type=""bibr"" target=""#b42"">[43,</ref><ref type=""bibr"" target=""#b43"">44]</ref>) focus on theoreti",0
"on poisoning attacks. Attackers try to fool a GNN by corrupting the graph topology during training <ref type=""bibr"" target=""#b36"">[37,</ref><ref type=""bibr"" target=""#b37"">38]</ref>. The attacker care",0
"Networks (GNNs), in particular, have achieved remarkable success in a variety of application areas <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target",0
"category loss term. To enable distribution q ϕ c (z|x q c ) differentiable, we follow previous work <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target",1
".org/ns/1.0""><head n=""2.2"">MAML</head><p>We give an overview of Model-Agnostic Meta-Learning method <ref type=""bibr"" target=""#b11"">[12]</ref> which is a representative algorithm of optimization-based problem, we propose to encode the information from support set into our parameter inspired by MAML <ref type=""bibr"" target=""#b11"">[12]</ref> and further we can obtain a category-specific model to acc n testing query data.</p><p>•Meta-Learning We select two state-of-the-art meta learning models MAML <ref type=""bibr"" target=""#b11"">[12]</ref> and Meta-SGD <ref type=""bibr"" target=""#b20"">[21]</ref> as ent based learning procedure for new task quick adaptation. In the optimization-based methods, MAML <ref type=""bibr"" target=""#b11"">[12]</ref> is a recent promising model which learns a set of model pa",1
"ion q ϕ c (z|x q c ) differentiable, we follow previous work <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b18"">19]</ref> to use reparameteriza",1
"ing over a potentially exponential number of configurations for z. As with variational autoencoders <ref type=""bibr"" target=""#b18"">[19]</ref>, we approximate the objective function using the evidence cted layers with weight matrix W 2d ×d µ and W 2d ×d σ to output mean µ and log(σ ) as suggested in <ref type=""bibr"" target=""#b18"">[19]</ref>.</p><p>Decoder The decoder is a fully connected layer with e follow previous work <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b18"">19]</ref> to use reparameterization trick to parameterize z. Reparame",1
"</ref>. The deep learning anomaly detection (DAD) approaches <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b29"">30]</ref> model the log data as a natural language sequence and apply",0
"o state-of-the-art meta learning models MAML <ref type=""bibr"" target=""#b11"">[12]</ref> and Meta-SGD <ref type=""bibr"" target=""#b20"">[21]</ref> as baselines. The model architectures of two baselines are ed to rapidly learn novel task with a small set of labeled data. Following this direction, Meta-SGD <ref type=""bibr"" target=""#b20"">[21]</ref> learns step sizes and updates directions besides initializ",0
"t neural NLI models <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b23"">24]</ref> that achieve promis",0
")] Inference Loss +λ D K L (q ϕc (z |x q c ) | | q ϕc (z |x s c ))</formula><p>Bridging Regularizer <ref type=""bibr"" target=""#b9"">(10)</ref> In this paper, we assume q ϕ c (z|x q c ) and q ϕ c (z|x s s related to natural language inference (NLI) problem. We select three state-of-the-art models ESIM <ref type=""bibr"" target=""#b9"">[10]</ref>, Transformer <ref type=""bibr"" target=""#b23"">[24]</ref>, BER b27"">[28]</ref> (MultiNLI) corpus have promoted the development of many different neural NLI models <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" tar",0
"d to anomaly detection which aims to find patterns in data that do not conform to expected behavior <ref type=""bibr"" target=""#b8"">[9]</ref>. In the anomaly detection, the most related line of research",0
"type=""bibr"" target=""#b19"">20]</ref> and domain adaptation <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b26"">27]</ref>. More concretely, t",0
"zation encourages the confidence of predictions and is commonly used in the semisupervised learning <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" targ",0
") are contributed by individual retailers, the catalog information unavoidably contains noisy facts <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b30"">31]</ref>. The existence of",0
"n which aims to find text, which can indicate the reasons and the nature of the failure of a system <ref type=""bibr"" target=""#b7"">[8]</ref>. The traditional methods typically extract features from uns ave achieved an improvement in the performance of anomaly detection due to their powerful abilities <ref type=""bibr"" target=""#b7"">[8]</ref>. The deep learning anomaly detection (DAD) approaches <ref t",0
"neural network based models, such as Transformer <ref type=""bibr"" target=""#b23"">[24]</ref> and BERT <ref type=""bibr"" target=""#b10"">[11]</ref> have shown promising performance towards NLI task. However e use two Transformers to take two parts separately to obtain fixed-dimensional features. Following <ref type=""bibr"" target=""#b10"">[11]</ref>, the first token of every sequence is always a special cla ef type=""bibr"" target=""#b9"">[10]</ref>, Transformer <ref type=""bibr"" target=""#b23"">[24]</ref>, BERT <ref type=""bibr"" target=""#b10"">[11]</ref> as baselines. All sublayers of ESIM produce the output wit romoted the development of many different neural NLI models <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" tar",0
"ep learning baselines and the proposed approach with Py-Torch 1.2. For training models, we use Adam <ref type=""bibr"" target=""#b17"">[18]</ref> optimizer in the default setting. The learning rate α is 0",0
"contradiction or neutral. Large annotated datasets such as the Stanford Natural Language Inference <ref type=""bibr"" target=""#b5"">[6]</ref> (SNLI) and Multi-Genre Natural Language Inference <ref type=",0
"require large amounts of task-specific data for fine-tuning <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b28"">29]</ref>. Thus, how to train a NLI model with a small set of dataset",0
".3"">Implementation Details</head><p>The 300 dimensional FastText pre-trained word-embedding weights <ref type=""bibr"" target=""#b4"">[5]</ref> are used to initialize the parameters of the word embedding",0
") in Eq. 2, we propose to use a more informative conditional prior distribution p(z|x s c ) as with <ref type=""bibr"" target=""#b12"">[13]</ref> and further rewrite our objective function as follows:</p>",0
"f type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b19"">20]</ref> and domain adaptation <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" ta",0
"2017;</ref><ref type=""bibr"" target=""#b34"">Veličković et al., 2018)</ref> or variants of Transformer <ref type=""bibr"" target=""#b33"">(Vaswani et al., 2017)</ref> that apply self-attention on all nodes t ype=""bibr"" target=""#b7"">Cai and Lam, 2020)</ref> base their encoder on the Transformer architecture <ref type=""bibr"" target=""#b33"">(Vaswani et al., 2017)</ref> and thus, in each layer, compute self-at raformer follows the general multi-layer encoderdecoder pattern known from the original Transformer <ref type=""bibr"" target=""#b33"">(Vaswani et al., 2017)</ref>. In the following, we first describe our computations for one head. The output of multiple heads is combined as in the original Transformer <ref type=""bibr"" target=""#b33"">(Vaswani et al., 2017)</ref>.</p><p>Text self-attention. <ref type=""b ead n=""3.4"">Graformer decoder</head><p>Our decoder follows closely the standard Transformer decoder <ref type=""bibr"" target=""#b33"">(Vaswani et al., 2017)</ref>, except for the modifications suggested of the ith node's label.</p><p>To compute the node representation H (L) in the Lth layer, we follow <ref type=""bibr"" target=""#b33"">Vaswani et al. (2017)</ref>, i.e., we first normalize the input from",1
"2018)</ref>; GTR-LSTM from<ref type=""bibr"" target=""#b32"">(Trisedya et al., 2018)</ref>; E2E GRU from<ref type=""bibr"" target=""#b8"">(Castro Ferreira et al., 2019)</ref>. Number of parameters in millions",0
"air of nodes, i.e., there are no unreachable nodes or disconnected subgraphs. Thus they use an LSTM <ref type=""bibr"" target=""#b13"">(Hochreiter and Schmidhuber, 1997)</ref> to compute a relation embedd",0
"an entity's label into its components from the vocabulary Σ T of text tokens. Following recent work <ref type=""bibr"" target=""#b28"">(Ribeiro et al., 2020)</ref>, we mimic this composition- </p><formula //www.tei-c.org/ns/1.0""><head n=""4.2"">Data preprocessing</head><p>Following previous work on AGENDA <ref type=""bibr"" target=""#b28"">(Ribeiro et al., 2020)</ref>, we put the paper title into the graph a n Table <ref type=""table"" target=""#tab_3"">4</ref> that the performance of both Graformer and CGE-LW <ref type=""bibr"" target=""#b28"">(Ribeiro et al., 2020)</ref> increases with more graph structure (lar b_8""><head>Table 7 :</head><label>7</label><figDesc>Example references and texts generated by CGE-LW<ref type=""bibr"" target=""#b28"">(Ribeiro et al., 2020)</ref> and Graformer (marked Ours) for samples =""#tab_8"">7</ref> shows three example generations from our Graformer model and the CGE-LW system by <ref type=""bibr"" target=""#b28"">Ribeiro et al. (2020)</ref>. Often CGE-LW generations have a high sur",0
"b15"">Koncel-Kedziorski et al., 2019;</ref><ref type=""bibr"" target=""#b27"">Ribeiro et al., 2019;</ref><ref type=""bibr"" target=""#b11"">Guo et al., 2019)</ref>. As one layer of these encoders only consider",0
"ssage passing, in order to capture richer topological properties. Our method draws inspiration from <ref type=""bibr"" target=""#b32"">[33]</ref>, where it was shown that GNNs become universal when the ve target=""#b35"">[36]</ref>.</p><p>It is important to note here that contrary to identifierbased GNNs <ref type=""bibr"" target=""#b32"">[33]</ref>, <ref type=""bibr"" target=""#b36"">[37]</ref>, <ref type=""bib rovide a unique identification of the vertices, then universality will also hold (Corollary 3.1. in <ref type=""bibr"" target=""#b32"">[33]</ref>).</p><p>We conjecture, that in real-world scenarios the nu ><p>Unique identifiers. From a different perspective, <ref type=""bibr"" target=""#b67"">[68]</ref> and <ref type=""bibr"" target=""#b32"">[33]</ref> showed the connections between GNNs and distributed local",1
"blishing connections with the WL hierarchy, similarly to <ref type=""bibr"" target=""#b16"">[17]</ref>, <ref type=""bibr"" target=""#b65"">[66]</ref>. The main drawbacks of these methods are the training and",0
"most of the datasets, with a considerable margin against the main GNN baselines in some cases. GCN <ref type=""bibr"" target=""#b103"">[104]</ref> 0.469±0.002 -GIN <ref type=""bibr"" target=""#b15"">[16]</re >In Table <ref type=""table"" target=""#tab_2"">3</ref> we compare against the following baselines: GCN <ref type=""bibr"" target=""#b103"">[104]</ref> and GIN <ref type=""bibr"" target=""#b15"">[16]</ref>, two c",0
"=""bibr"" target=""#b88"">[89]</ref> and diffusion operators <ref type=""bibr"" target=""#b89"">[90]</ref>, <ref type=""bibr"" target=""#b90"">[91]</ref>, <ref type=""bibr"" target=""#b91"">[92]</ref> that employ adj",0
"ges during training <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b55"">56]</ref>. An alternative to approximate the loss is to approximate t set as its own class <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b55"">56]</ref>. Dosovitskiy et al. <ref type=""bibr"" target=""#b15"">[16]</re ata. In addition, SwAV works with small and large batch sizes and does not need a large memory bank <ref type=""bibr"" target=""#b55"">[56]</ref> or a momentum encoder <ref type=""bibr"" target=""#b23"">[24]< h as many classes as images in the dataset. As this approach becomes quickly intractable, Wu et al. <ref type=""bibr"" target=""#b55"">[56]</ref> mitigate this issue by replacing the classifier with a mem fferent augmentations of the same image. This solution is inspired by contrastive instance learning <ref type=""bibr"" target=""#b55"">[56]</ref> as we do not consider the codes as a target, but only enfo feature. A similar comparison appears in contrastive learning where features are compared directly <ref type=""bibr"" target=""#b55"">[56]</ref>. In Fig. <ref type=""figure"" target=""#fig_0"">1</ref>, we il bel>2</label></formula><formula xml:id=""formula_3"">)</formula><p>where τ is a temperature parameter <ref type=""bibr"" target=""#b55"">[56]</ref>. Taking this loss over all the images and pairs of data au cating pass forwards to the assignments. This is similar to the memory bank introduced by Wu et al. <ref type=""bibr"" target=""#b55"">[56]</ref>, without momentum.</p><p>Assignment phase in DeepCluster-v .6 Image classification with KNN classifiers on ImageNet</head><p>Following previous work protocols <ref type=""bibr"" target=""#b55"">[56,</ref><ref type=""bibr"" target=""#b65"">66]</ref>, we evaluate the q",1
"get=""#b28"">29,</ref><ref type=""bibr"" target=""#b56"">57,</ref><ref type=""bibr"" target=""#b59"">60,</ref><ref type=""bibr"" target=""#b60"">61,</ref><ref type=""bibr"" target=""#b65"">66]</ref>. Caron et al. <ref",0
"formance gap with supervised pretraining in computer vision <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b41"">42]</ref>. Many recent state- of comparisons to random subsets of images during training <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b55"">56]</ref>. An alternative to can outperform supervised pretraining on object detection <ref type=""bibr"" target=""#b18"">[19,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b41"">42]</ref>. We report more det d does not need a large memory bank <ref type=""bibr"" target=""#b55"">[56]</ref> or a momentum encoder <ref type=""bibr"" target=""#b23"">[24]</ref>.</p><p>Besides our online clustering-based method, we also arning <ref type=""bibr"" target=""#b27"">[28,</ref><ref type=""bibr"" target=""#b44"">45]</ref>. He et al. <ref type=""bibr"" target=""#b23"">[24]</ref> improve the training of contrastive methods by storing rep ntrastive methods typically need to store the last 65K instances obtained from the last 250 batches <ref type=""bibr"" target=""#b23"">[24]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head en transferring to ImageNet with frozen or finetuned features. We report the results from He et al. <ref type=""bibr"" target=""#b23"">[24]</ref> but note that their setting is different. They use a curat c-tron2 <ref type=""bibr"" target=""#b54"">[55]</ref> and follow the finetuning protocol from He et al. <ref type=""bibr"" target=""#b23"">[24]</ref> making the following changes to the hyperparameters -our i 0.333 for 1000 iterations. Other training hyperparamters are kept exactly the same as in He et al. <ref type=""bibr"" target=""#b23"">[24]</ref>, i.e., batchsize of 16 across 8 GPUs, training for 24K ite ntly improves over training from scratch on ImageNet (+1.3%) <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b23"">24]</ref>. In Fig. <ref type=""figure"" target=""#fig_2"">4</ref> (right)",0
"shown that exploring architectures with search <ref type=""bibr"" target=""#b36"">[37]</ref> or pruning <ref type=""bibr"" target=""#b8"">[9]</ref> is possible without supervision, and we plan to evaluate the",0
"instances, which is a special form of contrastive learning <ref type=""bibr"" target=""#b27"">[28,</ref><ref type=""bibr"" target=""#b44"">45]</ref>. He et al. <ref type=""bibr"" target=""#b23"">[24]</ref> improv",0
"nd pursued in natural language processing <ref type=""bibr"" target=""#b9"">[10]</ref>, computer vision <ref type=""bibr"" target=""#b16"">[17]</ref>, and other domains. To date, the most powerful solution is e how to define (dis)similar instances.</p><p>Q2: Define (dis)similar instances. In computer vision <ref type=""bibr"" target=""#b16"">[17]</ref>, two random data augmentations (e.g., random crop, random d by the graph encoder, the final d-dimensional output vectors are then normalized by their L2-Norm <ref type=""bibr"" target=""#b16"">[17]</ref>.</p><p>A running example. We illustrate a running example ffectively build and maintain the dictionary, such as end-to-end (E2E) and momentum contrast (MoCo) <ref type=""bibr"" target=""#b16"">[17]</ref>. We discuss the two strategies as follows.</p><p>E2E sampl propagation. The parameters of f k (denoted by θ k ) are not updated by gradient descent. He et al. <ref type=""bibr"" target=""#b16"">[17]</ref> propose a momentum-based update rule for θ k . More formal the dictionary, such as memory bank <ref type=""bibr"" target=""#b58"">[59]</ref>. Recently, He et al. <ref type=""bibr"" target=""#b16"">[17]</ref> show that MoCo is a more effective option than memory bank >Contrastive loss mechanisms. The common belief is that MoCo has stronger expression power than E2E <ref type=""bibr"" target=""#b16"">[17]</ref>, and a larger dictionary size K always helps. We also obse r, the effect of a large dictionary size is not as significant as reported in computer vision tasks <ref type=""bibr"" target=""#b16"">[17]</ref>. For example, MoCo (K = 16384) merely outperforms MoCo (K in Table <ref type=""table"" target=""#tab_6"">5</ref> in the Appendix. Momentum. As mentioned in MoCo <ref type=""bibr"" target=""#b16"">[17]</ref>, momentum m plays a subtle role in learning high-quality r tasets. For US-Airport, the best performance is reached by m = 0.999, which is the desired value in <ref type=""bibr"" target=""#b16"">[17]</ref>, showing that building a consistent dictionary is importan brings better performance. Moreover, we do not observe the ""training loss oscillation"" reported in <ref type=""bibr"" target=""#b16"">[17]</ref> when setting m = 0. GCC (MoCo) converges well, but the acc ings.</p><p>In computer vision, a large collection of work <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b49"">50,</ref><ref type=""bibr"" tar objectives for graph structured data. Inspired by the recent success of contrastive learning in CV <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b58"">59]</ref> and NLP <ref type= ats each instance as a distinct class of its own and learns to discriminate between these instances <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b58"">59]</ref>. The promise is th",1
"get=""#b42"">43,</ref><ref type=""bibr"" target=""#b45"">46,</ref><ref type=""bibr"" target=""#b59"">60,</ref><ref type=""bibr"" target=""#b60"">61]</ref>. We first introduce the self-supervised pre-training settin from unlabeled graph data and then feed them into logistic regression or SVM. Examples include DGK <ref type=""bibr"" target=""#b60"">[61]</ref>, Struc2vec <ref type=""bibr"" target=""#b42"">[43]</ref>, Grap n=""4.2.2"">Graph Classification.</head><p>Setup. We use five datasets from Yanardag and Vishwanathan <ref type=""bibr"" target=""#b60"">[61]</ref> -COLLAB, IMDB-BINARY, IMDB-MULTI, REDDITBINARY and REDDIT- re GCC with several recent developed graph classification models, including Deep Graph Kernel (DGK) <ref type=""bibr"" target=""#b60"">[61]</ref>, graph2vec <ref type=""bibr"" target=""#b32"">[33]</ref>, Info tasks, we follow Struc2vec to use logistic regression. For graph classification tasks, we follow DGK<ref type=""bibr"" target=""#b60"">[61]</ref> and GIN<ref type=""bibr"" target=""#b59"">[60]</ref> to use SV .</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>A.2.2 Graph Classification.</head><p>DGK <ref type=""bibr"" target=""#b60"">[61]</ref>, graph2vec <ref type=""bibr"" target=""#b32"">[33]</ref>, Info",0
"s of datasets for pre-training.</p><p>Pre-training settings. We train for 75,000 steps and use Adam <ref type=""bibr"" target=""#b23"">[24]</ref> for optimization with learning rate of 0.005, β 1 = 0.9, β",0
"with restart (RWR) <ref type=""bibr"" target=""#b50"">[51]</ref>, subgraph induction, and anonymization <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b28"">29]</ref>.</p><p>(1) Random",0
"type=""bibr"" target=""#b47"">[48]</ref>, DeepWalk <ref type=""bibr"" target=""#b38"">[39]</ref>, node2vec <ref type=""bibr"" target=""#b13"">[14]</ref>, also follow the neighborhood similarity assumption.</p><p type=""bibr"" target=""#b47"">[48]</ref>, DeepWalk <ref type=""bibr"" target=""#b38"">[39]</ref>, node2vec <ref type=""bibr"" target=""#b13"">[14]</ref>, and metapath2vec <ref type=""bibr"" target=""#b10"">[11]</ref al representation model and apply it to unseen graphs, differing from traditional network embedding <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b38"">39,</ref><ref type=""bibr"" ta",0
"f big models have been demonstrated on supervised learning <ref type=""bibr"" target=""#b59"">[60]</ref><ref type=""bibr"" target=""#b60"">[61]</ref><ref type=""bibr"" target=""#b61"">[62]</ref><ref type=""bibr"" t",0
"reminiscent of the use of pseudo labels <ref type=""bibr"" target=""#b10"">[11]</ref> in self-training <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b11"">12]</ref>, but without much type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"">1,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b11"">12]</ref>, the proposed semi- et=""#b22"">[23,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b11"">12]</ref>, we use the fine-tu ><ref type=""bibr"" target=""#b14"">15]</ref> or self-training <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b23"">24]</ref>. The main differences between these methods and ours are th",0
"ourage the student network to mimic the teacher network's label predictions. Thus, the distillation <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b22"">23]</ref> phase of our metho task. Inspired by <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" tar ne-tune"" paradigm by combining it with (self-)distillation <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b10"">11]</ref> using unlabeled dat",0
"arget=""#b1"">2]</ref> or under different data augmentations <ref type=""bibr"" target=""#b12"">[13]</ref><ref type=""bibr"" target=""#b13"">[14]</ref><ref type=""bibr"" target=""#b14"">[15]</ref>.</p><p>Motivated get=""#b37"">38,</ref><ref type=""bibr"" target=""#b29"">30]</ref> ResNet-50 --47.0 83.4 UDA (w. RandAug) <ref type=""bibr"" target=""#b13"">[14]</ref> ResNet-50 -68.8 -88.5 FixMatch (w. RandAug) <ref type=""bib",0
"oves previous state-of-the-art in self-supervised learning <ref type=""bibr"" target=""#b19"">[20,</ref><ref type=""bibr"" target=""#b43"">44,</ref><ref type=""bibr"">1]</ref>. We were not aware of BYOL <ref ty",0
"arget=""#b8"">[9,</ref><ref type=""bibr"" target=""#b64"">65,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b65"">66]</ref>. However, it is still somewhat surprising that bigger model",0
"f-supervised pretraining, followed by supervised fine-tuning <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b3"">4]</ref>. This Correspondence to: iamtingchen@google.com 1 Code and pr",0
"sampling technique. Furthermore, we adapt the higher-order graph node information under the CoreSet <ref type=""bibr"" target=""#b30"">[31]</ref> for a new sampling technique by introducing latent space d 1]</ref> for a new sampling technique by introducing latent space distancing. In principle, CoreSet <ref type=""bibr"" target=""#b30"">[31]</ref> uses risk minimisation between core-sets on the learner fe , the first work applying it for CNNs as an active learning problem, CoreSet, has been presented in <ref type=""bibr"" target=""#b30"">[31]</ref>. The key principle depends on minimising the difference be ods: UncertainGCN and CoreGCN. UncertainGCN is based on the standard AL method uncertainty sampling <ref type=""bibr"" target=""#b30"">[31]</ref> which tracks the confidence scores of the designed graph n dence scores of the designed graph nodes. Furthermore, CoreGCN adapts the highly successful CoreSet <ref type=""bibr"" target=""#b30"">[31]</ref> on the induced graph embeddings by the sequentially traine formation between the labelled and unlabelled graph representation, we approach a CoreSet technique <ref type=""bibr"" target=""#b30"">[31]</ref> in our sampling stage. This has shown better performance i r performance in comparison to uncertainty-based methods <ref type=""bibr"" target=""#b37"">[38]</ref>. <ref type=""bibr"" target=""#b30"">[31]</ref> shows how bounding the difference between the loss of the ype=""bibr"" target=""#b32"">[33]</ref> Learning Loss <ref type=""bibr"" target=""#b41"">[42]</ref> CoreSet <ref type=""bibr"" target=""#b30"">[31]</ref> CoreGCN(Ours) FeatProp <ref type=""bibr"" target=""#b37"">[38] s which we describe here. Random sampling is by default the most common sampling technique. CoreSet <ref type=""bibr"" target=""#b30"">[31]</ref> on learner feature space is one of the best performing geo spectrum of baselines. One is random sampling which is the default mechanism. The other is CoreSet <ref type=""bibr"" target=""#b30"">[31]</ref>, one of the best performing baselines from the previous ex ng system due to its successful deployment in recent methods <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b30"">31,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" tar",1
"e in comparison to other networks with comparable parameter complexity. Any other model like VGG-11 <ref type=""bibr"" target=""#b31"">[32]</ref> can also be easily deployed (refer to Supplementary Materi 7,500 images. The experiment architecture and settings are similar to the one on the full scale.    <ref type=""bibr"" target=""#b31"">[32]</ref>. Therefore, we analyse how the AL methods are affected in",0
"ments. For the sampler, GCN has 2 layers and we set the dropout rate to 0.3 to avoid over-smoothing <ref type=""bibr"" target=""#b42"">[43]</ref>. The dimension of initial representations of a node is 102",0
"e learning model ( <ref type=""bibr"" target=""#b19"">[20,</ref><ref type=""bibr"" target=""#b34"">35,</ref><ref type=""bibr"" target=""#b13"">14]</ref>), the first work applying it for CNNs as an active learning",0
"our method on a challenging dataset for 3D Hand Pose Estimation benchmarks from depth images. ICVL <ref type=""bibr"" target=""#b33"">[34]</ref>  joints from depth images. Thus, we replace ResNet-18 by c baselines on one of the most challenging, widely been used and first of depth based datasets, ICVL <ref type=""bibr"" target=""#b33"">[34]</ref>. This is composed of 16,004 images for training and 1,600",0
"to-translations it has already been shown that a suitable structure for φ is a tensor field network <ref type=""bibr"" target=""#b24"">[25]</ref>, explained below. Note that Romero et al. <ref type=""bibr"" q. <ref type=""bibr"" target=""#b4"">(5)</ref>.</p><p>Tensor Field Networks Tensor field networks (TFN) <ref type=""bibr"" target=""#b24"">[25]</ref> are neural networks, which map point clouds to point cloud annels, but we omit it here. Weiler et al. <ref type=""bibr"" target=""#b32"">[33]</ref>, Thomas et al. <ref type=""bibr"" target=""#b24"">[25]</ref> and Kondor <ref type=""bibr"" target=""#b12"">[13]</ref> showe duces the kernel to a scalar w multiplied by the identity, W = w I, referred to as self-interaction <ref type=""bibr"" target=""#b24"">[25]</ref>. As such we can rewrite the TFN layer as</p><formula xml:i c c = w c c per representation degree, shared across all points.</p><p>As proposed in Thomas et al. <ref type=""bibr"" target=""#b24"">[25]</ref>, this is followed by a norm-based non-linearity.</p><p>Att tron-proton simulation.</p><p>Linear DeepSet <ref type=""bibr"" target=""#b39"">[40]</ref> Tensor Field <ref type=""bibr"" target=""#b24"">[25]</ref> Set Transformer <ref type=""bibr"" target=""#b13"">[14]</ref> type=""bibr"" target=""#b13"">[14]</ref>, a non-equivariant attention model, and Tensor Field Networks <ref type=""bibr"" target=""#b24"">[25]</ref>, which is similar to SE(3)-Transformer but does not levera s to train significantly larger versions of both the SE(3)-Transformer and the Tensor Field network <ref type=""bibr"" target=""#b24"">[25]</ref> and to apply these models to real-world datasets.</p><p>Ou be found in many mathematical physics libraries.</p><p>Tensor Field Layers In Tensor Field Networks <ref type=""bibr"" target=""#b24"">[25]</ref> and 3D Steerable CNNs <ref type=""bibr"" target=""#b32"">[33]< that all the Tensor Field networks we trained were significantly bigger than in the original paper <ref type=""bibr"" target=""#b24"">[25]</ref>, mostly enabled by the faster computation of the spherical k to obtain stable training. We used a norm based non-linearity for the Tensor Field network (as in <ref type=""bibr"" target=""#b24"">[25]</ref>) and no extra non-linearity (beyond the softmax in the sel he Tensor Field network and the linear baseline are SE(3) equivariant. For the Tensor Field Network <ref type=""bibr"" target=""#b24"">[25]</ref> baseline, we used the same hyper parameters as for the SE( linear self-interaction and an additional norm-based nonlinearity in each layer as in Thomas et al. <ref type=""bibr"" target=""#b24"">[25]</ref>. For the DeepSet <ref type=""bibr"" target=""#b39"">[40]</ref>",1
"weights. In general, the number of query vectors does not have to equal the number of input points <ref type=""bibr"" target=""#b13"">[14]</ref>. In the case of self-attention the query, key, and value v t=""#b36"">37,</ref><ref type=""bibr"" target=""#b13"">14]</ref>. One such example is the Set Transformer <ref type=""bibr"" target=""#b13"">[14]</ref>. When applied to object classification on ModelNet40 <ref br"" target=""#b39"">[40]</ref> Tensor Field <ref type=""bibr"" target=""#b24"">[25]</ref> Set Transformer <ref type=""bibr"" target=""#b13"">[14]</ref>  classification task. Here, the network is confronted with -art results as well as a set of our own baselines. Specifically, we compare to the Set-Transformer <ref type=""bibr"" target=""#b13"">[14]</ref>, a non-equivariant attention model, and Tensor Field Netwo pe=""bibr"" target=""#b39"">[40]</ref>.</p><p>Set Transformer Baseline We used the same architecture as <ref type=""bibr"" target=""#b13"">[14]</ref> in their object classification experiment on ModelNet40 <r idden layer was followed by a LeakyReLU. The learning rate was set to 1e-3. For the Set Transformer <ref type=""bibr"" target=""#b13"">[14]</ref>, we used 4 self-attention blocks with 64 hidden units and o point cloud data <ref type=""bibr"" target=""#b38"">[39,</ref><ref type=""bibr"" target=""#b36"">37,</ref><ref type=""bibr"" target=""#b13"">14]</ref>. One such example is the Set Transformer <ref type=""bibr"" t low the common practice from the self-attention literature <ref type=""bibr"" target=""#b27"">[28,</ref><ref type=""bibr"" target=""#b13"">14]</ref>, and chosen a softmax nonlinearity to normalise the attenti",0
"rmer <ref type=""bibr"" target=""#b13"">[14]</ref>. When applied to object classification on ModelNet40 <ref type=""bibr"" target=""#b35"">[36]</ref>, the input to the Set Transformer are the cartesian coordi <ref type=""bibr"" target=""#b39"">[40]</ref> for their object classification experiment on ModelNet40 <ref type=""bibr"" target=""#b35"">[36]</ref>. However, most likely due to the relatively small number o s <ref type=""bibr"" target=""#b13"">[14]</ref> in their object classification experiment on ModelNet40 <ref type=""bibr"" target=""#b35"">[36]</ref> with an ISAB (induced set attention block)-based encoder f",0
"nsformer but does not leverage attention.</p><p>Similar to <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b33"">34]</ref>, we measure the exactness of equivariance by applying unifo",0
"]</ref> to dynamic graphs by ignoring the temporal evolution, this has been shown to be sub-optimal <ref type=""bibr"" target=""#b65"">[66]</ref>, and in some cases, it is the dynamic structure that conta iN (t N ) ?(t -t N )].<label>(9)</label></formula><p>Here, ?(?) represents a generic time encoding <ref type=""bibr"" target=""#b65"">[66]</ref>, is the concatenation operator and</p><formula xml:id=""for ated information. Differently from the original formulation of this layer (firstly proposed in TGAT <ref type=""bibr"" target=""#b65"">[66]</ref>) where no node-wise temporal features were used, in our ca arget=""#b35"">[36]</ref> uses the time projection embedding module emb(i, t) = (1+?tw)?s i (t). TGAT <ref type=""bibr"" target=""#b65"">[66]</ref> is a specific case of TGN when the memory and its related ils are provided in the supplementary material.</p><p>Tasks. Our experimental setup closely follows <ref type=""bibr"" target=""#b65"">[66]</ref> and focuses on the tasks of future edge prediction and dyn setting is used for node classification. We perform the same 70%-15%-15% chronological split as in <ref type=""bibr"" target=""#b65"">[66]</ref>.</p><p>Future Edge Prediction. The goal is to predict the ref type=""bibr"" target=""#b46"">[47]</ref>, Jodie <ref type=""bibr"" target=""#b35"">[36]</ref>, and TGAT <ref type=""bibr"" target=""#b65"">[66]</ref>) as well as state-of-the-art models for static graphs (GAE <ref type=""bibr"" target=""#b26"">[27]</ref>, CTDNE <ref type=""bibr"" target=""#b46"">[47]</ref> and TGAT <ref type=""bibr"" target=""#b65"">[66]</ref> are taken directly from the TGAT paper <ref type=""bibr"" ta 47]</ref> and TGAT <ref type=""bibr"" target=""#b65"">[66]</ref> are taken directly from the TGAT paper <ref type=""bibr"" target=""#b65"">[66]</ref>.</p><p>For Jodie <ref type=""bibr"" target=""#b35"">[36]</ref> and datasets we used the same hyperparameters, which had been found to work well in the TGAT paper <ref type=""bibr"" target=""#b65"">[66]</ref>.</p></div>			</div> 			<div type=""references"">  				<listB",1
"bibr"" target=""#b17"">18]</ref>. Learning on such data is possible using graph neural networks (GNNs) <ref type=""bibr"" target=""#b25"">[26]</ref> that typically operate by a message passing mechanism <ref",1
"et=""#b36"">[37,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b68"">69,</ref><ref type=""bibr"" target=""#b53"">54,</ref><ref type=""bibr"" target=""#b47"">48,</ref><ref type=""bibr"" tar arget=""#b40"">41,</ref><ref type=""bibr"" target=""#b68"">69,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b53"">54,</ref><ref type=""bibr"" target=""#b47"">48]</ref>, or learned by impo",0
"ther recent works have focused on dynamic knowledge graphs <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b64"">65,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" tar",0
"arget=""#b55"">56,</ref><ref type=""bibr"" target=""#b30"">31,</ref><ref type=""bibr"" target=""#b0"">1,</ref><ref type=""bibr"" target=""#b1"">2]</ref>, assemble snapshots into tensors and factorize <ref type=""bib",0
"</ref><ref type=""bibr"" target=""#b42"">43]</ref> and biology <ref type=""bibr"" target=""#b75"">[76,</ref><ref type=""bibr"" target=""#b61"">62,</ref><ref type=""bibr"" target=""#b17"">18]</ref>. Learning on such d",0
"=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b43"">44]</ref>, used as components in RNNs <ref type=""bibr"" target=""#b54"">[55,</ref><ref type=""bibr"" target=""#b44"">45,</ref><ref type=""bibr"" ta",0
"embeddings that are then used for node-wise classification <ref type=""bibr"" target=""#b41"">[42,</ref><ref type=""bibr"" target=""#b60"">61,</ref><ref type=""bibr"" target=""#b34"">35]</ref> or edge prediction <ref type=""bibr"" target=""#b50"">[51]</ref>, Node2Vec <ref type=""bibr"" target=""#b22"">[23]</ref>, GAT <ref type=""bibr"" target=""#b60"">[61]</ref> and GraphSAGE <ref type=""bibr"" target=""#b26"">[27]</ref>).< <ref type=""bibr"" target=""#b50"">[51]</ref>, Node2Vec <ref type=""bibr"" target=""#b22"">[23]</ref>, GAT <ref type=""bibr"" target=""#b60"">[61]</ref> and GraphSAGE <ref type=""bibr"" target=""#b26"">[27]</ref>, C",0
"ve Continuous Time Dynamic Graphs (CTDGs) been addressed. Several approaches use random walk models <ref type=""bibr"" target=""#b46"">[47,</ref><ref type=""bibr"" target=""#b45"">46,</ref><ref type=""bibr"" ta nes. Our strong baselines are state-of-the-art approaches for continuous time dynamic graphs (CTDNE <ref type=""bibr"" target=""#b46"">[47]</ref>, Jodie <ref type=""bibr"" target=""#b35"">[36]</ref>, and TGAT type=""bibr"" target=""#b60"">[61]</ref> and GraphSAGE <ref type=""bibr"" target=""#b26"">[27]</ref>, CTDNE <ref type=""bibr"" target=""#b46"">[47]</ref> and TGAT <ref type=""bibr"" target=""#b65"">[66]</ref> are tak",0
"c knowledge graphs <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b64"">65,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b18"">19]</ref>.</p><p>Most recent",0
"y, projects such as Halide <ref type=""bibr"" target=""#b18"">(Ragan-Kelley et al., 2013)</ref> and TVM <ref type=""bibr"" target=""#b2"">(Chen et al., 2018a)</ref> have arisen that attempt to simplify the pr L )</formula><p>in both TensorFlow <ref type=""bibr"" target=""#b0"">(Abadi et al., 2016)</ref> and TVM <ref type=""bibr"" target=""#b2"">(Chen et al., 2018a)</ref>. We use TensorFlow for training binary netw fficient Code</head><p>To compile our described algorithms to efficient machine code, we extend TVM <ref type=""bibr"" target=""#b2"">(Chen et al., 2018a)</ref> to support bitserial operations. This allow",1
"ck(p k ) {At least 3HW F ops.} 12: end for 13: Y = BinaryDense(a L )</formula><p>in both TensorFlow <ref type=""bibr"" target=""#b0"">(Abadi et al., 2016)</ref> and TVM <ref type=""bibr"" target=""#b2"">(Chen",0
"too slow and memory hungry. To this end, all time measurements are made on a Raspberry Pi 3B (RPi) <ref type=""bibr"" target=""#b17"">(Pi, 2015)</ref>. The RPi has an ARM Cortex-A53 CPU with 4 cores cloc",0
"target=""#b6"">(Courbariaux et al., 2016;</ref><ref type=""bibr"" target=""#b22"">Zhou et al., 2016;</ref><ref type=""bibr"" target=""#b1"">Cai et al., 2017;</ref><ref type=""bibr"" target=""#b12"">Hubara et al., 2 tion has been proposed. In some cases (e.g., where bits are scaled using non-linear scaling factors <ref type=""bibr"" target=""#b1"">(Cai et al., 2017)</ref>), it is unclear that such implementations eve e models with multiple bitwidth and polarity configurations. In these comparisons, we consider HWGQ <ref type=""bibr"" target=""#b1"">(Cai et al., 2017)</ref> the current state-of-the-art for high accurac",0
"ibr"" target=""#b12"">Hubara et al., 2016;</ref><ref type=""bibr"" target=""#b20"">Tang et al., 2017;</ref><ref type=""bibr"" target=""#b8"">Dong et al., 2017;</ref><ref type=""bibr"" target=""#b9"">Fromm et al., 20",0
"ering effort that is outside the scope of most research projects. Recently, projects such as Halide <ref type=""bibr"" target=""#b18"">(Ragan-Kelley et al., 2013)</ref> and TVM <ref type=""bibr"" target=""#b",0
"dule itself, and in some cases supporting automated hyperparameter search to produce good schedules <ref type=""bibr"" target=""#b3"">(Chen et al., 2018b)</ref>. In Section 4.3, we describe how we extend h these primitives require well chosen hyperparameters to maximize performance, we leverage AutoTVM <ref type=""bibr"" target=""#b3"">(Chen et al., 2018b)</ref> to automatically search and find high quali",0
"e=""bibr"" target=""#b22"">Zhou et al., 2016;</ref><ref type=""bibr"" target=""#b1"">Cai et al., 2017;</ref><ref type=""bibr"" target=""#b12"">Hubara et al., 2016;</ref><ref type=""bibr"" target=""#b20"">Tang et al., y achievable with 1-bit activations is simply not compelling and instead focus on using N ≥ 2 bits. <ref type=""bibr"" target=""#b12"">Hubara et al. (2016)</ref> and <ref type=""bibr"" target=""#b22"">Zhou et",0
"=""bibr"" target=""#b8"">Dong et al., 2017;</ref><ref type=""bibr"" target=""#b9"">Fromm et al., 2018;</ref><ref type=""bibr"" target=""#b4"">Choi et al., 2019)</ref> has therefore focused on closing the accuracy hors.</p><p>Methods that use unipolar quantization for activations such as DoReFa-Net and PACT-SAWB <ref type=""bibr"" target=""#b4"">(Choi et al., 2019)</ref> are able to represent zeros but encounter ot 17)</ref> the current state-of-the-art for high accuracy binary AlexNets and VG-GNets and PACT-SAWB <ref type=""bibr"" target=""#b4"">(Choi et al., 2019)</ref> the state-ofthe-art for binarizing Resnets.",0
"is critical to realism.</p><p>In contrast to methods that use a parametric model of the human face <ref type=""bibr"" target=""#b0"">[1]</ref>, we directly predict the positions of face mesh vertices in",1
"proach by employing region-specific heads that transform the feature maps with spatial transformers <ref type=""bibr"" target=""#b3"">[4]</ref>, while being up to 30 percent faster during inference. We te atures that are used by the attention mechanism. Specifically, we use a spatial transformer mod-ule <ref type=""bibr"" target=""#b3"">[4]</ref> to extract 24 × 24 region features from the 64 × 64 feature and hard) have been developed for visual feature extraction <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b3"">4]</ref>. These attention mechanisms sample a grid of 2D points in fea",0
"parate networks that are chained together.</p><p>We use an architecture similar to one described in <ref type=""bibr"" target=""#b6"">[7]</ref>, where the authors build a network that is robust to the ini",0
"makeup where even small errors in alignment can drive the rendered effect into the ""uncanny valley"" <ref type=""bibr"" target=""#b7"">[8]</ref>. We built a lipstick rendering solution (Figure <ref type=""f",0
"ious human faces, we apply Laplacian mesh editing to morph a canonical mesh into the predicted mesh <ref type=""bibr"" target=""#b2"">[3]</ref>. This lets us use the blend shape coefficients for different",0
"s h a 1 _ b a s e 6 4 = "" e A Z 8 7  This paper presents progress in diffusion probabilistic models <ref type=""bibr"" target=""#b49"">[50]</ref>. A diffusion probabilistic model (which we will call a ""di onals in p θ (x t−1 |x t ), because both processes have the same functional form when β t are small <ref type=""bibr"" target=""#b49"">[50]</ref>. A notable property of the forward process is that it admi ing to upper and lower bounds on reverse process entropy for data with coordinatewise unit variance <ref type=""bibr"" target=""#b49"">[50]</ref>.</p><p>Second, to represent the mean µ θ (x t , t), we pro orithms serve as a compression interpretation of the variational bound (5) of Sohl-Dickstein et al. <ref type=""bibr"" target=""#b49"">[50]</ref>, not yet as a practical compression system. </p></div> <di educed variance variational bound for diffusion models. This material is from Sohl-Dickstein et al. <ref type=""bibr"" target=""#b49"">[50]</ref>; we include it here only for completeness.</p><formula xml ments so that the number of neural network evaluations needed during sampling matches previous work <ref type=""bibr"" target=""#b49"">[50,</ref><ref type=""bibr"" target=""#b51"">52]</ref>. We set the forwar",1
"s different aspects of reconstructions that θ must perform <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b19"">20]</ref>. We will see in our experiments that this reweighting leads",0
"14,</ref><ref type=""bibr"" target=""#b20"">21]</ref> and VAEs <ref type=""bibr"" target=""#b30"">[31,</ref><ref type=""bibr"" target=""#b43"">44,</ref><ref type=""bibr"" target=""#b33"">34]</ref>, diffusion models a",0
"arget=""#b29"">30,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b20"">21]</ref> and VAEs <ref type=""bibr"" target=""#b30"">[31,</ref><ref type",0
"get=""#b53"">54,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b30"">31,</ref><ref type=""bibr"" target=""#b41"">42]</ref>, and there have been remarkable advances in energy-based mo",0
"ed representation learning, has been proven to be successful techniques contributing to the success <ref type=""bibr"" target=""#b1"">[2]</ref>. In essence, embedding is a way to represent a sparse vector",1
"information retrieval community and search engine industry as the next generation search technology <ref type=""bibr"" target=""#b12"">[13]</ref>.</p><p>In general, a search engine comprises a recall laye l in Facebook search is not a text embedding problem, as is actively researched in the IR community <ref type=""bibr"" target=""#b12"">[13]</ref>. Instead it is a more complex problem that requires unders",1
"and Boolean matching together to score documents for retrieval. To this purpose, we employed Faiss <ref type=""bibr"" target=""#b8"">[9]</ref> library for embedding vector quantization and integrated it ated into the existing retrieval system which is based on inverted index. We employed Faiss library <ref type=""bibr"" target=""#b8"">[9]</ref> to quantize the vectors and then implemented the efficient N",0
"e vectors, on which we use cosine similarity as the distance metric. We propose to use triplet loss <ref type=""bibr"" target=""#b13"">[14]</ref> to approximate the recall objective to learn the neural ne e from computer vision field and for the classification task <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" tar",0
"sue. One is to go deep. In terms of modeling we could apply the latest advanced models such as BERT <ref type=""bibr"" target=""#b3"">[4]</ref> or build task-specific models to address particular segments",0
"arity function, we choose cosine similarity as it is one of the commonly used in embedding learning <ref type=""bibr"" target=""#b6"">[7]</ref>:</p><formula xml:id=""formula_3"">S(Q, D) = cos(E Q , E D ) = nt features that contributed to the major model improvements.</p><p>Text features. Character n-gram <ref type=""bibr"" target=""#b6"">[7]</ref> is a common approach to represent text for text embedding. I",0
"semantic meaning of documents, search techniques are mostly based on various term matching methods <ref type=""bibr"" target=""#b0"">[1]</ref>, which performs well for the cases that keyword match can ad",0
"ade significant progress in speech recognition, computer vision, and natural language understanding <ref type=""bibr"" target=""#b9"">[10]</ref>. Among them embedding, which is also called representation",0
"rget=""#b5"">[6,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b16"">17]</ref>, while search retrieval does not have concept of ""classes""",0
"returned by the first stage model. We shared the same spirit as the cascaded embedding training in <ref type=""bibr"" target=""#b17"">[18]</ref>, which ensembled a set of models trained with different le",0
"ww.tei-c.org/ns/1.0""><head n=""1"">Introduction</head><p>The task of Grounded video description (GVD) <ref type=""bibr"" target=""#b8"">[Zhou et al., 2019]</ref> aims to generate more grounded and accurate existing works either encode region proposals independently or using selfattention-based mechanisms <ref type=""bibr"" target=""#b8"">[Zhou et al., 2019]</ref>. Therefore, it either fails to consider impl a]</ref>, many works model the video in both global video features and regional object features. In <ref type=""bibr"" target=""#b8"">[Zhou et al., 2019]</ref>, they encode the objects with transformer <r /head><p>We model the video's global level feature by a Bi-directional LSTM network like most works <ref type=""bibr"" target=""#b8"">[Zhou et al., 2019]</ref> given by: h = BiLST M (v) = {h 1 , h 2 , ... ose a novel visual representation method from the perspective of regions. First of all, inspired by <ref type=""bibr"" target=""#b8"">[Zhou et al., 2019]</ref>, we enhance the proposal features by adding <div xmlns=""http://www.tei-c.org/ns/1.0""><head>Feature Enhancement</head><p>In this part, we follow <ref type=""bibr"" target=""#b8"">[Zhou et al., 2019]</ref>'s work, which fusing the spatial-temporal an feature aggregation on the enhanced feature R.</p><p>We adopt the same classification loss just as <ref type=""bibr"" target=""#b8"">[Zhou et al., 2019]</ref> do denoted as L cls .</p></div> <div xmlns="" > t , h f rame + h attention ). h t is used to generate descriptions. We adopt the same MLE loss as <ref type=""bibr"" target=""#b8"">[Zhou et al., 2019]</ref> which denoted by L sent .</p><p>Finally, the ad n=""4.1"">Dataset</head><p>We conduct our experiments on the Grounded ActivityNet-Entities Dataset <ref type=""bibr"" target=""#b8"">[Zhou et al., 2019]</ref> for evaluation. It contains 15k video with 1 ph2Seq method. Data processing. For a fair comparison, the data processing procedure is the same to <ref type=""bibr"" target=""#b8"">[Zhou et al., 2019]</ref>. For each video segment in the dataset, we u 2018]</ref>, BiM-STM+TempoAtnn <ref type=""bibr"" target=""#b7"">[Zhou et al., 2018]</ref> and ZhouGVD <ref type=""bibr"" target=""#b8"">[Zhou et al., 2019]</ref> on Grounded ActivityNet Captions Dataset to emove the hierarchical attention and replace it with the coarsegrain proposal attention proposed by <ref type=""bibr"" target=""#b8"">[Zhou et al., 2019]</ref>.</p><p>Table <ref type=""table"" target=""#tab_",1
"entional video description task that generates a human-like sentence to describe the video contents <ref type=""bibr"" target=""#b7"">[Zhou et al., 2018]</ref>, GVD has advantages of modelling the video b mance Comparisons</head><p>We compare HAST-Graph2Seq with the SOTA models, i.e., Masked Transformer <ref type=""bibr"" target=""#b7"">[Zhou et al., 2018]</ref>, BiM-STM+TempoAtnn <ref type=""bibr"" target="" i.e., Masked Transformer <ref type=""bibr"" target=""#b7"">[Zhou et al., 2018]</ref>, BiM-STM+TempoAtnn <ref type=""bibr"" target=""#b7"">[Zhou et al., 2018]</ref> and ZhouGVD <ref type=""bibr"" target=""#b8"">[Z",0
"based methods which model the regions with abundant semantic relations are introduced to this area. <ref type=""bibr"" target=""#b6"">[Yao et al., 2018]</ref>  </p></div> <div xmlns=""http://www.tei-c.org/",0
"es that separated in frames.</p><p>Borrowing the ideas of spatial-attention in image caption domain <ref type=""bibr"" target=""#b0"">[Anderson et al., 2018;</ref><ref type=""bibr"" target=""#b4"">Liu et al.,",0
"to generate the description of a video using the attention-based encoder-decoder like architectures <ref type=""bibr"" target=""#b5"">[Venugopalan et al., 2015;</ref><ref type=""bibr"" target=""#b5"">Xu et al ed encoder-decoder like architectures <ref type=""bibr"" target=""#b5"">[Venugopalan et al., 2015;</ref><ref type=""bibr"" target=""#b5"">Xu et al., 2018b;</ref><ref type=""bibr"" target=""#b4"">Liu et al., 2016] which learns a mapping between graph inputs to sequence outputs through attention-based mechanisms <ref type=""bibr"" target=""#b5"">[Xu et al., 2018a;</ref><ref type=""bibr"" target=""#b2"">Chen et al., 202",0
"ibr"" target=""#b0"">[Anderson et al., 2018;</ref><ref type=""bibr"" target=""#b4"">Liu et al., 2018;</ref><ref type=""bibr"" target=""#b3"">Li et al., 2019a]</ref>, many works model the video in both global vid",0
"understanding started attracting more attentions in some close related fields such as image caption <ref type=""bibr"" target=""#b4"">[Li et al., 2019b]</ref>. However, due to the complexity of video unde ibr"" target=""#b2"">[Krishna et al., 2017]</ref> dataset, we can train a semantic relation classifier <ref type=""bibr"" target=""#b4"">[Li et al., 2019b]</ref> on it. We adopt almost the same operation exc "" target=""#b5"">[Venugopalan et al., 2015;</ref><ref type=""bibr"" target=""#b5"">Xu et al., 2018b;</ref><ref type=""bibr"" target=""#b4"">Liu et al., 2016]</ref>. These methods are effective but they overlook patial-attention in image caption domain <ref type=""bibr"" target=""#b0"">[Anderson et al., 2018;</ref><ref type=""bibr"" target=""#b4"">Liu et al., 2018;</ref><ref type=""bibr"" target=""#b3"">Li et al., 2019a] {h 1 , h 2 , ..., h m } where v ∈ R n×d is the global feature extracted by a pre-trained 3D-ConvNet <ref type=""bibr"" target=""#b4"">[Tran et al., 2015]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ eo segment in the dataset, we uniformly sample 10 frames. And for each frame, we use a Faster R-CNN <ref type=""bibr"" target=""#b4"">[Ren et al., 2015]</ref> detector with ResNeXt-101 backbone to detect",0
"ce outputs through attention-based mechanisms <ref type=""bibr"" target=""#b5"">[Xu et al., 2018a;</ref><ref type=""bibr"" target=""#b2"">Chen et al., 2020;</ref><ref type=""bibr"" target=""#b2"">Gao et al., 2019 e=""bibr"" target=""#b5"">[Xu et al., 2018a;</ref><ref type=""bibr"" target=""#b2"">Chen et al., 2020;</ref><ref type=""bibr"" target=""#b2"">Gao et al., 2019]</ref>. However, since there is no explicit graph str ethod: Relation Graph. Since the region features are extracted by a pre-trained model trained on VG <ref type=""bibr"" target=""#b2"">[Krishna et al., 2017]</ref> dataset, we can train a semantic relation o detect 100 region proposals and extract the feature. The detector is pre-trained on Visual Genome <ref type=""bibr"" target=""#b2"">[Krishna et al., 2017]</ref>. Finally, for the video feature, the temp to aggregate the features of the nodes modeled by topology A.</p><p>Inspired by resnet architecture <ref type=""bibr"" target=""#b2"">[He et al., 2016]</ref>, we propose the basic module of our architectu",0
"nformation retrieval to capture the exact and soft matches between a query and a candidate document <ref type=""bibr"" target=""#b10"">[Xiong et al., 2017]</ref>. Specifically, we apply the basic BERT uni ors are disordered and independent from each other. Thus we adopt a RBF kernel aggregation function <ref type=""bibr"" target=""#b10"">[Xiong et al., 2017]</ref> to extract features about the accumulation",1
"N [Wu et al., 2019b]</ref>, relation-aware <ref type=""bibr"">GCN [Wu et al., 2019a]</ref> and VR-GCN <ref type=""bibr"" target=""#b12"">[Ye et al., 2019]</ref>.</p><p>Despite much effort taken on graph str",0
"#b15"">[Zhu et al., 2017]</ref> to recent emergent graph neural networks such as attention-based GCN <ref type=""bibr"" target=""#b11"">[Xu et al., 2019]</ref>, highway <ref type=""bibr"">GCN [Wu et al., 201 ings <ref type=""bibr"">[Wang et al., 2018;</ref><ref type=""bibr"" target=""#b9"">Wu et al., 2019a;</ref><ref type=""bibr"" target=""#b11"">Xu et al., 2019]</ref>. However, since different KGs are highly heter pe=""bibr"" target=""#b0"">[Cao et al., 2019;</ref><ref type=""bibr"" target=""#b9"">Wu et al., 2019a;</ref><ref type=""bibr"" target=""#b11"">Xu et al., 2019]</ref>, essentially, the GCNlike models still mix the of all their neighbors as existing works did <ref type=""bibr"" target=""#b9"">[Wu et al., 2019a;</ref><ref type=""bibr"" target=""#b11"">Xu et al., 2019]</ref>. This similar idea is widely used in informati",0
"ef type=""bibr"" target=""#b49"">(Rogers et al., 2020)</ref>, which specifically studies the BERT model <ref type=""bibr"" target=""#b11"">(Devlin et al., 2019)</ref>.</p><p>In this work, we adapt and extend In NLP, transformers are the backbone of state-of-the-art pre-trained language models such as BERT <ref type=""bibr"" target=""#b11"">(Devlin et al., 2019)</ref>. BERTology focuses on interpreting what t",1
"egories: interpreting the learned embeddings <ref type=""bibr"" target=""#b15"">(Ethayarajh, 2019;</ref><ref type=""bibr"" target=""#b66"">Wiedemann et al., 2019;</ref><ref type=""bibr"" target=""#b35"">Mickus et",0
"bibr"" target=""#b33"">Liu et al., 2019;</ref><ref type=""bibr"" target=""#b59"">Tenney et al., 2019;</ref><ref type=""bibr"" target=""#b23"">Htut et al., 2019;</ref><ref type=""bibr"" target=""#b21"">Hewitt &amp; M arget=""#b9"">(Clark et al., 2019;</ref><ref type=""bibr"" target=""#b64"">Vig &amp; Belinkov, 2019;</ref><ref type=""bibr"" target=""#b23"">Htut et al., 2019)</ref>. Depending on the task and model architectur",0
"t=""#b45"">(Rao et al., 2019)</ref>, which was pretrained on Pfam, a dataset of 31M protein sequences <ref type=""bibr"" target=""#b13"">(El-Gebali et al., 2019b)</ref>. We refer to this model as TapeBert.",0
"rained language models (e.g., ELMo <ref type=""bibr"" target=""#b29"">(Peters et al., 2018)</ref>, BERT <ref type=""bibr"" target=""#b5"">(Devlin et al., 2019)</ref>, XLnet <ref type=""bibr"" target=""#b45"">(Yan e achieved state-of-the-art performance in many popular NLP benchmarks with appropriate fine-tuning <ref type=""bibr"" target=""#b5"">(Devlin et al., 2019;</ref><ref type=""bibr"" target=""#b22"">Liu et al., entations of the fully supervised NER methods attain very close to the state-of-the-art performance <ref type=""bibr"" target=""#b5"">(Devlin et al., 2019;</ref><ref type=""bibr"" target=""#b19"">Limsopatham",1
"om distant supervision, our approach leverages the power of pre-trained language models (e.g., ELMo <ref type=""bibr"" target=""#b29"">(Peters et al., 2018)</ref>, BERT <ref type=""bibr"" target=""#b5"">(Devl",1
">(Peters et al., 2018)</ref>, BERT <ref type=""bibr"" target=""#b5"">(Devlin et al., 2019)</ref>, XLnet <ref type=""bibr"" target=""#b45"">(Yang et al., 2019)</ref>) which are particularly attractive to this ibr"" target=""#b5"">(Devlin et al., 2019;</ref><ref type=""bibr"" target=""#b22"">Liu et al., 2019b;</ref><ref type=""bibr"" target=""#b45"">Yang et al., 2019;</ref><ref type=""bibr"" target=""#b17"">Lan et al., 20",1
"l (HMM) <ref type=""bibr"" target=""#b46"">(Zhou and Su, 2002)</ref> and Conditional Random Field (CRF) <ref type=""bibr"" target=""#b15"">(Lafferty et al., 2001)</ref> based on hand-crafted features. To alle",0
"stic gradient-type algorithms, e.g., ADAM <ref type=""bibr"" target=""#b14"">(Kingma and Ba, 2014;</ref><ref type=""bibr"" target=""#b21"">Liu et al., 2020)</ref>. Following <ref type=""bibr"" target=""#b30"">Raf",0
"erest modeling <ref type=""bibr"" target=""#b12"">(Karatay and Karagoz, 2015)</ref>, question answering <ref type=""bibr"" target=""#b13"">(Khalid et al., 2008)</ref> and dialogue systems <ref type=""bibr"" tar",0
"ibr"" target=""#b26"">Miyato et al., 2018;</ref><ref type=""bibr"" target=""#b25"">Meng et al., 2018;</ref><ref type=""bibr"" target=""#b3"">Clark et al., 2018)</ref>. Different from distant supervision, these s",0
"close to the state-of-the-art performance <ref type=""bibr"" target=""#b5"">(Devlin et al., 2019;</ref><ref type=""bibr"" target=""#b19"">Limsopatham and Collier, 2016)</ref>. Our results are summarized as f",0
"is quite general, and can be naturally combined with other training techniques, e.g., mean teacher <ref type=""bibr"" target=""#b37"">(Tarvainen and Valpola, 2017)</ref> and virtual adversarial training oot"" target=""#foot_3"">4</ref>• For Ablation Study, we consider the following methods/tricks. (i) MT <ref type=""bibr"" target=""#b37"">(Tarvainen and Valpola, 2017)</ref> uses Mean Teacher method to avera work is related to low-resource NER.  <ref type=""bibr"" target=""#b33"">(Rosenberg et al., 2005;</ref><ref type=""bibr"" target=""#b37"">Tarvainen and Valpola, 2017;</ref><ref type=""bibr"" target=""#b26"">Miya",0
", some works adopt the partial annotation CRFs to consider all possible labels for unlabeled tokens <ref type=""bibr"" target=""#b44"">(Yang et al., 2018;</ref><ref type=""bibr"" target=""#b35"">Shang et al.,",0
"taset that consists of 2400 tweets (comprising 34k tokens) with 10 entity types. (iii) OntoNotes5.0 <ref type=""bibr"" target=""#b40"">(Weischedel et al., 2013</ref>) contains text documents from multiple",0
"tails</head><p>We first find potential entities by POS tagging obtained from POS tagger, e.g., NLTK <ref type=""bibr"" target=""#b23"">(Loper and Bird, 2002)</ref>. We then match these potential entities",0
"ledge extraction and is important to various downstream applications such as user interest modeling <ref type=""bibr"" target=""#b12"">(Karatay and Karagoz, 2015)</ref>, question answering <ref type=""bibr",0
"Matching Performance on Open-Domain <ref type=""bibr"" target=""#b34"">(Sang and De Meulder, 2003;</ref><ref type=""bibr"" target=""#b36"">Strauss et al., 2016)</ref> and Biomedical Domain NER Datasets <ref t",0
"get=""#b14"">15,</ref><ref type=""bibr"" target=""#b39"">40,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b28"">29]</ref> allow end-to-end differentable losses over data with arbitr for unsupervised losses, most work follows the semi-supervised setting for node classification from <ref type=""bibr"" target=""#b28"">[29]</ref>. For a complete introductions to the vast topic we refer i ider transductive GNNs that output a single embedding per node. Graph convolutional networks (GCNs) <ref type=""bibr"" target=""#b28"">[29]</ref> are simple yet effective <ref type=""bibr"" target=""#b50"">[5",1
"br"" target=""#b4"">[5]</ref>, however, a spectral relaxation of the problem can be solved efficiently <ref type=""bibr"" target=""#b36"">[37]</ref>. Let C ∈ 0, 1 n×k be the cluster assignment matrix and d b r iteration or Lanczos algorithm.</p><p>One can then obtain clusters by means of spectral bisection <ref type=""bibr"" target=""#b36"">[37]</ref> with iterative refinement akin to Kernighan-Lin algorithm",1
"l. Additionally, our M-GCN could serve as the ""graph-context"" head in semi-supervised architectures <ref type=""bibr"" target=""#b66"">[67]</ref>, allowing for stronger community-based constraints in thos",0
"=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b11"">12]</ref>, genomic feature discovery <ref type=""bibr"" target=""#b6"">[7]</ref>, anomaly detection <ref type=""bibr"" target=""#b43"">[44]</ref>",0
"s with the same degree distribution. While problems with the modularity metric have been identified <ref type=""bibr"" target=""#b21"">[22]</ref>, it remains one of the most commonly-used and eminently us",0
"have been axiomatic <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" target=""#b13"">14]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n",0
"as normalization.</p><p>In real networks, however, there is evidence against existence of good cuts <ref type=""bibr"" target=""#b31"">[32]</ref> in ground-truth communities. This can be explained by the sed with a lot of other items, so the effects discussed in <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b31"">32]</ref> are prohibiting good cuts. This corresponds to high values",0
"n <ref type=""bibr"" target=""#b44"">[45]</ref>, visualization <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b11"">12]</ref>, genomic feature discovery <ref type=""bibr"" target=""#b6"">[7",0
"ervised training is a desirable setting for clustering models. Works on supervised graph clustering <ref type=""bibr"" target=""#b65"">[66,</ref><ref type=""bibr"" target=""#b61"">62]</ref> are outside of the",0
"ns(features) is our baseline that only considers the feature data. We use the local Lloyd algorithm <ref type=""bibr"" target=""#b33"">[34]</ref> with the k-means++ seeding <ref type=""bibr"" target=""#b0"">[",0
"entation learning algorithms that use a contrastive loss have outperformed even supervised learning <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" targ rvation that a larger number of negative/positive examples in the objective leads to better results <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b15"">16]</ref>. The last two terms </ref>, or different views of the same scene <ref type=""bibr"" target=""#b32"">[33]</ref>. Chen et al. <ref type=""bibr"" target=""#b1"">[2]</ref> extensively study verious data augmentation methods. For lan br"" target=""#b22"">[23]</ref> and STL10 <ref type=""bibr"" target=""#b5"">[6]</ref>, we implement SimCLR <ref type=""bibr"" target=""#b1"">[2]</ref> with ResNet-50 <ref type=""bibr"" target=""#b14"">[15]</ref> as ef type=""bibr"" target=""#b18"">[19]</ref> with learning rate 0.001 and weight decay 1e − 6. Following <ref type=""bibr"" target=""#b1"">[2]</ref>, we set the temperature t = 0.5 and the dimension of the lat /ns/1.0""><head>B Experiment Details</head><p>Cifar10 and STL10 We adopt PyTorch to implement SimCLR <ref type=""bibr"" target=""#b1"">[2]</ref> with Resnet-50 <ref type=""bibr"" target=""#b14"">[15]</ref> as",1
"ure the representation quality. All of these works sample negative examples from p(x). Arora et al. <ref type=""bibr"" target=""#b0"">[1]</ref> theoretically analyze the effect of contrastive representati erparameter. Without loss of generality, we set t = 1 for all theoretical results.</p><p>Similar to <ref type=""bibr"" target=""#b0"">[1]</ref>, we assume an underlying set of discrete latent classes C th f ) = inf W∈R K×d L Softmax (T , W f ).<label>(10)</label></formula><p>In line with the approach of <ref type=""bibr"" target=""#b0"">[1]</ref> we analyze the supervised loss of a mean classifier <ref typ commonly the case. The dependence on on N and T in Theorem 5 is roughly equivalent to the result in <ref type=""bibr"" target=""#b0"">[1]</ref>, but the two bounds are not directly comparable since the pr .</formula><p>In order to derive our bound we will exploit a concentration of measure result due to <ref type=""bibr"" target=""#b0"">[1]</ref>. They consider an objective of the form</p><formula xml:id=""",1
"the language domain <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b24"">25]</ref>.</p><p>Recently, self-supervised representation learning al",0
"resting avenue of future work to adopt our debiased objective to a semi-supervised learning setting <ref type=""bibr"" target=""#b36"">[37]</ref> where true positive samples are accessible.</p></div> <div",0
"assification (TREC) <ref type=""bibr"" target=""#b33"">[34]</ref>, and paraphrase identification (MSRP) <ref type=""bibr"" target=""#b7"">[8]</ref>. Our experimental settings follow those for quick-thought (Q",0
"bjectivity classification (SUBJ) <ref type=""bibr"" target=""#b27"">[28]</ref>, opinion polarity (MPQA) <ref type=""bibr"" target=""#b35"">[36]</ref>, question type classification (TREC) <ref type=""bibr"" targ",0
"outlier detection <ref type=""bibr"" target=""#b9"">[10]</ref><ref type=""bibr"" target=""#b10"">[11]</ref><ref type=""bibr"" target=""#b11"">[12]</ref>. Our approach is related to unbiased PU learning, where th",0
"ghted appropriately <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b21"">22]</ref>. While these works focus on zero-one losses, we here addres",0
"ategy of obtaining positive pairs. Examples in computer vision include random cropping and flipping <ref type=""bibr"" target=""#b26"">[27]</ref>, or different views of the same scene <ref type=""bibr"" tar",0
"rget=""#b1"">[2,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b23"">24]</ref>. The key idea of contrastive learning is to contrast semant b1"">[2]</ref> extensively study verious data augmentation methods. For language, Logeswaran and Lee <ref type=""bibr"" target=""#b23"">[24]</ref> treat the context sentences as positive samples to efficie r"" target=""#b7"">[8]</ref>. Our experimental settings follow those for quick-thought (QT) vectors in <ref type=""bibr"" target=""#b23"">[24]</ref>.</p><p>In contrast to vision tasks, positive pairs here ar s/1.0""><head>Sentence Embedding</head><p>We adopt the official code 3 of quick-thought (QT) vectors <ref type=""bibr"" target=""#b23"">[24]</ref>. To implement the debiased objective, we only modify the """,0
"of a single device.</p><p>Recently, pipeline parallelism <ref type=""bibr"" target=""#b9"">[10]</ref>- <ref type=""bibr"" target=""#b11"">[12]</ref> has been proposed as a promising approach for training lar een those nodes during backpropagation. The other category is asynchronous(async) pipeline training <ref type=""bibr"" target=""#b11"">[12]</ref>. This manner inserts mini-batches into pipeline continuous lism, and hybrid approaches combining both. Current state-of-theart pipeline partitioning algorithm <ref type=""bibr"" target=""#b11"">[12]</ref> is not able to be applied to synchronous training effectiv evice assignment affects communication efficiency and computing resource utilization. Previous work <ref type=""bibr"" target=""#b11"">[12]</ref> uses hierarchical planning and works well for asynchronous D. Contributions over previous work</head><p>Previous works on pipeline planning includes PipeDream <ref type=""bibr"" target=""#b11"">[12]</ref> (for asynchronous training) and torchgpipe <ref type=""bibr it the micro-batch further into 2 even slices, and assign each to a device. An alternative approach <ref type=""bibr"" target=""#b11"">[12]</ref> (Fig. <ref type=""figure"" target=""#fig_6"">8(b)</ref>) is no",1
"udes PipeDream <ref type=""bibr"" target=""#b11"">[12]</ref> (for asynchronous training) and torchgpipe <ref type=""bibr"" target=""#b22"">[23]</ref>, a community implementation of GPipe <ref type=""bibr"" targ",0
"tion of GPipe <ref type=""bibr"" target=""#b9"">[10]</ref> which uses ""Block Partitioning of Sequences"" <ref type=""bibr"" target=""#b23"">[24]</ref>. Both aim to balance the workload across all GPUs. While t",0
"the model scale up to the limit of modern AI hardware. Many state-of-the-art DNN models (e.g., NLP <ref type=""bibr"" target=""#b1"">[2]</ref>, Internet scale E-commerce search/recommendation systems <re",0
"optimizing pipeline parallelism for synchronous training <ref type=""bibr"" target=""#b9"">[10]</ref>, <ref type=""bibr"" target=""#b10"">[11]</ref>. This approach requires necessary gradients synchronizatio llelism.</p><p>Pipeline parallelism. Pipeline Parallelism <ref type=""bibr"" target=""#b9"">[10]</ref>, <ref type=""bibr"" target=""#b10"">[11]</ref>, <ref type=""bibr"" target=""#b13"">[14]</ref>, <ref type=""bib",0
"quence Models for Peptide Binding Prediction</head><p>The approach builds on the UDSMProt-framework <ref type=""bibr"" target=""#b11"">[12]</ref> and related work in natural language processing <ref type= with a concat pooling layer and two fully connected layers. The setup closely follows that used in <ref type=""bibr"" target=""#b11"">[12]</ref>, where protein properties were predicted. The smaller data of the number of hidden units from 1150 to 64 and of the embedding size from 400 to 50. Similar to <ref type=""bibr"" target=""#b11"">[12]</ref>, the training procedure included 1-cycle learning rate sch e potential of unlabeled peptide data in order to observe similar improvements as seen for proteins <ref type=""bibr"" target=""#b11"">[12]</ref> in particular for small datasets.  Turning to MHC Class II uarcy of 0.137, which is is considerably lower than the accuracy of 0.41 reported in the literature <ref type=""bibr"" target=""#b11"">[12]</ref>. This effect is a direct consequence of the considerably s",1
"ffinity dataset from the IEDB site<ref type=""foot"" target=""#foot_1"">2</ref> based on the dataset by <ref type=""bibr"" target=""#b17"">[18]</ref>. We used it to train our prediction tools.</p><p>e) IEDB16",0
"e measurements are not available.</p><p>c) IEDB16 I: is made up of an IEDB snapshot of October 2016 <ref type=""bibr"" target=""#b1"">[2]</ref>. It was filtered for quantitative measurements with IC 50 ≤ We used it to train our prediction tools.</p><p>e) IEDB16 II: is a MHC II test dataset provided by <ref type=""bibr"" target=""#b1"">[2]</ref> from the same IEDB snapshot as the MHC I IEDB16 I test set a of USMPep. We compare to literature results that were evaluated in a recent comprehensive benchmark <ref type=""bibr"" target=""#b1"">[2]</ref> on this dataset. This benchmark includes evaluation metrics ype=""figure"" target=""#fig_0"">1</ref>, we show overall AUC ROC and overall Spearman r as reported by <ref type=""bibr"" target=""#b1"">[2]</ref> for the latest versions of the Table <ref type=""table"" targe y mean AUC ROC and mean Spearman r compared to results provided in the data repository accompanying <ref type=""bibr"" target=""#b1"">[2]</ref>. For the latter error bars could not be calculated for the l similar (within error bars) to the result of MHCFlurry, the best-performing method in the benchmark <ref type=""bibr"" target=""#b1"">[2]</ref>. This result stresses the claims of excellent prediction per",0
"ial sub task for neoantigen identification for practical realizations of personalized immunotherapy <ref type=""bibr"" target=""#b0"">[1]</ref>.</p><p>The MHC binding prediction is a well-established prob",0
"f stable feature distillation, which consists of a deep global balancing regression (DGBR) algorithm<ref type=""bibr"" target=""#b12"">[13]</ref>, a teacher network and a student network. The DGBR algorit ner is another promising direction.</p><p>Feature-Based Module. The current stable feature approach <ref type=""bibr"" target=""#b12"">[13]</ref> needs much time and computing resources. For implementing",1
">Knowledge Distillation</head><p>Hinton's work first proposes the concept of knowledge distillation <ref type=""bibr"" target=""#b9"">[10]</ref>. By introducing soft-targets related to teacher networks as eep the same output as the teacher network on a soft label will result in a significant improvement <ref type=""bibr"" target=""#b9"">[10]</ref>. We follow a similar setup in this strategy. Note that in t ef type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b26"">27]</ref> and model structure <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b21"">22]</ref>. Each method is bas",1
"><ref type=""bibr"" target=""#b5"">6]</ref>, previous model bias <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b16"">17]</ref> and position bias < niform data directly <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b22"">23]</ref>. In this paper, we f a recommender system, and that explicitly handling of the biases may help improve the performance <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" ta f>.</p><p>A recent work has shown that a uniform data can alleviate the previous model bias problem <ref type=""bibr"" target=""#b15"">[16]</ref>. But the uniform data is always few and expensive to colle us on how to solve the bias problems in a recommender system with a uniform data. Along the line of <ref type=""bibr"" target=""#b15"">[16]</ref>, we conduct empirical studies on a real advertising system /p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3"">MOTIVATION</head><p>In a recent work <ref type=""bibr"" target=""#b15"">[16]</ref>, it is shown that a uniform (i.e., unbiased) data can alle",1
"adaptive, collective and integrative) in transfer learning <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b18"">19]</ref>.</p><p>In addition, we must keep in mind that the different",0
"25]</ref> and heuristic-based approaches. The former mainly uses the inverse propensity score (IPS) <ref type=""bibr"" target=""#b23"">[24]</ref> and the counterfactual risk minimization (CRM) principle < such as imputation model learning <ref type=""bibr"" target=""#b32"">[33]</ref>, propensity computation <ref type=""bibr"" target=""#b23"">[24]</ref> and modeling with uniform data directly <ref type=""bibr"" t esentative counterfactual-based recommendation method as the second low-rank baseline, i.e., IPS-MF <ref type=""bibr"" target=""#b23"">[24]</ref>. Note that we estimate the propensity scores via the naïve arget=""#b27"">28]</ref>. IPS is one of the most popular counterfactual approaches for recommendation <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b30"">31]</ref>, where each sample or learning an imputation model and its variants. Sample-based distillation includes the IPS method <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b30"">31]</ref> and other approach ick and rate items as they wish. This can be considered as a stochastic logging policy by following <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b30"">31]</ref>, and thus the user",0
"ead n=""2.2"">Counterfactual Learning for Ranking</head><p>For learning-to-rank tasks, Agarwal et al. <ref type=""bibr"" target=""#b1"">[2]</ref> provides a general and theoretically rigorous framework with",0
"><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b16"">17]</ref> and position bias <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b27"">28]</ref>. Previous studies ha .e., SVM PropDCG and DeepPropDCG. Some position bias estimation methods for ranking are proposed in <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b27"">28]</ref>. IPS is one of the m",0
"nsity computation <ref type=""bibr"" target=""#b23"">[24]</ref> and modeling with uniform data directly <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" targ idation set (𝑆 𝑣𝑎 ), and the rest as test set (𝑆 𝑡𝑒 ). Following the settings of the previous works <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b32"">33]</ref>, we employ two evalu istillation can be realized.</p><p>• Causal Embedding Strategy (CausE). The causal embedding method <ref type=""bibr"" target=""#b4"">[5]</ref> first considers the scenario of training 𝑀 𝑐 and 𝑀 𝑡 simulta",0
"o solve the bias problems of recommender systems can be classified as counterfactual learning-based <ref type=""bibr"" target=""#b24"">[25]</ref> and heuristic-based approaches. The former mainly uses the <ref type=""bibr"" target=""#b23"">[24]</ref> and the counterfactual risk minimization (CRM) principle <ref type=""bibr"" target=""#b24"">[25]</ref>, while the latter mainly makes certain assumptions about t",0
"knowledge distillation and recommender systems has also attracted the attention of the researchers <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b29"">30,</ref><ref type=""bibr"" ta",0
"ibr"" target=""#b3"">[4]</ref>) and different forms of knowledge (e.g., alignment of the hidden layers <ref type=""bibr"" target=""#b21"">[22]</ref> or the relation between the hidden layers <ref type=""bibr"" ss. • Hint Strategy. Hint refers to the hidden layer in a neural network, also known as feature map <ref type=""bibr"" target=""#b21"">[22]</ref>. They contain higher-order non-linear relations between us ref type=""bibr"" target=""#b26"">27]</ref> and model structure <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b21"">22]</ref>. Each method is based on different concerns to mine the pot",0
"e hidden layers <ref type=""bibr"" target=""#b21"">[22]</ref> or the relation between the hidden layers <ref type=""bibr"" target=""#b31"">[32]</ref>). Some recent works are no longer limited to model structu",0
"rget=""#b4"">[5,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b22"">23]</ref>. In this paper, we would like to study methods for better u",0
"of the objective function, the training of student networks is guided to achieve knowledge transfer <ref type=""bibr"" target=""#b17"">[18]</ref>. A series of followup works develop different distillation ance, feature and model) and strategies (adaptive, collective and integrative) in transfer learning <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b18"">19]</ref>.</p><p>In addition",0
"t works are no longer limited to model structure, but considers sample-based knowledge distillation <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b26"">27]</ref>. In this paper, we tudy rather than the past knowledge distillation approaches such as considering the level of sample <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b26"">27]</ref> and model structur",0
"hes have been proposed <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" tar ype=""bibr"" target=""#b23"">[24]</ref>. The explicit approaches <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" tar to solve this problem <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" tar tly leverage subtopics to determine the diversity of results <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" tar target=""#b5"">6,</ref><ref type=""bibr"" target=""#b8"">9]</ref> and supervised approaches such as DSSA <ref type=""bibr"" target=""#b11"">[12]</ref>.</p><p>Studies have shown that supervised approaches <ref xplicit method calculating the distribution by counting the relevant document of the subtopic. DSSA <ref type=""bibr"" target=""#b11"">[12]</ref> introduces the machine learning method into explicit appro ent 𝐶 to fool the discriminator. So it also needs a score function. In our method we adapt the DSSA <ref type=""bibr"" target=""#b11"">[12]</ref> score function for the generator. We introduce the score f <p>In the training process, we first train R-LTR <ref type=""bibr"" target=""#b25"">[26]</ref> and DSSA <ref type=""bibr"" target=""#b11"">[12]</ref> respectively using MLE loss in both ways. It is because ou br"" target=""#b5"">[6]</ref>, R-LTR-NTN, PAMM-NTN <ref type=""bibr"" target=""#b23"">[24]</ref>, and DSSA <ref type=""bibr"" target=""#b11"">[12]</ref> as supervised baseline methods. Top 20 results of Lemur ar b7"">[8]</ref> as the RNN cell for comparison. In our experiments, we conduct the list-pairwise loss <ref type=""bibr"" target=""#b11"">[12]</ref> to train DSSA method. The feature vector 1 http://playbigd DSSA <ref type=""bibr"" target=""#b11"">[12]</ref>.</p><p>Studies have shown that supervised approaches <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" ta",1
"ll a challenge for training diversification models.</p><p>To tackle this problem, inspired by IRGAN <ref type=""bibr"" target=""#b18"">[19]</ref>, we introduce Generative Adversarial Network (GAN) <ref ty e Carlo search. It is also used in the traditional information retrieval area, Wang proposed IR-GAN <ref type=""bibr"" target=""#b18"">[19]</ref> which consists of two information retrieval models in it. 𝑞, 𝑆)) 1 + exp(𝑓 𝜙 (𝑑 |𝑞, 𝑆)) .<label>(7)</label></formula><p>Please note that different from IRGAN <ref type=""bibr"" target=""#b18"">[19]</ref>, DVGAN-doc has an additional component 𝑆 to represent the , it is difficult to calculate the generator gradient due to its discrete nature. Inspired by IRGAN <ref type=""bibr"" target=""#b18"">[19]</ref>, we generate negative document set 𝐷 ′ by selecting the do",1
"is large enough but the quality of it depends on some hyper-parameters such as the range of 𝛼-nDCG <ref type=""bibr"" target=""#b3"">[4]</ref> of negative ranking samples, which may cause the model hard that 𝑆 is ranked. So the sampler also needs to re-rank 𝑆 by the diversification metric like 𝛼-nDCG <ref type=""bibr"" target=""#b3"">[4]</ref>. In practice, half of the selected document ranking 𝑆 is sam <ref type=""bibr"">[2-4, 20, 21]</ref>, we use ERR-IA <ref type=""bibr"" target=""#b1"">[2]</ref>, 𝛼-NDCG <ref type=""bibr"" target=""#b3"">[4]</ref>, and NRBP <ref type=""bibr"" target=""#b2"">[3]</ref> as our div train. We use 5-fold cross validation to tune the parameters in all experiments based on 𝛼-nDCG@20 <ref type=""bibr"" target=""#b3"">[4]</ref>. A brief introduction to these baselines is as follows.</p><",0
"arget=""#b5"">6,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b22"">[23]</ref><ref type=""bibr"" ta rget=""#b5"">[6,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b24"">25]</ref> stress the relevanc arget=""#b5"">6,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b22"">[23]</ref><ref type=""bibr"" ta lso be categorized into heuristic approaches such as xQuAD <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b17"">18]</ref> and PM2 <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""b e heuristic approaches <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b17"">18]</ref> by learning an optimized ranking function. However, the lar rget=""#b5"">[6,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b17"">18]</ref>. Most explicit approaches focus on the subtopic coverage of relevance to the query 𝑞 and the 𝑆 sub function reflects the 𝑑's relevance to the subtopics. xQuAD <ref type=""bibr"" target=""#b17"">[18]</ref> is one of the representative methods of unsupervised expli fication methods. We use Lemur as our non-diversified baseline method. We use xQuAD, TxQuAD, HxQuAD <ref type=""bibr"" target=""#b17"">[18]</ref>, PM2 <ref type=""bibr"" target=""#b5"">[6]</ref>, TPM2 <ref ty",0
"/ref>) using Plackett-Luce model and the 𝐸 is the diversification metrics such as 𝛼-NDCG and ERR-IA <ref type=""bibr"" target=""#b1"">[2]</ref>. The form of ℑ is inspired by PAMM <ref type=""bibr"" target="" trics</head><p>Among all the evaluation metrics <ref type=""bibr"">[2-4, 20, 21]</ref>, we use ERR-IA <ref type=""bibr"" target=""#b1"">[2]</ref>, 𝛼-NDCG <ref type=""bibr"" target=""#b3"">[4]</ref>, and NRBP <r gs but makes it hard for generator to imitate the real distribution of data because it is too ideal <ref type=""bibr"" target=""#b1"">(2)</ref>. random sampling makes it easy to imitate for generator but",0
"b17"">18]</ref> and PM2 <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b8"">9]</ref> and supervised approaches such as DSSA <ref type=""bibr"" targe en at subtopic level. We use google query suggestions as subtopics, which are released by Hu et al. <ref type=""bibr"" target=""#b8"">[9]</ref> on their website 1 . we only use the first level subtopics a PM2 <ref type=""bibr"" target=""#b5"">[6]</ref>, TPM2 <ref type=""bibr"" target=""#b4"">[5]</ref>, and HPM2 <ref type=""bibr"" target=""#b8"">[9]</ref> as our unsupervised baseline methods. We use ListMLE <ref ty",0
"d HPM2 <ref type=""bibr"" target=""#b8"">[9]</ref> as our unsupervised baseline methods. We use ListMLE <ref type=""bibr"" target=""#b21"">[22]</ref>, R-LTR <ref type=""bibr"" target=""#b25"">[26]</ref>, PAMM <re",0
"e negative training samples with higher quality. In the personalized search area, Lu proposed PSGAN <ref type=""bibr"" target=""#b14"">[15]</ref> inspired by IRGAN. Our framework is inspired by the former",0
"• , 𝑑 𝑡 −1 in convenience of representation. In spite of the kinds of RNN(in our model, we use LSTM <ref type=""bibr"" target=""#b7"">[8]</ref>), we use 𝐻 to denote the RNN cell and ℎ 𝑡 to denote the hidd ensor slices is tuned from 1 to 10.</p><p>DSSA. DSSA is the supervised explicit method. We use LSTM <ref type=""bibr"" target=""#b7"">[8]</ref> as the RNN cell for comparison. In our experiments, we condu",0
"rations are used to model the sequential document selection process in search result diversi cation <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b34"">35]</ref> and multi-page sea",1
">, pairwise approaches <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b15"">16]</ref>, and listwise approaches <ref type=""bibr"" target=""#b1"">[2,< tion. We compared the proposed PPG to the traditional learning to rank baselines, including RankSVM <ref type=""bibr"" target=""#b15"">[16]</ref>, RankNet <ref type=""bibr"" target=""#b1"">[2]</ref>, ListNet",0
"as a dual-agent stochastic game, on the basis of partially observed Markov decision process (POMDP) <ref type=""bibr"" target=""#b21"">[22]</ref>. In <ref type=""bibr"" target=""#b41"">[42]</ref>, a log-based",0
"type=""bibr"" target=""#b15"">16]</ref>, and listwise approaches <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b35"">36]</ref>. Speci cally, the pai <ref type=""bibr"" target=""#b15"">[16]</ref>, RankNet <ref type=""bibr"" target=""#b1"">[2]</ref>, ListNet <ref type=""bibr"" target=""#b3"">[4]</ref>, AdaRank <ref type=""bibr"" target=""#b35"">[36]</ref>, and MDPR",0
"directly incorporate multi-hop neighborhood information of a node without explicit message-passing <ref type=""bibr"" target=""#b32"">[33]</ref>. Intuitively, propagation based on personalized PageRank c here the node influence decays exponentially with each layer. However, as proposed, Klicpera et al. <ref type=""bibr"" target=""#b32"">[33]</ref>'s approach does not easily scale to large graphs since it ng that can reduce predictive performance.</p><p>To tackle both of these challenges Klicpera et al. <ref type=""bibr"" target=""#b32"">[33]</ref> suggest decoupling the feature transformation from the pro instead. Unfortunately, even a moderate number of power iteration evaluations (e.g. Klicpera et al. <ref type=""bibr"" target=""#b32"">[33]</ref> used K = 10 to achieve a good approximation) is prohibitiv ency of the subsequent learning step. Both of these approaches are a special case of the PPNP model <ref type=""bibr"" target=""#b32"">[33]</ref> which experimentally shows higher classification performan . In addition to the two scalable baselines, we also evaluate how PPRGo compares to the APPNP model <ref type=""bibr"" target=""#b32"">[33]</ref> which we build upon. The results are summarized in Table < ich experimentally shows higher classification performance <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b32"">33]</ref>.</p><p>Approximating PageRank. Recent approaches combine ba",1
"ight decay to 10 −4 . We train for 200 epochs using a learning rate of 0.005 and the Adam optimizer <ref type=""bibr"" target=""#b30"">[31]</ref> with a batch size of 512. To achieve a consistent setup ac",0
"graph classification <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b36"">37]</ref>. The success of GNN target=""#b1"">2,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b31"">32,</ref><ref type=""bibr"" tar ny proposed GNN models can be analyzed using the messagepassing framework proposed by Gilmer et al. <ref type=""bibr"" target=""#b21"">[22]</ref> or other similar frameworks <ref type=""bibr"" target=""#b5"">",0
"se methods to larger graphs for use in real-world problems <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" tar roposed to improve the efficiency of graph neural networks <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" tar e benchmark datasets <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" tar ef> directly sample the receptive field for each layer using importance sampling, while Chen et al. <ref type=""bibr"" target=""#b13"">[14]</ref> use the historical activations of the nodes as a control v ter-GCN report significant speedup over FastGCN <ref type=""bibr"" target=""#b12"">[13]</ref> and VRGCN <ref type=""bibr"" target=""#b13"">[14]</ref> we omit these models from our comparison. We run the exper",0
"et=""#b24"">[25,</ref><ref type=""bibr"" target=""#b31"">32,</ref><ref type=""bibr"" target=""#b43"">44,</ref><ref type=""bibr"" target=""#b54"">55]</ref> to community detection and graph classification <ref type=""",0
"<ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b30"">31]</ref> or feature matrix <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b31"">32]</ref>) are not compatibl denoising autoencoder to disturb the structure information. To build a symmetric graph autoencoder, <ref type=""bibr"" target=""#b23"">[24]</ref> proposes Laplacian sharpening as the counterpart of Laplac ing the latent representations to match a prior distribution for robust node embeddings.</p><p>GALA <ref type=""bibr"" target=""#b23"">[24]</ref> proposes a symmetric graph convolutional autoencoder recov",1
"optimal choice.</p><p>It has been proved that the range of Laplacian eigenvalues is between 0 and 2 <ref type=""bibr"" target=""#b6"">[7]</ref>, hence GCN filter is not low-pass in the (1, 2] interval. So",0
"e encoder, we train the MLP encoder for 400 epochs with a 0.001 learning rate by the Adam optimizer <ref type=""bibr"" target=""#b13"">[14]</ref>. The encoder consists of a single 500-dimensional embeddin",0
"he learned node embeddings, we visualize the node representations in 2D space using t-SNE algorithm <ref type=""bibr"" target=""#b28"">[29]</ref>. The figures are shown in Figure <ref type=""figure"" target",0
"attributes/features and are widely applied to represent network-structured data in social networks <ref type=""bibr"" target=""#b11"">[12]</ref>, citation networks <ref type=""bibr"" target=""#b15"">[16]</re",0
"tworks (BGNNs). The proposed BGNN incorporates a random graph generative model based on nodecopying <ref type=""bibr"" target=""#b22"">[24]</ref>. The node-copying model can be used to produce sample grap type=""bibr"" target=""#b21"">[23]</ref> uses a non-parametric model for the graph generative model and <ref type=""bibr"" target=""#b22"">[24]</ref> proposes a node copying model to achieve flexibility in th rnative, we use a more general generative model for graphs based on copying nodes, as introduced in <ref type=""bibr"" target=""#b22"">[24]</ref>. We demonstrate in the following sections that this model setting.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.2"">Node Copying</head><p>In <ref type=""bibr"" target=""#b22"">[24]</ref>, Pal et al. introduce the node copying model for 𝑝 (G). Sa etworks (BGNNs). The proposed BGNN incorporates a random graph generative model based on nodecopying<ref type=""bibr"" target=""#b22"">[24]</ref>. The node-copying model can be used to produce sample grap ode classification when there are very few training labels <ref type=""bibr"" target=""#b21"">[23,</ref><ref type=""bibr"" target=""#b22"">24,</ref><ref type=""bibr"" target=""#b28"">30,</ref><ref type=""bibr"" tar node classification when there are very few training labels<ref type=""bibr"" target=""#b21"">[23,</ref><ref type=""bibr"" target=""#b22"">24,</ref><ref type=""bibr"" target=""#b28"">30,</ref><ref type=""bibr"" tar s. These limitations were addressed in the follow-up works <ref type=""bibr"" target=""#b21"">[23,</ref><ref type=""bibr"" target=""#b22"">24]</ref>, where <ref type=""bibr"" target=""#b21"">[23]</ref> uses a non",1
"ion bipartite graph. GNNs have also been employed for the specific task of social recommendation in <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b26"">28,</ref><ref type=""bibr"" targ Θ ||Θ|| 2 (13)</formula><p>Equation <ref type=""bibr"" target=""#b10"">(11)</ref>, in conjunction with <ref type=""bibr"" target=""#b6"">(7)</ref>, illustrates that our final prediction of a ranking involves",0
"=""#b16"">[18]</ref> models achieved the best performance in Netflix contest. MF models (such as pLAS <ref type=""bibr"" target=""#b13"">[15]</ref>, MF <ref type=""bibr"" target=""#b16"">[18]</ref> and SVD++ <r raction data. Latent factor models are common, such as probabilistic Latent Semantic Analysis (pLAS <ref type=""bibr"" target=""#b13"">[15]</ref>), Matrix Factorization (MF <ref type=""bibr"" target=""#b16"">",0
"ew training labels <ref type=""bibr"" target=""#b21"">[23,</ref><ref type=""bibr"" target=""#b22"">24,</ref><ref type=""bibr"" target=""#b28"">30,</ref><ref type=""bibr"" target=""#b37"">39]</ref>.</p><p>In this pape few training labels<ref type=""bibr"" target=""#b21"">[23,</ref><ref type=""bibr"" target=""#b22"">24,</ref><ref type=""bibr"" target=""#b28"">30,</ref><ref type=""bibr"" target=""#b37"">39]</ref>.In this paper, we p",0
"es and item features. More recently, deep learning models ( <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b11"">13]</ref>), which can learn more complex non-linear relationships bet been incorporated into collaborative filtering architectures <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b11"">13]</ref>. These use a combination of fully-connected layers, convolu arameters that maximize 𝑝 (Θ|{&gt; 𝑢 } 𝐷 𝑆 , G). This latter maximization is the same as BPR-OPT in <ref type=""bibr"" target=""#b11"">(13)</ref>, as outlined in Section 3.3, but the inclusion of G indica ) Classical collaborative filtering methods: BPRMF <ref type=""bibr"" target=""#b24"">[26]</ref>, NeuMF <ref type=""bibr"" target=""#b11"">[13]</ref> 2) Graph neural network-based CF methods: GC-MC <ref type= general learning framework for personalized ranking recommendation using implicit feedback. • NeuMF <ref type=""bibr"" target=""#b11"">[13]</ref>: NeuMF replaces the inner product with an MLP to learn the",0
"tive fields. Several recent works attribute this performance degradation to the oversmoothing issue <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" targ nerated by multiple GCN layers, like 6 layers, are very difficult to be separated.  Several studies <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b14"">15]</ref> attribute this perfo filter on the spectral domain, thus deriving smoothing features across a graph. Another recent work <ref type=""bibr"" target=""#b2"">[3]</ref> verify that smoothing is the nature of most typical graph co shallow architectures. A smoothness regularizer term and adaptive edge optimization are proposed in <ref type=""bibr"" target=""#b2"">[3]</ref> to relieve the over-smoothing problem. Jumping Knowledge Net",1
"bute this performance degradation to the oversmoothing issue <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b32"">33]</ref>, which states that descriptions of the over-smoothing issue simplify the assumption of non-linear activation function <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b32"">33]</ref> or make approximat , which means that representations of nodes converge to indistinguishable limits. To our knowledge, <ref type=""bibr"" target=""#b14"">[15]</ref> is the first attempt to demystify the over-smoothing issue whole graph when the number of training nodes is limited under a semi-supervised learning setting. <ref type=""bibr"" target=""#b14"">[15]</ref> applies co-training and self-training to overcome the limi tations has a slight downward trend as the number of propagation iterations increases. According to <ref type=""bibr"" target=""#b14"">[15]</ref>, the node representations suffering from the oversmoothing ervation when building very deep graph neural networks, which aligns with the over-smoothing issue. <ref type=""bibr"" target=""#b14"">[15]</ref> and <ref type=""bibr"" target=""#b32"">[33]</ref> study the ov layers, are very difficult to be separated.  Several studies <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b14"">15]</ref> attribute this performance degradation phenomenon to the ov",1
"ral networks. Great successes have been achieved for many applications, such as node classification <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target",0
", graph classification <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" tar",0
"ef type=""bibr"" target=""#b32"">33]</ref>, graph classification <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" targe",0
"<ref type=""bibr"" target=""#b37"">38]</ref> and link prediction <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" target=""#b36"">37]</ref>. Graph convolutions",0
"target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" tar features at different ranges rather than to capture equally smoothed representations for all nodes. <ref type=""bibr"" target=""#b11"">[12]</ref> utilizes the relationship between GCN and PageRank <ref ty obabilities. Notably, the separation of transformation and propagation processes is also adopted in <ref type=""bibr"" target=""#b11"">[12]</ref> and <ref type=""bibr"" target=""#b30"">[31]</ref> but for the ref type=""bibr"" target=""#b20"">[21]</ref>, Graph-SAGE <ref type=""bibr"" target=""#b8"">[9]</ref>, APPNP <ref type=""bibr"" target=""#b11"">[12]</ref>, and SGC <ref type=""bibr"" target=""#b30"">[31]</ref>. We aim",0
"vided in Appendix A.7. We implemented our proposed DAGNN and some necessary baselines using Pytorch <ref type=""bibr"" target=""#b23"">[24]</ref> and Pytorch Geometric <ref type=""bibr"" target=""#b4"">[5]</r",0
"field, like 200-hop, is adopted.  According to the Perron-Frobenius Theorem for Primitive Matrices <ref type=""bibr"" target=""#b26"">[27]</ref>, there exists an eigenvalue r for an n × n non-negative pr",0
"arget=""#b8"">9,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b29"">30,</ref><ref type=""bibr"" tar aph Attention Network(GAT) <ref type=""bibr"" target=""#b29"">[30]</ref>, Mixture Model Network (MoNet) <ref type=""bibr"" target=""#b20"">[21]</ref>, Graph-SAGE <ref type=""bibr"" target=""#b8"">[9]</ref>, APPNP",0
"cal applications pertaining to fairness, privacy, and safety <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b39"">40]</ref>. For example, we can train a GNN model to predict the effec on interpreting GNNs at the model-level. The existing study <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b39"">40]</ref> only provides example-level explanations for graph models. tudies focusing on the interpretability of deep graph models <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b39"">40]</ref>. The recent GNN interpretation tool GNN Explainer <ref type have no baseline to compare with. Note that existing studies <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b39"">40]</ref> only focus on interpreting GNNs at example-level while igno [4,</ref><ref type=""bibr"" target=""#b39"">40]</ref>. The recent GNN interpretation tool GNN Explainer <ref type=""bibr"" target=""#b39"">[40]</ref> proposes to explain deep graph models at the example-level",1
"and obtained the state-of-the-art performance on different graph tasks, such as node classification <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b36"">37]</ref>, graph classificat nal graph is generated based on G t +1 until termination and then evaluated by f (•) using Equation <ref type=""bibr"" target=""#b10"">(11)</ref>. Then the evaluations for m final graphs are averaged to s",0
"=""bibr"" target=""#b13"">[14]</ref>, Junction Tree VAE <ref type=""bibr"" target=""#b16"">[17]</ref>, DGMG <ref type=""bibr"" target=""#b21"">[22]</ref>, and Graph Convolutional Policy Network (GCPN) <ref type=""",0
"ccessful graph generation models, such as GraphGAN <ref type=""bibr"" target=""#b37"">[38]</ref>, ORGAN <ref type=""bibr"" target=""#b13"">[14]</ref>, Junction Tree VAE <ref type=""bibr"" target=""#b16"">[17]</re",0
"f there is any cycle existing in the graph. The graphs are obtained using Networkx software package <ref type=""bibr"" target=""#b14"">[15]</ref>. The first class refers to cyclic graphs, including grid-l",0
"s reported that these numerical errors brought about huge reputation risk, and even economic losses <ref type=""bibr"" target=""#b0"">[1]</ref>. Since the documents disclosed by the firm usually have the significance testing, presented in the academic papers in major psychology journals. A recent study <ref type=""bibr"" target=""#b0"">[1]</ref> published a system called AutoDoc, and introduced the module uch more numerical facts in tables than textual paragraphs. Therefore, as an important extension to <ref type=""bibr"" target=""#b0"">[1]</ref>, we propose Automatic Numerical Cross-Checking over Tables ( ncial, and politic fields. It has attracted a lot of research interests in recent years. Cao et al. <ref type=""bibr"" target=""#b0"">[1]</ref> propose a system to cross-check numerical facts by extractin",1
"cell search for a given query <ref type=""bibr"" target=""#b14"">[15]</ref>, ad hoc search over tables <ref type=""bibr"" target=""#b17"">[18]</ref>, transforming complex tables to the form that can be store",0
"cial information, so accounting errors also deserve our attention. Choudhary, Merkley, and Schipper <ref type=""bibr"" target=""#b1"">[2]</ref> find that investors believe that even immaterial errors mean",0
"give a classification <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b15"">16]</ref>. In academic field, Nuijten et al. <ref type=""bibr"" target=",0
"structured information which is laborious when collecting the labelling dataset. Vlachos and Riedel <ref type=""bibr"" target=""#b16"">[17]</ref> propose a dataset to verify the claims made by public figu",0
"""bibr"" target=""#b47"">48]</ref>. Our theory complements the previous studies of contrastive learning <ref type=""bibr"" target=""#b38"">[39]</ref>.</p><p>Based on the theoretical discovery, an easy-to-impl 𝑛 (𝑦 | 𝑥) <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b42"">43]</ref>. InfoNCE <ref type=""bibr"" target=""#b38"">[39]</ref> demonstrates that this loss maximizes a lower bound of the sed pretext tasks, recently achieves remarkable success in various domains, e.g., speech processing <ref type=""bibr"" target=""#b38"">[39]</ref>, computer vision <ref type=""bibr"" target=""#b10"">[11,</ref> 0]</ref>. The contrastive loss we investigate in this paper is a generalization of the InfoNCE loss <ref type=""bibr"" target=""#b38"">[39]</ref>. InfoNCE is previously understood as a bound of the mutual </ref>. InfoNCE is previously understood as a bound of the mutual information between two variables <ref type=""bibr"" target=""#b38"">[39]</ref>. Our work provides a new perspective on the effectiveness r"" target=""#b36"">37]</ref>.</p><p>Contrastive Loss. We study the following type of contrastive loss <ref type=""bibr"" target=""#b38"">[39,</ref><ref type=""bibr"" target=""#b42"">43]</ref> under a negative s in various fields <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b38"">39]</ref>. Nevertheless, it still remains a question why the loss is",1
"supervised tasks, e.g., discriminating whether two subsequences come from the same user's behaviors <ref type=""bibr"" target=""#b35"">[36]</ref>. We further improve upon CLRec and propose Multi-CLRec, wh the regular task where 𝑥 is a sequence of clicks and 𝑦 is the next click to be predicted. Task u2u <ref type=""bibr"" target=""#b35"">[36]</ref> adds an auxiliary loss where 𝑥 and 𝑦 are both sequences fr /div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>B.4 Complex Pretext Tasks</head><p>In task u2u <ref type=""bibr"" target=""#b35"">[36]</ref>, 𝑥 and 𝑦 are both sequences from the same user, before and",1
"the latter is a well-studied technique for bias reduction <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b40"">41,</ref><ref type=""bibr"" target=""#b47"">48]</ref>. Our theory complem eir connection with the inverse propensity weighting (IPW) <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b40"">41,</ref><ref type=""bibr"" target=""#b47"">48]</ref> techniques for bias ustrating its connection with inverse propensity weighting <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b40"">41,</ref><ref type=""bibr"" target=""#b47"">48]</ref>.</p><p>[63] Hao Zou",0
"g will still incur non-negligible overheads, e.g. communication costs, in a distributed environment <ref type=""bibr"" target=""#b44"">[45]</ref>. Sampling also cannot guarantee that every item will be sa =""#b37"">[38]</ref>. We use a distributed version of the sampling strategy based on the alias method <ref type=""bibr"" target=""#b44"">[45]</ref>, which is provided by AliGraph <ref type=""bibr"" target=""#b",0
"duct offline experiments and compare CLRec with sampled softmax. We report the aggregated diversity <ref type=""bibr"" target=""#b1"">[2]</ref> in Table <ref type=""table"" target=""#tab_3"">1</ref>, and the s clicked by them, divided by the total number of clicks on our platform. • The aggregate diversity <ref type=""bibr"" target=""#b1"">[2]</ref>, measured on a sampled subset of users for testing, is the n /1.0"" type=""table"" xml:id=""tab_3""><head>Table 1 :</head><label>1</label><figDesc>Aggregate diversity<ref type=""bibr"" target=""#b1"">[2]</ref>, i.e. the number of distinct items recommended to a randomly",0
"=""#b20"">[21]</ref> 0.7760 0.8471 Caser <ref type=""bibr"" target=""#b46"">[47]</ref> 0.7582 0.8745 DIEN <ref type=""bibr"" target=""#b59"">[60]</ref> 0.7770 0.8934 RUM <ref type=""bibr"" target=""#b12"">[13]</ref",0
"get=""#b19"">20,</ref><ref type=""bibr"" target=""#b41"">42,</ref><ref type=""bibr"" target=""#b43"">44,</ref><ref type=""bibr"" target=""#b51"">52,</ref><ref type=""bibr"" target=""#b53"">54,</ref><ref type=""bibr"" tar rget=""#b9"">10,</ref><ref type=""bibr"" target=""#b41"">42,</ref><ref type=""bibr"" target=""#b43"">44,</ref><ref type=""bibr"" target=""#b51"">52,</ref><ref type=""bibr"" target=""#b53"">54]</ref>. Most of the existi ependent bernoulli distribution 𝑞 𝜋 (recommend = 1 | 𝑥, 𝑦) <ref type=""bibr"" target=""#b41"">[42,</ref><ref type=""bibr"" target=""#b51"">52]</ref>. Here 𝑂 𝑦 ∈ {0, 1} denotes whether item 𝑦 is recommended by different from the previous works on debiased recommenders<ref type=""bibr"" target=""#b41"">[42,</ref><ref type=""bibr"" target=""#b51"">52]</ref>. We focus on multinomial propensities, i.e. whether an item",0
"py based approximations such as NCE <ref type=""bibr"" target=""#b17"">[18]</ref> and negative sampling <ref type=""bibr"" target=""#b37"">[38]</ref> at large scale.</p><p>However, the MLE paradigm and the sa ms other approximations such as NCE <ref type=""bibr"" target=""#b17"">[18]</ref> and negative sampling <ref type=""bibr"" target=""#b37"">[38]</ref> when the vocabulary is large <ref type=""bibr"" target=""#b13 encoders. Details of the alternative methods are in the Appendix. We observe that negative sampling <ref type=""bibr"" target=""#b37"">[38]</ref>, including its variant <ref type=""bibr"" target=""#b55"">[56] //www.tei-c.org/ns/1.0""><head>B.3 Loss Functions and Sampling Strategies</head><p>Negative sampling <ref type=""bibr"" target=""#b37"">[38]</ref>. We sample 𝐿 examples to pair with each positive click fro osal distribution 𝑞(𝑦) is proportional to the item's degree, i.e., degree(𝑦) 0.75 as recommended in <ref type=""bibr"" target=""#b37"">[38]</ref>. We use a distributed version of the sampling strategy bas roposal distribution <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b37"">38]</ref>. The proposal distribution not only affects convergence, bu",0
"s category ID, i.e., c 𝑡 ; embedding of item 𝑦 𝑡 's seller ID; embeddings of 𝑦 𝑡 's tags; . . . ]), <ref type=""bibr"" target=""#b4"">(5)</ref> where MLP 1 (•) represents a multilayer perceptron (MLP) who /formula><p>𝑝 𝑡 • g 𝜃 (𝑦 𝑡 ) .</p><p>(9) We approximate the gradient of argmax via straight-through <ref type=""bibr"" target=""#b4"">[5]</ref>. That is, we define</p><formula xml:id=""formula_13"">𝑤 ℎ = st",0
">, computer vision <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b21"">22]</ref>, graph data <ref type=""bibr"" target=""#b49"">[50]</ref>, and",0
"""#b51"">[52]</ref>, in which each network block consists of different convolutional kernels. ResNeXt <ref type=""bibr"" target=""#b60"">[61]</ref> adopts group convolution <ref type=""bibr"" target=""#b33"">[3 ight) depicts an overview of a Split-Attention Block.</p><p>Feature-map Group. As in ResNeXt blocks <ref type=""bibr"" target=""#b60"">[61]</ref>, the feature can be divided into several groups, and the n",1
"ces a channel-attention mechanism by adaptively recalibrating the channel feature responses. SK-Net <ref type=""bibr"" target=""#b37"">[38]</ref> brings the feature-map attention across two network branch /ref>: Comparing our ResNeSt block with SE-Net <ref type=""bibr"" target=""#b29"">[30]</ref> and SK-Net <ref type=""bibr"" target=""#b37"">[38]</ref>. A detailed view of Split-Attention unit is shown in Figur -Net operates on top of the entire block regardless of multiple groups. Previous models like SK-Net <ref type=""bibr"" target=""#b37"">[38]</ref> introduced feature attention between two network branches, type=""bibr"" target=""#b28"">[29]</ref>, ResNet-D <ref type=""bibr"" target=""#b25"">[26]</ref> and SKNet <ref type=""bibr"" target=""#b37"">[38]</ref>. Remarkably, our ResNeSt-50 achieves 80.64 top-1 accuracy, ach group is Split Attention in Cardinal Groups. Following <ref type=""bibr"" target=""#b29"">[30,</ref><ref type=""bibr"" target=""#b37"">38]</ref>, a combined representation for each cardinal group can be o obal average pooling across spatial dimensions s k ∈ R C/K <ref type=""bibr"" target=""#b28"">[29,</ref><ref type=""bibr"" target=""#b37"">38]</ref>. Here the c-th component is calculated as:</p><formula xml: Our method generalizes prior work on feature-map attention <ref type=""bibr"" target=""#b28"">[29,</ref><ref type=""bibr"" target=""#b37"">38]</ref> within a cardinal group setting <ref type=""bibr"" target=""#b",1
"et=""#b26"">[27,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b53"">54,</ref><ref type=""bibr"" target=""#b59"">60]</ref>. Despite their superior computation and accuracy tradeoff i t-Attention block are roughly the same as a residual block <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b59"">60]</ref> with the same cardinality and number of channels.</p><p>Rel et=""#b22"">[23,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b28"">29,</ref><ref type=""bibr"" target=""#b59"">60]</ref> usually use a fixed training crop size of 224, while the In r"" target=""#b28"">[29,</ref><ref type=""bibr"" target=""#b37"">38]</ref> within a cardinal group setting <ref type=""bibr"" target=""#b59"">[60]</ref>, and its implementation remains computationally efficient. network parameters and FLOPS, including: ResNet <ref type=""bibr"" target=""#b22"">[23]</ref>, ResNeXt <ref type=""bibr"" target=""#b59"">[60]</ref>, SENet <ref type=""bibr"" target=""#b28"">[29]</ref>, ResNet-D",1
"the previous methods, our Fig. <ref type=""figure"">1</ref>: Comparing our ResNeSt block with SE-Net <ref type=""bibr"" target=""#b29"">[30]</ref> and SK-Net <ref type=""bibr"" target=""#b37"">[38]</ref>. A de then the intermediate representation of each group is Split Attention in Cardinal Groups. Following <ref type=""bibr"" target=""#b29"">[30,</ref><ref type=""bibr"" target=""#b37"">38]</ref>, a combined repres ead of the 1 × 1 layer to better preserve such information <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b29"">30]</ref>. Convolutional layers require handling featuremap boundarie",1
"n the ResNet bottle block, which converts the multi-path structure into a unified operation. SE-Net <ref type=""bibr"" target=""#b28"">[29]</ref> introduces a channel-attention mechanism by adaptively rec ty and number of channels.</p><p>Relation to Existing Attention Methods. First introduced in SE-Net <ref type=""bibr"" target=""#b28"">[29]</ref>, the idea of squeeze-and-attention (called excitation in t <ref type=""bibr"" target=""#b22"">[23]</ref>, ResNeXt <ref type=""bibr"" target=""#b59"">[60]</ref>, SENet <ref type=""bibr"" target=""#b28"">[29]</ref>, ResNet-D <ref type=""bibr"" target=""#b25"">[26]</ref> and SK l-wise statistics can be gathered with global average pooling across spatial dimensions s k ∈ R C/K <ref type=""bibr"" target=""#b28"">[29,</ref><ref type=""bibr"" target=""#b37"">38]</ref>. Here the c-th com cy and scaling to large neural networks. Our method generalizes prior work on feature-map attention <ref type=""bibr"" target=""#b28"">[29,</ref><ref type=""bibr"" target=""#b37"">38]</ref> within a cardinal ks out some neurons during training (but not during inference) to form an implicit network ensemble <ref type=""bibr"" target=""#b28"">[29,</ref><ref type=""bibr"" target=""#b48"">49,</ref><ref type=""bibr"" ta ze. ResNet variants<ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b28"">29,</ref><ref type=""bibr"" target=""#b59"">60]</ref> usually use a fixed",1
"works have focused more on group or depth-wise convolution <ref type=""bibr"" target=""#b26"">[27,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b53"">54,</ref><ref type=""bibr"" tar representations cannot capture cross-channel relationships <ref type=""bibr"" target=""#b26"">[27,</ref><ref type=""bibr"" target=""#b27"">28]</ref>. Therefore, a network with cross-channel representations is",0
"b55"">[56]</ref> or use cross-channel feature-map attention <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b64"">65]</ref>. While these approaches do improve the transfer learning pe tream applications <ref type=""bibr"" target=""#b55"">[56,</ref><ref type=""bibr"" target=""#b63"">64,</ref><ref type=""bibr"" target=""#b64"">65]</ref>, while recent image classification networks have focused mo ain all models with FPN <ref type=""bibr"" target=""#b40"">[41]</ref>, synchronized batch normalization <ref type=""bibr"" target=""#b64"">[65]</ref> and image scale augmentation (short size of a image is pic is applied to the backbone network, resulting in a stride-8 model. Synchronized Batch Normalization <ref type=""bibr"" target=""#b64"">[65]</ref> is used during training, along with a polynomial-like lear xel cross entropy loss against the ground truth labels. We use multi-scale evaluation with flipping <ref type=""bibr"" target=""#b64"">[65,</ref><ref type=""bibr"" target=""#b68"">69,</ref><ref type=""bibr"" ta",0
"target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b68"">69]</ref> or introduce long-range connections <ref type=""bibr"" target=""#b55"">[56]</ref> or use cross-channel feature-map attention <ref type=""bibr sks at the same time? Cross-channel information has demonstrated success in downstream applications <ref type=""bibr"" target=""#b55"">[56,</ref><ref type=""bibr"" target=""#b63"">64,</ref><ref type=""bibr"" ta",0
"this benchmark, all inference speeds are measured using a mini-batch of 16 using the implementation <ref type=""bibr"" target=""#b0"">[1]</ref> from the original author on a single NVIDIA V100 GPU. The pr",0
"ref> as a baseline approach. Here a dilated network strategy <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b61"">62]</ref> is applied to the backbone network, resulting in a stride-8",0
"f>, while recent image classification networks have focused more on group or depth-wise convolution <ref type=""bibr"" target=""#b26"">[27,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" ta er well to other tasks as their isolated representations cannot capture cross-channel relationships <ref type=""bibr"" target=""#b26"">[27,</ref><ref type=""bibr"" target=""#b27"">28]</ref>. Therefore, a netw",0
"ge connections <ref type=""bibr"" target=""#b55"">[56]</ref> or use cross-channel feature-map attention <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b64"">65]</ref>. While these appro",0
"couple of tweaks that further improve performance, some of which have been empirically validated in <ref type=""bibr"" target=""#b24"">[25]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head",0
"designed for other applications, such as object detection <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b45"">46]</ref>, semantic segmentation <ref type=""bibr"" target=""#b5"">[6,</r downstream models <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b40"">41,</ref><ref type=""bibr"" target=""#b45"">46,</ref><ref type=""bibr"" target=""#b68"">69]</ref>. Our approach can a",0
"gt; 0. This mitigates network overconfidence and overfitting.</p><p>Auto Augmentation. Auto-Augment <ref type=""bibr"" target=""#b10"">[11]</ref> is a strategy that augments the training data with transfo",0
"head n=""4.2"">Training Strategy</head><p>Large Mini-batch Distributed Training. Following prior work <ref type=""bibr"" target=""#b18"">[19,</ref><ref type=""bibr"" target=""#b36"">37]</ref>, we train our mode gradually increasing the learning rate linearly from 0 to the initial value for the cosine schedule <ref type=""bibr"" target=""#b18"">[19,</ref><ref type=""bibr"" target=""#b38"">39]</ref>. The batch normali ests weight decay should only be applied to the weights of convolutional and fully connected layers <ref type=""bibr"" target=""#b18"">[19,</ref><ref type=""bibr"" target=""#b25"">26]</ref>. We do not subject zed to zero in the final BN operation of each block, as has been suggested for large batch training <ref type=""bibr"" target=""#b18"">[19]</ref>.</p><p>Label Smoothing Label smoothing was first used to i",0
"nearly from 0 to the initial value for the cosine schedule <ref type=""bibr"" target=""#b18"">[19,</ref><ref type=""bibr"" target=""#b38"">39]</ref>. The batch normalization (BN) parameter γ is initialized to",0
"e achieves 48.3% box mAP and 41.56% mask mAP on MS-COCO instance segmentation. Our single DeepLabV3 <ref type=""bibr"" target=""#b6"">[7]</ref> model, again using a ResNeSt-101 backbone, achieves mIoU of mentation, we use the GluonCV <ref type=""bibr"" target=""#b20"">[21]</ref> implementation of DeepLabV3 <ref type=""bibr"" target=""#b6"">[7]</ref> as a baseline approach. Here a dilated network strategy <ref",0
"ve as the backbone of the neural networks designed for other applications, such as object detection <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b45"">46]</ref>, semantic segmenta l preserves ResNet meta structure, which can be directly applied on many existing downstream models <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b40"">41,</ref><ref type=""bibr"" ta for which a more accurate dense image representation is desirable.</p><p>We evaluate the Mask-RCNN <ref type=""bibr"" target=""#b21"">[22]</ref> and Cascade-Mask-RCNN <ref type=""bibr"" target=""#b1"">[2]</r",0
"mation has demonstrated success in downstream applications <ref type=""bibr"" target=""#b55"">[56,</ref><ref type=""bibr"" target=""#b63"">64,</ref><ref type=""bibr"" target=""#b64"">65]</ref>, while recent image",0
"the short edge and apply a center crop. Our code implementation for ImageNet training uses GluonCV <ref type=""bibr"" target=""#b20"">[21]</ref> with MXNet <ref type=""bibr"" target=""#b8"">[9]</ref>.</p></d </head><p>In transfer learning for the downstream task of semantic segmentation, we use the GluonCV <ref type=""bibr"" target=""#b20"">[21]</ref> implementation of DeepLabV3 <ref type=""bibr"" target=""#b6""> ePose <ref type=""bibr"" target=""#b57"">[58]</ref> with ResNet50 and ResNet101 implemented in Glu-onCV <ref type=""bibr"" target=""#b20"">[21]</ref>. As comparison we replace the backbone with ResNeSt50 and",0
"arge Mini-batch Distributed Training. Following prior work <ref type=""bibr"" target=""#b18"">[19,</ref><ref type=""bibr"" target=""#b36"">37]</ref>, we train our models using 8 servers (64 GPUs in total) in",0
"h-wise convolution <ref type=""bibr"" target=""#b26"">[27,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b53"">54,</ref><ref type=""bibr"" target=""#b59"">60]</ref>. Despite their supe classification performance, such as: Amoe-baNet <ref type=""bibr"" target=""#b44"">[45]</ref>, MNASNet <ref type=""bibr"" target=""#b53"">[54]</ref>, and EfficientNet <ref type=""bibr"" target=""#b54"">[55]</ref",0
"timized for training efficiency or memory usage on general/commercial processing hardware (CPU/GPU) <ref type=""bibr"" target=""#b35"">[36]</ref>. Due to excessive memory consumption, some of the larger v",0
"rg/ns/1.0""><head n=""1"">Introduction</head><p>The majority of the research efforts on improving VAEs <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref> is dedicated to the st e posterior up to the (l − 1) th group. The objective is trained using the reparameterization trick <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref>.</p><p>The main questi and bidirectional encoder networks <ref type=""bibr"" target=""#b3"">[4]</ref>.</p><p>The goal of VAEs <ref type=""bibr"" target=""#b0"">[1]</ref> is to train a generative model in the form of p(x x x, z z z",1
"e feature map for the backward pass, instead of two. This trick is known as gradient check-pointing <ref type=""bibr"" target=""#b53"">[54,</ref><ref type=""bibr"" target=""#b54"">55]</ref> and it requires re",0
"). One simple solution is to use KL balancing coefficients <ref type=""bibr"" target=""#b19"">[20,</ref><ref type=""bibr"" target=""#b64"">65]</ref> to ensure that an equal amount of information is encoded in",0
"et=""#b10"">[11,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b13"">14]</ref>, reducing the gradient noise <ref type=""bibr"" target=""#b14""",0
"an augmentation method, backtranslation <ref type=""bibr"" target=""#b37"">(Sennrich et al., 2015;</ref><ref type=""bibr"" target=""#b14"">Edunov et al., 2018)</ref> refers to the procedure of translating an ation experiments. We use random sampling instead of beam search for decoding similar to the work by<ref type=""bibr"" target=""#b14"">Edunov et al. (2018)</ref>.</figDesc><table /></figure> 			<note xmln",1
"simply regularize model predictions to be invariant to small noise applied to either input examples <ref type=""bibr"" target=""#b23"">(Miyato et al., 2018;</ref><ref type=""bibr"" target=""#b34"">Sajjadi et r"" target=""#b1"">(Bachman et al., 2014)</ref> directly applies Gaussian noise and Dropout noise; VAT <ref type=""bibr"" target=""#b23"">(Miyato et al., 2018;</ref><ref type=""bibr"">2016)</ref> defines the n type=""bibr"" target=""#b34"">(Sajjadi et al., 2016;</ref><ref type=""bibr"">Laine &amp; Aila, 2016;</ref><ref type=""bibr"" target=""#b23"">Miyato et al., 2018)</ref>. But different from existing work, we focu has been shown to be beneficial <ref type=""bibr"" target=""#b15"">(Grandvalet &amp; Bengio, 2005;</ref><ref type=""bibr"" target=""#b23"">Miyato et al., 2018)</ref>, we sharpen predictions when computing the current parameters θ indicating that the gradient is not propagated through θ, as suggested by VAT <ref type=""bibr"" target=""#b23"">(Miyato et al., 2018)</ref>. We set λ to 1 for most of our experiment cally, we compare UDA with two highly competitive baselines: (1) Virtual adversarial training (VAT) <ref type=""bibr"" target=""#b23"">(Miyato et al., 2018)</ref>, an algorithm that generates adversarial =""#b41"">(Tarvainen &amp; Valpola, 2017)</ref> Conv-Large 3.1M 12.31 ± 0.28 3.95 ± 0.19 VAT + EntMin <ref type=""bibr"" target=""#b23"">(Miyato et al., 2018)</ref> Conv-Large 3.1M 10.55 ± 0.05 3.86 ± 0.11",1
"1998;</ref><ref type=""bibr"">Krizhevsky et al., 2012;</ref><ref type=""bibr"">Cubuk et al., 2018;</ref><ref type=""bibr"" target=""#b49"">Yu et al., 2018)</ref>, also perform well in semisupervised learning. recent years, there have been significant advancements on the design of data augmentations for NLP <ref type=""bibr"" target=""#b49"">(Yu et al., 2018</ref><ref type=""bibr"">), vision (Krizhevsky et al., her language B and then translating it back into A to obtain an augmented example x. As observed by <ref type=""bibr"" target=""#b49"">(Yu et al., 2018)</ref>, back-translation can generate diverse paraph",0
"ng for vision <ref type=""bibr"">(Zhai et al., 2019b;</ref><ref type=""bibr"">Hénaff et al., 2019;</ref><ref type=""bibr"" target=""#b42"">Trinh et al., 2019)</ref>.</p><p>Consistency Training in Other Domain",0
"br"" target=""#b51"">Zhai et al., 2019a;</ref><ref type=""bibr"" target=""#b24"">Najafi et al., 2019;</ref><ref type=""bibr"" target=""#b3"">Carmon et al., 2019)</ref>. Enforcing consistency w.r.t data augmentat",0
"o be helpful in adversarial robustness <ref type=""bibr"" target=""#b40"">(Stanforth et al., 2019;</ref><ref type=""bibr"" target=""#b51"">Zhai et al., 2019a;</ref><ref type=""bibr"" target=""#b24"">Najafi et al.",0
"<ref type=""bibr"">(Zhu et al., 2003)</ref> has been extended to neural methods via graph embeddings <ref type=""bibr"" target=""#b45"">(Weston et al., 2012;</ref><ref type=""bibr"" target=""#b46"">Yang et al.",0
"ntation method called RandAugment <ref type=""bibr"">(Cubuk et al., 2019)</ref>, which is inspired by <ref type=""bibr"" target=""#b8"">AutoAugment (Cubuk et al., 2018)</ref>. AutoAugment uses a search meth",0
"ing. Later, the pre-training of word embeddings was simplified and substantially scaled in Word2Vec <ref type=""bibr"" target=""#b21"">(Mikolov et al., 2013)</ref> and Glove <ref type=""bibr"" target=""#b29""",0
"l-established and recent spatial L2 prefetchers (prefetchers that prefetch within a spatial region) <ref type=""bibr"" target=""#b32"">[33]</ref>, <ref type=""bibr"" target=""#b12"">[13]</ref>, <ref type=""bib a prefetching (VLDP) <ref type=""bibr"" target=""#b44"">[45]</ref> and signature path prefetching (SPP) <ref type=""bibr"" target=""#b32"">[33]</ref> are well known delta prefetchers. VLDP stores the history ing proposals <ref type=""bibr"" target=""#b10"">[11]</ref>, <ref type=""bibr"" target=""#b13"">[14]</ref>, <ref type=""bibr"" target=""#b32"">[33]</ref> have also been coded and evaluated with ChampSim, helping improve performance for server workloads like CloudSuite <ref type=""bibr"" target=""#b13"">[14]</ref>, <ref type=""bibr"" target=""#b32"">[33]</ref>, <ref type=""bibr"" target=""#b52"">[53]</ref>, <ref type=""bib",1
"rks. We also use a set of Convolutional Neural Networks (CNNs) and a Recurrent Neural Network (RNN) <ref type=""bibr"" target=""#b18"">[19]</ref>, <ref type=""bibr"" target=""#b20"">[21]</ref>, <ref type=""bib",0
"l prefetchers <ref type=""bibr"" target=""#b53"">[54]</ref>, <ref type=""bibr"" target=""#b54"">[55]</ref>, <ref type=""bibr"" target=""#b23"">[24]</ref>, <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bib rs like temporal streaming <ref type=""bibr"" target=""#b54"">[55]</ref>, Irregular Stream Buffer (ISB) <ref type=""bibr"" target=""#b23"">[24]</ref>, and Domino <ref type=""bibr"" target=""#b11"">[12]</ref> trac "">[25]</ref>, <ref type=""bibr"" target=""#b28"">[29]</ref>, <ref type=""bibr"" target=""#b10"">[11]</ref>, <ref type=""bibr"" target=""#b23"">[24]</ref>, <ref type=""bibr"" target=""#b58"">[59]</ref>. However, if a ibr"" target=""#b58"">[59]</ref> and additional prefetchers <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" target=""#b23"">[24]</ref>, <ref type=""bibr"" target=""#b51"">[52]</ref>, <ref type=""bib",0
"mory system. ChampSim was used for the 2nd and 3rd data prefetching championships (DPC-2 and DPC-3) <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b2"">[3]</ref>. The simulation fra",0
""">[31]</ref>, <ref type=""bibr"" target=""#b38"">[39]</ref>- <ref type=""bibr"" target=""#b40"">[41]</ref>, <ref type=""bibr"" target=""#b49"">[50]</ref> that control the prefetch degree and prefetch distance bas",0
"""bibr"" target=""#b55"">[56]</ref>, CoLT <ref type=""bibr"" target=""#b45"">[46]</ref>, and Clustered TLBs <ref type=""bibr"" target=""#b44"">[45]</ref> combine near virtual-to-physical page translations into si",1
"to the reservation based paging strategy used in FreeBSD <ref type=""bibr"" target=""#b13"">[14]</ref>, <ref type=""bibr"" target=""#b40"">[41]</ref> and previously proposed in <ref type=""bibr"" target=""#b41"">",0
"these approaches to improve translation latency.</p><p>Prior work in fine-grained memory protection <ref type=""bibr"" target=""#b23"">[24]</ref>, <ref type=""bibr"" target=""#b56"">[57]</ref>, <ref type=""bib",0
"t TPS, we modify the L1 TLB to contain a 32 entry fully-associative (as in other commercial designs <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b46"">[47]</ref>) TPS TLB. The TPS",0
"Existing OS proposals like Ingens and Translation Ranger <ref type=""bibr"" target=""#b37"">[38]</ref>, <ref type=""bibr"" target=""#b59"">[60]</ref> already address the issues of maximizing memory contiguity",0
"as a single table, but starting with z15 it exploits a variation of the TAGE algorithm based off of <ref type=""bibr"" target=""#b8"">[8]</ref>. Two TAGE PHT tables are employed in z15-a short and a long wever, a weak TAGE PHT prediction can sometimes be detrimental, particularly after a context switch <ref type=""bibr"" target=""#b8"">[8]</ref>. As such, weak filtering is employed. Before allowing a weak",1
"allows the IDU to know when branch prediction has fallen behind. Starting with the IBM z13 machine <ref type=""bibr"" target=""#b17"">[17]</ref>, the branch predictor employs such strict synchronization.",0
"large system performance record (LSPR) workloads generally consist of a large instruction footprint <ref type=""bibr"" target=""#b2"">[2]</ref>.</p><p>The z15 is the latest mainframe offering from IBM. A",0
"ave tried to predicate only those instances of H2P branches which have low confidence of prediction <ref type=""bibr"" target=""#b6"">[7]</ref>, <ref type=""bibr"" target=""#b10"">[11]</ref>- <ref type=""bibr"" >[11]</ref>- <ref type=""bibr"" target=""#b13"">[14]</ref>. Policies like Diverge Merge Processor (DMP) <ref type=""bibr"" target=""#b6"">[7]</ref>, <ref type=""bibr"" target=""#b14"">[15]</ref> use careful compi control flow convergence using generic patterns of convergence. This is unlike previous approaches <ref type=""bibr"" target=""#b6"">[7]</ref>, <ref type=""bibr"" target=""#b11"">[12]</ref>- <ref type=""bibr"" paths can converge to some later point in the program (using the same convergence criterion as DMP <ref type=""bibr"" target=""#b6"">[7]</ref>). Loops are naturally converging and contribute to another 1 namically applied predication only on branch instances having low confidence from branch prediction <ref type=""bibr"" target=""#b6"">[7]</ref>, <ref type=""bibr"" target=""#b12"">[13]</ref>, <ref type=""bibr"" ofitably) and fetches both the directions of the hammock in hardware. Diverge Merge Processor (DMP) <ref type=""bibr"" target=""#b6"">[7]</ref> improves upon both Wish Branches and DHP. DMP uses compiler ce register or flags (like stores or branches), instantly releases its resources.</p><p>Prior works <ref type=""bibr"" target=""#b6"">[7]</ref>, <ref type=""bibr"" target=""#b10"">[11]</ref> have relied on se er transparency without resorting to complex RAT recovery mechanisms or re-execution as proposed in <ref type=""bibr"" target=""#b6"">[7]</ref>, <ref type=""bibr"" target=""#b10"">[11]</ref>.</p><p>3) Predica evaluate this trade-off for ACB.  In this section, we compare against Diverge-Merge Processor (DMP) <ref type=""bibr"" target=""#b6"">[7]</ref>, which relies on changes to the compiler, ISA and micro-arch e=""bibr"" target=""#b17"">[18]</ref> but due to large overheads, the realistic benefits are diminished <ref type=""bibr"" target=""#b6"">[7]</ref>, <ref type=""bibr"" target=""#b11"">[12]</ref>. Wish Branches <r target=""#b33"">[34]</ref>- <ref type=""bibr"" target=""#b35"">[36]</ref>. Diverge-Merge Processor (DMP) <ref type=""bibr"" target=""#b6"">[7]</ref>, <ref type=""bibr"" target=""#b14"">[15]</ref> uses branch predi",1
"known that speculative update of the branch history is very important for branch predictor accuracy <ref type=""bibr"" target=""#b29"">[30]</ref>. In the baseline, branch history is always speculatively u",0
"ck, nab, roms, perlbench, gcc, mcf, omnetpp, xalancbmk, x264, deepsjeng, leela, exchange, xz SPEC17 <ref type=""bibr"" target=""#b21"">[22]</ref> winzip, photoshop, sketchup, premiere SYSmark <ref type=""b",0
"e of prediction <ref type=""bibr"" target=""#b6"">[7]</ref>, <ref type=""bibr"" target=""#b10"">[11]</ref>- <ref type=""bibr"" target=""#b13"">[14]</ref>. Policies like Diverge Merge Processor (DMP) <ref type=""bi ious approaches <ref type=""bibr"" target=""#b6"">[7]</ref>, <ref type=""bibr"" target=""#b11"">[12]</ref>- <ref type=""bibr"" target=""#b13"">[14]</ref> that were dependent upon compiler analysis and profiling. anch prediction <ref type=""bibr"" target=""#b6"">[7]</ref>, <ref type=""bibr"" target=""#b12"">[13]</ref>, <ref type=""bibr"" target=""#b13"">[14]</ref>.</p><p>Wish Branches <ref type=""bibr"" target=""#b11"">[12]</",0
"ing to predicate frequently occurring basic blocks. Generalized multipath execution was proposed in <ref type=""bibr"" target=""#b33"">[34]</ref>- <ref type=""bibr"" target=""#b35"">[36]</ref>. Diverge-Merge",0
"he past, IBM z13 and many other systems have used FPGA based PCIe attached compression accelerators <ref type=""bibr"" target=""#b1"">[2]</ref>- <ref type=""bibr"" target=""#b4"">[5]</ref>. However, the FPGA >. However, the FPGA cost and limited number of PCIe slots restrict their usage to high-end servers <ref type=""bibr"" target=""#b1"">[2]</ref> and specialized applications such as storage controllers.</p hput <ref type=""bibr"" target=""#b5"">[6]</ref>. We would need 68 PCIe based compression cards such as <ref type=""bibr"" target=""#b1"">[2]</ref>, with 4GB/s peak throughput to match the NXU performance. Be ficient LZ77 encoding hardware that improves state of the art in Section IV, where the related work <ref type=""bibr"" target=""#b1"">[2]</ref>- <ref type=""bibr"" target=""#b3"">[4]</ref>, <ref type=""bibr"" t nd z15 to achieve highest possible compression ratio, as described in Section V, where related work <ref type=""bibr"" target=""#b1"">[2]</ref>- <ref type=""bibr"" target=""#b3"">[4]</ref>, <ref type=""bibr"" t oftware extensively in its products. IBM z13 and POWER systems used PCIe based Deflate accelerators <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b4"">[5]</ref>, <ref type=""bibr"" t jectives which is not suitable for an on-chip design. In <ref type=""bibr"" target=""#b3"">[4]</ref> In <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b14"">[15]</ref>, the 32KB sliding nt a static table with an assumed LZ symbol distribution which typically degrades compression ratio <ref type=""bibr"" target=""#b1"">[2]</ref>- <ref type=""bibr"" target=""#b3"">[4]</ref>, <ref type=""bibr"" t here one of 15 different static Huffman tables yielding the smallest output is selected at run time <ref type=""bibr"" target=""#b1"">[2]</ref>. We implemented in contrast, a true ""Dynamic Huffman"" mode f",1
"- <ref type=""bibr"" target=""#b57"">[58]</ref> and industry <ref type=""bibr"" target=""#b58"">[59]</ref>- <ref type=""bibr"" target=""#b60"">[61]</ref> for evaluating performance of big data SQL platforms and u",0
"given as -log ? p min , where ? = (1 + ? 5)/2 and p min is the probability of least frequent symbol <ref type=""bibr"" target=""#b23"">[24]</ref>. However, this upper bound, when used as a predictor of tr",0
"uation, we use Parquet files as data source because of its compatibility with Spark SQL, efficiency <ref type=""bibr"" target=""#b62"">[63]</ref> and performance <ref type=""bibr"" target=""#b63"">[64]</ref>.",0
"d><p>TPC-DS is a de-facto standard benchmark in academia <ref type=""bibr"" target=""#b51"">[52]</ref>, <ref type=""bibr"" target=""#b55"">[56]</ref>- <ref type=""bibr"" target=""#b57"">[58]</ref> and industry <r",0
""">[21]</ref>, <ref type=""bibr"" target=""#b21"">[22]</ref>, <ref type=""bibr"" target=""#b22"">[23]</ref>, <ref type=""bibr"" target=""#b23"">[24]</ref>. The main concept behind these approaches is to interpret s successfully transferred from NLP to protein sequences <ref type=""bibr"" target=""#b20"">[21]</ref>, <ref type=""bibr"" target=""#b23"">[24]</ref>, <ref type=""bibr"" target=""#b51"">[52]</ref>, with the excep f their surrounding context (residues next to it). As previously established for another protein LM <ref type=""bibr"" target=""#b23"">[24]</ref>, the t-SNE projections (e.g. ProtBert Fig. <ref type=""figu "">[20]</ref>, <ref type=""bibr"" target=""#b20"">[21]</ref>, <ref type=""bibr"" target=""#b21"">[22]</ref>, <ref type=""bibr"" target=""#b23"">[24]</ref>, we might expect an upper limit for what protein LMs can l",1
"d on proteins <ref type=""bibr"" target=""#b15"">[16]</ref>, <ref type=""bibr"" target=""#b16"">[17]</ref>, <ref type=""bibr"" target=""#b17"">[18]</ref>, <ref type=""bibr"" target=""#b18"">[19]</ref>, <ref type=""bib iven the experiments described here and in previous work <ref type=""bibr"" target=""#b16"">[17]</ref>, <ref type=""bibr"" target=""#b17"">[18]</ref>, <ref type=""bibr"" target=""#b18"">[19]</ref>, <ref type=""bib",0
"Q3) and over non-contextualized word2vec-type approaches <ref type=""bibr"" target=""#b63"">[64]</ref>, <ref type=""bibr"" target=""#b64"">[65]</ref>, <ref type=""bibr"" target=""#b65"">[66]</ref> (12-17 percenta",0
"eral orders of magnitude more tokens than corpora used in NLP, e.g., Google's Billion Word data set <ref type=""bibr"" target=""#b38"">[39]</ref> is one of the biggest for NLP with about 829 million token",0
"Additionally, some proteins are intrinsically hard to align (e.g. intrinsically disordered proteins <ref type=""bibr"" target=""#b31"">[32]</ref> or proteins which do not have any related sequences (dark varying batch-sizes <ref type=""bibr"" target=""#b0"">(1,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b31"">32)</ref> as well as sequence lengths (128, 256, 512), SeqVec provide ifferent batch sizes <ref type=""bibr"" target=""#b0"">(1,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b31"">32)</ref> on the inference time of the protein LMs introduced here is",0
"by all the modalities.</p><p>Inspired by such an assumption and the fact that the Total Correlation <ref type=""bibr"" target=""#b27"">[29]</ref> can measure the amount of information shared by M (M ≥ 2) sifiers of each modality.</p><p>Total Correlation/Mutual information maximization Total Correlation <ref type=""bibr"" target=""#b27"">[29]</ref>, as an extension of Mutual Information, measures the amoun",1
"et=""#b4"">[6]</ref>; and (ii) learning joint representation across modalities in an unsupervised way <ref type=""bibr"" target=""#b24"">[26,</ref><ref type=""bibr"" target=""#b26"">28]</ref>. These methods suf els in each modality, may not be consistent with the real settings.</p><p>The second branch of work <ref type=""bibr"" target=""#b24"">[26,</ref><ref type=""bibr"" target=""#b26"">28,</ref><ref type=""bibr"" ta",0
"e distance between joint distribution and the marginal distribution. In more details, as noted from <ref type=""bibr"" target=""#b13"">[15]</ref>, by applying first-order Taylor-expansion, we have log p(x",0
"target=""#b7"">9,</ref><ref type=""bibr"" target=""#b29"">31,</ref><ref type=""bibr"" target=""#b8"">10,</ref><ref type=""bibr"" target=""#b16"">18]</ref> centers on learning joint representations that project unim",0
"ations based on the mutual information maximization (e.g., unsupervised learning of representations <ref type=""bibr"" target=""#b12"">[14]</ref>, learning node representations within graph-structured dat",0
"ering what causes deep networks to be fragile to adversarial examples and how to improve robustness <ref type=""bibr"" target=""#b45"">[47,</ref><ref type=""bibr"" target=""#b11"">13,</ref><ref type=""bibr"" ta t has been theoretically shown that decreasing the input dimensionality of data improves robustness <ref type=""bibr"" target=""#b45"">[47]</ref>. Adversarial training <ref type=""bibr"" target=""#b33"">[35]< ining performance on natural examples.</p><p>Using the first order vulnerability of neural networks <ref type=""bibr"" target=""#b45"">[47]</ref>, we theoretically show that increasing output dimensionali ethods can improve the model's robustness without compromising clean accuracy. Simon-Gabriel et al. <ref type=""bibr"" target=""#b45"">[47]</ref> conducted a theoretical analysis of the vulnerability of n ref>. We denote the multitask predictor as F and each individual task predictor as F c . Prior work <ref type=""bibr"" target=""#b45"">[47]</ref> showed that the norm of gradients captures the vulnerabili rial noise is imperceptible, i.e., r → 0, we can approximate ∆L with a first-order Taylor expansion <ref type=""bibr"" target=""#b45"">[47]</ref>.</p><p>Lemma 1. For a given neural network F that predicts l></formula><p>Remark 1. By increasing the number of output tasks M , the first order vulnerability <ref type=""bibr"" target=""#b45"">[47]</ref> of network decreases. In the ideal case, if the model has s we add more tasks, the norm of the joint gradient decreases, indicating improvement to robustness <ref type=""bibr"" target=""#b45"">[47]</ref>. The only exception is the depth estimation task, which we e same dimension for baselines and ours during comparison because input dimension impacts robustness<ref type=""bibr"" target=""#b45"">[47]</ref>.</note> 		</body> 		<back>  			<div type=""acknowledgement""",1
"get=""#b19"">21,</ref><ref type=""bibr"" target=""#b53"">55,</ref><ref type=""bibr"" target=""#b48"">50,</ref><ref type=""bibr"" target=""#b40"">42,</ref><ref type=""bibr"" target=""#b5"">7,</ref><ref type=""bibr"" targe",0
"arget=""#b2"">[4,</ref><ref type=""bibr"" target=""#b13"">15,</ref><ref type=""bibr"" target=""#b8"">10,</ref><ref type=""bibr"" target=""#b23"">25,</ref><ref type=""bibr"" target=""#b46"">48]</ref> aims to solve sever",0
"as demonstrated that images with human-imperceptible noise <ref type=""bibr"" target=""#b33"">[35,</ref><ref type=""bibr"" target=""#b1"">3,</ref><ref type=""bibr"" target=""#b9"">11,</ref><ref type=""bibr"" target",0
"ining, later works <ref type=""bibr"" target=""#b19"">[21,</ref><ref type=""bibr"" target=""#b34"">36,</ref><ref type=""bibr"" target=""#b59"">61,</ref><ref type=""bibr"" target=""#b53"">55]</ref> achieve improved ro lose significant accuracy on clean (unperturbed) examples <ref type=""bibr"" target=""#b33"">[35,</ref><ref type=""bibr"" target=""#b59"">61,</ref><ref type=""bibr"" target=""#b49"">51]</ref>. Moreover, generati ltitask model, which complements both adversarial training <ref type=""bibr"" target=""#b33"">[35,</ref><ref type=""bibr"" target=""#b59"">61]</ref> and existing regularization methods <ref type=""bibr"" target",0
"milar semantics.</p><p>Sharing a similar philosophy, there have been works on contrastive attention <ref type=""bibr"" target=""#b30"">[31,</ref><ref type=""bibr"" target=""#b41"">42]</ref>. MGCAM <ref type="" attention <ref type=""bibr"" target=""#b30"">[31,</ref><ref type=""bibr"" target=""#b41"">42]</ref>. MGCAM <ref type=""bibr"" target=""#b30"">[31]</ref> uses the contrastive feature between persons and backgroun for context modeling; instead of using extra supervision to localize regions to compare as in MGCAM <ref type=""bibr"" target=""#b30"">[31]</ref>, ACM automatically learns to focus on meaningful regions t",1
"we model the way radiologists read X-rays? When radiologists read chest X-rays, they compare zones <ref type=""bibr"" target=""#b0"">[1]</ref>, paying close attention to any asymmetry between left and ri",0
"ays every day. Several studies regarding radiologic errors <ref type=""bibr"" target=""#b27"">[28,</ref><ref type=""bibr"" target=""#b8"">9]</ref> have reported that 20-30% of exams are misdiagnosed. To compe",0
"ion in COCO dataset <ref type=""bibr"" target=""#b23"">[24]</ref> with various backbones such as ResNet <ref type=""bibr"" target=""#b13"">[14]</ref>, ResNeXt <ref type=""bibr"" target=""#b39"">[40]</ref> or Dens s can be classified and located in a weakly-supervised multi-label classification framework. ResNet <ref type=""bibr"" target=""#b13"">[14]</ref> and DenseNet <ref type=""bibr"" target=""#b15"">[16,</ref><ref Average of 5 random runs are reported for each setting with standard deviation. RN stands for ResNet<ref type=""bibr"" target=""#b13"">[14]</ref>.</figDesc><table><row><cell>Method</cell><cell>AUC-ROC</ce is designed to be light-weight, self-contained, and compatible with popular backbone architectures <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" ta",0
"nd thoracic diseases from chest X-rays using deep learning <ref type=""bibr"" target=""#b40"">[41,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" tar ays. It is used as a benchmark dataset in previous studies <ref type=""bibr"" target=""#b40"">[41,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b28"">29,</ref><ref type=""bibr"" tar ty.</p><p>To address the issue of localizing diseases using only class-level labels, Guendel et al. <ref type=""bibr"" target=""#b11"">[12]</ref> propose an auxiliary localization task where the ground tr",0
"ts of three convolutions, which just cause finite increasing in additional computational complexity <ref type=""bibr"" target=""#b16"">[17]</ref>. The effectiveness of ACB has been verified in the fields [17]</ref>. The effectiveness of ACB has been verified in the fields including image classification <ref type=""bibr"" target=""#b16"">[17]</ref>, image denoising <ref type=""bibr"" target=""#b17"">[18]</ref> should be robust to rotation and renders consistent results in different rotations. As reported in <ref type=""bibr"" target=""#b16"">[17]</ref>, different asymmetric convolutions are robust with differe network.</p><p>Based on above-mentioned insight, we modify the asymmetric convolutions proposed in <ref type=""bibr"" target=""#b16"">[17]</ref> and design an asymmetric convolution block (ACB) to captur",1
"o make utmost of the multi-scale features, the full-scale skip connections are designed in U-Net 3+ <ref type=""bibr"" target=""#b15"">[16]</ref>. However, the design philosophy of full-scale skip connect",1
"ile feature maps generated by the decoder contain high-level and coarse-gained semantic information <ref type=""bibr"" target=""#b14"">[15]</ref>. And skip connections, which combine the low-level and hig ive method to boost the semantic extraction ability of encoder-decoder frameworks.</p><p>In U-Net++ <ref type=""bibr"" target=""#b14"">[15]</ref>, plain skip connections are substituted by nested and dens =""bibr"" target=""#b20"">[21]</ref>, FGC <ref type=""bibr"" target=""#b21"">[22]</ref>, MSFCN, and U-Net++ <ref type=""bibr"" target=""#b14"">[15]</ref>. The major C. Duan is with the State Key Laboratory of Inf =""bibr"" target=""#b20"">[21]</ref>, FGC <ref type=""bibr"" target=""#b21"">[22]</ref>, MSFCN, and U-Net++ <ref type=""bibr"" target=""#b14"">[15]</ref>. Excluding SegNet, DeepLab V3 and DeepLab V3+, the remaini",1
"cluding support vector machine (SVM) <ref type=""bibr"" target=""#b6"">[7]</ref> and random forest (RF) <ref type=""bibr"" target=""#b7"">[8]</ref>. However, the high dependency on hand-crafted visual feature",0
"he fields including image classification <ref type=""bibr"" target=""#b16"">[17]</ref>, image denoising <ref type=""bibr"" target=""#b17"">[18]</ref>, and medical image segmentation <ref type=""bibr"" target=""#",0
"ng the precise category to every pixel contained in an image <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref>, plays a critical role in wide range of application scenarios",0
"ibility and adaptability of these methods.</p><p>More recently, Convolutional Neural Networks (CNN) <ref type=""bibr"" target=""#b8"">[9]</ref> have demonstrated its powerful capacity of automatically cap",0
"7]</ref>, image denoising <ref type=""bibr"" target=""#b17"">[18]</ref>, and medical image segmentation <ref type=""bibr"" target=""#b18"">[19]</ref>. In <ref type=""bibr"" target=""#b18"">[19]</ref>, only the co get=""#b17"">[18]</ref>, and medical image segmentation <ref type=""bibr"" target=""#b18"">[19]</ref>. In <ref type=""bibr"" target=""#b18"">[19]</ref>, only the convolutional layers of encoder are replaced by",0
"source management, yield estimation, and economic assessment <ref type=""bibr"" target=""#b2"">[3]</ref><ref type=""bibr"" target=""#b3"">[4]</ref><ref type=""bibr"" target=""#b4"">[5]</ref>. Hitherto the remote",0
"target=""#b13"">[14]</ref>, FC-DenseNet57 <ref type=""bibr"" target=""#b19"">[20]</ref>, Attention U-Net <ref type=""bibr"" target=""#b20"">[21]</ref>, FGC <ref type=""bibr"" target=""#b21"">[22]</ref>, MSFCN, and 13"">[14]</ref>, FC-DenseNet57 (tiramisu) <ref type=""bibr"" target=""#b19"">[20]</ref>, Attention U-Net <ref type=""bibr"" target=""#b20"">[21]</ref>, FGC <ref type=""bibr"" target=""#b21"">[22]</ref>, MSFCN, and",0
"f application scenarios such as land resource management, yield estimation, and economic assessment <ref type=""bibr"" target=""#b2"">[3]</ref><ref type=""bibr"" target=""#b3"">[4]</ref><ref type=""bibr"" targe",0
"Net <ref type=""bibr"" target=""#b11"">[12]</ref>, and DeepLab <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b13"">14]</ref> have become the frequently-used schemes. Generally, feature =""bibr"" target=""#b11"">[12]</ref>, DeepLab V3 <ref type=""bibr"" target=""#b12"">[13]</ref>, DeepLab V3+ <ref type=""bibr"" target=""#b13"">[14]</ref>, FC-DenseNet57 <ref type=""bibr"" target=""#b19"">[20]</ref>, =""bibr"" target=""#b11"">[12]</ref>, DeepLab V3 <ref type=""bibr"" target=""#b12"">[13]</ref>, DeepLab V3+ <ref type=""bibr"" target=""#b13"">[14]</ref>, FC-DenseNet57 (tiramisu) <ref type=""bibr"" target=""#b19"">[",0
"<ref type=""bibr"" target=""#b5"">[6]</ref>, to advanced methods including support vector machine (SVM) <ref type=""bibr"" target=""#b6"">[7]</ref> and random forest (RF) <ref type=""bibr"" target=""#b7"">[8]</re",0
"d economic assessment <ref type=""bibr"" target=""#b2"">[3]</ref><ref type=""bibr"" target=""#b3"">[4]</ref><ref type=""bibr"" target=""#b4"">[5]</ref>. Hitherto the remote sensing community has tried to design a",0
"ion, the encoder-decoder frameworks such as SegNet <ref type=""bibr"" target=""#b10"">[11]</ref>, U-Net <ref type=""bibr"" target=""#b11"">[12]</ref>, and DeepLab <ref type=""bibr"" target=""#b12"">[13,</ref><ref the performance of proposed algorithm with SegNet <ref type=""bibr"" target=""#b10"">[11]</ref>, U-Net <ref type=""bibr"" target=""#b11"">[12]</ref>, DeepLab V3 <ref type=""bibr"" target=""#b12"">[13]</ref>, Dee >To evaluate the effectiveness of MACU-Net, SegNet <ref type=""bibr"" target=""#b10"">[11]</ref>, U-Net <ref type=""bibr"" target=""#b11"">[12]</ref>, DeepLab V3 <ref type=""bibr"" target=""#b12"">[13]</ref>, Dee",0
"n this paper, we introduce TOAD-GAN as a solution to these problems. Our work is inspired by SinGAN <ref type=""bibr"" target=""#b15"">(Shaham, Dekel, and Michaeli 2019)</ref>, a recent Generative Adversa s of TOAD-GAN on Super Mario Bros. level 1-2. The architecture is adapted from SinGAN (cf. Fig. 4 of<ref type=""bibr"" target=""#b15"">(Shaham, Dekel, and Michaeli 2019)</ref>). We use a downsampling meth e training process.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>SinGAN</head><p>SinGAN <ref type=""bibr"" target=""#b15"">(Shaham, Dekel, and Michaeli 2019</ref>) is a novel GAN architecture p at the lowest scale. For a more in-depth explanation please refer to the original SinGAN paper by <ref type=""bibr"" target=""#b15"">Shaham, Dekel, and Michaeli (2019)</ref>.</p></div> <div xmlns=""http:",1
"sequentially. They used a neural network architecture based on Long Short-Term Memory (LSTM) cells <ref type=""bibr"" target=""#b9"">(Hochreiter and Schmidhuber 1997)</ref> to predict the next token, giv",0
""" target=""#b3"">(Dahlskog and Togelius 2012</ref>) and combined them using simple statistical models <ref type=""bibr"" target=""#b17"">(Snodgrass and Ontanón 2013)</ref>. The quality of these algorithms c lations, which have to be defined manually. Recent approaches used PCG via Machine Learning (PCGML) <ref type=""bibr"" target=""#b17"">(Summerville et al. 2018)</ref> to learn the patterns and relations f 17"">(Summerville et al. 2018)</ref> to learn the patterns and relations from the data automatically <ref type=""bibr"" target=""#b17"">(Summerville and Mateas 2016;</ref><ref type=""bibr"" target=""#b22"">Vol cts the height of a token in a level slice, given the heights of all tokens in the previous slices. <ref type=""bibr"" target=""#b17"">Summerville and Mateas (2016)</ref> trained their model on levels by generation of SMB levels. There are 15 original SMB levels provided by the Video Game Level Corpus <ref type=""bibr"" target=""#b17"">(Summerville et al. 2016)</ref>, each with different characteristics.",0
"e of the key problems of PCGML algorithms <ref type=""bibr"" target=""#b20"">(Torrado et al. 2019;</ref><ref type=""bibr"" target=""#b1"">Bontrager and Togelius 2020)</ref>. Therefore, the goal of our work is",0
"atterns.</p><p>Search-based PCG <ref type=""bibr"">(Togelius et al. 2011</ref>) was applied to SMB by <ref type=""bibr"" target=""#b18"">Summerville, Philip, and Mateas (2015)</ref>. The authors used Monte",0
"=""#b22"">[23,</ref><ref type=""bibr"" target=""#b28"">29]</ref> or for member profiles (personalization) <ref type=""bibr"" target=""#b9"">[10]</ref>. It requires a huge amount of hard disk space to store the",1
"=""#b19"">[20,</ref><ref type=""bibr"" target=""#b25"">26]</ref> encodes word order information using CNN <ref type=""bibr"" target=""#b15"">[16]</ref>/LSTM <ref type=""bibr"" target=""#b11"">[12]</ref>, respective",0
"nking is a nontrivial task. The current effective approaches <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b21"">22]</ref> integrate BERT as a ""bibr"" target=""#b7"">[8]</ref> has shown superior performance <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b21"">22]</ref> in ranking. It is c",0
"pointwise, pairwise or listwise LTR <ref type=""bibr"" target=""#b2"">[3]</ref>, as well as Lambda rank <ref type=""bibr"" target=""#b3"">[4]</ref>. Binary classification loss (pointwise learning-to-rank) can",0
"are not available <ref type=""bibr"" target=""#b4"">[5]</ref>, <ref type=""bibr"" target=""#b5"">[6]</ref>, <ref type=""bibr"" target=""#b6"">[7]</ref>, <ref type=""bibr"" target=""#b7"">[8]</ref>, <ref type=""bibr"" t #b4"">[5]</ref>. The system first predicts 68 face landmarks from speech using an LSTM-based network <ref type=""bibr"" target=""#b6"">[7]</ref>, and then predicts a few talking face images from the condit",1
"Existing work <ref type=""bibr"" target=""#b22"">[23]</ref>, <ref type=""bibr"" target=""#b23"">[24]</ref>, <ref type=""bibr"" target=""#b24"">[25]</ref>, <ref type=""bibr"" target=""#b25"">[26]</ref>, <ref type=""bib is sensitive to stimuli from multiple modalities in acted data and naturalistic data. Jessen et al. <ref type=""bibr"" target=""#b24"">[25]</ref> suggested that emotional visual content allows more reliab",0
"ave shown that predicting emotions purely from speech audio is quite difficult for untrained people <ref type=""bibr"" target=""#b13"">[14]</ref> and that we heavily rely on visual cues in emotion interpr ification for humans. This observation is similar to a speech emotion classification observation in <ref type=""bibr"" target=""#b13"">[14]</ref>.</p><p>Task 1 -Realness Evaluation. For the realness quest",0
"=""#b4"">[5]</ref>, <ref type=""bibr"" target=""#b5"">[6]</ref>, <ref type=""bibr"" target=""#b6"">[7]</ref>, <ref type=""bibr"" target=""#b7"">[8]</ref>, <ref type=""bibr"" target=""#b8"">[9]</ref>, <ref type=""bibr"" t",0
"ed the image quality of the generated videos using Peak SNR (PSNR) and Structural Similarity (SSIM) <ref type=""bibr"" target=""#b35"">[36]</ref> between the generated video frames and the ground-truth vi",0
"cal works include XLA <ref type=""bibr"" target=""#b8"">[9]</ref> (applicable to training as well), TVM <ref type=""bibr"" target=""#b9"">[10]</ref>, Glow <ref type=""bibr"" target=""#b10"">[11]</ref>, Tensor Com gebra operations and calls into backend-specific libraries for execution on different backends. TVM <ref type=""bibr"" target=""#b9"">[10]</ref> is an end-to-end compiler framework with Halide at the core",1
"timizations based on local and global graph optimization, automated searches with genetic algorithm <ref type=""bibr"" target=""#b24"">[25]</ref> and reinforcement learning(RL) <ref type=""bibr"" target=""#b",0
"ef type=""bibr"" target=""#b11"">[12]</ref>, nGraph <ref type=""bibr"" target=""#b12"">[13]</ref>, OpenVINO <ref type=""bibr"" target=""#b13"">[14]</ref>, and TensorRT <ref type=""bibr"" target=""#b14"">[15]</ref>.</",0
""" target=""#b10"">[11]</ref>, Tensor Comprehensions <ref type=""bibr"" target=""#b11"">[12]</ref>, nGraph <ref type=""bibr"" target=""#b12"">[13]</ref>, OpenVINO <ref type=""bibr"" target=""#b13"">[14]</ref>, and T VM and aims to optimize CNN inference on CPUs by taking advantage of wide SIMD instructions. nGraph <ref type=""bibr"" target=""#b12"">[13]</ref> adopts a similar workflow to TVM, but was further extended",0
"<ref type=""bibr"" target=""#b5"">[6]</ref>, Mxnet <ref type=""bibr"" target=""#b6"">[7]</ref> and PyTorch <ref type=""bibr"" target=""#b7"">[8]</ref> all provide built-in support for GPUs. However, these framew",0
"ial for training large and complex deep learning architectures. RNN-T models are difficult to train <ref type=""bibr"" target=""#b10"">[11]</ref> and also require significantly large amount of data to joi oral classification (CTC) model <ref type=""bibr"" target=""#b1"">[2]</ref> or cross entropy (CE) model <ref type=""bibr"" target=""#b10"">[11]</ref>, and the prediction network with LSTM language model (LM) on.</p><p>model for encoder and prediction network in the context of TL the RNN-T model. Authors in <ref type=""bibr"" target=""#b10"">[11]</ref> have shown that CE initialized RNN-T models perform better in blocks. gets (necessary for CE training), is obtained from word level alignments as discussed in <ref type=""bibr"" target=""#b10"">[11]</ref>. From the word alignments, the start frame, end frame and ned by using byte pair encoding <ref type=""bibr"" target=""#b25"">[26]</ref> algorithm as described in <ref type=""bibr"" target=""#b10"">[11]</ref>.</p><p>We also report the word error rate (WER) on hybrid",1
"et=""#b11"">[12,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" tar >19,</ref><ref type=""bibr"" target=""#b19"">20]</ref>. Successful strategies include transfer learning <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b15"">16]</ref>, that leverage a w",0
"get=""#b14"">15,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" tar uage to bootstrap the low-resource AM; multi-task training <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b17"">18]</ref> and ensemble learning <ref type=""bibr"" target=""#b18"">[19,</",0
"el (LM) and pronunciation model with a single neural network <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target= pretrained models. Initializing the encoder with connectionist temporal classification (CTC) model <ref type=""bibr"" target=""#b1"">[2]</ref> or cross entropy (CE) model <ref type=""bibr"" target=""#b10"">[",0
"r"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b9"">10]</ref>. Recurrent neural network transducer (RNN-T) <ref type=""bibr",0
"equential recommenders <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b28"">29]</ref>. It has been demonstrated that contextual information is im ich is able to achieve the same effect as previous methods <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b28"">29]</ref>. Besides, the pre-trained data representations can be also them by the interaction timestamps ascendingly. Following <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b28"">29]</ref>, we only keep the 5-core datasets, and filter unpopular ite te the performance, which are widely used in related works <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b28"">29]</ref>. Since HR@1 is equal to NDCG@1, we report results on HR@{1, ation Machines to incorporate arbitrary real-valued features to the sequential recommendation. FDSA <ref type=""bibr"" target=""#b28"">[29]</ref> employed a feature-level self-attention block to leverage ttribute-aware sequential models such as TransFM <ref type=""bibr"" target=""#b15"">[16]</ref> and FDSA <ref type=""bibr"" target=""#b28"">[29]</ref> leverage the contextual features to improve the sequential and attribute as the input to the model. ( <ref type=""formula"" target=""#formula_16"">11</ref>) FDSA <ref type=""bibr"" target=""#b28"">[29]</ref> constructs a feature sequence and uses a featurelevel self",1
"tc. There are also studies that leverage other architectures <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b23"">24]</ref> for sequential reco xt from both directions for sequence representation learning <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b22"">23]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n owing previous works <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b22"">23]</ref>, we apply the leave-oneout strategy for evaluation. Concret model, which uses the multi-head attention mechanism to recommend the next item.</p><p>(7) BERT4Rec <ref type=""bibr"" target=""#b22"">[23]</ref> uses a Cloze objective loss for sequential recommendation",1
"mproving sequential recommendation. Self-supervised learning <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b14"">15]</ref> is a newly emerging paradigm, which aims to let the model l lf-supervised learning <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b14"">15]</ref> aims at training a network on an auxiliary objective where type=""bibr"" target=""#b4"">[5]</ref>. As for language modeling <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b14"">15]</ref>, it is a popular self-supervised objective for natural lang",0
"t=""#b6"">7]</ref>, hierarchical structures <ref type=""bibr"" target=""#b16"">[17]</ref>, copy mechanism <ref type=""bibr"" target=""#b17"">[18]</ref> and reinforcement learning <ref type=""bibr"" target=""#b26"">",0
"t self-supervised learning with MIM has been applied to improve the sequential recommendation task; <ref type=""bibr"" target=""#b1"">(2)</ref> We propose four self-supervised optimization objectives to m /ref><ref type=""bibr"" target=""#b24"">25]</ref>, which is based on Noise Contrastive Estimation (NCE) <ref type=""bibr"" target=""#b1"">[2]</ref>. InfoNCE is defined as:</p><formula xml:id=""formula_2"">E p(X",0
"not been well captured in data representations. As shown in increasing evidence from various fields <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target idea of self-supervised learning for improving sequential recommendation. Self-supervised learning <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b14"">15]</ref> is a newly emerging tp://www.tei-c.org/ns/1.0""><head n=""2.2"">Self-supervised Learning</head><p>Self-supervised learning <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target guide the visual feature learning <ref type=""bibr"" target=""#b4"">[5]</ref>. As for language modeling <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b14"">15]</ref>, it is a popular sel . It is beneficial to incorporate context from both directions for sequence representation learning <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b22"">23]</ref>.</p></div> <div xmln or sequences can improve the performance of downstream tasks such as machine reading comprehension <ref type=""bibr"" target=""#b0"">[1]</ref> and natural language understanding <ref type=""bibr"" target="" upervised learning objectives with the recently proposed pretraining framework in language modeling <ref type=""bibr"" target=""#b0"">[1]</ref>.</p><p>The overview of S 3 -Rec is presented in Fig. <ref ty multi-head self-attention function to remove all connections between Q i and K i . Inspired by BERT <ref type=""bibr"" target=""#b0"">[1]</ref>, at the pre-training stage, we remove the mask mechanism to deed observed by the model in the training process. Inspired by the masked language model like BERT <ref type=""bibr"" target=""#b0"">[1]</ref>, we propose to model the bidirectional information in item s",0
"on way is easy to suffer from issues such as data sparsity <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b21"">22]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>a ased approaches such as Factorization Machine <ref type=""bibr"" target=""#b19"">[20]</ref> and AutoInt <ref type=""bibr"" target=""#b21"">[22]</ref> mainly learn data representations through the interaction characterizes the pairwise interactions between variables using factorized model.</p><p>(3) AutoInt <ref type=""bibr"" target=""#b21"">[22]</ref> utilizes the multi-head self-attentive neural network to l",0
"arget=""#b7"">8,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b23"">24]</ref>.</p><p>Typically, sequential recommendation methods <ref ty target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b23"">24]</ref> capture useful sequential patterns from users' historical b it for high-order MCs <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b23"">24]</ref>. With the development of the neural networks, Hidasi et al. other architectures <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b23"">24]</ref> for sequential recommendation. However, these approaches ne "" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b23"">24]</ref> mainly emphasize the effect of sequential characteristics u that existing methods <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b23"">24]</ref> seldom directly model the correlation between the sequentia eural networks (RNNs) <ref type=""bibr"" target=""#b2"">[3]</ref>, convolutional neural networks (CNNs) <ref type=""bibr"" target=""#b23"">[24]</ref>, and self-attention mechanisms <ref type=""bibr"" target=""#b dation. We represent the items using embedding vectors rather than one-hot vectors.</p><p>(5) Caser <ref type=""bibr"" target=""#b23"">[24]</ref> is a CNN-based method capturing high-order Markov Chains b",0
"ncorporated rich contextual information (such as item attributes) to neural sequential recommenders <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target the last interaction of a user. A series of works follow this line and extend it for high-order MCs <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target /label></formula><p>where W MAP ∈ R d ×d is a parameter matrix to learn. Note that existing methods <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target ation and a surge of following variants modified this model by introducing pair-wise loss functions <ref type=""bibr"" target=""#b3"">[4]</ref>, memory networks <ref type=""bibr"" target=""#b5"">[6,</ref><ref ierarchical gating networks to capture long-term and short-term user interests.</p><p>(9) GRU4Rec F <ref type=""bibr"" target=""#b3"">[4]</ref> is an improved version of GRU4Rec, which leverages attribute ><head n=""4"">APPROACH 4.1 Overview</head><p>Existing studies <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=",0
"target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b23"">24]</ref>.</p><p>Typically, s recommendation methods <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b23"">24]</ref> capture useful sequ It has been found that such an optimization way is easy to suffer from issues such as data sparsity <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b21"">22]</ref>.</p></div> <div xm the interaction records by users and sort them by the interaction timestamps ascendingly. Following <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b28"">29]</ref>, we only keep the and Mean Reciprocal Rank (MRR) to evaluate the performance, which are widely used in related works <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b28"">29]</ref>. Since HR@1 is equ enhance data representations instead of making predictions.</p><p>Sequential models such as GRU4Rec <ref type=""bibr"" target=""#b20"">[21]</ref> and SASRec <ref type=""bibr"" target=""#b7"">[8]</ref> mainly as a similar effect to capture sequential dependencies as in <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b20"">21]</ref> except that it can also utilize bidirectional sequential in",0
"rmance, most of them ignore the interactions' timestamp values. While recent works such as TiSASRec <ref type=""bibr"" target=""#b13"">[14]</ref> successfully incorporated time information, their usage of ransformer <ref type=""bibr"" target=""#b21"">[22]</ref> and Cloze-task based training method. TiSASRec <ref type=""bibr"" target=""#b13"">[14]</ref> enhanced SASRec by merging timestamp information into self st, they don't utilize timestamp values which hold important contextual information. While TiSASRec <ref type=""bibr"" target=""#b13"">[14]</ref> successfully addressed this issue, they also used a simple pe=""bibr"" target=""#b31"">[32]</ref>,</p><p>SASRec <ref type=""bibr"" target=""#b9"">[10]</ref>, TiSASRec <ref type=""bibr"" target=""#b13"">[14]</ref>, and BERT4Rec <ref type=""bibr"" target=""#b18"">[19]</ref>. T essing procedure from <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b18"">19]</ref>. We convert each da g the custom practice <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b18"">19]</ref>, we discard users a We use the rest for training. We follow the common practice <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b18"">19]</ref> of letting the mode",1
"orms of time gates to better model the time intervals in user's interaction sequence. Recently, CTA <ref type=""bibr"" target=""#b24"">[25]</ref> used multiple parametrized kernel functions on temporal in",0
"butions for the winning of Netflix Grand Prize <ref type=""bibr"" target=""#b10"">[11]</ref>. Time-LSTM <ref type=""bibr"" target=""#b36"">[37]</ref> equipped LSTMs with several forms of time gates to better",0
"is is also in accordance with the recent report on the order of layer normalization in Transformers <ref type=""bibr"" target=""#b27"">[28]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head",0
"target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b28"">29]</ref>. BERT4Rec <ref type=""bibr"" target=""#b18"">[19]</ref> improve",0
"propagation process. To train the similar measure with a direct supervised signal from labels, like <ref type=""bibr"" target=""#b34"">[35]</ref>, we define the cross-entropy loss of the MLP at l-layer as",1
"lnerabilities of GNNs when graphs have noisy nodes and edges <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" targe ed fraudsters would sabotage the performance of GNN-based fraud detectors. Though some recent works <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target earn new graph structures from original graphs, which could Another approach is the metric learning <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b12"">13]</ref>. Those works devise 3"">2</ref>) and d is the feature dimension.</p><p>Label-aware Similarity Measure. Inspired by LAGCN <ref type=""bibr"" target=""#b3"">[4]</ref> which uses a Multi-layer Perceptron (MLP) as the edge label e the embedding of the node itself as the input instead of using combined embeddings like the LAGCN <ref type=""bibr"" target=""#b3"">[4]</ref>. Therefore, taking the S Optimization. To train the similari milarity between connect nodes and aggregate neighbors according to the metrics. Among those works, <ref type=""bibr"" target=""#b3"">[4]</ref> proposes a neural network to predict the labels of neighbori",1
"imilarity measure. A is the action space, f is the reward function, and T is the terminal condition <ref type=""bibr"" target=""#b35"">[36]</ref>. Given an initial p (l ) r , the neighbor selector choose",1
"roposed CARE-GNN. Given a multi-relational fraud graph, we employ the mini-batch training technique <ref type=""bibr"" target=""#b10"">[11]</ref> as the result of its large scale. In the beginning, we ran eat scales. To improve the training efficiency and avoid overfitting, we employ mini-batch training <ref type=""bibr"" target=""#b10"">[11]</ref> and under-sampling <ref type=""bibr"" target=""#b21"">[22]</re",0
"fitting, we employ mini-batch training <ref type=""bibr"" target=""#b10"">[11]</ref> and under-sampling <ref type=""bibr"" target=""#b21"">[22]</ref> techniques to train CARE-GNN and other baselines. Specific",0
"ION</head><p>As Internet services thrive, they also incubate various kinds of fraudulent activities <ref type=""bibr"" target=""#b13"">[14]</ref>. Fraudsters disguise as regular users to bypass the anti-f",0
"hich have been drawing great attention from both researchers <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" tar e=""bibr"" target=""#b40"">41]</ref> (so-called spamouflage), or employ deep language generation models <ref type=""bibr"" target=""#b14"">[15]</ref> to gloss over explicit suspicious outcomes. Like Figure <r easures could not identify the camouflaged fake reviews, which are even indistinguishable by humans <ref type=""bibr"" target=""#b14"">[15]</ref>. Therefore, we need a parameterized similarity measure to ehavior <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b9"">10]</ref> and semantic <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b40"">41]</ref> perspectives. Thos",0
"the end-to-end learning fashion.</p><p>GNN sampling methods <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b45"">46,</ref><ref type=""bibr"" target=""#b50"">51]</ref> also filter the nei",0
"ach in both academic <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b37"">38]</ref> and industrial communities <ref type=""bibr"" target=""#b1"">[2",0
"ely committing opinion fraud on online social networks. They can probe the graphs used by defenders <ref type=""bibr"" target=""#b42"">[43]</ref> and adjust their behavior to alleviate the suspiciousness",0
"ilarity metrics like Cosine Similarity <ref type=""bibr"" target=""#b24"">[25]</ref> or Neural Networks <ref type=""bibr"" target=""#b44"">[45]</ref>. However, many fraud problems like financial fraud and opi",0
"f><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b38"">39]</ref>, financial fraud <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" ta et=""#b18"">[19,</ref><ref type=""bibr"" target=""#b28"">29]</ref> or a transaction in the trading system <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b36"">37]</ref>. The node has a la egate the neighbor information from different relations. Previous methods adopt attention mechanism <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b36"">37,</ref><ref type=""bibr"" ta AGE <ref type=""bibr"" target=""#b11"">[12]</ref> to represent general GNN models. We choose Ge-niePath <ref type=""bibr"" target=""#b22"">[23]</ref>, Player2Vec <ref type=""bibr"" target=""#b47"">[48]</ref>, Sem -graph based on multiple relations and employ GNNs to aggregate neighborhood information. GeniePath <ref type=""bibr"" target=""#b22"">[23]</ref> learns convolutional layers and neighbor weights using LST",0
"><ref type=""bibr"" target=""#b38"">39]</ref>, financial fraud <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b36"">37]</ref>, mobile fraud <ref from the same user <ref type=""bibr"" target=""#b24"">[25]</ref> or transactions from the same devices <ref type=""bibr"" target=""#b23"">[24]</ref>. The graph-based fraud detection problem is a semi-supervi ibr"" target=""#b36"">37,</ref><ref type=""bibr"" target=""#b47"">48]</ref> or devise weighting parameters <ref type=""bibr"" target=""#b23"">[24]</ref> to learn the relation weights during aggregating informati ean, and they differ from each other in Attention <ref type=""bibr"" target=""#b33"">[34]</ref>, Weight <ref type=""bibr"" target=""#b23"">[24]</ref>, and Mean <ref type=""bibr"" target=""#b11"">[12]</ref> inter- ghbor weights using LSTM and the attention mechanism <ref type=""bibr"" target=""#b33"">[34]</ref>. GEM <ref type=""bibr"" target=""#b23"">[24]</ref>, SemiGNN <ref type=""bibr"" target=""#b36"">[37]</ref>, ASA <r",0
"""bibr"" target=""#b19"">[20]</ref> employs a Mahalanobis distance plus a Gaussian kernel, and DIAL-GNN <ref type=""bibr"" target=""#b5"">[6]</ref> uses the parameterized cosine similarity. However, those two //www.tei-c.org/ns/1.0""><head>3.3.2</head><p>Finding the Optimal Thresholds with RL. Previous works <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b24"">25]</ref> set the filtering th the GNN performance on graphs with noisy nodes. One approach is the graph structure learning (GSL) <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target",0
"otes as fraudulent entities. Though previous works have proposed other fraud datasets like Epinions <ref type=""bibr"" target=""#b17"">[18]</ref> and Bitcoin <ref type=""bibr"" target=""#b39"">[40]</ref>, the",0
"were developed to evaluate overall image quality and not fine-grained lip-sync errors. Although LMD <ref type=""bibr"" target=""#b3"">[4]</ref> focuses on the lip region, we found that lip landmarks can b",1
"work is that it can simultaneously also encourage efforts <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" tar",0
"s where previous speaker-independent lipsyncing approaches <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b17"">18]</ref> struggle to produce satisfactory results.</p></div> 			</ab ion of speaker-independent speech to lip generation models <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b17"">18]</ref> that are trained on thousands of identities and voices. The o the best of our knowledge, only two such prominent works <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b17"">18]</ref> exist in the current literature. Note that <ref type=""bibr"" <ref type=""bibr"" target=""#b6"">[7]</ref>. Both these works <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b17"">18]</ref> formulate the task of learning to lip-sync in the wild as f the L1 reconstruction loss used in both the existing works <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b17"">18]</ref> and the discriminator loss in LipGAN <ref type=""bibr"" targe Benchmark</head><p>We compare the previous two approaches <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b17"">18]</ref> on our newly created test set using the LSE-D and LSE-C met </ref>. An outcome worth noting is that the previous works <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b17"">18]</ref> which produce several out-of-sync segments are less preferr rget=""#b5"">[6,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b22"">23]</ref> in the research com ny identity in any voice, including that of a synthetic speech generated by a text-to-speech system <ref type=""bibr"" target=""#b17"">[18]</ref>. However, to be used for applications like translating a l ficantly hampers a model from learning the vast diversity of phoneme-viseme mappings in real videos <ref type=""bibr"" target=""#b17"">[18]</ref>. Our work focuses on lip-syncing unconstrained talking fac en trying to lip-sync unconstrained videos in the wild. In contrast to the GAN setup used in LipGAN <ref type=""bibr"" target=""#b17"">[18]</ref>, we use a pre-trained, accurate lip-sync discriminator tha rget=""#b16"">[17,</ref><ref type=""bibr"" target=""#b17"">18]</ref> and the discriminator loss in LipGAN <ref type=""bibr"" target=""#b17"">[18]</ref> are inadequate to penalize inaccurate lip-sync generation. ins morphing lips only at around half-way (≈ 11 th epoch) through its training process (≈ 20 epochs <ref type=""bibr"" target=""#b17"">[18]</ref>). Thus, it is crucial to have an additional discriminator . Thus, it is crucial to have an additional discriminator to judge lip-sync, as also done in LipGAN <ref type=""bibr"" target=""#b17"">[18]</ref>. But, how powerful is the discriminator employed in LipGAN construction loss <ref type=""bibr"" target=""#b16"">[17]</ref> or train a discriminator in a GAN setup <ref type=""bibr"" target=""#b17"">[18]</ref>, we use a pre-trained discriminator that is already quite ner 3.4.1 Generator Architecture Details. We use a similar generator architecture as used by LipGAN <ref type=""bibr"" target=""#b17"">[18]</ref>. Our key contribution lies in training this with the exper sed architecture by explaining how it works during the inference on real videos. Similar to Lip-GAN <ref type=""bibr"" target=""#b17"">[18]</ref>, the model generates a talking face video frame-by-frame. essential for future works that aspire to automatically translate videos (Face-to-Face Translation <ref type=""bibr"" target=""#b17"">[18]</ref>) or rapidly create new video content. We manually transcri ted from our proposed models (green and yellow outlines). We compare with the current best approach <ref type=""bibr"" target=""#b17"">[18]</ref> (red outline). The text is shown for illustration to denot",0
"test set, we create 28K pairs, and this set measures the performance on frontal/near-frontal videos <ref type=""bibr"" target=""#b1"">[2]</ref>.</p><p>We also create 14K pairs using the LRS3 test set, whi",0
"LRS2 train split (≈ 29 hours) with a batch size of 64, with T v = 5 frames using the Adam optimizer <ref type=""bibr"" target=""#b11"">[12]</ref> with an initial learning rate of 1e −3 . Our expert lip-sy ain set <ref type=""bibr"" target=""#b0"">[1]</ref>, with a batch size of 80. We use the Adam optimizer <ref type=""bibr"" target=""#b11"">[12]</ref> with an initial learning rate of 1e −4 and betas β 1 = 0.5",0
"size) is usually a pre-specified parameter. Current works <ref type=""bibr"" target=""#b5"">[6]</ref>, <ref type=""bibr"" target=""#b6"">[7]</ref> did not efficiently handle the change of discriminative attr y a graph auto-encoder, but this method neglects linkage between paper and author and coauthorship. <ref type=""bibr"" target=""#b6"">[7]</ref> addresses the pairwise classification problem by extracting",1
"ef>, the number of authors (i.e., cluster size) is usually a pre-specified parameter. Current works <ref type=""bibr"" target=""#b5"">[6]</ref>, <ref type=""bibr"" target=""#b6"">[7]</ref> did not efficiently ised <ref type=""bibr"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b7"">[8]</ref>, unsupervised <ref type=""bibr"" target=""#b5"">[6]</ref>, <ref type=""bibr"" target=""#b8"">[9]</ref> and graph-based one of author mentions belonging to the same author and are essential in the name disambiguation task. <ref type=""bibr"" target=""#b5"">[6]</ref> first learns representation for every name mention in a pair ative attribute to separate papers into small blocks and we use the same trainset and testset as in <ref type=""bibr"" target=""#b5"">[6]</ref>.</p><p>In Semantic Scholar, the selected meta-paths of our m block as sequence s ∈ S; 3 Construct meta-path based view {G p1</figDesc><table /><note>• Aminer-AND<ref type=""bibr"" target=""#b5"">[6]</ref>: This dataset contains 70,285 records of 12,798 unique autho",1
"to effective representation ability. While most GNN works <ref type=""bibr"" target=""#b9"">[10]</ref>- <ref type=""bibr"" target=""#b11"">[12]</ref> focus on transductive setting, there have been some recent",0
"blication data in digital libraries accurate, consistent, and up to date.</p><p>Name disambiguation <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b2"">[3]</ref>, which aims to iden umber of authors with the same name.</p><p>In existing clustering based name disambiguation methods <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b2"">[3]</ref>, <ref type=""bibr"" t ef type=""bibr"" target=""#b5"">[6]</ref>, <ref type=""bibr"" target=""#b8"">[9]</ref> and graph-based ones <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b4"">[5]</ref>. Graph-based works gical features in the academic network to enhance the representation of papers. For instance, GHOST <ref type=""bibr"" target=""#b1"">[2]</ref> constructs document graph based on co-authorship. <ref type= by relations of authors and papers and cluster them by hierarchical agglomerative algorithm.• GHOST<ref type=""bibr"" target=""#b1"">[2]</ref>: GHOST use affinity propagation algorithm for clustering on",0
"owledge graphs.</p><p>Heterogeneous information networks <ref type=""bibr"" target=""#b15"">[16]</ref>- <ref type=""bibr"" target=""#b18"">[19]</ref> have been studied in recent years. Meta-path is designed t",0
"diverse semantic information of node type and edge type <ref type=""bibr"" target=""#b19"">[20]</ref>- <ref type=""bibr"" target=""#b21"">[22]</ref>. GTN <ref type=""bibr"" target=""#b22"">[23]</ref> converts he",0
"Fraser University. The change of discriminative attributes may lead to the paper separation problem <ref type=""bibr"" target=""#b3"">[4]</ref>, i.e., papers of an author are regarded as belonging to diff",0
"Name disambiguation methods can be divided into supervised <ref type=""bibr"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b7"">[8]</ref>, unsupervised <ref type=""bibr"" target=""#b5"">[6]</ref>, <ref",0
". They brought great advancements in many applications of neural network, such as visual perception <ref type=""bibr"" target=""#b21"">[22]</ref>, <ref type=""bibr"" target=""#b22"">[23]</ref>, <ref type=""bib",1
""">[2]</ref>, <ref type=""bibr"" target=""#b2"">[3]</ref> to automatically designed neural architectures <ref type=""bibr"" target=""#b3"">[4]</ref>, <ref type=""bibr"" target=""#b4"">[5]</ref>, <ref type=""bibr"" t es human experts to frequently try and evaluate numerous different operation and connection options <ref type=""bibr"" target=""#b3"">[4]</ref>. In contrast to architectures that are manually designed, th NAS-generated architectures have shown promising results in many domains, such as image recognition <ref type=""bibr"" target=""#b3"">[4]</ref>, <ref type=""bibr"" target=""#b4"">[5]</ref>, <ref type=""bibr"" t rates a total search space of itive training procedure of each selected architecture can be avoided <ref type=""bibr"" target=""#b3"">[4]</ref>, <ref type=""bibr"" target=""#b15"">[16]</ref> so that researche",1
"t. (1) Different search space is utilized, e.g., range of macro skeletons of the whole architecture <ref type=""bibr"" target=""#b10"">[11]</ref>, <ref type=""bibr"" target=""#b11"">[12]</ref> and a different bibr"" target=""#b12"">[13]</ref>, <ref type=""bibr"" target=""#b13"">[14]</ref>, different regularization <ref type=""bibr"" target=""#b10"">[11]</ref>, different scheduler <ref type=""bibr"" target=""#b14"">[15]</ ge as inspired by <ref type=""bibr"" target=""#b6"">[7]</ref>, <ref type=""bibr"" target=""#b7"">[8]</ref>, <ref type=""bibr"" target=""#b10"">[11]</ref>. We summarize characteristics of our NATS-Bench and NAS-Be ed NAS algorithms <ref type=""bibr"" target=""#b4"">[5]</ref>, <ref type=""bibr"" target=""#b7"">[8]</ref>, <ref type=""bibr"" target=""#b10"">[11]</ref>. As shown in the middle part of Figure <ref type=""figure"" ed NAS algorithms <ref type=""bibr"" target=""#b6"">[7]</ref>, <ref type=""bibr"" target=""#b7"">[8]</ref>, <ref type=""bibr"" target=""#b10"">[11]</ref>. Since all cells in an architecture have the same topology ure to set up the hyper-parameters and training strategies <ref type=""bibr"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b10"">[11]</ref>, <ref type=""bibr"" target=""#b14"">[15]</ref>. We train each",0
"nity, there has been a growing attention to the field of joint searching for both topology and size <ref type=""bibr"" target=""#b42"">[43]</ref>, <ref type=""bibr"" target=""#b43"">[44]</ref>. By benchmarkin o regularize the discovered architecture to be efficient <ref type=""bibr"" target=""#b20"">[21]</ref>, <ref type=""bibr"" target=""#b42"">[43]</ref>, <ref type=""bibr"" target=""#b43"">[44]</ref>?</p><p>Since th",0
"eport the performance, e.g., different data augmentation <ref type=""bibr"" target=""#b12"">[13]</ref>, <ref type=""bibr"" target=""#b13"">[14]</ref>, different regularization <ref type=""bibr"" target=""#b10"">[",0
"type=""bibr"" target=""#b37"">[38]</ref>, mutation-based NAS <ref type=""bibr"" target=""#b38"">[39]</ref>, <ref type=""bibr"" target=""#b39"">[40]</ref>, etc.</p><p>Architecture Computational Costs: NATS-Bench p",0
"rchitectures which may be insufficient to comprehensively evaluate NAS algorithms. NAS-Bench-1shot1 <ref type=""bibr"" target=""#b19"">[20]</ref> reuses the NAS-Bench-101 dataset with some modification to ibility and generalization ability of the NAS algorithms <ref type=""bibr"" target=""#b17"">[18]</ref>, <ref type=""bibr"" target=""#b19"">[20]</ref>, <ref type=""bibr"" target=""#b24"">[25]</ref>, <ref type=""bib thm-agnostic while NAS-Bench-101 without any modification is only applicable to selected algorithms <ref type=""bibr"" target=""#b19"">[20]</ref>, <ref type=""bibr"" target=""#b27"">[28]</ref>. The original c evoted their effort to building a fair comparison and development environments for NAS. Zela et al. <ref type=""bibr"" target=""#b19"">[20]</ref> proposed a general framework for oneshot NAS methods and r ensive computational cost. We bring some results from <ref type=""bibr"" target=""#b17"">[18]</ref> and <ref type=""bibr"" target=""#b19"">[20]</ref> to provide some preliminary evidence of generalization. In",0
"structure has shown a surprising robustness and accuracy <ref type=""bibr"" target=""#b44"">[45]</ref>, <ref type=""bibr"" target=""#b45"">[46]</ref>. Regarding the parameters vs. the accuracy, the candidates",0
"<ref type=""bibr"" target=""#b0"">[1]</ref>, Inception <ref type=""bibr"" target=""#b2"">[3]</ref>, VGGNet <ref type=""bibr"" target=""#b8"">[9]</ref>, and Transformer <ref type=""bibr"" target=""#b9"">[10]</ref>. H",0
"16-120. (II) In many vision tasks, pyramid structure has shown a surprising robustness and accuracy <ref type=""bibr"" target=""#b44"">[45]</ref>, <ref type=""bibr"" target=""#b45"">[46]</ref>. Regarding the",0
"nformation, they suffer from sensitivity to data alignment, often involve complicated architectures <ref type=""bibr"" target=""#b10"">[11]</ref>, <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bib -based proposal generation might fail in some cases that could only be observed from 3D space. MV3D <ref type=""bibr"" target=""#b10"">[11]</ref> and AVOD <ref type=""bibr"" target=""#b11"">[12]</ref> project",1
""">[11]</ref>, <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" target=""#b12"">[13]</ref>, <ref type=""bibr"" target=""#b13"">[14]</ref>, and typically require pixel-level correspondences of sens better correspondence, MMF <ref type=""bibr"" target=""#b25"">[26]</ref> adopts continuous convolution <ref type=""bibr"" target=""#b13"">[14]</ref> to build dense LiDAR BEV feature maps and do point-wise fe",1
"data alignment, often involve complicated architectures <ref type=""bibr"" target=""#b10"">[11]</ref>, <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" target=""#b12"">[13]</ref>, <ref type=""bib that could only be observed from 3D space. MV3D <ref type=""bibr"" target=""#b10"">[11]</ref> and AVOD <ref type=""bibr"" target=""#b11"">[12]</ref> project the raw point cloud into bird's eye view (BEV) to",1
"xmlns=""http://www.tei-c.org/ns/1.0""><head>A. 3D Detection Using 2D Images</head><p>Mousavian et al. <ref type=""bibr"" target=""#b14"">[15]</ref> leverage the geometric constraints between 2D and 3D bound",0
"ype=""bibr"" target=""#b7"">[8]</ref>, Fast PointRCNN <ref type=""bibr"" target=""#b19"">[20]</ref> and STD <ref type=""bibr"" target=""#b20"">[21]</ref> applies a two-stage architecture that first generate 3D pr",0
"mpared to 2D object detection, which has been well-studied <ref type=""bibr"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b2"">[3]</ref>, <ref type=""bibr"" t",0
"g for object avoidance and navigation. Compared to 2D object detection, which has been well-studied <ref type=""bibr"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" t",0
"ing the similarity between 3D objects and CAD models. <ref type=""bibr"" target=""#b17"">[18]</ref> and <ref type=""bibr"" target=""#b18"">[19]</ref> explore using stereo images to generate dense point cloud",0
"erate 3D proposals in a bottomup manner and then refines these proposals in a second stage. PV-RCNN <ref type=""bibr"" target=""#b21"">[22]</ref> leverages the advantages of both 3D voxel CNN and PointNet f type=""bibr"" target=""#b8"">[9]</ref>, PointRCNN <ref type=""bibr"" target=""#b7"">[8]</ref> and PV-RCNN <ref type=""bibr"" target=""#b21"">[22]</ref>. While not the top performers within the KITTI leaderboard as, PointRCNN <ref type=""bibr"" target=""#b7"">[8]</ref> and Cascade R-CNN, as CLOCs PointCas, PV-RCNN <ref type=""bibr"" target=""#b21"">[22]</ref> and Cascade R-CNN, as CLOCs PVCas. All the other combinati",0
"=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b2"">[3]</ref>, <ref type=""bibr"" target=""#b3"">[4]</ref>, 3D object detection is more challenging with more output pa ead><p>We use a cross entropy entropy loss for target classification, modified by the focal loss in <ref type=""bibr"" target=""#b3"">[4]</ref> with parameters α = 0.25 and γ = 2 to address the large clas",0
"ween 2D and 3D bounding boxes to recover 3D information. <ref type=""bibr"" target=""#b15"">[16]</ref>, <ref type=""bibr"" target=""#b16"">[17]</ref> estimate 3D object information by calculating the similari",0
"=""#b4"">[5]</ref>, <ref type=""bibr"" target=""#b5"">[6]</ref>, <ref type=""bibr"" target=""#b6"">[7]</ref>, <ref type=""bibr"" target=""#b7"">[8]</ref>, <ref type=""bibr"" target=""#b8"">[9]</ref> are hampered by typ etection; it enables inference at 62 Hz; Compared with one-stage methods discussed above, PointRCNN <ref type=""bibr"" target=""#b7"">[8]</ref>, Fast PointRCNN <ref type=""bibr"" target=""#b19"">[20]</ref> an type=""bibr"" target=""#b5"">[6]</ref>, PointPillars <ref type=""bibr"" target=""#b8"">[9]</ref>, PointRCNN <ref type=""bibr"" target=""#b7"">[8]</ref> and PV-RCNN <ref type=""bibr"" target=""#b21"">[22]</ref>. While ef> and Cascade R-CNN <ref type=""bibr"" target=""#b30"">[31]</ref>, written as CLOCs SecCas, PointRCNN <ref type=""bibr"" target=""#b7"">[8]</ref> and Cascade R-CNN, as CLOCs PointCas, PV-RCNN <ref type=""bib",0
"ne-stage methods discussed above, PointRCNN <ref type=""bibr"" target=""#b7"">[8]</ref>, Fast PointRCNN <ref type=""bibr"" target=""#b19"">[20]</ref> and STD <ref type=""bibr"" target=""#b20"">[21]</ref> applies",0
"ef> estimate 3D object information by calculating the similarity between 3D objects and CAD models. <ref type=""bibr"" target=""#b17"">[18]</ref> and <ref type=""bibr"" target=""#b18"">[19]</ref> explore usin",0
"target=""#b23"">[24]</ref>, Pointfusion <ref type=""bibr"" target=""#b12"">[13]</ref> and Frustum ConvNet <ref type=""bibr"" target=""#b24"">[25]</ref> are the representatives of 2D driven 3D detectors, which e",0
"mising application area for deep learning <ref type=""bibr"" target=""#b11"">(Graves et al., 2020;</ref><ref type=""bibr"" target=""#b16"">Ingraham et al., 2019;</ref><ref type=""bibr"" target=""#b24"">Pereira et aphQA <ref type=""bibr"" target=""#b1"">(Baldassarre et al., 2020)</ref> on MQA, Structured Transformer <ref type=""bibr"" target=""#b16"">(Ingraham et al., 2019)</ref> on CPD, and ProteinSolver <ref type=""bi bstantial improvement both in terms of perplexity and sequence recovery over Structured Transformer <ref type=""bibr"" target=""#b16"">(Ingraham et al., 2019)</ref>, a GNN method which was trained using t ve model over the space of protein sequences conditioned on the given backbone structure. Following <ref type=""bibr"" target=""#b16"">Ingraham et al. (2019)</ref>, we frame this as an autoregressive task the same training and validation sets (Table <ref type=""table"" target=""#tab_3"">3</ref>). Following <ref type=""bibr"" target=""#b16"">Ingraham et al. (2019)</ref>, we report evaluation on short (100 or f >Protein design As described in the main text, we use the CATH 4.2 dataset and splits as curated by <ref type=""bibr"" target=""#b16"">Ingraham et al. (2019)</ref> and the TS50 test set as curated by <ref et should bear minimal similarity to the training structures. We use the CATH 4.2 dataset curated by<ref type=""bibr"" target=""#b16"">Ingraham et al. (2019)</ref> in which all available structures with 4",1
"target=""#b11"">(Graves et al., 2020;</ref><ref type=""bibr"" target=""#b16"">Ingraham et al., 2019;</ref><ref type=""bibr"" target=""#b24"">Pereira et al., 2016;</ref><ref type=""bibr"" target=""#b29"">Townshend e",0
"Transformer <ref type=""bibr"" target=""#b16"">(Ingraham et al., 2019)</ref> on CPD, and ProteinSolver <ref type=""bibr"" target=""#b27"">(Strokach et al., 2020)</ref> on CPD and mutation stability predictio /ref> 33.6 Wang's model <ref type=""bibr"" target=""#b32"">(Wang et al., 2018)</ref> 33.0 ProteinSolver <ref type=""bibr"" target=""#b27"">(Strokach et al., 2020)</ref> 30.8 SPIN <ref type=""bibr"" target=""#b20",0
"ructure and contact-based features<ref type=""bibr"" target=""#b5"">(Cheng et al., 2019)</ref>. FaeNNz 7<ref type=""bibr"" target=""#b28"">(Studer et al., 2020)</ref>, ProQ3D<ref type=""bibr"" target=""#b30"">(Uz",0
"ifferent domains require specialized normalization methods. In computer vision, batch normalization <ref type=""bibr"" target=""#b15"">[16]</ref> is a standard component. While in natural language process h set of feature values the normalization is applied to. For example, in computer vision, BatchNorm <ref type=""bibr"" target=""#b15"">[16]</ref> is the de facto method that normalizes the feature values ring testing, the estimated dataset-level statistics are used instead of the batch-level statistics <ref type=""bibr"" target=""#b15"">[16]</ref>.</p><p>In GNNs, for each feature dimension, the BatchNorm -invariant"" property <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b15"">16]</ref>. In comparison, BatchNorm in the lower branch suffers from e-invariant"" property<ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b15"">16]</ref>. In comparison, BatchNorm in the lower branch suffers from ed to improve the training process in different applications <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" tar h of the parameters; <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b20"">21]</ref> show that the norma rom BatchNorm layers under different settings of batch sizes <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b31"">32</ref>, 64], as in Figure < matrix in layer k. We apply the normalization after the linear transformation as in previous works <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" ta hich try to provide accurate estimations for the mean and standard deviation over the whole dataset <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" ta",1
"get=""#b23"">24,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" target=""#b35"">36]</ref>. The reason behind",1
"leads to unsatisfactory performance, see Figure10in Appendix. Similar phenomena are also observed in<ref type=""bibr"" target=""#b8"">[9]</ref>.</note> 		</body> 		<back>  			<div type=""acknowledgement"">",0
"ntroduce the Cauchy interlace theorem:</p><p>Lemma A.1 (Cauchy interlace theorem (Theorem 4.3.17 in <ref type=""bibr"" target=""#b13"">[14]</ref>)). Let S ∈ R (n−1)×(n−1) be symmetric, y ∈ R n and a ∈ R b",0
"ign, and it has been shown crucial when a neural network gets deeper, wider, and more sophisticated <ref type=""bibr"" target=""#b11"">[12]</ref>. Normalization methods that shift and scale feature values chNorm as in <ref type=""bibr"" target=""#b36"">[37]</ref> on the PROTEINS dataset and train a ResNet18 <ref type=""bibr"" target=""#b11"">[12]</ref> on the CIFAR10 dataset for comparison. The batch size of a the number of sub-layers in MLP is set to 2. For image task (CIFAR10 dataset), we train a ResNet18 <ref type=""bibr"" target=""#b11"">[12]</ref>. Note that for a 5-layer GIN model, it has four graph conv",0
"benchmark datasets of different scales in the experiments <ref type=""bibr"" target=""#b36"">[37,</ref><ref type=""bibr"" target=""#b39"">40]</ref>, including four medium-scale bioinformatics datasets (MUTAG",0
"[16]</ref> is a standard component. While in natural language processing (NLP), layer normalization <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b35"">36]</ref> is more popularly us rmalization techniques have been proposed to improve the training process in different applications <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" targ zes the feature values in the same channel across different samples in the batch. In NLP, LayerNorm <ref type=""bibr"" target=""#b3"">[4]</ref> is more popularly used, which normalizes the feature values",0
"alization ability <ref type=""bibr"" target=""#b38"">[39]</ref>, and infinite-width asymptotic behavior <ref type=""bibr"" target=""#b6"">[7]</ref>. These theoretical understandings lead to GNN architectures he same. We study the output of the standard shift operation in the first layer, i.e., k = 1 in Eq. <ref type=""bibr"" target=""#b6"">(7)</ref>. From the following proposition, we can see that when the st",0
"learning models are shown to be able to interpolate the data <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"">41</ref>]. But we found that GIN and GCN wi",0
"type=""bibr"" target=""#b20"">21]</ref> show that the normalization implicitly tunes the learning rate. <ref type=""bibr"" target=""#b27"">[28]</ref> reveals that normalization smooths the optimization landsc",0
"al networks has concentrated on node classification task <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" target=""#b12"">[13]</ref>, <ref type=""bibr"" target=""#b13"">[14]</ref>, <ref type=""bib ake a specific node (e.g., a person in social networks) misclassified. In this scenario, Dai et al. <ref type=""bibr"" target=""#b12"">[13]</ref> study the adversarial attack on graph structure data and p olutional Network (SGC) <ref type=""bibr"" target=""#b16"">[17]</ref> and gradient-based attack methods <ref type=""bibr"" target=""#b12"">[13]</ref>, <ref type=""bibr"" target=""#b14"">[15]</ref>, we propose a n cient way to generate destructive adversarial examples. Focusing on the targeted attack, Dai et al. <ref type=""bibr"" target=""#b12"">[13]</ref> propose GradArgmax, which extracts gradients of the surrog Network (GCN)</head><p>Since a number of existing works <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" target=""#b12"">[13]</ref>, <ref type=""bibr"" target=""#b14"">[15]</ref>, <ref type=""bib f-the-art targeted attack methods: Nettack <ref type=""bibr"" target=""#b11"">[12]</ref> and GradArgmax <ref type=""bibr"" target=""#b12"">[13]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head ly nodes belonging to the same class/different classes will be disconnected/connected. • GradArgmax <ref type=""bibr"" target=""#b12"">[13]</ref>. Since the attack budget ∆ is defined as the degrees of ta",1
"><p>R ECENTLY, with the enormous advancement of deep learning, many domains like speech recognition <ref type=""bibr"" target=""#b0"">[1]</ref> and visual object recognition <ref type=""bibr"" target=""#b1"">",0
"=""#b5"">[6]</ref>, <ref type=""bibr"" target=""#b6"">[7]</ref>, <ref type=""bibr"" target=""#b7"">[8]</ref>, <ref type=""bibr"" target=""#b8"">[9]</ref>. Undoubtedly, graph plays a crucial role in many high impact",0
"fication task <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" target=""#b12"">[13]</ref>, <ref type=""bibr"" target=""#b13"">[14]</ref>, <ref type=""bibr"" target=""#b14"">[15]</ref>. In order to fo",0
"of the current work on attacking graph neural networks has concentrated on node classification task <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" target=""#b12"">[13]</ref>, <ref type=""bib luence attack is limited to a few attacker nodes (usually the neighboring nodes of the target node) <ref type=""bibr"" target=""#b11"">[12]</ref>. Figure <ref type=""figure"">1</ref> demonstrates a toy exam s based on gradients of a surrogate model so as to fool the classifiers. In addition, Zügner et al. <ref type=""bibr"" target=""#b11"">[12]</ref> propose Nettack, which is capable of perturbing the Fig. < ween nodes with high correlation and connecting edges with low correlation. Moreover, Zügner et al. <ref type=""bibr"" target=""#b11"">[12]</ref> study both poisoning attacks and evasion attacks (a.k.a te ies of graphs and ensure the unnoticeability in most cases.</p><p>To bridge this gap, Zügner et al. <ref type=""bibr"" target=""#b11"">[12]</ref> enforce the perturbations to ensure its unnoticeability by ""><head n=""3.2.1"">Vanilla Graph Convolution Network (GCN)</head><p>Since a number of existing works <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" target=""#b12"">[13]</ref>, <ref type=""bib =""#b22"">[23]</ref> to modify the graph structure or node features, respectively. Following the work <ref type=""bibr"" target=""#b11"">[12]</ref>, we assume that attackers have prior knowledge about the g on task, here we briefly introduce other proposed state-of-the-art targeted attack methods: Nettack <ref type=""bibr"" target=""#b11"">[12]</ref> and GradArgmax <ref type=""bibr"" target=""#b12"">[13]</ref>.< model SGC, A ⊆ V is the set of attacker nodes and the perturbations are constrained to these nodes <ref type=""bibr"" target=""#b11"">[12]</ref>.</p><p>In particular, we set A = {t} for direct attack and q is given. However, there is no solution for estimating q exactly yet. To this end, Zügner et al. <ref type=""bibr"" target=""#b11"">[12]</ref> derive an efficient way to check for violations of the deg ula_37"">Λ (q, q ; G, G ) &lt; τ ≈ 0.004 ,</formula><p>where Λ is a discriminate function defined in <ref type=""bibr"" target=""#b11"">[12]</ref>, G denotes the original graph and G the perturbed one.</p> er posts and comments on content in different topical communities. We follow the setting of Nettack <ref type=""bibr"" target=""#b11"">[12]</ref> and only consider the largest connected component of the g ame surrogate model SGC (if necessary), and share the same weights θ. Follow the setting of Nettack <ref type=""bibr"" target=""#b11"">[12]</ref>, the attack budget ∆ is set to the degrees of target node instantiation of the adjacency matrix and computes the gradients of all N 2 edges.</p><p>• Nettack <ref type=""bibr"" target=""#b11"">[12]</ref>. Nettack is the strongest baseline that can modify the gra nt attack methods. Nettack, a strong baseline, also yields a significant performance as reported in <ref type=""bibr"" target=""#b11"">[12]</ref>. Most remarkably, even in attacking other robust graph neu /www.tei-c.org/ns/1.0"" place=""foot"" n=""1"" xml:id=""foot_0"">. We follow the attack settings of Nettack<ref type=""bibr"" target=""#b11"">[12]</ref> which modifies graph structure and node features by flippi",0
"orhood aggregation function), GVAE with GCN encoder Kipf &amp; Welling (2016), DGI with GIN encoder <ref type=""bibr"" target=""#b45"">Velickovic et al. (2019)</ref>, and EGI with GIN encoder. We train GV small), our t-tests have shown the improvements of EGI to be significant. 55.56% ? 6.83% DGI (GIN) <ref type=""bibr"" target=""#b45"">Velickovic et al. (2019)</ref> 57.75% ? 4.47% 62.44% ? 4.46% 68.15% ?",1
"rget=""#b9"">Defferrard et al. (2016)</ref>; <ref type=""bibr"" target=""#b5"">Bruna et al. (2014)</ref>; <ref type=""bibr"" target=""#b15"">Hammond et al. (2011)</ref>. While most GNN architectures are not ver",0
"rget=""#b7"">Chen et al. (2019)</ref>; <ref type=""bibr"" target=""#b31"">Oono &amp; Suzuki (2020)</ref>; <ref type=""bibr"" target=""#b21"">Huang et al. (2018)</ref>, due to their established performance towar",0
"le"" target=""#tab_7"">8</ref>. On the target graph, we also have the access to 24 different relations <ref type=""bibr"" target=""#b39"">Shi et al. (2018)</ref> such as isAdvisedBy, isMarriedTo and so on. S",0
"y or with only very few labeled examples <ref type=""bibr"" target=""#b26"">(Radford et al., 2019;</ref><ref type=""bibr"" target=""#b30"">Schick and Schütze, 2020a)</ref>.</p><p>Very recently, <ref type=""bib mited to a few hundred tokens.</p><p>An alternative to priming is pattern-exploiting training (PET) <ref type=""bibr"" target=""#b30"">(Schick and Schütze, 2020a)</ref>, which combines the idea of reformu are understood well by LMs is difficult <ref type=""bibr"" target=""#b12"">(Jiang et al., 2019)</ref>, <ref type=""bibr"" target=""#b30"">Schick and Schütze (2020a)</ref> propose PET, a method that uses know =""#b4"">(Dagan et al., 2006)</ref> are textual entailment tasks like MNLI, so we use PVPs similar to <ref type=""bibr"" target=""#b30"">Schick and Schütze (2020a)</ref>. For a premise p and hypothesis h, w ts. 5 We run PET on the FewGLUE training sets for all SuperGLUE tasks using the exact same setup as <ref type=""bibr"" target=""#b30"">Schick and Schütze (2020a)</ref>. For COPA, WSC and ReCoRD, we use ou ref>). Given 32 examples, PET clearly outperforms both baselines, which is in line with findings by <ref type=""bibr"" target=""#b30"">Schick and Schütze (2020a)</ref>.</p><p>We next compare PET directly t segments and mark p in s with asterisks.</p><p>MultiRC Deviating from the hyperparameters used by <ref type=""bibr"" target=""#b30"">Schick and Schütze (2019)</ref>, we use a maximum sequence length of",1
"nstead opt for multiclass hinge loss <ref type=""bibr"" target=""#b37"">(Weston and Watkins, 1999;</ref><ref type=""bibr"" target=""#b8"">Dogan et al., 2016)</ref> and minimize:</p><formula xml:id=""formula_5""",0
"es some modifications during training and inference that are discussed in Appendix A.</p><p>MultiRC <ref type=""bibr"" target=""#b13"">(Khashabi et al., 2018</ref>) is a QA task. Given a passage p, a ques",0
"rget=""#b30"">Schick and Schütze (2020a)</ref> propose PET, a method that uses knowledge distillation <ref type=""bibr"" target=""#b11"">(Hinton et al., 2015)</ref> to easily combine several reformulations. tween its output and q P .</p><p>As steps (2) and (3) above closely resemble knowledge distillation <ref type=""bibr"" target=""#b11"">(Hinton et al., 2015)</ref>, we also refer to them simply as distilla",0
"mentation of PET by Schick and Schütze (2020a) which, in turn, is based on the Transformers library <ref type=""bibr"" target=""#b39"">(Wolf et al., 2019)</ref> and PyTorch <ref type=""bibr"" target=""#b21"">",0
"two well-accepted propositions-deep neural networks learn meaningful patterns before fitting noise <ref type=""bibr"" target=""#b2"">[3]</ref> and minimum entropy regularisation principle [10]-we propose meaningful patterns before fitting noise, even when severe label noise exists in human annotations <ref type=""bibr"" target=""#b2"">[3]</ref>. (2) As a learner attains confident knowledge as time progre e meaningful patterns before fitting noise, even when severe label noise exists in human annotations<ref type=""bibr"" target=""#b2"">[3]</ref>. (2) As a learner attains confident knowledge as time progre",1
"mum entropy regularisation, which is widely evaluated in unsupervised and semi-supervised scenarios <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b9"">10]</ref>.</p><p>Secondly, note imum entropy regularisation, which is widely evaluated in unsupervised and semi-supervised scenarios<ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b9"">10]</ref>.Secondly, note that O n machine learning <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b37"">38,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target",1
"tion of noise labels <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b40"">41,</ref><ref type=""bibr"" target=""#b43"">44,</ref><ref type=""bibr"" tar",0
"n auxiliary trusted training set to differentiate examples <ref type=""bibr"" target=""#b44"">[45,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b15"">16]</ref>. This requires extr",0
"ibr"" target=""#b28"">29]</ref> and confidence penalty (CP) <ref type=""bibr"" target=""#b32"">[33]</ref>; <ref type=""bibr"" target=""#b1"">(2)</ref> Label correction (LC). On the one hand, LC regularises neura sible knowledge from other learners or itself (Figure <ref type=""figure"" target=""#fig_1"">1a</ref>); <ref type=""bibr"" target=""#b1"">(2)</ref> Non-self LC relies on accurate auxiliary models to generate layer, while Masking <ref type=""bibr"" target=""#b11"">[12]</ref> exploits human cognition. MD-DYR-SH <ref type=""bibr"" target=""#b1"">[2]</ref> is a combination of three techniques: dynamic mixup (MD), dy",0
"ven or we need to estimate a noisetransition matrix, which defines the distribution of noise labels <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" targ get=""#b44"">45,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" tar",0
"arget=""#b7"">8,</ref><ref type=""bibr"" target=""#b40"">41,</ref><ref type=""bibr"" target=""#b43"">44,</ref><ref type=""bibr"" target=""#b51"">52,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" tar get=""#b22"">23,</ref><ref type=""bibr"" target=""#b34"">35,</ref><ref type=""bibr"" target=""#b42"">43,</ref><ref type=""bibr"" target=""#b51"">52,</ref><ref type=""bibr"" target=""#b24"">25]</ref>, which relate to ou",0
"tion is the most widely used principle in machine learning <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b37"">38,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" targe",0
"is to annotate unlabelled samples or correct noisy labels.</p><p>LC and knowledge distillation (KD) <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b16"">17]</ref>. Mathematically, we",0
"to estimate in practice; (2) Exploiting an auxiliary trusted training set to differentiate examples <ref type=""bibr"" target=""#b44"">[45,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" ta ally necessary for any method and differs from the methods <ref type=""bibr"" target=""#b43"">[44,</ref><ref type=""bibr"" target=""#b44"">45,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" tar",0
"the support of better branch prediction, which could potentially offer more IPC gains. Prior works <ref type=""bibr"" target=""#b12"">[13]</ref>, <ref type=""bibr"" target=""#b13"">[14]</ref> have tried to a hose address is very predictable. Moreover, we do not make any modifications to the ISA. Gao et al. <ref type=""bibr"" target=""#b12"">[13]</ref> proposed a closely related work. They correlate the branch",1
"ised the bar for the prediction accuracy. Yeh and Patt came up with the two-level branch predictors <ref type=""bibr"" target=""#b24"">[25]</ref>. McFarling <ref type=""bibr"" target=""#b25"">[26]</ref> propo",0
"hich could potentially offer more IPC gains. Prior works <ref type=""bibr"" target=""#b12"">[13]</ref>, <ref type=""bibr"" target=""#b13"">[14]</ref> have tried to address different types of hardto-predict br sing data values as an input to the branch predictor improves the misprediction rate. Farooq et al. <ref type=""bibr"" target=""#b13"">[14]</ref> note that some hard-to-predict data-dependent branches man",0
"sub-section, we will explain load-branch chains in detail. LDBP needs to capture the backward slice <ref type=""bibr"" target=""#b17"">[18]</ref> of operation sequence starting from the branch. The exit p s power overhead. There are several works in the academic literature about building backward slices <ref type=""bibr"" target=""#b17"">[18]</ref>. We use a table indexed by the retiring logical register (",0
">, which is a derivative of its previous implementation from Championship Branch Prediction (CBP-4) <ref type=""bibr"" target=""#b7"">[8]</ref>, combined several of these techniques and was the winner of TAGE-like predictors are excellent, but there are still many difficult-to-predict branches. Seznec <ref type=""bibr"" target=""#b7"">[8]</ref>  <ref type=""bibr"" target=""#b6"">[7]</ref> studied the predict",0
"unctions. To exploit the sparsity of DNNs to improve energy efficiency, many architectures, such as <ref type=""bibr"" target=""#b24"">[25]</ref>, are proposed to detect and skip the multiplications assoc",1
"essing-in-memory (PIM) architectures <ref type=""bibr"" target=""#b3"">[4]</ref> and silicon prototypes <ref type=""bibr"" target=""#b4"">[5]</ref> stand out recently due to their ultra good energy efficiency",0
"mance is highly sensitive to data communication bandwidth and memory access latency. Reconfigurable <ref type=""bibr"" target=""#b13"">[14]</ref> and sparsity-aware <ref type=""bibr"" target=""#b14"">[15]</re ef type=""bibr"" target=""#b7"">[8]</ref> Wang et al. <ref type=""bibr"" target=""#b32"">[33]</ref> Thinker <ref type=""bibr"" target=""#b13"">[14]</ref> iFPNA <ref type=""bibr"" target=""#b27"">[28]</ref> This Work",0
"systemlevel in-memory computing prototype, capable of speech recognition, was recently reported in <ref type=""bibr"" target=""#b16"">[17]</ref>. It utilized multiple PIM macros. The precision issue was flow optimization to reduce unnecessary data movement. Only output-stationary designs were reported <ref type=""bibr"" target=""#b16"">[17]</ref>. Because analog signals' replication, storage, and accumul",0
"ng the reliability of analog computing. As an alternative, hybrid analog-digital computing circuits <ref type=""bibr"" target=""#b12"">[13]</ref> was proposed to break the limit.</p><p>The second issue is",0
"eam generates about 1.2TB intermediate data to count 4-motif on the MiCo graph with 1 million edges <ref type=""bibr"" target=""#b17"">[18]</ref>.</p><p>Recently, specialized systems have been developed f thms in these specialized pattern matching systems can be described with nested loops, and AutoMine <ref type=""bibr"" target=""#b17"">[18]</ref> and GraphZero <ref type=""bibr"" target=""#b11"">[12]</ref> re ate-of-the-art singlemachine pattern matching systems. GraphZero is an upgraded version of AutoMine <ref type=""bibr"" target=""#b17"">[18]</ref>, and it outperforms AutoMine by up to 40×. Fractal is a JV ef>, <ref type=""bibr"" target=""#b38"">[39]</ref>, <ref type=""bibr"" target=""#b39"">[40]</ref>. Automine <ref type=""bibr"" target=""#b17"">[18]</ref> is built upon a set-based representation and uses compilat",1
"Description Wiki-Vote <ref type=""bibr"" target=""#b30"">[31]</ref> 7.1K 100.8K Wiki Editor Voting MiCo <ref type=""bibr"" target=""#b31"">[32]</ref> 96.6K 1.1M Co-authorship Patents <ref type=""bibr"" target=""",0
"r of embeddings <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b41"">[42]</ref>, <ref type=""bibr"" target=""#b42"">[43]</ref>. ASAP <ref type=""bibr"" target=""#b22"">[23]</ref> is the sta",0
">[13]</ref>.</p><p>Recently, researchers have proposed several general-purpose graph mining systems <ref type=""bibr"" target=""#b13"">[14]</ref>- <ref type=""bibr"" target=""#b16"">[17]</ref>, such as Arabes abstractions and flexible programming models to express complex graph mining algorithms. Arabesque <ref type=""bibr"" target=""#b13"">[14]</ref> is a distributed graph mining system that uses a filter-pr /ref> and QKCount <ref type=""bibr"" target=""#b36"">[37]</ref>) and general-purpose systems (Arabesque <ref type=""bibr"" target=""#b13"">[14]</ref> and GraphFrames <ref type=""bibr"" target=""#b37"">[38]</ref>) ww.tei-c.org/ns/1.0""><head>VI. RELATED WORK</head><p>General-Purpose Graph Mining Systems Arabesque <ref type=""bibr"" target=""#b13"">[14]</ref> is the first distributed graph mining system that provides",0
"M Social network Orkut <ref type=""bibr"" target=""#b32"">[33]</ref> 3.1M 117.2M Social network Twitter <ref type=""bibr"" target=""#b33"">[34]</ref> 41.7M 1.2B Social network</p><p>Patterns We use six patter",0
"also trigger side effects, adverse reactions, and even serious toxicity, leading patients in danger <ref type=""bibr"" target=""#b1"">[2]</ref>. As there exists increasing needs of multi-drug treatments, unannotated DDIs, and cannot give alerts to potential DDIs before a combinational treatment is made <ref type=""bibr"" target=""#b1"">[2]</ref>. In contrast, machine learning-based methods provide a promi presents drugs in a form of feature vector according to drug properties, such as chemical structure <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b5"">[6]</ref><ref type=""bibr"" targe 2]</ref><ref type=""bibr"" target=""#b12"">[13]</ref><ref type=""bibr"" target=""#b13"">[14]</ref>, targets <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b7"">[8]</ref><ref type=""bibr"" targe ""bibr"" target=""#b11"">[12]</ref>, SVM <ref type=""bibr"" target=""#b11"">[12]</ref>, logistic regression <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target s, then deduces potential DDIs with the well-trained model. Most methods utilize a single predictor <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b4"">[5]</ref><ref type=""bibr"" targe",1
"est methods, the jackknife test is deemed the least arbitrary that can always yield a unique result <ref type=""bibr"" target=""#b24"">[25]</ref>. However, for large scale database, the jackknife test is",0
"wo-layer auto-encoder of graph convolutional network (GCN) <ref type=""bibr"" target=""#b35"">[36,</ref><ref type=""bibr"" target=""#b36"">37]</ref> to obtain embedding representations of drug nodes. Each dru",0
"target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b5"">[6]</ref><ref type=""bibr"" target=""#b6"">[7]</ref><ref type=""bibr"" target=""#b7"">[8]</ref><ref type=""bibr"" target=""#b8"">[9]</ref><ref type=""bibr"" targe [13]</ref><ref type=""bibr"" target=""#b13"">[14]</ref>, targets <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b7"">[8]</ref><ref type=""bibr"" target=""#b8"">[9]</ref><ref type=""bibr"" targe ef type=""bibr"" target=""#b10"">[11]</ref>, Anatomical Therapeutic Chemical classification (ATC) codes <ref type=""bibr"" target=""#b7"">[8]</ref><ref type=""bibr"" target=""#b8"">[9]</ref><ref type=""bibr"" targe rug network structure <ref type=""bibr"" target=""#b5"">[6]</ref><ref type=""bibr"" target=""#b6"">[7]</ref><ref type=""bibr"" target=""#b7"">[8]</ref>, label propagation <ref type=""bibr"" target=""#b12"">[13]</ref> target=""#b4"">[5]</ref><ref type=""bibr"" target=""#b5"">[6]</ref><ref type=""bibr"" target=""#b6"">[7]</ref><ref type=""bibr"" target=""#b7"">[8]</ref><ref type=""bibr"" target=""#b12"">[13]</ref><ref type=""bibr"" tar /ref><ref type=""bibr"" target=""#b9"">[10]</ref><ref type=""bibr"" target=""#b11"">12]</ref>, side effects <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target ef type=""bibr"" target=""#b11"">[12]</ref>, logistic regression <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b9"">10]</ref>, decision tree <ref ty",0
">Feature extractor</head><p>We employ a two-layer auto-encoder of graph convolutional network (GCN) <ref type=""bibr"" target=""#b35"">[36,</ref><ref type=""bibr"" target=""#b36"">37]</ref> to obtain embeddin",0
"nches remain the single most important category of mispredictions to tackle. Prior works like EXACT <ref type=""bibr"" target=""#b8"">[9]</ref>, SLB <ref type=""bibr"" target=""#b17"">[18]</ref> have proposed ei-c.org/ns/1.0""><head n=""2.2"">Existing Techniques to Handle Data Dependent Branches</head><p>EXACT <ref type=""bibr"" target=""#b8"">[9]</ref> is a branch prediction technique proposed to lower mispredic pairs that need to get tracked quickly increase the storage requirement over 10KB. As mentioned in <ref type=""bibr"" target=""#b8"">[9]</ref>, EXACT predictor does not win over an similarly sized TAGE p leading to the branch. To enable this, we employ a scheme similar to EXACT predictor ID generation <ref type=""bibr"" target=""#b8"">[9]</ref> where the ARF is extended to track the load addresses. Inste hat wrote to the same address and use the store data value for override. EXACT's active update unit <ref type=""bibr"" target=""#b8"">[9]</ref> and SLB <ref type=""bibr"" target=""#b17"">[18]</ref> use this o en. But, when data has high entropy, it affects their accuracy. Among recent works, EXACT predictor <ref type=""bibr"" target=""#b8"">[9]</ref> proposes associating data dependent branches with their corr BT. We assume that opcode and other instruction metadata is available with the ROB entry similar to <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b27"">28]</ref>.</p><p>1) 3D-Branch",1
"ally sufficient to predict branch directions with a high degree of accuracy as shown by CBP results <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b14"">15]</ref>.   (RAT), Physical",0
"oduces the value eventually feeding into the data-dependent branches. Control Flow Decoupling (CFD) <ref type=""bibr"" target=""#b35"">[36]</ref>, is another approach to hoist the load out the loop to cre",0
"iding a misprediction.</p><p>In our studies, we morph a highly accurate value predictor called EVES <ref type=""bibr"" target=""#b33"">[34]</ref> into a load address predictor (LAP). It is indexed using t ponent to predict the load address. We disable the stride component to simplify LAP. As observed by <ref type=""bibr"" target=""#b33"">[34]</ref>, this predictor achieves high accuracy predictions with li eder loads.</p><p>3) Load Address Prediction: As mentioned earlier, we repurpose the EVES predictor <ref type=""bibr"" target=""#b33"">[34]</ref> to predict the load addresses. We integrated the 384-entry",0
"mlns=""http://www.tei-c.org/ns/1.0""><head n=""5.8"">Comparison Against CESP</head><p>Palacharla et al. <ref type=""bibr"" target=""#b12"">[13]</ref> propose the complexity-effective superscalar processor (CE oint out the most closely related work.</p><p>Complexity-Effective Architectures. Palacharla et al. <ref type=""bibr"" target=""#b12"">[13]</ref> propose the complexity-effective superscalar processors (C",1
"memory-hierarchy parallelism (MHP). <ref type=""foot"" target=""#foot_0"">1</ref> Load Slice Core (LSC) <ref type=""bibr"" target=""#b4"">[5]</ref> was the first work to propose an sOoO core; Freeway <ref typ TIVATION</head><p>In this section, we briefly cover the background on the two prior sOoO cores -LSC <ref type=""bibr"" target=""#b4"">[5]</ref> and Freeway <ref type=""bibr"" target=""#b9"">[10]</ref> -and we FSC.</p><p>Restricted Out-of-Order Microarchitectures. We extensively discussed the Load Slice Core <ref type=""bibr"" target=""#b4"">[5]</ref> and Freeway <ref type=""bibr"" target=""#b9"">[10]</ref> through",1
"d of the pipeline; other non-ready instructions are dispatched to the out-of-order back-end. CASINO <ref type=""bibr"" target=""#b7"">[8]</ref> pursues a similar goal by augmenting an in-order core with a",0
"roving performance. More recently, Clairvoyance <ref type=""bibr"" target=""#b19"">[20]</ref> and SWOOP <ref type=""bibr"" target=""#b20"">[21]</ref> exploit the decoupled nature of access and execute phases",0
"/p><p>• InO: The baseline stall-on-use in-order core, which is modeled to resemble an ARM Cortex-A7 <ref type=""bibr"" target=""#b1"">[2]</ref>.</p><p>• LSC: The load slice core, as discussed in Section 2",0
"ional OoO core because of frequent dispatch stalls. A similar steering policy is used by Kim et al. <ref type=""bibr"" target=""#b8"">[9]</ref> in their Instruction-Level Distributed Processing (ILDP) wor",0
"bles out-of-order execution capabilities among in-order queues.</p><p>Decoupled Access-Execute. DAE <ref type=""bibr"" target=""#b18"">[19]</ref> is the first work to separate access and execute phases of",0
"evaluate FSC using the most detailed, cycle-level, and hardwarevalidated core model in Sniper v6.0 <ref type=""bibr"" target=""#b3"">[4]</ref>. The configurations for the InO, FSC and OoO cores are provi",0
"ower consumption numbers provided by McPAT.</p><p>We create representative 1B-instruction SimPoints <ref type=""bibr"" target=""#b16"">[17]</ref> for the SPEC CPU2017 benchmarks. We sort the benchmarks by",0
"lative-slice execution <ref type=""bibr"" target=""#b22"">[23]</ref>, flea-flicker multipass pipelining <ref type=""bibr"" target=""#b2"">[3]</ref>, braid processing <ref type=""bibr"" target=""#b21"">[22]</ref>",0
"ing significantly fewer training data comparing to the few previous works which address heterophily <ref type=""bibr"" target=""#b22"">(Pei et al. 2020;</ref><ref type=""bibr"" target=""#b38"">Zhu et al. 2020 nificantly smaller fraction of training samples compared to previous works that address heterophily <ref type=""bibr"" target=""#b22"">(Pei et al. 2020;</ref><ref type=""bibr"" target=""#b38"">Zhu et al. 2020 pe=""bibr"" target=""#b20"">Namata et al. 2012)</ref>. We use the features and class labels provided by <ref type=""bibr"" target=""#b22"">Pei et al. (2020)</ref>.</p></div> <div xmlns=""http://www.tei-c.org/n ch benchmark with an identity matrix I. We use the training, validation and test splits provided by <ref type=""bibr"" target=""#b22"">Pei et al. (2020)</ref>.</p><p>Heterophily. We report results on grap",1
"and Vandergheynst 2016;</ref><ref type=""bibr"" target=""#b12"">Kipf and Welling 2017)</ref>, GraphSAGE <ref type=""bibr"" target=""#b8"">(Hamilton, Ying, and Leskovec 2017)</ref>, MixHop (Abu-El-Haija et al. ibr"" target=""#b35"">Salakhudinov 2016) and</ref><ref type=""bibr"">GraphSAGE (Hamilton, Ying, and</ref><ref type=""bibr"" target=""#b8"">Leskovec 2017)</ref>. More recent works have looked into designs which",0
"current belief, and updates its own belief based on the estimations received from its neighborhood. <ref type=""bibr"" target=""#b13"">Koutra et al. (2011)</ref> and <ref type=""bibr"" target=""#b7"">Gatterba",0
"aphs. For synthetic benchmarks, we generate graphs and node labels following an approach similar to <ref type=""bibr"" target=""#b11"">Karimi et al. (2017)</ref> and Abu-El-Haija et al. ( <ref type=""formu",0
"ding recommendation systems <ref type=""bibr"" target=""#b37"">(Ying et al. 2018)</ref>, bioinformatics <ref type=""bibr"" target=""#b39"">(Zitnik, Agrawal, and Leskovec 2018;</ref><ref type=""bibr"" target=""#b",0
"rial attacks, designing defense mechanisms or building robust variants of GNNs have become critical <ref type=""bibr"" target=""#b25"">(Zhu et al. 2019)</ref>.</p><p>In this paper, we propose a new approa nism to attenuate the influence of neighbors with large variance (potentially corrupted). Following <ref type=""bibr"" target=""#b25"">(Zhu et al. 2019)</ref>, we set hidden dimensions at 16 and assume a poisoning attacks, UM-GNN consistently outperforms existing methods including the recent Robust GCN <ref type=""bibr"" target=""#b25"">(Zhu et al. 2019</ref>);</p><p>• UM-GNN achieves significantly lower tions for specifically defending against adversarial attacks, the recent robust GCN (RGCN) approach <ref type=""bibr"" target=""#b25"">(Zhu et al. 2019</ref>) has been the most effective, when compared to h a weighted aggregation of features in a closed neighborhood where the weights are trainable. RGCN <ref type=""bibr"" target=""#b25"">(Zhu et al. 2019</ref>): This is a recently proposed ap-proach that e random structural perturbations and its low performance strongly corroborates with the findings in <ref type=""bibr"" target=""#b25"">(Zhu et al. 2019</ref>).</p><p>(ii) DICE Attack: In this challenging <ref type=""bibr"" target=""#b26"">(Zügner, Akbarnejad, and Günnemann 2018)</ref>. Recently, Zhu et al. <ref type=""bibr"" target=""#b25"">(Zhu et al. 2019</ref>) introduced a robust variant of GCN based on a",1
"r three benchmark citation networks extensively used in similar studies: Cora, Citeseer, and Pubmed <ref type=""bibr"" target=""#b14"">(Sen et al. 2008)</ref>. The documents are represented by nodes, and",0
"graph adversarial attacks have been proposed <ref type=""bibr"" target=""#b11"">(Jin et al. 2020;</ref><ref type=""bibr"" target=""#b17"">Sun et al. 2018)</ref>. Adversarial attacks on graphs can be broadly",0
"ph neural network M is at the core of UM-GNN. We propose to utilize Bayesian Neural Networks (BNNs) <ref type=""bibr"" target=""#b1"">(Blundell et al. 2015)</ref>, in particular its scalable variant based",0
"tacks. The implementations for Mettack, PGD and FGA were based on the publicly available DeepRobust <ref type=""bibr"" target=""#b11"">(Jin et al. 2020)</ref> library. Due to the lack of computationally e rnejad, and Günnemann 2018)</ref>. Since then, several graph adversarial attacks have been proposed <ref type=""bibr"" target=""#b11"">(Jin et al. 2020;</ref><ref type=""bibr"" target=""#b17"">Sun et al. 2018 accard similarity between the constituent nodes were removed prior to training a GNN. Similarly, in <ref type=""bibr"" target=""#b11"">(Jin et al. 2019)</ref>, explicit graph smoothing was performed by tr",0
"e=""bibr"" target=""#b7"">Garg et al., 2017;</ref><ref type=""bibr"" target=""#b19"">May et al., 2010;</ref><ref type=""bibr"" target=""#b38"">Zhao et al., 2018;</ref><ref type=""bibr"" target=""#b27"">Rudinger et al",1
"arget=""#b28"">(Rudinger et al., 2018;</ref><ref type=""bibr"" target=""#b34"">Webster et al., 2018;</ref><ref type=""bibr"" target=""#b6"">Dinan et al., 2020)</ref> and relation extraction <ref type=""bibr"" tar",0
"n, and BERT being significantly behind both <ref type=""bibr"" target=""#b33"">(Wang et al., 2018;</ref><ref type=""bibr"" target=""#b13"">Lai et al., 2017;</ref><ref type=""bibr"" target=""#b25"">Rajpurkar et al",0
"that workers be in the United States and have a &gt; 98% acceptance rate. We use the Fair Work tool <ref type=""bibr"" target=""#b35"">(Whiting et al., 2019)</ref> to ensure a pay rate of at least $15/hou eted at least 5,000 HITs, and to have greater than a 98% acceptance rate. We use the Fair Work tool <ref type=""bibr"" target=""#b35"">(Whiting et al., 2019)</ref> to ensure a minimum of $15 hourly wage.<",0
"language models (MLMs) that have been successful at pushing the state-ofthe-art on a range of tasks <ref type=""bibr"" target=""#b33"">(Wang et al., 2018</ref><ref type=""bibr"">(Wang et al., , 2019))</ref> ALBERT generally outperforming RoBERTa by a small margin, and BERT being significantly behind both <ref type=""bibr"" target=""#b33"">(Wang et al., 2018;</ref><ref type=""bibr"" target=""#b13"">Lai et al., 2 t lower bias scores on the SEAT while maintaining downstream task performance on the GLUE benchmark <ref type=""bibr"" target=""#b33"">(Wang et al., 2018)</ref>.</p><p>Discussing Bias Upon surveying 146 N",0
"reported in Appendix A.1.</p><p>End-to-End Entity Linking (EL) For EL, we reproduce the setting of <ref type=""bibr"" target=""#b27"">Kolitsas et al. (2018)</ref> using the same in-domain and out-of-doma ntity mentions (a span contained in d j ) and e i ∈ E its corresponding entity in the KB. Following <ref type=""bibr"" target=""#b27"">Kolitsas et al. (2018)</ref>, we considered only mentions that have e tics for 10k steps and we do model selection on the validation set. Again, following previous works <ref type=""bibr"" target=""#b27"">(Kolitsas et al., 2018)</ref>, we considered only mentions that have",1
"pensive when E is very large (e.g., Wikipedia has ∼6M entities). Hence, we exploit Beam Search (BS, <ref type=""bibr"" target=""#b60"">Sutskever et al., 2014)</ref>, an established approximate decoding st",0
"lly, this formulation enables sub-linear search using modern maximum-inner-product-search libraries <ref type=""bibr"" target=""#b23"">(Johnson et al., 2019)</ref> and hence supports retrieving from large",0
"b18"">Hoffart et al., 2011;</ref><ref type=""bibr"" target=""#b48"">Piccinno &amp; Ferragina, 2014;</ref><ref type=""bibr"" target=""#b21"">Huang et al., 2015;</ref><ref type=""bibr"" target=""#b30"">Le &amp; Tito",0
"extent <ref type=""bibr"" target=""#b45"">(Petroni et al., 2019)</ref> and language translation skills <ref type=""bibr"" target=""#b50"">(Radford et al., 2019)</ref> among other things, both desirable prope",0
"/ref>. Concretely, we use the objective that is typically used for neural machine translation (NMT, <ref type=""bibr"" target=""#b65"">Wu et al., 2016)</ref>, that is maximizing log p θ (y|x) with respect",0
"g using Natural Questions <ref type=""bibr"" target=""#b29"">(Kwiatkowski et al., 2019)</ref>, HotpotQA <ref type=""bibr"" target=""#b70"">(Yang et al., 2018c)</ref>, TriviaQA <ref type=""bibr"" target=""#b24"">(",0
"so related to metric learning works that employ generators <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b88"">87]</ref>. Apart from not requiring labels, our method exploits the m od exploits the memory component, something not present in <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b88"">87]</ref>. It has no extra parameters or loss terms that need to be o </ref><ref type=""bibr"" target=""#b36"">35]</ref>. Works like <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b88"">87]</ref> use generators to synthesize negatives in a supervised scen and exploit its memory component. What is more, and unlike <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b88"">87]</ref>, we do not require a generator, i.e. have no extra paramete izing negatives was explored in metric learning literature <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b88"">87,</ref><ref type=""bibr"" target=""#b36"">35]</ref>. Works like <ref ty",1
"earning visual representations in a self-supervised manner <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b31"">30]</ref>. Pushing the embeddings of two transformed versions of the ations learned in an unsupervised way. It is however shown <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b31"">30]</ref> that increasing the memory/batch size leads to diminishing t also use contrastive learning losses. These include MoCo <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b31"">30]</ref>, SimCLR <ref type=""bibr"" target=""#b10"">[11,</ref><ref type= contain the whole training set, while the recent Momentum Contrast (or MoCo) approach of He et al. <ref type=""bibr"" target=""#b31"">[30]</ref> keeps a queue with features of the last few batches as mem t of self-supervised representation learning. We delve deeper into learning with a momentum encoder <ref type=""bibr"" target=""#b31"">[30]</ref> and show evidence that harder negatives are required to fa ions. a)</head><p>We delve deeper into a top-performing contrastive self-supervised learning method <ref type=""bibr"" target=""#b31"">[30]</ref> and observe the need for harder negatives; b) We propose h air, which is contrasted with every feature n in the bank of negatives (Q) also called the queue in <ref type=""bibr"" target=""#b31"">[30]</ref>. A popular and highly successful loss function for contras ""bibr"" target=""#b65"">64,</ref><ref type=""bibr"" target=""#b78"">77]</ref>, a queue of the last batches <ref type=""bibr"" target=""#b31"">[30]</ref>, or simply be every other image in the current minibatch < computing them as the encoder keeps changing. The Momentum Contrast (or MoCo) approach of He et al. <ref type=""bibr"" target=""#b31"">[30]</ref> offers a compromise between the two negative sampling extr and all features in Q are encoded with the key encoder.</p><p>How hard are MoCo negatives? In MoCo <ref type=""bibr"" target=""#b31"">[30]</ref> (resp. SimCLR <ref type=""bibr"" target=""#b10"">[11]</ref>) t helping a lot towards learning the proxy task.</p><p>On the difficulty of the proxy task. For MoCo <ref type=""bibr"" target=""#b31"">[30]</ref>, SimCLR <ref type=""bibr"" target=""#b10"">[11]</ref>, InfoMin i.e. the percentage of queries where the key is ranked over all negatives, across training for MoCo <ref type=""bibr"" target=""#b31"">[30]</ref> and MoCo-v2 <ref type=""bibr"" target=""#b12"">[13]</ref>. MoC rop testing. For object detection on PASCAL VOC <ref type=""bibr"" target=""#b18"">[19]</ref> we follow <ref type=""bibr"" target=""#b31"">[30]</ref> and fine-tune a Faster R-CNN <ref type=""bibr"" target=""#b55 pe=""foot"" target=""#foot_2"">3</ref> code and report the common AP, AP50 and AP75 metrics. Similar to <ref type=""bibr"" target=""#b31"">[30]</ref>, we do not perform hyperparameter tuning for the object de tic segmentation on the COCO dataset <ref type=""bibr"" target=""#b42"">[41]</ref>. Following He et al. <ref type=""bibr"" target=""#b31"">[30]</ref>, we use Mask R-CNN <ref type=""bibr"" target=""#b29"">[29]</re nd on the train2017 set (118k images) and evaluate on val2017. We adopt feature normalization as in <ref type=""bibr"" target=""#b31"">[30]</ref> when fine-tuning. MoCHi and MoCo use the same hyper-parame llowing our discussion in Section 3.2, we wanted to verify that hardness of the proxy task for MoCo <ref type=""bibr"" target=""#b31"">[30]</ref> is directly correlated to the difficulty of the transforma i.e. the percentage of queries where the key is ranked over all negatives, across training for MoCo <ref type=""bibr"" target=""#b31"">[30]</ref>, MoCo-v2 <ref type=""bibr"" target=""#b12"">[13]</ref> and som et=""#b53"">[52]</ref> study the robustness of contrastive self-supervised learning methods like MoCo <ref type=""bibr"" target=""#b31"">[30]</ref> and PIRL <ref type=""bibr"" target=""#b47"">[46]</ref> and saw [11]</ref>, e.g. the addition of a target network whose parameter update is lagging similar to MoCo <ref type=""bibr"" target=""#b31"">[30]</ref>  Synthesizing for supervised metric learning. Recently, sy rt in parenthesis the difference to MoCo-v2. * denotes reproduced results. † results are copied from<ref type=""bibr"" target=""#b31"">[30]</ref>. We bold (resp. underline) the highest results overall (re rg/ns/1.0"" place=""foot"" n=""1"" xml:id=""foot_0"">In this section we study contrastive learning for MoCo<ref type=""bibr"" target=""#b31"">[30]</ref> on ImageNet-100, a subset of ImageNet consisting of 100 cl et=""#b10"">[11,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b31"">30,</ref><ref type=""bibr"" target=""#b47"">46,</ref><ref type=""bibr"" tar d highly successful loss function for contrastive learning <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b31"">30,</ref><ref type=""bibr"" target=""#b65"">64]</ref> is the following:</ 2 -normalized. In a number of recent successful approaches <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b31"">30,</ref><ref type=""bibr"" target=""#b47"">46,</ref><ref type=""bibr"" tar pervised learning papers do not discuss variance ; in fact only papers from highly resourceful labs <ref type=""bibr"" target=""#b31"">[30,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" ta",1
"et=""#b80"">79]</ref>, predicting the ""arrow of time"" <ref type=""bibr"" target=""#b75"">[74]</ref>, pace <ref type=""bibr"" target=""#b73"">[72]</ref> or predicting the ""odd"" element <ref type=""bibr"" target=""#",0
"ntrastive learning <ref type=""bibr"" target=""#b35"">[34,</ref><ref type=""bibr"" target=""#b87"">86,</ref><ref type=""bibr"" target=""#b43"">42]</ref>. Very recently, Khosla et al. <ref type=""bibr"" target=""#b35",0
"t=""#b75"">[74]</ref>, pace <ref type=""bibr"" target=""#b73"">[72]</ref> or predicting the ""odd"" element <ref type=""bibr"" target=""#b20"">[21]</ref> from a set of clips. Recently, contrastive, memory-based s",0
"et=""#b33"">[32,</ref><ref type=""bibr"" target=""#b69"">68,</ref><ref type=""bibr"" target=""#b77"">76,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" targe",0
"of frames or clips <ref type=""bibr"" target=""#b48"">[47,</ref><ref type=""bibr"" target=""#b39"">38,</ref><ref type=""bibr"" target=""#b80"">79]</ref>, predicting the ""arrow of time"" <ref type=""bibr"" target=""#b",0
"et=""#b68"">[67,</ref><ref type=""bibr"" target=""#b74"">73,</ref><ref type=""bibr"" target=""#b60"">59,</ref><ref type=""bibr"" target=""#b40"">39]</ref>, while Mutual Information theory has been the basis and ins",0
"rget=""#b1"">[2,</ref><ref type=""bibr"" target=""#b28"">28,</ref><ref type=""bibr"" target=""#b34"">33,</ref><ref type=""bibr"" target=""#b46"">45,</ref><ref type=""bibr"" target=""#b58"">57,</ref><ref type=""bibr"" tar",0
"tone of many state-of-the-art models in various natural language understanding and generation tasks <ref type=""bibr"" target=""#b5"">(Devlin et al., 2019;</ref><ref type=""bibr"" target=""#b31"">Liu et al., els to generate more rele-vant and coherent text. We first study a planning model trained from BERT <ref type=""bibr"" target=""#b5"">(Devlin et al., 2019)</ref> to produce the initial content plan, which",1
"bibr"" target=""#b5"">(Devlin et al., 2019;</ref><ref type=""bibr"" target=""#b31"">Liu et al., 2019;</ref><ref type=""bibr"" target=""#b27"">Lewis et al., 2020</ref>), yet they are far from perfect. In generati eration framework, built upon the pre-trained sequence-to-sequence (seq2seq) Transformer model BART <ref type=""bibr"" target=""#b27"">(Lewis et al., 2020)</ref>. As shown in Figure <ref type=""figure"" tar et al., 2019;</ref><ref type=""bibr"" target=""#b25"">Lawrence et al., 2019)</ref>. Our work uses BART <ref type=""bibr"" target=""#b27"">(Lewis et al., 2020)</ref>, a state-of-the-art seq2seq model that off U card with 24 GB memory.</p><p>Model Sizes. Our generation model has the same architecture as BART <ref type=""bibr"" target=""#b27"">(Lewis et al., 2020)</ref> with 406M parameters. The content planner",1
"ially designed control codes and auxiliary planning modules have been integrated into neural models <ref type=""bibr"" target=""#b22"">(Keskar et al., 2019;</ref><ref type=""bibr"" target=""#b35"">Moryossef e",0
"ourse markers are crucial for coherence <ref type=""bibr"" target=""#b14"">(Grote and Stede, 1998;</ref><ref type=""bibr"" target=""#b1"">Callaway, 2003)</ref> and have received dedicated research efforts in",0
", 2018)</ref>, or display a certain attribute <ref type=""bibr"" target=""#b18"">(Hu et al., 2017;</ref><ref type=""bibr"" target=""#b32"">Luo et al., 2019;</ref><ref type=""bibr"" target=""#b0"">Balakrishnan et",0
"encourage the model to cover a given topic <ref type=""bibr"" target=""#b48"">(Wang et al., 2017;</ref><ref type=""bibr"" target=""#b45"">See et al., 2019)</ref>, mention specified entities <ref type=""bibr""",0
"omains and adaptability to large pre-trained Transformers. One exception is the Plug and Play model <ref type=""bibr"" target=""#b4"">(Dathathri et al., 2020)</ref>, which directly modifies the key and va",0
"r"" target=""#b29"">(Lin, 2004)</ref>, measuring recall of the longest common subsequences; and METEOR <ref type=""bibr"" target=""#b24"">(Lavie and Agarwal, 2007)</ref>, which accounts for paraphrase. For o",0
"ation with Reddit ChangeMyView data, opinion article writing with the New York Times (NYT) corpus 2 <ref type=""bibr"" target=""#b44"">(Sandhaus, 2008)</ref>, and news report production on NYT. Automatic ask 2: Opinion Article Generation. We collect opinion articles from the New York Times (NYT) corpus <ref type=""bibr"" target=""#b44"">(Sandhaus, 2008</ref>). An article is selected if its taxonomies labe",0
"Mix, a data augmentation method for generating sub-sequences along with their labels based on mixup <ref type=""bibr"" target=""#b46"">(Zhang et al., 2018)</ref>. Under the active sequence labeling framew ns=""http://www.tei-c.org/ns/1.0""><head n=""3.2"">Sequence Mixup in the Embedding Space</head><p>Mixup <ref type=""bibr"" target=""#b46"">(Zhang et al., 2018)</ref> is a data augmentation method that impleme lation-based Regularizations Mixup implements interpolation in the input space to regularize models <ref type=""bibr"" target=""#b46"">(Zhang et al., 2018)</ref>. Recently, the Mixup variants <ref type=""b r limit rather than a too narrow score range setting.</p><p>For the mixing coefficient λ, we follow <ref type=""bibr"" target=""#b46"">(Zhang et al., 2018)</ref> to sample it from Beta(α, α) and explore α",1
"because we need to generate sentences and token-level labels jointly. Prevailing generative models <ref type=""bibr"" target=""#b49"">(Zhang et al., 2016;</ref><ref type=""bibr"" target=""#b2"">Bowman et al.",0
"> measure the uncertainty of sequence models by the most likely predicted sequence. For a CRF model <ref type=""bibr"" target=""#b21"">(Lafferty et al., 2001)</ref>, we calculate γ with the predicted sequ",0
"2019b)</ref> turn to perform interpolation in the hidden space to capture higher-level information. <ref type=""bibr"" target=""#b13"">Guo et al. (2019a)</ref>; <ref type=""bibr"" target=""#b3"">Chen et al. (",0
"f the mixed-up samples.</p><p>Text Augmentation Our work is also related to text data augmentation. <ref type=""bibr"" target=""#b48"">Zhang et al. (2015)</ref>; <ref type=""bibr"" target=""#b41"">Wei and Zou",0
"ustering and the visualization of shifting sample topology with time. As an example of the lat-ter, <ref type=""bibr"" target=""#b24"">Malhotra et al. (2017)</ref> presented a multi-layered RNN sequence-t",0
"on from a limited number of time steps stored inside their hidden state (vanishing gradient problem <ref type=""bibr"" target=""#b13"">(Hochreiter, 1998;</ref><ref type=""bibr"" target=""#b25"">Pascanu et al.",0
"of time series under increasing ""missingness"" of values. For the purpose of time series clustering, <ref type=""bibr"" target=""#b17"">Lei et al. (2017)</ref> also follow a method which aims at preserving",0
"LP has been mainly attributed to extreme variation in sample length (i.e., sentences in most tasks) <ref type=""bibr"" target=""#b27"">(Shen et al., 2020)</ref>, while in the datasets we examine this vari",0
"erent KGs. Recent works on multilingual KG embeddings provide support for automated entity matching <ref type=""bibr"" target=""#b5"">(Chen et al., 2017</ref><ref type=""bibr"" target=""#b4"">(Chen et al., , extended embedding models to bridge multiple KGs, typically for KGs of multiple languages. MTransE <ref type=""bibr"" target=""#b5"">(Chen et al., 2017)</ref> jointly learns a transformation across two s The embedding learning process jointly trains the knowledge model and the alignment model following <ref type=""bibr"" target=""#b5"">Chen et al. (2017)</ref>, while self-learning is added to improve the G i and G j . λ is a positive hyperparameter that weights the two model components.</p><p>Following <ref type=""bibr"" target=""#b5"">Chen et al. (2017)</ref>, instead of directly optimizing J in Eq. ( <r",1
"ods include AdaBoost <ref type=""bibr"" target=""#b10"">(Freund and Schapire, 1997)</ref> and RankBoost <ref type=""bibr"" target=""#b9"">(Freund et al., 2004)</ref>, which target at classification and rankin ng model that makes more accurate predictions should receive a higher weight. Inspired by RankBoost <ref type=""bibr"" target=""#b9"">(Freund et al., 2004)</ref>, we reduce the ranking combination problem = {q1 = (The Tale of Genji, country, ?t) q2 = (The Tale of Genji, genre, ?t)} Similar to RankBoost <ref type=""bibr"" target=""#b9"">(Freund et al., 2004)</ref> Ranking loss. The overall objective of KEn <p>Let the set of all the critical entity pairs from all the validation queries of an entity as P . <ref type=""bibr"" target=""#b9"">Freund et al. (2004)</ref> have proved that, when using RankBoost, thi",1
"from latent representations of observed facts. Representative models including translational models <ref type=""bibr"" target=""#b0"">(Bordes et al., 2013;</ref><ref type=""bibr"" target=""#b35"">Wang et al., weighting (KEnS m ): MRR is a widely-used metric for evaluating the ranking performance of a model <ref type=""bibr"" target=""#b0"">(Bordes et al., 2013;</ref><ref type=""bibr"" target=""#b38"">Yang et al., , after applying a relation-specific translation vector r. The representative models include TransE <ref type=""bibr"" target=""#b0"">(Bordes et al., 2013)</ref> and its extensions TransD <ref type=""bibr"" true triple (h, r, t).</p><p>We here consider two representative triple scoring techniques: TransE <ref type=""bibr"" target=""#b0"">(Bordes et al., 2013)</ref> and RotatE <ref type=""bibr"" target=""#b30""> higher. Although another common metric, Mean Reciprocal Rank (MRR), has been used in previous works <ref type=""bibr"" target=""#b0"">(Bordes et al., 2013)</ref>, it is not applicable to the evaluation of ce techniques introduced in in Section 3. For baseline methods, besides the single-embedding TransE <ref type=""bibr"" target=""#b0"">(Bordes et al., 2013)</ref> and RotatE <ref type=""bibr"" target=""#b30"">",0
"e in biological <ref type=""bibr"" target=""#b12"">(Hao et al., 2020)</ref> and medical knowledge bases <ref type=""bibr"" target=""#b42"">(Zhang et al., 2020)</ref>. Pariticularly, we also seek to ensure the",0
"include TransE <ref type=""bibr"" target=""#b0"">(Bordes et al., 2013)</ref> and its extensions TransD <ref type=""bibr"" target=""#b13"">(Ji et al., 2015)</ref>. Despite their simplicity, translational mode b)</ref>, we also include DistMult <ref type=""bibr"" target=""#b38"">(Yang et al., 2015)</ref>, TransD <ref type=""bibr"" target=""#b13"">(Ji et al., 2015)</ref>, and HolE <ref type=""bibr"" target=""#b19"">(Nic",0
"edge bases, including DBpedia <ref type=""bibr"" target=""#b17"">(Lehmann et al., 2015)</ref>, Wikidata <ref type=""bibr"" target=""#b33"">(Vrandečić and Krötzsch, 2014)</ref> and YAGO <ref type=""bibr"" target",0
"knowledge-driven applications <ref type=""bibr"" target=""#b16"">(Koncel-Kedziorski et al., 2019;</ref><ref type=""bibr"" target=""#b3"">Chen et al., 2018a;</ref><ref type=""bibr"" target=""#b1"">Bordes et al.,",0
"sted readers are referred to recent surveys <ref type=""bibr"" target=""#b34"">(Wang et al., 2017;</ref><ref type=""bibr"" target=""#b14"">Ji et al., 2020)</ref> for more information.</p><p>Multilingual KG Em",0
"ell as neural models like HolE <ref type=""bibr"" target=""#b19"">(Nickel et al., 2016)</ref> and ConvE <ref type=""bibr"" target=""#b8"">(Dettmers et al., 2018)</ref>. Due to the large body of work in this l",0
"19a</ref><ref type=""bibr"" target=""#b27"">Sun et al., , 2020a) )</ref> and degree centrality measures <ref type=""bibr"" target=""#b21"">(Pei et al., 2019)</ref>. A systematic summary of relevant approaches",0
"user packets are not detoured in transmission, numerous network attack surfaces are opened up today <ref type=""bibr"" target=""#b1"">[2]</ref>- <ref type=""bibr"" target=""#b3"">[4]</ref>. For example, an at d path validation that fill the void, such as ICING <ref type=""bibr"" target=""#b2"">[3]</ref> and OPT <ref type=""bibr"" target=""#b1"">[2]</ref>. However, for the targeting environment which is adversarial performance of the Click router as the baseline, and compare our PSVM with the-state-of-the-art OPT <ref type=""bibr"" target=""#b1"">[2]</ref>.  Method and parameter setup. For fairness, we use the same ave been proposed in ICING <ref type=""bibr"" target=""#b2"">[3]</ref>, the Origin and Path Trace (OPT) <ref type=""bibr"" target=""#b1"">[2]</ref>, Orthogonal Sequence Verification (OSV) <ref type=""bibr"" tar",1
"pology analysis <ref type=""bibr"" target=""#b13"">[14]</ref>, obtained from some BGP related protocols <ref type=""bibr"" target=""#b14"">[15]</ref>, <ref type=""bibr"" target=""#b15"">[16]</ref>, or employing t",0
"a long time to serve the derivation of session keys. Furthermore, we select a sample trace of CAIDA <ref type=""bibr"" target=""#b20"">[22]</ref> to translate the cost of storing the notice list on a rout tice information, the routing node also needs to store its Pic. As shown in a sample trace of CAIDA <ref type=""bibr"" target=""#b20"">[22]</ref> (in Section V), with 16 B Pic and 1 B Index, fitting the c",0
"nodes (i.e.,the sender and receiver) could know path information, such as network topology analysis <ref type=""bibr"" target=""#b13"">[14]</ref>, obtained from some BGP related protocols <ref type=""bibr""",0
"gn is inspired by <ref type=""bibr"" target=""#b2"">[3]</ref>, <ref type=""bibr"" target=""#b3"">[4]</ref>, <ref type=""bibr"" target=""#b11"">[12]</ref>, some of which were actually used. Fig. <ref type=""figure""",0
"</p><p>The maximum length limit in BERT naturally reminds us the limited capacity of Working Memory <ref type=""bibr"" target=""#b1"">[2]</ref>, a human cognitive system storing information for logical re ttentional system capable of selecting and operating control processes and strategies"", as Baddeley <ref type=""bibr"" target=""#b1"">[2]</ref> pointed out in his 1992 classic. Later research detailed tha",1
"w method, hence we fail to answer the question.</p><p>Pretrained language models, pioneered by BERT <ref type=""bibr"" target=""#b11"">[12]</ref>, have emerged as silver bullets for many NLP tasks, such a ficial obstacle for long texts is that the pretrained max position embedding is usually 512 in BERT <ref type=""bibr"" target=""#b11"">[12]</ref>. However, even if the embeddings for larger positions are",1
"les attentions between faraway sentences. Similar ideas were investigated on document-level in DrQA <ref type=""bibr"" target=""#b5"">[6]</ref> and ORQA <ref type=""bibr"" target=""#b22"">[23]</ref>, and ther",0
"s posed on 12,744 long news articles. <ref type=""foot"" target=""#foot_2"">3</ref> Since previous SOTA <ref type=""bibr"" target=""#b42"">[43]</ref> is not BERT based (due to long texts) in NewsQA, to keep t , for example BiDAF <ref type=""bibr"" target=""#b40"">[41]</ref> (+17.8% F 1 ), previous SOTA DECAPROP <ref type=""bibr"" target=""#b42"">[43]</ref>, which incorporates elaborate self-attention and RNN mecha",0
"esearch attempts to simplify the structure of transformers <ref type=""bibr"" target=""#b19"">[20,</ref><ref type=""bibr"" target=""#b36"">37,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" targe d to BERT, e.g., Transformer-XL <ref type=""bibr"" target=""#b7"">[8]</ref> and Compressive Transformer <ref type=""bibr"" target=""#b36"">[37]</ref>. Reformer uses locality-sensitive hashing for content-base",0
"8]</ref> and GLUE <ref type=""bibr"" target=""#b46"">[47]</ref>, but very common for more complex tasks <ref type=""bibr"" target=""#b52"">[53]</ref> or real-world textual data.</p><p>A straightforward soluti the-art results on four tasks, including NewsQA <ref type=""bibr"" target=""#b43"">[44]</ref>, HotpotQA <ref type=""bibr"" target=""#b52"">[53]</ref>, 20NewsGroups <ref type=""bibr"" target=""#b21"">[22]</ref> an >2</ref>(a)) in nature suggest the answer block as relevant. Even multi-hop datasets, e.g. HotpotQA <ref type=""bibr"" target=""#b52"">[53]</ref>, usually annotate supporting sentences. In these cases, th m can be elegantly solved by concatenating all the paragraphs as the input of BERTs.</p><p>HotpotQA <ref type=""bibr"" target=""#b52"">[53]</ref> is a multi-hop QA dataset of 112,779 questions, whose dist",0
"e=""foot"" n=""3"" xml:id=""foot_2"">We use the original version instead of the simplified version in MRQA<ref type=""bibr"" target=""#b14"">[15]</ref>, which removed long texts.</note> 			<note xmlns=""http://w",0
"nformation is constantly updated with relevant items from long-term memory by retrieval competition <ref type=""bibr"" target=""#b51"">[52]</ref>, collecting sufficient information for reasoning in the wo",0
"re"">1</ref>). Since the problem roots in the high O(L 2 ) time and space complexity in transformers <ref type=""bibr"" target=""#b45"">[46]</ref> (L is the length of the text), another line of research at",0
"ion for logical reasoning and decision-making. Experiments <ref type=""bibr"" target=""#b26"">[27,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b30"">31]</ref> already showed that t",0
"preprocessing step and performs standard logistic regression to remove redundant computation. PPRGo <ref type=""bibr"" target=""#b3"">[4]</ref> uses Personalized PageRank to capture multi-hop neighborhood cency matrix Ã and feature matrix X in the precomputation phase, which requires O(LmF ) time. PPRGo <ref type=""bibr"" target=""#b3"">[4]</ref> calculates approximate the Personalized PageRank (PPR) matri in APPNP and PPRGo <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b3"">4]</ref>; 2) w = 0 for = 0, . . . , L − 1 and w L = 1, in which case P br"" target=""#b36"">[37]</ref>, SGC and PPRGo (linear model) <ref type=""bibr"" target=""#b29"">[30,</ref><ref type=""bibr"" target=""#b3"">4]</ref>.</p><p>We implement GBP in PyTorch and C++, and employ initia",1
"eed the following Chernoff Bound for bounded i.i.d. random variables.</p><p>Lemma 3 (Chernoff Bound <ref type=""bibr"" target=""#b8"">[9]</ref>) Consider a set {x i } (i ∈ [1, n r ]) of i.i.d. random vari",0
"ss all layers. Cluster-GCN <ref type=""bibr"" target=""#b7"">[8]</ref> uses graph clustering techniques <ref type=""bibr"" target=""#b13"">[14]</ref> to partition the original graph into several sub-graphs, a",0
"type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b25"">26]</ref>, recommendation systems <ref type=""bibr"" target=""#b35"">[36]</ref>, and computer vision <ref type=""bibr"" target=""#b38"">[39,</",0
"asing networks Amazon <ref type=""bibr"" target=""#b7"">[8]</ref> and a large social network Friendster <ref type=""bibr"" target=""#b33"">[34]</ref>. Table <ref type=""table"" target=""#tab_1"">2</ref> summarize le GNNs on a billion-scale graph Friendster. We extracted the top-500 ground-truth communities from <ref type=""bibr"" target=""#b33"">[34]</ref> and use the community ids as the labels of each node. Note",0
"where permutation equivariance is either learned from data <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b22"">23]</ref> or obtained by design <ref type=""bibr"" target=""#b23"">[24]</ ]</ref>. With node features acting as identifiers, MPNN were shown to become universal in the limit <ref type=""bibr"" target=""#b22"">[23]</ref>, which implies that they can solve the graph isomorphism t d evidence that the power of MPNN grows as a function of depth and width for certain graph problems <ref type=""bibr"" target=""#b22"">[23]</ref>, showing that (both anonymous and non-anonymous) MPNN cann ether depth and width needs to grow with the number of nodes solely in the worst-case (as proven in <ref type=""bibr"" target=""#b22"">[23]</ref>) or with certain probability over the input distribution.< apacity is an effective generalization of the previously considered product between depth and width <ref type=""bibr"" target=""#b22"">[23]</ref>, being able to consolidate more involved properties, as we lower bounds rely on a new technique which renders them applicable not only to worst-case instances <ref type=""bibr"" target=""#b22"">[23]</ref>, but in expectation over the input distribution.</p><p>An previous theoretical findings that non-anonymous MPNN are universal and can solve graph isomorphism <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b4"">5]</ref>, as well as that the",1
"nentially small when p &gt; log v/v. It is well known (see e.g., Theorem 4.1 by Frieze and Karoński <ref type=""bibr"" target=""#b49"">[50]</ref>) that, for any δ &gt; 0 and p = δ+log v v , a random graph",0
"e pessimistic in the non-anonymous case, where permutation equivariance is either learned from data <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b22"">23]</ref> or obtained by des a one-hot encoding <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b21"">22]</ref> or a random coloring <ref type=""bibr"" target=""#b25"">[26,</r ><ref type=""bibr"" target=""#b4"">5]</ref>, as well as that they can learn to be permutation invariant <ref type=""bibr"" target=""#b21"">[22]</ref>. On the other hand, anonymous MPNN are always permutation",0
"=""#b10"">[11]</ref><ref type=""bibr"" target=""#b11"">[12]</ref><ref type=""bibr"" target=""#b12"">[13]</ref><ref type=""bibr"" target=""#b13"">[14]</ref><ref type=""bibr"" target=""#b14"">[15]</ref><ref type=""bibr"" t",0
"hborhoods via different mechanisms (e.g., averaging, LSTM) <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b35"">36]</ref>. However, in the re ifferent neighbors more precisely as a weighted average of the ego-and neighbor-features. GraphSAGE <ref type=""bibr"" target=""#b10"">[11]</ref> generalizes the aggregation beyond averaging, and models t -and the aggregated neighbor-embeddings without 'mixing' them is with concatenation as in GraphSAGE <ref type=""bibr"" target=""#b10"">[11]</ref>-rather than averaging all of them as in the GCN model by K ef type=""bibr"" target=""#b35"">[36]</ref> GCN-Cheby <ref type=""bibr"" target=""#b6"">[7]</ref> GraphSAGE <ref type=""bibr"" target=""#b10"">[11]</ref> MixHop <ref type=""bibr"" target=""#b0"">[1]</ref> H2GCN (prop ding transformations per round in H 2 GCN? GCN <ref type=""bibr"" target=""#b16"">[17]</ref>, GraphSAGE <ref type=""bibr"" target=""#b10"">[11]</ref> and other GNN models embed the intermediate representation &amp; GCN-Cheby <ref type=""bibr"" target=""#b16"">[17]</ref>: https://github.com/tkipf/gcn • GraphSAGE <ref type=""bibr"" target=""#b10"">[11]</ref>: https://github.com/williamleif/graphsage-simple (PyTorch : 3 * early_stopping: 40 We report the best performance, for Set 1 with a = 64, b = 5e-4.• GraphSAGE<ref type=""bibr"" target=""#b10"">[11]</ref>:-hid_units: a ∈ {64, 128} lr: b ∈ {0.1, 0.7} epochs: 500</ led the default feature normalization in the official implementation for this baseline. • GraphSAGE <ref type=""bibr"" target=""#b10"">[11]</ref>:</p><p>-hid_units: a ∈ {64, 128} lr: b ∈ {0.1, 0.7} epochs led the default feature normalization in the official implementation for this baseline. • GraphSAGE <ref type=""bibr"" target=""#b10"">[11]</ref>:</p><p>-hid_units: 64 lr: {0.1, 0.7} epochs: 500</p><p>• M intermediate representations. While these designs have been utilized separately in some prior works <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" targ 1)d)</formula><p>Solving the above inequality for δ 1 , we get the amount of perturbation needed as <ref type=""bibr"" target=""#b10"">(11)</ref> and the least absolute amount of perturbation needed is |δ",1
"target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b0"">1,</ref><ref type=""bibr"" target=""#b37"">38]</ref>, we are the first to discuss their importance under heterop COMBINE functions that leverage each representation separately-e.g., concatenation, LSTM-attention <ref type=""bibr"" target=""#b37"">[38]</ref>. This design is introduced in jumping knowledge networks < <ref type=""bibr"" target=""#b37"">[38]</ref>. This design is introduced in jumping knowledge networks <ref type=""bibr"" target=""#b37"">[38]</ref> and shown to increase the representation power of GCNs und ons from two rounds with the embedded ego-representation (following the jumping knowledge framework <ref type=""bibr"" target=""#b37"">[38]</ref>), GCN's accuracy increases to 58.93%±3.17 for h = 0.1, a 2 label>(7)</label></formula><p>where we empirically find concatenation works better than max-pooling <ref type=""bibr"" target=""#b37"">[38]</ref> as the COMBINE function.</p><p>In the classification stage e compare GraphSAGE, GCN-Cheby and GCN to their corresponding variants enhanced with JK connections <ref type=""bibr"" target=""#b37"">[38]</ref>. GCN and GCN-Cheby benefit significantly from D3 in hetero with and without JK connections is similar (gaps mostly less than 2%), matching the observations in <ref type=""bibr"" target=""#b37"">[38]</ref>.</p><p>While other design choices and implementation detai K, we enhanced the corresponding base model with jumping knowledge (JK) connections using JK-Concat <ref type=""bibr"" target=""#b37"">[38]</ref> without changing the number of layers or other hyperparame K, we enhanced the corresponding base model with jumping knowledge (JK) connections using JK-Concat <ref type=""bibr"" target=""#b37"">[38]</ref> without changing the number of layers or other hyperparame",1
"cy matrix with a sparsified version of a diffusion matrix (e.g., heat kernel or PageRank). Geom-GCN <ref type=""bibr"" target=""#b25"">[26]</ref> precomputes unsupervised node embeddings and uses neighbor 0.7), and across the full spectrum (""Overall""). The ""*"" denotes ranks based on results reported in <ref type=""bibr"" target=""#b25"">[26]</ref>. Real datasets &amp; setup We now evaluate the performance of nodes per class for train/validation/test<ref type=""foot"" target=""#foot_2"">2</ref> ) provided by <ref type=""bibr"" target=""#b25"">[26]</ref>.</p><p>For Cora-Full, we generate 3 random splits, with 25 the best results among the three recentlyproposed GEOM-GCN variants ( § 4), directly from the paper <ref type=""bibr"" target=""#b25"">[26]</ref>: other models (including ours) outperform this method sign ng universities, originally collected by the CMU WebKB project. We used the preprocessed version in <ref type=""bibr"" target=""#b25"">[26]</ref>. In these networks, nodes are web pages, which are classif br"" target=""#b28"">[29]</ref>. For the classification task, we utilize the class labels generated by <ref type=""bibr"" target=""#b25"">[26]</ref>, where the nodes are categorized into 5 classes based on t erage traffic. • Actor is a graph representing actor co-occurrence in Wikipedia pages, processed by <ref type=""bibr"" target=""#b25"">[26]</ref> based on the film-director-actor-writer network in <ref ty ter network in <ref type=""bibr"" target=""#b34"">[35]</ref>. We also use the class labels generated by <ref type=""bibr"" target=""#b25"">[26]</ref>. • Cora, Pubmed and Citeseer are citation graphs originall fferent data splits. Best model per benchmark highlighted in gray. The ""*"" results are obtained from<ref type=""bibr"" target=""#b25"">[26]</ref> and ""N/A"" denotes non-reported results.</figDesc><table><r ix of A + I.</note> 			<note xmlns=""http://www.tei-c.org/ns/1.0"" place=""foot"" n=""2"" xml:id=""foot_2""><ref type=""bibr"" target=""#b25"">[26]</ref> claims that the ratios are 60%/20%/20%, which is different atent space to define graph convolution. Some of these works <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b11"">12]</ref> acknowledge the cha",1
"in some prior works <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b0"">1,</ref><ref type=""bibr"" target=""#b37"">38]</ref>, we are the first to or different. This design-employed in GCN-Cheby <ref type=""bibr"" target=""#b6"">[7]</ref> and MixHop <ref type=""bibr"" target=""#b0"">[1]</ref>-augments the implicit aggregation over higher-order neighbor rks generalize graph convolution outside of immediate neighborhoods. For example, apart from MixHop <ref type=""bibr"" target=""#b0"">[1]</ref> (cf. § 3.1), Graph Diffusion Convolution <ref type=""bibr"" ta <ref type=""bibr"" target=""#b6"">[7]</ref> GraphSAGE <ref type=""bibr"" target=""#b10"">[11]</ref> MixHop <ref type=""bibr"" target=""#b0"">[1]</ref> H2GCN (proposed)</p><p>Comparison of H 2 GCN to existing GNN ormula_2"">3</ref>)-(4) above. It explicitly considers higher-order neighborhoods up to N 2 , though <ref type=""bibr"" target=""#b0"">[1]</ref> defines the 2-hop neighborhoods as that including neighbors #b10"">[11]</ref>: https://github.com/williamleif/graphsage-simple (PyTorch implementation) • MixHop <ref type=""bibr"" target=""#b0"">[1]</ref>: https://github.com/samihaija/mixhop • GAT <ref type=""bibr"" ype=""bibr"" target=""#b10"">[11]</ref>:</p><p>-hid_units: 64 lr: {0.1, 0.7} epochs: 500</p><p>• MixHop <ref type=""bibr"" target=""#b0"">[1]</ref>:</p><p>-hidden_dims_csv: {64, 192} -adj_pows: 0, 1, 2</p><p> metric relationships in the resulting latent space to define graph convolution. Some of these works <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" targ ion We generate synthetic graphs with various heterophily levels by adopting an approach similar to <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b15"">16]</ref>. In general, the syn mentioned, H 2 GCN differs from GCN <ref type=""bibr"" target=""#b16"">[17]</ref> in a number of ways: <ref type=""bibr"" target=""#b0"">(1)</ref> In each round of propagation/aggregation, GCN ""mixes"" the eg",0
"ive label correlations for neighbors in heterophily (for binary class labels); and the recent CPGNN <ref type=""bibr"" target=""#b42"">[43]</ref> method models more complex label correlations by integrati y matrix instead of using a pre-defined one in the BP formulation. Moreover, the recent CPGNN model <ref type=""bibr"" target=""#b42"">[43]</ref> integrates the compatibility matrix as a set of learnable",0
"Since exact inference is NP-hard, approximate inference algorithms (e.g., iterative classification <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b19"">20]</ref>, loopy belief prop",0
"gs: the goal of the task is to infer the unknown labels of the nodes by using the network structure <ref type=""bibr"" target=""#b43"">[44]</ref>, given partially labeled networks with node features (or a",0
"at some pairs of classes exhibit homophily, while others exhibit heterophily. In belief propagation <ref type=""bibr"" target=""#b39"">[40]</ref>, a message-passing algorithm used for inference on graphic =""#b19"">20]</ref>, loopy belief propagation) are used to solve the problem. Belief propagation (BP) <ref type=""bibr"" target=""#b39"">[40]</ref> is a classic messagepassing algorithm for graph-based semi",0
"get=""#b28"">29,</ref><ref type=""bibr"" target=""#b29"">30,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b30"">31]</ref> with edge homophily r tion as the feature vector for each node. • Cora Full is an extended version of Cora, introduced in <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b30"">31]</ref>, which contain more",0
"pre-defined class compatibility or edge potential matrix ( § 2). We note, however, that Gatterbauer <ref type=""bibr"" target=""#b8"">[9]</ref> proposed estimating the class compatibility matrix instead o",0
"et=""#b34"">[35,</ref><ref type=""bibr"" target=""#b28"">29,</ref><ref type=""bibr"" target=""#b29"">30,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" targe and Citeseer are citation graphs originally introduced in <ref type=""bibr"" target=""#b29"">[30,</ref><ref type=""bibr"" target=""#b21"">22]</ref>, which are among the most widely used benchmarks for semi-s",0
"rs are more likely to connect to accomplices than to other fraudsters in online purchasing networks <ref type=""bibr"" target=""#b23"">[24]</ref>. Since many existing GNNs assume strong homophily, they fa tendency of connection between each pair of classes. For instance, in an online purchasing network <ref type=""bibr"" target=""#b23"">[24]</ref> with three classes-fraudsters, accomplices, and honest use",0
"ggregating them within various graph neighborhoods via different mechanisms (e.g., averaging, LSTM) <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" ta borhood is N 1 (v)-i.e., the 1-hop neighbors of v. As for f , in graph convolutional networks (GCN) <ref type=""bibr"" target=""#b16"">[17]</ref> each node repeatedly averages its own features and those o ssification (differences in accuracy of MLP for different h are due to randomness). Especially, GCN <ref type=""bibr"" target=""#b16"">[17]</ref> and GAT <ref type=""bibr"" target=""#b35"">[36]</ref> show up target=""#b10"">[11]</ref>-rather than averaging all of them as in the GCN model by Kipf and Welling <ref type=""bibr"" target=""#b16"">[17]</ref>.</p><p>Intuition. In heterophily settings, by definition ( (v)) may be different. However, the typical GCN design that mixes the embeddings through an average <ref type=""bibr"" target=""#b16"">[17]</ref> or weighted average <ref type=""bibr"" target=""#b35"">[36]</r ility matrix notion from belief propagation <ref type=""bibr"" target=""#b9"">[10]</ref> into GNNs. GCN <ref type=""bibr"" target=""#b16"">[17]</ref> GAT <ref type=""bibr"" target=""#b35"">[36]</ref> GCN-Cheby <r ditional conceptual and mechanism differences.</p><p>As we have mentioned, H 2 GCN differs from GCN <ref type=""bibr"" target=""#b16"">[17]</ref> in a number of ways: <ref type=""bibr"" target=""#b0"">(1)</re t are critical in heterophily.</p><p>Non-linear embedding transformations per round in H 2 GCN? GCN <ref type=""bibr"" target=""#b16"">[17]</ref>, GraphSAGE <ref type=""bibr"" target=""#b10"">[11]</ref> and o c.org/ns/1.0""><head>F Experimental Setup &amp; Hyperparameter Tuning</head><p>• GCN &amp; GCN-Cheby <ref type=""bibr"" target=""#b16"">[17]</ref>: https://github.com/tkipf/gcn • GraphSAGE <ref type=""bibr"" Function σ: ReLU -Dropout Rate: a ∈ {0, 0.5}</p><p>We report the best performance, for a = 0. • GCN <ref type=""bibr"" target=""#b16"">[17]</ref>:</p><p>-hidden1: a ∈ {16, 32, 64} -early_stopping: b ∈ {40 ∈ {40, 100, 200} epochs: 2000</p><p>We report the best performance, for a = 32, b = 40. • GCN-Cheby <ref type=""bibr"" target=""#b16"">[17]</ref>:</p><p>-Set 1:</p><p>-hidden1: 64 -early_stopping: {40, 10 method hurts the performance significantly. We report the best performance, for a = 40. • GCN-Cheby <ref type=""bibr"" target=""#b16"">[17]</ref>:</p><p>-hidden1: 64 -max_degree: 2 -early_stopping: 40 epo unction σ: {ReLU, None} -Dropout Rate: {0, 0.5} -L2 Regularization Weight: {1e-5, 1e-6}</p><p>• GCN <ref type=""bibr"" target=""#b16"">[17]</ref>: </p></div>			</div> 			<div type=""references"">  				<list",0
"e complex label correlations by integrating the compatibility matrix notion from belief propagation <ref type=""bibr"" target=""#b9"">[10]</ref> into GNNs. GCN <ref type=""bibr"" target=""#b16"">[17]</ref> GA homophily or heterophily <ref type=""bibr"" target=""#b18"">[19]</ref> and has fast linearized versions <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b7"">8]</ref>. Different from the s",0
"dominant and thus provide more relevant context. This observation is also confirmed by recent works <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b5"">6]</ref> in the context of bina",0
"considerable spatial redundancy occurs when inferring CNNs <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b61"">62]</ref>. Several approaches have been proposed to reduce the redund for different image regions. The methods proposed in <ref type=""bibr"" target=""#b18"">[19]</ref> and <ref type=""bibr"" target=""#b61"">[62]</ref> skip the computation on some less important regions of fea",1
"high memory footprint, both of which grow quadratically with respect to the image height (or width) <ref type=""bibr"" target=""#b37"">[38]</ref>. In real-world applications like content-based image searc ployed for higher efficiency. This differentiates our method from early recurrent attention methods <ref type=""bibr"" target=""#b37"">[38]</ref> which adopt pure recurrent models. In addition, we focus o "">8]</ref>.</p><p>One similar work to our GFNet is the recurrent visual attention model proposed in <ref type=""bibr"" target=""#b37"">[38]</ref>. However, our method differs from it in two important aspe ults with only a few class-discriminative patches, such as the head of a dog or the wings of a bird <ref type=""bibr"" target=""#b37"">[38,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" ta",1
"or 320×320 images <ref type=""bibr"" target=""#b46"">[47,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b60"">61,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" tar",0
"should be minimized for both safety and economical reasons <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b42"">43]</ref>.</p><p>In this pape </ref><ref type=""bibr"" target=""#b42"">43,</ref><ref type=""bibr"" target=""#b15"">16]</ref>, CondenseNet <ref type=""bibr"" target=""#b20"">[21]</ref>, ShuffleNets <ref type=""bibr"" target=""#b65"">[66,</ref><ref </ref><ref type=""bibr"" target=""#b42"">43,</ref><ref type=""bibr"" target=""#b15"">16]</ref>, CondenseNet <ref type=""bibr"" target=""#b20"">[21]</ref>, ShuffleNets <ref type=""bibr"" target=""#b65"">[66,</ref><ref br"" target=""#b35"">[36]</ref>, MobileNets-V2 <ref type=""bibr"" target=""#b42"">[43]</ref>, CondenseNets <ref type=""bibr"" target=""#b20"">[21]</ref>, FBNets <ref type=""bibr"" target=""#b58"">[59]</ref>, Proxyle",0
"et=""#b29"">[30,</ref><ref type=""bibr"" target=""#b30"">31,</ref><ref type=""bibr"" target=""#b33"">34,</ref><ref type=""bibr"" target=""#b34"">35]</ref> or quantizing the weights <ref type=""bibr"" target=""#b40"">[4",0
"pe=""bibr"" target=""#b33"">34,</ref><ref type=""bibr"" target=""#b34"">35]</ref> or quantizing the weights <ref type=""bibr"" target=""#b40"">[41,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" ta",0
"et=""#b62"">[63,</ref><ref type=""bibr"" target=""#b52"">53,</ref><ref type=""bibr"" target=""#b50"">51,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" target=""#b64"">65]</ref>. In the context of",0
"Net <ref type=""bibr"" target=""#b20"">[21]</ref>, ShuffleNets <ref type=""bibr"" target=""#b65"">[66,</ref><ref type=""bibr"" target=""#b35"">36]</ref> and EfficientNet <ref type=""bibr"" target=""#b49"">[50]</ref>, Net <ref type=""bibr"" target=""#b20"">[21]</ref>, ShuffleNets <ref type=""bibr"" target=""#b65"">[66,</ref><ref type=""bibr"" target=""#b35"">36]</ref> and EfficientNet <ref type=""bibr"" target=""#b49"">[50]</ref>. hly competitive baselines, i.e., MnasNets <ref type=""bibr"" target=""#b48"">[49]</ref>, ShuffleNets-V2 <ref type=""bibr"" target=""#b35"">[36]</ref>, MobileNets-V2 <ref type=""bibr"" target=""#b42"">[43]</ref>,",0
"olicy gradient algorithm. We implement the proximal policy optimization (PPO) algorithm proposed by <ref type=""bibr"" target=""#b43"">[44]</ref> to train the patch proposal network π. The details are int r 1 .</p><p>In this work, we implement the proximal policy optimization (PPO) algorithm proposed by <ref type=""bibr"" target=""#b43"">[44]</ref> to train the patch proposal network π. In the following, w L CPI usually leads to an excessively large policy update, a clipped surrogate objective is adopted <ref type=""bibr"" target=""#b43"">[44]</ref>:</p><p>where 0 &lt; &lt; 1 is a hyper-parameter. Then we a icient exploration <ref type=""bibr"" target=""#b57"">[58,</ref><ref type=""bibr"" target=""#b36"">37,</ref><ref type=""bibr"" target=""#b43"">44]</ref>, and L VF t is a squared-error loss on the estimated state",0
"dancy. Recent research has revealed that considerable spatial redundancy occurs when inferring CNNs <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b61"">62]</ref>. Several approaches spatial resolution by using low-frequency features. The Spatially Adaptive Computation Time (SACT) <ref type=""bibr"" target=""#b9"">[10]</ref> dynamically adjusts the number of executed layers for diffe AS <ref type=""bibr"" target=""#b4"">[5]</ref>, SkipNet <ref type=""bibr"" target=""#b54"">[55]</ref>, SACT <ref type=""bibr"" target=""#b9"">[10]</ref>, GoogLeNet <ref type=""bibr"" target=""#b47"">[48]</ref> and MS",0
"h each other. Current state-of-the-art models highly depend on Graph Convoluational Networks (GCNs) <ref type=""bibr"" target=""#b12"">[13]</ref> originated from the theory of Graph Fourier Transform (GFT tp://www.tei-c.org/ns/1.0""><head>Spectral Graph Convolution</head><p>The Spectral Graph Convolution <ref type=""bibr"" target=""#b12"">[13]</ref> is composed of three steps.</p><p>(1) The multivariate tim",1
"arget=""#b30"">31,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b28"">29,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" tar al networks and learns the parameters of the entire network through maximum log likelihood. DeepGLO <ref type=""bibr"" target=""#b24"">[25]</ref>   incorporates both spatial and temporal dependencies in t =""bibr"" target=""#b2"">[3]</ref>, Graph Wavenet <ref type=""bibr"" target=""#b28"">[29]</ref> and DeepGLO <ref type=""bibr"" target=""#b24"">[25]</ref>. We tune the hyper-parameters on the validation data by gr type=""bibr"" target=""#b20"">[21]</ref> GraphWaveNet<ref type=""bibr"" target=""#b28"">[29]</ref> DeelpGLO<ref type=""bibr"" target=""#b24"">[25]</ref> StemGNN (ours)</note></figure> 		</body> 		<back> 			<div",0
"tirely as a tensor input and considers a large receptive field through dilated convolutions. LSTNet <ref type=""bibr"" target=""#b13"">[14]</ref> uses convolution neural network (CNN) and recurrent neural <ref type=""bibr"" target=""#b18"">[19]</ref>, DCRNN <ref type=""bibr"" target=""#b16"">[17]</ref>, LSTNet <ref type=""bibr"" target=""#b13"">[14]</ref>, ST-GCN <ref type=""bibr"" target=""#b30"">[31]</ref>, DeepSta",0
"variate techniques <ref type=""bibr"" target=""#b19"">[20,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" tar get=""#b26"">27,</ref><ref type=""bibr"" target=""#b31"">32,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b17"">18]</ref> and multivariate techniques <ref type=""bibr"" target=""#b23"">",0
"pplied on the spectral representation to generate final output.</p><p>Graph Fourier Transform (GFT) <ref type=""bibr"" target=""#b7"">[8]</ref> is a basic operator for Spectral Graph Convolution. It proje",0
"tured data. In particular, the Information Bottleneck (IB) <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b18"">19]</ref> provides a critical principle for representation learning:",1
"A GeForce RTX 2080 GPUs, together with Intel(R) Xeon(R) Gold 6148 CPU @ 2.40GH CPUs. We use PyTorch <ref type=""bibr"" target=""#b49"">[50]</ref> and PyTorch Geometric <ref type=""bibr"" target=""#b50"">[51]<",0
"curate estimation of those bounds to learn IB-based models <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b21"">22]</ref>. However, for the graph-structured data D, this is impossib points now are correlated. We introduce a lower bound of I(Y ; Z (L) X ), which is reproduced from <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b22"">23]</ref>, and an upper boun 6)</label></formula><p>Other choices of Q 2 (Y ) may also be adopted and yield the contrastive loss <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b27"">28]</ref> (Appendix D). Howe Proposition 3.2: For any PDFs</p><p>Proof. We use the Nguyen, Wainright &amp; Jordan's bound I NWJ <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b22"">23]</ref>: Lemma B.1. <ref t I NWJ <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b22"">23]</ref>: Lemma B.1. <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b22"">23]</ref> For any two random (2)</head><p>To characterize Eq. ( <ref type=""formula"">2</ref>), We may also use a contrastive loss <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b27"">28]</ref> which empirically",0
"de can contain non-useful information that may negatively impact the prediction of the current node <ref type=""bibr"" target=""#b13"">[14]</ref>. Also, GNN's reliance on message passing over the edges of",0
"t) as possible.</p><p>An optimal representation is defined as the minimal sufficient representation <ref type=""bibr"" target=""#b48"">[49]</ref> that only covers I(D; Y ). In practice, due to the express",0
"assignment, non-maximum suppression (NMS) post-processing. They are not fully end-to-end. Recently, <ref type=""bibr"" target=""#b2"">Carion et al. (2020)</ref> proposed DETR to eliminate the need for suc d attention module suffers from a quadratic complexity growth with the feature map size. DETR. DETR <ref type=""bibr"" target=""#b2"">(Carion et al., 2020)</ref> is built upon the Transformer encoder-deco ng different feature levels. Other hyper-parameter setting and training strategy mainly follow DETR <ref type=""bibr"" target=""#b2"">(Carion et al., 2020)</ref>, except that Focal Loss <ref type=""bibr"" t or 50 epochs and the learning rate is decayed at the 40-th epoch by a factor of 0.1. Following DETR <ref type=""bibr"" target=""#b2"">(Carion et al., 2020)</ref>, we train our models using Adam optimizer",1
"re variants of convolution, such as deformable convolution <ref type=""bibr"">(Dai et al., 2017;</ref><ref type=""bibr"" target=""#b42"">Zhu et al., 2019b)</ref> and dynamic convolution <ref type=""bibr"" tar mable attention module. Inspired by deformable convolution <ref type=""bibr"">(Dai et al., 2017;</ref><ref type=""bibr"" target=""#b42"">Zhu et al., 2019b)</ref>, the deformable attention module only attend chieves 48.7 AP and 49.0 AP without bells and whistles, respectively. By using ResNeXt-101 with DCN <ref type=""bibr"" target=""#b42"">(Zhu et al., 2019b)</ref>, the accuracy rises to 50.1 AP. With additi",0
"R utilizes a simple architecture, by combining convolutional neural networks (CNNs) and Transformer <ref type=""bibr"" target=""#b33"">(Vaswani et al., 2017)</ref> encoder-decoders. They exploit the versa www.tei-c.org/ns/1.0""><head n=""2"">RELATED WORK</head><p>Efficient Attention Mechanism. Transformers <ref type=""bibr"" target=""#b33"">(Vaswani et al., 2017)</ref> involve both self-attention and cross-at n=""3"">REVISITING TRANSFORMERS AND DETR</head><p>Multi-Head Attention in Transformers. Transformers <ref type=""bibr"" target=""#b33"">(Vaswani et al., 2017)</ref> are of a network architecture based on a",0
"vie reviews (MR) <ref type=""bibr"" target=""#b36"">(Pang &amp; Lee, 2005)</ref>.</p><p>Comparison with <ref type=""bibr"" target=""#b23"">Kalantidis et al. (2020)</ref>: <ref type=""bibr"" target=""#b23"">Kalant , 2005)</ref>.</p><p>Comparison with <ref type=""bibr"" target=""#b23"">Kalantidis et al. (2020)</ref>: <ref type=""bibr"" target=""#b23"">Kalantidis et al. (2020)</ref> also consider ways to sample negatives",1
"tative set of latency-critical applications, we use the benchmarks of the TailBench benchmark suite <ref type=""bibr"" target=""#b18"">[20]</ref>. This suite includes eight representative applications of . However, we found that these values are in line with the results of this application presented in <ref type=""bibr"" target=""#b18"">[20]</ref>.</p><p>Once we have defined the QoS requirements for each saturation QPS (i.e., point at which the LQoS is achieved). This issue has been also pointed out in <ref type=""bibr"" target=""#b18"">[20]</ref>, where authors realized of this unexpected behavior and de",1
"-way LLC. It supports Intel Cache Allocation Technology (CAT) and Memory Bandwidth Allocation (MBA) <ref type=""bibr"" target=""#b11"">[12]</ref>, allowing to perform cache and memory bandwidth partitioni ted consumption.</p><p>To carry out this study, we use the Intel Resource Director Technology (RDT) <ref type=""bibr"" target=""#b11"">[12]</ref>, which implements the Cache Monitoring Technology (CMT) an",0
"ref> technology, that is, Intel's implementation of the simultaneous multi-threading (SMT) paradigm <ref type=""bibr"" target=""#b17"">[19]</ref>.</p><p>The key characteristic of the SMT architecture is i",0
"kloads. Many interactive services such as MongoDB <ref type=""bibr"" target=""#b0"">[1]</ref> and NGINX <ref type=""bibr"" target=""#b1"">[2]</ref> are examples of latency-critical workloads. An important cha",0
"Special interest is taken in latency-critical workloads. Many interactive services such as MongoDB <ref type=""bibr"" target=""#b0"">[1]</ref> and NGINX <ref type=""bibr"" target=""#b1"">[2]</ref> are exampl",0
"o gather to the requesting core, only the requested elements from the data array.</p><p>Song et al. <ref type=""bibr"" target=""#b10"">[11]</ref> propose a graph processor based on sparse matrix algebra,",1
"rg/ns/1.0""><head>E. Comparison to other Graph Processors</head><p>The Cray Urika-GD graph processor <ref type=""bibr"" target=""#b7"">[8]</ref> was one of the first commercial graph-oriented big data proc",0
"a graph. Determining the relationships between entities in a graph is the basis of graph analytics <ref type=""bibr"" target=""#b1"">[2]</ref>. Graph analytics poses important challenges on existing proc",0
"the network needs to have a high radix and a low diameter. This is achieved with a HyperX topology <ref type=""bibr"" target=""#b6"">[7]</ref>, with all-toall connections on each level. Links on the high",0
"even worse performance for small node counts, due to the overhead of the fine-grained communication <ref type=""bibr"" target=""#b12"">[13]</ref>. For PIUMA, the application code does not need to change f",0
"es has improved both the rate of learning and final performance. Similar to our findings about MoCo,<ref type=""bibr"" target=""#b32"">Wu et al. (2017)</ref> find that mining the very hardest negatives hu",1
"e=""bibr"" target=""#b31"">(Weinberger &amp; Saul, 2009)</ref>. In this paradigm, selecting the hardest <ref type=""bibr"" target=""#b5"">(Bucher et al., 2016)</ref> or harder <ref type=""bibr"" target=""#b23"">(",1
"<ref type=""bibr"" target=""#b25"">(Sung, 1996;</ref><ref type=""bibr"">Canévet &amp; Fleuret, 2015;</ref><ref type=""bibr"" target=""#b24"">Shrivastava et al., 2016)</ref>. However, none of the aforementioned",0
"cy. However, not all examples are learned at the same time: the networks learn ""easy"" examples first<ref type=""bibr"" target=""#b1"">(Arpit et al., 2017;</ref><ref type=""bibr"" target=""#b21"">Mangalam &amp",0
"ya &amp; Bilge, 2019)</ref> and on the impact of different training examples in supervised learning <ref type=""bibr"" target=""#b4"">(Birodkar et al., 2019)</ref> suggests that understanding the relative me supervised contexts, much or all training data seems important for reaching the highest accuracy <ref type=""bibr"" target=""#b4"">(Birodkar et al., 2019)</ref>. We aim to experimentally assess which o va et al., 2018)  or 10% from ImageNet<ref type=""bibr"" target=""#b29"">(Vodrahalli et al., 2018;</ref><ref type=""bibr"" target=""#b4"">Birodkar et al., 2019)</ref> without decreases in accuracy. However, n",0
"<ref type=""bibr"" target=""#b30"">(Wang &amp; Isola, 2020)</ref>, and proposed a theoretical framework <ref type=""bibr"" target=""#b0"">(Arora et al., 2019)</ref>, among others. However, existing works on C ctives implicitly try to align similar instances while uniformly utilizing the embedding space, and <ref type=""bibr"" target=""#b0"">Arora et al. (2019)</ref> propose a theoretical framework for understa",0
"s for self-supervised have been proposed that do not work within the CID paradigm, including RotNet <ref type=""bibr"" target=""#b15"">(Gidaris et al., 2018)</ref>, Jigsaw <ref type=""bibr"" target=""#b22"">(",0
"m to choose specific examples for training-for example hard negative mining and curriculum learning <ref type=""bibr"" target=""#b9"">(Chen et al., 2020a;</ref><ref type=""bibr"" target=""#b12"">Chuang et al. ines MoCo v1 <ref type=""bibr"" target=""#b18"">(He et al., 2019)</ref> with several features of SimCLR <ref type=""bibr"" target=""#b9"">(Chen et al., 2020a)</ref>. We use ImageNet for pre-training and evalu ibr"">(Tian et al., 2019)</ref>, Moco <ref type=""bibr"" target=""#b18"">(He et al., 2019)</ref>, SimCLR <ref type=""bibr"" target=""#b9"">(Chen et al., 2020a)</ref>, MoCo v2 <ref type=""bibr"" target=""#b11"">(Ch",0
"similar from every other image in the dataset <ref type=""bibr"" target=""#b50"">(Wu et al., 2018;</ref><ref type=""bibr"" target=""#b1"">Chen et al., 2020)</ref>. We propose to train feature representations "" target=""#b26"">Misra &amp; Maaten, 2020;</ref><ref type=""bibr"" target=""#b15"">He et al., 2020;</ref><ref type=""bibr"" target=""#b1"">Chen et al., 2020)</ref> which aims to learn representations by consid main. We use a state-of-the-art self-supervised loss function based on contrastive learning: SimCLR <ref type=""bibr"" target=""#b1"">(Chen et al., 2020)</ref>. The SimCLR loss encourages two augmentation seline, SimCLR that uses the novel domain unlabeled data D u to train a representation using SimCLR <ref type=""bibr"" target=""#b1"">(Chen et al., 2020)</ref>, and then uses the resulting representation",1
"ature exploration, we first describe the recently proposed contextual stochastic block model (cSBM) <ref type=""bibr"" target=""#b10"">(Deshpande et al., 2018)</ref>. The cSBM allows for smoothly controll rning of GNNs on graphs with arbitrary levels of homophily and heterophily, we propose to use cSBMs <ref type=""bibr"" target=""#b10"">(Deshpande et al., 2018)</ref> to generate synthetic graphs. We consi d have similar performances for φ and −φ. Due to space limitation we refer the interested reader to <ref type=""bibr"" target=""#b10"">(Deshpande et al., 2018)</ref> for a review of all formal theoretical philic graphs. The information-theoretic limits of reconstruction for the cSBM are characterized in <ref type=""bibr"" target=""#b10"">Deshpande et al. (2018)</ref>. The results show that, asymptotically, ate synthetic data is that the information-theoretic limit of the model is already characterized in <ref type=""bibr"" target=""#b10"">Deshpande et al. (2018)</ref>. This result is summarized below.</p><p de et al. (2018)</ref>. This result is summarized below.</p><p>Theorem A.7 (Informal main result in <ref type=""bibr"" target=""#b10"">Deshpande et al. (2018)</ref>). Assume that n, f → ∞, n f → ξ and d →",1
"rvised node classification and graph classification <ref type=""bibr"" target=""#b41"">(Zhu, 2005;</ref><ref type=""bibr"" target=""#b31"">Shervashidze et al., 2011;</ref><ref type=""bibr"" target=""#b24"">Lü &am",0
"ef>. For an excellent in-depth discussion of PageRank methods, the interested reader is referred to <ref type=""bibr"" target=""#b13"">(Gleich, 2015)</ref>. The work in <ref type=""bibr"" target=""#b22"">Li e",0
"lso a common assumption in graph clustering <ref type=""bibr"" target=""#b35"">(Von Luxburg, 2007;</ref><ref type=""bibr"" target=""#b33"">Tsourakakis, 2015;</ref><ref type=""bibr"" target=""#b8"">Dau &amp; Milen",0
"=""#b3"">[5]</ref>, <ref type=""bibr"" target=""#b4"">[6]</ref>, <ref type=""bibr"" target=""#b7"">[9]</ref>, <ref type=""bibr"" target=""#b9"">[11]</ref>, <ref type=""bibr"" target=""#b11"">[13]</ref>, <ref type=""bibr een these sub-problems. For example, coauthors, which are used as a strong evidence in many methods <ref type=""bibr"" target=""#b9"">[11]</ref>, <ref type=""bibr"" target=""#b30"">[32]</ref>, <ref type=""bibr Zhang"" in this example. More troubles may appear when multi-hop coauthorships are used as features <ref type=""bibr"" target=""#b9"">[11]</ref>, <ref type=""bibr"" target=""#b30"">[32]</ref>. For instance, "" icient, compared with the state-ofthe-art methods CE <ref type=""bibr"" target=""#b5"">[7]</ref>, GHOST <ref type=""bibr"" target=""#b9"">[11]</ref>, CSLR <ref type=""bibr"" target=""#b17"">[19]</ref>, MIX <ref t d the redundant information, we only consider valid 2-hop coauthorship paths connecting two authors <ref type=""bibr"" target=""#b9"">[11]</ref>. Specifically, a valid 2-hop coauthorship path in G is an A fficiency of NDCC versus state-of-the-art methods CE <ref type=""bibr"" target=""#b5"">[7]</ref>, GHOST <ref type=""bibr"" target=""#b9"">[11]</ref>, CSLR <ref type=""bibr"" target=""#b17"">[19]</ref>, MIX <ref t greedy agglomerative clustering method is used to merge the most similar clusters.</p><p>(2) GHOST <ref type=""bibr"" target=""#b9"">[11]</ref> is a graph-based method employing coauthorship only. Its si n baselines, only CE and GHOST analyze the time complexity <ref type=""bibr"" target=""#b5"">[7]</ref>, <ref type=""bibr"" target=""#b9"">[11]</ref>. The time complexity of CE is O(|A (0) |k log |A (0) |), wh and unsupervised <ref type=""bibr"" target=""#b5"">[7]</ref>, <ref type=""bibr"" target=""#b7"">[9]</ref>, <ref type=""bibr"" target=""#b9"">[11]</ref>, <ref type=""bibr"" target=""#b17"">[19]</ref>, <ref type=""bibr pe=""bibr"" target=""#b17"">[19]</ref>, <ref type=""bibr"" target=""#b37"">[39]</ref>, affinity propagation <ref type=""bibr"" target=""#b9"">[11]</ref> and Markov clustering <ref type=""bibr"" target=""#b39"">[41]</",1
"used real-life datasets AMiner (http://www. aminer.org) <ref type=""bibr"" target=""#b30"">[32]</ref>, <ref type=""bibr"" target=""#b31"">[33]</ref>, <ref type=""bibr"" target=""#b32"">[34]</ref>, <ref type=""bib",0
""">[21]</ref>, <ref type=""bibr"" target=""#b22"">[24]</ref>, <ref type=""bibr"" target=""#b30"">[32]</ref>, <ref type=""bibr"" target=""#b42"">[44]</ref>. For example, we read an interesting paper written by ""Wei "">[38]</ref>, <ref type=""bibr"" target=""#b37"">[39]</ref>, <ref type=""bibr"" target=""#b38"">[40]</ref>, <ref type=""bibr"" target=""#b42"">[44]</ref>. For each name to be disambiguated, these methods only dea LR <ref type=""bibr"" target=""#b17"">[19]</ref>, MIX <ref type=""bibr"" target=""#b16"">[18]</ref>, and AM <ref type=""bibr"" target=""#b42"">[44]</ref>. Specifically, (a) NDCC on average improves the Macro-F1 o LR <ref type=""bibr"" target=""#b17"">[19]</ref>, MIX <ref type=""bibr"" target=""#b16"">[18]</ref>, and AM <ref type=""bibr"" target=""#b42"">[44]</ref>, (2) the effectiveness of author number estimation, (3) ef we randomly choose 5 labeled names to train the model and use the others for testing.</p><p>(5) AM <ref type=""bibr"" target=""#b42"">[44]</ref> is the method deployed in AMiner to tackle the name disamb efficient to compute the clustering from scratch due to the local linkage learning and IO overhead <ref type=""bibr"" target=""#b42"">[44]</ref>. Indeed, this method even cannot disambiguate all names in "">[35]</ref>, <ref type=""bibr"" target=""#b36"">[38]</ref>, <ref type=""bibr"" target=""#b38"">[40]</ref>, <ref type=""bibr"" target=""#b42"">[44]</ref> and unsupervised <ref type=""bibr"" target=""#b5"">[7]</ref>,",0
"b7"">[9]</ref>, <ref type=""bibr"" target=""#b9"">[11]</ref>, <ref type=""bibr"" target=""#b11"">[13]</ref>, <ref type=""bibr"" target=""#b13"">[15]</ref>, <ref type=""bibr"" target=""#b15"">[17]</ref>, <ref type=""bib scores are no less than t, and updates the network G (equally, the matrix W AP ) accordingly (lines <ref type=""bibr"" target=""#b13"">[15]</ref><ref type=""bibr"" target=""#b14"">[16]</ref>. It also needs to disambiguation can be divided into two classes: supervised <ref type=""bibr"" target=""#b2"">[4]</ref>, <ref type=""bibr"" target=""#b13"">[15]</ref>, <ref type=""bibr"" target=""#b15"">[17]</ref>, <ref type=""bib >195,</ref><ref type=""bibr"" target=""#b17"">19)</ref> times faster than (CE, CSLR and MIX) on AMiner, <ref type=""bibr"" target=""#b13"">(15,</ref><ref type=""bibr"" target=""#b6"">8)</ref> times faster than (C",0
""">[19]</ref>, <ref type=""bibr"" target=""#b27"">[29]</ref>, <ref type=""bibr"" target=""#b30"">[32]</ref>, <ref type=""bibr"" target=""#b33"">[35]</ref>, <ref type=""bibr"" target=""#b34"">[36]</ref>, <ref type=""bib "">[15]</ref>, <ref type=""bibr"" target=""#b15"">[17]</ref>, <ref type=""bibr"" target=""#b16"">[18]</ref>, <ref type=""bibr"" target=""#b33"">[35]</ref>, <ref type=""bibr"" target=""#b36"">[38]</ref>, <ref type=""bib andom forests <ref type=""bibr"" target=""#b15"">[17]</ref>, <ref type=""bibr"" target=""#b16"">[18]</ref>, <ref type=""bibr"" target=""#b33"">[35]</ref>, which is then used to assign publications to different au",0
"w compact model (student) by transferring knowledge from a previously trained large model (teacher) <ref type=""bibr"" target=""#b6"">[7]</ref>. The knowledge transfer is conducted as follows: First, the to its small size. Most KD methods have focused on the image classification problem. An early work <ref type=""bibr"" target=""#b6"">[7]</ref> matches the softmax distribution of the teacher and the stud large"" model (teacher) <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b21"">22]</ref>. The student model tr",1
"d model architecture to capture complex patterns make the size of the model continuously increasing <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" ta by the significant success of knowledge distillation (KD) in the computer vision field, a few work <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b24"">25]</ref> have employed KD f s comparable performance to that of the teacher, and also has a lower latency due to its small size <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b24"">25]</ref>.</p><p>The core id 3]</ref>. By using the additional supervisions from the teacher model, the state-of-the-art methods <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b24"">25]</ref> have achieved comp r models with faster inference time.</p><p>However, there are still limitations in existing methods <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b24"">25]</ref>. First, the learni ns, contains more detailed information that Figure <ref type=""figure"">1</ref>: The existing methods <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b24"">25]</ref> distill the knowle 's predictions with direct consideration of ranking orders among items. Unlike the existing methods <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b24"">25]</ref> that distill the k ested in. RRD achieves superior recommendation performance compared to the state-of-the-art methods <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b24"">25]</ref>. An unified framew d the computational time and memory cost required for the inference are also increasing accordingly <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" ta ender System. Recently, inspired by the huge success of KD in the computer vision field, a few work <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b24"">25]</ref> have adopted KD to den layer. The limiting ratios (𝜙) are {0.1, 0.5, 1.0}. Following the notation of the previous work <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b24"">25]</ref>, we call the stude items that each user would not be interested in among numerous unobserved items only labeled as '0' <ref type=""bibr"" target=""#b12"">[13]</ref>. By using the additional supervisions from the teacher mod ting relative importance among top-𝐾 items.</p><p>A subsequent work Collaborative Distillation (CD) <ref type=""bibr"" target=""#b12"">[13]</ref> first samples unobserved items from the teacher's recommen king from the teacher's predictions for distilling the knowledge. • Collaborative Distillation (CD) <ref type=""bibr"" target=""#b12"">[13]</ref>: The state-of-the-art KD method for recommender system. CD",1
"atterns make the size of the model continuously increasing <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" tar required for the inference are also increasing accordingly <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" tar distillation (KD) in the computer vision field, a few work <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b24"">25]</ref> have employed KD for RS to reduce the size of models while eacher, and also has a lower latency due to its small size <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b24"">25]</ref>.</p><p>The core idea behind this process is that the soft l sions from the teacher model, the state-of-the-art methods <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b24"">25]</ref> have achieved comparable or even better performance to the p>However, there are still limitations in existing methods <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b24"">25]</ref>. First, the learning of the student is only guided by the t at Figure <ref type=""figure"">1</ref>: The existing methods <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b24"">25]</ref> distill the knowledge only based on the teacher's predictio of ranking orders among items. Unlike the existing methods <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b24"">25]</ref> that distill the knowledge of an item at a time, RRD formul ation performance compared to the state-of-the-art methods <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b24"">25]</ref>. An unified framework. We propose a novel framework-DE-RRD- uge success of KD in the computer vision field, a few work <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b24"">25]</ref> have adopted KD to RS. A pioneer work is Ranking Distillati sting items, we adopt a ranking position importance scheme <ref type=""bibr"" target=""#b19"">[20,</ref><ref type=""bibr"" target=""#b24"">25]</ref> that places more emphasis on the higher positions in the ra .1, 0.5, 1.0}. Following the notation of the previous work <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b24"">25]</ref>, we call the student model trained without the help of the recommendation list would have strong correlations to the items that the user has interacted before <ref type=""bibr"" target=""#b24"">[25]</ref>. Also, the soft labels provide guidance for distinguishing e=""bibr"" target=""#b24"">25]</ref> have adopted KD to RS. A pioneer work is Ranking Distillation (RD) <ref type=""bibr"" target=""#b24"">[25]</ref> which applies KD for the ranking problem; Providing recomm items); the high-ranked items in the recommendation list would have strong correlations to the user <ref type=""bibr"" target=""#b24"">[25]</ref>. By using such additional supervisions from the teacher, t s. The proposed framework is compared with the following methods:</p><p>• Ranking Distillation (RD) <ref type=""bibr"" target=""#b24"">[25]</ref>: A KD method for recommender system that uses items with t",1
"he teacher and that of the student. To this end, RRD adopts the list-wise learning-to-rank approach <ref type=""bibr"" target=""#b28"">[29]</ref> and learns to ensure the student to preserve the ranking o hat of the student model. To this end, RRD adopts the classical list-wise learning-to-rank approach <ref type=""bibr"" target=""#b28"">[29]</ref>. Its core idea is to define a probability of a permutation d of the ground-truth ranking order. For more details about the list-wise approach, please refer to <ref type=""bibr"" target=""#b28"">[29]</ref>.</p><p>However, merely adopting the list-wise loss can hav p>Relaxed permutation probability. Then, RRD defines a relaxed permutation probability motivated by <ref type=""bibr"" target=""#b28"">[29]</ref>. For user 𝑢, 𝝅 𝒖 denotes a ranked list of all the sampled",1
"ques <ref type=""bibr"" target=""#b25"">[26]</ref>, and approximated nearest neighbor search techniques <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b22"">23]</ref> have been successful",0
"eter that controls the effects of RD. The base recommender can be any existing RS model such as BPR <ref type=""bibr"" target=""#b20"">[21]</ref>, NeuMF <ref type=""bibr"" target=""#b5"">[6]</ref>, and L 𝐵𝑎𝑠𝑒 . The output of the mapping function can be a separate representation of a user, an item (e.g., BPR <ref type=""bibr"" target=""#b20"">[21]</ref>) or their combined representation (e.g., NeuMF <ref type="" p learning model that are broadly used for top-𝑁 recommendation with implicit feedback.</p><p>• BPR <ref type=""bibr"" target=""#b20"">[21]</ref>: A learning-to-rank model for implicit feedback. It assume Foursquare as done in <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b20"">21]</ref>. Data statistics are summarized in Table <ref type=""table""",0
"=""#b13"">14,</ref><ref type=""bibr"" target=""#b25"">26]</ref>. Specifically, tree-based data structures <ref type=""bibr"" target=""#b1"">[2]</ref>, data compression techniques <ref type=""bibr"" target=""#b25"">",0
"ut evaluation protocol <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b18"">19]</ref>. For each user, we leave out a single interacted item for t",0
""" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b21"">22]</ref>. The student model trained with KD has comparable performan o improved learning of the student model. Subsequent methods <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b21"">22]</ref> have focused on distilling knowledge from intermediate laye e layers are generally bigger than that of the student, they <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b21"">22]</ref> utilize additional layers to bridge the different dimension",0
"s. In addition, several work has focused on accelerating the inference of the existing recommenders <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" targ | I | . Each element of 𝑹 has a binary value indicating whether a user has interacted with an item <ref type=""bibr"" target=""#b0"">(1)</ref> or not (0). Note that an unobserved interaction does not nec",0
"use two public real-world datasets: CiteULike <ref type=""bibr"" target=""#b26"">[27]</ref>, Foursquare <ref type=""bibr"" target=""#b16"">[17]</ref>. We remove users and items having fewer than five ratings",0
"a workaround, we adopt a continuous relaxation of the discrete distribution by using Gumbel-Softmax <ref type=""bibr"" target=""#b7"">[8]</ref>. The Gumbel-Softmax is a continuous distribution on the simp",0
"the inference cost <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b29"">30]</ref>. They first learn b",0
"simplex that can approximate samples from a categorical distribution; it uses the Gumbel-Max trick <ref type=""bibr"" target=""#b17"">[18]</ref> to draw samples from the categorical distribution, then us",0
"nce gain when the student has the identical structure to the teacher model (i.e., self-distillation <ref type=""bibr"" target=""#b4"">[5]</ref>). Furthermore, we provide both qualitative and quantitative ingly, KD has turned out to be effective in improving the teacher model itself by self-distillation <ref type=""bibr"" target=""#b4"">[5]</ref>. Knowledge Distillation in Recommender System. Recently, ins knowledge from a previously trained ""large"" model (teacher) <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=",0
"endation performance. Since a user is interested in only a few items among the numerous total items <ref type=""bibr"" target=""#b9"">[10]</ref>, learning the detailed ranking orders of all items is not o anking performance. Because a user is interested in only a few items among the numerous total items <ref type=""bibr"" target=""#b9"">[10]</ref>, learning the detailed ranking orders of all the unobserved ctiveness and Efficiency. Several methods have adopted hash techniques to reduce the inference cost <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" tar three ranking metrics <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b9"">10]</ref>: hit ratio (H@𝑁 ), normalized discounted cumulative gain (N@",0
"e range of the rest. To sample the interesting items, we adopt a ranking position importance scheme <ref type=""bibr"" target=""#b19"">[20,</ref><ref type=""bibr"" target=""#b24"">25]</ref> that places more e",0
"d on accelerating the inference of the existing recommenders <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b25"">26]</ref>. Specifically, tree",0
"s have adopted hash techniques to reduce the inference cost <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" tar",0
"ompact"" model (student) by transferring knowledge from a previously trained ""large"" model (teacher) <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target the one-hot class label, which leads to improved learning of the student model. Subsequent methods <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b21"">22]</ref> have focused on dist e layers. Because teacher's intermediate layers are generally bigger than that of the student, they <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b21"">22]</ref> utilize additional l",0
"type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b7"">8]</ref> or high-order interactions <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b4"">5]</ref>, as shown in Fig. <ref the performance of existing state-of-the-art methods such as <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b2"">3]</ref>, especially when the size of embedding dimension is high.</p> 1432 AFM <ref type=""bibr"" target=""#b10"">[11]</ref> 0.8038 0.4478 0.7817 0.3792 0.6618 0.1457 DeepFM <ref type=""bibr"" target=""#b2"">[3]</ref> 0.8091 0.4423 0.7878 0.3753 0.6708 0.1372 DCN <ref type=""bib onduct the comparisons on another two datasets: Avazu and JD.com, and the results are shown in Tab. <ref type=""bibr"" target=""#b2"">3</ref>. We can see that, in all three datasets, our FED outperforms o",1
"t, thus it's effective to learn information from their interactions, such as low-order interactions <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b7"">8]</ref> or high-order interact",1
"ecifically, we propose a novel module based on dimension recalibration and self-attention mechanism <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b8"">9]</ref> to learn the relations",0
"8 0.1372 DCN <ref type=""bibr"" target=""#b9"">[10]</ref> 0.8093 0.4420 0.7875 0.3759 0.6702 0.1361 PNN <ref type=""bibr"" target=""#b6"">[7]</ref> 0.8094 0.4414 0.7879 0.3752 0.6705 0.1360 xDeepFM <ref type=",0
"ased on dimension recalibration and self-attention mechanism <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b8"">9]</ref> to learn the relations among latent fields. Since each latent",0
"360 xDeepFM <ref type=""bibr"" target=""#b4"">[5]</ref> 0.8096 0.4412 0.7877 0.3753 0.6710 0.1356 FGCNN <ref type=""bibr"" target=""#b5"">[6]</ref> 0.8093 0.4422 0.7878 0.3752 0.6709 0.1361 FED (Ours) 0.8113",0
"formation for CTR prediction and boost the performance of existing state-of-the-art methods such as <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b2"">3]</ref>, especially when the s le"" target=""#tab_0"">1</ref>. We find that both architectures get improvements in AUC and decreases  <ref type=""bibr"" target=""#b0"">[1]</ref> 0.8076 0.4436 0.7869 0.3755 0.6685 0.1432 AFM <ref type=""bib",0
"t samples are supposed to pay attention to different fields and its interactions for the given task <ref type=""bibr"" target=""#b10"">[11]</ref>. We propose to utilize attention mechanism to model field- nd decreases  <ref type=""bibr"" target=""#b0"">[1]</ref> 0.8076 0.4436 0.7869 0.3755 0.6685 0.1432 AFM <ref type=""bibr"" target=""#b10"">[11]</ref> 0.8038 0.4478 0.7817 0.3792 0.6618 0.1457 DeepFM <ref type",0
"tion from their interactions, such as low-order interactions <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b7"">8]</ref> or high-order interactions <ref type=""bibr"" target=""#b2"">[3,<",0
"0.1457 DeepFM <ref type=""bibr"" target=""#b2"">[3]</ref> 0.8091 0.4423 0.7878 0.3753 0.6708 0.1372 DCN <ref type=""bibr"" target=""#b9"">[10]</ref> 0.8093 0.4420 0.7875 0.3759 0.6702 0.1361 PNN <ref type=""bi dy clear compared with recent works such as xDeepFM <ref type=""bibr"" target=""#b4"">[5]</ref> and DCN <ref type=""bibr"" target=""#b9"">[10]</ref>.</p><p>Since DRM learns the relations of dimensions in embe",0
"ts decrease along with neighbor hops increasing. Inspired by the architecture design of Transformer <ref type=""bibr"" target=""#b30"">[31]</ref>, we leverage attention mechanism to learn the weight of so",1
"tion learning (GraphSAGE) <ref type=""bibr"" target=""#b8"">[9]</ref> and graph attention network (GAT) <ref type=""bibr"" target=""#b31"">[32]</ref>, have been attracting considerable attention recently. The performs multi-head attention to jointly attend information from different representation subspaces <ref type=""bibr"" target=""#b31"">[32]</ref>, while treats each subspace equally, and can be seen as a",0
"g score based on similar users. Yu et al. <ref type=""bibr"" target=""#b39"">[40]</ref> and Zhao et al. <ref type=""bibr"" target=""#b42"">[43]</ref> employ various types of meta paths between users and items ation and non-linearity of deep neural networks for modeling the user-item interactions. • FMG 𝑟𝑎𝑛𝑘 <ref type=""bibr"" target=""#b42"">[43]</ref> (B): This is a HIN based method which combines meta graph",0
"ent features for recommendation. Shi et al. <ref type=""bibr"" target=""#b24"">[25]</ref> and Hu et al. <ref type=""bibr"" target=""#b13"">[14]</ref> explicitly extract meta paths connecting users to items an sed matrix factorization with factorization machine for rating prediction. we follow the prior work <ref type=""bibr"" target=""#b13"">[14]</ref> to modify its optimization objective as BPR for top-𝑁 reco b13"">[14]</ref> to modify its optimization objective as BPR for top-𝑁 recommendation.</p><p>• MCRec <ref type=""bibr"" target=""#b13"">[14]</ref> (B): This method leverages meta-path based context with co and rank the test items among the 100 items for each user <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b13"">14]</ref>. To evaluate the performance of top-𝑁 recommendation for al",0
"hborhood information <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b29"">30,</ref><ref type=""bibr"" target=""#b33"">[34]</ref><ref type=""bibr"" target=""#b34"">[35]</ref><ref type=""bibr"" t ef type=""bibr"" target=""#b35"">[36]</ref>, KGCN <ref type=""bibr"" target=""#b34"">[35]</ref> and KGCN-LS <ref type=""bibr"" target=""#b33"">[34]</ref> Full Paper Track CIKM '20, October 19-23, 2020, Virtual Ev",0
"ackground knowledge <ref type=""bibr"" target=""#b0"">[1]</ref>. Although Neural Attention Models (NAM) <ref type=""bibr"" target=""#b1"">[2]</ref> are endowed with a certain degree of interpretability in vis 8]</ref>), (c) discover inherent bias in the model's predictive strategy (e.g., contextual modeling <ref type=""bibr"" target=""#b1"">[2]</ref>[9]), (d) prevent prediction errors in unintuitive scenarios e-infusion during the model learning using information-theoretic loss function (e.g., KL divergence <ref type=""bibr"" target=""#b1"">[2]</ref>) can check conceptual drifting at the representational level ge in DL models can be categorized into the shallow infusion, semi-deep infusion, and deep infusion <ref type=""bibr"" target=""#b1"">[2]</ref>. In shallow infusion, both the external information and meth here sentences are purposefully paraphrased to elicit meaningful responses from an agent (or user); <ref type=""bibr"" target=""#b1"">(2)</ref> The clinical conversation contains implicit references to he",1
"s a human-understandable justification for the prediction <ref type=""bibr"" target=""#b12"">[13]</ref> <ref type=""bibr"" target=""#b13"">[14]</ref>. Such systems are considered to be potentially useful for",0
"nswering domains, particularly towards queries with disjunctive clauses (questions that contain or) <ref type=""bibr"" target=""#b2"">[3]</ref>. We have observed unpredictable responses to questions like",0
"hensible explanations around the decision-making process <ref type=""bibr"" target=""#b4"">[5]</ref>[6] <ref type=""bibr"" target=""#b6"">[7]</ref>. In contrast, interpretability is the ability to discern the",0
"=""#b10"">[11]</ref><ref type=""bibr"" target=""#b11"">[12]</ref><ref type=""bibr"" target=""#b12"">[13]</ref><ref type=""bibr"" target=""#b13"">[14]</ref>. To generate diversified results, these methods either exp ef type=""bibr"" target=""#b7"">[8]</ref>, HxQuAD/HPM2 <ref type=""bibr"" target=""#b8"">[9]</ref> and DSSA <ref type=""bibr"" target=""#b13"">[14]</ref>.</p><p>Those existing approaches used greedy document sequ strategy may not lead to global optimal rankings. Based on the reinforced learning approach MDP-DIV <ref type=""bibr"" target=""#b13"">[14]</ref>, Feng <ref type=""bibr"" target=""#b14"">[15]</ref> proposed t rmula><p>Here 𝒘 𝑟 is a learnable parameter. We use the same relevance features as the previous work <ref type=""bibr"" target=""#b13"">[14]</ref> for 𝒙 𝑞 and 𝒙 𝑞 𝑖 , including BM25, TF-IDF, language model bers of incoming links and outgoing links, et al. More details about these features can be found in <ref type=""bibr"" target=""#b13"">[14]</ref> and we omit the details due to space limitation. In the fu used are actually the document embeddings. We use the subtopic embeddings released by Jiang et al. <ref type=""bibr"" target=""#b13"">[14]</ref> based on doc2vec. The subtopic embeddings is produced from t diversification task is limited, we inherit the list-pairwise sampling approach from Jiang et al. <ref type=""bibr"" target=""#b13"">[14]</ref> in order to get enough training samples. We are using pair ance features and embeddings exactly the same as the DSSA, which have been released by Jiang et al. <ref type=""bibr"" target=""#b13"">[14]</ref> in the repository on GitHub<ref type=""foot"" target=""#foot_ =""#b10"">[11]</ref><ref type=""bibr"" target=""#b11"">[12]</ref><ref type=""bibr"" target=""#b12"">[13]</ref><ref type=""bibr"" target=""#b13"">[14]</ref>, all those metrics are computed on top 20 results of a doc ""#b11"">[12]</ref> and PAMM-NTN <ref type=""bibr"" target=""#b12"">[13]</ref>. Inspired by previous work <ref type=""bibr"" target=""#b13"">[14]</ref>, we use the metric of 𝛼 − 𝑛DCG@20 to tune the parameters. 100-dimensional vectors generated by the LDA <ref type=""bibr"" target=""#b31"">[32]</ref>.</p><p>DSSA <ref type=""bibr"" target=""#b13"">[14]</ref>. We train the DSSA model with the code and data released b lt is denoted as DSSA (doc2vec).</p><p>Since the deep reinforced learning based models e.g. MDP-DIV <ref type=""bibr"" target=""#b13"">[14]</ref> and M2DIV <ref type=""bibr"" target=""#b14"">[15]</ref> are ta target=""#b6"">[7]</ref><ref type=""bibr"" target=""#b7"">[8]</ref><ref type=""bibr"" target=""#b8"">[9]</ref><ref type=""bibr"" target=""#b13"">14]</ref> (i.e., explicit approaches), or directly reduce result redu",1
"sequence. Recently, the models fully based on self-attention mechanism (denoted as self-attention  <ref type=""bibr"" target=""#b15"">[16]</ref> in the Neural Machine Translation (NMT) task, have achieve",1
"erfitting, we follow <ref type=""bibr"" target=""#b21"">[22]</ref> and use unsupervised methods doc2vec <ref type=""bibr"" target=""#b22"">[23]</ref> to generate the initial document representations instead o",0
"e initial distributed representation of the document 𝑑 𝑡 . In order to avoid overfitting, we follow <ref type=""bibr"" target=""#b21"">[22]</ref> and use unsupervised methods doc2vec <ref type=""bibr"" targ",0
"p>Research shows that most queries issued by users are short <ref type=""bibr"" target=""#b0"">[1]</ref><ref type=""bibr"" target=""#b1"">[2]</ref><ref type=""bibr"" target=""#b2"">[3]</ref><ref type=""bibr"" targe",0
"sed and they are based on handcrafted features and functions <ref type=""bibr"" target=""#b4"">[5]</ref><ref type=""bibr"" target=""#b5"">[6]</ref><ref type=""bibr"" target=""#b6"">[7]</ref><ref type=""bibr"" targe enerate diversified results, these methods either explicitly model subtopic coverage of the results <ref type=""bibr"" target=""#b5"">[6]</ref><ref type=""bibr"" target=""#b6"">[7]</ref><ref type=""bibr"" targe are used in our experiments. Besides the metrics above, we also include the metrics of Precision-IA <ref type=""bibr"" target=""#b5"">[6]</ref> (denoted as Pre-IA) and Subtopic Recall <ref type=""bibr"" tar",0
"s to RNNs and CNNs in many NLP tasks. However, to the best of our knowledge, only a few researchers <ref type=""bibr"" target=""#b19"">[20,</ref><ref type=""bibr"" target=""#b20"">21]</ref> have tried to use on the sequential selection process. This is similar to some ad-hoc ranking models such as SetRank <ref type=""bibr"" target=""#b19"">[20]</ref>.</p><p>This process is formulated as the following equatio ead>3.3.2</head><p>The Multi-Head Attention Component. Following by some previous work e.g. SetRank <ref type=""bibr"" target=""#b19"">[20]</ref>, we use the multi-head strategy in order to learn multiple [1, ℎ].<label>(6)</label></formula><p>Here all those 𝑾 parameters are learnable. Previous research <ref type=""bibr"" target=""#b19"">[20]</ref> has shown that using the multi-head strategy may help the all those documents. Similar as some other selfattention based ad-hoc ranking approach e.g. SetRank <ref type=""bibr"" target=""#b19"">[20]</ref>, the model can return the ranking list with sorting all th sion</head><p>DESA is inspired by several existing models in IR based on selfattention e.g. SetRank <ref type=""bibr"" target=""#b19"">[20]</ref>. And the implicit implementation of DESA with no subtopics",0
"ch result diversification are unsupervised and they are based on handcrafted features and functions <ref type=""bibr"" target=""#b4"">[5]</ref><ref type=""bibr"" target=""#b5"">[6]</ref><ref type=""bibr"" targe the more diversified it will be. The most typical implicit model is the MMR (Max Margin Relevance) <ref type=""bibr"" target=""#b4"">[5]</ref> model:</p><formula xml:id=""formula_0"">Score MMR = 𝜆score(𝑑 𝑖 reduce result redundancy by comparing document-document similarity regardless the use of subtopics <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b9"">[10]</ref><ref type=""bibr"" targ",0
"s relevant or irrelevant at subtopic level. We conduct all the experiments on the ClueWeb09 dataset <ref type=""bibr"" target=""#b25"">[26]</ref>.</p><p>The subtopics used by the model come from the Googl",0
"target=""#b5"">[6]</ref><ref type=""bibr"" target=""#b6"">[7]</ref><ref type=""bibr"" target=""#b7"">[8]</ref><ref type=""bibr"" target=""#b8"">[9]</ref>. While in recent years, more and more researchers tried to u target=""#b5"">[6]</ref><ref type=""bibr"" target=""#b6"">[7]</ref><ref type=""bibr"" target=""#b7"">[8]</ref><ref type=""bibr"" target=""#b8"">[9]</ref><ref type=""bibr"" target=""#b13"">14]</ref> (i.e., explicit appr D <ref type=""bibr"" target=""#b6"">[7]</ref>, PM2 <ref type=""bibr"" target=""#b7"">[8]</ref>, HxQuAD/HPM2 <ref type=""bibr"" target=""#b8"">[9]</ref> and DSSA <ref type=""bibr"" target=""#b13"">[14]</ref>.</p><p>Th ber of the queries is 10, and the average subtopic number is about 9.48. As those previous works do <ref type=""bibr"" target=""#b8"">[9]</ref> we treat all those subtopics with uniform weights.</p><p>For "" target=""#foot_2"">3</ref> as the non-diversified baseline. These results are released by Hu et at. <ref type=""bibr"" target=""#b8"">[9]</ref> and can be found on the website <ref type=""foot"" target=""#fo ef type=""bibr"" target=""#b6"">[7]</ref>, PM2 <ref type=""bibr"" target=""#b7"">[8]</ref>, HxQuAD and HPM2 <ref type=""bibr"" target=""#b8"">[9]</ref>. These are the unsupervised explicit baseline approaches for",0
"urbations to the input node features with graph structures unchanged. FLAG leverages ""free"" methods <ref type=""bibr"" target=""#b27"">(Shafahi et al., 2019)</ref> to conduct efficient adversarial trainin de feature space.</p><p>Augmentation for ""free"". We leverage the ""free"" adversarial training method <ref type=""bibr"" target=""#b27"">(Shafahi et al., 2019)</ref> to craft adversarial data augmentations.",1
"ange of tasks, including visual recognition <ref type=""bibr"" target=""#b42"">(Zhao et al., 2019;</ref><ref type=""bibr"" target=""#b29"">Shen et al., 2018)</ref>, meta-learning <ref type=""bibr"" target=""#b10",0
"mple, FLAG lifts the test accuracy of GAT on ogbn-products by an absolute value of 2.31%. DeeperGCN <ref type=""bibr"" target=""#b21"">(Li et al., 2020</ref>) is another strong baseline that achieves top GraphSAGE and GAT to make the experiments scalable. For DeeperGCN, we follow the original setup by <ref type=""bibr"" target=""#b21"">Li et al. (2020)</ref> to randomly split the graph into clusters. Not al. (2020)</ref> proposed to average incoming edge features to obtain initial node features, while <ref type=""bibr"" target=""#b21"">Li et al. (2020)</ref> used summation and achieved competitive result",0
", is known to boost neural network robustness and promote resistance to adversarially chosen inputs <ref type=""bibr"" target=""#b13"">(Goodfellow et al., 2014;</ref><ref type=""bibr"" target=""#b23"">Madry e lassifier to fail, one may hope that adversarial training should be beneficial to standard accuracy <ref type=""bibr"" target=""#b13"">(Goodfellow et al., 2014;</ref><ref type=""bibr"" target=""#b30"">Tsipras onstrate effectiveness. Our implementation always uses M = 3 ascent steps for simplicity. Following <ref type=""bibr"" target=""#b13"">Goodfellow et al. (2014)</ref>; <ref type=""bibr"" target=""#b23"">Madry",0
"e left part of sampling <ref type=""bibr"" target=""#b14"">(Hamilton et al., 2017)</ref> and GraphSAINT <ref type=""bibr"" target=""#b39"">(Zeng et al., 2019</ref>) can all work with FLAG to further boost per",0
"ghbors into node representation learning and achieve state-ofthe-art performance for recommendation <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" ta pervision Signal. Most models approach the recommendation task under a supervised learning paradigm <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" ta n users and items, where 𝑦 𝑢𝑖 indicates that user 𝑢 has adopted item 𝑖 before. Most existing models <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" ta rimental Settings</head><p>We conduct experiments on three widely used benchmark datasets: Yelp2018 <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b43"">44]</ref>, Amazon-Book <ref 2018 <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b43"">44]</ref>, Amazon-Book <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b43"">44]</ref>, and Alibaba-iFash shion <ref type=""bibr"" target=""#b5"">[6]</ref> <ref type=""foot"" target=""#foot_0"">1</ref> . Following <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b43"">44]</ref>, we use the same 1 et=""#b35"">[36]</ref>, and PinSage <ref type=""bibr"" target=""#b47"">[48]</ref> since the previous work <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" ta type=""bibr"" target=""#b1"">[2]</ref>, to GCN that propagates user and item embeddings over the graph <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" ta embedding and item embedding. Here we implement it on a state-of-the-art GCN-based model, LightGCN <ref type=""bibr"" target=""#b16"">[17]</ref>. Experimental studies on three benchmark datasets demonstr ibr"" target=""#b47"">48]</ref>, concatenation <ref type=""bibr"" target=""#b43"">[44]</ref>, or summation <ref type=""bibr"" target=""#b16"">[17]</ref> over the representations of all layers.</p><p>Supervised L rization coefficient 𝜆 2 and the number of GCN layers within the suggested ranges.</p><p>• LightGCN <ref type=""bibr"" target=""#b16"">[17]</ref>. This is the state-of-the-art graph-based CF method which between LightGCN and SGL-ED.trainable parameters, the space complexity remains the same as LightGCN<ref type=""bibr"" target=""#b16"">[17]</ref>. The time complexity of model inference is also the same,",1
"via making changes on the input labeled data, achieving remarkable improvements in downstream tasks <ref type=""bibr"" target=""#b4"">[5]</ref>.</p><p>Here we wish to bring the SSL's superiority into reco rvision of negative pairs enforces the divergence among different nodes. Formally, we follow SimCLR <ref type=""bibr"" target=""#b4"">[5]</ref> and adopt the contrastive loss, InfoNCE <ref type=""bibr"" tar er a better initialization for LightGCN, which is consistent to the observation in previous studies <ref type=""bibr"" target=""#b4"">[5]</ref>. However, the better performance of joint training admits th 3.1"">Data Augmentation</head><p>Directly grafting the data augmentation adopted in CV and NLP tasks <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target type=""bibr"" target=""#b28"">29,</ref><ref type=""bibr"" target=""#b36"">37]</ref> and contrastive models <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target "" target=""#b19"">[20,</ref><ref type=""bibr"" target=""#b37"">38]</ref> or global-global contrast manner <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b14"">15]</ref>. The former focuses comparison between different samples, which typically requires multiple different views of samples <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" targ e the time complexity is treating only the users (or the items) within the batch as negative samples<ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b46"">47]</ref>, resulting in total",1
"ons. However, the observed interactions are extremely sparse <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b15"">16]</ref> compared to the whole interaction space, making it insuffic",0
"click an item and finds it unintersting after consuming it. <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b21"">22]</ref>. The neighborhood aggregation scheme in GCNs enlarges the i",0
"=""#b20"">[21]</ref> extend the idea to learn GCN for graph representation. Furthermore, Kaveh et al. <ref type=""bibr"" target=""#b13"">[14]</ref> adopt the contrastive model for learning both node and gra",0
"ied with the user-item graph, which enables considering the detailed types of linkage between items <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b41"">42,</ref><ref type=""bibr"" targ",0
"hrough a Noise Contrastive Estimation (NCE) objective, which can be in either global-local contrast <ref type=""bibr"" target=""#b19"">[20,</ref><ref type=""bibr"" target=""#b37"">38]</ref> or global-global c",0
"=""#b47"">48]</ref>. Recently, attention mechanism is introduced into GCN-based recommendation models <ref type=""bibr"" target=""#b42"">[43]</ref>, which learns to weigh the neighbors so as to capture the inkage between items <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b41"">42,</ref><ref type=""bibr"" target=""#b42"">43]</ref>.</p><p>Despite the tremendous efforts devoted on these meth",0
"target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b44"">45]</ref> is infeasible for graph-based recommendation, due to specif",0
"0""><head n=""1"">INTRODUCTION</head><p>Self-attention-based architectures, in particular Transformers <ref type=""bibr"" target=""#b43"">(Vaswani et al., 2017)</ref>, have become the model of choice in natu ww.tei-c.org/ns/1.0""><head n=""3"">METHOD</head><p>In model design we follow the original Transformer <ref type=""bibr"" target=""#b43"">(Vaswani et al., 2017)</ref> as closely as possible. An advantage of sulting sequence of embedding vectors serves as input to the encoder.</p><p>The Transformer encoder <ref type=""bibr"" target=""#b43"">(Vaswani et al., 2017)</ref> consists of alternating layers of multih xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2"">RELATED WORK</head><p>Transformers were proposed by <ref type=""bibr"" target=""#b43"">Vaswani et al. (2017)</ref> for machine translation, and have since b i-c.org/ns/1.0""><head>APPENDIX A MULTIHEAD SELF-ATTENTION</head><p>Standard qkv self-attention (SA, <ref type=""bibr"" target=""#b43"">Vaswani et al. (2017)</ref>) is a popular building block for neural a",1
"proach is to pre-train on a large text corpus and then fine-tune on a smaller task-specific dataset <ref type=""bibr"" target=""#b13"">(Devlin et al., 2019)</ref>. Thanks to Transformers' computational ef -based models are often pre-trained on large corpora and then fine-tuned for the task at hand: BERT <ref type=""bibr"" target=""#b13"">(Devlin et al., 2019)</ref> uses a denoising self-supervised pre-trai standing like localization.</p><p>Model Variants. We base ViT configurations on those used for BERT <ref type=""bibr"" target=""#b13"">(Devlin et al., 2019)</ref>, as summarized in Table <ref type=""table"" ms not only from their excellent scalability but also from large scale self-supervised pre-training <ref type=""bibr"" target=""#b13"">(Devlin et al., 2019;</ref><ref type=""bibr"" target=""#b34"">Radford et (10%) or just keeping them as is (10%). This setup is very similar to the one used for language by <ref type=""bibr"" target=""#b13"">Devlin et al. (2019)</ref>. Finally, we predict the 3-bit, mean color se it has shown best few-shot performance. We also experimented with 15% corruption rate as used by <ref type=""bibr"" target=""#b13"">Devlin et al. (2019)</ref> but results were also slightly worse on ou",0
"#b36"">(Ramachandran et al., 2019;</ref><ref type=""bibr"" target=""#b11"">Cordonnier et al., 2020;</ref><ref type=""bibr"" target=""#b54"">Zhao et al., 2020)</ref>. Alternatively, works such as Sparse Transfo",0
"al Predictions (APPNP) framework is most relevant to our work, as they also smooth base predictions <ref type=""bibr"" target=""#b22"">(Klicpera et al., 2018)</ref>. However, they focus on integrating thi ep, decoupled from the other steps. This type of prediction smoothing is similar in spirit to APPNP <ref type=""bibr"" target=""#b22"">(Klicpera et al., 2018)</ref>, which we compare against later. Howeve",1
"u et al., 2020)</ref>; the Cora, Citeseer, and Pubmed are three classic citation network benchmarks <ref type=""bibr"" target=""#b10"">(Getoor et al., 2001;</ref><ref type=""bibr"" target=""#b9"">Getoor, 2005 s: 3 layers and 256 hidden channels with learning rate equal to 0.01.</p><p>• Cora, Citseer, Pubmed <ref type=""bibr"" target=""#b10"">(Getoor et al., 2001;</ref><ref type=""bibr"" target=""#b9"">Getoor, 2005",0
"lutional Network (GCN) <ref type=""bibr"" target=""#b21"">(Kipf &amp; Welling, 2017)</ref> or GraphSAGE <ref type=""bibr"" target=""#b13"">(Hamilton et al., 2017a)</ref>; examples include Graph Attention Netw",0
"on et al., 2017b)</ref>. In our pipeline, we augment features with a regularized spectral embedding <ref type=""bibr"" target=""#b3"">(Chaudhuri et al., 2012;</ref><ref type=""bibr"" target=""#b48"">Zhang &am",0
"ype=""bibr"" target=""#b41"">Wang &amp; Leskovec (2020)</ref> (in contrast to lower label rate settings <ref type=""bibr"" target=""#b45"">(Yang et al., 2016)</ref>) to ameliorate sensitivity to hyperparamete",0
"al., 2019)</ref>, and some techniques use ad hoc incorporation of label information in the features <ref type=""bibr"" target=""#b37"">(Shi et al., 2020)</ref>. However, these approaches are still expensi ally, we include several ""state-of-the-art"" (SOTA) baselines. For Arxiv and Products, this is UniMP <ref type=""bibr"" target=""#b37"">(Shi et al., 2020)</ref> (top of OGB leaderboard, as of October 1, 20",0
"re easily scales to large datasets. Our framework also complements the Simplified Graph Convolution <ref type=""bibr"" target=""#b42"">(Wu et al., 2019)</ref>, as well as algorithms designed to increase s that GCNs gain performance by having smoothed outputs over the graph, a similar observation made by <ref type=""bibr"" target=""#b42"">Wu et al. (2019)</ref>. However, there are still gaps in performance",0
"Networks <ref type=""bibr"" target=""#b40"">(Veličković et al., 2018)</ref>, Graph Isomorphism Networks <ref type=""bibr"" target=""#b44"">(Xu et al., 2018)</ref>, and various deep models <ref type=""bibr"" tar",0
"icit data augmentation. Moreover, to supplement the input graph with more global information, MVGRL <ref type=""bibr"" target=""#b14"">[15]</ref> proposes to augment the input graph using graph diffusion. asure MI between input and representations of both nodes and edges without data augmentation; MVGRL <ref type=""bibr"" target=""#b14"">[15]</ref> proposes to learn both node-level and graph-level represen <ref type=""bibr"" target=""#b43"">[44]</ref>, GMI <ref type=""bibr"" target=""#b29"">[30]</ref>, and MVGRL <ref type=""bibr"" target=""#b14"">[15]</ref> in Table <ref type=""table"" target=""#tab_0"">1</ref>, where MI) <ref type=""bibr"" target=""#b29"">[30]</ref>, and Multi-View Graph Representation Learning (MVGRL) <ref type=""bibr"" target=""#b14"">[15]</ref>. Furthermore, we report the performance obtained using a l",1
"posed to maximize the MI between node embeddings and a global summary embedding. Following DGI, GMI <ref type=""bibr"" target=""#b29"">[30]</ref> proposes two node-level contrastive objectives to directly the agreement of node embeddings across two corrupted views of the graph.</p><p>Following DGI, GMI <ref type=""bibr"" target=""#b29"">[30]</ref> employs two discriminators to directly measure MI between summary, we provide a brief comparison between the  <ref type=""bibr"" target=""#b43"">[44]</ref>, GMI <ref type=""bibr"" target=""#b29"">[30]</ref>, and MVGRL <ref type=""bibr"" target=""#b14"">[15]</ref> in Ta ax (DGI) <ref type=""bibr"" target=""#b43"">[44]</ref>, Graphical Mutual Information Maximization (GMI) <ref type=""bibr"" target=""#b29"">[30]</ref>, and Multi-View Graph Representation Learning (MVGRL) <ref",1
"uctural augmentation schemes, we calculate edge centrality scores of the famous Karate club dataset <ref type=""bibr"" target=""#b49"">[50]</ref>, containing two groups of students leading by two coaches",0
"g GNN models are mostly established in a supervised manner <ref type=""bibr"" target=""#b18"">[19,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b42"">43]</ref>, which require abun literature has grown up around the theme of supervised GNN <ref type=""bibr"" target=""#b18"">[19,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b42"">43,</ref><ref type=""bibr"" tar rts, we also report the performance of two representative models Graph Convolutional Networks (GCN) <ref type=""bibr"" target=""#b21"">[22]</ref> and Graph Attention Networks (GAT) <ref type=""bibr"" target ://www.tei-c.org/ns/1.0""><head n=""4.1.4"">Implementation details.</head><p>We employ a two-layer GCN <ref type=""bibr"" target=""#b21"">[22]</ref> as the encoder for all deep learning baselines due to its ref type=""formula"" target=""#formula_22"">21</ref>), we finally arrive at inequality J ≤ 𝐼 (X; U, V), <ref type=""bibr"" target=""#b21"">(22)</ref> which concludes the proof. □ </p></div><figure xmlns=""http",0
"f development, unsupervised GNNs receive little attention. Representative methods include GraphSAGE <ref type=""bibr"" target=""#b13"">[14]</ref>, which incorporates DeepWalk-like objectives. Recent work",0
"br"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b38"">39]</ref> and natural language processing <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target",0
"hs, data augmentation schemes, proved to be a critical component for visual representation learning <ref type=""bibr"" target=""#b45"">[46]</ref>, remain rarely explored in existing literature. Unlike abu",0
"""#b19"">[20]</ref> on all datasets. The ℓ 2 weight decay factor is set to 10 −5 and the dropout rate <ref type=""bibr"" target=""#b37"">[38]</ref> is set to zero on all datasets. The probability parameters",0
"supervised manner <ref type=""bibr"" target=""#b18"">[19,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b42"">43]</ref>, which require abundant labeled nodes for training. Recentl of supervised GNN <ref type=""bibr"" target=""#b18"">[19,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b42"">43,</ref><ref type=""bibr"" target=""#b44"">45]</ref>, which requires lab utional Networks (GCN) <ref type=""bibr"" target=""#b21"">[22]</ref> and Graph Attention Networks (GAT) <ref type=""bibr"" target=""#b42"">[43]</ref>, where they are trained in an endto-end fashion. For all b",0
"et=""#b18"">[19,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b42"">43,</ref><ref type=""bibr"" target=""#b44"">45]</ref>, which requires labeled datasets that may not be accessible",0
"ion.</p><p>Accurately predicting (black-box) workload criticality is itself a challenge. Prior work <ref type=""bibr"" target=""#b5"">[6]</ref> associated a diurnal utilization pattern with user interacti system</head><p>To integrate predictions into VM scheduling in practice, we target Resource Central <ref type=""bibr"" target=""#b5"">[6]</ref>, the existing ML and predictionserving system in Azure. The e workload of each VM is performance-critical or not, before we can train a model. As in prior work <ref type=""bibr"" target=""#b5"">[6]</ref>, we consider a workload critical if it is user-facing, i.e. he problem reduces to identifying VMs whose time series of CPU utilizations exhibit 24-hour periods <ref type=""bibr"" target=""#b5"">[6]</ref>.</p><p>Obviously, some background VMs may exhibit 24-hour pe identifying periods in time series, such as FFT or the autocorrelation function (ACF). For example, <ref type=""bibr"" target=""#b5"">[6]</ref> assumes a workload is user-facing if the FFT indicates a 24- /task length for provisioning or scheduling purposes, e.g. <ref type=""bibr"" target=""#b4"">[5]</ref>, <ref type=""bibr"" target=""#b5"">[6]</ref>, <ref type=""bibr"" target=""#b9"">[10]</ref>, <ref type=""bibr""",1
""">[15]</ref>, <ref type=""bibr"" target=""#b19"">[20]</ref>, <ref type=""bibr"" target=""#b21"">[22]</ref>- <ref type=""bibr"" target=""#b24"">[25]</ref>, <ref type=""bibr"" target=""#b26"">[27]</ref>, <ref type=""bib",0
"orrelated peaks <ref type=""bibr"" target=""#b8"">[9]</ref>, <ref type=""bibr"" target=""#b10"">[11]</ref>, <ref type=""bibr"" target=""#b13"">[14]</ref>, <ref type=""bibr"" target=""#b30"">[31]</ref>. Our work exten ave studied hierarchical capping in production datacenters <ref type=""bibr"" target=""#b8"">[9]</ref>, <ref type=""bibr"" target=""#b13"">[14]</ref>, <ref type=""bibr"" target=""#b31"">[32]</ref>. Our paper focu",0
"d on selecting the DVFS setting required to meet a tight power budget as applications execute, e.g. <ref type=""bibr"" target=""#b12"">[13]</ref>, <ref type=""bibr"" target=""#b14"">[15]</ref>, <ref type=""bib",0
"nergy usage, e.g. <ref type=""bibr"" target=""#b2"">[3]</ref>, <ref type=""bibr"" target=""#b3"">[4]</ref>, <ref type=""bibr"" target=""#b7"">[8]</ref>, <ref type=""bibr"" target=""#b25"">[26]</ref>, <ref type=""bibr""",0
"e challenging. A number of GPM frameworks have been proposed to reduce the burden on the programmer <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b28"">29,</ref><ref type=""bibr"" ta lgorithms. Low-level systems such as RStream <ref type=""bibr"" target=""#b55"">[56]</ref> and Pangolin <ref type=""bibr"" target=""#b11"">[12]</ref> provide low-level API functions for the user to control th form the state-of-the-art GPM systems, AutoMine <ref type=""bibr"" target=""#b38"">[39]</ref>, Pangolin <ref type=""bibr"" target=""#b11"">[12]</ref>, and Peregrine <ref type=""bibr"" target=""#b28"">[29]</ref> b e an example in Appendix B.6). This technique is called Memoization of Embedding Connectivity (MEC) <ref type=""bibr"" target=""#b11"">[12]</ref>. • For edge-induced extension, a set of edges instead of v tern Classification (CP): FP and CP are low-level optimizations enabled in a prior system, Pangolin <ref type=""bibr"" target=""#b11"">[12]</ref>, so we describe them in Appendices B.4 and B.5. To support type=""foot"" target=""#foot_0"">3</ref> : AutoMine <ref type=""bibr"" target=""#b38"">[39]</ref>, Pangolin <ref type=""bibr"" target=""#b11"">[12]</ref>, and Peregrine <ref type=""bibr"" target=""#b28"">[29]</ref>. ""#b10"">[11]</ref> is a distributed GPM system which incorporates task-parallel processing. Pangolin <ref type=""bibr"" target=""#b11"">[12]</ref> is a shared-memory GPM system targeting both CPU and GPU. ferent patterns. For small implicit patterns, Sandslash uses customized pattern classification (CP) <ref type=""bibr"" target=""#b11"">[12]</ref>. For example, in FSM, the labeled wedge patterns can be di",1
"4,</ref><ref type=""bibr"" target=""#b50"">51]</ref>, and GPUs <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" target=""#b42"">43]</ref>. kClist <ref type=""",0
"rget=""#b2"">[3,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b44"">45]</ref> can use algorithmic insight to aggressively prune the searc "">[3]</ref> counts 3 and 4-motifs by leveraging proven formulas to reduce enumeration space. Escape <ref type=""bibr"" target=""#b44"">[45]</ref> extends this approach to 5-motifs. Subgraph listing <ref t",0
"tention to, attention networks achieve a better performance with fewer layers. As an example, SENet <ref type=""bibr"" target=""#b5"">[5]</ref> introduces Squeeze-and-Excitation (SE) blocks to study the c rs in the spatial pyramid structure are not learnable, which is nearly cost-free. Compared to SENet <ref type=""bibr"" target=""#b5"">[5]</ref>, our structure only modifies the first fully-connected layer ter concatenation as input to the next block. More recently, attention based networks such as SENet <ref type=""bibr"" target=""#b5"">[5]</ref> and CBAM <ref type=""bibr"">[6]</ref> provide an independent a s suppress insignificant features. Thus, visual features could be better captured and exploited. In <ref type=""bibr"" target=""#b5"">[5]</ref>, a Squeeze-and-Extraction block was proposed to learn the ch e last feature map which is small in size (7 ? 7 for example). However, attention based CNNs (e.g., <ref type=""bibr"" target=""#b5"">[5]</ref>, <ref type=""bibr"">[6]</ref>, <ref type=""bibr"" target=""#b7"">[ effectiveness of the attention mechanism. To address this problem, we leverage the excitation block <ref type=""bibr"" target=""#b5"">[5]</ref> to encode v and generate a 1D attention map ?. The excitatio re ? is a rectified linear unit (ReLU) function and sig denotes the sigmoid function. Like in SENet <ref type=""bibr"" target=""#b5"">[5]</ref>, we set r to 16.</p></div> <div xmlns=""http://www.tei-c.org/ ls and whistles, SPANet outperforms related stateof-art work <ref type=""bibr"" target=""#b2"">[2,</ref><ref type=""bibr"" target=""#b5"">5,</ref><ref type=""bibr"" target=""#b10"">10,</ref><ref type=""bibr"" targe tention map from a feature map and then apply the learned attention map to the original feature map <ref type=""bibr"" target=""#b5"">[5,</ref><ref type=""bibr"" target=""#b18"">18]</ref> . However, being con rg/ns/1.0""><head n=""3.3."">Spatial Pyramid Attention</head><p>Many existing attention based networks <ref type=""bibr"" target=""#b5"">[5,</ref><ref type=""bibr"">6,</ref><ref type=""bibr"">6,</ref><ref type=""",1
"ection in deep learning was first used in Highway Networks <ref type=""bibr"" target=""#b13"">[13,</ref><ref type=""bibr"" target=""#b14"">14]</ref>. By allowing an unimpeded information flowed across several",0
"ovides an end-to-end training paradigm for attention learning. Inspired by SENet, Competitive-SENet <ref type=""bibr"" target=""#b17"">[17]</ref> studies attention from both the residual path and the shor d networks <ref type=""bibr"" target=""#b5"">[5,</ref><ref type=""bibr"">6,</ref><ref type=""bibr"">6,</ref><ref type=""bibr"" target=""#b17"">17]</ref> aggregate input feature maps into a 1D vector using global",0
"s add more and more convolutional layers to the CNN architecture. For example, from 8-layer AlexNet <ref type=""bibr"" target=""#b1"">[1]</ref> to 1000-layer ResNet <ref type=""bibr"" target=""#b2"">[2,</ref>",0
"target=""#b2"">[2,</ref><ref type=""bibr"" target=""#b5"">5,</ref><ref type=""bibr"" target=""#b10"">10,</ref><ref type=""bibr"" target=""#b11"">11]</ref>. Experimental results show that structural information in t other base CNN architectures, including VGG <ref type=""bibr"" target=""#b25"">[25]</ref>, MobileNetV2 <ref type=""bibr"" target=""#b11"">[11]</ref>, DenseNet <ref type=""bibr"" target=""#b10"">[10]</ref>, and R ith SENet and the base networks. We employ four base networks, i.e., light-weight model MobileNetV2 <ref type=""bibr"" target=""#b11"">[11]</ref>, heavy-weight model DenseNet <ref type=""bibr"" target=""#b10",0
"research area, the studies using deep learning approaches mostly focus on dimensionality reduction <ref type=""bibr"" target=""#b1"">[2]</ref>- <ref type=""bibr"" target=""#b5"">[6]</ref> and anomaly-based i",1
"o pick out the best matching class and fuzzy logic for selecting the flow class label. Costa et al. <ref type=""bibr"" target=""#b27"">[28]</ref> proposed an intrusion detection method employing Optimum-p",0
"med values are 0.001 for the learning rate and 0.3 for momentum. The parameter of l2 regularization <ref type=""bibr"" target=""#b64"">[65]</ref> is set to the commonly used value of 0.001 and Xavier <ref oid activation function is used, except for the output layer employing soft-max activation function <ref type=""bibr"" target=""#b64"">[65]</ref>, with mean square error loss function. Both AE and VAE neu",0
"ng (SSL) paradigm is employed as learning strategy. SSL covers several different settings including <ref type=""bibr"" target=""#b55"">[56]</ref>: a) Semi-supervised classification: Its alternative name i",0
"located around hosts of the networks. The Ward Clustering approach was recommended by Satoh et al. <ref type=""bibr"" target=""#b31"">[32]</ref> to identify simple and malicious attacks from the SSH dict",0
"ters is performed by taking into consideration the recommendations proposed by Patterson and Gibson <ref type=""bibr"" target=""#b63"">[64]</ref>. After that, the hyperparameters of AE and VAE are mostly od, which is the default in the library and is recommended if ReLU is employed in the hidden layers <ref type=""bibr"" target=""#b63"">[64]</ref>. Both neural networks are trained with the backpropagation nt optimization algorithm, which is a recommended choice for large datasets and real-valued outputs <ref type=""bibr"" target=""#b63"">[64]</ref>. Nesterovs updater <ref type=""bibr"" target=""#b66"">[67]</re learning procedure to escape local minima and discover better solutions to the optimization process <ref type=""bibr"" target=""#b63"">[64]</ref>. The additional parameters used in VAE configuration are a s the ''dying ReLU'' problem of the vanilla ReLU and the vanishing gradient problem of sigmoid/tanh <ref type=""bibr"" target=""#b63"">[64]</ref>. Gaussian reconstruction distribution is used with hyperbo",0
"large datasets and real-valued outputs <ref type=""bibr"" target=""#b63"">[64]</ref>. Nesterovs updater <ref type=""bibr"" target=""#b66"">[67]</ref> is selected because it uses the momentum that supports the",0
"bounded between 0 and 1. If AUC value is less than 0.5, it means that the classifier is unrealistic <ref type=""bibr"" target=""#b53"">[54]</ref>. A rough classifying system can serve as a guidance to the",0
"tures, too, is used to evaluate the methods in the studies <ref type=""bibr"" target=""#b5"">[6]</ref>, <ref type=""bibr"" target=""#b11"">[12]</ref>. The NSL-KDD dataset <ref type=""bibr"" target=""#b12"">[13]</",0
"h Repositioning (EDITOR), which builds on recent progress on non-autoregressive sequence generation <ref type=""bibr"" target=""#b26"">(Lee et al., 2018;</ref><ref type=""bibr"" target=""#b20"">Ghazvininejad et al., 2018;</ref><ref type=""bibr"" target=""#b46"">Stern et al., 2018)</ref> or multi-pass decoding <ref type=""bibr"" target=""#b26"">(Lee et al., 2018;</ref><ref type=""bibr"" target=""#b20"">Ghazvininejad ""bibr"" target=""#b22"">(Gu et al., 2019)</ref>, or 2) the maximum number of decoding steps is reached <ref type=""bibr"" target=""#b26"">(Lee et al., 2018;</ref><ref type=""bibr"" target=""#b20"">Ghazvininejad s widely used in nonautoregressive generation <ref type=""bibr"" target=""#b21"">(Gu et al., 2018;</ref><ref type=""bibr"" target=""#b26"">Lee et al., 2018;</ref><ref type=""bibr"" target=""#b22"">Gu et al., 2019",1
"slation quality on par or better than both EDITOR and Levenshtein Transformer with hard constraints <ref type=""bibr"" target=""#b47"">(Susanto et al., 2020)</ref>.</p></div> <div xmlns=""http://www.tei-c. r"" target=""#b16"">(Dinu et al., 2019;</ref><ref type=""bibr"" target=""#b36"">Post and Vilar, 2018;</ref><ref type=""bibr"" target=""#b47"">Susanto et al., 2020)</ref>. Compared to <ref type=""bibr"" target=""#b3 constraints (Table <ref type=""table"" target=""#tab_6"">6</ref>). Consistent with previous findings by <ref type=""bibr"" target=""#b47"">Susanto et al. (2020)</ref>, incorporating soft constraints in LevT i s in LevT improves BLEU by +0.3 on Wiktionary and by +0.4 on IATE. Enforcing hard constraints as in <ref type=""bibr"" target=""#b47"">Susanto et al. (2020)</ref> increases the term usage by +8-10% and im they help close the small gap to reach 100% term usage and do not 14 We use our implementations of <ref type=""bibr"" target=""#b47"">Susanto et al. (2020)</ref>'s technique for a more controlled compari b47"">Susanto et al. (2020)</ref>'s technique for a more controlled comparison. The LevT baseline in <ref type=""bibr"" target=""#b47"">Susanto et al. (2020)</ref> achieves higher BLEU than ours on the sma",1
"ss on non-autoregressive sequence generation <ref type=""bibr"" target=""#b26"">(Lee et al., 2018;</ref><ref type=""bibr"" target=""#b20"">Ghazvininejad et al., 2019)</ref>. 1  Specifically, the Levenshtein T maximum number of decoding steps is reached <ref type=""bibr"" target=""#b26"">(Lee et al., 2018;</ref><ref type=""bibr"" target=""#b20"">Ghazvininejad et al., 2019)</ref>.<ref type=""foot"" target=""#foot_3"">5 n et al., 2018)</ref> or multi-pass decoding <ref type=""bibr"" target=""#b26"">(Lee et al., 2018;</ref><ref type=""bibr"" target=""#b20"">Ghazvininejad et al., 2019;</ref><ref type=""bibr"" target=""#b22"">Gu et s by iteratively editing the outputs from previous iterations. Edit operations such as substitution <ref type=""bibr"" target=""#b20"">(Ghazvininejad et al., 2019)</ref> and insertion-deletion <ref type=""",1
"l machine translation (MT) architectures <ref type=""bibr"" target=""#b3"">(Bahdanau et al., 2015;</ref><ref type=""bibr"" target=""#b49"">Vaswani et al., 2017)</ref> make it difficult for users to specify pr . By contrast, autoregressive NMT models <ref type=""bibr"" target=""#b3"">(Bahdanau et al., 2015;</ref><ref type=""bibr"" target=""#b49"">Vaswani et al., 2017)</ref> do not explicitly separate lexical choice y distribution P (A) over the action space A. Our model is based on the Transformer encoder-decoder <ref type=""bibr"" target=""#b49"">(Vaswani et al., 2017)</ref> and we extract the decoder representa- F",0
"standard for many sequence generation tasks <ref type=""bibr"" target=""#b12"">(Cho et al., 2014;</ref><ref type=""bibr"" target=""#b13"">Chorowski et al., 2015;</ref><ref type=""bibr"" target=""#b50"">Vinyals a",0
"ndependence assumptions between target tokens <ref type=""bibr"" target=""#b28"">(Ma et al., 2019;</ref><ref type=""bibr"" target=""#b52"">Wang et al., 2019)</ref>. These issues have been addressed via partia",0
"w.tei-c.org/ns/1.0""><head n=""1"">Introduction</head><p>Neural machine translation (MT) architectures <ref type=""bibr"" target=""#b3"">(Bahdanau et al., 2015;</ref><ref type=""bibr"" target=""#b49"">Vaswani et tion and reordering operations over bilingual minimal units. By contrast, autoregressive NMT models <ref type=""bibr"" target=""#b3"">(Bahdanau et al., 2015;</ref><ref type=""bibr"" target=""#b49"">Vaswani et",0
"br"">(Koehn et al., 2007)</ref> and have been shown to be useful for interactive machine translation <ref type=""bibr"" target=""#b19"">(Foster et al., 2002;</ref><ref type=""bibr"" target=""#b5"">Barrachina e",0
"t=""#b52"">Wang et al., 2019)</ref>. These issues have been addressed via partially parallel decoding <ref type=""bibr"" target=""#b51"">(Wang et al., 2018;</ref><ref type=""bibr"" target=""#b46"">Stern et al.,",0
"s offer a promising alternative to speed up decoding by generating a sequence of tokens in parallel <ref type=""bibr"" target=""#b21"">(Gu et al., 2018;</ref><ref type=""bibr"" target=""#b33"">van den Oord et edge distillation from autoregressive teacher models as widely used in nonautoregressive generation <ref type=""bibr"" target=""#b21"">(Gu et al., 2018;</ref><ref type=""bibr"" target=""#b26"">Lee et al., 201",0
"or via segmentlevel ""side-constraints"" <ref type=""bibr"" target=""#b41"">(Sennrich et al., 2016a;</ref><ref type=""bibr"" target=""#b18"">Ficler and Goldberg, 2017;</ref><ref type=""bibr"" target=""#b40"">Scarto",0
")</ref>, or the Operation Sequence Model <ref type=""bibr"" target=""#b17"">(Durrani et al., 2015;</ref><ref type=""bibr"" target=""#b44"">Stahlberg et al., 2018)</ref>, which views translation as a sequence",0
""" target=""#b6"">(Bojar et al., 2014)</ref>, and English-Japanese (En-Ja) from WAT2017 Small-NMT Task <ref type=""bibr"" target=""#b30"">(Nakazawa et al., 2017)</ref>. We also evaluate EDITOR on the two En-",0
"earning has been closing the gap with, and in some cases even surpassing its supervised counterpart <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" targ air), creating contradicting objectives.</p><p>While recent efforts focus on improved architectures <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" targe ef type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b21"">22]</ref> and data augmentation <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b42"">43]</ref>, relatively little w get=""#b41"">42,</ref><ref type=""bibr"" target=""#b33"">34,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b8"">9]</ref>.</p><p>In contrastive learning, the embedding space is govern different images, regardless of their semantic information <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b33"">34]</ref>. Figure <ref type=""fi s this problem by maintaining a momentum encoder and a limited queue of previous samples. SimCLR v1 <ref type=""bibr"" target=""#b8"">[9]</ref> eschews a momentum encoder in favor of a large batch size, a ""bibr"" target=""#b33"">[34]</ref> 63.6 -PCL <ref type=""bibr"" target=""#b31"">[32]</ref> 65.9 -SimCLR v1 <ref type=""bibr"" target=""#b8"">[9]</ref> 69.3 89.0 MoCo v2 <ref type=""bibr"" target=""#b10"">[11]</ref>",1
"arget=""#b44"">45,</ref><ref type=""bibr"" target=""#b37"">38,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b49"">50,</ref><ref type=""bibr"" target=""#b29"">30]</ref>. In fact, selfsuper rget=""#b7"">8]</ref> or generating one view of an image from another, e.g., split-brain auto-encoder <ref type=""bibr"" target=""#b49"">[50]</ref> or colorization <ref type=""bibr"" target=""#b30"">[31,</ref><",0
"and in some cases even surpassing its supervised counterpart <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" tar get=""#b23"">24,</ref><ref type=""bibr"" target=""#b41"">42,</ref><ref type=""bibr"" target=""#b33"">34,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b8"">9]</ref>.</p><p>In contrastive pairs are formed by sampling views from different images, regardless of their semantic information <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" targ mproved architectures <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b21"">22]</ref> and data augmentation <ref type=""bibr"" target=""#b8"">[9,</re learning-based approaches across a wide range of settings, e.g., with or without momentum contrast <ref type=""bibr"" target=""#b21"">[22]</ref>. Further, we show that our methods are effectively complem maintains a memory bank of previous representation of all images, which limits scalability. MoCo v1 <ref type=""bibr"" target=""#b21"">[22]</ref> addresses this problem by maintaining a momentum encoder a der. Here, we investigate the behaviors of the proposed method in these settings. Momentum contrast <ref type=""bibr"" target=""#b21"">[22]</ref> employs two encoders, the main encoder and momentum encode s=""http://www.tei-c.org/ns/1.0""><head>Representation Learning Contrastive learning</head><p>MoCo v1 <ref type=""bibr"" target=""#b21"">[22]</ref> 60.6 -PIRL <ref type=""bibr"" target=""#b33"">[34]</ref> 63.6 mbeddings, we finetune the model on PASCAL VOC object detection. We use similar settings as in MoCo <ref type=""bibr"" target=""#b21"">[22]</ref>, where we finetune on the VOC trainval07+12 set using Fast",0
"br"" target=""#b42"">[43]</ref>, we use random crops, color distortion, Gaussian blur, and RandAugment <ref type=""bibr"" target=""#b13"">[14]</ref> for data augmentation. We also adopt multi- </p></div> <di",0
"n downstream tasks <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" target=""#b48"">49,</ref><ref type=""bibr"" target=""#b26"">27]</ref>. While conventional",0
"ion probabilistic modeling (DDPM) <ref type=""bibr"" target=""#b39"">(Sohl-Dickstein et al., 2015;</ref><ref type=""bibr"" target=""#b17"">Ho et al., 2020)</ref> trains a sequence of probabilistic models to r at generation of images <ref type=""bibr"">(Song &amp; Ermon, 2019;</ref><ref type=""bibr"">2020;</ref><ref type=""bibr"" target=""#b17"">Ho et al., 2020)</ref>, audio <ref type=""bibr"" target=""#b6"">(Chen et s of SMLD and DDPM can be unified into our framework as discretizations of different SDEs. Although <ref type=""bibr"" target=""#b17"">Ho et al. (2020)</ref> has reported higher sample quality than <ref t Eq. ( <ref type=""formula"" target=""#formula_3"">3</ref>) described here is equivalent to L simple in <ref type=""bibr"" target=""#b17"">Ho et al. (2020)</ref>, but we re-write it in a slightly different fo rget=""#b16"">(Ho et al., 2019)</ref> or discrete data). Main results: (i) For the same DDPM model in <ref type=""bibr"" target=""#b17"">Ho et al. (2020)</ref>, we obtain better bits/dim compared to the upp e=""bibr"" target=""#b17"">(Ho et al., 2020)</ref>. With PC samplers and the same model architecture in <ref type=""bibr"" target=""#b17"">Ho et al. (2020)</ref>, the gaps can be reduced, but score-based mode data pxq. In our experiments, we let βmin "" 0.1 and βmax "" 20, which correspond to the settings in <ref type=""bibr"" target=""#b17"">Ho et al. (2020)</ref>. The perturbation kernel is given by</p><formu arget=""#fig_4"">4</ref>, we use a DDPM model trained on 256 ˆ256 CelebA-HQ with the same settings in <ref type=""bibr"" target=""#b17"">Ho et al. (2020)</ref>. We use the RK45 ODE solver <ref type=""bibr"" t on and temperature scaling. The model tested here is a DDPM model trained with the same settings in <ref type=""bibr"" target=""#b17"">Ho et al. (2020)</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns tially a different discretization to the same reverse-time SDE. This unifies the sampling method in <ref type=""bibr"" target=""#b17"">Ho et al. (2020)</ref> as a numerical solver to the reverse-time VP S ´σ2 i´1 qIq, i "" 1, 2, ¨¨¨, N.</formula><p>Here we assume σ 0 "" 0 to simplify notations. Following <ref type=""bibr"" target=""#b17"">Ho et al. (2020)</ref>, we can compute</p><formula xml:id=""formula_55 "" x i `pσ 2 i ´σ2 i´1 qs θ px i , iq,</formula><p>where s θ px i , iq is to estimate z{σ i . As in <ref type=""bibr"" target=""#b17"">Ho et al. (2020)</ref>, we let τ i ""</p><formula xml:id=""formula_58""> DDITIONAL DETAILS ON PREDICTOR-CORRECTOR SAMPLERS</head><p>Training We use the same architecture in <ref type=""bibr"" target=""#b17"">Ho et al. (2020)</ref> for our score-based models. For the VE SDE, we noise scales at test time. The specific architecture of the noise-conditional score-based model in <ref type=""bibr"" target=""#b17"">Ho et al. (2020)</ref> uses sinusoidal positional embeddings for cond amples with TF-GAN. For sampling, we use the PC sampler discretized at 1000 noise scales. We follow <ref type=""bibr"" target=""#b17"">Ho et al. (2020)</ref> for optimization, including the learning rate, mula"" target=""#formula_0"">1</ref>) and use a batch size of 128. Our architecture is mostly based on <ref type=""bibr"" target=""#b17"">Ho et al. (2020)</ref>. We additionally search over the following com pe=""bibr"" target=""#b20"">Karras et al. (2018)</ref>; <ref type=""bibr"">Song &amp; Ermon (2019)</ref>; <ref type=""bibr"" target=""#b17"">Ho et al. (2020)</ref>, the FID value here is the lowest over the cou odel upon conditioning on continuous time variables, we change positional embeddings, the layers in <ref type=""bibr"" target=""#b17"">Ho et al. (2020)</ref> for conditioning on discrete time steps, to ra al., 2018)</ref> 3.40 -Flow++ <ref type=""bibr"" target=""#b16"">(Ho et al., 2019)</ref> 3.29 -DDPM (L) <ref type=""bibr"" target=""#b17"">(Ho et al., 2020)</ref> ď 3.70 13.51 DDPM (Lsimple) <ref type=""bibr"" > 3.29 -DDPM (L) <ref type=""bibr"" target=""#b17"">(Ho et al., 2020)</ref> ď 3.70 13.51 DDPM (Lsimple) <ref type=""bibr"" target=""#b17"">(Ho et al., 2020)</ref> ď 3.75 3.17   <ref type=""bibr"">(Song &amp; Er )</ref> 25.32 8.87 ˘.12 NCSNv2 <ref type=""bibr"">(Song &amp; Ermon, 2020)</ref> 10.87 8.40 ˘.07 DDPM <ref type=""bibr"" target=""#b17"">(Ho et al., 2020)</ref> 3.17 9.46 ˘.11 Exact likelihood computation L s on CIFAR-10 is 10.23 <ref type=""bibr"">(Song &amp; Ermon, 2020)</ref>, whereas for DDPM it is 3.17 <ref type=""bibr"" target=""#b17"">(Ho et al., 2020)</ref>. With PC samplers and the same model architec ""#formula_52"">41</ref>)) reverse diffusion samplers.</p><p>Note that the ancestral sampling of DDPM <ref type=""bibr"" target=""#b17"">(Ho et al., 2020)</ref> (Eq. ( <ref type=""formula"" target=""#formula_4",1
"orks, including ProgressiveGAN <ref type=""bibr"" target=""#b20"">(Karras et al., 2018)</ref>, StyleGAN <ref type=""bibr"" target=""#b21"">(Karras et al., 2019)</ref> and <ref type=""bibr"">StyleGAN-2 (Karras e models like Progressive-GAN <ref type=""bibr"" target=""#b20"">(Karras et al., 2018)</ref> and StyleGAN <ref type=""bibr"" target=""#b21"">(Karras et al., 2019)</ref>. However, we found it harmful at an early",0
"generative models, and related techniques <ref type=""bibr"" target=""#b3"">(Bordes et al., 2017;</ref><ref type=""bibr"" target=""#b13"">Goyal et al., 2017)</ref>, have proven effective at generation of ima",0
"al., 2019b)</ref> 3.32 -Residual Flow <ref type=""bibr"">(Chen et al., 2019)</ref> 3.28 46.37 FFJORD <ref type=""bibr"" target=""#b14"">(Grathwohl et al., 2018)</ref> 3.40 -Flow++ <ref type=""bibr"" target="" a"" target=""#formula_39"">33</ref>). In many cases computing ∇ ¨f θ px, tq is expensive, so we follow <ref type=""bibr"" target=""#b14"">Grathwohl et al. (2018)</ref> to estimate it with the Skilling-Hutchi _ivp in all cases. The bits/dim values in Tab. 2 are computed with atol=1e-5 and rtol=1e-5, same as <ref type=""bibr"" target=""#b14"">Grathwohl et al. (2018)</ref>. To give the results for DDPM (probabil",0
"tional Score Network (NCSN), denoted by s θ px, σq, with a weighted sum of denoising score matching <ref type=""bibr"" target=""#b45"">(Vincent, 2011)</ref> objectives:</p><formula xml:id=""formula_0"">θ ˚""",0
"training a score-based model on samples with score matching <ref type=""bibr"">(Hyvärinen, 2005;</ref><ref type=""bibr"" target=""#b42"">Song et al., 2019a</ref> </p></div> <div xmlns=""http://www.tei-c.org/ ) uses denoising score matching, but other score matching objectives, such as sliced score matching <ref type=""bibr"" target=""#b42"">(Song et al., 2019a)</ref> and finite-difference score matching <ref not require computing ∇ xptq log p 0t pxptq | xp0qq. For example, when using sliced score matching <ref type=""bibr"" target=""#b42"">(Song et al., 2019a)</ref>, our training objective Eq. ( <ref type=""f",0
"ref> for conditioning on discrete time steps, to random Fourier feature embeddings, as advocated in <ref type=""bibr"" target=""#b44"">Tancik et al. (2020)</ref>. The scale parameter of these random Fouri AR-10 with VE perturbations. The classifier is conditioned on log σ i using random Fourier features <ref type=""bibr"" target=""#b44"">(Tancik et al., 2020)</ref>, and the training objective is a simple s",0
"of computing paths and cycles with larger networks, IUAD adopts Weisfeiler-Lehman sub-graph kernel <ref type=""bibr"" target=""#b38"">[39]</ref> to evaluate the similarity between two vertices. WL-kernel a j , v a j ) .<label>(4)</label></formula><p>Due to page limitation, more details please refer to <ref type=""bibr"" target=""#b38"">[39]</ref>, <ref type=""bibr"" target=""#b39"">[40]</ref>.</p><p>For the",1
"ltiple authors.</p><p>Author disambiguation is related to several similar tasks like record linkage <ref type=""bibr"" target=""#b0"">[1]</ref>- <ref type=""bibr"" target=""#b2"">[3]</ref>, entity resolution ""http://www.tei-c.org/ns/1.0""><head>II. RELATED WORK</head><p>Our work is related to record linkage <ref type=""bibr"" target=""#b0"">[1]</ref>- <ref type=""bibr"" target=""#b2"">[3]</ref>, entity resolution serve a performance reduction for incremental author disambiguation. This is due to the facts that: <ref type=""bibr"" target=""#b0"">(1)</ref> single paper only provides limited information to identify t",0
"<ref type=""bibr"" target=""#b2"">[3]</ref>, entity resolution <ref type=""bibr"" target=""#b3"">[4]</ref>- <ref type=""bibr"" target=""#b6"">[7]</ref>, object identification <ref type=""bibr"" target=""#b7"">[8]</re <ref type=""bibr"" target=""#b2"">[3]</ref>, entity resolution <ref type=""bibr"" target=""#b3"">[4]</ref>- <ref type=""bibr"" target=""#b6"">[7]</ref>, object identification <ref type=""bibr"" target=""#b7"">[8]</re",0
"iangles, which a vertex participates in, also follows the power-law pattern in a scale-free network <ref type=""bibr"" target=""#b35"">[36]</ref>. Thus, if (a, b), (a, c) and (b, c) are ?-SCRs, then (a, b",0
"""bibr"" target=""#b8"">(Gilmer et al., 2017;</ref><ref type=""bibr"" target=""#b22"">Wu et al., 2018;</ref><ref type=""bibr"" target=""#b6"">Fey et al., 2020)</ref>. Graph neural networks have been proven to be ibr"" target=""#b0"">Abu-El-Haija et al., 2019;</ref><ref type=""bibr"" target=""#b16"">Loukas, 2019;</ref><ref type=""bibr"" target=""#b6"">Fey et al., 2020)</ref>. In the context of molecular property predicti olution on both the input graph and a coarser version of it from which all cycles have been removed <ref type=""bibr"" target=""#b6"">(Fey et al., 2020)</ref>. Despite interesting results, this approach g >Fey et al., 2020)</ref>. In the context of molecular property prediction, we highlight the work of <ref type=""bibr"" target=""#b6"">Fey et al. (2020)</ref> who proposed to perform message passing betwee",1
"t very simple tasks, like detecting small cycles <ref type=""bibr"" target=""#b16"">(Loukas, 2019;</ref><ref type=""bibr"" target=""#b3"">Chen et al., 2020)</ref>. This hints at the fact that current networks re related tasks, such as detecting small cycles <ref type=""bibr"" target=""#b16"">(Loukas, 2019;</ref><ref type=""bibr"" target=""#b3"">Chen et al., 2020)</ref>. This seems like a serious problem, as bioche =""bibr"">2018)</ref> or relational pooling <ref type=""bibr"" target=""#b20"">(Murphy et al., 2019;</ref><ref type=""bibr"" target=""#b3"">Chen et al., 2020)</ref>. Despite being provably more powerful, the am graphs such as found in organic chemistry. This problem has been investigated by many recent works <ref type=""bibr"" target=""#b3"">(Chen et al., 2020;</ref><ref type=""bibr"" target=""#b21"">Nikolentzos et",1
"that modern graph neural networks can still fail at very simple tasks, like detecting small cycles <ref type=""bibr"" target=""#b16"">(Loukas, 2019;</ref><ref type=""bibr"" target=""#b3"">Chen et al., 2020)< etworks can have trouble solving even basic structure related tasks, such as detecting small cycles <ref type=""bibr"" target=""#b16"">(Loukas, 2019;</ref><ref type=""bibr"" target=""#b3"">Chen et al., 2020)< irtual node, convolution based networks are at most as discriminative as the Weisfeiler-Lehman test <ref type=""bibr"" target=""#b16"">(Loukas, 2019)</ref>. More practically, this means that the informati ""#b21"">Nikolentzos et al., 2020;</ref><ref type=""bibr"" target=""#b0"">Abu-El-Haija et al., 2019;</ref><ref type=""bibr"" target=""#b16"">Loukas, 2019;</ref><ref type=""bibr"" target=""#b6"">Fey et al., 2020)</r",1
"ion can be seen as a computationally simple first-order approximation of spectral graph convolution <ref type=""bibr"" target=""#b1"">(Bruna et al., 2014;</ref><ref type=""bibr"" target=""#b4"">Defferrard et",0
"a prediction could be reduced by many orders of magnitude while increasing the predictive accuracy <ref type=""bibr"" target=""#b8"">(Gilmer et al., 2017;</ref><ref type=""bibr"" target=""#b22"">Wu et al., 2 is an idea that emerged a few years ago <ref type=""bibr"" target=""#b5"">(Duvenaud et al., 2015;</ref><ref type=""bibr"" target=""#b8"">Gilmer et al., 2017;</ref><ref type=""bibr"" target=""#b13"">Kong et al.,",0
"amentally diverging from the standard convolution based framework, such as invariant graph networks <ref type=""bibr"" target=""#b18"">(Maron et al., 2019;</ref><ref type=""bibr"">2018)</ref> or relational",0
"ness have been proposed. A first approach is to develop methods to train larger and deeper networks <ref type=""bibr"" target=""#b14"">(Li et al., 2020)</ref>, which can already lead to a considerable boo",0
"e=""bibr"" target=""#b18"">(Maron et al., 2019;</ref><ref type=""bibr"">2018)</ref> or relational pooling <ref type=""bibr"" target=""#b20"">(Murphy et al., 2019;</ref><ref type=""bibr"" target=""#b3"">Chen et al.,",0
""" target=""#b19"">(Morris et al., 2019)</ref>, or to increase the receptive field of each convolution <ref type=""bibr"" target=""#b7"">(Flam-Shepherd et al., 2020;</ref><ref type=""bibr"" target=""#b21"">Nikol",0
"ularisation or global pooling functions which may be more suited to small graphs have been proposed <ref type=""bibr"" target=""#b2"">(Cai et al., 2020;</ref><ref type=""bibr"" target=""#b13"">Kong et al., 20",0
"n the context of chemistry. We propose a very simple correction to the now standard GIN convolution <ref type=""bibr"" target=""#b24"">(Xu et al., 2019)</ref> that enables the network to detect small cycl <p>A second problem is that the most common framework, namely message passing neural network (MPNN) <ref type=""bibr"" target=""#b24"">(Xu et al., 2019)</ref>, is not an universal approximator of function that the operation is injective on multisets, or at least on a large finite ensemble of multisets. <ref type=""bibr"" target=""#b24"">(Xu et al., 2019)</ref> have proposed graph isomorphism network(GIN), ations encoding structure and node features, some drawbacks have been pointed by followup works. In <ref type=""bibr"" target=""#b24"">Xu et al. (2019)</ref> it is shown that GCNs are limited in its capab",0
"r graph containing no cycles. The authors rely on fixed rules similar to the junction tree approach <ref type=""bibr"" target=""#b11"">(Jin et al., 2018)</ref> to annotate graph cycles and represent them",0
""" target=""#b5"">(Duvenaud et al., 2015;</ref><ref type=""bibr"" target=""#b8"">Gilmer et al., 2017;</ref><ref type=""bibr"" target=""#b13"">Kong et al., 2020)</ref>. Many additions and modifications of the ori ore suited to small graphs have been proposed <ref type=""bibr"" target=""#b2"">(Cai et al., 2020;</ref><ref type=""bibr"" target=""#b13"">Kong et al., 2020), and</ref><ref type=""bibr"">Hu* et al. (2020)</ref>",0
"t generation with a single unified model <ref type=""bibr"" target=""#b36"">(Radford et al., 2019;</ref><ref type=""bibr"" target=""#b1"">Brown et al., 2020)</ref>. In the CTRLsum framework, prompts are a kin ref type=""bibr"" target=""#b36"">Radford et al., 2019;</ref><ref type=""bibr"">Keskar et al., 2019;</ref><ref type=""bibr"" target=""#b1"">Brown et al., 2020)</ref>.</p></div> <div xmlns=""http://www.tei-c.org/",1
"g and cannot generalize to unseen control aspects easily at test time.</p><p>We use pretrained BART <ref type=""bibr"" target=""#b24"">(Lewis et al., 2019)</ref> as the underlying architecture and perform ""bibr"" target=""#b43"">(Sharma et al., 2019)</ref>. For all datasets the source documents are trun-   <ref type=""bibr"" target=""#b24"">(Lewis et al., 2019)</ref>, which achieves state-of-the-art performan",1
"oding. They have been utilized to perform multi-purpose text generation with a single unified model <ref type=""bibr"" target=""#b36"">(Radford et al., 2019;</ref><ref type=""bibr"" target=""#b1"">Brown et al bility present in large pretrained models <ref type=""bibr"" target=""#b28"">(McCann et al., 2018;</ref><ref type=""bibr"" target=""#b36"">Radford et al., 2019;</ref><ref type=""bibr"">Keskar et al., 2019;</ref ng with the vanilla BART model, we also include the zero-shot performance from GPT2 language models <ref type=""bibr"" target=""#b36"">(Radford et al., 2019)</ref> (without fine-tuning) as a reference poi",1
"2"">Narayan et al., 2018)</ref>, and abstractive summarization that freely generates novel sentences <ref type=""bibr"" target=""#b39"">(Rush et al., 2015;</ref><ref type=""bibr"" target=""#b42"">See et al., 2",0
"et al., 2018)</ref>, and (2) transformer seq2seq with the same hyperparameters as the base model in <ref type=""bibr"" target=""#b46"">(Vaswani et al., 2017)</ref>. Note that the transformer model is trai",0
"rmation that is included in the summary <ref type=""bibr"" target=""#b0"">(Bornstein et al., 1999;</ref><ref type=""bibr"" target=""#b23"">Leuski et al., 2003)</ref>. More broadly, controllable text generatio",0
"><p>Uncontrolled Summarization. We follow <ref type=""bibr"" target=""#b11"">(Grusky et al., 2018;</ref><ref type=""bibr"" target=""#b5"">Fabbri et al., 2020)</ref> to ask human annotators from Amazon Mechani oor correlation with expert judgement <ref type=""bibr"" target=""#b10"">(Gillick &amp; Liu, 2010;</ref><ref type=""bibr"" target=""#b5"">Fabbri et al., 2020)</ref>.</p></div> <div xmlns=""http://www.tei-c.org",0
"he uncontrolled summarization performance. <ref type=""bibr"" target=""#b47"">Wang et al. (2016)</ref>, <ref type=""bibr"" target=""#b30"">Mou et al. (2016), and</ref><ref type=""bibr"">Yao et al. (2019)</ref>",0
"keywords with the highest selection probability computed by the sequence tagger. This is similar to <ref type=""bibr"" target=""#b40"">(Saito et al., 2020a)</ref>, which uses the number of ""guiding words"" eneralize for controlling aspects of the summarization that were not seen during training. Recently <ref type=""bibr"" target=""#b40"">Saito et al. (2020a)</ref> use the number of word prototypes to contr",0
"s and thus suffers from poor scores on both dimensions.</p><p>Uncontrolled Summarization. We follow <ref type=""bibr"" target=""#b11"">(Grusky et al., 2018;</ref><ref type=""bibr"" target=""#b5"">Fabbri et al",0
"4"">(Tang et al., 2019;</ref><ref type=""bibr"" target=""#b17"">Huang et al., 2019)</ref>, and templates <ref type=""bibr"" target=""#b12"">(Guu et al., 2018;</ref><ref type=""bibr"" target=""#b49"">Wiseman et al.",0
"(2018)</ref> utilize copying words at test time to mask copying operations in a summarization task. <ref type=""bibr"" target=""#b25"">Li et al. (2018)</ref> and <ref type=""bibr"" target=""#b41"">Saito et al",0
"plit graphs. So DMGI still puts more emphasis on learning the correlation of homogeneous nodes. GMI <ref type=""bibr"" target=""#b26"">[26]</ref> proposes a new approach  The yellow dotted lines (Eq.( <re",1
"for modeling bipartite graphs. As a result, they are suboptimal to learn bipartite graph embedding <ref type=""bibr"" target=""#b7"">[7,</ref><ref type=""bibr"" target=""#b8"">8]</ref>. To remedy such a prob e roughly divided into two branches: random walk-based and reconstruction-based methods. The former <ref type=""bibr"" target=""#b7"">[7,</ref><ref type=""bibr"" target=""#b8"">8,</ref><ref type=""bibr"" target ures with the assumption that nodes within the sliding window or neighborhoods are closely relevant <ref type=""bibr"" target=""#b7"">[7,</ref><ref type=""bibr"" target=""#b37"">37,</ref><ref type=""bibr"" targ <ref type=""bibr"" target=""#b44"">[44]</ref>, PinSage <ref type=""bibr"" target=""#b40"">[40]</ref>, BiNE <ref type=""bibr"" target=""#b7"">[7]</ref> and FOBE <ref type=""bibr"" target=""#b32"">[32]</ref> are speci iv xmlns=""http://www.tei-c.org/ns/1.0""><head n=""5.1.1"">Data Preprocessing.</head><p>As used in BiNE <ref type=""bibr"" target=""#b7"">[7]</ref>, we select 60% edges for training and remaining edges for te [36]</ref>. • Bipartite graph embedding: PinSage <ref type=""bibr"" target=""#b40"">[40]</ref> and BiNE <ref type=""bibr"" target=""#b7"">[7]</ref>.</p><p>PinSage integrates random walk into GNN architectures",1
"aggregate all homogeneous node information. Our insight is similar to the classic few-shot learning <ref type=""bibr"" target=""#b29"">[29]</ref> which would generate a prototype representation of each cl",0
"ype=""formula"" target=""#formula_17"">14</ref>) is similar to <ref type=""bibr"" target=""#b12"">[12,</ref><ref type=""bibr"" target=""#b21"">21]</ref>: 𝐸 ′ (𝑢,𝑣) is composed of real interactions with either the",0
"𝑣 𝑗 . In contrast with common graph convolutional operators <ref type=""bibr"" target=""#b9"">[9,</ref><ref type=""bibr"" target=""#b13"">13,</ref><ref type=""bibr"" target=""#b20"">20]</ref>, we only aggregate",0
"ster them via the well-known K-Means algorithm. The clustering metric Calinski-Harabasz Index (CHI) <ref type=""bibr"" target=""#b3"">[3]</ref> is used here. CHI measures the ratio between the withinclust",0
"f type=""bibr"" target=""#b33"">[33]</ref>, Node2vec <ref type=""bibr"" target=""#b11"">[11]</ref> and VGAE <ref type=""bibr"" target=""#b19"">[19]</ref>. Some representative heterogeneous graph methods are Metap f type=""bibr"" target=""#b33"">[33]</ref>, Node2vec <ref type=""bibr"" target=""#b11"">[11]</ref> and VGAE <ref type=""bibr"" target=""#b19"">[19]</ref>. DeepWalk and Node2vec are typically random-walk based. LI",0
"s. Afterwards, they learn node representations via predicting context nodes within a sliding window <ref type=""bibr"" target=""#b45"">[45]</ref>. The reconstruction-based works <ref type=""bibr"" target=""#",0
"get=""#b34"">34,</ref><ref type=""bibr"" target=""#b37"">37,</ref><ref type=""bibr"" target=""#b40"">40,</ref><ref type=""bibr"" target=""#b42"">42]</ref> are closely related with collaborative filtering <ref type= et=""#b34"">[34,</ref><ref type=""bibr"" target=""#b37"">37,</ref><ref type=""bibr"" target=""#b40"">40,</ref><ref type=""bibr"" target=""#b42"">42]</ref> train graph neural networks (GNNs) <ref type=""bibr"" target= ph structures in the latent space.</p><p>Matrix completion <ref type=""bibr"" target=""#b34"">[34,</ref><ref type=""bibr"" target=""#b42"">42]</ref> and collaborative filtering <ref type=""bibr"" target=""#b15""> esentation is a bipartite edge i.e., (𝑢, 𝑣), and we further arm it with an h-hop enclosing subgraph <ref type=""bibr"" target=""#b42"">[42]</ref> to describe the surrounding environment of (𝑢, 𝑣). The con training and remaining edges for test in both of DBLP and ML-10M. We use the same division in IGMC <ref type=""bibr"" target=""#b42"">[42]</ref> for ML-100K. Following experimental settings in the previo a unified framework. • Matrix completion: GC-MC <ref type=""bibr"" target=""#b34"">[34]</ref> and IGMC <ref type=""bibr"" target=""#b42"">[42]</ref>. GC-MC introduces a relation-aware graph auto-encoder to l ade-off between effectiveness and efficiency, we use 1-hop enclosing subgraphs as suggested by IGMC <ref type=""bibr"" target=""#b42"">[42]</ref>. The depth of our encoder 𝑘 (the number of stacked layers)",0
"ed methods. The former <ref type=""bibr"" target=""#b7"">[7,</ref><ref type=""bibr"" target=""#b8"">8,</ref><ref type=""bibr"" target=""#b44"">44]</ref> relies on designing the heuristics of random walks to gener graphs, and the structural characteristics of bipartite graph are hard to be preserved by them. IGE <ref type=""bibr"" target=""#b44"">[44]</ref>, PinSage <ref type=""bibr"" target=""#b40"">[40]</ref>, BiNE <",0
"r/item embeddings by mimicking the meta-learning setting via episode based training, as proposed in <ref type=""bibr"" target=""#b33"">[34]</ref>. Specifically, we pick the users/items with sufficient int rget=""#b8"">[9,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b33"">34]</ref>, which consists of metric-based recommendation <ref type=""b",1
"ce between the predicted target embedding h 𝐿 𝑢 and the ground-truth embedding h 𝑢 , as proposed by <ref type=""bibr"" target=""#b15"">[16]</ref>, due to its popularity as an indicator for the semantic si models and the GNN models are initialized by the NCF embedding results. We use Spearman correlation <ref type=""bibr"" target=""#b15"">[16]</ref> to measure the agreement between the ground truth embeddin",0
"based recommendation <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b23"">24]</ref>. However, few of th",0
"> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""1"">INTRODUCTION</head><p>Recommendation systems <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b20"">21]</ref> have been extensiv s matrix factorization <ref type=""bibr"" target=""#b20"">[21]</ref> and neural collaborative filtering <ref type=""bibr"" target=""#b13"">[14]</ref>, is to learn embeddings, i.e. the preferences for users an is learned upon the observed abundant interactions by NCF<ref type=""foot"" target=""#foot_0"">1</ref>  <ref type=""bibr"" target=""#b13"">[14]</ref>. To mimic the cold-start users/items, in each training epi split 𝐷 𝑇 into the training set 𝑇𝑟𝑎𝑖𝑛 𝑇 and the test set 𝑇 𝑒𝑠𝑡 𝑇 with a ratio of 7:3. We train NCF <ref type=""bibr"" target=""#b13"">[14]</ref> to get the ground-truth embeddings for the target users/it torization model, the general GNN models and the special GNN models for recommendation:</p><p>• NCF <ref type=""bibr"" target=""#b13"">[14]</ref>: is a neural matrix factorization model which combines Mul",0
"ss. Although some GNN models such as GrageSAGE <ref type=""bibr"" target=""#b10"">[11]</ref> or FastGCN <ref type=""bibr"" target=""#b3"">[4]</ref> filter neighbors before aggregating them, they usually follo <ref type=""bibr"" target=""#b18"">[19]</ref>, GAT <ref type=""bibr"" target=""#b30"">[31]</ref> or FastGCN <ref type=""bibr"" target=""#b3"">[4]</ref>, the proposed pre-training GNN model is model-agnostic. The a_163"">7</ref>) by maximizing the expected reward 𝜏 𝑃 (𝜏; Θ 𝑠 )𝑅(𝜏), where 𝜏 = {𝑠 1 1 , 𝑎 • FastGCN <ref type=""bibr"" target=""#b3"">[4]</ref>: is also a general GNN model which samples the neighbors by",0
"d architecture efficient, as well as maintain higher prediction capacity?</p><p>Vanilla Transformer <ref type=""bibr"" target=""#b29"">(Vaswani et al. 2017</ref>) has three significant limitations when so tei-c.org/ns/1.0""><head>Efficient Self-attention Mechanism</head><p>The canonical self-attention in <ref type=""bibr"" target=""#b29"">(Vaswani et al. 2017</ref>) is defined on receiving the tuple input ( ing long sequential outputs through one forward procedure</p><p>We use a standard decoder structure <ref type=""bibr"" target=""#b29"">(Vaswani et al. 2017)</ref> in Fig.</p><p>(2), and it is composed of",1
"m, let q i , k i , v i stand for the i-th row in Q, K, V respectively. Following the formulation in <ref type=""bibr"" target=""#b28"">(Tsai et al. 2019)</ref>, the i-th query's attention is defined as a",1
"<ref type=""bibr"" target=""#b3"">(Box et al. 2015;</ref><ref type=""bibr"" target=""#b23"">Ray 1990;</ref><ref type=""bibr"" target=""#b24"">Seeger et al. 2017;</ref><ref type=""bibr"" target=""#b25"">Seeger, Salin",0
"dewumi, and Ayo 2014), Prophet <ref type=""bibr"" target=""#b27"">(Taylor and Letham 2018)</ref>, LSTMa <ref type=""bibr"" target=""#b1"">(Bahdanau, Cho, and Bengio 2015)</ref> and LSTnet <ref type=""bibr"" tar",0
"d et al. 2019</ref><ref type=""bibr"">), LogSparse Transformer (Li et al. 2019)</ref>, and Longformer <ref type=""bibr"" target=""#b2"">(Beltagy, Peters, and Cohan 2020)</ref> all use a heuristic method to tion and forces each cell to attend to its previous one by an exponential step size. The Longformer <ref type=""bibr"" target=""#b2"">(Beltagy, Peters, and Cohan 2020)</ref> extend previous two works to m",0
"tputs. Some large-scale Transformer models pour resources and yield impressive results on NLP tasks <ref type=""bibr"" target=""#b4"">(Brown et al. 2020</ref>), but the training on dozens of GPUs and expe",0
"uts, in which the sparsity arises from the separated spatial correlation. The LogSparse Transformer <ref type=""bibr"" target=""#b15"">(Li et al. 2019</ref>) notices the cyclical pattern in self-attention _2"">3</ref> : It collects the electricity consumption (Kwh) of 321 clients. Due to the missing data <ref type=""bibr"" target=""#b15"">(Li et al. 2019)</ref>, we convert the dataset into hourly consumptio ""#b13"">(Kitaev, Kaiser, and Levskaya 2019)</ref> and the most related work LogSparse self-attention <ref type=""bibr"" target=""#b15"">(Li et al. 2019)</ref> in the experiments.</p><p>Hyper-parameter tuni",0
"uce the complexity of selfattention mechanism to O(L log L), where their efficiency gain is limited <ref type=""bibr"" target=""#b21"">(Qiu et al. 2019)</ref>. Reformer <ref type=""bibr"" target=""#b13"">(Kit",0
"e one.</p><p>Generative Inference Start token is an efficient technique in NLP's ""dynamic decoding"" <ref type=""bibr"" target=""#b9"">(Devlin et al. 2018)</ref>, and we extend it into a generative way. In",0
"ing new samples, etc. Two of the most popular methods are Generative Adversarial Networks (GANs) by <ref type=""bibr"" target=""#b2"">Goodfellow et al. (2014)</ref> and Variational 2.z combines with x for",1
"the target gene and is fed into an MLP model for predicting gene expression. Autoencoders (VAEs) by <ref type=""bibr"" target=""#b8"">Kingma and Welling (2013)</ref>. While GANs were shown to generate hig dient-based method. While the expectation of the likelihood term can be hard to derive in practice, <ref type=""bibr"" target=""#b8"">Kingma and Welling (2013)</ref> introduce a Stochastic Gradient Variat",1
"greatly benefit.There exist multiple dimension reduction techniques in literature, specifically by <ref type=""bibr"" target=""#b11"">Scholz et al. (2008)</ref>; <ref type=""bibr"" target=""#b7"">Kasun et al",0
"tone modifications and Gene expression. Following computational methods, most notably DeepChrome by <ref type=""bibr"" target=""#b12"">Singh et al. (2016)</ref> and AttentiveChrome by <ref type=""bibr"" tar Ho et al. (2015)</ref>, and Deep Learning by <ref type=""bibr"" target=""#b13"">Singh et al. (2017</ref><ref type=""bibr"" target=""#b12"">Singh et al. ( , 2016))</ref>. DeepChrome and AttentiveChrome are cel g/ns/1.0""><head n=""3.3.1"">Data Preparation and Dimension Reduction</head><p>Following the work from <ref type=""bibr"" target=""#b12"">Singh et al. (2016</ref><ref type=""bibr"" target=""#b13"">Singh et al. (",0
"al. (2010)</ref>, SVM by <ref type=""bibr"" target=""#b0"">Cheng et al. (2011)</ref>, Random Forests by <ref type=""bibr"" target=""#b1"">Dong et al. (2012)</ref>, Rule-Based Learning by <ref type=""bibr"" targ",0
">, Random Forests by <ref type=""bibr"" target=""#b1"">Dong et al. (2012)</ref>, Rule-Based Learning by <ref type=""bibr"" target=""#b5"">Ho et al. (2015)</ref>, and Deep Learning by <ref type=""bibr"" target=""",0
"ion task have applied a slew of different techniques. These techniques include Linear Regression by <ref type=""bibr"" target=""#b6"">Karlic et al. (2010)</ref>, SVM by <ref type=""bibr"" target=""#b0"">Cheng",0
"tably DeepChrome by <ref type=""bibr"" target=""#b12"">Singh et al. (2016)</ref> and AttentiveChrome by <ref type=""bibr"" target=""#b13"">Singh et al. (2017)</ref> that employ deep learning to learn complex , Rule-Based Learning by <ref type=""bibr"" target=""#b5"">Ho et al. (2015)</ref>, and Deep Learning by <ref type=""bibr"" target=""#b13"">Singh et al. (2017</ref><ref type=""bibr"" target=""#b12"">Singh et al. ( n Reduction</head><p>Following the work from <ref type=""bibr"" target=""#b12"">Singh et al. (2016</ref><ref type=""bibr"" target=""#b13"">Singh et al. ( , 2017))</ref>, we use five core Histone Modification",0
"g and thus may converge to a better model. Beside, multiple embedding modules were also proposed by <ref type=""bibr"" target=""#b4"">Guo et al. (2020b)</ref> to learn DNA representations which was shown",0
"lculation.</p><p>Input: PSL Rules R, Prediction ŷi , and Probability P(y|s i ), i = {1, 2, 3}; BERT <ref type=""bibr"" target=""#b17"">(Devlin et al. 2018)</ref>, to derive the sentence representation v i etails</head><p>In the framework of CTRL-PG, any contextualized word embedding method, such as BERT <ref type=""bibr"" target=""#b17"">(Devlin et al. 2018)</ref>, ELMo <ref type=""bibr"" target=""#b41"">(Pete nd RoBERTa <ref type=""bibr"" target=""#b37"">(Liu et al. 2019b)</ref>, can be utilized. We choose BERT <ref type=""bibr"" target=""#b17"">(Devlin et al. 2018)</ref> to derive contextualized sentence embeddin ch tokenized sequence and learns an embedding vector for it. We follow the experimental settings in <ref type=""bibr"" target=""#b17"">(Devlin et al. 2018)</ref> to use 12 Transformer layers and attention #b29"">(Kingma and Ba 2014)</ref> to optimize the parameters. We follow the experimental settings in <ref type=""bibr"" target=""#b17"">(Devlin et al. 2018)</ref> to set the dropout rate, and batch size as",1
"rics. For TB-Dense dataset, we compute the Precision, Recall, and Microaverage F1 scores. Following <ref type=""bibr"" target=""#b25"">(Han, Ning, and Peng 2019;</ref><ref type=""bibr"" target=""#b38"">Meng a type=""bibr"" target=""#b32"">(Leeuwenberg and Moens 2017;</ref><ref type=""bibr"">Han et al. 2019;</ref><ref type=""bibr"" target=""#b25"">Han, Ning, and Peng 2019;</ref><ref type=""bibr"" target=""#b40"">Ning, F",0
"from the datasets in the news domain <ref type=""bibr"" target=""#b43"">(Pustejovsky et al. 2003;</ref><ref type=""bibr"" target=""#b21"">Graff 2002)</ref>, the corpora in the clinical domain require rich do",0
"=""#b19"">(Galvan et al. 2018)</ref>. <ref type=""bibr"" target=""#b38"">Meng and Rumshisky (2018)</ref>; <ref type=""bibr"" target=""#b31"">Lee et al. (2016)</ref> apply machine learning models with lexical, s ype=""bibr"" target=""#b49"">Xu et al. 2013;</ref><ref type=""bibr"" target=""#b47"">Tang et al. 2013;</ref><ref type=""bibr"" target=""#b31"">Lee et al. 2016;</ref><ref type=""bibr"" target=""#b14"">Chikka 2016</ref",0
"on specialized cases such as on heterogeneous graphs, temporal networks, generative modeling, etc. <ref type=""bibr"" target=""#b35"">(Yun et al. 2019;</ref><ref type=""bibr"" target=""#b33"">Xu, Joshi, and",1
"et=""#b15"">(Li et al. 2019;</ref><ref type=""bibr"" target=""#b20"">Nguyen, Nguyen, and Phung 2019;</ref><ref type=""bibr"" target=""#b36"">Zhang et al. 2020)</ref> with few focused on specialized cases such a letting go sparsity and local contexts. For example, the use of graph-specific positional features <ref type=""bibr"" target=""#b36"">(Zhang et al. 2020)</ref>, or node Laplacian position eigenvectors <r s of GraphTransformer (GT) using different PE schemes. Notations x: No PE; L: LapPE (ours); W: WL-PE<ref type=""bibr"" target=""#b36"">(Zhang et al. 2020)</ref>. Bold: the best performing model for each d raph Transformer in this section, by its comparison with different PE schemes applied in Graph-BERT <ref type=""bibr"" target=""#b36"">(Zhang et al. 2020</ref>).<ref type=""foot"" target=""#foot_0"">2</ref> I L-PE which are absolute structural roles of nodes in the original graph computed using WL algorithm <ref type=""bibr"" target=""#b36"">(Zhang et al. 2020;</ref><ref type=""bibr"" target=""#b21"">Niepert, Ahme",1
"ous success in the field of natural language processing (NLP) since the development of Transformers <ref type=""bibr"" target=""#b27"">(Vaswani et al. 2017)</ref> which are currently the best performing n nal information. Since Laplacian PEs are generalization of the PE used in the original transformers <ref type=""bibr"" target=""#b27"">(Vaswani et al. 2017)</ref> to graphs and these better help encode di r</head><p>The Graph Transformer is closely the same transformer architecture initially proposed in <ref type=""bibr"" target=""#b27"">(Vaswani et al. 2017)</ref>, see Figure <ref type=""figure"">1</ref> (L target=""#tab_2"">2</ref>), which employs multi-headed attention inspired by the original transformer <ref type=""bibr"" target=""#b27"">(Vaswani et al. 2017)</ref> and have been often used in the literatur e each word attending to each other word in a sentence, as followed by the Transformer architecture <ref type=""bibr"" target=""#b27"">(Vaswani et al. 2017</ref>). -b) Next, the so-called graph considered",1
"b18"">(Monti et al. 2019)</ref>, in physics <ref type=""bibr"" target=""#b6"">(Cranmer et al. 2019;</ref><ref type=""bibr"" target=""#b24"">Sanchez-Gonzalez et al. 2020)</ref>, etc.</p><p>In particular, GNNs e",0
"s, same as in GNNs <ref type=""bibr"" target=""#b7"">(Defferrard, Bresson, and Vandergheynst 2016;</ref><ref type=""bibr"" target=""#b13"">Kipf and Welling 2017;</ref><ref type=""bibr"" target=""#b17"">Monti et a ERT uses  <ref type=""table"" target=""#tab_1"">1</ref>) on each dataset against the GNN baselines (GCN <ref type=""bibr"" target=""#b13"">(Kipf and Welling 2017)</ref>, GAT <ref type=""bibr"" target=""#b28"">(Ve",0
"sentence can alternatively be viewed as a case of a GNN applied on a fully connected graph of words <ref type=""bibr"" target=""#b12"">(Joshi 2020)</ref>. Transformers based models have led to state-of-th",0
"""bibr"" target=""#b17"">Monti et al. 2017;</ref><ref type=""bibr"" target=""#b9"">Gilmer et al. 2017;</ref><ref type=""bibr"" target=""#b28"">Veličković et al. 2018;</ref><ref type=""bibr"" target=""#b3"">Bresson an n and Ribeiro 2020)</ref>. This is a critical reason why simple attention based models, such as GAT <ref type=""bibr"" target=""#b28"">(Veličković et al. 2018)</ref>, where the attention is a function of et against the GNN baselines (GCN <ref type=""bibr"" target=""#b13"">(Kipf and Welling 2017)</ref>, GAT <ref type=""bibr"" target=""#b28"">(Veličković et al. 2018)</ref>, Gat-edGCN <ref type=""bibr"" target=""#b",0
"fication PATTERN is a node classification dataset generated using the Stochastic Block Models (SBM) <ref type=""bibr"" target=""#b0"">(Abbe 2017</ref>). The task is classify the nodes into 2 communities.",0
"original graph computed using WL algorithm <ref type=""bibr"" target=""#b36"">(Zhang et al. 2020;</ref><ref type=""bibr"" target=""#b21"">Niepert, Ahmed, and Kutzkov 2016)</ref>, are not variant to the subgr",0
"nchmarking protocol introduced in Dwivedi et al. ( <ref type=""formula"">2020</ref>) based on PyTorch <ref type=""bibr"" target=""#b22"">(Paszke et al. 2019)</ref> and DGL <ref type=""bibr"">(Wang et al. 2019",0
"art performance on several NLP applications <ref type=""bibr"" target=""#b8"">(Devlin et al. 2018;</ref><ref type=""bibr"" target=""#b23"">Radford et al. 2018;</ref><ref type=""bibr"" target=""#b4"">Brown et al.",0
"trained on graph datasets learn structural node information that are invariant to the node position <ref type=""bibr"" target=""#b26"">(Srinivasan and Ribeiro 2020)</ref>. This is a critical reason why si =""#b19"">(Murphy et al. 2019;</ref><ref type=""bibr"" target=""#b34"">You, Ying, and Leskovec 2019;</ref><ref type=""bibr"" target=""#b26"">Srinivasan and Ribeiro 2020;</ref><ref type=""bibr"" target=""#b8"">Dwive",0
"either be Layer-Norm <ref type=""bibr"" target=""#b1"">(Ba, Kiros, and Hinton 2016)</ref> or BatchNorm <ref type=""bibr"" target=""#b10"">(Ioffe and Szegedy 2015)</ref>. The bias terms are omitted for clarit",0
"r"" target=""#b13"">Kipf and Welling 2017;</ref><ref type=""bibr"" target=""#b17"">Monti et al. 2017;</ref><ref type=""bibr"" target=""#b9"">Gilmer et al. 2017;</ref><ref type=""bibr"" target=""#b28"">Veličković et",0
"ks which attempt to develop graph transformers <ref type=""bibr"" target=""#b15"">(Li et al. 2019;</ref><ref type=""bibr"" target=""#b20"">Nguyen, Nguyen, and Phung 2019;</ref><ref type=""bibr"" target=""#b36"">Z ral graphs by directly learning from dynamic information in networks. The architecture presented in <ref type=""bibr"" target=""#b20"">Nguyen, Nguyen, and Phung (2019)</ref> somewhat proceeds along our go",0
"t al., 2020)</ref>, SciPy <ref type=""bibr"" target=""#b24"">(Jones et al., 2001)</ref>, and Matplotlib <ref type=""bibr"" target=""#b22"">(Hunter, 2007)</ref>.</p></div> 			</div>  			<div type=""annex""> <div",0
"ch pointwise operations are equivariant with respect to the regular representation.</p><p>LayerNorm <ref type=""bibr"" target=""#b1"">(Ba et al., 2016)</ref> is defined as follows:</p><p>Inputs: {g, f (g)",0
"h perception tasks. This has led to the success of CNNs in multiple domains such as computer vision <ref type=""bibr"" target=""#b32"">(Krizhevsky et al., 2012)</ref> and audio <ref type=""bibr"" target=""#b",0
"ies, such as natural-language processing <ref type=""bibr"" target=""#b48"">(Vaswani et al., 2017;</ref><ref type=""bibr"" target=""#b4"">Brown et al., 2020</ref><ref type=""bibr"">), computer vision (Zhang et",0
"get=""#b49"">Walt et al., 2011;</ref><ref type=""bibr"" target=""#b18"">Harris et al., 2020)</ref>, SciPy <ref type=""bibr"" target=""#b24"">(Jones et al., 2001)</ref>, and Matplotlib <ref type=""bibr"" target=""#",0
"tead of learning the dynamics directly <ref type=""bibr"" target=""#b17"">(Greydanus et al., 2019;</ref><ref type=""bibr"" target=""#b44"">Sanchez-Gonzalez et al., 2019;</ref><ref type=""bibr"">Zhong et al., 20 s of the physical system we are modeling. We test our model on the spring dynamics task proposed in <ref type=""bibr"" target=""#b44"">Sanchez-Gonzalez et al. (2019)</ref>.  Specifically, we consider a sy",0
"ce KL-adaptive DPG (Distributional Policy Gradient), a variant of an algorithm recently proposed in <ref type=""bibr"" target=""#b36"">(Parshakova et al., 2019b)</ref>. We train the policy π θ to approxim includes an autoregressive component a(x), is the DPG (""Distributional Policy Gradient"") algorithm <ref type=""bibr"" target=""#b36"">(Parshakova et al., 2019b)</ref>. The objective of DPG is to obtain a",1
"g loops of neural networks, blurring different dimensions of the problem. By contrast -similarly to <ref type=""bibr"" target=""#b35"">Parshakova et al. (2019a;</ref><ref type=""bibr"">b)</ref> in a differe =""bibr"" target=""#b23"">(Kim &amp; Bengio, 2016;</ref><ref type=""bibr"" target=""#b34"">Owen, 2013;</ref><ref type=""bibr"" target=""#b35"">Parshakova et al., 2019a</ref>) SNIS consists in computing:</p><formu et=""#b4"">Belanger &amp; McCallum, 2016)</ref>. Some current applications to text generation include <ref type=""bibr"" target=""#b35"">Parshakova et al. (2019a)</ref> and <ref type=""bibr"" target=""#b14"">De",1
"picuous in pretrained language models. <ref type=""bibr"" target=""#b51"">(Stanovsky et al., 2019;</ref><ref type=""bibr"" target=""#b40"">Prates et al., 2020;</ref><ref type=""bibr"" target=""#b47"">Sheng et al.",0
"ncerned with neural-based sequence labelling problems (e.g. tagging) exploiting the global sequence <ref type=""bibr"" target=""#b1"">(Andor et al., 2016;</ref><ref type=""bibr"" target=""#b4"">Belanger &amp;",0
"it from distributional control, is that of social biases conspicuous in pretrained language models. <ref type=""bibr"" target=""#b51"">(Stanovsky et al., 2019;</ref><ref type=""bibr"" target=""#b40"">Prates e",0
"type=""formula"" target=""#formula_0"">1</ref>) is a generalization of the Maximum Entropy Principle of <ref type=""bibr"" target=""#b21"">Jaynes (1957)</ref>, which corresponds to the limit case where a is t",0
"machine translation <ref type=""bibr"" target=""#b10"">(Gu et al. 2018)</ref>, task-oriented dialogues <ref type=""bibr"" target=""#b24"">(Qian and Yu 2019;</ref><ref type=""bibr"" target=""#b20"">Mi et al. 2019",1
"initialization from training tasks with the ability of fast adaptation to new tasks, meta-learning <ref type=""bibr"" target=""#b8"">(Finn, Abbeel, and Levine 2017;</ref><ref type=""bibr"" target=""#b39"">Zh task T i and ? denotes a learning rate. To measure the quality of the adapted parameter ? i , MAML <ref type=""bibr"" target=""#b8"">(Finn, Abbeel, and Levine 2017)</ref>, which is an optimization-based",0
"=""bibr"" target=""#b15"">(Lian et al. 2019;</ref><ref type=""bibr"" target=""#b37"">Zhao et al. 2019;</ref><ref type=""bibr"" target=""#b13"">Kim, Ahn, and Kim 2020)</ref>. Different from them, our MDG model is",0
"pe=""bibr"" target=""#b16"">(Lin et al. 2019;</ref><ref type=""bibr"" target=""#b33"">Wei et al. 2018;</ref><ref type=""bibr"" target=""#b35"">Xu et al. 2019)</ref>. It has a significant potential to simplify the ialogue policy learning <ref type=""bibr"" target=""#b33"">(Wei et al. 2018)</ref>, dialogue management <ref type=""bibr"" target=""#b35"">(Xu et al. 2019)</ref>, and make promising progress to build a satisf ei et al. (2018)</ref> proposed to learn dialogue policy with RL to facilitate automatic diagnosis. <ref type=""bibr"" target=""#b35"">Xu et al. (2019)</ref> incorporated the knowledge inference into dial",0
"ically, which has gained increasing attention <ref type=""bibr"" target=""#b16"">(Lin et al. 2019;</ref><ref type=""bibr"" target=""#b33"">Wei et al. 2018;</ref><ref type=""bibr"" target=""#b35"">Xu et al. 2019)< sirable to study how to transfer the diagnostic experience among diseases. dialogue policy learning <ref type=""bibr"" target=""#b33"">(Wei et al. 2018)</ref>, dialogue management <ref type=""bibr"" target= 14"">Li et al. 2017</ref>) focus on reinforcement learning (RL) based task-oriented dialogue system. <ref type=""bibr"" target=""#b33"">Wei et al. (2018)</ref> proposed to learn dialogue policy with RL to",0
"a few fine-tuning updates. The proposed learning to pre-train can be deemed a form of meta-learning <ref type=""bibr"" target=""#b12"">(Finn, Abbeel, and Levine 2017)</ref>, also known as learning to lear ermore, our strategy is a form of meta-learning, in particular, model agnostic meta-learning (MAML) <ref type=""bibr"" target=""#b12"">(Finn, Abbeel, and Levine 2017)</ref>. Meta-learning aims to learn pr hods directly adjust the optimization algorithm to enable quick adaptation with just a few examples <ref type=""bibr"" target=""#b12"">(Finn, Abbeel, and Levine 2017;</ref><ref type=""bibr"" target=""#b43"">Y",1
"t=""#b28"">(Navarin, Tran, and Sperduti 2018;</ref><ref type=""bibr"" target=""#b18"">Hu et al. 2019</ref><ref type=""bibr"" target=""#b17"">Hu et al. , 2020) )</ref> is to learn transferable prior knowledge fr t=""#b18"">Hu et al. 2019)</ref>, or still require supervised information for graph-level pretraining <ref type=""bibr"" target=""#b17"">(Hu et al. 2020</ref>). While at the node level, predicting links bet is to learn a generic initialization for model parameters using readily available graph structures <ref type=""bibr"" target=""#b17"">(Hu et al. 2020</ref><ref type=""bibr"" target=""#b18"">(Hu et al. , 2019 atasets. We conduct experiments on data from two domains: biological function prediction in biology <ref type=""bibr"" target=""#b17"">(Hu et al. 2020</ref>) and research field prediction in bibliography. ers with three unsupervised tasks to capture different aspects of a graph. More recently, Hu et al. <ref type=""bibr"" target=""#b17"">(Hu et al. 2020)</ref> propose different strategies to pre-train grap subgraph is centered at a paper and contains the associated information of For biology data, as in <ref type=""bibr"" target=""#b17"">(Hu et al. 2020)</ref>, we use 306,925 unlabeled protein ego-networks that correspond to 40 binary classification tasks. We split the downstream data with species split <ref type=""bibr"" target=""#b17"">(Hu et al. 2020)</ref>, and evaluate the test performance with averag across the graph's patch representations; (3) Context Prediction strategy (denoted by ContextPred) <ref type=""bibr"" target=""#b17"">(Hu et al. 2020)</ref> to explore graph structures and (4) Attribute 2020)</ref> to explore graph structures and (4) Attribute Masking strategy (denoted by AttrMasking) <ref type=""bibr"" target=""#b17"">(Hu et al. 2020)</ref> to learn the regularities of the node and edge which harms the generalization of the pre-trained GNNs. This finding confirms previous observations <ref type=""bibr"" target=""#b17"">(Hu et al. 2020;</ref><ref type=""bibr"" target=""#b31"">Rosenstein et al",1
""" target=""#b39"">Velickovic et al. 2018;</ref><ref type=""bibr"" target=""#b45"">Ying et al. 2018b;</ref><ref type=""bibr"" target=""#b15"">Hasanzadeh et al. 2019;</ref><ref type=""bibr"" target=""#b30"">Qu, Bengi",0
"et=""#b12"">(Finn, Abbeel, and Levine 2017;</ref><ref type=""bibr"" target=""#b43"">Yao et al. 2019;</ref><ref type=""bibr"" target=""#b21"">Lee et al. 2019;</ref><ref type=""bibr"" target=""#b24"">Lu, Fang, and Sh",0
"rget=""#b7"">(Devlin et al. 2019;</ref><ref type=""bibr"">Mikolov et al. 2013</ref>) and image encoders <ref type=""bibr"" target=""#b13"">(Girshick et al. 2014;</ref><ref type=""bibr"" target=""#b8"">Donahue et uning, which is widely known in the literature <ref type=""bibr"" target=""#b25"">(Lv et al. 2020;</ref><ref type=""bibr"" target=""#b13"">Gururangan et al. 2020</ref>), whether it is on the graph data, or in",0
"aggregation schemes have been proposed <ref type=""bibr"" target=""#b19"">(Kipf and Welling 2017;</ref><ref type=""bibr"" target=""#b14"">Hamilton, Ying, and Leskovec 2017;</ref><ref type=""bibr"" target=""#b39 target=""#b19"">(Kipf and Welling 2017;</ref><ref type=""bibr"">Niepert, Ahmed, and Kutzkov 2016;</ref><ref type=""bibr"" target=""#b14"">Hamilton, Ying, and Leskovec 2017;</ref><ref type=""bibr"" target=""#b39 , such as node and graph classification <ref type=""bibr"" target=""#b19"">(Kipf and Welling 2017;</ref><ref type=""bibr"" target=""#b14"">Hamilton, Ying, and Leskovec 2017)</ref>, recommendation systems <ref upervised objective of predicting the link between u and v <ref type=""bibr"">(Tang et al. 2015;</ref><ref type=""bibr"" target=""#b14"">Hamilton, Ying, and Leskovec 2017)</ref>, as follows.</p><formula xml r self-supervised or unsupervised baselines: (1) the original Edge Prediction (denoted by EdgePred) <ref type=""bibr"" target=""#b14"">(Hamilton, Ying, and Leskovec 2017)</ref> to predict the connectivity architectures, namely, GCN <ref type=""bibr"" target=""#b19"">(Kipf and Welling 2017)</ref>, GraphSAGE <ref type=""bibr"" target=""#b14"">(Hamilton, Ying, and Leskovec 2017)</ref>, GAT <ref type=""bibr"" targe",0
"Hamilton, Ying, and Leskovec 2017;</ref><ref type=""bibr"" target=""#b39"">Velickovic et al. 2018;</ref><ref type=""bibr"" target=""#b45"">Ying et al. 2018b;</ref><ref type=""bibr"" target=""#b15"">Hasanzadeh et t al. 2015)</ref> or more complex approaches <ref type=""bibr"" target=""#b5"">(Bruna et al. 2014;</ref><ref type=""bibr"" target=""#b45"">Ying et al. 2018b</ref>). We abstract READOUT as a parameterized func",0
"res as well as node or edge features <ref type=""bibr"" target=""#b47"">(Zhang, Cui, and Zhu 2020;</ref><ref type=""bibr"" target=""#b42"">Wu et al. 2020;</ref><ref type=""bibr"" target=""#b10"">Dwivedi et al. 20 =""bibr"" target=""#b5"">Bruna et al. 2014;</ref><ref type=""bibr"" target=""#b22"">Levie et al. 2019;</ref><ref type=""bibr"" target=""#b42"">Xu et al. 2019a</ref>) and message passing architectures to aggregate al. 2019)</ref>. For a more comprehensive understanding of GNNs, we refer readers to the literature <ref type=""bibr"" target=""#b42"">(Wu et al. 2020;</ref><ref type=""bibr"" target=""#b2"">Battaglia et al.",0
"Leskovec 2017)</ref>, recommendation systems <ref type=""bibr"" target=""#b11"">(Fan et al. 2019;</ref><ref type=""bibr"" target=""#b44"">Ying et al. 2018a</ref>) and graph generation <ref type=""bibr"" target",0
"rnal layers in addition to final predictions <ref type=""bibr"" target=""#b15"">(Jiao et al. 2019;</ref><ref type=""bibr"" target=""#b21"">Sun et al. 2020</ref><ref type=""bibr"">Sun et al. , 2019))</ref>, but ther models such as TinyBERT <ref type=""bibr"" target=""#b15"">(Jiao et al. 2019)</ref> and MobileBERT <ref type=""bibr"" target=""#b21"">(Sun et al. 2020</ref>) also found it crucial for training competitiv",1
"been proposed to this end, which compare networks' internal layers in addition to final predictions <ref type=""bibr"" target=""#b15"">(Jiao et al. 2019;</ref><ref type=""bibr"" target=""#b21"">Sun et al. 202 e teacher layers into m buckets with approximately the same sizes and pick only one layer from each <ref type=""bibr"" target=""#b15"">(Jiao et al. 2019;</ref><ref type=""bibr"">Sun et al. 2019)</ref>. Ther PKD is not the only model that utilizes internal layers' information. Other models such as TinyBERT <ref type=""bibr"" target=""#b15"">(Jiao et al. 2019)</ref> and MobileBERT <ref type=""bibr"" target=""#b21",1
"onsider it as a complementary and generic add-on to enrich the training process of any neural model <ref type=""bibr"" target=""#b11"">(Furlanello et al. 2018)</ref>.</p><p>In KD, a student network (S) is",0
"the student model may fail to provide high-quality results.</p><p>To tackle this problem, Wu et al. <ref type=""bibr"" target=""#b1"">(2020)</ref> proposed a combinatorial technique, called CKD. In their ation produced by the function F c given a subset of teacher layers indicated by A(j). In Wu et al. <ref type=""bibr"" target=""#b1"">(2020)</ref>, F c is implemented via a simple concatenation. Depending <p>The application of KD in NLP and NLU is not limited to the aforementioned models. Aguilar et al. <ref type=""bibr"" target=""#b1"">(2020)</ref> followed the same architecture as PKD but they introduced To this end, we run a grid search similar to <ref type=""bibr"">Sun et al. (2019)</ref> and Wu et al. <ref type=""bibr"" target=""#b1"">(2020)</ref>. For our experiments, the batch size is set to 32 and the",0
"nguage understanding (NLU) models such as <ref type=""bibr"">ELMO (Peters et al. 2018)</ref> and BERT <ref type=""bibr"" target=""#b8"">(Devlin et al. 2019</ref>) KD has gained extra attention. Deep models (also known as BERT Base ) as our teacher. We are faithful to the experimental setting proposed by <ref type=""bibr"" target=""#b8"">Devlin et al. (2019)</ref> for this model. Therefore, our in-house ver -tuned during training. The concept of fine-tuning and its details are comprehensively discussed in <ref type=""bibr"" target=""#b8"">Devlin et al. (2019)</ref>, so we skip that part and refer the reader _0"">By the output, we mean the output of the layer for the CLS token. For more details about CLS see<ref type=""bibr"" target=""#b8"">Devlin et al. (2019)</ref>.</note> 			<note xmlns=""http://www.tei-c.or",0
"yals, and Dean 2015)</ref> is a commonly-used technique to reduce the size of large neural networks <ref type=""bibr"" target=""#b18"">(Sanh et al. 2019)</ref>. Apart from this, we also consider it as a c rget=""#tab_0"">1</ref>  S RKD is equivalent to a configuration known as DistilBERT in the literature <ref type=""bibr"" target=""#b18"">(Sanh et al. 2019)</ref>. To have precise results and a better compar",0
"layers. They claim that the way internal layers are trained during KD can play a significant role. <ref type=""bibr"" target=""#b16"">Liu et al. (2019)</ref> investigated KD from another perspective. Ins",0
"et al. ( <ref type=""formula"">2019</ref>) squeezed multiple translation engines into one transformer <ref type=""bibr"" target=""#b22"">(Vaswani et al. 2017</ref>) and showed that knowledge can be distille",0
"er BERT (Devlin et al. 2019) into 6-, 4-, and 2-layer counterparts and evaluated them on GLUE tasks <ref type=""bibr"" target=""#b23"">(Wang et al. 2018)</ref>. Experimental results show that our combinat ir performance. We followed the same tradition in this paper and selected a set of eight GLUE tasks <ref type=""bibr"" target=""#b23"">(Wang et al. 2018)</ref> including CoLA, MNLI, MRPC, QNLI, QQP, RTE,",0
"ve to automatically generate prompts given the few-shot training data using the generative T5 model <ref type=""bibr"" target=""#b30"">(Raffel et al., 2020)</ref>. This allows us to cheaply obtain effecti lly from a fixed set of label words M(Y). To address this challenging problem, we propose to use T5 <ref type=""bibr"" target=""#b30"">(Raffel et al., 2020)</ref>, a large pre-trained text-to-text Transfo e <ref type=""table"">B</ref>.1.</p><p>For automatic template search with T5, we take the T5-3B model <ref type=""bibr"" target=""#b30"">(Raffel et al., 2020)</ref>, which is the largest publicly available",1
"mple examples that are semantically close to x in . Specifically, we use a pretrained Sentence-BERT <ref type=""bibr"" target=""#b32"">(Reimers and Gurevych, 2019)</ref> model to obtain embeddings for all",0
"d language model such as BERT <ref type=""bibr"" target=""#b11"">(Devlin et al., 2019)</ref> or RoBERTa <ref type=""bibr"" target=""#b21"">(Liu et al., 2019)</ref>, and a small number of examples (i.e., a few",0
""">Giampiccolo et al., 2007;</ref><ref type=""bibr"" target=""#b5"">Bentivogli et al., 2009)</ref>, MRPC <ref type=""bibr"" target=""#b13"">(Dolan and Brockett, 2005)</ref>, QQP<ref type=""foot"" target=""#foot_9",0
"=""#b31"">(Rajpurkar et al., 2016)</ref>, RTE <ref type=""bibr"" target=""#b9"">(Dagan et al., 2005;</ref><ref type=""bibr"" target=""#b3"">Bar-Haim et al., 2006;</ref><ref type=""bibr"" target=""#b15"">Giampiccolo",0
"div xmlns=""http://www.tei-c.org/ns/1.0""><p>We generalize deep self-attention distillation in MINILM <ref type=""bibr"" target=""#b40"">(Wang et al., 2020)</ref> by only using self-attention relation disti den size the same, layer-wisely transferring hidden states and self-attention distributions. MINILM <ref type=""bibr"" target=""#b40"">(Wang et al., 2020)</ref> proposes deep self-attention distillation, teacher.</p><p>In this work, we generalize and simplify deep self-attention distillation of MINILM <ref type=""bibr"" target=""#b40"">(Wang et al., 2020)</ref> by using self-attention relation distillati the student has the same number of layers as its teacher to perform layer-wise distillation. MINILM <ref type=""bibr"" target=""#b40"">(Wang et al., 2020)</ref> transfers selfattention knowledge of teache ""bibr"" target=""#b14"">Jiao et al., 2019;</ref><ref type=""bibr"" target=""#b34"">Sun et al., 2019b;</ref><ref type=""bibr"" target=""#b40"">Wang et al., 2020)</ref>. The student models are distilled from large ""bibr"" target=""#b14"">Jiao et al., 2019;</ref><ref type=""bibr"" target=""#b33"">Sun et al., 2019a;</ref><ref type=""bibr"" target=""#b40"">Wang et al., 2020)</ref>  MobileBERT compresses a specially designed esults of Distil-BERT, TinyBERT 2 , BERT SMALL , Truncated BERT BASE and 6×768 MINILM are taken from<ref type=""bibr"" target=""#b40"">Wang et al. (2020)</ref>. BERT SMALL</figDesc><table><row><cell>Model",1
"target=""#b39"">(Wang et al., 2019)</ref> consists of two single-sentence classification tasks (SST-2 <ref type=""bibr"" target=""#b31"">(Socher et al., 2013)</ref> and CoLA <ref type=""bibr"" target=""#b41"">(",0
"05)</ref>, STS-B <ref type=""bibr"">(Cer et al., 2017)</ref> and QQP), and four inference tasks (MNLI <ref type=""bibr"" target=""#b42"">(Williams et al., 2018)</ref>, QNLI <ref type=""bibr"" target=""#b27"">(R",0
"al., 2015)</ref> and attention distributions <ref type=""bibr"">(Zagoruyko and Komodakis, 2017;</ref><ref type=""bibr"" target=""#b13"">Hu et al., 2018)</ref> are introduced to improve the student model.</",0
"uned to adapt to downstream tasks. It can also be utilized to initialize task-specific distillation <ref type=""bibr"" target=""#b33"">(Sun et al., 2019a;</ref><ref type=""bibr"" target=""#b37"">Turc et al., bibr"" target=""#b30"">(Sanh et al., 2019;</ref><ref type=""bibr"" target=""#b14"">Jiao et al., 2019;</ref><ref type=""bibr"" target=""#b33"">Sun et al., 2019a;</ref><ref type=""bibr"" target=""#b40"">Wang et al., 2",0
