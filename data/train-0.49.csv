text,label
"bp1.5 2.1 Overview</head><p>Predictor cbp1.5 is a global-history based predictor derived from PPM. PPM was originally introduced for text compression <ref type=""bibr"" target=""#b0"">[1]</ref>, and it was used in <ref type=""bibr"" target=""#b1"">[2]</ref> for branch prediction. Figure <ref type=""figure"" t the branch ending the sequence was taken. At the end of the PCF, we build S * and sort sequences in decreasing potentials. Then we compute m(T ) from <ref type=""bibr"" target=""#b0"">(1)</ref>. Because of memory limitations on our machines, when |S| exceeded a certain size, we decreased N . However, as </ref> Step 5 is the biggest qualitative step. Starting at this step, the content of T is allowed to change dynamically. We can no longer use formula <ref type=""bibr"" target=""#b0"">(1)</ref>, and so now we generate results by performing a second pass on the traces. We use the 4 global banks of cbp1.5",1
"<ref type=""figure"" target=""#fig_0"">1</ref> shows a synopsis of cbp1.5, which features 5 banks. It can be viewed as a 4 th order approximation to PPM <ref type=""bibr"" target=""#b2"">[3]</ref>, while YAGS <ref type=""bibr"" target=""#b3"">[4]</ref>, which is a GPPM predictor too, can be viewed as a 1 st or mplemented by taking advantage of the fact that we are not folding a random value, but a global history value derived from the previous history value <ref type=""bibr"" target=""#b2"">[3]</ref>. Figure <ref type=""figure"" target=""#fig_1"">2</ref> shows two examples of how global history folding can be imp",0
"tory based predictor derived from PPM. PPM was originally introduced for text compression <ref type=""bibr"" target=""#b0"">[1]</ref>, and it was used in <ref type=""bibr"" target=""#b1"">[2]</ref> for branch prediction. Figure <ref type=""figure"" target=""#fig_0"">1</ref> shows a synopsis of cbp1.5, which fea",0
"false"" mispredictions on sequences u for which there exists a longer matching sequence in T , i.e., s∈T f (s.0 u ). More explanations can be found in <ref type=""bibr"" target=""#b6"">[7]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.3"">A heuristic for set T</head><p>We assume |T",0
"synopsis of cbp1.5, which features 5 banks. It can be viewed as a 4 th order approximation to PPM <ref type=""bibr"" target=""#b2"">[3]</ref>, while YAGS <ref type=""bibr"" target=""#b3"">[4]</ref>, which is a GPPM predictor too, can be viewed as a 1 st order approximation. The leftmost bank on Figure <ref",0
"><p>In Section 2 we discuss previous related work, including state of the art fetch architectures like the FTB proposed by Reinman, Austin and Calder <ref type=""bibr"" target=""#b29"">[30]</ref> and the trace cache architecture as proposed by Rotenberg, Bennett and Smith in <ref type=""bibr"" target=""#b3 tecture leads to a decoupling of the dynamic branch prediction mechanism and the instruction cache access, as proposed by Reinman, Austin, and Calder <ref type=""bibr"" target=""#b29"">[30]</ref>. The branch prediction mechanism is a fully autonomous engine, capable of following a speculative path witho stored in a fetch target queue (FTQ). The instruction cache is then driven by the requests stored in the FTQ.</p><p>Another important contribution of <ref type=""bibr"" target=""#b29"">[30]</ref> is the Fetch Target Buffer (FTB). It extends the BTB by allowing the storage of variable length fetch blocks s information about a whole instruction stream, possibly containing multiple basic blocks.</p><p>The use of an FTQ is not novel, it was introduced in <ref type=""bibr"" target=""#b29"">[30]</ref>. It decouples the branch prediction from the memory access, and stores information about the instruction seq y.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.3."">Fetch target queue</head><p>Following the proposal of Reinman, Austin and Calder <ref type=""bibr"" target=""#b29"">[30]</ref> we have decoupled the branch prediction stage from the instruction cache access stage. The stream predictor on the instruction cache.</p><p>We compare our stream fetch architecture with three other state-of-the-art fetch architectures: the FTB architecture <ref type=""bibr"" target=""#b29"">[30]</ref> using a perceptron branch predictor <ref type=""bibr"" target=""#b17"">[18]</ref>, the Alpha EV8 architecture us",1
"Reinman, Austin and Calder <ref type=""bibr"" target=""#b29"">[30]</ref> and the trace cache architecture as proposed by Rotenberg, Bennett and Smith in <ref type=""bibr"" target=""#b31"">[32]</ref>.</p><p>In Section 3 we describe our proposed stream fetch architecture in detail, showing how it improves on . Figure <ref type=""figure"" target=""#fig_1"">3</ref> shows a block diagram of the trace cache mechanism as proposed by Rotenberg, Benett and Smith in <ref type=""bibr"" target=""#b31"">[32]</ref>. The trace cache captures the dynamic instruction stream, and fragments it in smaller segments called traces Alpha EV8 architecture using a 2bcgskew predictor <ref type=""bibr"" target=""#b33"">[34]</ref>, and the trace cache architecture using a trace predictor <ref type=""bibr"" target=""#b31"">[32]</ref> and selective trace storage <ref type=""bibr"" target=""#b28"">[29]</ref>.</p><p>Table <ref type=""table"" target= cessary.</p><p>The trace cache <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b30"">31,</ref><ref type=""bibr"" target=""#b31"">32]</ref> is one such high fetch width mechanism, recently implemented in the Pentium4 processor <ref type=""bibr"" targe",1
"ous stream starting addresses (the previous fetch addresses). The hash function uses a DOLC scheme similar to what was used in multiscalar processors <ref type=""bibr"" target=""#b15"">[16]</ref>.   The predictor maintains two separate path history registers: a lookup register which is updated immediate",0
"which profile data is unavailable, it is still possible to benefit from code layout optimizations either using heuristics to replace the profile data <ref type=""bibr"" target=""#b1"">[2]</ref>, or dynamic code optimizers <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b18"">19]</ref>.</",0
"enefit the underlying fetch architecture and branch predictor <ref type=""bibr"" target=""#b3"">[4]</ref>. Our previous work presents a detailed analysis <ref type=""bibr"" target=""#b24"">[25]</ref> of the effects of these optimizations, concluding that the improvements on the instruction cache performance ful instructions to cache lines. These results show that even a very long 128-byte line (32 instructions) is usually fully used before being replaced <ref type=""bibr"" target=""#b24"">[25]</ref>.</p><p>Considering both benefits together (the reduced stream misalignment, and the instruction cache miss r",0
"wide superscalar processor, fetching multiple basic blocks per cycle becomes necessary.</p><p>The trace cache <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b30"">31,</ref><ref type=""bibr"" target=""#b31"">32]</ref> is one such high fetch width",0
"encountered taken branch, much in the way the SEQ.3 engine described in <ref type=""bibr"" target=""#b30"">[31]</ref>.</p><p>The rePLay microarchitecture <ref type=""bibr"" target=""#b21"">[22]</ref> uses a front end derived from the trace cache, making extensive use of the branch promotion technique to bui essors like the Pentium4 <ref type=""bibr"" target=""#b13"">[14]</ref>, or enabling dynamic optimization of traces <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b21"">22]</ref>. Our instruction stream fetch architecture is intended as another alternative for a high-performance fetch ar",0
"ntained. A trace may contain multiple basic blocks, and several branches, regardless of them being taken or not taken.</p><p>The next trace predictor <ref type=""bibr"" target=""#b16"">[17]</ref> provides trace level sequencing. That is, the fetch engine steps through the code at the trace level granula",0
"ltiple basic blocks per cycle becomes necessary.</p><p>The trace cache <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b30"">31,</ref><ref type=""bibr"" target=""#b31"">32]</ref> is one such high fetch width mechanism, recently implemented in the P ch predictor to fetch instructions from multiple basic blocks up to the first encountered taken branch, much in the way the SEQ.3 engine described in <ref type=""bibr"" target=""#b30"">[31]</ref>.</p><p>The rePLay microarchitecture <ref type=""bibr"" target=""#b21"">[22]</ref> uses a front end derived from erformance similar to that of a trace cache when using layout optimized codes. The sequential engine used in that work is the SEQ.3 unit described in <ref type=""bibr"" target=""#b30"">[31]</ref>. However, such engine proves very complex to implement, and is still limited to 3 consecutive basic blocks p rom a multi-banked instruction cache, so that we can always guarantee a full width of instructions, as done in <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b30"">31]</ref>. Our solution requires a wider read port to the instruction cache, while this solution requires an interchang",0
"ecial purpose cache.</p><p>The next line and set predictor (NLS) architecture <ref type=""bibr"" target=""#b4"">[5]</ref>, implemented in the Alpha 21264 <ref type=""bibr"" target=""#b11"">[12]</ref>, also allows fetching of multiple basic blocks in a single cycle, as long as they reside sequentially in the",0
"or architectures (see <ref type=""bibr"">Section 4.3)</ref>. In this paper we use Checkpoint Processing and Recovery (CPR) as the baseline architecture <ref type=""bibr"" target=""#b1"">[2]</ref> since it has been shown to outperform conventional ROB-based architectures. CPR is a reorder-bufferfree archit http://www.tei-c.org/ns/1.0""><head n=""3.1"">CPR overview</head><p>CPR is a ROB-free proposal for building scalable large instruction window processors <ref type=""bibr"" target=""#b1"">[2]</ref>. CPR addresses the scalability and performance limitations of conventional branch misprediction recovery mecha",1
"herry <ref type=""bibr"" target=""#b14"">[15]</ref>. Out-of-order commit processors <ref type=""bibr"" target=""#b7"">[8]</ref> combine a checkpoint proposal <ref type=""bibr"" target=""#b8"">[9]</ref> with the WIB <ref type=""bibr"" target=""#b13"">[14]</ref> to address scheduler limitations for a checkpoint proce",0
"in parallel with the outstanding memory miss. Prior research has shown a significant amount of useful work can be done in the shadow of a memory miss <ref type=""bibr"" target=""#b12"">[13]</ref>. CFP uses that observation to achieve memory latency tolerance. By providing mechanisms to obtain nonblockin the non-speculative instruction-window in the shadow of a long latency miss comprises miss-independent instructions for all benchmark suites. Others <ref type=""bibr"" target=""#b12"">[13]</ref> have also observed similar results for the SINT2K suite. Thus, a significant amount of useful work can be co data returns. Our experiments show that a very small fraction of mispredicted branches is dependent on a long-latency load miss. Karkhanis and Smith <ref type=""bibr"" target=""#b12"">[13]</ref> identified structural, data, and controlinduced stalls as key performance limiters. While branch prediction",0
"of completed source registers. Unlike VPR, CFP provides a mechanism to release both of the above types of registers.</p><p>In Dynamic Multithreading <ref type=""bibr"" target=""#b0"">[1]</ref>, instructions from the speculative thread execute in a multithreaded processor using data speculation, leave t",0
"ahead</head><p>Runahead execution <ref type=""bibr"" target=""#b10"">[11]</ref> in out-of-order processors has been proposed to tolerate memory latencies <ref type=""bibr"" target=""#b17"">[18]</ref>. In runahead execution, the processor state is checkpointed at a long latency miss operation. Execution cont",0
"head n=""2.1."">Spatial Patterns and Generations</head><p>We formalize our notion of spatial correlation similar to prior studies of spatial footprints <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b16"">17]</ref>. We define a spatial region as a fixed-size portion of the system's ad has shown that spatial correlation can be predicted in hardware by correlating patterns with the code and/or data address that initiates the pattern <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b16"">17]</ref>. Whereas existing spatial pattern prefetching designs are effective fo ased predictor with equivalent storage. • Accurate tracking of spatial correlation. We show that the cache-coupled structures used in previous work ( <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b16"">17]</ref>) are suboptimal for observing spatial correlation. Accesses to multipl the address and program counter to construct an index consistently provides the most accurate predictions when correlation table storage is unbounded <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b16"">17]</ref>. By combining both quantities, which we call PC+address indexing, a pr c storage constraints.</p><p>For SPEC CPU 2000 applications, PC+address indexing can be approximated by combining the PC with a spatial region offset <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b16"">17]</ref>. The spatial region offset of a data address is the distance, in cache , despite its small capacity.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.2."">Indexing</head><p>Prior studies of spatial predictors <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b16"">17]</ref> advocate predictor indices that include address information. In this s regions are interleaved-and thus reduce predictor coverage and/or fragment prediction entries, consequently polluting the PHT.</p><p>Past predictors <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b16"">17]</ref> couple the predictor training structure to a sectored (i.e., sub-block ref><ref type=""bibr"" target=""#b16"">17]</ref>. Whereas existing spatial pattern prefetching designs are effective for desktop/engineering applications <ref type=""bibr"" target=""#b3"">[4]</ref>, the only practical implementation evaluated on server workloads provides less than 20% miss rate reduction <r stinguish among traversals allows it to achieve the highest coverage.</p><p>For scientific applications, we corroborate the conclusions of prior work <ref type=""bibr"" target=""#b3"">[4]</ref> that indicate PC+offset indexing generally approaches the peak coverage achieved by the PC+address indexing sc =""bibr"" target=""#b16"">[17]</ref> employed a decoupled sectored cache <ref type=""bibr"" target=""#b21"">[22]</ref>, whereas the spatial pattern predictor <ref type=""bibr"" target=""#b3"">[4]</ref> provided a logical sectored-cache tag array alongside a traditional cache. The logical sectored-cache tag arra",1
"rations</head><p>We formalize our notion of spatial correlation similar to prior studies of spatial footprints <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b16"">17]</ref>. We define a spatial region as a fixed-size portion of the system's address space, consisting of multiple con be predicted in hardware by correlating patterns with the code and/or data address that initiates the pattern <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b16"">17]</ref>. Whereas existing spatial pattern prefetching designs are effective for desktop/engineering applications <ref • Accurate tracking of spatial correlation. We show that the cache-coupled structures used in previous work ( <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b16"">17]</ref>) are suboptimal for observing spatial correlation. Accesses to multiple independent patterns are frequently i truct an index consistently provides the most accurate predictions when correlation table storage is unbounded <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b16"">17]</ref>. By combining both quantities, which we call PC+address indexing, a predictor generates distinct patterns whe PU 2000 applications, PC+address indexing can be approximated by combining the PC with a spatial region offset <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b16"">17]</ref>. The spatial region offset of a data address is the distance, in cache blocks, of the address from the start <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.2."">Indexing</head><p>Prior studies of spatial predictors <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b16"">17]</ref> advocate predictor indices that include address information. In this section, we show that PC+offset indexing e predictor coverage and/or fragment prediction entries, consequently polluting the PHT.</p><p>Past predictors <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b16"">17]</ref> couple the predictor training structure to a sectored (i.e., sub-blocked) cache tag array. In a sectored cach s <ref type=""bibr"" target=""#b3"">[4]</ref>, the only practical implementation evaluated on server workloads provides less than 20% miss rate reduction <ref type=""bibr"" target=""#b16"">[17]</ref>.</p><p>In this paper, we reconsider prediction and streaming of spatially-correlated access patterns to impr tion of scientific and commercial applications. This work demonstrates: • Effective spatial correlation and prediction. Contrary to previous findings <ref type=""bibr"" target=""#b16"">[17]</ref>, address-based correlation is not needed to predict the access stream of commercial workloads. Instead, we s ration. The precise interval over which a spatial region generation is defined can significantly impact the accuracy and coverage of spatial patterns <ref type=""bibr"" target=""#b16"">[17]</ref>. A generation must be defined to ensure that, when SMS streams blocks into the cache upon a future trigger a ess information) is less accurate because it cannot distinguish among distinct access patterns to different data structures by the same code (e.g.,   <ref type=""bibr"" target=""#b16"">[17]</ref>, which indicated that PC+address provides superior coverage. Indices that correlate primarily based on progr ches are less flexible than traditional caches and experience worse conflict behavior. To mitigate this disadvantage, the spatial footprint predictor <ref type=""bibr"" target=""#b16"">[17]</ref> employed a decoupled sectored cache <ref type=""bibr"" target=""#b21"">[22]</ref>, whereas the spatial pattern p",1
"bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b28"">29]</ref>), few studies have demonstrated success at improving memory system performance for commercial applications. I Vleet and co-authors <ref type=""bibr"" target=""#b27"">[28]</ref> propose offline profiling to select fetch size upon a miss. Guided region prefetching <ref type=""bibr"" target=""#b28"">[29]</ref> uses compiler hints to direct both spatial and non-spatial (e.g., pointer-chasing) prefetches. However, the",0
"ose PC/DC (program counter / delta correlation) variant was shown to be the most effective prefetching technique for desktop/engineering applications <ref type=""bibr"" target=""#b11"">[12]</ref>. Like SMS, GHB-PC/DC exploits spatial relationships between addresses. However, GHB seeks to predict the seq nsecutive memory accesses by the same instruction.</p><p>We consider GHB with two history buffer sizes: 256 entries (sufficient for SPEC applications <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b19"">20]</ref>) and 16k entries (to roughly match the capacity of the SMS PHT). The",0
"y-based shared-memory multiprocessor system running Solaris 8. We employ a wait-free implementation of the total store order memory consistency model <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b9"">10]</ref>. We perform speculative load and store prefetching <ref type=""bibr"" tar",0
"to Section 4.7 use this tracebased methodology.</p><p>For cycle-accurate simulations, we use a sampling approach developed in accordance with SMARTS <ref type=""bibr"" target=""#b31"">[32]</ref>. Our samples are drawn over an interval of 10 to 30 seconds of simulated time (as observed by the operating",0
"trated great success in improving MLP and hiding memory latency in desktop and scientific applications through memory prefetching or streaming (e.g., <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b24",0
"iations in spatial locality at runtime, including the dual data cache <ref type=""bibr"" target=""#b10"">[11]</ref>, the spatial locality detection table <ref type=""bibr"" target=""#b15"">[16]</ref>, and caches that dynamically adjust block size <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" targe",0
"struction cache stalls <ref type=""bibr"" target=""#b20"">[21]</ref>. Software prefetching can accelerate certain database operations, such as hash joins <ref type=""bibr"" target=""#b4"">[5]</ref>. Temporal streaming reduces coherence stalls by streaming repetitive, temporally-correlated coherence miss seq",0
"(DSS), and web server workloads is spent on memory system-related stalls <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b29""",0
"tics makes supplying a high bandwidth of useful instructions a di cult task, even in the presence of aiding devices like a hardware trace cache (HTC) <ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b3"">4]</ref>. Indeed, a trace cache does not work very well with very large codes wi",1
"has largely focused on simply reducing the instruction cache miss rate <ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b7"">8]</",0
"ut the N 2 nature of the matrix makes larger schedulers still difficult to implement.</p><p>Academic researchers have proposed dataflow prescheduling <ref type=""bibr"" target=""#b5"">[5,</ref><ref type=""bibr"" target=""#b8"">8,</ref><ref type=""bibr"" target=""#b21"">21,</ref><ref type=""bibr"" target=""#b22"">22 operand readiness between partitions. There has also been academic work in attempting to reduce use of the picker. Select-free instruction scheduling <ref type=""bibr"" target=""#b5"">[5]</ref> and grandchild scheduling <ref type=""bibr"" target=""#b30"">[30]</ref> both move the picker into a separate pipel",1
"=""bibr"" target=""#b1"">[1,</ref><ref type=""bibr"" target=""#b22"">22,</ref><ref type=""bibr"" target=""#b23"">23,</ref><ref type=""bibr"" target=""#b26"">26,</ref><ref type=""bibr"" target=""#b27"">27]</ref>, though the most common in industry is by execution resource. The cost of any partitioning technique, however",0
"ly relevant to our discussion because it has shown that age is not the optimal conflict-resolution heuristic <ref type=""bibr"" target=""#b10"">[10,</ref><ref type=""bibr"" target=""#b11"">11,</ref><ref type=""bibr"" target=""#b31"">31]</ref>. Criticality prediction, however, is quite complex and largely ineffe",0
"/ref><ref type=""bibr"" target=""#b8"">8,</ref><ref type=""bibr"" target=""#b21"">21,</ref><ref type=""bibr"" target=""#b22"">22]</ref> and dependence collapsing <ref type=""bibr"" target=""#b3"">[3,</ref><ref type=""bibr"" target=""#b24"">24,</ref><ref type=""bibr"" target=""#b25"">25]</ref>, both of which use dataflow in",0
"tch architecture complexity. Our approach to achieve high fetch bandwidth, while maintaining the complexity under control, is the stream fetch engine <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b15"">16]</ref>.</p><p>This fetch engine design is based on the next stream predicto trace cache and the instruction cache.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.2"">Fetch Models</head><p>The stream fetch engine <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b15"">16]</ref> model is shown in Figure <ref type=""figure"" target=""#fig_2"">2</ref>. ne ends in a taken branch. This makes it possible for the stream fetch engine to provide high fetch bandwidth while requiring low implementation cost <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b15"">16]</ref>. However, having high average length does not involve that most stre e stream predictor.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""5.1"">The Multiple Stream Predictor</head><p>The next stream predictor <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b15"">16]</ref>, which is shown in Figure <ref type=""figure"">4</ref>.a, is a special ata are shown for both the baseline and the optimized code layout. In addition, data are shown for the original single-stream predictor, described in <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b15"">16]</ref>, and a 2-stream multiple predictor.</p><p>The main difference betwee >, to hide the prediction table access delay.</p><p>To avoid this increase in the fetch engine complexity, we propose to use long instruction streams <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b15"">16]</ref> as basic prediction unit, which makes it possible to hide the predic not provide additional performance improvements.</p><p>Our instruction cache setup uses wide cache lines, that is, 4 times the processor fetch width <ref type=""bibr"" target=""#b10"">[11]</ref>, and 64KB total hardware budget. The trace fetch architecture is actually evaluated using a 32KB instruction",1
"ually divided into a filter trace cache <ref type=""bibr"" target=""#b12"">[13]</ref> and a main trace cache. In addition, we use selective trace storage <ref type=""bibr"" target=""#b9"">[10]</ref> to avoid trace redundancy between the trace cache and the instruction cache.</p></div> <div xmlns=""http://www",0
"a 32KB instruction cache, while the remainder 32KB are devoted to the trace cache. This hardware budget is equally divided into a filter trace cache <ref type=""bibr"" target=""#b12"">[13]</ref> and a main trace cache. In addition, we use selective trace storage <ref type=""bibr"" target=""#b9"">[10]</ref>",0
"ck frequency, as well as the larger wire delays caused by modern technologies, prevent branch prediction tables from being accessed in a single cycle <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b6"">7]</ref>. This fact limits fetch engine performance because each branch predictio ease in processor clock frequency, as well as the slower wires in modern technologies, cause branch prediction tables to require multi-cycle accesses <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b6"">7]</ref>.</p><p>The trace predictor <ref type=""bibr"" target=""#b4"">[5]</ref> is a e fetch architecture design. Higher clock frequencies and larger wire delays cause branch prediction tables to require multiple cycles to be accessed <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b6"">7]</ref>, limiting the fetch engine performance. This fact has led to the develop s time from nanoseconds to cycles, we assumed an aggressive 8 fan-out-of-four delays clock period, that is, a 3.47 GHz clock frequency as reported in <ref type=""bibr"" target=""#b0"">[1]</ref>. It has been claimed in <ref type=""bibr"" target=""#b3"">[4]</ref> that 8 fan-out-of-four delays is the optimal c",0
"and return instructions. However, we have found that techniques for enlarging the streams finalizing in particular branch types achieve poor results <ref type=""bibr"" target=""#b17"">[18]</ref>. This is due to Amdahl's law: although these techniques enlarge a set of instructions streams, there are oth",0
"ts that it has. In addition, larger register files have a longer access time, and this may increase the critical path length and penalize performance <ref type=""bibr"" target=""#b0"">[1]</ref>.</p><p>In this paper we propose a novel register renaming approach that significantly reduces the register pre",1
"le.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>Functional Unit Count Latency</head><p>Simple The processor has a lookup-free data cache <ref type=""bibr"" target=""#b6"">[7]</ref> that allows up to 8 pending misses to different cache lines. The cache size is 16 KB, and it is direct-mapped",0
"icular, the two following approaches are the most common solutions to provide the rename storage locations:</p><p>• The entries of the reorder buffer <ref type=""bibr"" target=""#b10"">[11]</ref>. In this case, the result of every instruction is kept in the reorder buffer until it is committed. It is th",0
"with the previous model. Both register renaming schemes are being used in the latest microprocessors. The first one is used by the Intel Pentium Pro <ref type=""bibr"" target=""#b1"">[2]</ref>, the PowerPC 604 <ref type=""bibr"" target=""#b11"">[12]</ref>, and the HAL SPARC64 <ref type=""bibr"" target=""#b2"">",0
"//www.tei-c.org/ns/1.0""><head n=""2."">Register renaming</head><p>Register renaming was first implemented for the floating-point unit of the IBM 360/91 <ref type=""bibr"" target=""#b13"">[14]</ref>. Register renaming is a key issue for the performance of out-of-order execution processors and therefore, it",0
"ry distance is defined as a dynamic quantifiable distance in terms of memory references between two accesses to the same memory location. Fang et al. <ref type=""bibr"" target=""#b9"">[9]</ref> define memory distance to include reuse distance, access distance and value distance. The reuse distance of a th the consideration that a load depends only on the first store in a sequence of stores writing to the same address with the same value. Fang et al. <ref type=""bibr"" target=""#b9"">[9]</ref> propose a feedback-directed mechanism based upon access distance and value distance to determine whether or no evaluations indicate that the store distance based method performs much better than the previous access distance based mem-ory disambiguation scheme <ref type=""bibr"" target=""#b9"">[9]</ref>, and yields performance very close to perfect memory disambiguation, which uses exact knowledge of memory depe outperforms store set algorithm when predictor space is small.</p><p>While the above schemes are based on memory dependence predictions, Fang et al. <ref type=""bibr"" target=""#b9"">[9]</ref> have proposed a feedback-directed memory scheme which use memory distance prediction to determine whether or n and the load as well as the instruction window size, the load/store queue size, and machine state. Taking all these factors into account, Fang et al. <ref type=""bibr"" target=""#b9"">[9]</ref> define the access dis-tance of a load as the number of memory references between the load and the previous sto e memory dependences for inputs other than the profiled one. Our method assumes that small store distances are independent of input size. Fang et al. <ref type=""bibr"" target=""#b9"">[9]</ref> have observed that over 80% of the memory instructions in SPEC CPU2000 have constant memory distances. For sho ce. In cases where the influence of input size upon store distances is not negligible, the memory distance analysis framework proposed by Fang et al. <ref type=""bibr"" target=""#b9"">[9]</ref> can be applied to enhance our mechanism by predicting store distances.</p><p>Finally, we would like to note he different memory disambiguation schemes which are listed in Table <ref type=""table"" target=""#tab_2"">1</ref>(b). For the access distance based scheme <ref type=""bibr"" target=""#b9"">[9]</ref> as described in Section 2.3.2, we use the test and train input sets of SPEC CPU2000 for the two training runs. e store distance has increased with the data size. This last phenomenon can be overcome using the memory distance prediction developed by Fang et al. <ref type=""bibr"" target=""#b9"">[9]</ref>.</p><p>When comparing SD and SS16K, SD suffers from issues related to path-correlated store distance on 168.wu on. Both whole-program <ref type=""bibr"">[7,</ref><ref type=""bibr"" target=""#b28"">28]</ref> and instructionbased <ref type=""bibr"" target=""#b8"">[8,</ref><ref type=""bibr"" target=""#b9"">9,</ref><ref type=""bibr"" target=""#b13"">13]</ref> reuse distances have been predicted accurately across all program input e instruction-based reuse distance analysis in their performance models to calculate cache misses. Fang et al. <ref type=""bibr"" target=""#b8"">[8,</ref><ref type=""bibr"" target=""#b9"">9]</ref> introduce the notion of memory distance to encompass reuse distance, access distance and value distance. They p",1
"s tools to estimate cache misses <ref type=""bibr"" target=""#b1"">[1,</ref><ref type=""bibr"" target=""#b4"">4,</ref><ref type=""bibr"" target=""#b24"">24,</ref><ref type=""bibr"" target=""#b27"">27]</ref> and to evaluate the effect of program transformations <ref type=""bibr"" target=""#b1"">[1,</ref><ref type=""bibr""",0
"re techniques to disambiguate memory references to improve performance <ref type=""bibr"" target=""#b5"">[5,</ref><ref type=""bibr"" target=""#b10"">10,</ref><ref type=""bibr"" target=""#b11"">11,</ref><ref type=""bibr"" target=""#b15"">15,</ref><ref type=""bibr"" target=""#b16"">16,</ref><ref type=""bibr"" target=""#b17"" presence of a large number of unissued store instructions in the instruction window.</p><p>Work in this area has produced increasingly better results <ref type=""bibr"" target=""#b11"">[11,</ref><ref type=""bibr"" target=""#b16"">16,</ref><ref type=""bibr"" target=""#b15"">15,</ref><ref type=""bibr"" target=""#b5"" a store and its dependent load. They do not apply their technique to memory disambiguation. Various patents <ref type=""bibr"" target=""#b23"">[23,</ref><ref type=""bibr"" target=""#b11"">11]</ref> also exist that are aimed at identifying those loads and stores that cause memory order violations and synchr",0
"er collect reuse distance distribution for memory instructions through one profiling run to generate cache replacement hints for an Itanium processor <ref type=""bibr"" target=""#b3"">[3]</ref>. Marin and Mellor-Crummey <ref type=""bibr"" target=""#b13"">[13]</ref> incorporate instruction-based reuse distan",0
"21064, the recorded PC corresponds to the instruction that is at the head of the instruction queue 6cycles after the one that triggered the overflow <ref type=""bibr"" target=""#b11"">[12]</ref>. On an Intel Core 2 machine, we observed a similar phenomenon. The reported PC corresponds to the instructio more promising approach.</p><p>AMD processors, on the other hand, provide instructionbased sampling (IBS) which is similar to the ProfileMe approach <ref type=""bibr"" target=""#b11"">[12]</ref>. Unfortunately, this facility only allows sam-pling instructions fetched (which include instructions on misp rdware has also been proposed to facilitate PMU-based profiling. ProfileMe was proposed hardware support to allow accurate instruction-level sampling <ref type=""bibr"" target=""#b11"">[12]</ref> for Alpha processors. AMD adopts the ProfileMe approach in the Opteron processors. As discussed in Section 3",1
"significantly higher than compiler instrumentation, binary instrumentation is hard to deploy in actual production environments. Synchronous sampling <ref type=""bibr"" target=""#b3"">[4]</ref>, <ref type=""bibr"" target=""#b23"">[24]</ref> using instrumentation has been proposed to reduce overhead. However",0
"ired (which are at a finer granularity than ISA instructions). Since the number of ?ops per instruction is unknown, using IBS also proves problematic <ref type=""bibr"" target=""#b12"">[13]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.2.3"">Multi-Instruction Retirement</head><p>On",0
"ng techniques to collect them. Though researchers have proposed using PMU-based sample profiles to drive other optimizations such as data prefetching <ref type=""bibr"" target=""#b0"">[1]</ref>, these techniques are mainly adopted in dynamic optimizers, which are not the focus of this paper. However, ou",0
"ead and potential inaccuracy introduced by replacement policy. Sampling based value profiling is proposed to pursue better efficiency and flexibility <ref type=""bibr"" target=""#b6"">[7]</ref>. However, it still incurs an average overhead of around 10%.</p><p>Not surprisingly, performance counter sampl",0
"bp1.5 2.1 Overview</head><p>Predictor cbp1.5 is a global-history based predictor derived from PPM. PPM was originally introduced for text compression <ref type=""bibr"" target=""#b0"">[1]</ref>, and it was used in <ref type=""bibr"" target=""#b1"">[2]</ref> for branch prediction. Figure <ref type=""figure"" t the branch ending the sequence was taken. At the end of the PCF, we build S * and sort sequences in decreasing potentials. Then we compute m(T ) from <ref type=""bibr"" target=""#b0"">(1)</ref>. Because of memory limitations on our machines, when |S| exceeded a certain size, we decreased N . However, as </ref> Step 5 is the biggest qualitative step. Starting at this step, the content of T is allowed to change dynamically. We can no longer use formula <ref type=""bibr"" target=""#b0"">(1)</ref>, and so now we generate results by performing a second pass on the traces. We use the 4 global banks of cbp1.5",1
"<ref type=""figure"" target=""#fig_0"">1</ref> shows a synopsis of cbp1.5, which features 5 banks. It can be viewed as a 4 th order approximation to PPM <ref type=""bibr"" target=""#b2"">[3]</ref>, while YAGS <ref type=""bibr"" target=""#b3"">[4]</ref>, which is a GPPM predictor too, can be viewed as a 1 st or mplemented by taking advantage of the fact that we are not folding a random value, but a global history value derived from the previous history value <ref type=""bibr"" target=""#b2"">[3]</ref>. Figure <ref type=""figure"" target=""#fig_1"">2</ref> shows two examples of how global history folding can be imp",0
"tory based predictor derived from PPM. PPM was originally introduced for text compression <ref type=""bibr"" target=""#b0"">[1]</ref>, and it was used in <ref type=""bibr"" target=""#b1"">[2]</ref> for branch prediction. Figure <ref type=""figure"" target=""#fig_0"">1</ref> shows a synopsis of cbp1.5, which fea",0
"false"" mispredictions on sequences u for which there exists a longer matching sequence in T , i.e., s∈T f (s.0 u ). More explanations can be found in <ref type=""bibr"" target=""#b6"">[7]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.3"">A heuristic for set T</head><p>We assume |T",0
"synopsis of cbp1.5, which features 5 banks. It can be viewed as a 4 th order approximation to PPM <ref type=""bibr"" target=""#b2"">[3]</ref>, while YAGS <ref type=""bibr"" target=""#b3"">[4]</ref>, which is a GPPM predictor too, can be viewed as a 1 st order approximation. The leftmost bank on Figure <ref",0
"target=""#b8"">[9]</ref> [19], SPSM <ref type=""bibr"" target=""#b4"">[5]</ref>, Superthreaded <ref type=""bibr"" target=""#b19"">[20]</ref>, Trace Processors <ref type=""bibr"" target=""#b16"">[17]</ref> [21], Speculative Multithreaded <ref type=""bibr"" target=""#b11"">[12]</ref> and Dynamic Multithreaded <ref typ",1
", Superthreaded <ref type=""bibr"" target=""#b19"">[20]</ref>, Trace Processors <ref type=""bibr"" target=""#b16"">[17]</ref> [21], Speculative Multithreaded <ref type=""bibr"" target=""#b11"">[12]</ref> and Dynamic Multithreaded <ref type=""bibr"" target=""#b0"">[1]</ref>. In such architectures, each cluster execu",0
"cessors <ref type=""bibr"" target=""#b16"">[17]</ref> [21], Speculative Multithreaded <ref type=""bibr"" target=""#b11"">[12]</ref> and Dynamic Multithreaded <ref type=""bibr"" target=""#b0"">[1]</ref>. In such architectures, each cluster executes a different thread of control, all except one being speculative.",0
"is based on the proposal made by Palacharla and Smith <ref type=""bibr"" target=""#b15"">[16]</ref> and also investigated by Sastry, Palacharla and Smith <ref type=""bibr"" target=""#b17"">[18]</ref>, which extends a conventional microarchitecture in order to allow simple integer and logic instructions to b e memory access LD/ ST. The backward slice of an RDG with respect to a node v is defined as the set of nodes from which v can be reached, including v <ref type=""bibr"" target=""#b17"">[18]</ref>. Figure <ref type=""figure"">2</ref> shows the backward slice with respect to node 13 of the sample RDG.</p><p n are executed in the same cluster. On the other hand, its hardware complexity is negligible.</p><p>The static partitioning proposed by Sastry et al. <ref type=""bibr"" target=""#b17"">[18]</ref> is based on sending to the integer cluster all instructions that belong to the subgraph defined by the LdSt refer to this dynamic partitioning scheme as LdSt slice steering. The numbers for the static partitioning have been obtained from the original paper <ref type=""bibr"" target=""#b17"">[18]</ref> and the dynamic approach has been simulated using the same compiler, the same compiler options, the same ben 62 communications.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4."">Related Work</head><p>The proposal of Sastry, Palacharla and Smith <ref type=""bibr"" target=""#b17"">[18]</ref> and that of Palacharla, Jouppi and Smith <ref type=""bibr"" target=""#b14"">[15]</ref> are two partitioning sche",0
"ply that every cluster should have any type of functional unit. However, since FP applications are rich in integer instructions, Palacharla and Smith <ref type=""bibr"" target=""#b15"">[16]</ref> addressed this drawback by proposing a more costeffective approach based on extending the FP data-path with s/1.0""><head n=""2."">Processor Microarchitecture</head><p>The target processor microarchitecture is based on the proposal made by Palacharla and Smith <ref type=""bibr"" target=""#b15"">[16]</ref> and also investigated by Sastry, Palacharla and Smith <ref type=""bibr"" target=""#b17"">[18]</ref>, which exten",0
"y. However, ULB-LSQs can also provide efficient memory disambiguation for largewindow processors, in which thousands of instructions may be in flight <ref type=""bibr"" target=""#b12"">[12,</ref><ref type=""bibr"" target=""#b14"">14,</ref><ref type=""bibr"" target=""#b24"">24]</ref>, by exploiting the ability t",1
"om a L0 cache. In the second phase, the memory instructions are Subramaniam and Loh <ref type=""bibr"" target=""#b4"">[4]</ref> and Sha, Martin, and Roth <ref type=""bibr"" target=""#b28"">[28]</ref> both propose methods for completely eliminating the store queue, by bypassing store values through the regis e, if slots 25, 28 and 29 are occupied in the LSQ, linearly scanning the the LSQ from position 29 will reveal the most recent older instruction first <ref type=""bibr"" target=""#b28"">(28)</ref> and then then next oldest <ref type=""bibr"" target=""#b25"">(25)</ref> and so on. In some cases, circuit implem",0
"eads of large LSQs. The penalty associated with these schemes correspond to different ""load loops"" and changes as the time for load execution changes <ref type=""bibr"" target=""#b11"">[11]</ref>.</p><p>These buffering approaches effectively stall processing of certain memory instructions, which could p",0
"cefully tolerates the rare overflow conditions with minimum effect on performance.</p><p>Related Work: Research proposals for clustered architectures <ref type=""bibr"" target=""#b30"">[30,</ref><ref type=""bibr"">9]</ref> employ multiple partitions of an age-indexed LSQ, but instead of reserving a slot i",0
"pe=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b29"">30,</ref><ref type=""bibr"" target=""#b36"">37]</ref>, is that repetitive control flow graph traversals lead to recurring instruction-cache miss sequences, much li lly requesting additional addresses distinguishes temporal streaming <ref type=""bibr"" target=""#b34"">[35,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" target=""#b36"">37]</ref> from prefetching approaches that only retrieve a constant number of blocks in response to a miss (e.g., <ref can only discover stream length after the fact, by counting successful prefetches. Hence, like past designs <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b36"">37]</ref>, TIFS uses the next-best option, the Recent heuristic, as it is easy to implement in hardware while still yie ior address-correlated prefetching proposals that target data accesses <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b36"">37]</ref>. TIFS adds three logical structures to the chip: a set of Streamed Value Buffers (SVBs), one per L1-I cache; rs</head><p>Figure <ref type=""figure"">9</ref> depicts the anatomy of the SVB. Our SVB design is adapted from <ref type=""bibr"" target=""#b34"">[35,</ref><ref type=""bibr"" target=""#b36"">37]</ref>. The SVB contains a small fully-associative buffer for temporary storage of streamed blocks. Each entry inclu pe=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b29"">30,</ref><ref type=""bibr"" target=""#b36"">37]</ref>. These prefetchers target primarily off-chip data references and require large off-chip tables to capture pre ected, a simple ratematching mechanism enables the prefetcher to retrieve instruction-cache blocks ahead of the fetch unit for the rest of the stream <ref type=""bibr"" target=""#b36"">[37]</ref>.</p><p>Figure <ref type=""figure"">5</ref> shows the cumulative distribution of recurring stream lengths as id >, a technique for storing prefetcher meta-data in the L2 cache (see <ref type=""bibr"">Section 5)</ref>.</p><p>The term temporal stream, introduced in <ref type=""bibr"" target=""#b36"">[37]</ref>, refers to extended sequences of data references that recur over the course of program execution. Similar re",1
"br"" target=""#b5"">[6]</ref>, off-chip data misses <ref type=""bibr"" target=""#b34"">[35,</ref><ref type=""bibr"" target=""#b35"">36]</ref>, and program paths <ref type=""bibr"" target=""#b15"">[16]</ref>, we use the SEQUITUR <ref type=""bibr"" target=""#b9"">[10]</ref> hierarchical data compression algorithm to ide s that recur over the course of program execution. Similar repetition in the sequence of basic blocks visited by a program has been reported by Larus <ref type=""bibr"" target=""#b15"">[16]</ref> and underlies trace scheduling <ref type=""bibr"" target=""#b8"">[9]</ref> and trace caches <ref type=""bibr"" tar ng (TPC-C)</head></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>Oracle</head><p>Oracle 10g Enterprise Database Server, 100 warehouses (10 GB), <ref type=""bibr"" target=""#b15"">16</ref>   </p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.4"">Stream lookup heuristics</head><p>The SEQU",0
"n recognized by computer architects. Sequential instruction (next-line) prefetching was implemented in the IBM System 360 Model 91 in the late 1960's <ref type=""bibr"" target=""#b1"">[2]</ref>, and prefetching into caches was analyzed by Smith in the late 1970's <ref type=""bibr"" target=""#b28"">[29]</ref",0
"the context of a four-core chip multiprocessor. We study a system with aggressive, out-of-order cores and decoupled front-end instruction fetch units <ref type=""bibr"" target=""#b24"">[25]</ref> which partially hide instruction-fetch latency in commercial server workloads. We expect the impact of TIFS",0
"h recurring sequences, which we call temporal instruction streams. In contrast to the instruction streams in <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b26"">27]</ref>, which comprise a sequence of contiguous basic blocks, temporal instr r"" target=""#b25"">[26]</ref>. Temporal instruction streams differ from previously-defined instruction streams <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b26"">27]</ref> in two key respects: (1) temporal instruction streams are defined at nstruction-stream prefetching generalizes the notion of the next-line instruction prefetcher to arbitrary-length sequences of contiguous basic blocks <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b26"">27]</ref>.</p><p>A greater challenge lies in prefetching at fetch discontinuit",0
"ses prediction latency and memory traffic, which reduces the effectiveness of prefetching.</p><p>Recent solutions use the Global History Buffer (GHB) <ref type=""bibr"" target=""#b27"">[28]</ref>, which organizes correlation information by storing recent memory accesses in a time-ordered circular histor et=""#b36"">37]</ref>.</p><p>Nesbit and Smith introduce the GHB as a general structure for prefetching streams of temporally correlated memory requests <ref type=""bibr"" target=""#b27"">[28]</ref>. However, when used to record address correlation <ref type=""bibr"" target=""#b41"">[42]</ref>, the GHB is quit ys.</p><p>Third, we simulate Nesbit and Smith's PC/DC prefetcher, which which learns the deltas, or differences, between consecutive memory addresses <ref type=""bibr"" target=""#b27"">[28]</ref>. Delta correlation allows PC/DC to store all meta-data on chip, so this prefetcher can realistically train o target=""#b37"">[38]</ref>.</note> 			<note xmlns=""http://www.tei-c.org/ns/1.0"" place=""foot"" n=""2"" xml:id=""foot_1"">Using Nesbit and Smith's terminology<ref type=""bibr"" target=""#b27"">[28]</ref>, in which the name before the slash describes the reference scheme and the name after the slash describes th hat segregates the prefetcher input into multiple streams based on the PC of the loading instruction, which is known to improve coverage and accuracy <ref type=""bibr"" target=""#b27"">[28,</ref><ref type=""bibr"" target=""#b38"">39,</ref><ref type=""bibr"" target=""#b37"">38,</ref><ref type=""bibr"" target=""#b24 w that the ISB is superior to two other recent prefetchers, SMS <ref type=""bibr"" target=""#b38"">[39]</ref>, which exploits spatial locality, and PC/DC <ref type=""bibr"" target=""#b27"">[28,</ref><ref type=""bibr"" target=""#b12"">13]</ref>, which uses delta correlation instead of address correlation.</p><p> 75× <ref type=""bibr"" target=""#b42"">[43]</ref> for long streams. Rather than use address correlation, other GHBbased prefetchers use delta correlation <ref type=""bibr"" target=""#b27"">[28,</ref><ref type=""bibr"" target=""#b26"">27]</ref>, whose space requirements are dramatically smaller, but we show that o improve the accuracy and coverage of correlation-based prefetchers <ref type=""bibr"" target=""#b28"">[29,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b38"">39,</ref><ref type=""bibr"" target=""#b37"">38]</ref>, but until now, the combinati ased designs forsake either PC localization <ref type=""bibr"" target=""#b42"">[43]</ref> or address correlation <ref type=""bibr"" target=""#b26"">[27,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b11"">12]</ref>, sacrificing significant coverage for design simplicity.</p><formula",1
"he application's memory footprint, the fundamental challenge for these prefetchers is the management of megabytes of off-chip correlation information <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b40"">41,</ref><ref type=""bibr"" target=""#b42"">43]</ref>. Access to this off-chip meta- f solutions store the Markov table off-chip and optimize the memory bandwidth requirements and prefetch look-ahead distance for off-chip table access <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b36"">37]</ref>.</p><p>Nesbit and Smith introduce the GHB as a general structure for p nd updating the off-chip history buffer and index table <ref type=""bibr"" target=""#b42"">[43]</ref>. These techniques reduce the memory traffic from 3× <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b36"">37,</ref><ref type=""bibr"" target=""#b43"">44]</ref> to 1.05-1.75× <ref type=""bibr"" equire multiple table lookups to prefetch temporal streams. To reduce the number of table lookups, each table entry could store a fixed-length stream <ref type=""bibr"" target=""#b7"">[8]</ref>, but because temporal stream lengths vary widely from two to several hundred <ref type=""bibr"" target=""#b5"">[6, prefetchers, such as STMS <ref type=""bibr"" target=""#b41"">[42]</ref>, which itself represented an order of magnitude improvement over its predecessors <ref type=""bibr"" target=""#b7"">[8]</ref>.</p><p>2. It improves coverage and accuracy by supporting the combination of PC localization and address corre",0
"tions to accurately capture all phases of the benchmark. The SimPoints are generated using the SimPoint Tool <ref type=""bibr"" target=""#b30"">[31,</ref><ref type=""bibr"" target=""#b15"">16]</ref>. We choose a SimPoint length of 250 million instruction because it is large enough to capture long-range beha",0
"r address' structural address. In our above example, an access to physical address 0xba1f00 is translated to structural address 0x1100 by the PS-AMC. <ref type=""bibr"" target=""#b1"">(2)</ref> The Stream Predictor predicts the next consecutive structural addresses to prefetch, which for degree 1 prefet ad n=""4.4"">Off-chip Storage</head><p>To organize the ISB's off-chip meta-data, we use the Predictor Virtualization framework proposed by Burcea at al <ref type=""bibr"" target=""#b1"">[2]</ref>. In particular, we use a dedicated region of physical memory to maintain the mapping from the physical to the",0
"mprove coverage and accuracy <ref type=""bibr"" target=""#b27"">[28,</ref><ref type=""bibr"" target=""#b38"">39,</ref><ref type=""bibr"" target=""#b37"">38,</ref><ref type=""bibr"" target=""#b24"">25]</ref>. In particular, the ISB can combine PC localization and address correlation because any PC-localized temporal ed on Spatial Locality.</p><p>Irregular memory accesses can also be prefetched by detecting spatial locality <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b4"">5]</ref>. Variations of the Spatial Locali y.</p><p>PC localization has been used to improve the accuracy and coverage of correlation-based prefetchers <ref type=""bibr"" target=""#b28"">[29,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b38"">39,</ref><ref type=""bibr"" target=""#b37""",0
"The rest of the principal O(m) is the group of m by m orthonormal matrices. We refer the readers to <ref type=""bibr"" target=""#b24"">(Wong, 1967;</ref><ref type=""bibr"" target=""#b0"">Absil et al., 2004)</ref> for details on the Riemannian geometry of the space.</p><p>angles are similarly defined. It is",1
"ure such as triangle inequality, for example, we can speed up the nearest neighbor searches by estimating lower and upper limits of unknown distances <ref type=""bibr"" target=""#b4"">(Faragó et al., 1993)</ref>.</p><p>From this point of view, the max correlation is not a metric and may not be used with",0
".tei-c.org/ns/1.0""><head n=""1.1."">Contributions of the Paper</head><p>Although the Projection metric and the Binet-Cauchy metric were previously used <ref type=""bibr"" target=""#b1"">(Chang et al., 2006;</ref><ref type=""bibr"" target=""#b23"">Wolf &amp; Shashua, 2003)</ref>, their potential for subspace-b",0
"rg/ns/1.0""><head n=""6.2."">Testing Illumination-Invariance with Yale Face Database</head><p>The Yale face database and the Extended Yale face database <ref type=""bibr"" target=""#b7"">(Georghiades et al., 2001)</ref> together consist of pictures of 38 subjects with 9 different poses and 45 different lig",0
"resources to best fit the varying structure of the running program. Although previous work has quantified the theoretical benefits of high adaptivity <ref type=""bibr"" target=""#b0"">[1]</ref>, predicting and delivering this adaptation is still an open and challenging problem. The key question is how t represents the design space of a high-performance out-of-order superscalar processor and is similar to spaces that other researchers have considered <ref type=""bibr"" target=""#b0"">[1]</ref>. We vary fourteen different microarchitectural parameters across a range of values, giving a total design spac gnificant.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>IX. PRIOR WORK ON MICROARCHITECTURAL ADAPTIVITY</head><p>Recently, Lee and Brooks <ref type=""bibr"" target=""#b0"">[1]</ref> showed that it is possible to significantly increase processor energy efficiency by adapting it as a program i",1
"=""#b14"">[15]</ref>. Later Sankaralingam et. al. proposed the TRIPS architecture <ref type=""bibr"" target=""#b32"">[33]</ref>, Ipek et. al. ""Core Fusion"" <ref type=""bibr"" target=""#b8"">[9]</ref> and Tarjan et al. ""Core Federation"" <ref type=""bibr"" target=""#b9"">[10]</ref>. These last two approaches merge",0
"se the microarchitectural configuration with the largest structures and the highest level of branch speculation (named the profiling configuration).  <ref type=""bibr"" target=""#b16"">[17]</ref>, <ref type=""bibr"" target=""#b17"">[18]</ref> Total 627bn</p><p>For each program phase we gather hardware count i, et. al.</ref> separately proposed variable latency architectures where additional stages can be added to the pipeline to combat process variations <ref type=""bibr"" target=""#b16"">[17]</ref>, <ref type=""bibr"" target=""#b17"">[18]</ref>.</p><p>However, these studies considered only a limited adaptivit",0
"table I summarises this information. In particular the issue queue <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b2"">[3]</ref>, <ref type=""bibr"" target=""#b10"">[11]</ref>, <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" target=""#b12"">[13]</ref>, <ref type=""bibr"" targ ref>, <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" target=""#b12"">[13]</ref>, <ref type=""bibr"" target=""#b29"">[30]</ref>, reorder buffer <ref type=""bibr"" target=""#b10"">[11]</ref>, <ref type=""bibr"" target=""#b11"">[12]</ref>, register files <ref type=""bibr"" target=""#b10"">[11]</ref>, <ref t =""bibr"" target=""#b29"">[30]</ref>, reorder buffer <ref type=""bibr"" target=""#b10"">[11]</ref>, <ref type=""bibr"" target=""#b11"">[12]</ref>, register files <ref type=""bibr"" target=""#b10"">[11]</ref>, <ref type=""bibr"" target=""#b11"">[12]</ref>, pipeline <ref type=""bibr"" target=""#b15"">[16]</ref>, <ref type=""b",0
"represents the trade-offs between power and performance, or the efficiency of each design point. It is widely used within the architecture community <ref type=""bibr"" target=""#b25"">[26]</ref> to indicate how efficient a configuration is at converting energy into processing speed.</p></div> <div xmln",0
"still retaining the ability to do offline code generation.</p><p>Unlike other trace-driven runtime optimizers for native binary code, such as Dynamo <ref type=""bibr"" target=""#b3"">[4]</ref>, we have both the rich V-ISA and a cooperating code generator. Our V-ISA provides us with ability to perform s ftware and hardware techniques for transparent dynamic optimization of programs. Transmeta's CMS <ref type=""bibr"" target=""#b10"">[11]</ref> and Dynamo <ref type=""bibr"" target=""#b3"">[4]</ref> identify and optimize hot traces at runtime, similar to our re-optimization strategy but without the benefits",1
"ome subset of these types (if any), including optimizations that require array dependence analysis, pointer analysis (even field-sensitive algorithms <ref type=""bibr"" target=""#b15"">[16]</ref>), and call graph construction.</p><p>All instructions in the V-ISA have strict type rules, and most are over",0
"le"" target=""#tab_1"">2</ref> presents metrics to evaluate the low-level nature of the LLVA V-ISA. The benchmarks we use include the PtrDist benchmarks <ref type=""bibr"" target=""#b1"">[2]</ref> and the SPEC CINT2000 benchmarks (we omit three SPEC codes because their LLVAobject code versions fail to link",0
"powerful interprocedural transformation that uses Data Structure Analysis to partition the heap into separate pools for each data structure instance <ref type=""bibr"" target=""#b24"">[25]</ref>. Finally, we have shown that the LLVA representation is rich enough to perform complete, static analysis of",0
"lass of architectures we term Virtual Instruction Set Computers (VISC) <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b31"">32]</ref>. Such an architecture defines a virtual instruction set (called the V proposal for Codesigned Virtual Machines in the Strata project <ref type=""bibr"" target=""#b31"">[32]</ref>, and Transmeta's Crusoe family of processors <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b10"">11]</ref>. All of these distinguish the virtual and physical ISAs as a fundame any offline translation.</p><p>Transmeta's Crusoe uses a dynamic translation scheme to emulate Intel IA-32 instructions on a VLIW hardware processor <ref type=""bibr"" target=""#b22"">[23]</ref>. The hardware includes important supporting mechanisms such as shadowed registers and a gated store buffer f",0
"every three years versus 2X every two years), and the DIMM count per channel is declining (e.g., two DIMMs per channel on DDR3 versus eight for DDR) <ref type=""bibr"" target=""#b5"">[5]</ref>. Figure <ref type=""figure"" target=""#fig_0"">1</ref>(a) aggregates these trends to show historical and extrapola",0
"protocols <ref type=""bibr"">[14][17]</ref>; changes to the host operating system and device drivers <ref type=""bibr"" target=""#b11"">[12]</ref>[13][14] <ref type=""bibr"" target=""#b15"">[16]</ref>; reduced reliability in the face of remote server crashes <ref type=""bibr"">[13][16]</ref>; and/or impractica",0
"s. On the demand side, current trends point to increased number of cores per socket, with some studies predicting a two-fold increase every two years <ref type=""bibr"" target=""#b1"">[1]</ref>. Concurrently, we are likely to see an increased number of virtual machines (VMs) per core (VMware quotes 2-4X",0
".</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>B. FIFO Queue Benchmark</head><p>A simple FIFO queue implementation was written based upon <ref type=""bibr"" target=""#b7"">[8]</ref>. The queue is based around a ring buffer, with read and write pointers pointing to the item at the top of the any given time nothing beyond normal load and store operations are needed (i.e. we do not need CAS). For MIPS64 this has been implemented exactly as <ref type=""bibr"" target=""#b7"">[8]</ref>, for Mamba a simple modification was made. The FIFO ring buffer starts with all its presence bits unset. When",1
"otherwise this can be accomplished via a lightweight construct we call a 'notify chain'. These are similar in structure to MCS queue based spin-locks <ref type=""bibr"" target=""#b6"">[7]</ref> with one crucial difference. Rather than spinning on a particular value waiting for it to become 1, the presen e same in both cases so are not considered here.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>A. MCS Lock Benchmark</head><p>The MCS Lock <ref type=""bibr"" target=""#b6"">[7]</ref> is a standard way of implementing a scalable spin-lock. Any thread wishing to enter the lock first constructs spin does not generate any needless remote memory accesses.</p><p>We have implemented the MCS lock on MIPS64 using the CAS primitive as described in <ref type=""bibr"" target=""#b6"">[7]</ref>. In Mamba a notify chain as described above is used. When a thread attempts to acquire a lock it simply waits",0
"ication by choosing what data is shared between threads. Much effort has been made to optimize coherency protocols to reduce needless communication ( <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b2"">[3]</ref>). Furthermore, in order to achieve good performance sequential consis",0
"complexity, is preferable and the costlier global cross-chip communcations can be made architecturally explicit, allowing software to optimize for it <ref type=""bibr"" target=""#b0"">[1]</ref>.</p><p>Most CMPs utilize shared memory along with a cache coherency protocol to allow communication between se",0
"a thread simply spins until it is, eventually trapping to a software handler if this takes too long. Mamba builds on earlier concepts from Cambridge <ref type=""bibr"" target=""#b12"">[13]</ref>.</p><p>The Intel x86 ISA <ref type=""bibr"" target=""#b13"">[14]</ref> offers an MWAIT instruction, which combin",0
"rk suite that ""focuses on emerging workloads and was designed to be representative of next-generation sharedmemory programs for chip-multiprocessors"" <ref type=""bibr"" target=""#b2"">[3]</ref>. Our experiments show that for those programs, no matter how the threads are placed on cores (they may share t the measurement of times and hardware performance.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2.1"">Benchmarks</head><p>We use PARSEC <ref type=""bibr"" target=""#b2"">[3]</ref> as the benchmark suite. It is a recently released suite designed for CMP research. The suite includes emerging h the Black-Scholes partial differential equation. Because there is no close-form expression for the equation, the program uses numerical computation <ref type=""bibr"" target=""#b2"">[3]</ref>.</p><p>The input data file of this benchmark includes an array of options. The program computes the price for >video encoding</cell><cell>pipeline</cell><cell>16MB</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>* : see<ref type=""bibr"" target=""#b2"">[3]</ref> for detail.</note></figure> <figure xmlns=""http://www.tei-c.org/ns/1.0"" type=""table"" xml:id=""tab_1""><head>Tabl applications in recognition, mining and synthesis, as well as systems applications that mimic large-scale multithreaded commercial programs. Studies <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b1"">2]</ref> have shown that the suite covers a wide range of working set sizes, and 0 {1 5 0 4},{3 7 2 6},{1 5 0 4},{3 7 2 6} 3 161.1 0 {0 2 4 6},{1 3 5 7},{0 2 4 6},{1 3 5 7} 1 161.6 0 {0 2 1 3},{4 5 6 7},{0 2 1 3},{4 5 6 7} 4 161. <ref type=""bibr"" target=""#b2"">3</ref> No binding 165.7</p><p>In PARSEC, ferret and dedup are two such programs. The program ferret is a search engine, on workload characterization and performance measurement are relevant to this current work. Bienia and others <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b2"">3]</ref> have shown a detailed exploration of the characterization of the PARSEC benchmark suite on CMP. Because their g",1
"ype=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b20"">21]</ref>, are on multiprogramming environments, attempting to alleviate shared-c",0
"""#b25"">[26]</ref>), or have concentrated on a specific class of applications, such as server programs <ref type=""bibr"" target=""#b22"">[23]</ref>; some <ref type=""bibr"" target=""#b11"">[12]</ref> have used old CMP machines with no shared cache equipped. These limitations may not be critical for the part aracteristics of the benchmarks, their measurement runs on simlarge input only, and uses a CMP simulator rather than actual machines. Liao and others <ref type=""bibr"" target=""#b11"">[12]</ref> examine the performance of OpenMP applications on a Sun Fire V490 machine with private cache only. Tuck and",0
"shared cache by the other thread.</p><p>We notice that one may improve data locality inside a thread using traditional unroll-and-jam transformation <ref type=""bibr"" target=""#b0"">[1]</ref>. The transformed code is shown in Figure <ref type=""figure"" target=""#fig_7"">7 (c</ref>). (In our implementatio",0
"of contemporary multithreaded applications. Many previous explorations <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b19"">20]</ref> are concentrated on co-runs of in scheduling including thread clustering. Many job co-scheduling studies <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" target=""#b5"">6,</",0
"computer. Worse, iterative solvers do not exhibit the data reuse patterns that would make them well-suited to an efficient out-ofcore implementation <ref type=""bibr"" target=""#b28"">[Toledo 1999</ref>], since iterations computed only within a local area of the domain may not make significant progress",0
"seam selection and gradient-domain compositing for removing any artifacts remaining at the seams. This approach has been used to fill holes in images <ref type=""bibr"" target=""#b11"">[Hays and Efros 2007]</ref> and to compute multi-viewpoint <ref type=""bibr"" target=""#b2"">[Agarwala et al. 2006</ref>] a",0
"ue, and <ref type=""bibr"" target=""#b12"">Jia et al. [2006]</ref> improved on this basic approach by first optimizing the boundary of the copied region. <ref type=""bibr"" target=""#b0"">Agarwala et al. [2004]</ref> extended gradient-domain compositing to the case of compositing an entire image from region the colors at the boundaries are fixed from image I B . When simultaneously compositing multiple regions from multiple images, the simplest approach <ref type=""bibr"" target=""#b0"">[Agarwala et al. 2004</ref>] is to use the gradient of the source image between any two pixels inside of one region, and",0
"ield; recent examples include high dynamic range (HDR) compression <ref type=""bibr"" target=""#b7"">[Fattal et al. 2002]</ref>, intrinsic image recovery <ref type=""bibr"" target=""#b30"">[Weiss 2001</ref>], shadow removal <ref type=""bibr"" target=""#b8"">[Finlayson and Drew 2002]</ref>, flash artifact correc",0
"optimization (RCPO) <ref type=""bibr"" target=""#b10"">[11]</ref>, <ref type=""bibr"" target=""#b24"">[24]</ref>, <ref type=""bibr"" target=""#b20"">[21]</ref>, <ref type=""bibr"" target=""#b5"">[6]</ref> or devirtualization <ref type=""bibr"" target=""#b28"">[28]</ref>. This optimization statically converts an indire t-oriented languages <ref type=""bibr"" target=""#b10"">[11]</ref>, <ref type=""bibr"" target=""#b24"">[24]</ref>, <ref type=""bibr"" target=""#b20"">[21]</ref>, <ref type=""bibr"" target=""#b5"">[6]</ref>, <ref type=""bibr"" target=""#b28"">[28]</ref>. Ishizaki et al. <ref type=""bibr"" target=""#b28"">[28]</ref> classify can convert the indirect call to multiple guarded direct calls <ref type=""bibr"" target=""#b20"">[21]</ref>, <ref type=""bibr"" target=""#b17"">[18]</ref>, <ref type=""bibr"" target=""#b5"">[6]</ref>, as shown in Fig. <ref type=""figure"" target=""#fig_0"">11b</ref>. This compiler optimization is called Receiver =""figure"" target=""#fig_0"">11</ref>. A virtual function call and its devirtualized form. usually has higher accuracy than an indirect branch predictor <ref type=""bibr"" target=""#b5"">[6]</ref>. However, not all indirect calls can be converted to multiple conditional branches. In order to perform RCPO, to multiple conditional branches. In order to perform RCPO, the following conditions need to be fulfilled <ref type=""bibr"" target=""#b17"">[18]</ref>, <ref type=""bibr"" target=""#b5"">[6]</ref>:</p><p>1. The number of frequent target addresses from a caller site should be small (1-2). 2. The majority of",1
"imization, called receiver class prediction optimization (RCPO) <ref type=""bibr"" target=""#b10"">[11]</ref>, <ref type=""bibr"" target=""#b24"">[24]</ref>, <ref type=""bibr"" target=""#b20"">[21]</ref>, <ref type=""bibr"" target=""#b5"">[6]</ref> or devirtualization <ref type=""bibr"" target=""#b28"">[28]</ref>. This thod call with direct method calls in object-oriented languages <ref type=""bibr"" target=""#b10"">[11]</ref>, <ref type=""bibr"" target=""#b24"">[24]</ref>, <ref type=""bibr"" target=""#b20"">[21]</ref>, <ref type=""bibr"" target=""#b5"">[6]</ref>, <ref type=""bibr"" target=""#b28"">[28]</ref>. Ishizaki et al. <ref ty either an instance of the Rectangle class or the Circle class at runtime, the compiler can convert the indirect call to multiple guarded direct calls <ref type=""bibr"" target=""#b20"">[21]</ref>, <ref type=""bibr"" target=""#b17"">[18]</ref>, <ref type=""bibr"" target=""#b5"">[6]</ref>, as shown in Fig. <ref t",1
"ion algorithm is inspired by a compiler optimization, called receiver class prediction optimization (RCPO) <ref type=""bibr"" target=""#b10"">[11]</ref>, <ref type=""bibr"" target=""#b24"">[24]</ref>, <ref type=""bibr"" target=""#b20"">[21]</ref>, <ref type=""bibr"" target=""#b5"">[6]</ref> or devirtualization <ref ation is the substitution of an indirect method call with direct method calls in object-oriented languages <ref type=""bibr"" target=""#b10"">[11]</ref>, <ref type=""bibr"" target=""#b24"">[24]</ref>, <ref type=""bibr"" target=""#b20"">[21]</ref>, <ref type=""bibr"" target=""#b5"">[6]</ref>, <ref type=""bibr"" target 80 <ref type=""bibr"" target=""#b10"">[11]</ref>, polymorphic inline caches <ref type=""bibr"" target=""#b23"">[23]</ref>, and type feedback/devirtualization <ref type=""bibr"" target=""#b24"">[24]</ref>, <ref type=""bibr"" target=""#b28"">[28]</ref>. As we show in Section 6, the benefit of devirtualization is limi",1
"f>, <ref type=""bibr"" target=""#b24"">[24]</ref>, <ref type=""bibr"" target=""#b20"">[21]</ref>, <ref type=""bibr"" target=""#b5"">[6]</ref> or devirtualization <ref type=""bibr"" target=""#b28"">[28]</ref>. This optimization statically converts an indirect branch to 2. With a 64-bit address space, a conventional or accurate profiling, and it is applicable to only a subset of indirect branches with a limited number of targets that can be determined statically <ref type=""bibr"" target=""#b28"">[28]</ref>. Our proposed VPC prediction mechanism provides the benefit of using conditional branch predictors for indir get=""#b10"">[11]</ref>, <ref type=""bibr"" target=""#b24"">[24]</ref>, <ref type=""bibr"" target=""#b20"">[21]</ref>, <ref type=""bibr"" target=""#b5"">[6]</ref>, <ref type=""bibr"" target=""#b28"">[28]</ref>. Ishizaki et al. <ref type=""bibr"" target=""#b28"">[28]</ref> classify the devirtualization techniques into gua /ref>, <ref type=""bibr"" target=""#b20"">[21]</ref>, <ref type=""bibr"" target=""#b5"">[6]</ref>, <ref type=""bibr"" target=""#b28"">[28]</ref>. Ishizaki et al. <ref type=""bibr"" target=""#b28"">[28]</ref> classify the devirtualization techniques into guarded devirtualization and direct devirtualization. Guarded amic class loading, like Java. Dynamic recompilation can overcome this limitation, but it requires an expensive mechanism called on-stack replacement <ref type=""bibr"" target=""#b28"">[28]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""6.1"">Limitations of Compiler-Based Devirtualiza guide the method call transformation. Devirtualization based on static analysis requires type analysis, which in turn requires whole program analysis <ref type=""bibr"" target=""#b28"">[28]</ref>, and unsafe languages like Cþþ also require pointer alias analysis. Note that these analyses need to be cons formation, which may be very difficult to obtain for large applications. Due to the limited applicability of static devirtualization, Ishizaki et al. <ref type=""bibr"" target=""#b28"">[28]</ref> report only an average 40 percent reduction in the number of virtual method calls on a set of Java benchmark >, polymorphic inline caches <ref type=""bibr"" target=""#b23"">[23]</ref>, and type feedback/devirtualization <ref type=""bibr"" target=""#b24"">[24]</ref>, <ref type=""bibr"" target=""#b28"">[28]</ref>. As we show in Section 6, the benefit of devirtualization is limited by its lack of adaptivity. We compare a",1
"d replace one possibly mispredicted indirect call with multiple conditional branch mispredictions, if the guard tests become hard-to-predict branches <ref type=""bibr"" target=""#b48"">[48]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""6.1.3"">Lack of Adaptivity</head><p>The most fre form of direct devirtualization that converts only provably monomorphic virtual function calls into direct function calls (e.g., the Bartok compiler <ref type=""bibr"" target=""#b48"">[48]</ref>, <ref type=""bibr"" target=""#b42"">[42]</ref> or the .NET Runtime <ref type=""bibr"" target=""#b43"">[43]</ref>).</",0
"nal branch predictors, which operate on the observation that the control-flow path leading to a branch is correlated with the direction of the branch <ref type=""bibr"" target=""#b15"">[16]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2.1.1"">A Source Code Example</head><p>The examp",0
"tel Pentium M and AMD Barcelona implement specialized hardware to help the prediction of indirect branches <ref type=""bibr"" target=""#b19"">[20]</ref>, <ref type=""bibr"" target=""#b1"">[2]</ref>, demonstrating that hardware designers are increasingly concerned with the performance impact of indirect bran",0
"ion of indirect branch mispredictions per 1000 retired instructions (MPKI) in different Windows applications run on an Intel Core Duo T2500 processor <ref type=""bibr"" target=""#b26"">[26]</ref> which includes a specialized indirect branch predictor <ref type=""bibr"" target=""#b19"">[20]</ref>. The data a",0
"irtual function calls. These approaches include the method cache in Smalltalk-80 <ref type=""bibr"" target=""#b10"">[11]</ref>, polymorphic inline caches <ref type=""bibr"" target=""#b23"">[23]</ref>, and type feedback/devirtualization <ref type=""bibr"" target=""#b24"">[24]</ref>, <ref type=""bibr"" target=""#b28",0
"n=""2.1.1"">A Source Code Example</head><p>The example in Fig. <ref type=""figure"" target=""#fig_1"">2</ref> shows an indirect branch from the GAP program <ref type=""bibr"" target=""#b16"">[17]</ref> to provide insight into why historybased prediction of indirect branch targets works. GAP implements and int",0
"ns, and the behavior of the Java Virtual Machine, which uses a large number of indirect branches in its interpretation and dynamic translation phases <ref type=""bibr"" target=""#b14"">[15]</ref>. As a result, the potential performance improvement possible with perfect indirect branch prediction is sign ne itself uses a significant number of indirect jumps with many targets in its interpretation routines, as shown in previous work on virtual machines <ref type=""bibr"" target=""#b14"">[15]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""7.3"">Performance of VPC Prediction on Java Prog ck of adaptivity. We compare and contrast VPC prediction with compiler-based devirtualization extensively in Section 6.</p><p>Finally, Ertl and Gregg <ref type=""bibr"" target=""#b14"">[15]</ref> proposed code replication and superinstructions to improve indirect branch prediction accuracy on virtual ma",0
"rts only provably monomorphic virtual function calls into direct function calls (e.g., the Bartok compiler <ref type=""bibr"" target=""#b48"">[48]</ref>, <ref type=""bibr"" target=""#b42"">[42]</ref> or the .NET Runtime <ref type=""bibr"" target=""#b43"">[43]</ref>).</p></div> <div xmlns=""http://www.tei-c.org/n",0
"et=""#b9"">[10]</ref>, <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" target=""#b33"">[33]</ref>, <ref type=""bibr"" target=""#b12"">[13]</ref>, <ref type=""bibr"" target=""#b13"">[14]</ref>, <ref type=""bibr"" target=""#b47"">[47]</ref> require large hardware resources to store the target addresses of "" target=""#b11"">[12]</ref> and by combining multiple indirect branch predictors using a cascaded predictor <ref type=""bibr"" target=""#b12"">[13]</ref>, <ref type=""bibr"" target=""#b13"">[14]</ref>. The cascaded predictor is a hybrid of two or more target predictors. A relatively simple first-stage predic (single-target) indirect branches, whereas a complex second-stage predictor is used to predict hard-topredict indirect branches. Driesen and Ho ¨lzle <ref type=""bibr"" target=""#b13"">[14]</ref> concluded that a three-stage cascaded predictor performed the best for a particular set of C and Cþþ benchma design. 5 Moreover, many of the previously proposed indirect branch predictors are themselves complicated <ref type=""bibr"" target=""#b12"">[13]</ref>, <ref type=""bibr"" target=""#b13"">[14]</ref>, <ref type=""bibr"" target=""#b33"">[33]</ref>, <ref type=""bibr"" target=""#b47"">[47]</ref>, which further increas",0
"rfect indirect branch predictor. Table <ref type=""table"">3</ref> provides a brief description of the other two Cþþ benchmarks.</p><p>We use Pinpoints <ref type=""bibr"" target=""#b45"">[45]</ref> to select a representative simulation region for each benchmark using the reference input set. Each benchmar",0
"t=""#b31"">[31]</ref>, <ref type=""bibr"" target=""#b30"">[30]</ref> recently proposed handling hard-to-predict indirect branches using dynamic predication <ref type=""bibr"" target=""#b36"">[36]</ref>. In this technique, if the target address of an indirect branch is found to be hard to predict, the processo res compiler support, new instructions in the instruction set architecture, and significant hardware support for dynamic predication (as described in <ref type=""bibr"" target=""#b36"">[36]</ref>). However, the two approaches can be combined and used together: dynamic predication can be a promising appr",0
"ree virtual branches.</p><p>The VPC prediction algorithm is inspired by a compiler optimization, called receiver class prediction optimization (RCPO) <ref type=""bibr"" target=""#b10"">[11]</ref>, <ref type=""bibr"" target=""#b24"">[24]</ref>, <ref type=""bibr"" target=""#b20"">[21]</ref>, <ref type=""bibr"" targ BASED DEVIRTUALIZATION</head><p>Devirtualization is the substitution of an indirect method call with direct method calls in object-oriented languages <ref type=""bibr"" target=""#b10"">[11]</ref>, <ref type=""bibr"" target=""#b24"">[24]</ref>, <ref type=""bibr"" target=""#b20"">[21]</ref>, <ref type=""bibr"" targ proposed specifically for mitigating the performance impact due to virtual function calls. These approaches include the method cache in Smalltalk-80 <ref type=""bibr"" target=""#b10"">[11]</ref>, polymorphic inline caches <ref type=""bibr"" target=""#b23"">[23]</ref>, and type feedback/devirtualization <re",0
"are becoming more common as more programs are written in modern high-level languages such as Java, Cþþ, and C#. These languages support polymorphism <ref type=""bibr"" target=""#b7"">[8]</ref>, which significantly eases the development and maintenance of large modular software projects. To support poly",0
"ilarity between indirect and conditional branch prediction. There are two types of indirect branch predictors: history based and precomputation based <ref type=""bibr"" target=""#b46"">[46]</ref>. The technique we introduce in this paper utilizes history information, so we focus on history-based indirec ion can be a promising approach to reduce the performance impact of indirect branches that are hard to predict with VPC prediction.</p><p>Roth et al. <ref type=""bibr"" target=""#b46"">[46]</ref> proposed dependence-based precomputation, which precomputes targets for future virtual function calls as soo",0
"o not dedicate separate hardware but instead use the branch target buffer (BTB) to predict indirect branches <ref type=""bibr"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b21"">[22]</ref>, <ref type=""bibr"" target=""#b34"">[34]</ref>. The BTB implicitly-and usually inaccurately-assumes that the ind",0
"=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b26"">27]</ref> are effective for repetitive, ef><ref type=""bibr"" target=""#b26"">27]</ref> and similar recent proposals <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b22"">23]</ref>, a sequence of successors. For modern commercial workloads, early add ct a sequence of successor misses <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b26"">27]</ref>. We adopt the terminology of aintaining storage efficiency, several designs separate the storage of address sequences and correlation data <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b26"">27]</ref>. A history buffer records the application's recent miss-address seque t by applying these two mechanisms to a previously-proposed prefetch meta-data organization where misses are logged continuously in a circular buffer <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b26"">27]</ref>. By separating indexing and logging, this prefetcher organization al field will improve the coverage of stream-based address-correlating prefetchers beyond our idealized implementation of the proposed prior techniques <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b26"">27]</ref>. However, improved predictors can leverage the basic mechanisms we p tion tables entirely on-chip <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b20"">21]</ref>. However, correlation table storage requirements are proportional to the application's working set. Hence, fo ms of the Temporal Streaming Engine (TSE) <ref type=""bibr"" target=""#b26"">[27]</ref> and the predictor organization of the Global History Buffer (GHB) <ref type=""bibr"" target=""#b20"">[21]</ref>. After we describe the basic hardware operation, the following sections provide details of how the proposed s a block diagram of a four-core single-chip processor equipped with STMS. STMS comprises on-chip prefetch buffers and queues and offchip index table <ref type=""bibr"" target=""#b20"">[21]</ref> and history buffers <ref type=""bibr"" target=""#b20"">[21]</ref> 1 . The prefetch buffers and queues, located a d with STMS. STMS comprises on-chip prefetch buffers and queues and offchip index table <ref type=""bibr"" target=""#b20"">[21]</ref> and history buffers <ref type=""bibr"" target=""#b20"">[21]</ref> 1 . The prefetch buffers and queues, located along side the L1 victim caches <ref type=""bibr"" target=""#b16""> before prefetching can proceed. However, the set-associative table design limits maximum prefetch sequence length (referred to as the prefetch depth <ref type=""bibr"" target=""#b20"">[21]</ref>) based on the size of a table entry. Because storage cost, complexity, and update-band-   We propose an effi",1
"egy to address the processor-memory performance gap. Today's systems employ spatial/stride based prefetchers <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b28"">29]</ref> because they are practical to implement. These prefetchers require simple hardware additions and minimal on-c ble <ref type=""table"" target=""#tab_3"">1</ref>.</p><p>We include a stride-based prefetcher in our base system <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b28"">29]</ref>. All results report only coverage in excess of that provided by the stride prefetcher.</p><p>We measure perfo",0
"is simple design to optimize correlation table storage <ref type=""bibr"" target=""#b12"">[13]</ref>, or trigger prefetchers earlier to improve lookahead <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b17"">18]</ref>.</p><p>The key limitation of pairwise-correlating prefetchers is tha",0
"of these prefetchers is limited in commercial workloads (e.g., online transaction processing), which are dominated by pointer-chasing access patterns <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b26"">27]</ref>.</p><p>In contrast to stride-b ream length, MLP is an inherent property of a workload, and is typically low in pointer-chasing applications such as the studied commercial workloads <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b26"">27]</ref>.</p><p>As discussed in Section 3, when temporal streams are stored in",0
"1,</ref><ref type=""bibr"" target=""#b19"">20]</ref>.</p><p>Prefetching improves both throughput and response time by increasing memory level parallelism <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b26"">27 <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b26"">27]</ref> and similar recent proposals <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b22"">23 ef type=""bibr"" target=""#b17"">18]</ref>.</p><p>To improve practicality, recent address-correlating prefetchers store meta-data off chip in main memory <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b26"">27 algorithm must be designed to tolerate this long lookup latency by targeting prefetches several misses ahead in the anticipated future miss sequence <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b26"">27]</ref>. Second, the extra memory traffic used to lookup and maintain meta-dat vel parallelism and prefetch lookahead. More recent addresscorrelating prefetchers use a single correlation to predict a sequence of successor misses <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b20"">21, tore a temporal stream (i.e., a short sequence of successors) rather than only a single future access in each set-associative correlation table entry <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b22"">23]</ref>. The primary shortcoming of this set-associative organization is that ads, even the most storage-efficient design <ref type=""bibr"" target=""#b12"">[13]</ref> requires megabytes of correlation table storage to be effective <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b26"">27]</ref>. Figure <ref type=""figure"">1</ref> (left) shows the number of correlat rage. High storage requirements make on-chip correlation tables impractical.</p><p>More recent prefetchers locate correlation metadata in main memory <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b26"">27 tes), while prefetch buffers each require 2KB. We do not report sensitivity to prefetch buffer size, as it has been studied extensively in prior work <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b26"">27]</ref>. In addition, STMS uses a shared 8KB bucket buffer to store index tabl , a realistic implementation may lose prefetch opportunity on each lookup because time must be spent to locate and retrieve the miss address sequence <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b26"">27]</ref>.</p><p>We estimate lost prefetch opportunity by examining workload and storage<ref type=""bibr"" target=""#b5"">[6]</ref>.The right graph shows the memory traffic overheads of existing designs based on their published results<ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b26"">27]</ref> </figDesc><table><row><cell></ p>In contrast to stride-based approaches, addresscorrelating prefetchers <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b12"">13,< yet arbitrarily-irregular access patterns, such as the pointer-chasing access patterns of commercial workloads <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b26"">27]</ref>. Addresscorrelating prefetchers chasing relationships, and thus substantially improve the performance of pointerintensive commercial workloads <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b26"">27]</ref>.</p><p>Pair-wise-correlating prefetchers. The Markov prefetcher <ref ty isses in pointerintensive commercial server workloads, whereas stride prefetchers provide only minimal benefit <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b26"">27]</ref>. Nevertheless, stride prefetche across commercial workloads for the idealized address-correlating prefetcher we analyze in detail in Section 5.2. Our result corroborates prior work <ref type=""bibr"" target=""#b5"">[6]</ref>: to achieve maximum coverage in commercial workloads, correlation tables must store more than one million entr >Instead, the prefetching mechanism must be designed to account for long correlation table lookup latency. Epoch-based correlation prefetching (EBCP) <ref type=""bibr"" target=""#b5"">[6]</ref> explicitly accounts for off-chip lookup latency and the memory level parallelism already obtained by outof-ord meta-data in main memory, the User Level Memory Thread (ULMT <ref type=""bibr"" target=""#b22"">[23]</ref>), the Epoch-Based Correlation Prefetcher (EBCP <ref type=""bibr"" target=""#b5"">[6]</ref>), and the Temporal Streaming Engine (TSE <ref type=""bibr"" target=""#b26"">[27]</ref>), based on their published ocated and retrieved. Cache misses incurred during this time result in lost prefetch opportunity, even if they comprise a predictable temporal stream <ref type=""bibr"" target=""#b5"">[6]</ref>. Prior prefetchers targeting desktop/engineering applications rely on long temporal streams to overcome the st e=""bibr"" target=""#b6"">[7]</ref>), the average number of off-chip loads issued while at least one such load is outstanding. As discussed in prior work <ref type=""bibr"" target=""#b5"">[6]</ref>, a predictor that retrieves meta-data from main memory loses coverage for each followed temporal stream, propo target=""#b26"">27]</ref>.</p><p>As discussed in Section 3, when temporal streams are stored in a single set-associative correlation table (as in EBCP <ref type=""bibr"" target=""#b5"">[6]</ref> and ULMT <ref type=""bibr"" target=""#b22"">[23]</ref>), lookups require only a single memory access before prefet on table entries required for a given coverage in commercial server workloads. One million correlation table entries can require up to 64MB of storage<ref type=""bibr"" target=""#b5"">[6]</ref>.The right graph shows the memory traffic overheads of existing designs based on their published results<ref ty",0
"counters obtain frequency measures for architectural components such as adders, caches, decoders, and buses to estimate power. Researchers at Intel, <ref type=""bibr"" target=""#b10"">10</ref> Princeton, <ref type=""bibr"" target=""#b11"">11</ref>  </p></div><figure xmlns=""http://www.tei-c.org/ns/1.0"" type",0
"ch from synchronous designs.</p><p>However, asynchronous techniques can play an important role in globally asynchronous, locally synchronous systems. <ref type=""bibr"" target=""#b1"">2</ref> Such systems reduce clock power and help with the growing problem of clock skew across a large chip, while allow",0
"ecessarily so. Branch prediction is perhaps the best-known example of speculation. If the predictors are accurate, it can increase the MIPS/W figure. <ref type=""bibr"" target=""#b2"">3</ref> New architectural ideas can contribute most profitably to reducing the dynamic power consumption term, specifica",0
"niques have failed. Here we survey the range of published work on MCTS, to provide the reader Fig. <ref type=""figure"">1</ref>. The basic MCTS process <ref type=""bibr"" target=""#b16"">[17]</ref>.</p><p>with the tools to solve new problems using MCTS and to investigate this powerful approach to searchin c.org/ns/1.0""><head n=""1.1"">Overview</head><p>The basic MCTS process is conceptually very simple, as shown in Figure <ref type=""figure"">1</ref> (from <ref type=""bibr"" target=""#b16"">[17]</ref>). A tree 1 is built in an incremental and asymmetric manner. For each iteration of the algorithm, a tree pol ""#b74"">[75]</ref> which stores replies for the last two moves and uses LGR-1 if there is no LGR-2 entry for the last two moves.</p><p>Baier and Drake <ref type=""bibr"" target=""#b16"">[17]</ref> propose an extension to LGR-1 and LGR-2 called Last Good Reply with Forgetting (LGRF). In this context, ""for ake <ref type=""bibr"" target=""#b74"">[75]</ref> suggests using the Last Good Reply heuristic (6.1.8) to inform simulations, modified by Baier and Drake <ref type=""bibr"" target=""#b16"">[17]</ref> to include the forgetting of bad moves. Most programs use parallelisation, often with lock-free hashtables <",1
"S methods may be extended to procedural content generation (PCG) for creative domains, such as game design, linguistics, and generative art and music <ref type=""bibr"" target=""#b29"">[30]</ref>. An important difference from the standard approach is that each search attempts to produce not a single opt",0
"nd towards computer Go. As a domain in which computers are not yet at the level of top human players, Go has become the new benchmark for AI in games <ref type=""bibr"" target=""#b123"">[123]</ref>.</p><p>The most popular application of MCTS methods is to games and of these the most popular application",0
"eriments. UCB1-Tuned has subsequently been used in a variety of MCTS implementations, including Go <ref type=""bibr"" target=""#b94"">[95]</ref>, Othello <ref type=""bibr"" target=""#b103"">[103]</ref> and the real-time game Tron <ref type=""bibr"" target=""#b185"">[184]</ref>.</p></div> <div xmlns=""http://www. youts. MONTHELLO achieved a reasonable level of play but could not compete against human experts or other strong AI players.</p><p>Hingston and Masek <ref type=""bibr"" target=""#b103"">[103]</ref> describe an Othello player that uses straight UCT, but with playouts guided by a weighted distribution of",0
"utive moves.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""6.1.3"">Fill the Board</head><p>Fill the Board is an enhancement described in <ref type=""bibr"" target=""#b52"">[53]</ref>, <ref type=""bibr"" target=""#b54"">[55]</ref> designed to increase simulation diversity for the game of Go. At",0
"ibution.</p><p>To avoid this problem, Eeckhout et al proposed the Boundary Line Reuse Latency (BLRL<ref type=""foot"" target=""#foot_2"">3</ref> ) method <ref type=""bibr"" target=""#b9"">[10]</ref>, in which every memory reference in a sampling unit is directly examined instead of relying on aggregated dis ection 4.2.</p></note> 			<note xmlns=""http://www.tei-c.org/ns/1.0"" place=""foot"" n=""3"" xml:id=""foot_2""><p>The method was not given an official name in<ref type=""bibr"" target=""#b9"">[10]</ref>. It is called ""Boundary Line Reuse Latency"" because it is equivalent to profiling the memory reuses that cros",1
"can be large. For example, Haskins et al reported that in their experiment, ignoring warm-up could result in an error as high as 15% in simulated CPI <ref type=""bibr"" target=""#b11"">[12]</ref>. Thus adequate warm-up is critical to the accuracy of sampled simulation. Warm-up does not only affect accur n time can be accomplished with better warm-up techniques. For example, MRRL is claimed to have achieved 90% of the maximum possible simulation speed <ref type=""bibr"" target=""#b11"">[12]</ref>. However, careful analysis of the experiment reveals functional simulation as the bottleneck because every b n), and cycle-accurate simulation, is 1:1/2.8:1/16 <ref type=""foot"" target=""#foot_0"">1</ref> . Fifty 1 million-instruction sampling units are used in <ref type=""bibr"" target=""#b11"">[12]</ref>. Suppose that a benchmark is 100 billion instructions long and on average each sampling unit needs 30 millio ly handles warm-up for the first-level data and instruction caches. In their second technique, they measure the Memory Reference Reuse Latency (MRRL) <ref type=""bibr"" target=""#b11"">[12]</ref>, which refers to the elapsed time measured in number of instructions between a reference to some memory addr and the accuracy in CPI. In the experiment, we choose a sampling unit size of 1 million instructions. This sampling unit size was used in MRRL paper <ref type=""bibr"" target=""#b11"">[12]</ref>, and Variance SimPoint <ref type=""bibr"" target=""#b14"">[15]</ref>.</p><p>In this section, each benchmark exec bout 0.2%, so SMA is very accurate and rarely mispredicts.</p><p>For MRRL, we choose the p-value to be 99.9%, which is the default value suggested in <ref type=""bibr"" target=""#b11"">[12]</ref>. For BLRL, we use the p-value of 90%. Both methods are also accurate, exhibiting an average error of 0.4% an on being simulated.</p></note> 			<note xmlns=""http://www.tei-c.org/ns/1.0"" place=""foot"" n=""2"" xml:id=""foot_1""><p>No warm up length number is given in<ref type=""bibr"" target=""#b11"">[12]</ref>. This number is based on our experiment with MRRL. See Section 4.2.</p></note> 			<note xmlns=""http://www.te",1
"mulated in the cycle-accurate mode before each sampling units to warm up the pipeline. The profiler for MRRL was downloaded from its author's website <ref type=""bibr"" target=""#b17"">[18]</ref>.</p><p>Although not implemented in the current simulator, we hope to further improve simulation speed by dis",1
"misses. If, on the other hand, the cache state is assumed to be the same as the state at the end of last sampling unit, the scheme is called ""STITCH"" <ref type=""bibr"" target=""#b4"">[5]</ref>. The efficacy of these assumptions depends on workload, cache organization, and choice of sampling parameters.",0
"red. The number of sampling units in a sample is the sample size. Recently, Wunderlich et al. applied sampling theory to microarchitecture simulation <ref type=""bibr"" target=""#b1"">[2]</ref>. Under the assumption of no measuring error, they showed that CPI can be estimated to within an error of 3% wi ><p>Obviously the most accurate way to warm up the caches is to do cache simulation throughout the benchmark execution. This is how the SMARTS scheme <ref type=""bibr"" target=""#b1"">[2]</ref> does the warm-up. The simulator switches between functional warm-up and cycle-accurate simulation. During func ing the effect of out-of-order execution and wrong path execution on the caches during functional warm-up. It has been shown that this error is small <ref type=""bibr"" target=""#b1"">[2]</ref>  <ref type=""bibr"" target=""#b2"">[3]</ref>. Although this warm-up scheme is by far the most accurate, it is stil pe=""table"" target=""#tab_1"">2</ref> shows the main processor configuration used in our experiment. This configuration is adapted from the SMARTS paper <ref type=""bibr"" target=""#b1"">[2]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.1."">Variability in warm-up process</head><p>Muc . Once the cache is deemed warmed up enough, the simulator executes 4,000 instructions in cycle accurate mode to warm up the pipeline as suggested in <ref type=""bibr"" target=""#b1"">[2]</ref>, and then the CPI of 1 million instruction sampling unit is measured. As discussed in the above section, the L e additionally run a simulation with full cache warm-up. In this simulation the caches are always simulated between every sampling units as in SMARTS <ref type=""bibr"" target=""#b1"">[2]</ref>. The sampling units and the cycleaccurate warm-up are the same in all of our simulations, so the difference be",0
"ar web site <ref type=""bibr"" target=""#b15"">[16]</ref>, are compiled for the Alpha ISA. All the experiments are done on our modified SimpleScalar v3.0 <ref type=""bibr"" target=""#b0"">[1]</ref>. Table <ref type=""table"" target=""#tab_1"">2</ref> shows the main processor configuration used in our experiment",0
"ultiple machines.</p><p>Another simple warm-up scheme is to devote a fixed number of instructions to warm up the cache. This is called ""PRIME"" scheme <ref type=""bibr"" target=""#b5"">[6]</ref> following Crowley et. al.'s terminology <ref type=""bibr"" target=""#b3"">[4]</ref>.</p><p>At an extreme, zero ins",0
"ery instruction, which is inefficient. Secondly, always warming up the cache makes distributed simulation hard. For sampling methods such as SimPoint <ref type=""bibr"" target=""#b13"">[14]</ref> and Variance SimPoint <ref type=""bibr"" target=""#b14"">[15]</ref>, where a small number of relatively large sa",0
"r of instructions to warm up the cache. This is called ""PRIME"" scheme <ref type=""bibr"" target=""#b5"">[6]</ref> following Crowley et. al.'s terminology <ref type=""bibr"" target=""#b3"">[4]</ref>.</p><p>At an extreme, zero instructions are used to warm up the cache before cycle-simulating a sampling unit.",0
"up the cache makes distributed simulation hard. For sampling methods such as SimPoint <ref type=""bibr"" target=""#b13"">[14]</ref> and Variance SimPoint <ref type=""bibr"" target=""#b14"">[15]</ref>, where a small number of relatively large sampling units are taken, each sampling unit can be simulated on d unit size of 1 million instructions. This sampling unit size was used in MRRL paper <ref type=""bibr"" target=""#b11"">[12]</ref>, and Variance SimPoint <ref type=""bibr"" target=""#b14"">[15]</ref>.</p><p>In this section, each benchmark execution is divided into segments of 200 million instructions. We us",0
"unknown whether the first reference to each cache block will be a hit or a miss. Such references are referred to as coldstart references. Laha et al <ref type=""bibr"" target=""#b6"">[7]</ref> proposed not counting these cold-start references when calculating cache misses. This effectively assumes that",0
"CPI.</p><p>Haskins and Skadron have proposed two techniques to determine the warm-up length for a sampling unit. The Minimal Subset Evaluation (MSE) <ref type=""bibr"" target=""#b8"">[9]</ref> technique uses formulas derived from combinatorics and probability theory to calculate, for some user-chosen p",0
"2000, listed in Table <ref type=""table"" target=""#tab_0"">1</ref>, are used in our experiment. The programs, downloaded from the SimpleScalar web site <ref type=""bibr"" target=""#b15"">[16]</ref>, are compiled for the Alpha ISA. All the experiments are done on our modified SimpleScalar v3.0 <ref type=""b",0
"with this challenge, existing memory controllers tend to sustain only a small fraction of the peak bandwidth <ref type=""bibr"" target=""#b27"">[27,</ref><ref type=""bibr"" target=""#b37"">37]</ref>. The end result is either a significant performance hit, or an over-provisioned (and therefore expensive) mem his paper. Interested readers can find more detailed descriptions in <ref type=""bibr"" target=""#b11"">[11,</ref><ref type=""bibr"" target=""#b12"">12,</ref><ref type=""bibr"" target=""#b37"">37]</ref> on DRAM systems and in <ref type=""bibr"" target=""#b6"">[6,</ref><ref type=""bibr"" target=""#b28"">28,</ref><ref ty all workloads and under all circumstances. However, the FR-FCFS (first-ready first-come first-serve) policy <ref type=""bibr"" target=""#b36"">[36,</ref><ref type=""bibr"" target=""#b37"">37]</ref> provides the best average performance. Among all ready commands, FR-FCFS prioritizes (1) column (CAS) command : (1) Rixner et al.'s FR-FCFS scheduling policy, which was shown to be the best-performing policy on average <ref type=""bibr"" target=""#b36"">[36,</ref><ref type=""bibr"" target=""#b37"">37]</ref>, (2) a conventional inorder memory controller <ref type=""bibr"" target=""#b37"">[37]</ref>, and (3) an optimisti gible (less than 0.3% improvement over FR-FCFS on average) in our more aggressive setup.</p><p>Rixner et al. <ref type=""bibr"" target=""#b36"">[36,</ref><ref type=""bibr"" target=""#b37"">37]</ref> examine various DRAM command scheduling policies and propose the FR-FCFS policy. Hong et al. <ref type=""bibr"" ation (SCALPARC <ref type=""bibr"" target=""#b20"">[20]</ref>) with both a realistic, contemporary controller design (using the FR-FCFS scheduling policy <ref type=""bibr"" target=""#b37"">[37,</ref><ref type=""bibr"" target=""#b49"">49]</ref>), and an optimistic (and unrealizable) design that is able to sustai ests to optimize system performance. Different orderings and interleavings of DRAM commands result in different levels of DRAM throughput and latency <ref type=""bibr"" target=""#b37"">[37]</ref>. Finding a good schedule is not an easy task as scheduling decisions have long-term consequences: not only c tainly is not under the scheduler's control.</p><p>Current memory controllers use relatively simple policies to schedule DRAM accesses. Rixner et al. <ref type=""bibr"" target=""#b37"">[37]</ref> show that none of the fixed policies studied provide the best performance for all workloads and under all ci ing policy on average <ref type=""bibr"" target=""#b36"">[36,</ref><ref type=""bibr"" target=""#b37"">37]</ref>, (2) a conventional inorder memory controller <ref type=""bibr"" target=""#b37"">[37]</ref>, and (3) an optimistic (i.e., ideally efficient) scheduler that can sustain 100% of the peak DRAM throughput T. Note that the conventional in-order controller significantly underperforms the baseline FR-FCFS controller, in line with previous research results <ref type=""bibr"" target=""#b37"">[37]</ref>.</p><p>Figure <ref type=""figure"" target=""#fig_13"">8</ref> provides insight into the performance improvement the long 13 Even though it might seem low, the data bus utilization in our FR-FCFS baseline is consistent with what is reported in previous research <ref type=""bibr"" target=""#b37"">[37]</ref> and by DRAM manufacturers <ref type=""bibr"" target=""#b27"">[27]</ref>. Rixner <ref type=""bibr"" target=""#b37"">[ s reported in previous research <ref type=""bibr"" target=""#b37"">[37]</ref> and by DRAM manufacturers <ref type=""bibr"" target=""#b27"">[27]</ref>. Rixner <ref type=""bibr"" target=""#b37"">[37]</ref> reported an average utilization of approximately 35% for a different set of applications. Due to the difficu bibr"" target=""#b17"">[17,</ref><ref type=""bibr"" target=""#b25"">25,</ref><ref type=""bibr"" target=""#b30"">30,</ref><ref type=""bibr"" target=""#b36"">36,</ref><ref type=""bibr"" target=""#b37"">37,</ref><ref type=""bibr"" target=""#b38"">38,</ref><ref type=""bibr"" target=""#b48"">48,</ref><ref type=""bibr"" target=""#b49""",1
"ibr"" target=""#b4"">[4]</ref>, NAS OpenMP <ref type=""bibr"" target=""#b5"">[5]</ref>, Nu-MineBench <ref type=""bibr"" target=""#b33"">[33]</ref>, and SPLASH-2 <ref type=""bibr"" target=""#b46"">[46]</ref> benchmark suites. On a 4-core CMP with a single-channel DDR2-800 memory subsystem (6.4GB/s peak bandwidth in arget=""#b26"">[26]</ref>.</p><p>Our parallel workloads represent a mix of scalable scientific applications (three applications from the SPLASH-2 suite <ref type=""bibr"" target=""#b46"">[46]</ref>, three applications from the SPEC OpenMP suite <ref type=""bibr"" target=""#b4"">[4]</ref>, and two parallel NAS",0
"bibr"">[8]</ref>, eliminating bank conflicts <ref type=""bibr"" target=""#b44"">[44,</ref><ref type=""bibr"" target=""#b47"">47]</ref>, and refresh scheduling <ref type=""bibr"" target=""#b15"">[15]</ref> in DRAM controllers. These approaches are orthogonal to our proposal.</p></div> <div xmlns=""http://www.tei-c",0
"pe=""bibr"" target=""#b17"">[17,</ref><ref type=""bibr"" target=""#b39"">39]</ref>, Sun Niagara <ref type=""bibr"" target=""#b22"">[22]</ref>, AMD Athlon/Opteron <ref type=""bibr"" target=""#b1"">[1]</ref>, and Intel Nehalem <ref type=""bibr"" target=""#b3"">[3]</ref>), it is relatively easy to communicate sequence num",0
"d problems; for example, recursive spectral algorithms have been used to find k-way partitions, spectral clusterings, and separators in planar graphs <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b13"">14",1
"n the following results:</p><p>? We give an improved algorithm for computing approximate PageRank vectors. We use a technique introduced by Jeh-Widom <ref type=""bibr"" target=""#b6"">[7]</ref>, and further developed by Berkhin in his Bookmark Coloring Algorithm <ref type=""bibr"" target=""#b0"">[1]</ref>. bability on a single starting vertex.</p><p>Here are some useful properties of PageRank vectors (also see <ref type=""bibr"" target=""#b5"">[6]</ref> and <ref type=""bibr"" target=""#b6"">[7]</ref>). The proofs are given in the Appendix.</p><p>Proposition 1. For any starting distribution s, and any constant vector r satisfies max u?V r(u) d(u) &lt; , and such that vol(Supp(p)) ? 1 ? . We remark that this algorithm is based on the algorithms of Jeh-Widom <ref type=""bibr"" target=""#b6"">[7]</ref> and Berkhin <ref type=""bibr"" target=""#b0"">[1]</ref>, both of which can be used to compute similar approximate used to provide personalized search ranking and context-sensitive search <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b6"">7]</ref>. The preference vectors used in our algorithms have all probability on a single starting vertex.</p><p>Here are",0
"=""2.4."">Sweeps</head><p>A sweep is an efficient technique for producing cuts from an embedding of a graph, and is often used in spectral partitioning <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b13"">14]</ref>. We will use the following degree-normalized version of a sweep. Giv some value such that k 0 ? m and such that this equation holds with b = b 0 and k = k 0 . Notice that s b 0 -1 &lt; k 0 ? s b 0 , because if equation<ref type=""bibr"" target=""#b10"">(11)</ref> holds for b = b 0 and k = k 0 , it also holds for b = b 0 -1 and k 0 .When PageRank-Nibble is run with b = b",0
"use a technique introduced by Jeh-Widom <ref type=""bibr"" target=""#b6"">[7]</ref>, and further developed by Berkhin in his Bookmark Coloring Algorithm <ref type=""bibr"" target=""#b0"">[1]</ref>. The algorithms of Jeh-Widom and Berkhin compute many personalized PageRank vectors simultaneously, more quick h that vol(Supp(p)) ? 1 ? . We remark that this algorithm is based on the algorithms of Jeh-Widom <ref type=""bibr"" target=""#b6"">[7]</ref> and Berkhin <ref type=""bibr"" target=""#b0"">[1]</ref>, both of which can be used to compute similar approximate PageRank vectors in time O( log n ? ). The extra fac roduced by Haveliwala <ref type=""bibr"" target=""#b5"">[6]</ref>, and have been used to provide personalized search ranking and context-sensitive search <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b6"">7]</ref>. The preference vectors used in ou",0
"and separators in planar graphs <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b13"">14]</ref>. There is no known way to lower bound the size of the small side of the cut produced by spectral partitioning icient technique for producing cuts from an embedding of a graph, and is often used in spectral partitioning <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b13"">14]</ref>. We will use the following degree-normalized version of a sweep. Given a distribution p, with support size N",0
"ent short threads in parallel, and detects unpredicted dependence violations -squashing those threads where parallelism was not found. Helper threads <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b51"">52,</ref><ref type=""bibr"" target=""#b30"">31",1
"""bibr"" target=""#b29"">30,</ref><ref type=""bibr"" target=""#b31"">32,</ref><ref type=""bibr"" target=""#b42"">43,</ref><ref type=""bibr"" target=""#b44"">45,</ref><ref type=""bibr"" target=""#b46"">47]</ref>, or thread-level speculation, which executes multiple not-necessarily-independent short threads in parallel,",0
"he YAGS predictor <ref type=""bibr"" target=""#b10"">[11]</ref>, and several flavors of the Perceptron predictor <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b15"">16]</ref>, already mentioned.</p><p>More recently proposed branch predictors show that even longer histories can provid",0
"avy sampling is used to find the best mapping of threads to cores (resulting in frequent switching of threads between cores), or in Annavaram, et al. <ref type=""bibr"" target=""#b1"">[2]</ref> where new threads are initiated at each new instance of parallelism.</p><p>In striving to exploit the ever-inc",0
"t to employ for performance gains, energy savings, etc. Specific examples include speculative multithreading <ref type=""bibr"" target=""#b39"">[40,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b29"">30,</ref><ref type=""bibr"" target=""#b31"">3 target=""#b39"">[40]</ref>. Those proposed include models that work best when threads are largely independent <ref type=""bibr"" target=""#b43"">[44,</ref><ref type=""bibr"" target=""#b6"">7]</ref> and those that work even in the presence of dependences <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bib",0
"e best CHAR proposal achieves an average throughput improvement of 3.2% compared to the two-bit TC-AGE baseline (analogue of SRRIP for exclusive LLC) <ref type=""bibr"" target=""#b5"">[5]</ref> while saving 66.6% data write transactions from the L2 cache to the on-die interconnect for the multi-programm tion patterns have been used to arrive at bypass decisions and assign insertion ages to the non-bypassed blocks in the context of exclusive L3 caches <ref type=""bibr"" target=""#b5"">[5]</ref>. LLC insertion and replacement policies based on static and dynamic re-reference interval prediction (SRRIP an ssification based on trip count and use count of cache blocks presented in an earlier study in the context of a cache hierarchy with an exclusive LLC <ref type=""bibr"" target=""#b5"">[5]</ref>. According to the terminology used in that study, the set C0 ? C1 ? C2 ? C3 contains the zero trip count block ache blocks can be encoded in the L2 cache with just two extra state bits (S1, S0) per L2 cache block (as opposed to three bits per L2 cache block in <ref type=""bibr"" target=""#b5"">[5]</ref>). Figure <ref type=""figure"" target=""#fig_4"">4</ref>, through the class transitions, unambiguously defines the xclusive LLC. Our baseline exclusive LLC allocates all L2 cache evictions and decides the insertion age of a block based on the two-bit TC-AGE policy <ref type=""bibr"" target=""#b5"">[5]</ref>. This policy is the analogue of SRRIP for exclusive LLCs. It inserts all C0, C1, C2, and C3 blocks at age two foot_4""><p>We use the term ""age"" instead of ""RRPV"" in this discussion to conform to the terminology used in the prior work on exclusive LLC management<ref type=""bibr"" target=""#b5"">[5]</ref>. Age can be considered synonymous to RRPV in this discussion.</p></note> 		</body> 		<back>  			<div type=""ack",1
"tio between the LLC and the L2 cache <ref type=""bibr"">[7]</ref>, thereby eliminating most of the negative effects of inclusion victims. A recent work <ref type=""bibr"" target=""#b27"">[27]</ref> explores an orthogonal dimension of the problem by proposing global cache management schemes to decide which",0
"constructs a PC-less dead-on-fill predictor for use in cache bypassing by dynamically segmenting the LLC between referenced and not referenced blocks <ref type=""bibr"" target=""#b11"">[11]</ref>. PC-less light-weight dead block predictors exploiting the fill order of LLC blocks have also been proposed",0
"some similarities with the dead block predictors. The existing dead block predictors predict the last access or the last burst of accesses to a block <ref type=""bibr"" target=""#b6"">[6,</ref><ref type=""bibr"" target=""#b12"">12,</ref><ref type=""bibr"" target=""#b13"">13,</ref><ref type=""bibr"" target=""#b14"">",0
"ache onto the same chip as the fragment generator.</p><p>Finally, a recent trend in computer graphics has been the use of rendered images as textures <ref type=""bibr"" target=""#b2"">[3]</ref>. As a result, it has become desirable to unify the framebuffer and texture memories to avoid copy-ing data bet distribution which allows up to four texels to be accessed in parallel is possible if the texels are stored in a morton order within the cache lines <ref type=""bibr"" target=""#b2"">[3]</ref>. Morton order implies that the texels are stored in 2x2 blocks. The texels within each 2x2 block are interleav",1
"meters including reflection of the environment <ref type=""bibr"" target=""#b20"">[21]</ref>, bumps <ref type=""bibr"" target=""#b8"">[9]</ref>, transparency <ref type=""bibr"" target=""#b6"">[7]</ref>, and shadows <ref type=""bibr"" target=""#b19"">[20,</ref><ref type=""bibr"" target=""#b24"">25]</ref>.</p><p>One char",0
"type=""bibr"" target=""#b18"">[19]</ref> and Accelerated Graphics Port (AGP) <ref type=""bibr"" target=""#b0"">[1]</ref> from Intel Corporation, Magic Carpet <ref type=""bibr"" target=""#b11"">[12]</ref> from MIPS Technologies, and Talisman <ref type=""bibr"" target=""#b12"">[13]</ref> from Microsoft Corporation. I",0
"bibr"" target=""#b17"">[18]</ref> from Sun Microsystems, MMX TM Technology <ref type=""bibr"" target=""#b18"">[19]</ref> and Accelerated Graphics Port (AGP) <ref type=""bibr"" target=""#b0"">[1]</ref> from Intel Corporation, Magic Carpet <ref type=""bibr"" target=""#b11"">[12]</ref> from MIPS Technologies, and Tal",0
"sional image are ordered consecutively in memory. The idea of texture blocking is previously discussed in <ref type=""bibr"" target=""#b3"">[4]</ref> and <ref type=""bibr"" target=""#b9"">[10]</ref>. There are several issues that are raised by this representation. First, what is the addressing overhead asso",0
"endences to enable more aggressive out-oforder issue of memory instructions. Chrysos and Emer proposed using Store Sets to predict memory dependences <ref type=""bibr"" target=""#b2"">[3]</ref>. They show that an out-of-order processor using store sets achieves performance levels that are very close to cessor detects a memory ordering violation, the corresponding store is added to the load's store set.</p><p>The store set implementation presented in <ref type=""bibr"" target=""#b2"">[3]</ref> uses a pair of tables to predict memory dependences. The first table learns the actual store sets. The second 23) (P18) (P18) (P23)</formula><p>We used an aggressive processor model that is comparable to the configuration used in the original store sets study <ref type=""bibr"" target=""#b2"">[3]</ref> and the 8-wide configuration from the Stack Value File study <ref type=""bibr"" target=""#b6"">[7]</ref>. The proc nce predictor and another processor that uses store sets based memory bypassing. In both cases, we also use the set merging optimization described in <ref type=""bibr"" target=""#b2"">[3]</ref>, which we found to improve performance slightly. We simulated configurations using both squash recovery and se hardware <ref type=""bibr"" target=""#b9"">[10]</ref>. Chrysos and Emer proposed store sets for memory dependence prediction which we used in this study <ref type=""bibr"" target=""#b2"">[3]</ref>. The Alpha 21264 speculatively issues loads, but uses a simple table to track loads that cause misspeculations",1
"des <ref type=""bibr"">[16]</ref>. We use a mix of input sets from the test data set and the reduced run-length inputs from the University of Minnesota <ref type=""bibr"" target=""#b5"">[6]</ref>. We skipped the initial start-up sections for each benchmark, and then simulated 200 million instructions. The",0
"[11]</ref>. Calder and Reinmann compare a variety of aggressive load speculation techniques and examine the effects of combining different approaches <ref type=""bibr"" target=""#b1"">[2]</ref>. They show that memory dependence prediction and value prediction together provide the greatest performance be",0
"g/ns/1.0""><head n=""3.1."">Methodology</head><p>Our processor simulator is based on the SimpleScalar toolset, version 3.0 for the Alpha instruction set <ref type=""bibr"" target=""#b0"">[1]</ref>. Specifically, our simulator is derived from sim-outorder, a cycleaccurate out-of-order processor simulator ba",0
"figuration used in the original store sets study <ref type=""bibr"" target=""#b2"">[3]</ref> and the 8-wide configuration from the Stack Value File study <ref type=""bibr"" target=""#b6"">[7]</ref>. The processor parameters are listed in Table 1. We use a McFarling style hybrid branch predictor (gshare/PAs) , but further augmenting their processor with memory renaming does not provide any significant additional benefit. Lee et al's Stack Value File (SVF) <ref type=""bibr"" target=""#b6"">[7]</ref> computes the addresses of stack references early in the pipeline and redirects the stack traffic to a separate",0
"to massive graphs, e.g., <ref type=""bibr"" target=""#b19"">[20]</ref>, and heuristics that are used in practice <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b28"">29]</ref>. Our work contributes towards bridging the gap between theory and pra vides a unifying framework that subsumes two of the most popular heuristics used for streaming balanced graph partitioning: the folklore heuristic of <ref type=""bibr"" target=""#b25"">[26]</ref> which places a vertex to the cluster with the fewest non-neighbors, and the degree-based heuristic of <ref t th respect to performance.</p><p>? Our framework allows us to define formally the notion of interpolation between between the non-neighbors heuristic <ref type=""bibr"" target=""#b25"">[26]</ref> and the neighbors heuristic <ref type=""bibr"" target=""#b28"">[29]</ref>. This provides improved performance fo of v inside Si, i.e. |Si \ N (v)|. Hence, this yields the following heuristic: place a vertex to the partition with the least number of nonneighbors <ref type=""bibr"" target=""#b25"">[26]</ref>. This assignment accounts for both the cost of cut edges and the balance of partition sizes.</p><p>Finally, at most ?? n k . This algorithm for 1 ? ? ? 2 amounts to interpolating between the basic heuristics of <ref type=""bibr"" target=""#b28"">[29]</ref> and <ref type=""bibr"" target=""#b25"">[26]</ref>. The overall complexity of our algorithm is O(n + m).</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><he particularly suitable for dynamic graph data. Also, it allows us to quantify the notion of interpolation between the two state-of-the-art heuristics <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b28"">29]</ref> for streaming partitioning. Specifically, we evaluate our proposed f",1
"pe=""table"">1</ref>: Fraction of edges cut ? and the normalized maximum load ? for Fennel, the previously bestknown heuristic (linear weighted degrees <ref type=""bibr"" target=""#b28"">[29]</ref>) and hash partitioning of vertices for the Twitter graph with approximately 1.5 billion edges. Fennel and be pically involve using network infrastructure. The balanced graph partitioning problem in the dynamic setting is known as streaming graph partitioning <ref type=""bibr"" target=""#b28"">[29]</ref>. Vertices (or edges) arrive and the decision of the placement of each vertex (edge) has to be done ""on-the-f ic of <ref type=""bibr"" target=""#b25"">[26]</ref> which places a vertex to the cluster with the fewest non-neighbors, and the degree-based heuristic of <ref type=""bibr"" target=""#b28"">[29]</ref>, which serves as the current state-of-the-art method with respect to performance.</p><p>? Our framework allo ormally the notion of interpolation between between the non-neighbors heuristic <ref type=""bibr"" target=""#b25"">[26]</ref> and the neighbors heuristic <ref type=""bibr"" target=""#b28"">[29]</ref>. This provides improved performance for the balanced partitioning problem in the streaming setting. Moreover ple, Table <ref type=""table"">1</ref> shows the performance of Fennel versus the best previously-known heuristic, which is the linear weighted degrees <ref type=""bibr"" target=""#b28"">[29]</ref>, and the baseline Hash Partition of vertices. We observe that Fennel achieves, simultaneously, significantly ensive summary of existing heuristics can be found in <ref type=""bibr"">[1]</ref>.</p><p>Online graph partitioning was introduced by Stanton and Kliot <ref type=""bibr"" target=""#b28"">[29]</ref>. The online setting is also well adapted to dynamic graphs, where offline methods incur an expensive computa uffle of the data, which is very expensive in a distributed system. Currently the most advanced online partitioning algorithm is by Stanton and Kliot <ref type=""bibr"" target=""#b28"">[29]</ref>, against which we extensively compare our approach.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head oint subsets of vertices. The vertices arrive in some order, each one with the set of its neighbors. We consider three different stream orders, as in <ref type=""bibr"" target=""#b28"">[29]</ref>.</p><p>? Random: Vertices arrive according to a random permutation.</p><p>? BFS: This ordering is generated it is worth mentioning that even if we focus on vertex balanced partitions in Section 5, Fennel also works for edge balanced parititions as well, see <ref type=""bibr"" target=""#b28"">[29]</ref>.</p><p>Application to Classical Balanced Partitioning. Classical balanced graph partitioning problem is the vertex v to partition i with the largest number of neighbours in Si, i.e |N (v)?Si|. This is one of the greedy rules considered by Stanton and Kliot <ref type=""bibr"" target=""#b28"">[29]</ref>, and is a greedy rule that may result in highly imbalanced partition sizes.</p><p>On the other hand, in case forces to consider only machines whose load is at most ?? n k . This algorithm for 1 ? ? ? 2 amounts to interpolating between the basic heuristics of <ref type=""bibr"" target=""#b28"">[29]</ref> and <ref type=""bibr"" target=""#b25"">[26]</ref>. The overall complexity of our algorithm is O(n + m).</p></div rposes.</p><p>For competitors, we consider the state-of-the-art heuristics. Specifically, in our evaluation we consider the following heuristics from <ref type=""bibr"" target=""#b28"">[29]</ref>, which we briefly describe here for completeness. Let v be the newly arrived vertex, then place v to a clust (ET): maximum</p><formula xml:id=""formula_9"">tS i (v)(1 -exp |Si| -n/k)). ? Non-Neighbors (NN): minumum |Si \ N (v)|.</formula><p>In accordance with <ref type=""bibr"" target=""#b28"">[29]</ref>, we observed that LDG is the best performing heuristic. Even if Stanton and Kliot do not compare with NN, LD "" target=""#b19"">[20]</ref>, and heuristics that are used in practice <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b28"">29]</ref>. Our work contributes towards bridging the gap between theory and practice.</p><p>Summary of our Contribution 0""><head n=""7."">CONCLUSION</head><p>In this work we provide a novel perspective on a recent line of research <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b28"">29]</ref> for the balanced graph partitioning problem, which results in state-of-the-art performance in terms of speed data. Also, it allows us to quantify the notion of interpolation between the two state-of-the-art heuristics <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b28"">29]</ref> for streaming partitioning. Specifically, we evaluate our proposed framework extensively both on synthetic an",1
"graph with probability p. This is intimately related to the concepts of graph modularity <ref type=""bibr"" target=""#b10"">[11]</ref> and quasi-cliques <ref type=""bibr"" target=""#b29"">[30]</ref>. Approximation guarantees. For the special case of ? = 2 we can derive an approximation algorithm with prova",0
"nce is comparable and in one case (k=128) Fennel clearly outperforms Metis.</p><p>Power Law: It is well known that power law graphs have no good cuts <ref type=""bibr"" target=""#b11"">[12]</ref>, but power-law graphs are commonly observed in practice so we consider them. We examine the effect of parame",0
"we first generate a power-law degree sequence with a given slope ? and use the Chung-Lu random graph model to create an instance of a power law graph <ref type=""bibr"" target=""#b4"">[5]</ref>. The model CL(n, ?) has two parameters: the number of vertices n and the slope ? of the expected power law deg",0
"ndamental problem in every parallel and distributed application since data placement typically affects significantly the execution efficiency of jobs <ref type=""bibr"" target=""#b16"">[17]</ref>. The goal of balanced graph partitioning is to minimize an application's overall runtime. This is achieved b",0
"""#b26"">27]</ref>. It is worth emphasizing that the balanced graph partitioning problem appears in various guises in numerous other domains, e.g., see <ref type=""bibr"" target=""#b15"">[16]</ref>.</p><p>Another major challenge in the area of big graph data is efficient processing of dynamic graphs. For ibr"" target=""#b28"">[29]</ref>, which we briefly describe here for completeness. Let v be the newly arrived vertex, then place v to a cluster Si with  <ref type=""bibr"" target=""#b15"">[16]</ref> averaged over 5 random graphs generated according to the HP(5 000, k, 0.8, 0.5) model.</p></div> <div xmlns= ly work and algorithms that do not scale to massive graphs, e.g., <ref type=""bibr"" target=""#b19"">[20]</ref>, and heuristics that are used in practice <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b28"">29]</ref>. Our work contributes toward",0
"tween different partitions. Many popular graph processing platforms such as Pregel <ref type=""bibr"" target=""#b22"">[23]</ref> that builds on MapReduce <ref type=""bibr"" target=""#b6"">[7]</ref>, and its open source cousin Apache Giraph, PEGASUS <ref type=""bibr"" target=""#b12"">[13]</ref> and GraphLab <ref",0
"e=""bibr"" target=""#b22"">[23]</ref> that builds on MapReduce <ref type=""bibr"" target=""#b6"">[7]</ref>, and its open source cousin Apache Giraph, PEGASUS <ref type=""bibr"" target=""#b12"">[13]</ref> and GraphLab <ref type=""bibr"" target=""#b21"">[22]</ref> use as a default partitioner Hash Partition of vertic",0
"bibr"" target=""#b6"">[7]</ref> and program paths <ref type=""bibr"" target=""#b15"">[16]</ref>, we use the SEQUITUR hierarchical data compression algorithm <ref type=""bibr"" target=""#b9"">[10]</ref> to identify repetitive sub-sequences within the miss traces.</p><p>The SEQUITUR compression algorithm. SEQUIT",1
"GI interface, where a pool of perl processes await client requests, and the web server dispatches requests to an available process as requests arrive <ref type=""bibr"" target=""#b4"">[5]</ref>. FastCGI drastically improves performance over traditional CGI by avoiding process creation overheads on each",0
"ion, we demonstrate:</p><p>? Miss classification across system organizations. We classify miss behavior using categories similar to the ""4 C's"" model <ref type=""bibr"" target=""#b11"">[12]</ref>. Our results confirm prior observations that up to 80% of off-chip misses are coherence-induced in multi-chi /www.tei-c.org/ns/1.0""><head n=""4.1"">Miss Classification</head><p>We begin by classifying misses using a categorization based on the ""four C's model"" <ref type=""bibr"" target=""#b11"">[12]</ref>. These high-level breakdowns demonstrate the substantial differences in miss behavior across single-chip and",0
"1.0""><head n=""1."">Introduction</head><p>Off-chip memory accesses continue to pose a critical performance bottleneck in commercial server applications <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b21"">22,<",0
"pose a critical performance bottleneck in commercial server applications <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b24"">25]</ref>. Extensive research has shown t ive memory accesses and are highly-effective for pointerbased structures <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b13"">14, Furthermore, prior studies have considered only a single system organization, focusing either on uniprocessors <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b20"">21]</ref> or multi-chip distributed-shared- rring stream), or not part of any stream (Non-repetitive).</p><p>Discussion. Our results confirm prior studies <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b24"">25]</ref>: a substantial miss fraction, 35%-90%, occurs in temporal streams. In t ction processing and web serving, because these applications are dominated by pointer-based data structures with complex, non-strided access patterns <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b24"">25]</ref>.</p><p>To improve performance >Stream length is a critical factor affecting the usefulness of temporal streams. Long streams amortize prefetch costs (e.g., off-chip lookup latency <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b24"">25]</ref>). However, long streams may also hese results is that streams are generally quite long-in all cases the median stream length exceeds the fixed prefetch depths of many prior proposals <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b20"">21]</ref>. Overall, the median stream le ref type=""bibr"" target=""#b18"">[19]</ref>, the user-level memory thread <ref type=""bibr"" target=""#b20"">[21]</ref>, epoch-based correlation prefetching <ref type=""bibr"" target=""#b7"">[8]</ref>, and last-touch correlated data streaming <ref type=""bibr"" target=""#b8"">[9]</ref>.</p><p>Prefetching mechanism",0
"tation. For a fixed This work is supported by NSF CAREER Award CCR -0133777.</p><p>technology, the larger the cache, the slower the cache will become <ref type=""bibr"" target=""#b1"">[2]</ref>, and larger caches increase the cost of manufacturing. Another way to increase cache hit rate is to more effec 2"">[14]</ref> has an on die 256kB exclusive L2 cache. An inclusive cache system implies that the contents of the L1 cache be a subset of the L2 cache <ref type=""bibr"" target=""#b1"">[2]</ref>. This decreases the effective cache capacity available for unique information <ref type=""bibr"" target=""#b2"">[3",1
"xclusivity may occur when L1 is set associative, the block size is not the same for L1 and L2, or the number of sets in L2 is smaller than that of L1 <ref type=""bibr"" target=""#b7"">[8]</ref>. Fragmentary exclusivity implies that some but not all of the L1 contents are duplicated in L2, so that the ef",0
"g. If exclusive caching is used in a multiprocessor environment, then the L1 cache needs dedicated snoop ports as used in the AMD Athlon MP processor <ref type=""bibr"" target=""#b10"">[11]</ref>. This method solves the coherence problem, but at the cost of die area and cache access latency.</p><p>In th",0
"ct of the cycle time differential. Cache memories are small, high-speed buffer memories, which contain the most recently used portions of main memory <ref type=""bibr"" target=""#b0"">[1]</ref>. For on-chip cache, the cycle time of a small cache can match that of the processor, yielding an access time o",0
"can be selected.</p><p>The exclusive L2 cache model verification was performed through deterministic benchmarks, the procedure of which is available <ref type=""bibr"" target=""#b13"">[15]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.3"">Methodology and Benchmarks</head><p>10 ben",0
"l Framework</head><p>Technically our verification work is carried out in the Forte verification framework, originally built on top of the Voss system <ref type=""bibr"" target=""#b7"">[8]</ref>. The interface language to Forte is reFLect, a lazy, strongly-typed functional language in the ML family <ref",1
"a technique for reducing symbolic evaluation complexity when we are interested in the result of the evaluation only under a given set of constraints <ref type=""bibr"" target=""#b9"">[10]</ref>. It is an algorithm that takes a Boolean condition A, and computes a substitution list v/B = [(v 1 , B 1 ), (",0
"ructions.</p><p>Most of the particular verification techniques we applied are already documented in literature <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b14"">15]</ref>, and our goal here is to disc ng instead of word-level tools. Some of the verification efforts on the more complex operations, such as division and multiplication, are reported in <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b14"">15]</ref> The verification code was ma [15,</ref><ref type=""bibr"" target=""#b20"">21]</ref>. -The FDIV, FSQRT and IDIV families of operations need an iterative sequential decomposition as in <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b12"">13]</ref>. -Intel Core i7 added a collection of SSE4 instructions for accelera",0
"ible for the functional behaviour of all microinstructions. In the current paper we discuss further expansion of this work on Intel Core TM i7 design <ref type=""bibr"" target=""#b0"">[1]</ref>. For this project, we used formal verification as the primary validation vehicle for the execution cluster, in and interfaces with the external environment of the processor, e.g. the main memory or external bus.</p><p>In a multi-core design like Intel Core i7 <ref type=""bibr"" target=""#b0"">[1]</ref>, a single processor contains several cores and logic for communication and arbitration between different cores",0
"tapaths. In this area verification methods have reached sufficient maturity that they have now been routinely applied for a series of design projects <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b20""> r builds on and extends this body of work.The earliest concerted floating-point verification effort in Intel was carried out on the Pentium Pro design<ref type=""bibr"" target=""#b16"">[17]</ref>. The goal of the effort was full formal verification of all floating-point datapaths of the design, and it w",0
"number of branches, the more complete the branch information stored per BTB entry, the better the performance, provided the content reduces aliasing <ref type=""bibr"" target=""#b16"">[16]</ref>.</p><p>A large BTB tends to be more accurate, but the increased size adds to the latency of accessing the BT all amount of data is accessed quickly (e.g., a L1 cache) and large amount of data is accessed slowly (L2, L3, main memory etc.). It was suggested by <ref type=""bibr"" target=""#b16"">[16]</ref> that multilevel BTBs may be able to strike a balance between the large number of entries needed in the BTB a r among branches within a thread and among branches in different threads <ref type=""bibr"" target=""#b4"">[4,</ref><ref type=""bibr"" target=""#b5"">5,</ref><ref type=""bibr"" target=""#b16"">16]</ref>.</p><p>A balance must be made among the number of entries, content per entry, access latency and accuracy of",1
"13 IEEE</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2."">Background</head><p>Branch target buffers (BTBs) have been around for decades <ref type=""bibr"" target=""#b5"">[5]</ref>. Sussenguth describes a mechanism for looking up addresses and redirecting the flow of instructions to prevent al address as a tag, aliasing can occur among branches within a thread and among branches in different threads <ref type=""bibr"" target=""#b4"">[4,</ref><ref type=""bibr"" target=""#b5"">5,</ref><ref type=""bibr"" target=""#b16"">16]</ref>.</p><p>A balance must be made among the number of entries, content per",0
"ssesses <ref type=""bibr"" target=""#b6"">[6,</ref><ref type=""bibr"" target=""#b8"">8,</ref><ref type=""bibr"">9,</ref><ref type=""bibr"" target=""#b10"">10,</ref><ref type=""bibr"" target=""#b11"">11,</ref><ref type=""bibr"" target=""#b12"">12]</ref>. The solutions are analogous to a memory hierarchy, where a small amo inding a branch in the first level, it is looked for in the second level which can then provide a prediction <ref type=""bibr"" target=""#b10"">[10,</ref><ref type=""bibr"" target=""#b11"">11]</ref>. Other approaches use a larger, slower second level to preload entries into the smaller, faster level <ref ty",0
"oposed to help alleviate the inherent shortfalls that a traditional branch prediction implementation possesses <ref type=""bibr"" target=""#b6"">[6,</ref><ref type=""bibr"" target=""#b8"">8,</ref><ref type=""bibr"">9,</ref><ref type=""bibr"" target=""#b10"">10,</ref><ref type=""bibr"" target=""#b11"">11,</ref><ref ty 1]</ref>. Other approaches use a larger, slower second level to preload entries into the smaller, faster level <ref type=""bibr"" target=""#b6"">[6,</ref><ref type=""bibr"" target=""#b8"">8,</ref><ref type=""bibr"" target=""#b12"">12]</ref>.</p><p>Virtualization of prediction storage structures has recently eme",0
".5% comes from the average broker call during the period under study, which was just over 14% and we were long 653 out of 1077 days.)</p><p>As Sharpe <ref type=""bibr"" target=""#b5"">(6)</ref> points out, instead of buying and selling short the DJIA, we could have used a diversified portfolio of high b",1
"his indicators have undergone rigorous testing. For example, one Granville indicator is the mutual fund cash fund to asset ratio. This is analyzed in <ref type=""bibr"" target=""#b4"">(5)</ref>, which concludes this indicator has no predictive power. Another example where Granville made a testable asser",0
"ted to outperform significantly semantic hashing and other conventional semantic models.</p><p>In this study, based on a convolutional neural network <ref type=""bibr"" target=""#b0"">[1]</ref>, we present a new Convolutional Deep Structured Semantic Models (C-DSSM). Compared with DSSM, C-DSSM has a con",1
"ted via a semantic hashing approach using a deep auto-encoder. Most recently, a Deep Structured Semantic Models (DSSM) for Web search was proposed in <ref type=""bibr"" target=""#b5"">[6]</ref>, which is reported to outperform significantly semantic hashing and other conventional semantic models.</p><p> /p><p>The word hashing layer transforms each word in an input word sequence into a feature vector using the technique called word hashing proposed in <ref type=""bibr"" target=""#b5"">[6]</ref>. For example, the word is represented by a count vector of its letter-tri-grams.</p><p>The convolution operati of the clicked documents given a query, using stochastic gradient ascent. Learning of the C-DSSM is similar to that of learning the DSSM described in <ref type=""bibr"" target=""#b5"">[6]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3."">EXPERIMENTS</head><p>We have evaluated the re",1
"suffer from the data sparseness problem. In a separate line of research, deep learning based techniques have been proposed for semantic understanding <ref type=""bibr"" target=""#b2"">[3]</ref>[6][9] <ref type=""bibr"" target=""#b9"">[10]</ref>. Salakhutdinov and Hinton <ref type=""bibr"" target=""#b8"">[9]</re",0
"tic models, such as latent semantic analysis (LSA) and its extensions, are able to map a query to its relevant documents at the semantic level (e.g., <ref type=""bibr"" target=""#b1"">[2]</ref>). However, most latent semantic models still view a query (or a document) as a bag of words. Therefore, they a tion retrieval.</p><p>Modeling contextual information in search queries and documents is a long-standing research topic in information retrieval (IR) <ref type=""bibr"" target=""#b1"">[2]</ref>[4] <ref type=""bibr"" target=""#b7"">[8]</ref>. Usually, the contextual information captured by models such as TF-",0
"nformation in search queries and documents is a long-standing research topic in information retrieval (IR) <ref type=""bibr"" target=""#b1"">[2]</ref>[4] <ref type=""bibr"" target=""#b7"">[8]</ref>. Usually, the contextual information captured by models such as TF-IDF, BM25, and topic models, is often too c",0
"separate line of research, deep learning based techniques have been proposed for semantic understanding <ref type=""bibr"" target=""#b2"">[3]</ref>[6][9] <ref type=""bibr"" target=""#b9"">[10]</ref>. Salakhutdinov and Hinton <ref type=""bibr"" target=""#b8"">[9]</ref> demonstrated that the semantic structures c",0
"proposed for semantic understanding <ref type=""bibr"" target=""#b2"">[3]</ref>[6][9] <ref type=""bibr"" target=""#b9"">[10]</ref>. Salakhutdinov and Hinton <ref type=""bibr"" target=""#b8"">[9]</ref> demonstrated that the semantic structures can be extracted via a semantic hashing approach using a deep auto-e",0
">[5]</ref>, we only used the title field of a Web document for ranking. The results are reported by mean Normalized Discounted Cumulative Gain (NDCG) <ref type=""bibr"" target=""#b6"">[7]</ref>. In our experiments, the clickthrough data used for model training include 30 million of query/clicked-title p",0
"topic models, is often too coarse-grained to be effective. As an alternative, there are retrieval methods such as the phrase-based translation model <ref type=""bibr"" target=""#b4"">[5]</ref> that directly model phrases (or word n-grams), but they often suffer from the data sparseness problem. In a se ies sampled from one-year query log files of a commercial search engine. On average, each query is associated with 65 Web documents (URLs). Following <ref type=""bibr"" target=""#b4"">[5]</ref>, we only used the title field of a Web document for ranking. The results are reported by mean Normalized Disco",0
"L scheme by 7%.</p><p>Researchers have proposed several techniques for nonsequential instruction prefetching <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b16"">17]</ref>. Of these, the work that is clo",1
"llowing pedagogical example. Figure <ref type=""figure"" target=""#fig_1"">2</ref> shows a segment of a call graph for adding a record to a file in SHORE <ref type=""bibr"" target=""#b5"">[6]</ref>. SHORE is a storage manager that provides storage volumes, B+-trees, R*-trees, con-currency control and transa 1."">Methodology</head><p>To evaluate the effectiveness of CGP we implemented a subset of the relational operators on top of the SHORE storage manager <ref type=""bibr"" target=""#b5"">[6]</ref>. SHORE is a fully functional storage manager which has been used extensively in the database research communit",0
"ode and data footprints suffer significantly from poor cache performance <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b19"">20]</ref>. Thus the key challenge in im p>It is only recently that researchers have examined the performance impact of architectural features on DBMSs <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b18""> rom frequent context switches, causing significant increases in the instruction cache miss rates <ref type=""bibr"" target=""#b10"">[11]</ref>. Lo et al. <ref type=""bibr"" target=""#b11"">[12]</ref> also showed that in OLTP workloads, the instruction cache miss rate is nearly three times the data cache mis",0
"ut recent studies have shown that current database systems with their large code and data footprints suffer significantly from poor cache performance <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b14"">15 these cache-conscious algorithms.</p><p>It is only recently that researchers have examined the performance impact of architectural features on DBMSs <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b9"">1 ""#b11"">[12]</ref> also showed that in OLTP workloads, the instruction cache miss rate is nearly three times the data cache miss rate. Ailamaki et al. <ref type=""bibr"" target=""#b0"">[1]</ref> analyzed three commercial DBMSs on a Xeon processor and showed that TPC-D queries spend about 20% of their exe",0
"rchitectural features on DBMSs <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b10"">11",0
"in the memory system faithfully. All the mentioned effects are modeled correctly and bandwidth limitations are enforced in our model as described in <ref type=""bibr"" target=""#b15"">[16]</ref>. The memory bus has a bandwidth of 4.5 GB/s.</p><p>The baseline hardware data prefetcher we model is a strea",1
"is needed. In comparison, our mechanism is simpler because it does not require phase detection or prediction mechanisms.</p><p>Recently, Hur and Lin <ref type=""bibr"" target=""#b6"">[7]</ref> proposed a probabilistic technique that adjusts the aggressiveness of a stream prefetcher based on the estimat",0
"al to pref-total.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.1.2."">Prefetch Lateness:</head><p>Miss Status Holding Register (MSHR) <ref type=""bibr"" target=""#b11"">[12]</ref> is a hardware structure that keeps track of all in-flight memory requests. Before allocating an MSHR entry f",0
"ssiveness of a global-historybuffer (GHB) based delta correlation prefetcher <ref type=""bibr"" target=""#b9"">[10]</ref> or a PC-based stride prefetcher <ref type=""bibr"" target=""#b0"">[1]</ref>. Compared to a conventional GHB-based delta correlation prefetcher configuration that consumes similar amount p://www.tei-c.org/ns/1.0""><head n=""5.9."">Effect of FDP on a PC-Based Stride Prefetcher</head><p>We also evaluated FDP on a PC-based stride prefetcher <ref type=""bibr"" target=""#b0"">[1]</ref> and found that the results are similar to those achieved on both stream and GHB-based prefetchers. On average,",0
"ism works similarly well when implemented to dynamically adjust the aggressiveness of a global-historybuffer (GHB) based delta correlation prefetcher <ref type=""bibr"" target=""#b9"">[10]</ref> or a PC-based stride prefetcher <ref type=""bibr"" target=""#b0"">[1]</ref>. Compared to a conventional GHB-based Buffer Prefetcher</head><p>We have also implemented FDP on the C/DC (C-Zone Delta Correlation) variant of the Global History Buffer (GHB) prefetcher <ref type=""bibr"" target=""#b9"">[10]</ref>. In order to vary the aggressiveness of this prefetcher dynamically, we vary the Prefetch Degree. <ref type="" nd pollution.    When the program enters a new phase of execution, the prefetcher is tuned based on the characteristics of the phase in Nesbit et al. <ref type=""bibr"" target=""#b9"">[10]</ref>. In order to perform phase detection/prediction and identification of the best prefetcher configuration for a",0
"y over the surface of any single polygon. This problem is apparent in Figure <ref type=""figure"" target=""#fig_0"">2</ref>. or ""Bezier patches,"" Gouraud <ref type=""bibr"" target=""#b10"">[11]</ref> developed an algorithm to shade curved surfaces. With his algorithm, a surface represented by a patch is app",1
"the development of fast hidden surface algorithms have been made by Watkins <ref type=""bibr"" target=""#b8"">[9]</ref> and by Newell, Newell, and Sancha <ref type=""bibr"" target=""#b9"">[10]</ref>. Watkins generates the displayed picture scan line by scan line. On each scan line he computes which polygons",0
"off throughput when the express channels are not highly utilized. While recent work has proposed speculation <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b12"">13]</ref> to cut down the router critical path delay by parallelizing multiple pipeline stages, such techniques show di hereby disallowing pipeline bypassing, aggressive speculation (SP) can be used to cut down the critical path <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b15"">16]</ref>. Fig. <ref type=""bibr"">3(d)</ref> shows the router pipeline where VA le-ported buffers and a single shared port into the crossbar from each input. Separable VC and switch allocators modeled closely after the designs in <ref type=""bibr"" target=""#b12"">[13]</ref> are assumed as they are fast and of low complexity, while still providing a reasonable throughput, making th",1
"letely non-speculative and, hence, can improve performance at all traffic conditions while simultaneously lowering the energy consumption. Kim et al. <ref type=""bibr"" target=""#b28"">[29]</ref> target throughput by proposing path-sensitive packet buffering and partitioning the crossbar into separate r",0
"using physical express channels, trades off throughput when the express channels are not highly utilized. While recent work has proposed speculation <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b12"">13]</ref> to cut down the router critical path delay by parallelizing multiple nflict. When the router ports are busy, thereby disallowing pipeline bypassing, aggressive speculation (SP) can be used to cut down the critical path <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b15"">16]</ref>. Fig. <ref type=""bibr"">3(d)< </p><p>Our baseline router already incorporates several speculation mechanisms that target Trouter, such as bypassing and speculation. Mullins et al. <ref type=""bibr"" target=""#b11"">[12]</ref> propose a doubly-speculative single-stage design in which arbitration decisions are precomputed to further r",0
"ns=""http://www.tei-c.org/ns/1.0""><head n=""1."">INTRODUCTION</head><p>Driven by technology limitations to wire scaling and increasing bandwidth demands <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref>, packet-switched on-chip networks are fast replacing shared buses and de in practice: a 7?7 network will require a chip size of 4760mm<ref type=""foot"" target=""#foot_1"">2</ref> (L edge =69mm), way beyond the ITRS projection <ref type=""bibr"" target=""#b0"">[1]</ref> of a chip size of 310mm 2 for high-performance chips. Ideal energy: The energy consumption E ideal of a packet",0
"target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b3"">4]</ref> and application-specific systems-on-a-chip (SoCs) <ref type=""bibr"" target=""#b4"">[5]</ref><ref type=""bibr"" target=""#b5"">[6]</ref><ref type=""bibr"" target=""#b6"">[7]</ref><ref type=""bibr"" target=""#b7"">[8]</ref>. While there has been significan",0
"elays often make other idealistic assumptions such as exponential service times, infinite buffers, and so on <ref type=""bibr"" target=""#b6"">[7]</ref>, <ref type=""bibr"" target=""#b8"">[9]</ref>, <ref type=""bibr"" target=""#b12"">[13]</ref>. Likewise, analysis of power consumption of the NoC architectures d ibr"" target=""#b2"">[3]</ref>, <ref type=""bibr"" target=""#b6"">[7]</ref> and hypercubes <ref type=""bibr"" target=""#b22"">[24]</ref>. The study presented in <ref type=""bibr"" target=""#b8"">[9]</ref> is not restricted to a particular topology, but it assumes an exponential message length distribution and it h ormula><formula xml:id=""formula_20"">)</formula><p>where T Bus is the service time of the Bus architecture and C Bus is the contention matrix given in <ref type=""bibr"" target=""#b8"">(9)</ref>. Finally, we note that when each buffer shown in Fig. <ref type=""figure"" target=""#fig_1"">4</ref> is connected",1
"type=""bibr"" target=""#b13"">[14]</ref>. In order to compute the delay experienced at the intermediate routers, in our experimental work, we first apply <ref type=""bibr"" target=""#b15"">(17)</ref> by going through all the flows x sd and, following the routing algorithm, traverse the network from source s outers and packet service time (including the waiting time due to blocking) are computed using ( <ref type=""formula"" target=""#formula_0"">1</ref>) and <ref type=""bibr"" target=""#b15"">(17)</ref>. In order to find the average packet latency, we follow the path on the left in Fig. <ref type=""figure"">6</r new formalism is needed to estimate the latency of packets blocked across several routers, similar to the recursive computation approach presented in <ref type=""bibr"" target=""#b15"">[17]</ref>. Extensions of our current framework for dealing with performance analysis in regimes close to criticality a",0
"specifications (H S , W ), and target application (S). Equation ( <ref type=""formula"" target=""#formula_8"">5</ref>) generalizes the single queue model <ref type=""bibr"" target=""#b1"">[2]</ref>; this is one of the major contributions of this paper.</p><p>We note that when det(I − T C) = 0, the packet po ml:id=""formula_10"">N = λR 1 − Tλ .</formula><p>Furthermore, the residual waiting time R = λT 2 /2, where T 2 is the second moment of the service time <ref type=""bibr"" target=""#b1"">[2]</ref>. As a result</p><formula xml:id=""formula_11"">N = λ 2 T 2 2 (1 − Tλ) (6)</formula><p>which is precisely the ave , 0.081) 99.634 (1, 0) East at <ref type=""bibr"" target=""#b2"">(3,</ref><ref type=""bibr"" target=""#b2"">3)</ref> 101.820 (0, 0.054) 82.477 (1, 0) West at <ref type=""bibr"" target=""#b1"">(2,</ref><ref type=""bibr"" target=""#b0"">1)</ref> 122.610 (0, 0.504) 98.617 (0, 0.021) South at <ref type=""bibr"" target=""#",0
"queue systems.</p><p>We note that our evaluations on the actual distribution of the header inter-arrival times based on the Pearson's chisquare test <ref type=""bibr"" target=""#b24"">[26]</ref> (see Section VII-F) show that the Poisson assumption holds pretty well at low and medium traffic rates, and the discrepancy between the actual distribution of the header inter-arrival times and the hypothesized distribution via the Pearson's chi-square test <ref type=""bibr"" target=""#b24"">[26]</ref>. According to the Pearson statistical test result h = 0 shows that the collected evidence (i.e., inter-arriv",0
"get specific network topologies such as k-ary n-cubes <ref type=""bibr"" target=""#b2"">[3]</ref>, <ref type=""bibr"" target=""#b6"">[7]</ref> and hypercubes <ref type=""bibr"" target=""#b22"">[24]</ref>. The study presented in <ref type=""bibr"" target=""#b8"">[9]</ref> is not restricted to a particular topology, </ref> When lim k→∞ (T C) k exists [or equivalently the eigenvalues of the matrix (T C) are less than one <ref type=""bibr"" target=""#b26"">[28]</ref>], <ref type=""bibr"" target=""#b22"">(24)</ref> holds. Consequently, using the result of Lemma 1, we have</p><formula xml:id=""formula_28"">(I − T C P2P ) −1",0
"IaaS Cloud systems where the mixture of applications is determined by dynamic external customers. The Sharing Architecture is inspired by Core Fusion <ref type=""bibr"" target=""#b22"">[24]</ref>, WiD-GET <ref type=""bibr"" target=""#b62"">[64]</ref>, the Distributed ILP mechanisms in the TRIPS architecture Whenever there is a stall that ripples back to the fetch stage of the pipeline, it stalls all of the Slices in a VCore.</p><p>Similar to Core Fusion <ref type=""bibr"" target=""#b22"">[24]</ref>, a distributed branch predictor is used, thus the effective branch predictor capacity grows with number of S re queues (LSQs) are challenging to build in wide-issue processors. The Sharing Architecture  <ref type=""bibr"" target=""#b6"">[8]</ref> and Core Fusion <ref type=""bibr"" target=""#b22"">[24]</ref> require LSQ bank prediction because a memory operation's effective address is unknown until the execution st tributed Reorder Buffer</head><p>We partition and distribute Reorder Buffer (ROB) entries across Slices. We leverage the approach used by Core Fusion <ref type=""bibr"" target=""#b22"">[24]</ref> which uses a pre-commit pointer to guarantee that all ROBs are up to date several cycles before true commit. more flexible than TFlex, but possibly with less potential to exploit high ILP.</p><p>The Sharing Architecture leverages many ideas from Core Fusion <ref type=""bibr"" target=""#b22"">[24]</ref> on how to distribute resources across cores, but unlike Core Fusion, the Sharing Architecture is designed to",1
"bibr"" target=""#b12"">[14,</ref><ref type=""bibr"" target=""#b17"">19,</ref><ref type=""bibr"" target=""#b19"">21,</ref><ref type=""bibr"" target=""#b23"">25,</ref><ref type=""bibr"" target=""#b46"">48,</ref><ref type=""bibr"" target=""#b54"">56]</ref>. These Cloud provider changes can enable improvements in economic eff",0
", WiD-GET <ref type=""bibr"" target=""#b62"">[64]</ref>, the Distributed ILP mechanisms in the TRIPS architecture <ref type=""bibr"" target=""#b9"">[11,</ref><ref type=""bibr"" target=""#b15"">17,</ref><ref type=""bibr"" target=""#b47"">49,</ref><ref type=""bibr"" target=""#b48"">50,</ref><ref type=""bibr"" target=""#b51"" and functional units and has been explored in Raw <ref type=""bibr"" target=""#b57"">[59]</ref>, Tilera <ref type=""bibr"" target=""#b63"">[65]</ref>, TRIPS <ref type=""bibr"" target=""#b15"">[17,</ref><ref type=""bibr"" target=""#b47"">49,</ref><ref type=""bibr"" target=""#b48"">50]</ref>, and Wavescalar <ref type=""b",0
"ons which have traditionally been desktop applications have successfully been transitioned into the Cloud, including word processing (eg. Google Apps <ref type=""bibr"" target=""#b1"">[2]</ref>), spreadsheets (eg. Google Apps), data backup clients (eg. Box.net, Dropbox, Mozy, Backblaze ), email clients",0
"se they allocate into the LSQ when entering the issue window. In contrast, the Sharing Architecture uses an unordered LSQ such as those in prior work <ref type=""bibr"" target=""#b52"">[54]</ref> in order to reduce LSQ occupancy and power consumption. Our work is the first to utilize a general-purpose s",0
"/ref> <ref type=""bibr"" target=""#b16"">[19]</ref>. In this work, we focus on three of these proposed schemes -the first based on working set signatures <ref type=""bibr"" target=""#b9"">[10]</ref> <ref type=""bibr"" target=""#b10"">[11]</ref>, the second based on basic block vectors <ref type=""bibr"" target=""# ing to tune and reconfigure in unstable regions can lead to unpredictable, non-optimal results. Consequently, algorithms such as the ones proposed in <ref type=""bibr"" target=""#b9"">[10]</ref> <ref type=""bibr"" target=""#b10"">[11]</ref> do not perform tuning while in unstable regions. Tuning algorithms comparing techniques based on unbounded working sets, BBVs, and conditional branch counters. In addition to instruction working set based techniques <ref type=""bibr"" target=""#b9"">[10]</ref>[11], we evaluate branch and procedure working set based techniques.</p><p>We equalize the granularity of thes interval. Similarly, branch/procedure working sets are defined as the set of branches/procedures touched over the sampling interval. In previous work <ref type=""bibr"" target=""#b9"">[10]</ref>[11], we defined a similarity metric called the relative working set distance, to compare working sets. The re ng unbounded hardware resources. In practice, BBVs and working sets are too large to be efficiently stored and compared in hardware. In previous work <ref type=""bibr"" target=""#b9"">[10]</ref> <ref type=""bibr"" target=""#b10"">[11]</ref>, we proposed a hardware structure called the working set signature, .org/ns/1.0""><head n=""5.1.1."">Working Set Signature</head><p>A working set signature is a lossy-compressed representation of the complete working set <ref type=""bibr"" target=""#b9"">[10]</ref> <ref type=""bibr"" target=""#b10"">[11]</ref>. The signature is formed by sampling the working set i.e. program ) es for instruction/branch signatures (4096, 1024, 512 bits) and accumulator tables (1024, 128, 32 entries) are similar to those used in previous work <ref type=""bibr"" target=""#b9"">[10]</ref>[11] <ref type=""bibr"" target=""#b16"">[19]</ref>. Procedure signature sizes were chosen to be 1024, 256 and 64 b e in phase detection techniques. This property can be used in tuning algorithms to reuse previously found optimal configurations for recurring phases <ref type=""bibr"" target=""#b9"">[10]</ref>[11] <ref type=""bibr"">[12] [19]</ref>. This eliminates a significant fraction of reconfigurations, leading to",1
"mically adapts to changing program requirements in order to achieve better power/performance characteristics <ref type=""bibr"" target=""#b0"">[1]</ref>- <ref type=""bibr"" target=""#b11"">[12]</ref>. Similarly, on the software side, dynamic code optimization <ref type=""bibr"" target=""#b12"">[13]</ref> <ref t et=""#b12"">[13]</ref> or explicitly detect program phase changes <ref type=""bibr"" target=""#b1"">[2]</ref>[10] <ref type=""bibr"" target=""#b10"">[11]</ref> <ref type=""bibr"" target=""#b11"">[12]</ref> [19] <ref type=""bibr"" target=""#b17"">[20]</ref> <ref type=""bibr"" target=""#b18"">[21]</ref>. Recently, several tecting phase changes, identifying phases and predicting phases <ref type=""bibr"" target=""#b1"">[2]</ref>[10] <ref type=""bibr"" target=""#b10"">[11]</ref> <ref type=""bibr"" target=""#b11"">[12]</ref> <ref type=""bibr"" target=""#b16"">[19]</ref>. In this work, we focus on three of these proposed schemes -the fi f type=""bibr"" target=""#b16"">[19]</ref>. Their results indicate that as few as 32 24-bit counters are sufficient to represent BBVs.</p><p>Huang et al. <ref type=""bibr"" target=""#b11"">[12]</ref> use subroutines to identify program phases. They propose the use of a hardware based call stack to identify he best one. These algorithms require several intervals at the beginning of a stable phase to complete the tuning process (the algorithm presented in <ref type=""bibr"" target=""#b11"">[12]</ref> is an exception). If phases are short (i.e. small number of intervals), tuning never completes. Also, short",0
"ed multi-configuration hardware that dynamically adapts to changing program requirements in order to achieve better power/performance characteristics <ref type=""bibr"" target=""#b0"">[1]</ref>- <ref type=""bibr"" target=""#b11"">[12]</ref>. Similarly, on the software side, dynamic code optimization <ref ty",0
"ation may enable reuse of configuration information for recurring phases, thereby improving performance <ref type=""bibr"" target=""#b10"">[11]</ref>[12] <ref type=""bibr"" target=""#b16"">[19]</ref>.</p><p>There have been several proposals to implicitly <ref type=""bibr"" target=""#b12"">[13]</ref> or explicit and predicting phases <ref type=""bibr"" target=""#b1"">[2]</ref>[10] <ref type=""bibr"" target=""#b10"">[11]</ref> <ref type=""bibr"" target=""#b11"">[12]</ref> <ref type=""bibr"" target=""#b16"">[19]</ref>. In this work, we focus on three of these proposed schemes -the first based on working set signatures <ref t on working set signatures <ref type=""bibr"" target=""#b9"">[10]</ref> <ref type=""bibr"" target=""#b10"">[11]</ref>, the second based on basic block vectors <ref type=""bibr"" target=""#b16"">[19]</ref>, and the third based on a conditional branch counter <ref type=""bibr"" target=""#b1"">[2]</ref>. Because phases reshold. Because entire BBVs cannot be stored in hardware, BBVs are approximated by hashing into an accumulator table containing a few large counters <ref type=""bibr"" target=""#b16"">[19]</ref>. Their results indicate that as few as 32 24-bit counters are sufficient to represent BBVs.</p><p>Huang et a ave been used in power/performance optimization algorithms <ref type=""bibr"" target=""#b1"">[2]</ref>[10] <ref type=""bibr"" target=""#b10"">[11]</ref> [12] <ref type=""bibr"" target=""#b16"">[19]</ref> and to reduce simulation time of benchmarks by identifying sections of code whose performance is representat a BBV to be a set of counters, each of which counts the number of times a static basic block is entered in a given execution interval. In later work <ref type=""bibr"" target=""#b16"">[19]</ref>, they approximate the BBV with an array of counters, where each counter tracks the number of instructions ex , we proposed a hardware structure called the working set signature, which is a compact representation of the working set. Similarly, Sherwood et al. <ref type=""bibr"" target=""#b16"">[19]</ref> proposed an array of accumulators (counters) called the accumulator table to represent BBVs. In this section 024, 512 bits) and accumulator tables (1024, 128, 32 entries) are similar to those used in previous work <ref type=""bibr"" target=""#b9"">[10]</ref>[11] <ref type=""bibr"" target=""#b16"">[19]</ref>. Procedure signature sizes were chosen to be 1024, 256 and 64 bits because procedure working sets are much s r observation was made there.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""5.3.1."">Conditional Branch Counter</head><p>Sherwood et al. <ref type=""bibr"" target=""#b16"">[19]</ref> evaluated the performance of accumulator tables by using a metric called the visible phase difference. The v phase change is detected in both cases. This explains why the 32-counter method agrees with the unbounded BBV 93% of the time even when results from <ref type=""bibr"" target=""#b16"">[19]</ref> show that it achieves a visible phase difference of only 70%.</p><p>This means that perhaps an even smaller",0
"introduced in 1962 <ref type=""bibr"" target=""#b18"">[19]</ref> as a way to study brain function. We consider the simplest of many types of perceptrons <ref type=""bibr"" target=""#b1"">[2]</ref>, a single-layer perceptron consisting of one artificial neuron connecting several input units by weighted edge",1
"us, we have not yet compared our hybrid against existing global/per-branch hybrid schemes. Per-branch and path information can yield greater accuracy <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b13"">14]</ref>, but our restriction to global information is typical of recent work i of Context Switching</head><p>Branch predictors can suffer a loss in performance after a context switch, having to warm up while relearning patterns <ref type=""bibr"" target=""#b5"">[6]</ref>. We simulate the effects of context switching by interleaving branch traces from each of the SPEC 2000 integer hybrid gshare/perceptron predictor performs better in the presence of context switching; this benefit of hybrid predictors has been noticed by others <ref type=""bibr"" target=""#b5"">[6]</ref>. </p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""6"">Implementation</head><p>We now suggest ways t ptrons in a hybrid predictor that uses both global and local histories, since hybrid predictors have proven to work better than purely global schemes <ref type=""bibr"" target=""#b5"">[6]</ref>. We have preliminary experimental evidence that such hybrid schemes can be improved by using perceptrons, and",0
"oses pipelining the predictor to reduce delay.</p><p>Jiménez et al study a number of techniques for reducing the impact of delay on branch predictors <ref type=""bibr"" target=""#b12"">[13]</ref>. For example, a cascading perceptron predictor would use a simple predictor to anticipate the address of the",0
"pcode information, as input to a trained neural network. This approach achieves an 80% correct prediction rate, compared to 75% for static heuristics <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b2"">3]</ref>. Static branch prediction performs worse than existing dynamic technique",0
"each bit in parallel if Ø Ü then Û := Û • ½ else Û := Û ½ end if</formula><p>Delay. A ¢ multiplier in a 0.25 m process can operate in 2.7 nanoseconds <ref type=""bibr"" target=""#b8"">[9]</ref>, which is approximately two clock cycles with a 700 MHz clock. At the longer history lengths, an implementatio",0
"ion</head><p>The fusion of different modalities is generally performed at two levels: feature level or early fusion and decision level or late fusion <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b44"">45,</ref><ref type=""bibr"" target=""#b121"">121]</ref>. Some researchers have also the basic SVM formalism is replaced using a non-linear kernel function.</p><p>Many existing literature use the SVM-based fusion scheme. Adams et al. <ref type=""bibr"" target=""#b2"">[3]</ref> adopted a late fusion approach in order to detect semantic concepts (e.g. sky, fire-smoke) in videos using vis atrix to classify the target concept. Unlike GLF, the NLF method is used for nonlinear combination of multimodal information. This method is based on <ref type=""bibr"" target=""#b2"">[3]</ref>, where SVM is first used as a classifier for the individual modality and then super kernel non-linear fusion i vector SVM boundary Fig. <ref type=""figure"">3</ref> SVM based score space classification of combined information from multiple intermediate concepts <ref type=""bibr"" target=""#b2"">[3]</ref> The key idea was to utilize the synchrony measure between the talking face's voice and the corresponding video coefficients of the lips region) while preserving their correlation over time. This approach is used for speech recognition. The work by Adams et al. <ref type=""bibr"" target=""#b2"">[3]</ref> also used a Bayesian network in addition to SVM and showed the comparison of both for video shot retrieval.</p et=""#b37"">[38]</ref>, Chaisorn et al. <ref type=""bibr"" target=""#b24"">[25]</ref>, Xie et al. <ref type=""bibr"" target=""#b145"">[145]</ref>, Adams et al. <ref type=""bibr"" target=""#b2"">[3]</ref> Photo and video annotation Linear weighted fusion Iyenger et al. <ref type=""bibr"" target=""#b57"">[58]</ref> lin </ref> Semantic concept detection Linear weighted fusion Iyenger et al. <ref type=""bibr"" target=""#b57"">[58]</ref> Support vector machine Adams et al. <ref type=""bibr"" target=""#b2"">[3]</ref>, Iyenger et al. <ref type=""bibr"" target=""#b57"">[58]</ref>, Wu et al. <ref type=""bibr"" target=""#b141"">[141]</re",1
"lso presented here. Some other issues (e.g. the use of correlation, confidence, and the context), also related to how to fuse, are described in Sect. <ref type=""bibr"" target=""#b3"">4</ref>. This section further elaborates the issues when to fuse (the synchronization), and what to fuse (the optimal me hrony were combined and passed to the SVM model, which provided the decision about the identity of the talking face. On another front, Aguilar et al. <ref type=""bibr"" target=""#b3"">[4]</ref> provided a comparison between the rule-based fusion and learning-based fusion (trained) strategy. The scores o",1
"e coming out of elevator'' usually occur at relative times one after another. This temporal relationship between events has been utilized by Stauffer <ref type=""bibr"" target=""#b123"">[123]</ref> for detecting events in a surveillance environment. This kind of analysis of the events has been called th",0
"lities; MYCT <ref type=""bibr"" target=""#b92"">[93]</ref> that contains 10-print fingerprint and signature modalities and several others as mentioned in <ref type=""bibr"" target=""#b103"">[104]</ref>.</p><p>Another popular dataset standardization effort has been the agenda of performance evaluation of tra ate the fusion result for biometric verification, false acceptance rate (FAR) and false rejection rate (FRR) are used to identify the types of errors <ref type=""bibr"" target=""#b103"">[104]</ref>. The FAR and FRR are often used to present the half total error rate (HTER), which is a measure to assess",0
"get=""#b120"">120]</ref>; automatic audio-visual speech recognition <ref type=""bibr"" target=""#b105"">[106]</ref>; biometric audiovisual speech synchrony <ref type=""bibr"" target=""#b19"">[20]</ref>; multi-sensor management for information fusion <ref type=""bibr"" target=""#b146"">[146]</ref>; face recognitio Gaussian distribution.</p><p>One of the most simple and widely used forms of the correlation coefficient is the Pearson's product-moment coefficient <ref type=""bibr"" target=""#b19"">[20]</ref>, which is computed as follows. Assuming that I i and I j are the two modalities (of same or different types) ors also used LSA and achieved about 42% overall improvement in error rate with CCA and 61% improvement with LSA. In another work, Bredin and Chollet <ref type=""bibr"" target=""#b19"">[20]</ref> also demonstrated the utility of considering CCA for audio-video based talking-face identity verification. S",0
"and video modalities for better location estimation in real time. A decision level fusion approach has been adopted in this work.</p><p>A recent work <ref type=""bibr"" target=""#b154"">[154]</ref> presents a multi-camera based tracking system, where multiple features such as spatial position, shape and u and Bhanu <ref type=""bibr"" target=""#b158"">[158]</ref> Kalman filter Talantzis et al. <ref type=""bibr"" target=""#b125"">[125]</ref>, Zhou and Aggarwal <ref type=""bibr"" target=""#b154"">[154]</ref>, Strobel et al. <ref type=""bibr"" target=""#b124"">[124]</ref> Human computer interaction and multimodal dial nity <ref type=""bibr"" target=""#b0"">[1]</ref>. Several researchers have used PETS datasets for multimodal analysis tasks, for example, object tracking <ref type=""bibr"" target=""#b154"">[154]</ref>.</p><p>Although there are several available datasets that can be used for various analysis tasks, there la easured tracking error and calculated average distance between true and estimated position of speaker <ref type=""bibr"" target=""#b125"">[125]</ref>. In <ref type=""bibr"" target=""#b154"">[154]</ref>, the authors calculated variance of motion direction and variance of compactness to calculate the accuracy",0
"the final decision.</p><p>A hybrid fusion approach can utilize the advantages of both early and late fusion strategies. Therefore, many researchers ( <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b87"">88,</ref><ref type=""bibr"" target=""#b149"">149]</ref>, etc.) have used the hybri >[27]</ref>.</p><p>Some of the representative works that have used the D-S fusion method for various multimedia analysis tasks are Bendjebbour et al. <ref type=""bibr"" target=""#b15"">[16]</ref> (at hybrid level) and Mena and Malpica <ref type=""bibr"" target=""#b83"">[84]</ref> (at the feature level) for rint classification, and <ref type=""bibr"" target=""#b110"">[110]</ref> for human computer interaction (at the decision level).</p><p>Bendjebbour et al. <ref type=""bibr"" target=""#b15"">[16]</ref> proposed to use the D-S theory to fuse the mass functions of two regions (cloud and no B e l i e f P l a u s t al. <ref type=""bibr"" target=""#b87"">[88]</ref> Dempster-Shafer Theory Mena and Malpica <ref type=""bibr"" target=""#b83"">[84]</ref>, Bendjebbour et al. <ref type=""bibr"" target=""#b15"">[16]</ref> Video classification and retrieval Linear weighted fusion Yan et al. <ref type=""bibr"" target=""#b151"">[151]</",0
"sion literature related to biometric identification and verification make ongoing efforts to build multimodal biometric databases. For example, BANCA <ref type=""bibr"" target=""#b13"">[14]</ref> that contains face and speech modalities; XM2VTS <ref type=""bibr"" target=""#b81"">[82]</ref> that contains syn",0
"ature level, which is also known as early fusion. The other approach is decision level fusion or late fusion <ref type=""bibr"" target=""#b44"">[45,</ref><ref type=""bibr"" target=""#b121"">121]</ref> which fuses multiple modalities in the semantic space. A combination of these approaches is also practiced evels: feature level or early fusion and decision level or late fusion <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b44"">45,</ref><ref type=""bibr"" target=""#b121"">121]</ref>. Some researchers have also followed a hybrid approach by performing fusion at the feature as well as the d rent modalities at an early stage which helps in better task accomplishment. Also, it requires only one learning phase on the combined feature vector <ref type=""bibr"" target=""#b121"">[121]</ref>. However, in this approach it is hard to represent the time synchronization between the multimodal feature s demonstrates that learning-based RBF SVM scheme outperforms the rule-based scheme based on some appropriate parameter selection.</p><p>Snoek et al. <ref type=""bibr"" target=""#b121"">[121]</ref> have compared both the early and late fusion strategies for semantic video analysis. Using the former appr used in fusion for various multimedia analysis tasks, such as video shot retrieval <ref type=""bibr"" target=""#b82"">[83]</ref>, semantic video analysis <ref type=""bibr"" target=""#b121"">[121]</ref>, news video story segmentation <ref type=""bibr"" target=""#b51"">[52]</ref>, video concept detection <ref typ age precision metric is used to determine the accuracy of semantic concept detection at the video shot level <ref type=""bibr"" target=""#b57"">[58,</ref><ref type=""bibr"" target=""#b121"">121,</ref><ref type=""bibr"" target=""#b143"">143]</ref>. For news video story segmentation, the precision and recall metr",0
"method for various multimedia analysis tasks are Bendjebbour et al. <ref type=""bibr"" target=""#b15"">[16]</ref> (at hybrid level) and Mena and Malpica <ref type=""bibr"" target=""#b83"">[84]</ref> (at the feature level) for segmentation of satellite images, Guironnet et al. <ref type=""bibr"" target=""#b43"" sions about a pixel obtained from the HMM classifier were used as mass and then the HMM outputs were combined. Similar to this work, Mena and Malpica <ref type=""bibr"" target=""#b83"">[84]</ref> also used the D-S fusion approach for the segmentation of color images for extracting information from terre ef type=""bibr"" target=""#b156"">[156]</ref> Neural networks Ni et al. <ref type=""bibr"" target=""#b87"">[88]</ref> Dempster-Shafer Theory Mena and Malpica <ref type=""bibr"" target=""#b83"">[84]</ref>, Bendjebbour et al. <ref type=""bibr"" target=""#b15"">[16]</ref> Video classification and retrieval Linear weig",0
"design the interconnection network is critical to tackle the challenges of multicore scaling in the dark silicon age.</p><p>Recently, Raghavan et al. <ref type=""bibr"" target=""#b16"">[17]</ref> proposed computational sprinting, in which a chip improves its responsiveness to short-burst of computations ial increase of inactive chip resources -Dark Silicon.</p><p>Instead of shrinking the chip or sacrificing transistor density, computational sprinting <ref type=""bibr"" target=""#b16"">[17]</ref> embraces dark silicon by leveraging the extra transistors transiently when performance really counts. Specia ype=""bibr"" target=""#b1"">[2]</ref> by assuming the chip can sustain computational sprinting for one second in the worst case, which is consistent with <ref type=""bibr"" target=""#b16"">[17]</ref>. Later we will analyze how NoC-sprinting influences the sprint duration. We first start running the benchmar",1
"system will activate the required number of cores while the others remain ""dark"". There are some existing work <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b11"">12]</ref> on adapting system configurations like core count/frequency to meet runtime application requirements. Since o ng) is to activate all the 16 cores during sprinting. While the methods to predict the application parallelism <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b11"">12]</ref> is beyond the scope of this paper, we conduct off-line profiling on PARSEC to capture the internal parallelis",0
"em5 <ref type=""bibr"" target=""#b2"">[3]</ref> full system simulator to setup a sprinting-based multicore architecture with 16 ALPHA CPUs. We use Garnet <ref type=""bibr"" target=""#b0"">[1]</ref> to model a 4 ? 4 mesh network and DSENT <ref type=""bibr"" target=""#b18"">[19]</ref> for network power analysis.",0
"rs have proposed various schemes <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b17"">18]</ref> to mitigate the latency overhead caused by frequent router wake-up. However, these techniques do not account",0
"e turned off. Figure <ref type=""figure"" target=""#fig_3"">3</ref> shows the chip power breakdown when scaling the number of cores based on the Niagara2 <ref type=""bibr"" target=""#b15"">[16]</ref> processor. We evaluate the power dissipation with McPAT <ref type=""bibr"" target=""#b12"">[13]</ref> for cores,",0
"benchmark is STREAM <ref type=""bibr"" target=""#b11"">[11]</ref> although it disregards many architectural details. Babka and T?ma present their work in <ref type=""bibr"" target=""#b4"">[4]</ref>, focusing mainly on translation lookaside buffers and cache associativity. Peng et al. compare the memory perf",1
"the Nehalem processor provides the closely related states shared and forward. However, the forward state apparently is only relevant for the L3 cache <ref type=""bibr"" target=""#b8"">[8]</ref>. A cache line that is shared among multiple processors will only be in forward state in one L3 cache. For exam . In general, our L1-L3 latency results correspond well to the numbers that have been published by the vendors <ref type=""bibr"" target=""#b3"">[3,</ref><ref type=""bibr"" target=""#b8"">8]</ref>.</p><p>The latencies to other cores on the same processor strongly depend on the cache line's coherency state. ons. This is mostly due to the novelty of the Nehalem microarchitecture and the MESIF protocol. Some information can be gathered from Intel documents <ref type=""bibr"" target=""#b8"">[8,</ref><ref type=""bibr"" target=""#b9"">9]</ref>.</p><p>Another important aspect is the use of a performance measurement",0
"Intel's Turbo Boost Technology. Another important factor is the architecture of the memory subsystem in conjunction with the cache coherency protocol <ref type=""bibr"" target=""#b12"">[12]</ref>.</p><p>While the basic memory hierarchy structure is similar for Nehalem and Shanghai systems, the implement et al. compare the memory performance of dual-core processors including a ping-pong implementation to analyze the latency of cache-to-cache transfers <ref type=""bibr"" target=""#b12"">[12]</ref>. However, they do not cover inter-core bandwidths. To the best of the authors knowledge, the memory benchmar",0
"ons automatically may appear to be a relatively simple task which can be easily extracted from who is 'standing with whom' (as suggested by Yu et al. <ref type=""bibr"" target=""#b19"">[20]</ref>), but one quickly finds that in more crowded situations where perhaps, the more interesting behavioural phen ed framework from which more complex social behaviours can be identified. Second, we make improvements over the baseline method proposed by Yu et al. <ref type=""bibr"" target=""#b19"">[20]</ref> by formulating the problem as one of identifying dominant sets <ref type=""bibr"" target=""#b16"">[17]</ref>. A get=""#b21"">[22]</ref>. However, the identification of F-formations was not explicitly addressed.</p><p>The closest work to ours was done by Yu et al. <ref type=""bibr"" target=""#b19"">[20]</ref>, who proposed a system that could track and discover groups of interacting people. The modularity cut algori ice of data and experimental design. The 30 minute test data set was rather artificial as 23 people were asked to ""mingle in a 3-group configuration"" <ref type=""bibr"" target=""#b19"">[20]</ref> (p.1468). Three groups were indeed identified but given the average number of people per group, it is unlike ch of its elements aij = w(i, j).</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.1"">F-formations as High Modularity</head><p>Yu et al. <ref type=""bibr"" target=""#b19"">[20]</ref> defined the task of identifying groups of people in terms of the recursive global partitioning of the graph roximity.</head><p>Perhaps the most obvious way of measuring the affinity between people is to use their relative proximity, as proposed by Yu et al. <ref type=""bibr"" target=""#b19"">[20]</ref>. The symmetric distance function between person i and j is defined as</p><formula xml:id=""formula_17"">A prox when w S (i) &gt; 0 for any i not in the identified dominant set S (GC), as described in Sec. 6. We compare our results with the method of Yu et al. <ref type=""bibr"" target=""#b19"">[20]</ref>, that used modularity cut and proximity to create the affinity matrix (MC: Sec. 5).</p><p>For space reasons, used the proximity and SMEFO features with modularity cut and Kernighan-Lin refinement (MC+KL), though the performance in all cases was improved over <ref type=""bibr"" target=""#b19"">[20]</ref>. In addition to the performance of different methods, we also provide a baseline based on if all people in t ting singletons in the data.</p><p>Table <ref type=""table"" target=""#tab_1"">2</ref> shows that all our proposed methods out-performs that the baseline <ref type=""bibr"" target=""#b19"">[20]</ref>. Both fully manual methods with our proposed dominant set technique (P+O, DS) performed the best but was com rmation alone. When comparing the singleton baseline to all other methods, we see that using the basic modularity cut algorithm proposed by Yu et al. <ref type=""bibr"" target=""#b19"">[20]</ref> only just out performs it, indicating how poor modularity cut is for detecting F-formations. We also found t paper, a new approach for detecting F-formations was presented by formulating the problem as one of finding dominant sets. Compared to modularity cut <ref type=""bibr"" target=""#b19"">[20]</ref>, significant and more stable performance improvements were observed. Our SMEFO feature also provided better s not as important as how connected each person is to everyone else in the network. For the task of identifying spatially separated groups, Yu et al. <ref type=""bibr"" target=""#b19"">[20,</ref><ref type=""bibr"" target=""#b2"">3]</ref> have shown the effectiveness of using modularity cut. However, as we h",1
"hough some studies have started to consider scenarios where more people <ref type=""bibr"" target=""#b6"">(7)</ref><ref type=""bibr"" target=""#b7"">(8)</ref><ref type=""bibr"" target=""#b8"">(9)</ref><ref type=""bibr"" target=""#b9"">(10)</ref><ref type=""bibr"" target=""#b10"">(11)</ref><ref type=""bibr"" target=""#b11"" ect people who are walking together as the scenes are more likely to be filled by mostly unacquainted groups <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b0"">1]</ref>. Unacquainted groups will interact with each other through avoidance stra on detecting groups has tended to focus more on finding people who are 'together' based on the persistence of their proximity and direction of motion <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b0"">1,</ref><ref type=""bibr"" target=""#b17"">18",0
"=""bibr"" target=""#b7"">(8)</ref><ref type=""bibr"" target=""#b8"">(9)</ref><ref type=""bibr"" target=""#b9"">(10)</ref><ref type=""bibr"" target=""#b10"">(11)</ref><ref type=""bibr"" target=""#b11"">(12)</ref> are involved <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b12"">13]</ref>. In most stud of the function. Given the nature of our data, ? was set to approximately 2 metres; the commonly accepted distance at which focused encounters occur <ref type=""bibr"" target=""#b11"">[12]</ref>. Note that unlike Yu et al., our affinity matrix is generated from a single static image.</p></div> <div xml",0
"the scenes are more likely to be filled by mostly unacquainted groups <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b0"">1]</ref>. Unacquainted groups will interact with each other through avoidance strategies but this is clearly a different r' based on the persistence of their proximity and direction of motion <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b0"">1,</ref><ref type=""bibr"" target=""#b17"">18]</ref> during walking, which are readily measurable from the extracted traject",0
"er the baseline method proposed by Yu et al. <ref type=""bibr"" target=""#b19"">[20]</ref> by formulating the problem as one of identifying dominant sets <ref type=""bibr"" target=""#b16"">[17]</ref>. A dominant set is a form of maximal clique that can be applied to edge weighted graphs so that the affinity Based on these definitions, it seems clear that using modularity to identify F-formations is not precise enough for our task.</p><p>Pavan and Pelillo <ref type=""bibr"" target=""#b16"">[17]</ref> proposed a different way of thinking about a cluster as a dominant set, which is a generalisation of maximal ue sizes are not affected by the number of positive links and the cardinality of the graph <ref type=""bibr"" target=""#b7"">[8]</ref>. Pavan and Pelillo <ref type=""bibr"" target=""#b16"">[17]</ref> showed that the notion of a cluster and its relationship to dominant sets were mathematically equivalent by l></formula><formula xml:id=""formula_15"">)</formula><p>where W (S) = P i?S wS(i) is the total weight, which must be greater than 0. Pavan and Pelillo <ref type=""bibr"" target=""#b16"">[17]</ref> proved that by using this definition of x, the maximisation of the objective function is the same as finding this definition of x, the maximisation of the objective function is the same as finding dominant sets. Further details of this proof can be found in <ref type=""bibr"" target=""#b16"">[17]</ref>.</p><p>To find a local solution of the objective function f , a method taken from evolutionary game theory, verges on the most dominant set of a particular graph, an effective way of identifying further clusters in the network is to apply a peeling strategy <ref type=""bibr"" target=""#b16"">[17]</ref>. This involves identifying a dominant set using Eq. 13, removing the corresponding nodes, and then re-applyi ose that are not clustered yet are more likely to be singletons than in an F-formation. We modify the peeling strategy suggested by Pavan and Pelillo <ref type=""bibr"" target=""#b16"">[17]</ref> by introducing a more principled stopping criterion, which takes into account the global context of the comp",0
"order (like a sequential fetch unit).</p><p>The basic idea is very similar to instruction fetch in Multiscalar <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b20"">21]</ref>: Multiscalar divides the sequential instruction stream into tasks and assigns each task to a processing eleme arallel Renaming</head><p>T h e fi r s t s o l u t i o n i s s i m i l a r t o t h a t u s e d b y Multiscalar <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b20"">21]</ref>. When a fragment is renamed, the hardware determines which register mappings are not yet available (either vi the program can be obtained by concatenating all fragments. This is similar to the idea of traces <ref type=""bibr"" target=""#b19"">[20]</ref> or tasks <ref type=""bibr"" target=""#b20"">[21]</ref>, except that fragments are completely general, whereas the other terms make assumptions about the nature of",1
"ase the time in which no fetch activity occurs.</p><p>Many enhancements to these mechanisms have been proposed <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b21"">",0
"f><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b17"">18]</ref>. This rearrangement may be done statically, for example at link time <ref type=""bibr"" target=""#b11"">[12]</ref>, or dynamically <ref type=""bibr"" target=""#b0"">[1]</ref>. Since this approach is not always successful, espec",0
"arget=""#b17"">18]</ref>. This rearrangement may be done statically, for example at link time <ref type=""bibr"" target=""#b11"">[12]</ref>, or dynamically <ref type=""bibr"" target=""#b0"">[1]</ref>. Since this approach is not always successful, especially as the demands on fetch bandwidth are increased, oth ce the fetch unit may be fetching many wrong-path instructions. However, it is important to realize that there are two problems in instruction fetch: <ref type=""bibr"" target=""#b0"">(1)</ref> what to fetch, and (2) how to fetch. Parallel fetch is aimed at the second problem, and the above results show",0
"aches to achieve high fetch throughput were possible. Furthermore, it makes inefficient use of storage resources due to fragmentation and duplication <ref type=""bibr"" target=""#b16"">[17]</ref>.</p><p>Sequential fetch mechanisms, like in-order issue mechanisms, are also susceptible to stalls. A stall",0
"c. We measure the amount of <ref type=""bibr"">IPC</ref>  Register traffic characteristics. We collect a number of characteristics concerning registers <ref type=""bibr"" target=""#b5"">[6]</ref>. Our first characteristic is the average number of input operands to an instruction. Our second characteristic",1
"hed for both instruction and data accesses.</p><p>Data stream strides. The data stream is characterized with respect to local and global data strides <ref type=""bibr"" target=""#b9"">[10]</ref>. A global stride is defined as the difference in the data memory addresses between temporally adjacent memory",1
"r work, researchers have proposed benchmark suite composition techniques <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b11"">12]</ref>. These techniques first measure a number of program characteristics, then apply principal components analysis",0
"f work has also been done on the correlation between microarchitecture-independent program characteristics and processor performance, see for example <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b13"">14]</ref>. However, these techniques do no",0
"nto how (dis)similar benchmarks are from each other.</p><p>Based on this prior work, researchers have proposed benchmark suite composition techniques <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b11"">12]</ref>. These techniques first measure",0
"a good quantitative measure for program similarity. Several researchers have proposed methods for quantifying program similarity. Saavedra and Smith <ref type=""bibr"" target=""#b12"">[13]</ref> use the squared Euclidean distance computed in a benchmark space built up using dynamic program characterist",0
"ation between microarchitecture-independent program characteristics and processor performance, see for example <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b13"">14]</ref>. However, these techniques do not predict performance for an applicatio",0
"s the microarchitecture-independent characteristics to correlate, gets a higher weight in the Euclidean distance. Principal components analysis (PCA) <ref type=""bibr"" target=""#b6"">[7]</ref> is a statistical data analysis technique that extracts uncorrelated dimensions from a data set.</p><p>The inpu",0
"rk suite. This reduced benchmark suite yields accurate performance predictions compared the original benchmark suite.</p><p>The current paper extends <ref type=""bibr"" target=""#b10"">[11]</ref> which used the above workload characterization methodology consisting of principal components analysis and c differences in microarchitectureindependent characteristics relate to performance differences yields better results. The current paper also improves <ref type=""bibr"" target=""#b10"">[11]</ref> in three other ways: (i) the use of a better set of microarchitecture-independent characteristics, (ii) limi cs, (ii) limiting the number of proxies and (iii) the use of more benchmarks in the evaluation. Doing a head-to-head comparison between the method in <ref type=""bibr"" target=""#b10"">[11]</ref> and our approach, for the benchmarks considered in <ref type=""bibr"" target=""#b10"">[11]</ref>, shows an impro n. Doing a head-to-head comparison between the method in <ref type=""bibr"" target=""#b10"">[11]</ref> and our approach, for the benchmarks considered in <ref type=""bibr"" target=""#b10"">[11]</ref>, shows an improvement of the average rank correlation coefficient from 0.76 to 0.91. A large body of work ha",0
"re branch predictability in a microarchitecture-independent manner we used the Prediction by Partial Matching (PPM) predictor proposed by Chen et al. <ref type=""bibr"" target=""#b1"">[2]</ref>, which is a universal compression/prediction technique.</p><p>A PPM predictor is built on the notion of a Mark",0
"ional in size to a program's memory footprint, which precludes fast prediction.</p><p>A recent proposal, the Dead-Block Correlating Prefetcher (DBCP) <ref type=""bibr"" target=""#b11"">[12]</ref>, achieves maximal prefetch lookahead through correlation to ""last touch"" accesses-the last access to each ca achieved by a perfect L1D. In contrast, the delta-correlating GHB prefetcher achieves only 31% performance improvement. DBCP with 2MB on-chip storage <ref type=""bibr"" target=""#b11"">[12]</ref> achieves only 17% performance improvement. Finally, increasing L2 cache size by a factor of four achieves on div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2"">Background: DBCP Prefetching</head><p>Lai et al. proposed Dead-Block Correlated Prefetching <ref type=""bibr"" target=""#b11"">[12]</ref> to predict the last touch to a cache block prior to that block's eviction, and replace the block by prefetch div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2.1"">Why is DBCP Impractical?</head><p>Figure <ref type=""figure"">3</ref> depicts the anatomy of DBCP <ref type=""bibr"" target=""#b11"">[12]</ref>. On every memory access, DBCP computes a last-touch history trace by combining the PC and address of that ac der.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.3"">Lookahead for Sequence Retrieval</head><p>Cache misses are frequently clustered <ref type=""bibr"" target=""#b11"">[12]</ref>, corresponding to bursts of last touches. During such bursts, last touches may remain undetected by LT-cords persist across context switches</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.1"">Recording Last-Touch Signatures</head><p>As in DBCP <ref type=""bibr"" target=""#b11"">[12]</ref>, LT-cords constructs signatures using the history table. Each history table entry maintains a PC trace of co e=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b14"">15]</ref>. The realistic DBCP is implemented with a 2MB on-chip correlation table as in <ref type=""bibr"" target=""#b11"">[12]</ref>. For comparison with a larger L2, we quadruple the size of the L2 cache of the baseline processor to 4MB, co tion of L1 block dead-times-the number of cycles between a last touch to a block until its eventual eviction. 1 The figure corroborates prior results <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b25"">26]</ref>, showing that over 85% of al",1
"ecific access patterns, such as strided accesses <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b18"">19]</ref>, pointer dereference <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b7"">8]</ref>, or accesses to linked data structures <ref type=""bibr"" target=""#b16"">[1",0
"cycles between a last touch to a block until its eventual eviction. 1 The figure corroborates prior results <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b25"">26]</ref>, showing that over 85% of all cache-block dead-times are longer than",0
"e processor-memory performance gap. Many proposals are inherently limited in scope, targeting only specific access patterns, such as strided accesses <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b18"">19]</ref>, pointer dereference <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=",0
"ry Buffer (GHB) prefetcher <ref type=""bibr"" target=""#b14"">[15]</ref> was recently shown to outperform a variety of other hardware prefetching schemes <ref type=""bibr"" target=""#b8"">[9]</ref>. Although delta correlation subsumes many previous approaches, it is not effective for data structures with ir achieves negligible coverage with less than 80MB. Our results corroborate prior work showing that DBCP with practically-sized storage is ineffective <ref type=""bibr"" target=""#b8"">[9]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3"">LT-cords</head><p>In this paper, we propose La CP implementation, and a baseline processor with a larger L2 cache. GHB uses 256-entry index and history tables, as recommended for SPEC applications <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b14"">15]</ref>. The realistic DBCP is implemented with a 2MB on-chip correlation tabl of the L2 cache of the baseline processor to 4MB, conservatively assuming the same access latency as the base 1MB cache. We corroborate prior results <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b14"">15]</ref> showing that GHB, an advanced stride/delta correlating predictor, can",0
"blocks to other cache lines in the same/different cache set or in the SRAM in order to reduce the average write frequency of the STT-RAM cache lines <ref type=""bibr"" target=""#b4"">[4]</ref>.</p><p>However, there exist intrinsic limitations in both approaches, which cannot be resolved independently - his work, we assume that the L2 cache is a hybrid cache architecture with 4-way SRAM and 12-way STT-RAM, which is similar to the setting described in <ref type=""bibr"" target=""#b4"">[4]</ref> <ref type=""bibr"" target=""#b7"">[7]</ref>. The block-level initial placement and dynamic migration is allowed to 2 cache than other data which are written more frequently. Note that life time of the STT-RAM mainly depends on the peak write count of all the cells <ref type=""bibr"" target=""#b4"">[4]</ref> <ref type=""bibr"" target=""#b5"">[5]</ref>. Even static optimizations can reduce the total STT-RAM writes compare tation application <ref type=""bibr"" target=""#b8"">[8]</ref> for both of the pure static <ref type=""bibr"" target=""#b10"">[10]</ref> and the pure dynamic <ref type=""bibr"" target=""#b4"">[4]</ref> scheme. The peak write count of static optimization is significantly larger than that of the dynamic one. The o correct the initial placement by migrating the actually write-intensive STT-RAM data blocks to SRAM. We use the dynamic migration scheme similar to <ref type=""bibr"" target=""#b4"">[4]</ref>, which is briefly described as follows. Each L2 cache block is associated with a saturate 2-bit write counter p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>Pure dynamic optimization (dynamic):</head><p>We use the dynamic migration scheme proposed in <ref type=""bibr"" target=""#b4"">[4]</ref>. Our dynamic migration scheme in Section 4.3 uses this scheme with the same migration threshold as 3. There is measured from the start of the simulation until the first STT-RAM line becomes defective, which is similar to the estimation methodology proposed in <ref type=""bibr"" target=""#b4"">[4]</ref> and <ref type=""bibr"" target=""#b5"">[5]</ref>.</p><p>Figure <ref type=""figure"">4</ref> demonstrates the lifetime",1
".0""><head>Figure 2: One sample code and its memory access behavior</head><p>To capture this effect, we use the concept of memory reuse distance (MRD) <ref type=""bibr"" target=""#b11"">[11]</ref>, which equals the total size of unique data elements accessed between two references to X. A larger memory r",0
"he compiler support for hint generation is implemented based on LLVM compiler infrastructure <ref type=""bibr"" target=""#b15"">[15]</ref>. Omega library <ref type=""bibr"" target=""#b16"">[16]</ref> is used in this flow to perform memory dependency analysis. Given a source program written in C/C++, we pars",0
"orrelating Prefetchers (DBCPs). A DBP is a novel hardware mechanism that predicts ""when"" a block in a data cache becomes evictable. In a recent paper <ref type=""bibr"" target=""#b6"">[7]</ref>, we proposed trace-based predictors that record a trace of shared memory references to predict a last referenc o-level predictor to predict both a cache block replacement and a subsequent address to prefetch for the corresponding block frame. In a recent paper <ref type=""bibr"" target=""#b6"">[7]</ref>, we proposed Last-Touch Predictors (LTPs) to predict memory invalidations for shared data in a multiprocessor. ce-based DBP. A history table duplicates the L1 tag array and stores a trace encoding associated with every tag. We use truncated addition (as before <ref type=""bibr"" target=""#b6"">[7]</ref>) to maintain a fixed-size encoding for every instruction trace. In practice, truncated addition allows a compa ities in applications, multiple cache blocks may have dead-block signatures that are proper subsequences of each other resulting in subtrace aliasing <ref type=""bibr"" target=""#b6"">[7]</ref>. To prevent aliasing, DBP maintains deadblock signatures per cache block address. Because, the number of signa d predictors to predict memory system events is beyond just predicting memory invalidation and sharing for scientific applications in multiprocessors <ref type=""bibr"" target=""#b6"">[7]</ref>. The results corroborate the intuition that because memory instructions drive the movement of data in the cach ory depth increases an MCP's accuracy and coverage is that while data structures are often referenced in multiple distinct program contexts or phases <ref type=""bibr"" target=""#b6"">[7]</ref> (e.g., a given sequence procedure invocations), they are not always used in conjunction with the same other da",1
"arget=""#b5"">6]</ref> or software <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b11"">12]</ref>, many researchers and vendors opt for hardware implementations for transparency and due to availability of ru f memory access patterns to detect both what memory addresses are subsequently referenced and when the data can be placed in the cache. Mowry, et al. <ref type=""bibr"" target=""#b11"">[12]</ref>, show that for numerical and scientific applications, software prefetchers can successfully hide the memory",0
"ng helps fetch data in advance to hide the memory latency by predicting future memory requests. While prefetching can be initiated in either hardware <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b2"">3, pe=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b5"">6]</ref> and accesses to linked data structures <ref type=""bibr"" target=""#b16"">[17]</ref>. While effective for the targeted access patterns, these prefetchers have limited general applicability acro get=""#b9"">[10]</ref> proposed the indirect reference buffers to identify data address dependence in recursive or linked data structures. Roth, et al. <ref type=""bibr"" target=""#b16"">[ 17]</ref>, proposed prefetchers that capture memory reference dependence in linked data structures and associate them",0
"ig_2"">6</ref> also compares the prediction accuracy and coverage of DBP against simple LRU stacks (bars labeled ""B"" and ""C"") proposed by Peir, et al. <ref type=""bibr"" target=""#b15"">[16]</ref> to predict evictability of L1 blocks. The LRU stacks simply maintain a list of least-recently used addresses applications <ref type=""bibr"" target=""#b17"">[ 18]</ref>, LRU stacks fail to predict dead blocks accurately. The graphs corroborate previous findings <ref type=""bibr"" target=""#b15"">[16]</ref> on stacks and indicate that a 64-entry stack achieves a coverage of 77% while prematurely predicting dead bl",0
"uction. Feedbackbased CDG relies on feedback from the coverage analysis to automatically modify the directives to the test generator. For example, in <ref type=""bibr"" target=""#b1"">[2]</ref>, a genetic algorithm is used to select and modify test-cases to increase coverage. In <ref type=""bibr"" target=",1
"a is used to modify the parameters of a Markov Chain that represents the DUT. The Markov Chain is then used to generate test-cases for the design. In <ref type=""bibr"" target=""#b10"">[11]</ref>, the coverage analysis results trigger a set of generation rules that modify the testing directives. In cont",1
"erification. Moreover, if a coverage task is reached via different directions, the chances to discover hidden bugs related to this task are increased <ref type=""bibr"" target=""#b7"">[8]</ref>.</p><p>In the past, two general approaches for CDG have been proposed: feedback-based CDG and CDG by construct",0
"anipulations in high dimensions (the ""curse of dimensionality""). The main breakthrough emerged in the late 1980s and can be attributed to Judea Pearl <ref type=""bibr"" target=""#b11"">[12]</ref>, who introduced 'modularity', thus enabling large and complex models and theirs associated calculations, to le X i given its parents Pa i . Together, these two components represent a unique joint probability distribution over the complete set of variables X <ref type=""bibr"" target=""#b11"">[12]</ref>. The joint probability distribution is given by the following equation:</p><formula xml:id=""formula_0"">p(X) to learn both the structure and probabilities of a Bayesian network (cf. <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b11"">12]</ref>).</p><p>Typical types of queries that can be efficiently answered by the Bayesian network model are derived f",0
"ww.tei-c.org/ns/1.0""><head n=""1."">INTRODUCTION</head><p>Functional verification is widely acknowledged as the bottleneck in the hardware design cycle <ref type=""bibr"" target=""#b0"">[1]</ref>. To date, up to 70% of the design development time and resources are spent on functional verification. The inc",0
"les. This representation was originally designed to encode the uncertain knowledge of an expert and can be dated back to the geneticist Sewall Wright <ref type=""bibr"" target=""#b14"">[15]</ref>. Their initial development in the late 1970s was motivated by the need to model the top-down (semantic) and",0
"CDG by construction, an external model of the DUT is used to generate test directives designed to accurately hit the coverage tasks. For example, in <ref type=""bibr"" target=""#b13"">[14]</ref> an FSM model of pipelines is used to generate tests that cover instruction interdependencies in the pipes.</",0
"ved values.</p><p>There are two important extensions of Bayesian networks: Dynamic Bayesian networks and influence diagrams. The first extension (see <ref type=""bibr"" target=""#b5"">[6]</ref>) enables the incorporation of time, thus modeling temporal dependencies in a stochastic process. The second ex led the temporal dependencies between the instructions and coverage tasks and among the instructions using a two-slice Dynamic Bayesian Network (DBN) <ref type=""bibr"" target=""#b5"">[6]</ref>. Rather than an accurate mapping of the specific state machine structure, the DBN encoded the general knowledg",0
"our approach is based on modeling the relationship between the coverage information and the directives to the test generator using Bayesian networks <ref type=""bibr"" target=""#b8"">[9]</ref>. A Bayesian network is a directed graph whose nodes are random variables and whose edges represent direct depe d to a growing interest in using data to learn both the structure and probabilities of a Bayesian network (cf. <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b11"">12]</ref>).</p><p>Typical types of queries that can be efficiently answered by th",0
"vailable and cheaper to acquire, has led to a growing interest in using data to learn both the structure and probabilities of a Bayesian network (cf. <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b11"">12]</ref>).</p><p>Typical types of queries br"" target=""#b5"">[6]</ref>) enables the incorporation of time, thus modeling temporal dependencies in a stochastic process. The second extension (see <ref type=""bibr"" target=""#b2"">[3]</ref>) enriches the Bayesian network paradigm with decision making and utility considerations which create a powerfu and construct a training set out of the directives used and the resulting coverage tasks. We then use one of the many known learning algorithms (cf. <ref type=""bibr"" target=""#b2"">[3]</ref>) to estimate the Bayesian network's parameters (i.e. the set of conditional probability distributions). This c",0
"ular multiplication without affecting the modular addition and subtraction algorithms.</p><p>Other recent algorithms for modular arithmetic appear in <ref type=""bibr"" target=""#b2"">[3]</ref>, <ref type=""bibr"" target=""#b5"">[6]</ref>.</p><p>Fix N &gt; 1. Define an N-residue to be a residue class modulo",1
"</abstract> 		</profileDesc> 	</teiHeader> 	<text xml:lang=""en""> 		<body> <div xmlns=""http://www.tei-c.org/ns/1.0""><p>1. Description. Some algorithms <ref type=""bibr"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b3"">[4]</ref>, <ref type=""bibr"" target=""#b",0
"modular addition and subtraction algorithms.</p><p>Other recent algorithms for modular arithmetic appear in <ref type=""bibr"" target=""#b2"">[3]</ref>, <ref type=""bibr"" target=""#b5"">[6]</ref>.</p><p>Fix N &gt; 1. Define an N-residue to be a residue class modulo N. Select a radix R coprime to N (possib",0
"ns=""http://www.tei-c.org/ns/1.0""><p>1. Description. Some algorithms <ref type=""bibr"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b3"">[4]</ref>, <ref type=""bibr"" target=""#b4"">[5]</ref> require extensive modular arithmetic. We propose a representation of",0
"> 	<text xml:lang=""en""> 		<body> <div xmlns=""http://www.tei-c.org/ns/1.0""><p>1. Description. Some algorithms <ref type=""bibr"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b3"">[4]</ref>, <ref type=""bibr"" target=""#b4"">[5]</ref> require extensive modular ar",0
"ion rules Acquirement</head><p>Transformation Based Error-Driven Learning is an effective learning algorithm which was proposed by Eric Brill in 1992 <ref type=""bibr"" target=""#b13"">[14]</ref> . It could obtain transformation rules automatically during the learning process. In this paper, the reasons",1
"b2"">3]</ref> , Hidden Markov Model (HMM) <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b4"">5]</ref> , Condition Random Field (CRF) <ref type=""bibr"" target=""#b5"">[6]</ref> , Decision Tree <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b7"">8]</ref> , Support Vector",0
"ation/Clustering, Machine Translation and Chunking.</p><p>The statistical models which were used in Japanese NER, such as Maximum Entropy Model (MEM) <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b2"">3]</ref> , Hidden Markov Model (HMM) <ref type=""bibr"" target=""#b3"">[4,</ref><ref",0
"correctly. They always imply the main contents of the article. Named Entity was proposed by Message Understanding Conference (MUC) for the first time <ref type=""bibr"" target=""#b0"">[1]</ref> . Named Entity Recognition (NER) is a difficult and challenging task, and it is a very important subtask of In",0
""" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b4"">5]</ref> , Condition Random Field (CRF) <ref type=""bibr"" target=""#b5"">[6]</ref> , Decision Tree <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b7"">8]</ref> , Support Vector Machine (SVM) <ref type=""bibr"">[9~12]</ref> , have perf",0
"clusters on the left hand side consists of a multithreaded SPL fabric shared by four single issue out-of-order processor cores. In our previous work <ref type=""bibr"" target=""#b44"">[43]</ref>, we evaluated the use of SPL with a range of in-order and out-of-order core types and found that a simple ou tional clusters (as well as other cluster types) are possible, but this consideration is beyond the scope of this paper.</p><p>Each SPL, adopted from <ref type=""bibr"" target=""#b44"">[43]</ref> and shown in more detail in Figures <ref type=""figure"" target=""#fig_1"">1(b</ref>) and (c), is a highly pipel e and power efficiency. In the remainder of Section 2, we provide an overview of the SPL microarchitecture. A more complete treatment is available in <ref type=""bibr"" target=""#b44"">[43]</ref>. </p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>Rows/ SPL</head><note type=""other"">Total</note></ div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2.1"">SPL Hardware Microarchitecture</head><p>We adopt the row-based fabric of our earlier work <ref type=""bibr"" target=""#b44"">[43]</ref> in which the space of shared SPL configurations was explored using validated SPL delay, power, and area mode y is not an issue as all configurations are immediately available after the initial configuration overhead is paid.</p><p>Using our analytical models <ref type=""bibr"" target=""#b44"">[43]</ref>, we arrive at the area and power results for eight single issue out-of-order cores, eight private 12-row SPL a than the private SPLs, and is much more power-efficient as well. While one might consider shrinking the private SPL even further, our previous work <ref type=""bibr"" target=""#b44"">[43]</ref> has shown that that this yields poor performance.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n temporal sharing among the threads executing on the four processor cores, as well as spatial partitioning to permit private or semi-private operation <ref type=""bibr"" target=""#b44"">[43]</ref>. Spatial partitioning is enabled by inserting additional multiplexers at each point where the SPL pool might parallelization with custom ISA support provides larger speedups than the product of the two techniques applied in isolation.</p><p>Our previous work <ref type=""bibr"" target=""#b44"">[43]</ref> identifies a number of characteristics of past reconfigurable proposals that are found to be highly amenable ared SPL based on these fea-tures, and analyzed the impact of incorporating the fabric with processors of different complexity. While the emphasis of <ref type=""bibr"" target=""#b44"">[43]</ref> is on the fabric design, this paper proposes a complete hardware/software approach to managing multithreaded temporal sharing among the threads executing on the four processor cores, as well as spatial partitioning to permit private or semi-private operation<ref type=""bibr"" target=""#b44"">[43]</ref>. Spatial partitioning is enabled by inserting additional multiplexers at each point where the SPL pool might",1
"hat compilers can produce good mappings for reconfigurable architectures <ref type=""bibr"" target=""#b2"">[2,</ref><ref type=""bibr"" target=""#b4"">4,</ref><ref type=""bibr"" target=""#b46"">45]</ref>. Since dynamic thread scheduling is most useful when applications experience phase changes, we need to run th",0
"ies that combine machine learning, phased-based analysis, and stability control. Prior work in optimizing resource allocation during different phases <ref type=""bibr"" target=""#b15"">[14,</ref><ref type=""bibr"" target=""#b34"">33]</ref> only address single applications. In our multithreaded environment,",0
"th that of the processor core frequency of 2GHz (the same as the Pentium Core2 Duo <ref type=""bibr"" target=""#b25"">[24]</ref> and the AMD X2 Dual-Core <ref type=""bibr"" target=""#b1"">[1]</ref>, both of which are implemented in 65 nm). This latency permits each row to complete the longest possible compu",0
""">INTRODUCTION</head><p>Reconfigurable logic has been proposed as one possible way to improve the performance and power efficiency of microprocessors <ref type=""bibr"" target=""#b24"">[23,</ref><ref type=""bibr"" target=""#b42"">41]</ref>. Researchers have proposed specialized fabrics that are specifically "">RELATED WORK</head></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""6.1"">Reconfigurable Processors</head><p>Several excellent survey papers <ref type=""bibr"" target=""#b24"">[23,</ref><ref type=""bibr"" target=""#b42"">41]</ref> provide an overview of the contributions of prior reconfigurable com",0
"rthogonal. For instance, common stride prefetchers <ref type=""bibr"" target=""#b3"">[2,</ref><ref type=""bibr"" target=""#b5"">4]</ref> and the predictor in <ref type=""bibr"" target=""#b13"">[12]</ref> separate the stream of misses according to the PC of the missing memory access instruction, which we call PC heme captures well common application behavior and we show that it can be easily implemented on a simple extension to the Global History Buffer (GHB) <ref type=""bibr"" target=""#b13"">[12]</ref>, which is a convenient structure to implement various prefetching algorithms. The paper focuses on the lowes fetcher with benchmarks from the SPEC 2006 and BioBench suites and compare it against both a state-of-the-art PC based localization prefetcher -PC/DC <ref type=""bibr"" target=""#b13"">[12]</ref> -and its non-localized counterpart -G/DC <ref type=""bibr"" target=""#b10"">[9,</ref><ref type=""bibr"" target=""#b s Space</head><p>Based on the mostly orthogonal stages of localization and correlation, a taxonomy to classify prefetching algorithms was proposed in <ref type=""bibr"" target=""#b13"">[12]</ref>. In this taxonomy, each algorithm is denoted as a pair X/Y, where X denotes the method used to localize the ance, a simple stride prefetcher with PC based localization and constant stride correlation would be referred to as PC/CS, and the best prefetcher in <ref type=""bibr"" target=""#b13"">[12]</ref> uses PC based localization and address delta correlation and is then referred to as PC/DC. A prefetcher that ch degrees. Unfortunately, to increase coverage and amortize long memory access times, these prefetchers often resort to large degrees of prefetching <ref type=""bibr"" target=""#b13"">[12]</ref>. The final result is that this leads to two unfortunate behaviors. First, prefetching a large number of line /1.0""><head n=""2.3"">The Global History Buffer</head><p>Many (hardware) data structures can be used to implement different prefetching algorithms, but <ref type=""bibr"" target=""#b13"">[12]</ref> has proposed a common data structure that is flexible enough to allow efficient implementation of a number o ich allows for the implementation of different localization schemes. Figure <ref type=""figure"">2a</ref> shows an example GHB for the PC/DC prefetcher <ref type=""bibr"" target=""#b13"">[12]</ref>. A detailed description of a hardware implementation of delta correlation using the GHB can be found in <ref advantages over other methods to store miss history, such as tables: reduction of stale data, a more complete history, and lower storage requirements <ref type=""bibr"" target=""#b13"">[12]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3."">STREAM CHAINING</head><p>In this section we etcher that uses the stream chaining approach, namely the Miss Graph prefetcher. To accommodate the new level of operation, we extend the taxonomy of <ref type=""bibr"" target=""#b13"">[12]</ref> with a third term, so that prefetching schemes are denoted by a triple X/Y/Z, where X and Y are as before, a NextIT is invalid and Ctr is set to zero on all entries of the IT. As misses occur, the IT and the GHB are populated as described in Section 2.3 and <ref type=""bibr"" target=""#b13"">[12]</ref>. The new PreviousIT pointer is left pointing to the last stream to suffer a miss (i.e., last IT entry used). structure. The additional storage our prefetcher re-quires are the NextIT and Ctr for each IT entry and a single PreviousIT register. As observed in <ref type=""bibr"" target=""#b13"">[12]</ref> and in our own experience, both an IT and a GHB with 512 entries each are sufficient to capture the prefetch ><head n=""4."">EVALUATION METHODOLOGY</head></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.1"">Prefetching Algorithms</head><p>We use PC/DC <ref type=""bibr"" target=""#b13"">[12]</ref> as a representative of a modern highperformance prefetcher. This prefetcher is based on PC stream localizati s first proposed in <ref type=""bibr"" target=""#b10"">[9]</ref> for TLB prefetching. The adaptation of delta correlation to data prefetching was done in <ref type=""bibr"" target=""#b13"">[12]</ref>, where the PC/DC prefetcher was proposed. That work also proposed the GHB structure used in our work and int s from the SPEC 2006 and BioBench suites and compared it against a state-of-the-art memory access instruction PC based localization prefetcher -PC/DC <ref type=""bibr"" target=""#b13"">[12]</ref>. Experimental results showed that the proposed prefetcher consistently achieves better performance than PC/D lization prefetcher -PC/DC <ref type=""bibr"" target=""#b13"">[12]</ref> -and its non-localized counterpart -G/DC <ref type=""bibr"" target=""#b10"">[9,</ref><ref type=""bibr"" target=""#b13"">12]</ref>. Experimental results show that the proposed prefetcher consistently achieves better performance than PC/DCit ouping misses according to the PC of the instruction that generated them <ref type=""bibr"" target=""#b3"">[2,</ref><ref type=""bibr"" target=""#b5"">4,</ref><ref type=""bibr"" target=""#b13"">12]</ref>, or to the region of memory they reference <ref type=""bibr"" target=""#b14"">[13,</ref><ref type=""bibr"" target="" ess instructions that generate the misses can potentially lead to higher performance gains than those that treat all misses as a single global stream <ref type=""bibr"" target=""#b13"">[12,</ref><ref type=""bibr"" target=""#b16"">15]</ref>, although this varies according to individual applications. Their ke consider finer classifications and adapt the depth for each stream depending on the ""strength"" of the links followed.</p><p>As with other prefetchers <ref type=""bibr"" target=""#b13"">[12,</ref><ref type=""bibr"" target=""#b14"">13]</ref>, in order to avoid prefetched data modifying the natural stream of m prefetcher. This prefetcher is based on PC stream localization and was shown to consistently outperform other localized and non-localized prefetchers <ref type=""bibr"" target=""#b13"">[12,</ref><ref type=""bibr"" target=""#b16"">15]</ref>. We also use a G/DC prefetcher in order to assess the impact of PC b",1
"e a spatial stream. Mostly orthogonal to the issues of how to localize streams and correlate addresses, <ref type=""bibr"" target=""#b11"">[10]</ref> and <ref type=""bibr"" target=""#b6"">[5]</ref> proposed using a dead-block predictor to identify replaceable cache lines and trigger prefetches. Prefetchers",0
"ality of misses as PC localization approaches do, but only indirectly in as much as there is or not temporal locality in the use of data. The work in <ref type=""bibr"" target=""#b11"">[10]</ref> proposed localizing streams based on a history of the last few memory access instructions. However, to have e miss order by recording miss distances inside a spatial stream. Mostly orthogonal to the issues of how to localize streams and correlate addresses, <ref type=""bibr"" target=""#b11"">[10]</ref> and <ref type=""bibr"" target=""#b6"">[5]</ref> proposed using a dead-block predictor to identify replaceable ca",0
"h grows, computing this conditional probability becomes unfeasible. A recent relaxation in language modeling <ref type=""bibr"" target=""#b28"">[27,</ref><ref type=""bibr"" target=""#b29"">28]</ref> turns the prediction problem on its head. First, instead of using the context to predict a missing word, it u",1
".0""><head n=""4.2.3"">Optimization</head><p>The model parameter set is θ = {Φ, Ψ} where the size of each is O(d|V |). Stochastic gradient descent (SGD) <ref type=""bibr"" target=""#b5"">[5]</ref> is used to optimize these parameters (Line 4, Algorithm 2). The derivatives are estimated using the back-propa",0
"ms to our work incorporate community information by learning clusters <ref type=""bibr"" target=""#b34"">[33]</ref>, by adding edges between nearby nodes <ref type=""bibr"" target=""#b15"">[14]</ref>, by using PageRank <ref type=""bibr"" target=""#b25"">[24]</ref>, or by extending relational classification to t",0
"omputer vision <ref type=""bibr"" target=""#b23"">[22]</ref>, speech recognition <ref type=""bibr"" target=""#b8"">[8]</ref>, and natural language processing <ref type=""bibr"" target=""#b1"">[1,</ref><ref type=""bibr"">7]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""8."">CONCLUSIONS</head><p",0
"tent recommendation <ref type=""bibr"" target=""#b12"">[12]</ref>, anomaly detection <ref type=""bibr"" target=""#b6"">[6]</ref>, and missing link prediction <ref type=""bibr"" target=""#b24"">[23]</ref>) must be able to deal with this sparsity in order to survive.</p><p>In this paper we introduce deep learning",0
"en using processors built for the mobile space may be more efficient <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b27"">29,</ref><ref type=""bibr"" target=""#b36"">38,</ref><ref type=""bibr"" target=""#b37"">39]</ref>.</p><p>In this work, we observe that scale-out workloads share many i public internet. We mimic real-world setups by making sure that the search index fits in memory, eliminating page faults and minimizing disk activity <ref type=""bibr"" target=""#b36"">[38]</ref>. We simulate clients using the Faban driver. The clients are configured to achieve the maximum search reques rocessors offer excessively complex cores, resulting in inefficiency through waste of resources. At the same time, our results corroborate prior work <ref type=""bibr"" target=""#b36"">[38]</ref>, indicating that niche processors offer excessively simple (e.g., in-order <ref type=""bibr"" target=""#b7"">[8, ization of large-scale online services provided by Microsoft and showed the implications of such workloads on data-center server design. Reddi et al. <ref type=""bibr"" target=""#b36"">[38]</ref> characterized the Bing search engine, showing that the computational intensity of search tasks is increasing ng has become ubiquitous, there has been significant research activity on characterizing particular scale-out workloads, either micro-architecturally <ref type=""bibr"" target=""#b36"">[38,</ref><ref type=""bibr"" target=""#b40"">42]</ref>, or at the system level <ref type=""bibr"" target=""#b6"">[7,</ref><ref",1
"k I/O. SAT Solver. We benchmark one instance per core of the Klee SAT Solver, an important component of the Cloud9 parallel symbolic execution engine <ref type=""bibr"" target=""#b5"">[6]</ref>. Input traces for the engine are produced by Cloud9 by symbolically executing the command-line printf utility",0
"a popular cloud database. Fan et al. discuss web mail, web search, and map-reduce as three representative workloads present in the Google datacenter <ref type=""bibr"" target=""#b15"">[16]</ref>. <ref type=""bibr"">Lim et al. extend</ref> this set of benchmarks and add an additional media streaming workl",0
"d><p>To find a set of applications that dominate today's cloud infrastructure, we examined a selection of internet services based on their popularity <ref type=""bibr"" target=""#b1"">[2]</ref>. For each popular service, we analyzed the class of application software used by the major providers to offer",0
"<div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.1"">Measurement tools and methodology</head><p>We analyze architectural behavior using Intel VTune <ref type=""bibr"" target=""#b24"">[26]</ref>, a tool that provides an interface to the processor performance counters. For all scale-out and traditional",0
"aseline cache is 64 byte in size, the overhead of UMON-global is still substantial. To reduce the overhead of UMON, we use Dynamic Set Sampling (DSS) <ref type=""bibr"" target=""#b11"">[12]</ref>. The key idea behind DSS is that the behavior of the cache can be approximated by sampling only a few sets. -global in Section 5.4. Unless stated otherwise, we use 32 sets for UMON-DSS. The sampled sets for UMON-DSS are chosen using the simple static policy <ref type=""bibr"" target=""#b11"">[12]</ref>, which means set 0 and every 33rd set is selected. For the remainder of the paper UMON by default means UMON set of accesses with high memory-level parallelism (MLP) now fits in the cache which reduces the average MLP and increases the average mlp-based cost<ref type=""bibr"" target=""#b11"">[12]</ref> of each miss.</p></note> 			<note xmlns=""http://www.tei-c.org/ns/1.0"" place=""foot"" n=""4"" xml:id=""foot_2""><p> "" target=""#b11"">[12]</ref> of each miss.</p></note> 			<note xmlns=""http://www.tei-c.org/ns/1.0"" place=""foot"" n=""4"" xml:id=""foot_2""><p>DSS was used in<ref type=""bibr"" target=""#b11"">[12]</ref> to choose between two replacement policies. Thus, it was used to approximated a global decision which had a cision which is a discrete value (how many ways to allocate) by using the hit counter information of the sampled sets. Therefore the bounds derived in<ref type=""bibr"" target=""#b11"">[12]</ref> are not applicable to our mechanism.</p></note> 			<note xmlns=""http://www.tei-c.org/ns/1.0"" place=""foot"" n=",1
"rying phase behavior of the competing applications, which makes it possible for dynamic partitioning to out perform even the best static partitioning <ref type=""bibr"" target=""#b12"">[13]</ref>.</p><p>Dynamic partitioning of shared cache was first investigated by Suh et al. <ref type=""bibr"">[17][18]</",0
"cks to art (unless the utility of that block for even the other application is zero). To address this shortcoming of the greedy algorithm, Suh et. al <ref type=""bibr"" target=""#b17"">[18]</ref> propose to also invoke the greedy algorithm for each combination of the non-convex points of all application s, indicating that the number of combinations of non-convex points of all the competing applications can be very large. To avoid the time complexity, <ref type=""bibr"" target=""#b17"">[18]</ref> suggests that the greedy algorithm be invoked only for some number of randomly chosen combination of non-con ype=""bibr"" target=""#b12"">[13]</ref>.</p><p>Dynamic partitioning of shared cache was first investigated by Suh et al. <ref type=""bibr"">[17][18]</ref>. <ref type=""bibr"" target=""#b17"">[18]</ref> describes a low-overhead scheme that uses recency position of the hits for the lines in the cache to estimat",0
"red cache, the hardware overhead of multiple directories makes this scheme impractical. Fortunately, the baseline LRU policy obeys the stack property <ref type=""bibr"" target=""#b9"">[10]</ref>, which means that an access that hits in a LRU managed cache containing N ways is guaranteed to also hit if t",0
"issue features of modern superscalars with the latency-hiding ability of multithreaded architectures. Unlike conventional multithreaded architectures <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b22"">23",1
"a parallel ray-tracing application. They do not simulate caches or TLBs and their architecture has no branch prediction mechanism. Yamamoto, et al., <ref type=""bibr"" target=""#b28"">[29]</ref> present an analytical model of multithreaded superscalar performance, backed up by simulation. Their study m",0
"ng. This paper presents an architecture that realizes much of the potential demonstrated by that work, simulating it in detail.</p><p>Hirata, et al., <ref type=""bibr"" target=""#b12"">[13]</ref> present an architecture for a multithreaded superscalar processor and simulate its performance on a parallel",0
"ith the latency-hiding ability of multithreaded architectures. Unlike conventional multithreaded architectures <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b22"">23]</ref>, which depend on fast context s fewest instructions in decode, rename, and the instruction queues. This achieves three purposes: (1) it prevents any one thread from filling the IQ, <ref type=""bibr"" target=""#b1"">(2)</ref>   priority to threads that are moving instructions through the IQ most efficiently, and (3) it provides a more t=""#b24"">[25]</ref> combine multiple-issue with multithreading, but assign work onto processors at a coarser level than individual instructions. Tera <ref type=""bibr"" target=""#b1"">[2]</ref> combines LIW with fine-grain multithreading.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""9"">Su",0
"reams and the misalignment of branch destinations make it difficult to fill the entire fetch bandwidth from one thread,  even for smaller block sizes <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b23"">24]</ref>. In this processor, we can spread the burden of filling the fetch band",0
"tic program, and moreover tends to create the same cycle-bycycle schedule repeatedly. Caching these schedules is the motivation for Execution Caching <ref type=""bibr"" target=""#b46"">[47]</ref>, to allow the scheduler to be shut down (which they argue produces power savings) some of the time.</p><p>Ou s or to allow a compiler/runtime to indicate when dynamic schedules should be generated. The major motivation in this regard has been energy savings, <ref type=""bibr"" target=""#b46"">[47,</ref><ref type=""bibr"" target=""#b48"">49]</ref>, generally at the cost of performance degradation though off-thecrit te a significant percentage of their instructions via the dynamic scheduler (upwards of 70%, 45% and 20% for <ref type=""bibr"" target=""#b33"">[34,</ref><ref type=""bibr"" target=""#b46"">47,</ref><ref type=""bibr"" target=""#b48"">49]</ref> respectively) rather than executing the captured OOO traces on the in",1
"ve the branch.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.5"">Avoiding Cache Miss-Induced Stalls</head><p>Continual Flow Pipelining <ref type=""bibr"" target=""#b45"">[46]</ref> was originally proposed to give an existing OOO processor a larger effective window without growing complex",0
"processing <ref type=""bibr"" target=""#b40"">[41]</ref>, data/control-plane processing <ref type=""bibr"" target=""#b19"">[20]</ref> and the embedded space <ref type=""bibr"" target=""#b15"">[16]</ref>. While direct comparisons between in-order and OOO designs are difficult, OOO designs seem to consistently p",0
"sm as even the most sophisticated static schedule is hobbled by the stall-on-use/head-of-line blocking problem inherent to cache misses in IO designs <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b21"">22]</ref>. In contrast, the enduring trend over several generations of OOO desig",0
"si-dynamic scheduling but even a limited form combined with specialized functional units can facilitate performance comparable to a narrow-window OOO <ref type=""bibr"" target=""#b7"">[8]</ref>. Though we do not address it in this work, quasi-dynamic and static scheduling all suffer from the region boun",0
"mpled simulation, statistical sampling (as done in SMARTS <ref type=""bibr"" target=""#b28"">[29]</ref>) and representative sampling (as done in SimPoint <ref type=""bibr"" target=""#b21"">[22]</ref>). Our experimental results using the SPEC CPU2000 benchmarks confirm that statistical sampling is generally ive sampling unit for each unique behavior. The most well known representative sampling approach is the SimPoint approach proposed by Sherwood et al. <ref type=""bibr"" target=""#b21"">[22]</ref>. SimPoint picks a small number of sampling units that accurately create a representation of the complete exe the initial cluster centers. Therefore, SimPoint considers multiple randomly chosen cluster centers and uses the Bayesian Information Criterion (BIC) <ref type=""bibr"" target=""#b21"">[22]</ref> to assess the quality of the clustering: the clustering with the highest BIC score is selected.</p><p>Thresh avior, and different authors have been proposing different ways for doing so, such as code working sets <ref type=""bibr"" target=""#b5"">[6]</ref>, BBVs <ref type=""bibr"" target=""#b21"">[22]</ref>, procedure calls <ref type=""bibr"" target=""#b12"">[13]</ref>, and performance data <ref type=""bibr"" target=""#b",1
"extreme workload behavior, e.g., max power, max energy, etc. There are two common ways in sampled simulation, statistical sampling (as done in SMARTS <ref type=""bibr"" target=""#b28"">[29]</ref>) and representative sampling (as done in SimPoint <ref type=""bibr"" target=""#b21"">[22]</ref>). Our experiment mpling bias is fundamental to the selection of sampling units.</p><p>The SMARTS (Sampling Microarchitecture Simulation) approach by Wunderlich et al. <ref type=""bibr"" target=""#b28"">[29]</ref> proposes systematic sampling, which selects sampling units periodically across the entire program execution, v xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.2"">Sampled Simulation</head><p>For statistical sampling, we use periodic sampling, as done in SMARTS <ref type=""bibr"" target=""#b28"">[29]</ref>, i.e., we select a sampling unit every n intervals. We will vary the sampling rate 1/n in the results presen ""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b28"">29]</ref>.</p><p>There are basically two major ways for determining what sampling units to select, namely (i) statistic",0
"ues -there is a wealth of literature covering this area, see for example <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b27""",0
"tion are not costeffective though because of the large gap between maximum and typical power consumption. Dynamic thermal management (DTM) techniques <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b22"">23]</ref> seek to exploit this gap: the microprocessor cooling apparatus is desi",0
"redictors, etc.) can be estimated with microarchitecture state warmup techniques -there is a wealth of literature covering this area, see for example <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b18"">19 istical sampling for evaluating cache performance. They select multiple sampling units by randomly picking intervals of execution.</p><p>Conte et al. <ref type=""bibr"" target=""#b4"">[5]</ref> pioneered the use of statistical sampling in processor simulation. They made a distinction between sampling bi",0
"associativity in the BVIT helps minimize the thrashing that often occurs in direct-mapped buffers. A 3-bit performance counter based on Heil's design <ref type=""bibr"" target=""#b16"">[17]</ref> tracks the effectiveness of each entry and is used to select which entry to replace when a new entry is adde alues based on the current state along the data dependence chain. If the generating values are present then ARVI's predictions are near perfect. Heil <ref type=""bibr"" target=""#b16"">[17]</ref> proposed another approach that correlates on the differences between branch source operand values. We consid",1
"ium 4 design <ref type=""bibr"" target=""#b12"">[13]</ref> and higher clock rates will likely continue to increase the number of stages in future designs <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b27"">28]</ref>. We have extended the base simulator to support two levels of branch",0
"he branch. In the DDT table, the data dependence chain is immediately available.</p><p>£ Optimizations driven by parallelism metrics: Bahar and Manne <ref type=""bibr"" target=""#b1"">[2]</ref> propose gating off pipeline resources based on recent IPC performance in order to save power. Similarly, Foleg",0
"at the second level achieves a 12.6% overall IPC improvement for the SPEC95 integer benchmarks as compared to the stateof-the-art two-level predictor <ref type=""bibr"" target=""#b25"">[26]</ref> proposed for the Alpha EV8.</p><p>The rest of this paper is organized as follows. The hardware mechanism for on. In all configurations, the first level of branch prediction is a hybrid predictor based on the Alpha EV8 branch predictor design called 2Bc-gskew <ref type=""bibr"" target=""#b25"">[26]</ref>. There are three predictor tables and one table that controls which table provides the prediction. Each tabl ranch predictors use some combination of the branch address, path information <ref type=""bibr"" target=""#b23"">[24]</ref>, and the local/global history <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b35"">36]</ref> of branch outcomes to make the prediction. Despite many attempts to",0
"s=""http://www.tei-c.org/ns/1.0""><head n=""4.4"">Forming the register set tag</head><p>Differentiating paths to a branch can improve prediction accuracy <ref type=""bibr"" target=""#b23"">[24]</ref>. ARVI uses the set of registers from the RSE as a path differentiator. Since a full concatenation of the reg te register as an explicit data dependence.</p><p>Most current dynamic branch predictors use some combination of the branch address, path information <ref type=""bibr"" target=""#b23"">[24]</ref>, and the local/global history <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b35"">36]</r",0
"instructions within the processor. The generally accepted method of determining the critical path of program execution was proposed by Fields et al. <ref type=""bibr"" target=""#b5"">[5]</ref>. A graph of dynamic instructions is constructed, modeling each instruction as three nodes: dispatch time, exec determining the critical path of an application using directed graphs, and proposed a token-based hardware mechanism to approximate this in hardware <ref type=""bibr"" target=""#b5"">[5]</ref>. Runahead <ref type=""bibr"" target=""#b3"">[3,</ref><ref type=""bibr"" target=""#b18"">18]</ref> and CLEAR <ref type=",1
"t integrates eight cores with a quad-channel, quad-ranked DDR3-2133 memory subsystem. Our memory model is based on the Micron DDR3 DRAM specification <ref type=""bibr"" target=""#b15"">[15]</ref>. The microarchitectural features of the baseline processor are shown in Table <ref type=""table"" target=""#tab",0
"o maintain the 2:1 ratio of processor cores to channels used so far. We also cut the number of L2 MSHR entries in half.</p><p>We use weighted speedup <ref type=""bibr"" target=""#b24"">[24]</ref> to quantify the schedulers' effects on throughput. To calculate weighted speedup, the IPC for each applicati",0
"ng a sophisticated multicore simulator that includes a detailed DDR3 DRAM model, we show that pairing this mechanism up with a lean FR-FCFS scheduler <ref type=""bibr"" target=""#b22"">[22]</ref> can improve performance by 9.3%, on average, for parallel workloads on an 8-core CMP, with essentially no ch -c.org/ns/1.0""><head n=""3.2"">Incorporating Criticality into FR-FCFS</head><p>We add our concept of load criticality into the FR-FCFS memory scheduler <ref type=""bibr"" target=""#b22"">[22]</ref>. The basic FR-FCFS algorithm calls for CAS commands to be prioritized over RAS commands, and in the case of scheduler, which affords them priority. We quantitatively show that pairing this mechanism with a novel criticality-aware scheduler, based on FR-FCFS <ref type=""bibr"" target=""#b22"">[22]</ref>, can improve performance by 9.3%, on average, for parallel workloads on an 8-core CMP, with minimal hardware",0
"ve high temporal reuse -blocks that are likely to be accessed multiple times within a short time interval. However, as identi ed by prior work (e.g., <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b33"">34,</ref><ref type=""bibr"" target=""#b34 ork proposed to modify the cache insertion policy to mitigate the negative e ects of pollution and thrashing <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b33"">34,</ref><ref type=""bibr"" target=""#b34"" to the most closely related work on addressing pollution or thrashing <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b33"">34,</ref><ref type=""bibr"" target=""#b54"" of memory accesses, and hence complements these compiler-based techniques.</p><p>Much prior research (e.g., <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" target=""#b38"" when the working set is larger than the cache size. When multiple threads share the cache, prior approaches <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b16"">17]</ref> learn the thrashing behavior of individual threads using a technique called set-dueling <ref type=""bibr"" targ che is its storage overhead compared to certain other prior approaches <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b16"">17]</ref> to address cache thrashing or pollution individually (see Table <ref type=""table"" target=""#tab_1"">1</ref>). H ad-aware dynamic insertion policy <ref type=""bibr"" target=""#b15"">[16]</ref> (TA-DIP), 2) Thread-aware dynamic re-reference interval prediction policy <ref type=""bibr"" target=""#b16"">[17]</ref> (TA-DRRIP), 3) Signature-based Hit Prediction using Instruction pointers (SHIP) <ref type=""bibr"" target=""#b5 the best performance when blocks within a thread have di erent reuse behavior.</p><p>Thread-Aware Dynamic Re-Reference Interval Prediction (TA-DRRIP) <ref type=""bibr"" target=""#b16"">[17]</ref> improves upon TA-DIP by using a better replacement policy than LRU, RRIP. Unlike the LRU policy, which inser izes (up to 16MB). For all mechanisms, except baseline LRU and DIP, the last-level cache uses the re-reference interval prediction replacement policy <ref type=""bibr"" target=""#b16"">[17]</ref>. For our EAF proposals, we assume that the operations on the EAF can be overlapped with the long latency of ads increases.</p><p>For server workloads, D-EAF improves weighted speedup by 17% compared to the LRU policy. These workloads are known to have scans <ref type=""bibr"" target=""#b16"">[17]</ref> (accesses to a large number of blocks with no reuse) which pollute the cache, making pollution the major pro ig_5"">8</ref> shows the bene ts of augmenting our EAF mechanism to 1) a cache following the LRU replacement policy, and 2) a cache following the RRIP <ref type=""bibr"" target=""#b16"">[17]</ref> policy for all 4-core workloads. As the gure shows, EAF-cache consistently improves performance in both case",1
"um of individual benchmark intensities/sensitivities (low = 0, medium = 1, high = 2). 5 We provide details of individual workloads in our tech report <ref type=""bibr"" target=""#b46"">[47]</ref>. 6 For clarity, results for DIP and RTB are excluded from Figures <ref type=""figure"" target=""#fig_2"">4</ref> on't present single-core results for benchmarks where all mechanisms perform within 1% of the baseline LRU. Our tech report provides the full results <ref type=""bibr"" target=""#b46"">[47]</ref>.  best mechanisms to address pollution (SHIP) or thrashing (DRRIP) for most benchmarks. On average, D-EAF pr rkloads due to space constraints. The observed trends for them are similar to those for 4-core workloads. We include these results in our tech report <ref type=""bibr"" target=""#b46"">[47]</ref>.</p><p>the workloads increases from low to high. 8 Regardless, both EAF and D-EAF outperform other prior app",0
"uently reduce system performance. Prior work proposed to modify the cache insertion policy to mitigate the negative e ects of pollution and thrashing <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b33 ned in the cache, increasing the hit rate when the working set is larger than the cache size. When multiple threads share the cache, prior approaches <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b16"">17]</ref> learn the thrashing behavior of individual threads using a technique EAF-cache) with ve state-of-the-art cache management approaches that aim to prevent pollution or thrashing: 1) Thread-aware dynamic insertion policy <ref type=""bibr"" target=""#b15"">[16]</ref> (TA-DIP), 2) Thread-aware dynamic re-reference interval prediction policy <ref type=""bibr"" target=""#b16"">[17 hrashing, along with its implementation complexity. We now describe each mechanism individually.</p><p>Thread-Aware Dynamic Insertion Policy (TA-DIP) <ref type=""bibr"" target=""#b15"">[16]</ref> addresses the thrashing problem by determining thrashing at a thread granularity. It does so by using set-du (Section 7.3). One shortcoming of EAF-cache is its storage overhead compared to certain other prior approaches <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b16"">17]</ref> to address cache thrashing or pollution individually (see Table <ref qualitative and quantitative comparisons to the most closely related work on addressing pollution or thrashing <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b33""",0
"m. In most modern processor designs, such concurrently running applications share the on-chip last-level cache <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b21"">22]</ref>. As a result, e ective use of the . Our framework faithfully models all memory-related processor-stalls. All systems use a 3-level cache hierarchy similar to some modern architectures <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b21"">22]</ref>. The L1 and L2 caches are private to individual cores and the L3 cache",0
"""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b33"">34,</ref><ref type=""bibr"" target=""#b34"">35,</ref><ref type=""bibr"" target=""#b52"">53,</ref><ref type=""bibr"" target=""#b54"">55]</ref>.</p><p>To prevent cache pollution, prior approaches <ref type=""bibr"" ""#b54"">55]</ref>.</p><p>To prevent cache pollution, prior approaches <ref type=""bibr"" target=""#b18"">[19,</ref><ref type=""bibr"" target=""#b33"">34,</ref><ref type=""bibr"" target=""#b52"">53,</ref><ref type=""bibr"" target=""#b54"">55]</ref> predict the reuse behavior of missed cache blocks and insert blocks p er related work.</p><p>A number of cache insertion policies have been proposed to prevent L1-cache pollution <ref type=""bibr"" target=""#b42"">[43,</ref><ref type=""bibr"" target=""#b52"">53]</ref>. Similar to some approaches we have discussed in our paper <ref type=""bibr"" target=""#b18"">[19,</ref><ref type",0
"semantics differ from those of ordinary Unix file systems. A more detailed description of the protocol and a proof of its security were presented in <ref type=""bibr"" target=""#b12"">[14]</ref>, though the published version of that paper did not present a protocol for group-writable files.</p><p>At th oup i-handles, the client must reflect any previous modifications to the group in the PVL.</p><p>A proof of the consistency protocol was presented in <ref type=""bibr"" target=""#b12"">[14]</ref>, but is beyond the scope of this paper. At a high level, however, its security follows from two properties.",1
"ef> and Pastry <ref type=""bibr"" target=""#b22"">[24]</ref> show the potential to scale to millions of separately administered volunteer nodes, with CFS <ref type=""bibr"" target=""#b4"">[5]</ref> layering a read-only file system on top of such a highly distributed architecture. These systems could potenti",0
". Duchamp <ref type=""bibr"" target=""#b5"">[6]</ref>, BFS <ref type=""bibr"" target=""#b2"">[3]</ref>, SFSRO <ref type=""bibr"" target=""#b7"">[9]</ref> and TDB <ref type=""bibr"" target=""#b11"">[13]</ref> have all made use of hash trees for comparing data or checking the integrity of part of a larger collection",0
"developed cryptographic storage techniques could be applied to achieve some degree of privacy in SUNDR, e.g. <ref type=""bibr"" target=""#b10"">[12,</ref><ref type=""bibr"" target=""#b0"">1]</ref>. Extensive work has also been done on ensuring data availability in the face of disk and server failures. The f a file. An attacker with read access could, by controlling the network or file server, substitute arbitrary data for any version of a file.</p><p>CFS <ref type=""bibr"" target=""#b0"">[1]</ref> allows users to keep directories of files that get transparently encrypted before being written to disk. CFS d",0
"ntegrity without touching the entire file system. Duchamp <ref type=""bibr"" target=""#b5"">[6]</ref>, BFS <ref type=""bibr"" target=""#b2"">[3]</ref>, SFSRO <ref type=""bibr"" target=""#b7"">[9]</ref> and TDB <ref type=""bibr"" target=""#b11"">[13]</ref> have all made use of hash trees for comparing data or checki",0
"ion rules Acquirement</head><p>Transformation Based Error-Driven Learning is an effective learning algorithm which was proposed by Eric Brill in 1992 <ref type=""bibr"" target=""#b11"">[14]</ref> . It could obtain transformation rules automatically during the learning process. In this paper, the reasons",1
""" target=""#b4"">5]</ref> , Condition Random Field (CRF) <ref type=""bibr"" target=""#b5"">[6]</ref> , Decision Tree <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b7"">8]</ref> , Support Vector Machine (SVM) <ref type=""bibr"">[9~12]</ref> , have performed good results, but they also have",0
"ation/Clustering, Machine Translation and Chunking.</p><p>The statistical models which were used in Japanese NER, such as Maximum Entropy Model (MEM) <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b2"">3]</ref> , Hidden Markov Model (HMM) <ref type=""bibr"" target=""#b3"">[4,</ref><ref",0
"correctly. They always imply the main contents of the article. Named Entity was proposed by Message Understanding Conference (MUC) for the first time <ref type=""bibr"" target=""#b0"">[1]</ref> . Named Entity Recognition (NER) is a difficult and challenging task, and it is a very important subtask of In",0
"lower. Moreover, OBM makes prediction based on PC to improve performance. A bypass and insertion algorithm for exclusive LLCs was presented recently <ref type=""bibr"" target=""#b7"">[7]</ref>. It classifies blocks based on their access number in the L2 cache and the hit number in the LLC. NUcache <ref",1
"5"">[5,</ref><ref type=""bibr"" target=""#b8"">8,</ref><ref type=""bibr"" target=""#b33"">33]</ref> and address based <ref type=""bibr"" target=""#b13"">[13,</ref><ref type=""bibr"" target=""#b15"">15,</ref><ref type=""bibr"" target=""#b30"">30,</ref><ref type=""bibr"" target=""#b31"">31]</ref>. LRF <ref type=""bibr"" target=",0
"he LLC management is firstly proposed. We believe that OBM can be widely used in other relative research areas, such as memory and storage management <ref type=""bibr"" target=""#b1"">[1,</ref><ref type=""bibr"" target=""#b24"">24]</ref>.</p></div><figure xmlns=""http://www.tei-c.org/ns/1.0"" xml:id=""fig_0""><",0
"We believe that OBM can be widely used in other relative research areas, such as memory and storage management <ref type=""bibr"" target=""#b1"">[1,</ref><ref type=""bibr"" target=""#b24"">24]</ref>.</p></div><figure xmlns=""http://www.tei-c.org/ns/1.0"" xml:id=""fig_0""><head>Figure 1 :Figure 2 :</head><label>",0
"bibr"" target=""#b10"">[10,</ref><ref type=""bibr"" target=""#b19"">19,</ref><ref type=""bibr"" target=""#b20"">20,</ref><ref type=""bibr"" target=""#b21"">21,</ref><ref type=""bibr"" target=""#b22"">22]</ref> try to identify distant reuse blocks to evict them earlier. Several adaptive methods <ref type=""bibr"" target= "">21]</ref>, time based <ref type=""bibr"" target=""#b10"">[10]</ref>, and counter based <ref type=""bibr"" target=""#b20"">[20]</ref>. Cache burst predictor <ref type=""bibr"" target=""#b22"">[22]</ref> makes prediction for continuous access sequences rather than individual accesses to improve prediction accur",0
"seful prefetches, then significant performance improvement can be achieved.</p><p>We use a stream prefetcher <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b11"">12]</ref> similar to the one in IBM's POWER 4 <ref type=""bibr"" target=""#b24"">[25]</ref> for most of our experiments. St already used in many proposals <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b28"">29,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b11"">12]</ref> to indicate whether or not a cache line (or request) was brought in (or made) by the prefetcher.</p><p>None o bibr"" target=""#b19"">[20,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b11"">12]</ref> have been proposed. Although these proposals have the similar goal of improving performance by increasing DRA ill quickly become full.</p><p>To implement this, we need to measure the run-time accuracy of the prefetcher <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b4"">5]</ref>. Therefore, we add an extra prefetch bit per L2 cache line and MSHR ent 6.3"">Prefetch Handling</head><p>Adaptive prefetch handling techniques <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b4"">5]</ref> aim to reduce the interference between prefetch and demand requests in div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""5.5"">Comparison with Prefetch-Aware DRAM Controllers</head><p>Prefetch-Aware DRAM Controllers (PADC) <ref type=""bibr"" target=""#b11"">[12]</ref> was proposed to maximize DRAM row buffer hits for useful requests (demands and useful prefetches). PADC also fore is orthogonal to prefetch handling mechanisms. As we discuss in Section 5.5, our mechanisms are complementary to prefetch-aware DRAM controllers <ref type=""bibr"" target=""#b11"">[12]</ref> which employ an adaptive prefetch handling technique that is reported to outperform feedback-directed prefet",1
"and therefore the actual design cost/effort is not expensive. Prefetch bits are already used in many proposals <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b28"">29,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b11"">12]</ref> to indicate whether or not a",0
"stem performance, we use Individual Speedup (IS), Weighted Speedup (WS) <ref type=""bibr"" target=""#b21"">[22]</ref>, and Harmonic mean of Speedups (HS) <ref type=""bibr"" target=""#b12"">[13]</ref>. WS corresponds to system throughput and HS corresponds to the inverse of job turnaround time <ref type=""bib",0
"types of prefetchers: GHB (Global History Buffer)-based CZone Delta Correlation (C/DC) <ref type=""bibr"" target=""#b17"">[18]</ref> and PC-based stride <ref type=""bibr"" target=""#b0"">[1]</ref>. Both the C/DC and stride prefetchers accurately capture a substantial number of memory accesses that are mapp",0
"Speedups (HS) <ref type=""bibr"" target=""#b12"">[13]</ref>. WS corresponds to system throughput and HS corresponds to the inverse of job turnaround time <ref type=""bibr"" target=""#b5"">[6]</ref>. In the following equations, N is the number of cores in the CMP system. IP C alone i is the IPC when applicat",0
"hows that L1 instruction fetch misses remain a critical performance bottleneck in traditional server workloads <ref type=""bibr"" target=""#b9"">[9,</ref><ref type=""bibr"" target=""#b8"">8,</ref><ref type=""bibr"" target=""#b11"">11,</ref><ref type=""bibr"" target=""#b12"">12,</ref><ref type=""bibr"" target=""#b32"">3 re recent hardware prefetching proposals rely on the observation that instruction cache miss or instruction execution sequences are highly repetitive <ref type=""bibr"" target=""#b8"">[8,</ref><ref type=""bibr"" target=""#b9"">9]</ref>. These designs log the miss/execution streams in a large circular buffer ntain an index on this buffer to locate and replay past sequences on subsequent triggers. The most recent proposal, Proactive Instruction Fetch (PIF) <ref type=""bibr"" target=""#b8"">[8]</ref>, can eliminate nearly all L1 instruction misses, but requires impractical storage and substantial complexity t rs because it records less state and requires simpler indexing; we find that coverage saturates with only 64kB of storage (relative to over 200kB for <ref type=""bibr"" target=""#b8"">[8]</ref>). Using the gem5 <ref type=""bibr"" target=""#b4"">[4]</ref> full-system simulation infrastructure running a suite 39"">39]</ref> also prefetch instructions by using control flow speculation to explore ahead of the instruction fetch unit. As shown by Ferdman et al. <ref type=""bibr"" target=""#b8"">[8]</ref>, these prefetchers suffer from interference caused by wrong path execution and insufficient lookahead when the vocations of a function call are uniquely identified leading to more accurate prefetching.</p><p>TIFS <ref type=""bibr"" target=""#b9"">[9]</ref> and PIF <ref type=""bibr"" target=""#b8"">[8]</ref> address the limitations of branchpredictor-directed prefetching by directly recording the instruction fetch mi overflows.</p><p>To reduce the size of miss table entries, we first compress the sequence of misses using a scheme similar to that pro-posed for PIF <ref type=""bibr"" target=""#b8"">[8]</ref>. The key observation is that the misses associated with a particular signature usually comprise a small number DIP. However, storing complete addresses for 15 cache blocks in each Miss Table entry remains prohibitive. Fortunately, as observed by Ferdman et al. <ref type=""bibr"" target=""#b8"">[8]</ref>, misses tend to be closely clustered with only a few discontinuities. Hence, we adopt a compression scheme sim r prefetch depths, and found next-2-line to perform best.</p><p>? PIF is our implementation of Proactive Instruction Fetch proposed by Ferdman et al. <ref type=""bibr"" target=""#b8"">[8]</ref>, which is the most effective instruction prefetching design reported to date.</p><p>? RDIP is our prefetcher d",1
"how later, RDIP requires only 64kB of dedicated storage. We analyze energy overheads of each approach in greater detail in Section 5.4.3.</p><p>SHIFT <ref type=""bibr"" target=""#b15"">[15]</ref> is a concurrent work that employs a similar record and replay mechanism as PIF, while reducing storage overh",0
"ruction prefetching. Even early computer systems included nextline instruction prefetchers to exploit the common case of sequential instruction fetch <ref type=""bibr"" target=""#b1"">[1]</ref>. This early concept evolved into next-N-line and instruction stream prefetchers <ref type=""bibr"" target=""#b14""",0
"search attempts at more sophisticated instruction prefetch leverage the branch predictor to run ahead of fetch <ref type=""bibr"" target=""#b5"">[5,</ref><ref type=""bibr"" target=""#b24"">24,</ref><ref type=""bibr"" target=""#b27"">27,</ref><ref type=""bibr"" target=""#b28"">28,</ref><ref type=""bibr"" target=""#b33"" >28,</ref><ref type=""bibr"" target=""#b33"">33]</ref>. Run-ahead execution <ref type=""bibr"" target=""#b22"">[22]</ref>, wrong path instruction prefetching <ref type=""bibr"" target=""#b24"">[24]</ref>, and speculative threading mechanisms <ref type=""bibr"" target=""#b34"">[34,</ref><ref type=""bibr"" target=""#b39",0
"5,</ref><ref type=""bibr"" target=""#b27"">27,</ref><ref type=""bibr"" target=""#b28"">28,</ref><ref type=""bibr"" target=""#b33"">33]</ref>. Run-ahead execution <ref type=""bibr"" target=""#b22"">[22]</ref>, wrong path instruction prefetching <ref type=""bibr"" target=""#b24"">[24]</ref>, and speculative threading mec",0
"-the-art instruction prefetchers for server workloads rely on temporal streaming to record, and subsequently replay, entire sequences of instructions <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b14"">15]</ref>. As a class, these stream-based prefetchers have been shown to be hi etchers.</p><p>To overcome the next-line prefetcher's inability to predict instruction cache misses that are not contiguous, stream-based prefetchers <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b14"">15]</ref> exploit the recurring control-flow graph traversals in server worklo f type=""bibr"" target=""#b26"">[26]</ref> prefetcher to record instruction fetch streams in a similar vein to prior stream-based instruction prefetchers <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b14"">15]</ref>. We augment each core with simple logic to read instruction streams misses.</p><p>The SHIFT design adopts its key history record and replay mechanisms from previously proposed per-core data and instruction prefetchers <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b34"">34,</ref><ref type=""bibr"" target=""#b43 ve Instruction Fetch (PIF), can eliminate an average of 90% of instruction cache misses, but necessitates over 200KB per core for its history storage <ref type=""bibr"" target=""#b13"">[14]</ref>.</p><p>Stream-based instruction prefetchers were proposed for conventional fat-core processors, where the pr ier approaches, improving performance per unit of core area by 16% and 59% for two lean-core designs over the state-of-the-art instruction prefetcher <ref type=""bibr"" target=""#b13"">[14]</ref>. The absolute performance improvement on a suite of diverse server workloads is 19%, on average.</p><p>? By truction misses are eliminated by the prefetcher.</p><p>The state-of-the-art stream-based instruction prefetcher is Proactive Instruction Fetch (PIF) <ref type=""bibr"" target=""#b13"">[14]</ref>, which extends earlier work on temporal streaming <ref type=""bibr"" target=""#b14"">[15]</ref>. PIF's key innov access streams to eliminate the microarchitectural noise in streams introduced by the instruction cache replacement policy and branch mispredictions <ref type=""bibr"" target=""#b13"">[14]</ref>. To mitigate increased history storage requirements resulting from recording instruction cache accesses rath "">Methodology</head><p>We evaluate SHIFT and compare it to the state-of-the-art instruction prefetcher with per-core private instruction history, PIF <ref type=""bibr"" target=""#b13"">[14]</ref>, using trace-based and cycle-accurate simulations of a 16-core CMP, running server workloads. For our evalua -art prefetcher configuration. We compare SHIFT's effectiveness and history storage requirements with the state-ofthe-art instruction prefetcher, PIF <ref type=""bibr"" target=""#b13"">[14]</ref>. Like other stream-based instruction and data prefetchers <ref type=""bibr"" target=""#b14"">[15,</ref><ref type record in the history buffer contains 41 bits. PIF requires 32K spatial region records in the history buffer targeting 90% instruction miss coverage <ref type=""bibr"" target=""#b13"">[14]</ref>, also validated with our experiments. As a result, the history buffer is 164KB for each core in total.</p><p Instruction Miss Coverage</head><p>To show SHIFT's effectiveness, we first compare the fraction of instruction cache misses predicted by SHIFT to PIF <ref type=""bibr"" target=""#b13"">[14]</ref>. For the purposes of this study, we only track the predictions that would be made through replaying recurrin che misses with 16% overprediction, while PIF_32K eliminates 92% of the instruction cache misses with 13% overprediction, corroborating prior results <ref type=""bibr"" target=""#b13"">[14]</ref>. However, PIF_2K, which has the same aggregate storage overhead as SHIFT, can eliminate only 53% of instruct ref type=""bibr"" target=""#b14"">[15]</ref> records streams of discontinuities in its history, enhancing the lookahead of discontinuity prefetching. PIF <ref type=""bibr"" target=""#b13"">[14]</ref>, records the complete retire-order instruction cache access history, capturing both discontinuities and next",1
"uation, we use the SimFlex multiprocessor sampling methodology <ref type=""bibr"" target=""#b44"">[44]</ref>, which extends the SMARTS sampling framework <ref type=""bibr"" target=""#b46"">[46]</ref>. Our samples are collected over 10-30 seconds of workload execution (for the DSS workloads, they are collect",0
"r"" target=""#b35"">35,</ref><ref type=""bibr"" target=""#b39"">39]</ref>, exposing instruction-fetch stalls as a dominant performance bottleneck in servers <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b18""> i-c.org/ns/1.0""><head n=""7."">RELATED WORK</head><p>Instruction fetch stalls have long been recognized as a dominant performance bottleneck in servers <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b18"">1",0
"RK</head><p>Instruction fetch stalls have long been recognized as a dominant performance bottleneck in servers <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b18"">18,</ref><ref type=""bibr"" target=""#b20"">",0
"ill order). A recent study assigns insertion age based on re-reference interval prediction and updates the predicted age on a hit in an inclusive LLC <ref type=""bibr"" target=""#b15"">[15]</ref>. Although such an option of age update is nonexistent in an exclusive LLC, we will show how to design somewh will refer to this policy as the TC-AGE policy. The TC-AGE policy is similar to SRRIP-style static algorithms originally proposed for inclusive LLCs <ref type=""bibr"" target=""#b15"">[15]</ref>. In our age assignment setting where a lower age corresponds to a higher replacement priority, the SRRIP alg cks between insertion age of zero and one. This policy shows one way to implement DRRIP-style dynamic policies originally proposed for inclusive LLCs <ref type=""bibr"" target=""#b15"">[15]</ref>.</p><p>The TC-AGE policy improves performance by more than 1% averaged over the 97 traces (see the ALL group",1
"ress-based locality predictor <ref type=""bibr"">[8]</ref>, or carry out an instruction-based characterization of the potential of data cache bypassing <ref type=""bibr"" target=""#b28"">[28]</ref>, or employ a classification of data cache misses into capacity or conflict to drive the bypass decision <ref",0
"LLC with bypass enabled) has been explored in the context of small victim caches that work well with L1 caches <ref type=""bibr"" target=""#b5"">[5,</ref><ref type=""bibr"" target=""#b11"">11]</ref>. A design of a large victim cache with selective insertion based on frequency of misses has been presented in ead blocks in an inclusive LLC to configure an ""embedded"" victim cache <ref type=""bibr"" target=""#b21"">[21]</ref>.</p><p>Dead block prediction schemes <ref type=""bibr"" target=""#b11"">[11,</ref><ref type=""bibr"" target=""#b19"">19,</ref><ref type=""bibr"" target=""#b21"">21,</ref><ref type=""bibr"" target=""#b22",0
"ow to design an address-oblivious dead block predictor that exploits the reuse probabilities to improve the replacement decisions in an inclusive LLC <ref type=""bibr"" target=""#b4"">[4]</ref>. On the other hand, our bypass algorithms identify a block that would be dead-on-fill in the LLC at the time o",0
"ype=""bibr"" target=""#b43"">[44]</ref>, among others).</p><p>Recently, van Renesse and Schneider presented a chain replication method for object storage <ref type=""bibr"" target=""#b46"">[47]</ref> over fail-stop servers, designed to provide strong consistency yet improve throughput. The basic approach or (Chain Replication with Apportioned Queries), an object storage system that, while maintaining the strong consistency properties of chain replication <ref type=""bibr"" target=""#b46"">[47]</ref>, provides lower latency and higher throughput for read operations by supporting apportioned queries: that is ple concurrent writes can be pipelined down the chain, with transmission costs equally spread over all nodes. The simulation results of previous work <ref type=""bibr"" target=""#b46"">[47]</ref> showed competitive or superior throughput for CR compared to primary/backup replication, while arguing a pri integration of our coordination service. In particular, CRAQ's choice of allowing a node to join anywhere in a chain (as opposed only to at its tail <ref type=""bibr"" target=""#b46"">[47]</ref>), as well as properly handling failures during recovery, requires some careful consideration.</p></div> <div 15,</ref><ref type=""bibr"" target=""#b15"">16]</ref>. An alternative approach, taken by GFS <ref type=""bibr"" target=""#b21"">[22]</ref> and promoted in CR <ref type=""bibr"" target=""#b46"">[47]</ref>, is to use the membership management service as a directory service in assigning and storing randomized chai ontention. CRAQ's use of optimistic locking for multi-chain multi-object updates was heavily influenced by Sinfonia.</p><p>CRAQ and Chain Replication <ref type=""bibr"" target=""#b46"">[47]</ref> are both examples of object-based storage systems that expose wholeobject writes (updates) and expose a flat",1
"emantics due to their perceived costs (as at Google <ref type=""bibr"" target=""#b21"">[22]</ref>, Amazon <ref type=""bibr"" target=""#b14"">[15]</ref>, eBay <ref type=""bibr"" target=""#b45"">[46]</ref>, and Facebook <ref type=""bibr"" target=""#b43"">[44]</ref>, among others).</p><p>Recently, van Renesse and Schn ntuallyconsistent while an auction is still far from over, but use strong consistency-even at the cost of availability-right before an auction closes <ref type=""bibr"" target=""#b45"">[46]</ref>.</p><p>A number of filesystems and object stores have traded consistency for scalability or operation under",0
"vely-scalable systems being deployed in commercial datacenters (e.g., Amazon's Dynamo <ref type=""bibr"" target=""#b14"">[15]</ref>, Facebook's Cassandra <ref type=""bibr"" target=""#b15"">[16]</ref>, and the popular Memcached <ref type=""bibr"" target=""#b17"">[18]</ref>). To achieve the requisite reliability, type=""bibr"" target=""#b14"">[15]</ref>, to support ""always-on"" writes and continued operation when partitioned. Facebook's new Cassandra storage system <ref type=""bibr"" target=""#b15"">[16]</ref> also offers only eventual consistency. The common use of memcached <ref type=""bibr"" target=""#b17"">[18]</ref> ain identifiers to a single head node. This is similar to a growing number of datacenter-based object stores <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b15"">16]</ref>. An alternative approach, taken by GFS <ref type=""bibr"" target=""#b21"">[22]</ref> and promoted in CR <ref type",0
"rg/ns/1.0""><head n=""4.1.2"">Single-Chain Operations</head><p>Sinfonia's recently proposed ""mini-transactions"" provide an attractive lightweight method <ref type=""bibr"" target=""#b1"">[2]</ref> of performing transactions on multiple keys within a single chain. A minitransaction is defined by a compare, br"" target=""#b35"">[36]</ref> explores exporting various higher-layer data abstractions, such as a B-tree, while offering strict consistency. Sinfonia <ref type=""bibr"" target=""#b1"">[2]</ref> provides lightweight ""mini-transactions"" to allow for atomic updates to exposed memory regions in storage node",0
"</ref><ref type=""bibr"" target=""#b27"">28]</ref>. Agreement protocols have also been extended to malicious settings, both for state machine replication <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b33"">34]</ref> and quorum systems <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""",0
"warmup lengths are chosen.</p><p>The second paper mentioning branch predictor warmup is by Haskins and Conte <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b19"">20]</ref> in which they propose memory reference reuse latency (MRRL). The idea of MRRL is to look in the pre-sampling , BHM is better able to approach the target 1M warmup length per sampling unit. (Note that the MRRL approach <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b19"">20]</ref> corresponds to a zero BHM history length.) We further observe that an 8 bit and a 16 bit BHM history length y",1
"e state (registers and memory) along with (ii) checkpointed cache warmup <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b8"">9]</ref> and (iii) compressed branch traces <ref type=""bibr"" target=""#b9"">[10]</re =""bibr"" target=""#b15"">[16]</ref>, memory reference reuse latency (MRRL) <ref type=""bibr"" target=""#b16"">[17]</ref>, boundary line reuse latency (BLRL) <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b17"">18]</ref>, self-monitored adaptive cache warmup (SMA) <ref type=""bibr"" target=""#",0
"target=""#b8"">9]</ref>. Checkpointing is especially beneficial for the parallel simulation of sampling units <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b11"">12]</ref>.</p><p>The third issue with sampled simulation is to estimate the microarchitecture state at the beginning of [13]</ref>, fixed warmup <ref type=""bibr"" target=""#b0"">[1]</ref>, cache miss rate estimators <ref type=""bibr"" target=""#b13"">[14]</ref>, no-state-loss <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b14"">15]</ref>, minimal subset evaluation (MSE) <ref type=""bibr"" target=""#b15"">[16]",0
"entire benchmark executions is infeasible for exploring huge microarchitecture design spaces. Therefore, researchers have proposed sampled simulation <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b3"">4]</r its are representative for the entire program execution. Various authors have proposed various approaches for achieving this, such as random sampling <ref type=""bibr"" target=""#b0"">[1]</ref>, periodic sampling as done in SMARTS <ref type=""bibr"" target=""#b2"">[3]</ref> and targeted sampling based on pr up. Various approaches have been proposed such as no warmup, stale state (also called stitch) <ref type=""bibr"" target=""#b12"">[13]</ref>, fixed warmup <ref type=""bibr"" target=""#b0"">[1]</ref>, cache miss rate estimators <ref type=""bibr"" target=""#b13"">[14]</ref>, no-state-loss <ref type=""bibr"" target="" che warmup, very little work has been done on branch predictor warmup.</p><p>The first paper dealing with branch predictor warmup was by Conte et al. <ref type=""bibr"" target=""#b0"">[1]</ref>. They proposed two approaches to branch predictor warmup, namely stale state and fixedlength warmup. Stale sta",0
"type=""bibr"" target=""#b14"">15]</ref>, minimal subset evaluation (MSE) <ref type=""bibr"" target=""#b15"">[16]</ref>, memory reference reuse latency (MRRL) <ref type=""bibr"" target=""#b16"">[17]</ref>, boundary line reuse latency (BLRL) <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b17"">18 ieves good accuracy if sufficiently long warmup lengths are chosen.</p><p>The second paper mentioning branch predictor warmup is by Haskins and Conte <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b19"">20]</ref> in which they propose memory reference reuse latency (MRRL). The ide get. By increasing the BHM history length, BHM is better able to approach the target 1M warmup length per sampling unit. (Note that the MRRL approach <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b19"">20]</ref> corresponds to a zero BHM history length.) We further observe that a",0
"tional replacement was recently proposed by Hallnor et al. for making replacement decisions in a software-managed UCA called the Indirect Index Cache <ref type=""bibr"" target=""#b8"">[9]</ref>. In our scheme, when a hit occurs to a cache line, it is swapped with the line in the bank that is the next cl s. Kessler examined designs for multi-megabyte caches built with discrete components <ref type=""bibr"" target=""#b16"">[17]</ref>. Hallnor and Reinhardt <ref type=""bibr"" target=""#b8"">[9]</ref> studied a fully associative software-managed design for large on-chip L2 caches, but not did not consider non-",1
"NIFORM ACCESS CACHES</head><p>Large modern caches are subdivided into multiple sub-banks to minimize access time. Cache modeling tools, such as Cacti <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b32"">33]</ref>, enable fast exploration of the cache design space by automatically",0
"xamined using associativity to balance power and performance. Albonesi examines turning off ""ways"" of each set to save power when cache demand is low <ref type=""bibr"" target=""#b1"">[2]</ref>. Poweil et al. evaluate the balance between incremental searches of the sets to balance power and performance",0
"d be compared with Przybylski's dissertation, in which be exhaustively searched the space of multi-level caches to find the performance-optimal point <ref type=""bibr"" target=""#b24"">[25]</ref>.</p><p>Finally, many researchers have examined adaptive cache policies, a concept which is inherent in the D",0
"p://www.tei-c.org/ns/1.0""><head n=""8."">SUMMARY AND CONCLUSIONS</head><p>Non-uniform accesses have started to appear in high performance cache designs <ref type=""bibr"" target=""#b22"">[23]</ref>. In this paper, we proposed several new designs that treat the cache as a network of banks and facilitates n",0
"quires a large amount of RAM memory, the product cost will be increased.</p><p>This paper surveys the-state-of-the-art FTL algorithms. Gal and Toledo <ref type=""bibr"" target=""#b6"">[7]</ref> also provided algorithms and data structures for flash memory systems. Compared to their work, the present stu",1
"read (write) data from (to) flash memory.</p><p>One basic hardware characteristics of flash memory is that it has an erase-before-write architecture <ref type=""bibr"" target=""#b4"">[5]</ref>. That is, to update a location in flash memory, the location must first be erased before new data can be writt assume that the capacities of flash memory are 128 MB (with 8192 blocks) and 8 GB (524288 blocks). Furthermore, each block is composed of 32 sectors <ref type=""bibr"" target=""#b4"">[5]</ref>. In sector mapping, three bytes are needed to represent all sector numbers in both 128 MB and 8 GB flash memor similar to the result of the total elapsed time because the erase count is the most dominant factor in the overall system performance. A recent study <ref type=""bibr"" target=""#b4"">[5]</ref> found a running time ratio of read (1 page), write (1 page), and erase (1 block) is approximately 1:7:63. It i",0
"e location must first be erased before new data can be written to it. The memory portion for erasing differs in size from that for reading or writing <ref type=""bibr"" target=""#b1"">[2]</ref>, resulting in the major performance degradation of the overall flash memory system.</p><p>Therefore, a type of ry system.</p><p>Therefore, a type of system software termed FTL (Flash Translation Layer) has been introduced <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b12"">13,</ requires a large amount of memory space (RAM), it is hardly feasible for small embedded systems. For this reason, block mapping-based FTL algorithms <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b12"">13]</ref> are proposed. Some detailed algo",0
"e degradation of the overall flash memory system.</p><p>Therefore, a type of system software termed FTL (Flash Translation Layer) has been introduced <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b8"">9,</r <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.1.1."">Sector mapping</head><p>A naive and intuitive FTL algorithm is the sector mapping algorithm <ref type=""bibr"" target=""#b0"">[1]</ref>. In sector mapping, every logical sector is mapped to a corresponding physical sector. Therefore, if there are",0
"system software termed FTL (Flash Translation Layer) has been introduced <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b13"">14] e (RAM), it is hardly feasible for small embedded systems. For this reason, block mapping-based FTL algorithms <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b12"">13]</ref> are proposed. Some detailed algorithms will be presented in Section 4. count of flash memory blocks may be stored in RAM.</p><p>For example, Fig. <ref type=""figure"" target=""#fig_4"">8</ref> shows an example of a RAM table <ref type=""bibr"" target=""#b5"">[6]</ref>. The flash memory block of this system is composed of 16 sectors. In the example, a 1:2 block mapping method i",0
". <ref type=""bibr"" target=""#b18"">Chan et al. (2008)</ref> proposed FloWorM system that includes tracker, analyzer and reporter based on NetFlow data. <ref type=""bibr"" target=""#b5"">Abdulla et al. (2011)</ref> presented a support vector machine (SVM) method based on the fact that a scanning activity o ger distance <ref type=""bibr"" target=""#b132"">(Valenti and Rossi, 2011)</ref>, or data fusion with other log files such as Snort, DNS related requests <ref type=""bibr"" target=""#b5"">(Abdulla et al., 2011)</ref> (number of DNS requests, response, normals, and anomalies for each host over a certain peri",1
"f> Statistic DDoS and port scan 2011 <ref type=""bibr"" target=""#b40"">(Franc -ois et al., 2011)</ref> Host behavior and PageRank Botnets detection 2011 <ref type=""bibr"" target=""#b123"">(Sperotto and Pras, 2011)</ref> Time series IDS 2011 <ref type=""bibr"" target=""#b41"">(Francois et al., 2011)</ref> Page",0
"ref type=""bibr"" target=""#b64"">Kind et al. (2006)</ref> presented a method to uncover the relationships between IT infrastructures using NetFlow data. <ref type=""bibr"" target=""#b20"">Chen et al. (2011)</ref> developed novel heuristics to analyze characteristics and correlations between inter-data cent",0
"7)</ref> Flow signature and honeypot logs Worm detection 2008 <ref type=""bibr"" target=""#b18"">(Chan et al., 2008)</ref> Heuristics Worm detection 2008 <ref type=""bibr"" target=""#b154"">(Zhenqi and Xinyu, 2008)</ref> Statistic Anomaly detection 2009 <ref type=""bibr"" target=""#b69"">(Krmicek et al., 2009)<",0
"""bibr"" target=""#b1"">[2]</ref><ref type=""bibr"" target=""#b2"">[3]</ref><ref type=""bibr"" target=""#b3"">[4]</ref><ref type=""bibr"" target=""#b4"">[5]</ref>[8] <ref type=""bibr"" target=""#b9"">[10]</ref><ref type=""bibr"" target=""#b10"">[11]</ref><ref type=""bibr"" target=""#b11"">[12]</ref><ref type=""bibr"" target=""#b1 o shows an address correlation between 100003F8 and 100003FA, which is often observed and utilized for prediction in the Markov prefetching algorithm <ref type=""bibr"" target=""#b9"">[10]</ref> . The following section discusses data prefetching methodologies based on the proposed DAHC. </p></div> <div etching, but the limitation is that only constant strides are recognizable. To capture repetitiveness in data reference addresses, Markov prefetching <ref type=""bibr"" target=""#b9"">[10]</ref> was proposed. This strategy assumes the history might repeat itself among data accesses and build a state tra",1
"ther work targeting these directions.</p><p>Another work closely related to this study is the instruction pointer based prefetcher developed by Intel <ref type=""bibr"" target=""#b6"">[7]</ref> . The IP prefetcher is a RPT-like prefetcher; thus, it suffers the limitation that it only works for constant",0
"br"" target=""#b3"">[4]</ref><ref type=""bibr"" target=""#b4"">[5]</ref>[8] <ref type=""bibr"" target=""#b9"">[10]</ref><ref type=""bibr"" target=""#b10"">[11]</ref><ref type=""bibr"" target=""#b11"">[12]</ref><ref type=""bibr"" target=""#b12"">[13]</ref><ref type=""bibr"" target=""#b13"">[14]</ref><ref type=""bibr"" target=""#b",0
"nts the analysis results.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.1"">Simulation Methodology</head><p>The SimpleScalar simulator <ref type=""bibr"" target=""#b0"">[1]</ref> was enhanced with data prefetching We chose the sim-outorder simulator for our experiments. Figure <ref type=""",0
"r"" target=""#b12"">[13]</ref><ref type=""bibr"" target=""#b13"">[14]</ref><ref type=""bibr"" target=""#b14"">[15]</ref><ref type=""bibr"" target=""#b15"">[16]</ref><ref type=""bibr"" target=""#b16"">[17]</ref>[19] <ref type=""bibr"" target=""#b22"">[23]</ref> . These studies concluded that prefetching is a promising solu",0
"metric History Length predictors by Seznec <ref type=""bibr"" target=""#b20"">[21]</ref> and the introduction of the TAGE predictor by Seznec and Michaud <ref type=""bibr"" target=""#b25"">[26]</ref>. In practice, on the traces distributed for the first two CBPs, more accuracy progress was achieved between rediction accuracy improvement is to associate side predictors targeting special categories of branches with a state-of-the-art main predictor (TAGE, <ref type=""bibr"" target=""#b25"">[26]</ref>, OGEHL <ref type=""bibr"" target=""#b20"">[21]</ref>, Piecewise Linear <ref type=""bibr"" target=""#b10"">[11]</ref> near <ref type=""bibr"" target=""#b10"">[11]</ref>, FTL <ref type=""bibr"" target=""#b7"">[8]</ref>), e.g. a loop predictor with the TAGE predictor in L-TAGE <ref type=""bibr"" target=""#b25"">[26]</ref> or the address-branch correlator in <ref type=""bibr"" target=""#b4"">[5]</ref>.</p><p>In this paper, we illustr ictor.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3."">Background on the TAGE Predictor</head><p>The TAGE predictor was introduced in <ref type=""bibr"" target=""#b25"">[26]</ref> and is the core predictor of the L-TAGE predictor that won the second Championship Branch Prediction in 2006 hole application. Dynamically monitoring it through a single 4-bit counter USE_ALT_ON_NA was found to allow to (slightly) improve prediction accuracy <ref type=""bibr"" target=""#b25"">[26]</ref>. The prediction computation algorithm is as follows:</p><p>1. Find the matching component with the longest h ing 12-bit tag is good tradeoff. However Experiments showed that one can use wider tag for long histories for a better tradeoff. Previous experiments <ref type=""bibr"" target=""#b25"">[26]</ref> have shown that the TAGE predictor performs efficiently on a wide spectrum of history lengths with maximum h",1
"esearch in branch prediction <ref type=""bibr"" target=""#b27"">[28,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b2"">3]</ref> resulting in the design of comple",0
"tapredictors were shown to be poorly efficient and new solutions combining local history and global history were proposed for neural based predictors <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b7"">8]</ref>. Up to now, there has not been any elegant proposal to combine the glo",0
"ired predictors led by Jimenez <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b0"">1]</ref>, the introduction of GEometric History Length predictors by Seznec <ref type=""bibr"" target=""#b20"">[21]</ref> an <ref type=""bibr"" target=""#b15"">[16]</ref>.</p><p>Since 2006, TAGE has been often considered as state-of-theart in terms of branch prediction accuracy <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b4"">5]</ref>. In this paper, we reenforce the case for the TAGE predictor in two dire espectively GEHL combined with LGEHL for FTL++ and piecewise linear <ref type=""bibr"" target=""#b10"">[11]</ref> combined with dynamic weight adaptation <ref type=""bibr"" target=""#b0"">[1]</ref> for OH-SNAP.</p><p>Both these predictors are significantly outperformed on the 33 most predictable benchmarks >Conclusion</head><p>The TAGE predictor has often been considered as state-of-theart in conditional branch prediction in terms of prediction accuracy <ref type=""bibr"" target=""#b0"">[1]</ref>. Asserting confidence to predictions by TAGE has recently been shown to be simple and storage free <ref type=""",0
"ei-c.org/ns/1.0""><head n=""6.3"">Comparisons with Alternative Branch Predictors</head><p>At the 3rd ChampionShip Branch Prediction, the FTL++ predictor <ref type=""bibr"" target=""#b6"">[7]</ref> and the OH-SNAP <ref type=""bibr"" target=""#b13"">[14]</ref> were respectively ranked 2nd and 3rd with respective",0
"g-based, and time-based. <ref type=""bibr"">Lai et al.</ref> were the first to propose the concept of dead-block prediction and a trace-based predictor <ref type=""bibr"" target=""#b15"">[16]</ref>, which predicts a block dead once it has been accessed by a certain sequence of instructions. They use the p 28]</ref> or in hardware <ref type=""bibr"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b6"">[7]</ref>, <ref type=""bibr"" target=""#b13"">[14]</ref>, <ref type=""bibr"" target=""#b15"">[16]</ref>. Software solutions pass hints about dead-block information collected through profiling or compiler analysis ddress based <ref type=""bibr"" target=""#b6"">[7]</ref> and PC based <ref type=""bibr"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b13"">[14]</ref>, <ref type=""bibr"" target=""#b15"">[16]</ref>. Compared to data-address based approaches, PC-based approaches require much lower storage overhead. Based o d into three categories: trace-based, counting-based, and time-based.</p><p>Lai et al. were the first to propose the concept of dead-block prediction <ref type=""bibr"" target=""#b15"">[16]</ref> and a trace-based dead-block predictor for the L1 cache, called DBP. Because we use DBP in this paper to ref he optimizations, including prefetching, replacement, bypassing, power reduction, and coherence protocol optimizations.</p><p>Prefetching: Lai et al. <ref type=""bibr"" target=""#b15"">[16]</ref> and Hu et al. <ref type=""bibr"" target=""#b6"">[7]</ref> used deadblock prediction to trigger prefetches into d predictions improves the timeliness of prefetching compared to triggering prefetches on cache misses. Ferdman and Falsafi later extended the work in <ref type=""bibr"" target=""#b15"">[16]</ref> to store correlation patterns off-chip and stream them on-chip as needed <ref type=""bibr"" target=""#b3"">[4]</ U and updates the state when it becomes MRU.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>By references By bursts Trace</head><p>RefTrace <ref type=""bibr"" target=""#b15"">[16]</ref> BurstTrace Prediction Counting RefCount <ref type=""bibr"" target=""#b13"">[14]</ref> BurstCount metric Time Tim table></figure> 			<note xmlns=""http://www.tei-c.org/ns/1.0"" place=""foot"" n=""1"" xml:id=""foot_0""><p>RefTrace was evaluated on directly mapped caches in<ref type=""bibr"" target=""#b15"">[16]</ref>. This paper uses it on set-associative caches. In contrast to this study which evaluates RefTrace in the MRU",1
"optimize LRU replacement without dead-block prediction: Wong and Baer modified the LRU algorithm by replacing blocks with no temporal locality first <ref type=""bibr"" target=""#b28"">[29]</ref>, Kampe et al. proposed an Self-Correcting LRU algorithm <ref type=""bibr"" target=""#b11"">[12]</ref> to correct",0
"by replacing blocks with no temporal locality first <ref type=""bibr"" target=""#b28"">[29]</ref>, Kampe et al. proposed an Self-Correcting LRU algorithm <ref type=""bibr"" target=""#b11"">[12]</ref> to correct LRU replacement mistakes, whereas Qureshi et al. proposed to adaptively place missing blocks into",0
"ead block prediction can be performed in software <ref type=""bibr"" target=""#b22"">[23]</ref>, <ref type=""bibr"" target=""#b27"">[28]</ref> or in hardware <ref type=""bibr"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b6"">[7]</ref>, <ref type=""bibr"" target=""#b13"">[14]</ref>, <ref type=""bibr"" target="" have lower coverage. Hardware solutions can be classified into two categories: dataaddress based <ref type=""bibr"" target=""#b6"">[7]</ref> and PC based <ref type=""bibr"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b13"">[14]</ref>, <ref type=""bibr"" target=""#b15"">[16]</ref>. Compared to data-addres of cycles a block stays alive and if the block is not accessed in more than twice this number of cycles, it is predicted dead. Abella et al. proposed <ref type=""bibr"" target=""#b0"">[1]</ref> another timebased predictor to turn off dead blocks dynamically in the L2 cache. They observed that both the i to turn off blocks in the L1 D-cache <ref type=""bibr"" target=""#b12"">[13]</ref>. Abella et al. proposed to turn off blocks in the L2 cache dynamically <ref type=""bibr"" target=""#b0"">[1]</ref>. Both schemes predict how many cycles have to pass before a block can be turned off without affecting performa diction Counting RefCount <ref type=""bibr"" target=""#b13"">[14]</ref> BurstCount metric Time TimeKeeping <ref type=""bibr"" target=""#b6"">[7]</ref>, IATAC <ref type=""bibr"" target=""#b0"">[1]</ref> Future work</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>Table 2. A taxonomy of dead-block predict",0
"how many cycles have to pass before a block can be turned off without affecting performance. Dead block prediction can also be used in drowsy caches <ref type=""bibr"" target=""#b4"">[5]</ref>, to decide which blocks should switch to the drowsy state.</p><p>Coherence protocol optimization: Cache cohere",0
"master system).</p><p>In our implementation, we augment every physical register with a Superseded bit and a Pending count. This support is similar to <ref type=""bibr"" target=""#b16"">[17]</ref>. The Superseded bit marks whether the instruction that supersedes the register is older than oldest´Í Ë Í µ and the History Buffer are presented, among other techniques.</p><p>The second category includes work related to register recycling. Moudgill et al. <ref type=""bibr"" target=""#b16"">[17]</ref> discuss performing early register recycling in out-of-order processors that support precise exceptions. Howe performing early register recycling in out-of-order processors that support precise exceptions. However, the implementation of precise exceptions in <ref type=""bibr"" target=""#b16"">[17]</ref> relies on either checkpoint/rollback for every replay event, or a history buffer that restricts register rec",1
"rs to the execution stage. This is complementary to our work, and can be combined with it to achieve even better resource utilization. Lozano and Gao <ref type=""bibr"" target=""#b11"">[12]</ref>, Martin et al. <ref type=""bibr"" target=""#b14"">[15]</ref>, and Lo et al. <ref type=""bibr"" target=""#b10"">[11]< rmation to the hardware, in order to deallocate physical registers. The latter approaches require instruction set support: special symbolic registers <ref type=""bibr"" target=""#b11"">[12]</ref>, register kill instructions <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b14"">15]</ref",0
"structions hold most of the resources that they use until they retire. Examples of such resources are load/store queue entries and physical registers <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b22"">23 ""http://www.tei-c.org/ns/1.0""><head n=""3.1"">Load/Store Unit</head><p>Typical load/store units comprise one reorder queue for loads and one for stores <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b20"">21]</ref>. Either reorder queue may become a performance bottleneck if it fills ory of related work would include work that recycles load and store queue entries. Many current processors support speculative loads and replay traps <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b20"">21]</ref> and, to the best of our knowledge, this is the first proposal for earl tions. A memory replay trap occurs when a load is found to have issued to memory out of order with respect to an older memory operation that overlaps <ref type=""bibr"" target=""#b0"">[1]</ref>. When the event is identified, the offending load and all younger instructions are re-executed (Section 3.1.1) or from an earlier store. Thus, the load and all instructions following it are aborted and replayed. This mechanism is called store-load replay trap <ref type=""bibr"" target=""#b0"">[1]</ref>.</p><p>Figure <ref type=""figure"" target=""#fig_3"">4</ref>(b) shows an example of a check for possible store-loa dress disambiguation, (2) no older load is subject to replay traps, and (3) the store is not subject to squash due to branch misprediction. Condition <ref type=""bibr"" target=""#b0"">(1)</ref> means that all older loads have already generated their address and, therefore, located their ""supplier"", whet",0
".org/ns/1.0""><head n=""1"">INTRODUCTION</head><p>Modern out-of-order processors typically employ a reorder buffer (ROB) to retire instructions in order <ref type=""bibr"" target=""#b17"">[18]</ref>. In-order retirement enables precise bookkeeping of the architectural state, while making out-of-order execu handled without any rollback. Specifically, processor execu- of No Return (PNR). We assume a circular ROB implementation with Head and Tail pointers <ref type=""bibr"" target=""#b17"">[18]</ref>. tion seamlessly falls back to non-Cherry mode (Section 2.1). Then, the interrupt is processed. After that, an exception, the processor rolls back to the checkpoint, and then executes code in order until the excepting instruction is met. Smith and Pleszkun <ref type=""bibr"" target=""#b17"">[18]</ref> discuss several methods to support precise exceptions. The Reorder Buffer (ROB) and the History Buffer are p",0
"ype=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b29"" cy on a heterogeneous multi-core is to apply sampling-based scheduling <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b32"">33]</ref>. Sampling-based scheduling dynamically samples different workload-to- IP replacement. We compare PIE scheduling to a sampling-based strategy <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b32"">33]</ref> that assumes running a workload for one time interval on one core and cution behavior can yield substantial energy savings. They evaluate both static and dynamic scheduling policies. In their followon work, Kumar et al. <ref type=""bibr"" target=""#b18"">[19]</ref> study scheduling on heterogeneous multi-cores while running multi-program workloads.</p><p>The dynamic sched",1
">; or CPU plus accelerators, e.g., IBM's * A large part of this work was performed while Kenzo Van Craeynest was an intern at Intel/VSSAD.</p><p>Cell <ref type=""bibr"" target=""#b15"">[16]</ref>. Other commercial products integrate different CPU core types on a single chip, e.g., NVidia's Kal-El <ref t",0
"propriate core type. Recent work illustrates the potential of heterogeneous multi-cores to dramatically improve energyefficiency and power-efficiency <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b16"">17,< power consumption. To address this scheduling problem, recent proposals use workload memory intensity as an indicator to guide application scheduling <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b16"">17,< ependent on the underlying workload scheduling policy. A number of recent proposals use memory intensity as an indicator to guide workload scheduling <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b16"">17,< timation (PIE)</head><p>A direct approach to determine the best scheduling policy on a heterogeneous multi-core is to apply sampling-based scheduling <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b32""> ic PIE scheduling on private and shared LLCs with LRU, and a shared LLC with RRIP replacement. We compare PIE scheduling to a sampling-based strategy <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b32""> as follows:</p><formula xml:id=""formula_0"">M LP ratio = M LP big /M LP small<label>(1)</label></formula><p>ILP ratio = CP I base big /CP I base small <ref type=""bibr"" target=""#b1"">(2)</ref> with M LP defined as the average number of outstanding memory requests if at least one is outstanding <ref typ m workloads.</p><p>The dynamic scheduling policies explored in these studies use sampling to gauge the most energy-efficient core. Becchi and Crowley <ref type=""bibr"" target=""#b1"">[2]</ref> also explore sample-based scheduling. Unfortunately, sample-based scheduling, in contrast to PIE, does not sca",0
">Cell <ref type=""bibr"" target=""#b15"">[16]</ref>. Other commercial products integrate different CPU core types on a single chip, e.g., NVidia's Kal-El <ref type=""bibr"" target=""#b25"">[26]</ref> which integrates four performance-tuned cores along with one energy-tuned core, and ARM's big.LITTLE chip <r ling for heterogeneous multi-cores with one big core and multiple small cores, as well as several big cores and one small core (e.g., NVidia's Kal-El <ref type=""bibr"" target=""#b25"">[26]</ref>); we assume all cores are active all the time. Figure <ref type=""figure"" target=""#fig_8"">7</ref> shows that",0
"type=""bibr"" target=""#b27"">[28,</ref><ref type=""bibr"" target=""#b28"">29]</ref> study how to best integrate an MLP technique (such as runahead execution <ref type=""bibr"" target=""#b23"">[24]</ref>) into an asymmetric multi-core processor, i.e., should one integrate the MLP technique into the small or big",0
"I range on Nehalem to a only 0.5-2 MPKI range on Haswell.</p><p>? We evaluate the performance of a state-of-the-art indirect branch predictor, ITTAGE <ref type=""bibr"" target=""#b30"">[31]</ref>, proposed in the literature on the same interpreters, and we show that, when executing interpreters, the bra he evolution of branch prediction over the last decades, and presents the state-of-the-art branch predictors TAGE for conditional branches and ITTAGE <ref type=""bibr"" target=""#b30"">[31]</ref> for indirect branches. Section 4 presents experimental setup. In Section 5, we present our experimental resu a PC-based table (might be the branch target buffer) with a tagged (PC+global branch history) indexed table.</p><p>More recently, Seznec and Michaud <ref type=""bibr"" target=""#b30"">[31]</ref> derived IT-TAGE from their TAGE predictor. Instead of simple conditional branch directions, ITTAGE stores th /ns/1.0""><head n=""4.2.2"">TAGE -Simulation</head><p>We also experimented with a state-of-the-art branch predictor from the literature: TAGE and ITTAGE <ref type=""bibr"" target=""#b30"">[31]</ref>. The performance is provided through simulation of traces produced by Pin <ref type=""bibr"" target=""#b16"">[17 ceptroninspired predictors <ref type=""bibr"" target=""#b14"">[15]</ref> and geometric history length predictors <ref type=""bibr"" target=""#b27"">[28,</ref><ref type=""bibr"" target=""#b30"">31]</ref>. All these propositions have influenced the design of the predictors embedded in state-of-art processors.</p> prediction schemes (a global history component, a loop predictor and maybe a local history predictor), TAGE <ref type=""bibr"" target=""#b28"">[29,</ref><ref type=""bibr"" target=""#b30"">31]</ref> is generally considered as the state-of-the-art in global history based conditional branch prediction. TAGE f",1
"level branch prediction <ref type=""bibr"" target=""#b36"">[37]</ref>, hybrid predictors <ref type=""bibr"" target=""#b19"">[20]</ref>, de-aliased predictors <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b32"">33]</ref>, multiple history length use",0
">2</ref>), direct threading <ref type=""bibr"" target=""#b0"">[1]</ref>, inline threading <ref type=""bibr"" target=""#b23"">[24]</ref>, or context threading <ref type=""bibr"" target=""#b1"">[2]</ref>. All forms of threading require extensions to ANSI C. Some also require limited forms of dynamic code generati",0
"namic typing, also make it more difficult to develop efficient JIT compilers. These dynamic features turn out to be heavily used in real applications <ref type=""bibr"" target=""#b24"">[25]</ref>. On lower-end systems, where short time-to-market is key, JIT compilers may also not be commercially viable,",0
"-market is key, JIT compilers may also not be commercially viable, and they rely on interpreters.</p><p>Scientists from both CERN and Fermilab report <ref type=""bibr"" target=""#b22"">[23]</ref> that ""many of LHC experiments' algorithms are both designed and used in interpreters"". As another example, t",0
"f type=""bibr"" target=""#b22"">(Pascanu et al., 2014)</ref>.</p><p>We use a minibatch stochastic gradient descent (SGD) algorithm together with Adadelta <ref type=""bibr"" target=""#b28"">(Zeiler, 2012)</ref> to train each model. Each SGD update direction is computed using a minibatch of 80 sentences. We t 2 .</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>B.2 TRAINING</head><p>We used the stochastic gradient descent (SGD) algorithm. Adadelta <ref type=""bibr"" target=""#b28"">(Zeiler, 2012)</ref> was used to automatically adapt the learning rate of each parameter ( = 10 −6 and ρ = 0.95). We ex",0
"days.</p><p>Once a model is trained, we use a beam search to find a translation that approximately maximizes the conditional probability (see, e.g., <ref type=""bibr"" target=""#b11"">Graves, 2012;</ref><ref type=""bibr"" target=""#b5"">Boulanger-Lewandowski et al., 2013)</ref>. <ref type=""bibr"" target=""#b",0
"ef type=""bibr"" target=""#b6"">Cho et al. (2014a)</ref>, we reduce the size of the combined corpus to have 348M words using the data selection method by <ref type=""bibr"" target=""#b0"">Axelrod et al. (2011)</ref>. <ref type=""foot"" target=""#foot_2"">5</ref>We do not use any monolingual data other than the",0
"m search to find a translation that approximately maximizes the conditional probability (see, e.g., <ref type=""bibr"" target=""#b11"">Graves, 2012;</ref><ref type=""bibr"" target=""#b5"">Boulanger-Lewandowski et al., 2013)</ref>. <ref type=""bibr"" target=""#b27"">Sutskever et al. (2014)</ref> used this approa",0
"sors. We first characterize big data analytics workloads with a more pragmatic experiment approach in comparison with that of CloudSuite described in <ref type=""bibr"" target=""#b16"">[17]</ref>. We adopt larger input data sets varying from 147 to 187 GB that are stored in both the memory and disk syst sets varying from 147 to 187 GB that are stored in both the memory and disk systems instead of completely storing data (only 4.5GB for Naive Bayes in <ref type=""bibr"" target=""#b16"">[17]</ref>) in the memory system. And for each workload, we collect the performance data of the whole run time after th mory system. And for each workload, we collect the performance data of the whole run time after the warm-up instead of a short period (180 seconds in <ref type=""bibr"" target=""#b16"">[17]</ref>).</p><p>We find that big data analytics applications share many inherent characteristics, which place them i CC), traditional service (SPECweb2005 and TPC-W), chip multiprocessors (PARSEC), and scale-out service (four among six benchmarks in ClousSuite paper <ref type=""bibr"" target=""#b16"">[17]</ref>) workloads. Meanwhile the service workloads in data center (scale-out service workloads) share many similari than those of computation-intensive HPCC workloads, e.g., HPC-HPL, HPC-DGEMM and chip multiprocessors workloads.</p><p>• Corroborating previous work <ref type=""bibr"" target=""#b16"">[17]</ref>, both the big data analytics workloads and service workloads suffer from notable pipeline front end stalls.< alls.</p><p>• The significant differences between the big data analytics workloads and the service workloads (four among six benchmarks in ClousSuite <ref type=""bibr"" target=""#b16"">[17]</ref>, SPECweb and TPC-W) in terms of processor pipeline stall breakdown: the big data analytics workloads suffer 5% of L2 cache misses are hit in L3 cache (last level cache), respectively. For the service workloads, our observations corroborate the previous work <ref type=""bibr"" target=""#b16"">[17]</ref>: the L2 cache is ineffective.</p><p>• For the big data analytics workloads, the misprediction ratios are low ent clusters' performance. The state-of-the-art work of characterizing scale-out (data center) workloads on a micro-architecture level is Cloud-Suite <ref type=""bibr"" target=""#b16"">[17]</ref>. However, CloudSuite paper is biased towards online service workloads: among six benchmarks, there are four ctions retired of each big data analytics workload. The input data size varies from 147 to 187 GB. In comparison with that of CloudSuite described in <ref type=""bibr"" target=""#b16"">[17]</ref>, our approach are more pragmatic. We adopt a larger data input that are stored in both memory and disk syste atic. We adopt a larger data input that are stored in both memory and disk systems instead of completely storing data (only 4.5 GB for Naive Bayes in <ref type=""bibr"" target=""#b16"">[17]</ref>) in memory. The number of instructions retired of the big data analytics workloads ranges from thousand of b d several benchmark suites, including SPEC CPU2006, HPCC, PARSEC, TPC-W, SPECweb 2005, and CloudSuite-a scale-out benchmark suite for cloud computing <ref type=""bibr"" target=""#b16"">[17]</ref>, and compared them with big data analytics workloads.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><he workload-Naive Bayes. We also choose Naive Bayes as one of the representative big data analytics workloads with a larger data input set (147 GB). In <ref type=""bibr"" target=""#b16"">[17]</ref>, the data input size is only 4.5 GB.</p><p>We set up the other five benchmarks following the introduction on rmance data in Section 4.3. The notable instruction fetch stalls indicates the front end inefficiency. Our observation corroborates the previous work <ref type=""bibr"" target=""#b16"">[17]</ref>. The front end inefficiency may caused by high-level languages, third-party libraries, deep software stacks ir stall data vary dramatically from each other in Figure <ref type=""figure"" target=""#fig_3"">5</ref>.</p><p>Implications: Corroborating previous work <ref type=""bibr"" target=""#b16"">[17]</ref>, both the big data analytics workloads and the service workloads suffer from notable front-end stalls (i.e. the front end has to wait for fetching instructions, which may be caused by two factors: deep memory hierarchy with long latency in modern processor <ref type=""bibr"" target=""#b16"">[17]</ref>, and large binary size complicated by high-level language, third-party libraries and deep software stacks. A y but also improve the energy-efficiency of processor and save the die area. For the service workloads, our observation corroborate the previous work <ref type=""bibr"" target=""#b16"">[17]</ref>: the L2 cache is ineffective.</p><formula xml:id=""formula_1"">0</formula></div> <div xmlns=""http://www.tei-c. tions. There are many potential optimization points in modern superscalar processors as previous work found e.g., on-chip bandwidth, die area and etc <ref type=""bibr"" target=""#b16"">[17]</ref>. According to our correlation analysis in this section, architects should focus on improving TLB performance ormance, since a smaller last level cache can shorten last level cache hit latency and reduce L2 cache miss penalty, which corroborates previous work <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b30"">31]</ref>. Moreover, for modern processors dedicate approximately half of the <p>Modern superscalar Out-of-Order (OoO) processors prevent us from breaking down the execution time precisely due to overlapped work in the pipeline <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b15"">16]</ref>. The retirement centric anal bibr"" target=""#b32"">[33,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b16"">17]</ref> and etc. Narayanan et al. <ref type=""bibr"" target=""#b32"">[33]</ref> characterize traditional data analytics w",1
"o update.</p><p>Media Streaming: we use Darwin streaming server 6.0.6. We set 20 Java processes and issue 20 client threads by using the Faban driver <ref type=""bibr"" target=""#b5"">[6]</ref> with GetMediumLow 70 and GetshortHi 30.</p><p>Software Testing: we use the cloud9 execution engine, and run th",0
"the execution time precisely due to overlapped work in the pipeline <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b15"">16]</ref>. The retirement centric analysis is also difficult to account how the CPU cycles are used because the pipelin",0
"processors prevent us from breaking down the execution time precisely due to overlapped work in the pipeline <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b15"">16]</ref>. The retirement centric analysis is also difficult to account how the",0
"be able to use them for transfer learning <ref type=""bibr"" target=""#b2"">(Caruana, 1995;</ref><ref type=""bibr"" target=""#b1"">Bengio et al., 2011;</ref><ref type=""bibr"" target=""#b0"">Bengio, 2011)</ref>. In transfer learning, we first train a base network on a base dataset and task, and then we repurpo",1
"re. (Section 4.2)</p><p>4. On the relatively large ImageNet dataset, we find lower performance than has been previously reported for smaller datasets <ref type=""bibr"" target=""#b7"">(Jarrett et al., 2009)</ref> when using features computed from random lower-layer weights vs. trained weights. We compar r both.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.3"">Random Weights</head><p>We also compare to random, untrained weights because <ref type=""bibr"" target=""#b7"">Jarrett et al. (2009)</ref> showed -quite strikingly -that the combination of random convolutional filters, rectificatio dom weights to work in convolutional neural networks may not be as straightforward as it was for the smaller network size and smaller dataset used by <ref type=""bibr"" target=""#b7"">Jarrett et al. (2009)</ref>. However, the comparison is not straightforward. Whereas our networks have max pooling and l 2009)</ref>. However, the comparison is not straightforward. Whereas our networks have max pooling and local normalization on layers 1 and 2, just as <ref type=""bibr"" target=""#b7"">Jarrett et al. (2009)</ref> did, we use a different nonlinearity (relu(x) instead of abs(tanh(x))), different layer size ilar tasks. Second, transferring even from a distant task is better than using random filters. One possible reason this latter result may differ from <ref type=""bibr"" target=""#b7"">Jarrett et al. (2009)</ref> is because their fully-trained (non-random) networks were overfitting more on the smaller Ca",0
"12)</ref>, unsupervised density learning <ref type=""bibr"" target=""#b11"">(Lee et al., 2009)</ref>, and unsupervised learning of sparse representations <ref type=""bibr"" target=""#b10"">(Le et al., 2011)</ref>.</p><p>Because finding these standard features on the first layer seems to occur regardless of",0
"tent that features within a network are general, we will be able to use them for transfer learning <ref type=""bibr"" target=""#b2"">(Caruana, 1995;</ref><ref type=""bibr"" target=""#b1"">Bengio et al., 2011;</ref><ref type=""bibr"" target=""#b0"">Bengio, 2011)</ref>. In transfer learning, we first train a base",0
"ntage of this fact to obtain state-of-the-art results when transferring from higher layers <ref type=""bibr"" target=""#b4"">(Donahue et al., 2013a;</ref><ref type=""bibr"" target=""#b13"">Zeiler and Fergus, 2013;</ref><ref type=""bibr"" target=""#b12"">Sermanet et al., 2014)</ref>, collectively suggesting that",0
"nt brought by the increased ConvNet depth in a fair setting, all our ConvNet layer configurations are designed using the same principles, inspired by <ref type=""bibr"" target=""#b3"">Ciresan et al. (2011)</ref>; <ref type=""bibr"" target=""#b21"">Krizhevsky et al. (2012)</ref>. In this section, we first de in Network"" architecture of <ref type=""bibr"" target=""#b23"">Lin et al. (2014)</ref>.</p><p>Small-size convolution filters have been previously used by <ref type=""bibr"" target=""#b3"">Ciresan et al. (2011)</ref>, but their nets are significantly less deep than ours, and they did not evaluate on the larg",1
">(Zeiler &amp; Fergus, 2013;</ref><ref type=""bibr"" target=""#b6"">Donahue et al., 2013;</ref><ref type=""bibr"" target=""#b27"">Razavian et al., 2014;</ref><ref type=""bibr"" target=""#b1"">Chatfield et al., 2014)</ref>, as it turns out that deep image representations, learnt on ILSVRC, generalise well to oth s. Our methods set the new state of the art across image representations, pretrained on the ILSVRC dataset, outperforming the previous best result of <ref type=""bibr"" target=""#b1"">Chatfield et al. (2014)</ref>  We found that unlike VOC, on Caltech datasets the stacking of descriptors, computed over rformance across the splits, which is measured by the mean class recall (which compensates for a different number of test images per class). Following<ref type=""bibr"" target=""#b1"">Chatfield et al. (2014)</ref>;<ref type=""bibr"" target=""#b33"">Zeiler &amp; Fergus (2013)</ref>;<ref type=""bibr"" target=""# l. (2014)</ref>, which, however, performs significantly worse than our nets on VOC-2007. On Caltech-256, our features outperform the state of the art <ref type=""bibr"" target=""#b1"">(Chatfield et al., 2014</ref>) by a large margin (8.6%).</p><p>Action Classification on VOC-2012. We also evaluated our",0
"his section, we evaluate our ConvNets, pre-trained on ILSVRC, as feature Published as a conference paper at ICLR 2015 served in semantic segmentation <ref type=""bibr"" target=""#b24"">(Long et al., 2014)</ref>, image caption generation <ref type=""bibr"" target=""#b19"">(Kiros et al., 2014;</ref><ref type=",0
"the training is carried out by optimising the multinomial logistic regression objective using mini-batch gradient descent (based on back-propagation <ref type=""bibr"" target=""#b22"">(LeCun et al., 1989</ref>)) with momentum. The batch size was set to 256, momentum to 0.9. The training was regularised es the best result (7.0% test error), outperforming a single GoogLeNet by 0.9%. Notably, we did not depart from the classical ConvNet architecture of <ref type=""bibr"" target=""#b22"">LeCun et al. (1989)</ref>, but improved it by substantially increasing the depth. </p></div> <div xmlns=""http://www.tei ification accuracy, and that state-of-the-art performance on the ImageNet challenge dataset can be achieved using a conventional ConvNet architecture <ref type=""bibr"" target=""#b22"">(LeCun et al., 1989;</ref><ref type=""bibr"" target=""#b21"">Krizhevsky et al., 2012)</ref> with substantially increased de",0
"ibr"" target=""#b1"">[1,</ref><ref type=""bibr"" target=""#b2"">2,</ref><ref type=""bibr"" target=""#b3"">3]</ref>. Parallel research exists in vocal expression <ref type=""bibr"" target=""#b4"">[4,</ref><ref type=""bibr"" target=""#b5"">5]</ref>. Many prosodic features, e.g., pitch, duration, loudness, voice quality, urately recognizing the intended emotion. This agrees with an earlier study examining gender differences in emotion detection using the voice channel <ref type=""bibr"" target=""#b4"">[4]</ref>. More detailed studies can be performed in the future.</p><p>In conclusion, we presented a sampling of the ana",1
"each trial would have 9 unique alternate choices on the multiplechoice grid. The target words and the foil words were chosen from WRAT4 reading lists <ref type=""bibr"" target=""#b46"">[45]</ref> so that all participants would know the words. For each rater, the 12 target words and the 27 foil words are",0
"larly on the issues arising when conflicting interpretations of the facial and vocal expression are possible <ref type=""bibr"" target=""#b14"">[14,</ref><ref type=""bibr"" target=""#b15"">15]</ref>. However, we still do not have a full understanding of the interplay between the two modalities. Specifically",0
"d that prototypical basic emotions can be universally recognized by different groups of people based on the activation of specific facial expressions <ref type=""bibr"" target=""#b1"">[1,</ref><ref type=""bibr"" target=""#b2"">2,</ref><ref type=""bibr"" target=""#b3"">3]</ref>. Parallel research exists in vocal",0
"""bibr"" target=""#b48"">47,</ref><ref type=""bibr"" target=""#b49"">48,</ref><ref type=""bibr"" target=""#b50"">49,</ref><ref type=""bibr"" target=""#b51"">50,</ref><ref type=""bibr"" target=""#b52"">51]</ref>.</p><p>Our study was designed to be less attractive to cheaters as suggested by Eickhoff and de Vries <ref ty",0
"t is required to provide training targets. The combination of bidirectional LSTM and CTC has been applied to characterlevel speech recognition before <ref type=""bibr"" target=""#b5"">(Eyben et al., 2009)</ref>, however the relatively shallow architecture used in that work did not deliver compelling res",1
"bidirectional LSTM network <ref type=""bibr"" target=""#b10"">(Graves et al., 2013)</ref> with a Connectionist Temporal Classification (CTC) output layer <ref type=""bibr"" target=""#b9"">(Graves et al., 2006;</ref><ref type=""bibr"">Graves, 2012, Chapter 7)</ref>. The network is trained directly on the text occur. Eq. ( <ref type=""formula"" target=""#formula_8"">15</ref>) can be efficiently evaluated and differentiated using a dynamic programming algorithm <ref type=""bibr"" target=""#b9"">(Graves et al., 2006)</ref>. Given a target transcription y * , the network can then be trained to minimise the CTC obje",1
"g. We have therefore chosen spectrograms as a minimal preprocessing scheme.</p><p>The spectrograms are processed by a deep bidirectional LSTM network <ref type=""bibr"" target=""#b10"">(Graves et al., 2013)</ref> with a Connectionist Temporal Classification (CTC) output layer <ref type=""bibr"" target=""#b ard and backward layers at the level below. If LSTM is used for the hidden layers the complete architecture is referred to as deep bidirectional LSTM <ref type=""bibr"" target=""#b10"">(Graves et al., 2013)</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3."">Connectionist Temporal Cla",1
"ch as Maximum Mutual Information have been used to directly train HMM-neural network hybrids to maximise the probability of the correct transcription <ref type=""bibr"" target=""#b0"">(Bahl et al., 1986;</ref><ref type=""bibr"" target=""#b16"">Jaitly et al., 2012)</ref>. However these techniques are only su",0
"expertise. For example, convolutional neural networks are now able to directly classify raw pixels into high-level concepts such as object categories <ref type=""bibr"" target=""#b17"">(Krizhevsky et al., 2012)</ref> and messages on traffic signs <ref type=""bibr"" target=""#b3"">(Ciresan et al., 2011)</ref",0
"ctly train HMM-neural network hybrids to maximise the probability of the correct transcription <ref type=""bibr"" target=""#b0"">(Bahl et al., 1986;</ref><ref type=""bibr"" target=""#b16"">Jaitly et al., 2012)</ref>. However these techniques are only suitable for retraining a system already trained at frame",0
"Lexical states such as graphemes and characters have been considered for HMMbased recognisers as a way of dealing with out of vocabulary (OOV) words <ref type=""bibr"" target=""#b6"">(Galescu, 2003;</ref><ref type=""bibr"" target=""#b1"">Bisani &amp; Ney, 2005)</ref>, however they were used to augment rath",0
"neural network-HMM hybrid (DNN-HMM). The DNN-HMM was created using alignments from an SGMM-HMM system trained using Kaldi recipe 's5', model 'tri4b' <ref type=""bibr"" target=""#b20"">(Povey et al., 2011)</ref>.</p><p>The 14 hour subset was first used to train a Deep Belief Network (DBN) <ref type=""bib",0
"ing on context and syntactic structure. We employ a novel adaptive multi-compositionality layer in recursive neural network, which is named as AdaRNN <ref type=""bibr"" target=""#b1"">(Dong et al., 2014)</ref>. It consists of more than one composition functions, and we model the adaptive sentiment propa",1
") to leverage the ability of deep learning models. The neural models use distributed representation <ref type=""bibr"" target=""#b6"">(Hinton, 1986;</ref><ref type=""bibr"" target=""#b12"">Rumelhart et al., 1986;</ref><ref type=""bibr"" target=""#b0"">Bengio et al., 2003)</ref> to automatically learn features f >where Θ represents the parameters, and the L 2regularization penalty is used.</p><p>Based on the converted tree, we employ backpropagation algorithm <ref type=""bibr"" target=""#b12"">(Rumelhart et al., 1986)</ref> to propagate the errors from root node to the leaf nodes. We calculate the derivatives t",0
"l models use distributed representation <ref type=""bibr"" target=""#b6"">(Hinton, 1986;</ref><ref type=""bibr"" target=""#b12"">Rumelhart et al., 1986;</ref><ref type=""bibr"" target=""#b0"">Bengio et al., 2003)</ref> to automatically learn features for target-dependent sentiment classification. RNN utilizes t",0
"and setting their POS tags to NN. Liblinear <ref type=""bibr"" target=""#b3"">(Fan et al., 2008)</ref> is used for baselines. A tweet-specific tokenizer <ref type=""bibr"" target=""#b5"">(Gimpel et al., 2011)</ref> is employed, and the dependency parsing results are computed by Stanford Parser <ref type=""b",0
"t. The experimental results suggest that our approach yields better performances than the baseline methods.</p><p>2 RNN: Recursive Neural Network RNN <ref type=""bibr"" target=""#b13"">(Socher et al., 2011)</ref> represents the phrases and words as D-dimensional vectors. It performs compositions based o",0
"en given unlimited computational resources. A known technique for achieving truthfulness in online auctions is based on the concept of a supply curve <ref type=""bibr"" target=""#b21"">[22]</ref>, as applied by Zhang et al. <ref type=""bibr"" target=""#b3"">[4]</ref> in their design of an online cloud aucti",1
"ocation and a carefully designed pricing rule. However, when the underlying allocation problem is NP-hard, which is common for combinatorial auctions <ref type=""bibr"" target=""#b15"">[16]</ref>, VCG becomes computationally infeasible. When polynomial-time approximation algorithms are applied to solvin",0
"enables prompt and on-demand access to computing resources. As exemplified in Amazon EC2 <ref type=""bibr"" target=""#b0"">[1]</ref> and Microsoft Azure <ref type=""bibr"" target=""#b1"">[2]</ref>, cloud providers invest substantially into their datacenter infrastructure, providing a virtually unlimited ""s way, the adjustment of in each round can be understood as the adjustment of the dual variable toward an optimal solution to the offline dual problem <ref type=""bibr"" target=""#b1"">(2)</ref>.</p><p>The performance of our online algorithm in Algorithm 1 is stated in Theorem 1, with a detailed proof in lgorithm, it produces feasible solution for dual (2). 2) Let be the value of the objective function in (1) after th iteration, , the same for in dual <ref type=""bibr"" target=""#b1"">(2)</ref>. Then, satisfies , at any round.</p><p>3) The algorithm produces an almost feasible solution for primal <ref t rantees Also notice is nondecreasing with , so (2a) holds.</p><p>Proof of ( <ref type=""formula"">2</ref>): At time . Substitute for , and we get claim <ref type=""bibr"" target=""#b1"">(2)</ref>. Proof of (3): Constraints (1a), (1c), (1d) are guaranteed by the constraints in <ref type=""bibr"" target=""#b2"" ly difference between Theorems 5 and 1 is we introduce randomness here. Recall the proof of Theorem 1, the only claim affected by randomness is claim <ref type=""bibr"" target=""#b1"">(2)</ref>. We analyze the expectation of the increment on the primal and dual and . At time . , which finishes our proof",0
"scheduling. Xu et al. <ref type=""bibr"" target=""#b24"">[25]</ref> summarize the recent attempts in managing performance overhead in clouds. Lin et al. <ref type=""bibr"" target=""#b25"">[26]</ref>, <ref type=""bibr"" target=""#b26"">[27]</ref> study the energy efficiency in VM provisioning. VM migration over",0
". Alicherry et al. <ref type=""bibr"" target=""#b22"">[23]</ref> consider network load when allocating VMs in a distributed cloud system. Maguluri et al. <ref type=""bibr"" target=""#b23"">[24]</ref> tackle the randomness of arriving workloads and solve optimization problems for load balancing and VM schedu",0
"[14]</ref>. <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b15"">16]</ref> further extend the deep models for multimodal learning. <ref type=""bibr"" target=""#b16"">[17]</ref> design a cross-media learning method based on DNN, and leverage the model for detecting psychological states ributes, we further design a convolutional neural network (CNN) with cross autoencoders to learn the latent high-level attributes on crossmodal units <ref type=""bibr"" target=""#b16"">[17]</ref> <ref type=""bibr"" target=""#b17"">[18]</ref>. Finally, we propose a deep neural network (DNN) model to incorpor he detail attributes with multiple modalities of every tweets by utilizing a recently proposed cross-media model, namely the Cross Autoencoders (CAE) <ref type=""bibr"" target=""#b16"">[17]</ref>.</p><p>An auto encoder is a basic unit in deep neural networks for learning distinctive attributes from data pooling methods it also get competitive results. As for comparison with previous work, due to the different goal, our results are not comparable with <ref type=""bibr"" target=""#b16"">[17]</ref>. Actually, the most related user-level prediction work is <ref type=""bibr"" target=""#b10"">[11]</ref>, with th",1
"developed many methods to measure psychological stress, including psychological questionnaire based interviews <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b3"">4]</ref> and physiological signal based measures <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b5"">6]",0
"years, extensive researches on deep learning show superior ability of deep neural networks (DNN) in learning features from large scale unlabeled data <ref type=""bibr"" target=""#b11"">[12]</ref><ref type=""bibr"" target=""#b12"">[13]</ref><ref type=""bibr"" target=""#b13"">[14]</ref>. <ref type=""bibr"" target="" ref type=""bibr"" target=""#b16"">[17]</ref>.</p><p>An auto encoder is a basic unit in deep neural networks for learning distinctive attributes from data <ref type=""bibr"" target=""#b11"">[12]</ref><ref type=""bibr"" target=""#b12"">[13]</ref><ref type=""bibr"" target=""#b13"">[14]</ref>. It is a shallow network c",0
"problems and mental diseases. Chronic stress increases the risk of developing health problems such as insomnia, obesity, heart diseases, cancer etc. <ref type=""bibr"" target=""#b0"">[1]</ref>. Many studies have revealed a link between stress and mental diseases like anxiety disorders, depression etc. /p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2.2.3"">Observations on Behavioral Correlation</head><p>As revealed by psychology theories <ref type=""bibr"" target=""#b0"">[1]</ref>, there are many common symptoms may be related to stress, including insomnia, social withdrawal .etc. These sy",0
"cent years. Researchers are trying to leverage pervasive devices like personal computers and mobile phones for routine stress detection. Hong L. etc. <ref type=""bibr"" target=""#b6"">[7]</ref> proposed StressSense to unobtrusively recognize stress from human voice using smartphones. Paredes, P. etc. <r rue) as non-stressed (negative). Accuracy is the proportion of correct prediction or true results among testing samples. More formally it is given by <ref type=""bibr"" target=""#b6"">(7)</ref> F1-score, on the other hand, considers both the precision and recall of the result, which is given by <ref typ",0
"t=""#b13"">[14]</ref>.  We propose a Cross-media Auto-Encoder (CAE) to learn the joint representation using Denoising Auto-Encoder (DAE) style learning <ref type=""bibr"" target=""#b14"">[15]</ref>. Fig. <ref type=""figure"" target=""#fig_4"">5</ref> shows a sketch of a CAE. With training data containing all",1
"ition and extraction of the middle-level representations is based on psychological principles and art theories <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b6"">7]</ref>. Finally, a deep neural network is implemented to learn the stress detection model from the extracted middle-le representations from tweets' low level features based on previous psychological principles and art theories in <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b6"">7]</ref>. The definitions are as follows:</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>1) Linguistic Attribu",0
"traditional stress detection, many efforts have been devoted to detecting psychological stress more efficiently and timely recent years. Most of them <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b2"">3]</ref> focus on detecting stress in real-time via body-worn sensors and mobile al-time via body-worn sensors and mobile devices. The equipment is used to gather physiological parameters and daily activity data. Andrew Raij .etc. <ref type=""bibr"" target=""#b1"">[2]</ref> designed a mobile phone based system, named mStress, using 6 extra wireless sensors to collect physiological d",0
"ibo users has reached 500 million <ref type=""foot"" target=""#foot_1"">3</ref> . And according to the report given by Chinese Academy of Social Sciences <ref type=""bibr"" target=""#b0"">[1]</ref>, self-expression is the first main usage of microblog The hashtag (stress label) word given by the user himsel",0
"typical cross-media setting. It has been reported that deep neural networks can be used to learn cross-media or share representations in speech video <ref type=""bibr"" target=""#b12"">[13]</ref> or images with text tag <ref type=""bibr"" target=""#b13"">[14]</ref>.  We propose a Cross-media Auto-Encoder (C earn feature across modalities and they tend to learn feature within each single modality with few correlation connection between multiple modalities <ref type=""bibr"" target=""#b12"">[13]</ref>. We train the CAE with a cropped set of data that input from or two modalities are absent while require it t",0
"dbox Prefetching to Feedback Directed Prefetching (FDP) <ref type=""bibr"" target=""#b20"">[21]</ref> and Address Map Pattern Matching Prefetching (AMPM) <ref type=""bibr"" target=""#b11"">[12]</ref>. We now describe both of these techniques in some detail.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""",1
"pe=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b0"">1,</ l locality. As such, many studies showed these applications can benefit from sequential and stride prefetching <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b8"">9]</ref>. However, applications that lack spatial locality receive very little benefit from sequential prefetching. Ther",0
"<div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3"">Related Work</head><p>There are numerous studies that have proposed novel prefetching algorithms <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b18""> h distance is increased, otherwise it is decreased. Jiminez et al. present a real life dynamic implementation of a prefetcher in the POWER7 processor <ref type=""bibr"" target=""#b12"">[13]</ref>. The POWER7 processor supports a number of prefetcher configurations and prefetch distances (seven in all).",0
"type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b0"">1,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b15"">16]</ref>. Initial research on prefetchin 0""><head n=""5.2"">Workloads</head><p>We evaluate the Sandbox Prefetching method by testing it with a variety of workloads from the SPEC CPU 2006 suite <ref type=""bibr"" target=""#b0"">[1]</ref>. We selected workloads that exhibit a non-trivial rate of last level cache misses per instruction. Some of the",0
"ype=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b21"">",0
"en layer activations as the inputs to train the next layer. This greedy layer-wise training process forms the model of the Stacked Sparse Autoencoder <ref type=""bibr"" target=""#b23"">(Vincent et al. 2010)</ref>. When all the layers are trained in this manner, we use the output of the final layer as th",1
"ype=""bibr"" target=""#b19"">(Shi and Malik 2000)</ref>, community detection <ref type=""bibr"" target=""#b20"">(Smyth and White 2005)</ref>, and VLSI design <ref type=""bibr"" target=""#b5"">(Chan, Schlag, and Zien 1994)</ref>; on the other hand, it is easy to transform a clustering problem in the vector space",0
"d Vincent 2013)</ref>).</p><p>In the greedy layerwised pretraining process, autoencoder <ref type=""bibr"" target=""#b4"">(Bourlard and Kamp 1988)</ref>  <ref type=""bibr"" target=""#b11"">(Hinton and Zemel 1994)</ref> is commonly used as a basic unit to generate new representations in each layer, and it is",0
"ectures possible and effective is the greedy layerwised unsupervised pretraining <ref type=""bibr"" target=""#b10"">(Hinton and Salakhutdinov 2006</ref>) <ref type=""bibr"" target=""#b0"">(Bengio et al. 2007</ref>). This strategy aims to learn useful representations one layer at a time, and then to set the ltney et al. 2006))</ref>. By stacking these non-linear single layers together, deep learning are believed to yield better representations (Part 4 in <ref type=""bibr"" target=""#b0"">(Bengio, Courville, and Vincent 2013)</ref>).</p><p>In the greedy layerwised pretraining process, autoencoder <ref type= r DNN-based model, we evaluated its performance on several real world datasets.</p><p>1. Wine. This is a dataset from UCI Machine Learning Repository <ref type=""bibr"" target=""#b0"">(Asuncion and Newman 2007)</ref>, consisting of 178 instances with 13 attributes. Every instance corresponds to a certai",0
"eories, and large-scale training systems towards deep learning have been developed and successfully adopted in real tasks, such as speech recognition <ref type=""bibr"" target=""#b8"">(Dahl et al. 2012)</ref>, image classification <ref type=""bibr"" target=""#b13"">(Krizhevsky, Sutskever, and Hinton 2012)</ many applications such as image classification, speech recognition, and natural language processing <ref type=""bibr"" target=""#b1"">(Bengio 2009</ref>) <ref type=""bibr"" target=""#b8"">(Dahl et al. 2012</ref>) <ref type=""bibr"" target=""#b7"">(Collobert et al. 2011)</ref>  <ref type=""bibr"" target=""#b13"">(Kr",0
"Neural Network (RecurrentNN). This model analyzes a text word by word and stores the semantics of all the previous text in a fixed-sized hidden layer <ref type=""bibr"" target=""#b7"">(Elman 1990</ref>). The advantage of RecurrentNN is the ability to better capture the contextual information. This could",1
"previous studies on CNNs tends to use simple convolutional kernels such as a fixed window <ref type=""bibr"" target=""#b5"">(Collobert et al. 2011;</ref><ref type=""bibr"" target=""#b12"">Kalchbrenner and Blunsom 2013)</ref>. When using such kernels, it is difficult to determine the window size: small wind alyse sentiment of phrases and sentences. <ref type=""bibr"" target=""#b17"">Mikolov (2012)</ref> uses recurrent neural network to build language models. <ref type=""bibr"" target=""#b12"">Kalchbrenner and Blunsom (2013)</ref> proposed a novel recurrent network for dialogue act classification. Collobert et",1
"ral networks</head><p>Recently, deep neural networks <ref type=""bibr"" target=""#b10"">(Hinton and Salakhutdinov 2006)</ref> and representation learning <ref type=""bibr"" target=""#b3"">(Bengio, Courville, and Vincent 2013)</ref> have led to new ideas for solving the data sparsity problem, and many neural",0
"target=""#b18"">Mnih and Hinton 2007;</ref><ref type=""bibr"" target=""#b17"">Mikolov 2012;</ref><ref type=""bibr"" target=""#b5"">Collobert et al. 2011;</ref><ref type=""bibr"" target=""#b11"">Huang et al. 2012;</ref><ref type=""bibr"">Mikolov et al. 2013)</ref>. The neural representation of a word is called word This baseline uses the weighted average of the word embeddings and subsequently applies a softmax layer. The weight for each word is its tfidf value. <ref type=""bibr"" target=""#b11"">Huang et al. (2012)</ref> also used this strategy as the global context in their task. <ref type=""bibr"" target=""#b13"">K",0
"ion</head><p>Text classification is an essential component in many applications, such as web searching, information filtering, and sentiment analysis <ref type=""bibr"" target=""#b0"">(Aggarwal and Zhai 2012)</ref>. Therefore, it has attracted considerable attention from many researchers.</p><p>A key pr",0
"representations have been proposed <ref type=""bibr"" target=""#b2"">(Bengio et al. 2003;</ref><ref type=""bibr"" target=""#b18"">Mnih and Hinton 2007;</ref><ref type=""bibr"" target=""#b17"">Mikolov 2012;</ref><ref type=""bibr"" target=""#b5"">Collobert et al. 2011;</ref><ref type=""bibr"" target=""#b11"">Huang et al . <ref type=""bibr"" target=""#b25"">Socher et al. (2013)</ref> introduced recursive neural tensor network to analyse sentiment of phrases and sentences. <ref type=""bibr"" target=""#b17"">Mikolov (2012)</ref> uses recurrent neural network to build language models. <ref type=""bibr"" target=""#b12"">Kalchbrenne",0
"g is a distributed representation of words and greatly alleviates the data sparsity problem <ref type=""bibr"" target=""#b2"">(Bengio et al. 2003)</ref>. <ref type=""bibr"" target=""#b16"">Mikolov, Yih, and Zweig (2013)</ref> shows that pre-trained word embeddings can capture meaningful syntactic and semant",0
"|D, θ) (9)</formula><p>where D is the training document set and class D is the correct class of document D.</p><p>We use stochastic gradient descent <ref type=""bibr"" target=""#b4"">(Bottou 1991)</ref> to optimize the training target. In each step, we randomly select an example (D, class D ) and make",0
"r each word is its tfidf value. <ref type=""bibr"" target=""#b11"">Huang et al. (2012)</ref> also used this strategy as the global context in their task. <ref type=""bibr"" target=""#b13"">Klementiev, Titov, and Bhattarai (2012)</ref> used this in crosslingual document classification.</p><p>LDA LDA-based ap",0
"onality <ref type=""bibr"" target=""#b2"">(Bengio et al. 2003)</ref>. Recent research <ref type=""bibr"" target=""#b10"">(Hinton and Salakhutdinov 2006;</ref><ref type=""bibr"" target=""#b8"">Erhan et al. 2010)</ref> shows that neural networks can converge to a better local minima with a suitable unsupervised p",0
"o predict the relationship between two marked nouns.</p><p>The idea of extracting features for NLP using convolutional DNN was previously explored by <ref type=""bibr"" target=""#b3"">Collobert et al. (2011)</ref>, in the context of POS tagging, chunking (CHUNK), Named Entity Recognition (NER) and Seman ext of POS tagging, chunking (CHUNK), Named Entity Recognition (NER) and Semantic Role Labeling (SRL). Our work shares similar intuition with that of <ref type=""bibr"" target=""#b3"">Collobert et al. (2011)</ref>. In <ref type=""bibr"" target=""#b3"">(Collobert et al., 2011)</ref>, all of the tasks are con >Word Representation</head><p>In the word representation component, each input word token is transformed into a vector by looking up word embeddings. <ref type=""bibr"" target=""#b3"">Collobert et al. (2011)</ref> reported that word embeddings learned from significant amounts of unlabeled data are far m res and predict a relation globally. When using neural network, the convolution approach is a natural method to merge all of the features. Similar to <ref type=""bibr"" target=""#b3"">Collobert et al. (2011)</ref>, we first process the output of Window Processing using a linear transformation.</p><formu Figure <ref type=""figure"" target=""#fig_2"">3</ref>), we heuristically choose d e = 5. Finally, the word dimension and learning rate are the same as in <ref type=""bibr"" target=""#b3"">Collobert et al. (2011)</ref>. Table <ref type=""table"">2 reports</ref>  Table <ref type=""table"">2</ref>: Hyperparameters e organizers. License details: http://creativecommons.org/licenses/by/4.0/ 1 A word embedding is a distributed representation for a word. For example,<ref type=""bibr"" target=""#b3"">Collobert et al. (2011)</ref> use a 50-dimensional vector to represent a word.2 http://en.wikipedia.org/wiki/Bag-of-word esent a word.2 http://en.wikipedia.org/wiki/Bag-of-words model</note> 			<note xmlns=""http://www.tei-c.org/ns/1.0"" place=""foot"" n=""3"" xml:id=""foot_1""><ref type=""bibr"" target=""#b3"">Collobert et al. (2011)</ref> proposed a pairwise ranking approach to train the word embeddings, and the total training NER) and Semantic Role Labeling (SRL). Our work shares similar intuition with that of <ref type=""bibr"" target=""#b3"">Collobert et al. (2011)</ref>. In <ref type=""bibr"" target=""#b3"">(Collobert et al., 2011)</ref>, all of the tasks are considered as the sequential labeling problems in which each word i",1
"the output of preexisting NLP systems, which leads to the propagation of the errors in the existing tools and hinders the performance of such systems <ref type=""bibr"" target=""#b0"">(Bach and Badaskar, 2007)</ref>. It is attractive to consider extracting features that are as independent from existing",0
"ted to convert the classification clues (such as sequences and parse trees) into feature vectors <ref type=""bibr"" target=""#b9"">(Kambhatla, 2004;</ref><ref type=""bibr"" target=""#b15"">Suchanek et al., 2006)</ref>. Feature-based methods suffer from the problem of selecting a suitable feature set when co",0
"a machine learning community <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b6"">7]</ref>. To perform distributed (parallel get=""#b17"">18,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b19"">20]</ref> for GP. Very recently, the authors of PowerGraph <ref type=""bibr"" target=""#b5"">[6]</ref> find that the vertex-cut methods can achieve better performance than edge-cut methods, especially for power-la ethods, especially for power-law graphs. Hence, vertex-cut has attracted more and more attention from DGC research community. For example, PowerGraph <ref type=""bibr"" target=""#b5"">[6]</ref> adopts a random vertex-cut method and two greedy variants for GP.</p><p>GraphBuilder <ref type=""bibr"" target="" ion cost than existing methods and can simultaneously guarantee good workload balance. • DBH can be implemented as an execution engine for PowerGraph <ref type=""bibr"" target=""#b5"">[6]</ref>, and hence all PowerGraph applications can be seamlessly supported by DBH. • Empirical results on several larg . , p} be the span of vertex v over different machines. Hence, |A(v)| is the number of replicas of v among different machines. Similar to PowerGraph <ref type=""bibr"" target=""#b5"">[6]</ref>, one of the replicas of a vertex is chosen as the master and the others are treated as the mirrors of the mast ower parameter α is a positive constant. The lower the α is, the more skewed a graph will be. This power-law degree distribution makes GP challenging <ref type=""bibr"" target=""#b5"">[6]</ref>. Although vertex-cut methods can achieve better performance than edge-cut methods for power-law graphs <ref ty enging <ref type=""bibr"" target=""#b5"">[6]</ref>. Although vertex-cut methods can achieve better performance than edge-cut methods for power-law graphs <ref type=""bibr"" target=""#b5"">[6]</ref>, existing vertex-cut methods, such as random method in PowerGraph and grid-based method in GraphBuilder <ref t ad><p>In this section, we present theoretical analysis for our DBH method. For comparison, the random vertex-cut method (called Random) of PowerGraph <ref type=""bibr"" target=""#b5"">[6]</ref> and the grid-based constrained solution (called Grid) of GraphBuilder <ref type=""bibr"" target=""#b7"">[8]</ref> erent methods.</p><p>Random assigns each edge evenly to the p machines via a randomized hash function. The result can be directly got from PowerGraph <ref type=""bibr"" target=""#b5"">[6]</ref>. Lemma 1. Assume that we have a sequence of n vertices {v i } n i=1 and the corresponding degree sequence D = where h i ≤ d i − 1 for any v i .</formula><p>This theorem says that our DBH method has smaller expected replication factor than Random of PowerGraph <ref type=""bibr"" target=""#b5"">[6]</ref>.</p><p>Next we turn to the analysis of the balance constraints. We still fix the degree sequence and have the <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""5.2"">Baselines and Evaluation Metric</head><p>In our experiment, we adopt the Random of PowerGraph <ref type=""bibr"" target=""#b5"">[6]</ref> and the Grid of GraphBuilder [8]<ref type=""foot"" target=""#foot_0"">1</ref> as baselines for empirical compariso",1
"For example, PowerGraph <ref type=""bibr"" target=""#b5"">[6]</ref> adopts a random vertex-cut method and two greedy variants for GP.</p><p>GraphBuilder <ref type=""bibr"" target=""#b7"">[8]</ref> provides some heuristics, such as the grid-based constrained solution, to improve the random vertex-cut method raphs <ref type=""bibr"" target=""#b5"">[6]</ref>, existing vertex-cut methods, such as random method in PowerGraph and grid-based method in GraphBuilder <ref type=""bibr"" target=""#b7"">[8]</ref>, cannot make effective use of the powerlaw distribution to achieve satisfactory performance.</p><p>3 Degree-Ba ut method (called Random) of PowerGraph <ref type=""bibr"" target=""#b5"">[6]</ref> and the grid-based constrained solution (called Grid) of GraphBuilder <ref type=""bibr"" target=""#b7"">[8]</ref> are adopted as baselines. Our analysis is based on randomization. Moreover, we assume that the graph is undire",1
"≥ 1 are imbalance factors. We de- The degrees of natural graphs usually follow skewed power-law distributions <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b0"">1]</ref>:</p><formula xml:id=""formula_2"">fine 1 n n i=1 |A(v i )| as replication factor, p |E| max m |{e ∈ E | M (e) =</",0
"ized assignment is close to the perfect balance. This problem is well studied in the model of uniformly throwing n balls into p bins when n p(ln p) 3 <ref type=""bibr"" target=""#b16"">[17]</ref>.</p><p>Lemma 2. The maximum number of master vertices for each machine is bounded as follows:</p><formula xm",0
"-world graphs are the same as those in the experiment of PowerGraph. And some additional real-world graphs are from the UF Sparse Matrices Collection <ref type=""bibr"" target=""#b4"">[5]</ref>. </p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""5.2"">Baselines and Evaluation Metric</head><p>In",0
"=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b6"">7]</ref>. To perform distributed (parallel) graph-computing on clusters with sev",0
"5]</ref>, use edge-cut methods <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b19"">20]</ref> for GP. Very recently, the authors of PowerGraph <ref type=""bibr"" target=""#b5"">[6]</ref> find that the vertex",0
"rmance events were historically defined in an ad-doc bottom-up fashion, where PMU designers attempted to cover key issues via ""dedicated miss events"" <ref type=""bibr"" target=""#b0"">[1]</ref>. Yet, how does one pin-point performance issues that were not explicitly foreseen at design time?</p><p>Bottle urate and robust which is a necessity at the hierarchy's top level. This accurate classification distinguishes our method from previous approaches in <ref type=""bibr"" target=""#b0"">[1]</ref>[5][6].</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.3."">Frontend Bound category</head><p>Reca "" target=""#b6"">[7]</ref> is another scenario with same symptom. Such scenarios of L1 hits and near caches' misses, are not handled by some approaches <ref type=""bibr"" target=""#b0"">[1]</ref> <ref type=""bibr"" target=""#b4"">[5]</ref>.</p><p>Note performance hiccups, as the mentioned L1 Bound scenarios, s required as in IBM POWER5 <ref type=""bibr"" target=""#b5"">[6]</ref>, nor complex structures with latency counters as in Accurate CPI Stacks proposals <ref type=""bibr"" target=""#b0"">[1]</ref>[8] <ref type=""bibr"" target=""#b8"">[9]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.1.""> et=""#b4"">[5]</ref>.</p><p>Some researchers have attempted to accurately classify performance impacts on out-of-order architectures. Eyerman et al. in <ref type=""bibr"" target=""#b0"">[1]</ref>[9] use a simulation-based interval analysis model in order to propose a counter architecture for building accu d model. A key drawback of this approach (and its reference model) is that it restricts all stalls to a fixed set of eight predefined miss events. In <ref type=""bibr"" target=""#b0"">[1]</ref>[4] <ref type=""bibr"" target=""#b4"">[5]</ref> there is no consideration of (fetch) bandwidth issues, and short-la",1
""" target=""#b5"">[6]</ref>, nor complex structures with latency counters as in Accurate CPI Stacks proposals <ref type=""bibr"" target=""#b0"">[1]</ref>[8] <ref type=""bibr"" target=""#b8"">[9]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.1."">Top-Down Events</head><p>The basic Top-Down",0
"eculation than the FP applications. This aligns with simulations data using a propriety cycle-accurate simulator, as well as prior analysis by Jaleel <ref type=""bibr"" target=""#b10"">[11]</ref>. For example, Jaleel's analysis reported that gcc, perlbench, xalancbmk, gobmk, and sjeng have code footprin etween threads in the shared L3 cache has forced many more L3 misses. This conclusion can be validated by consulting the working-set of this workload <ref type=""bibr"" target=""#b10"">[11]</ref>: a single copy demands 8MB (same as LLC capacity) in 1-copy, vs 2MB effective per-core LLC share in 4-copy r peculation than the FP applications. This aligns with simulations data using a propriety cycle-accurate simulator, as well as prior analysis by Jaleel<ref type=""bibr"" target=""#b10"">[11]</ref>. For example, Jaleel's analysis reported that gcc, perlbench, xalancbmk, gobmk, and sjeng have code footprin between threads in the shared L3 cache has forced many more L3 misses. This conclusion can be validated by consulting the working-set of this workload<ref type=""bibr"" target=""#b10"">[11]</ref>: a single copy demands 8MB (same as LLC capacity) in 1-copy, vs 2MB effective per-core LLC share in 4-copy r",0
"counters are used to overcome bottleneck identification challenges (detailed in next section). Multiple tools have adopted our method including VTune <ref type=""bibr"" target=""#b1"">[2]</ref> and an add-on package to the standard Linux perf utility <ref type=""bibr"" target=""#b2"">[3]</ref>. Field experi speedup may apply. Examining metrics at higher program scope first, may be applied to our method as already done in VTune's General Exploration view <ref type=""bibr"" target=""#b1"">[2]</ref>. While <ref type=""bibr"" target=""#b11"">[12]</ref> estimates speedups (our method does not), it accounts for sub s; such as Sandy Bridge's support of up to eight general-purpose counters <ref type=""bibr"" target=""#b9"">[10]</ref>, or eventmultiplexing in the tools <ref type=""bibr"" target=""#b1"">[2]</ref> <ref type=""bibr"" target=""#b2"">[3]</ref>. Still a better hardware support is desired. Additionally, the ability",0
"ltiple tools have adopted our method including VTune <ref type=""bibr"" target=""#b1"">[2]</ref> and an add-on package to the standard Linux perf utility <ref type=""bibr"" target=""#b2"">[3]</ref>. Field experience with the method has revealed some performance issues that used to be underestimated by tradi to eight general-purpose counters <ref type=""bibr"" target=""#b9"">[10]</ref>, or eventmultiplexing in the tools <ref type=""bibr"" target=""#b1"">[2]</ref> <ref type=""bibr"" target=""#b2"">[3]</ref>. Still a better hardware support is desired. Additionally, the ability to pinpoint an identified issue back to",0
"have been fetched completely. This requires renaming instructions out of order. We believe that a solution similar to those proposed by Stark et al. <ref type=""bibr"" target=""#b18"">[19]</ref> and Cher et al. <ref type=""bibr"" target=""#b2"">[3]</ref> can be used to solve this problem.</p><p>The trace p e important factor in tolerating small cache sizes.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4"">Related Work</head><p>Stark et al. <ref type=""bibr"" target=""#b18"">[19]</ref> proposed a limited form of out-of-order instruction fetch for tolerating instruction cache misses, and propo",1
"s than a monolithic fetch unit would allow. It is much easier to implement techniques like dual-path execution <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b9"">10]</ref>, speculative threads <ref type=""bibr"" target=""#b21"">[22]</ref>, etc., that require fetching multiple threads.",0
""">[4]</ref>, and (b) storing instructions in dynamic execution order in the cache (i.e. using a trace cache) <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b15"">16]</ref>. The first solution makes the branch predictor and the cache more com",0
"ing table to eliminate the easy to predict traces from the primary table. The trace predictor is based on the Multiscalar task predictor named ""DOLC"" <ref type=""bibr"" target=""#b0"">[1]</ref>.</p><p>Each entry in the predictor contains the starting address of the trace, a bitmap encoding the direction nsity of taken branches in the instruction stream.</p><p>S p e c u l a t ive m u l t i t h r e a d i n g a r c h i t e c t u r e s l i ke Multiscalar <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b17"">18]</ref> come closest to this technique as far as the nature of instruction fet",0
"e reasonable size, high prediction accuracy, and a small working set. The reader is referred to other papers <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b16"">17]</ref> for a more detailed exploration of trace selection techniques.</p></div> <div xmlns=""http://www.tei-c.org/ns/",0
"_6"">? ? ? ? ? ? ??? ? ? ? ? ? ? ? ? ? ??. Furthermore, the left side of expression 3 becomes equal to AE ? ?. Thus AE ? ? AE ?</formula><p>? ? ? ? ?? <ref type=""bibr"" target=""#b4"">(5)</ref> Consider the sum in this expression. The number of terms with ? ? equal to some constant ?, is equal to the nu time sampling <ref type=""bibr"">[22][11]</ref> [5] <ref type=""bibr"" target=""#b7"">[8]</ref> and set sampling <ref type=""bibr"" target=""#b10"">[11]</ref>  <ref type=""bibr"" target=""#b4"">[5]</ref>.</p><p>Hardware monitoring tools collect statistics from hardware and present the information in aggregated fo",1
"e caches to the desired detail, but cannot capture operating system interaction. Source instrumentation have also been explored, for example in MHSIM <ref type=""bibr"" target=""#b9"">[10]</ref>.</p><p>Trace sampling is used to speed up cache hierarchy simulation. It can be applied to all levels of deta",0
"plementation. To cut simulation time, we used the twenty benchmarks from the SPEC CPU2000 suite that are available with large reduced input data sets <ref type=""bibr"" target=""#b11"">[12]</ref>. The length of the reduced traces are between ? ? ?? and ?? ? ?? references.</p><p>The StatCache simulator s",0
"/ref>.</p><p>Hardware monitoring tools collect statistics from hardware and present the information in aggregated form to the user. Examples are DCPI <ref type=""bibr"" target=""#b0"">[1]</ref>, which uses an advanced hardware support to collect detailed information to the programmer, and PAPI <ref type",0
"d StatCache by comparing it to a functional cache simulator. The evaluation is based on trace-driven simulation, using traces generated by the Simics <ref type=""bibr"" target=""#b16"">[17]</ref> full system simulator. It simulates a Sun UltraSPARC II workstation-like computer. We simulate both StatCach trumentation, trap driven cache simulation and measurements on hardware using hardware profiling support.</p><p>Full system simulators include Simics <ref type=""bibr"" target=""#b16"">[17]</ref> and SimOS <ref type=""bibr"" target=""#b14"">[15]</ref>. They allow very detailed cache simulations, but suffer",0
"f type=""bibr"">61</ref>, and the basic blocks in a routine <ref type=""bibr"">[8,</ref><ref type=""bibr"" target=""#b14"">17,</ref><ref type=""bibr"">25,</ref><ref type=""bibr"" target=""#b16"">191</ref> to minimize the number of conflict misses. Reducing the number of conflict misses in the instruction cache, c",1
"erence happens more often than positive interference, and is the main cause of decreased prediction accuracy <ref type=""bibr"" target=""#b25"">[28,</ref><ref type=""bibr"" target=""#b17"">20]</ref>.</p><p>Dealiased branch predictors reduce negative PHT interference by changing the way they store data in th",1
"ent branches end up sharing the same PHT entry. This is called prediction table interference, and is the main cause for decreased prediction accuracy <ref type=""bibr"" target=""#b25"">[28]</ref>.</p><p>Dynamic prediction tables can be organized in a clever way to reduce prediction table interference, l ut negative interference. Negative interference happens more often than positive interference, and is the main cause of decreased prediction accuracy <ref type=""bibr"" target=""#b25"">[28,</ref><ref type=""bibr"" target=""#b17"">20]</ref>.</p><p>Dealiased branch predictors reduce negative PHT interference",1
"ey do not conflict with each other, we can reduce the number of cache misses by almost an order of magnitude <ref type=""bibr"" target=""#b14"">[17,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"">6]</ref>. By aligning basic blocks so that they execute sequentially, we can further increase s ilization of the instruction cache, and use profile data or heuristics to lay out the routines in a program <ref type=""bibr"" target=""#b14"">[ 17,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"">61</ref>, and the basic blocks in a routine <ref type=""bibr"">[8,</ref><ref type=""bibr"" target=""",0
"on the fact that most aliasing in the prediction tables is due to conflict aliasing, not capacity problems. Derived from the skew-associative caches <ref type=""bibr"" target=""#b18"">[21]</ref>, the gskew predictor stores r; branches in three separate tables, which are accessed with three different in",0
"ed to improve static branch prediction accuracy, usually implying code replication <ref type=""bibr"" target=""#b26"">[14,</ref><ref type=""bibr"">27,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"">13,</ref><ref type=""bibr"">161</ref>. These code transformations are beyond the scope of this wo",0
"avoided because each branch only updates the gshare which keeps track of its sub-stream.</p><p>The gskew branch predictor <ref type=""bibr"">[12,</ref><ref type=""bibr"" target=""#b19"">22]</ref> (Figure <ref type=""figure"" target=""#fig_0"">2</ref>.c) is based on the fact that most aliasing in the predicti",0
"ping the routines in a program so that they do not conflict with each other, we can reduce the number of cache misses by almost an order of magnitude <ref type=""bibr"" target=""#b14"">[17,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"">6]</ref>. By aligning basic blocks so that they execut t optimizations usually target a better utilization of the instruction cache, and use profile data or heuristics to lay out the routines in a program <ref type=""bibr"" target=""#b14"">[ 17,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"">61</ref>, and the basic blocks in a routine <ref type target=""#b14"">[ 17,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"">61</ref>, and the basic blocks in a routine <ref type=""bibr"">[8,</ref><ref type=""bibr"" target=""#b14"">17,</ref><ref type=""bibr"">25,</ref><ref type=""bibr"" target=""#b16"">191</ref> to minimize the number of conflict misses.",0
"Lagrange Multipliers (DLM), while pointing out inaccuracies, limitations and pitfalls of the related technique known as negotiated-congestion routing <ref type=""bibr"" target=""#b27"">[28]</ref>. In Copyright (c) 2008 IEEE. Personal use of this material is permitted. However, permission to use this mat arious implementations <ref type=""bibr"" target=""#b5"">[6]</ref>, <ref type=""bibr"" target=""#b12"">[13]</ref>, <ref type=""bibr"" target=""#b14"">[15]</ref>, <ref type=""bibr"" target=""#b27"">[28]</ref>, <ref type=""bibr"" target=""#b31"">[32]</ref>, <ref type=""bibr"" target=""#b32"">[33]</ref> include which nets are er incentive for maze routers to avoid highly congested regions, often at the cost of increased wirelength.</p><p>Negotiated-congestion Routing (NCR) <ref type=""bibr"" target=""#b27"">[28]</ref> was introduced in the mid-1990s for global routing in FPGAs and is used in VPR (the dominant place-and-route get=""#b0"">(1)</ref> is a function of the base cost (b e ), added cost reflecting congestion history (h e ), and penalty for current congestion (p e ) <ref type=""bibr"" target=""#b27"">[28]</ref>. NCR seeks to minimize e c e .</p><p>To begin negotiated-congestion routing, each net is routed using the sm ed in FGR. The ordering of nets during rip-up-and-re-route is the same for each iteration, but can be chosen arbitrarily, according to the authors of <ref type=""bibr"" target=""#b27"">[28]</ref>, because the gradual cost increase in congested areas removes ties that require sophisticated net ordering t ation of NCR, c e is derived as</p><formula xml:id=""formula_7"">c e = b e + h e • p e<label>(8)</label></formula><p>which is different than Equation 1 <ref type=""bibr"" target=""#b27"">[28]</ref>, but also is more intuitive since it preserves the base cost. Therefore FGR uses this Discrete Lagrange Mult",1
"ct delay, which lead to wire density constraints, and (v) considerations of chemical mechanical polishing (CMP) that also lead to density constraints <ref type=""bibr"" target=""#b7"">[8]</ref>.</p><p>The ISPD '07 routing contest challenged the research community by distributing 16 very large routing be",0
"several times slower than BFS. A*search is a minor modification to Dijkstra's algorithm that significantly improves speed during 2-d and 3-d routing <ref type=""bibr"" target=""#b15"">[16]</ref>. In A*-search, a lower bound of the distance to the target is added to node priority in Dijkstra's algorithm cated techniques that may not scale to large routing instances such as multi-commodity flow based techniques <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b15"">[16]</ref>, described below. Essential to the coarsening stage is the proper aggregation of routing resources so that r ther sophisticated techniques for routing have been proposed, such as the use of multi-commodity flows (MCF) <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b15"">[16]</ref> and integer linear programming (ILP) <ref type=""bibr"" target=""#b5"">[6]</ref>. Both of these techniques attem",0
"eas of EDA to be automated in the 1960s, VLSI routing remains an area of active research and development as evidenced by a growing body of literature <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b5"">[6]</ref>, <ref type=""bibr"" target=""#b14"">[15]</ref>, <ref type=""bibr"" target="" s small enough to be solved with sophisticated techniques that may not scale to large routing instances such as multi-commodity flow based techniques <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b15"">[16]</ref>, described below. Essential to the coarsening stage is the proper a orial optimization techniques.</head><p>Other sophisticated techniques for routing have been proposed, such as the use of multi-commodity flows (MCF) <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b15"">[16]</ref> and integer linear programming (ILP) <ref type=""bibr"" target=""#b5"">",0
"sistance may vary by up to 30 times <ref type=""bibr"" target=""#b36"">[37]</ref>, which requires via doubling <ref type=""bibr"" target=""#b22"">[23]</ref>, <ref type=""bibr"" target=""#b24"">[25]</ref> and motivates additional effort to minimize via counts, (iv) signal integrity constraints and the dramatic i f type=""bibr"" target=""#b36"">[37]</ref> and the common practice of post-route via doubling to improve yield <ref type=""bibr"" target=""#b22"">[23]</ref>, <ref type=""bibr"" target=""#b24"">[25]</ref> suggest that via minimization is a key issue in routing at the nanometer scale. Table I illustrates just how",0
"ing negligible invalidations and keeping an exact sharer representation. We leverage recent prior work on efficient highly-associative caches (ZCache <ref type=""bibr"" target=""#b24"">[25]</ref> and Cuckoo Directory <ref type=""bibr"" target=""#b9"">[10]</ref>), which, due to their multiple hash functions , work in 978-1-4673-0826-7/12/$26.00 ?2011 IEEE practice as if replacement candidates were selected randomly, independently of the addresses tracked <ref type=""bibr"" target=""#b24"">[25]</ref>. We exploit this property to design and analyze SCD:</p><p>? First, we recognize that, to be scalable, a dir ng set-associative, and would not work with other array designs, such as skew-associative caches <ref type=""bibr"" target=""#b26"">[27]</ref> or zcaches <ref type=""bibr"" target=""#b24"">[25]</ref>. SPACE <ref type=""bibr"" target=""#b35"">[36]</ref> observes that applications typically exhibit a limited numb nt Highly-Associative Caches</head><p>Recent work has proposed cache designs that provide high associativity with a small number of ways. Both ZCache <ref type=""bibr"" target=""#b24"">[25]</ref> and Cuckoo Directory <ref type=""bibr"" target=""#b9"">[10]</ref> build on skewassociative caches <ref type=""bib are accurate in practice. Prior work leverages these models to show that associativity depends only on the number of replacement candidates, not ways <ref type=""bibr"" target=""#b24"">[25]</ref>, and to implement scalable and efficient cache partitioning <ref type=""bibr"" target=""#b25"">[26]</ref>. In th ncurrent operations to the same address are serialized, and processed in FCFS order, to preserve atomicity and ensure fairness. Second, as in zcaches <ref type=""bibr"" target=""#b24"">[25]</ref>, the array is pipelined, and we allow concurrent non-conflicting lookups and writes, but only allow one repl ce protocol. The CMP is divided in 64 tiles, each having 16 cores, a directory and L3 bank, and a memory controller. Both L2 and L3 are 4-way zcaches <ref type=""bibr"" target=""#b24"">[25]</ref> with 16 and 52 replacement candidates, respectively. Caches and directories use H 3 hash functions, which ar ncy and energy efficiency of a low-way cache on hits, but replacements incur similar energy costs as a set-associative cache of similar associativity <ref type=""bibr"" target=""#b24"">[25]</ref>. In directories, the cost of a replacement is also much smaller since replacements are stopped early.</p></d lacement process have an uniform random distribution over the cache array. We have shown that in practice, this is an accurate assumption for zcaches <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b25"">26]</ref>. We leverage this assumption in the derivations, and verify its accu tively. Caches and directories use H 3 hash functions, which are simple to implement and work well in practice <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b24"">25]</ref>. Tiles are connected with an 8?8 mesh network-on-chip (NoC) with physical express links.</p><p>The system we",1
"type=""bibr"" target=""#b31"">[32]</ref> (barnes, fft, lu, ocean, radix, water), SPECOMP (applu, equake, wupwise), SPECJBB2005 (specjbb), and BioParallel <ref type=""bibr"" target=""#b15"">[16]</ref> (svm). We have selected workloads that scale reasonably well to 1024 cores and exhibit varied behaviors in t",0
"adeoffs in meeting these properties, but no scheme satisfies all of them. Traditional schemes scale poorly with core count: Duplicate-tag directories <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b28"">29]</ref> maintain a copy of all tags in the tracked caches. They incur reasonab ut they have huge associativity requirements (e.g., tracking 1024 16-way caches would require 16384 ways), so they are limited to small-scale systems <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b28"">29]</ref>. In contrast, sparse directories <ref type=""bibr"" target=""#b12"">[13]</ y become an issue, they can be reduced by processing invalidations in a hierarchical fashion, using multicast networks, or cruise-missile invalidates <ref type=""bibr"" target=""#b1"">[2]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.5."">Storage Efficiency</head><p>We define stora",0
"the number of replacement candidates, not ways <ref type=""bibr"" target=""#b24"">[25]</ref>, and to implement scalable and efficient cache partitioning <ref type=""bibr"" target=""#b25"">[26]</ref>. In this paper, we extend these models   to characterize and show how to provision directories implemented w istribution over the cache array. We have shown that in practice, this is an accurate assumption for zcaches <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b25"">26]</ref>. We leverage this assumption in the derivations, and verify its accuracy in Section 6 using simulation. Evict",0
""">Experimental Methodology</head><p>Modeled system: We perform microarchitectural, execution-driven simulation using an x86-64 simulator based on Pin <ref type=""bibr"" target=""#b22"">[23]</ref>, and model a large-scale CMP with 1024 cores, shown in Figure <ref type=""figure"" target=""#fig_5"">6</ref>. Ta",0
"wo. It is critical that analysis tools remain able to handle the variety of models of parallelism that appear in HPC programs. Toward this end, PEBIL <ref type=""bibr"" target=""#b0"">[1]</ref> has recently added support for handling multithreaded x86 64 code in addition to its existing support for code -c.org/ns/1.0""><head>A. Instrumenting for Thread Safety</head><p>PEBIL generates and inserts code into the program which has two principle functions: <ref type=""bibr"" target=""#b0"">(1)</ref> to add functionality to a program, functionality which is usually focused on collecting some type of informati",1
"ful tool for dealing with those aspects of compiler and program development. Models of performance <ref type=""bibr"" target=""#b5"">[6]</ref> and energy <ref type=""bibr"" target=""#b6"">[7]</ref> can also depend on the ability to understand how programs utilize a memory subsystem.</p><p>The remainder of t",0
"s static instrumentation model. PEBIL's threading model is explored and compared to two other popular x86/Linux binary instrumentation platforms -Pin <ref type=""bibr"" target=""#b1"">[2]</ref> and DyninstAPI <ref type=""bibr"" target=""#b2"">[3]</ref>. We then go on to compare PEBIL to Pin experimentally i head><p>Here we identify other notable x86/Linux binary instrumentation projects. There are many other x86/Linux binary instrumentation projects -Pin <ref type=""bibr"" target=""#b1"">[2]</ref>, DyninstAPI <ref type=""bibr"" target=""#b2"">[3]</ref> and Valgrind <ref type=""bibr"" target=""#b7"">[8]</ref> being",0
"6/Linux binary instrumentation projects -Pin <ref type=""bibr"" target=""#b1"">[2]</ref>, DyninstAPI <ref type=""bibr"" target=""#b2"">[3]</ref> and Valgrind <ref type=""bibr"" target=""#b7"">[8]</ref> being the most popular. This section focuses on Pin and DyninstAPI due to space considerations and because Val",0
"ntext of capturing memory address traces under several sampling scenarios for the OpenMP-multithreaded implementations of the NAS Parallel Benchmarks <ref type=""bibr"" target=""#b3"">[4]</ref>. Collecting a memory address trace is a useful way of stressing PEBIL's thread support facilities, but is also riments use aggressively optimized PEBIL and Pin tools to collect memory address traces for the OpenMP implementations of the NAS Parallel Benchmarks <ref type=""bibr"" target=""#b3"">[4]</ref> A list of all benchmarks along with breif descriptions of them is provided in Table <ref type=""table"">I</ref>.",0
", prior research does not address both issues jointly. On one hand, prior non-uniform cache access (NUCA) work <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b10"">11,< /ref>. Dynamic NUCA (D-NUCA) schemes improve on S-NUCA by adaptively placing data close to the requesting core <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b10"">11,< tion, and replication policies to reduce network distance. However, these best-effort techniques often result in hotspots and additional interference <ref type=""bibr"" target=""#b2"">[3]</ref>. On the other hand, prior work has proposed a variety of partitioning techniques <ref type=""bibr"" target=""#b8"" er precluding QoS. Indeed, prior work has shown that D-NUCA often causes significant bank contention and uneven distribution of accesses across banks <ref type=""bibr"" target=""#b2"">[3]</ref>. We also see this effect in Sec. VI -R-NUCA has the highest worst-case degradation of all schemes. Dynamic Spi",1
"partitioning techniques (e.g., way-partitioning) only provide few partitions and often degrade performance, D-NUCA schemes seldom use them. ASP-NUCA <ref type=""bibr"" target=""#b11"">[12]</ref>, ESP-NUCA <ref type=""bibr"" target=""#b30"">[31]</ref>, and Elastic Cooperative Caching <ref type=""bibr"" target that, due to smart placement, approach the low latency of private caches. These schemes often size partitions using hill-climbing (e.g., shadow tags <ref type=""bibr"" target=""#b11"">[12]</ref> or LRU way hit counters <ref type=""bibr"" target=""#b15"">[16]</ref>), which can get stuck in local optima, whe",1
"eve this. Specifically, we use an H 3 hash function (H in Fig. <ref type=""figure"">1</ref>), which is universal and efficient to implement in hardware <ref type=""bibr"" target=""#b6"">[7]</ref>. All STBs implement the same hash function. Second, increasing N, the number of entries in a share descriptor,",0
"ll applications for 20 billion instructions. We use a fixed-work methodology and equalize sample lengths to avoid sample imbalance, similar to FIESTA <ref type=""bibr"" target=""#b16"">[17]</ref>: First, we run each application in isolation, and measure the number of instructions I i that it executes in",0
"crease cache utilization. Unfortunately, prior research does not address both issues jointly. On one hand, prior non-uniform cache access (NUCA) work <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b9"">10,</ CA <ref type=""bibr"" target=""#b20"">[21]</ref>. Dynamic NUCA (D-NUCA) schemes improve on S-NUCA by adaptively placing data close to the requesting core <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b9"">10,</ through a directory-based or snoopy protocol, which is often also leveraged to implement NUCA techniques. For example, Adaptive Selective Replication <ref type=""bibr"" target=""#b1"">[2]</ref> controls replication by probabilistically deciding whether to store a copy of a remotely fetched line in the l UCA schemes that include sharedprivate partitioning, selective replication, and adaptive spilling (DCC <ref type=""bibr"" target=""#b14"">[15]</ref>, ASR <ref type=""bibr"" target=""#b1"">[2]</ref>, and ECC <ref type=""bibr"" target=""#b15"">[16]</ref>), often by significant margins (up to 30%).</p><p>Vantage a",0
"ems: We perform microarchitectural, executiondriven simulation using zsim <ref type=""bibr"" target=""#b37"">[38]</ref>, an x86-64 simulator based on Pin <ref type=""bibr"" target=""#b26"">[27]</ref>, and model tiled CMPs with 16 and 64 cores and a 3-level cache hierarchy, as shown in Fig. <ref type=""figure",0
"""bibr"" target=""#b2"">[3]</ref>. On the other hand, prior work has proposed a variety of partitioning techniques <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b41"">42,</ref><ref type=""bibr"" target=""#b43"" significant changes to cache arrays. Alternatively, virtual memory and page coloring can be used to constrain the pages of a process to specific sets <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b41"">42]</ref>. While software-only, these schemes are incompatible with superpages , that uses this information and sets partition sizes to maximize some metric, such as throughput <ref type=""bibr"" target=""#b33"">[34]</ref>, fairness <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b40"">41]</ref>, or QoS <ref type=""bibr"" target=""#b22"">[23]</ref>.</p><p>Utility-bas",0
"a single memory controller. The 16-core CMP has a total LLC capacity of 16 MB (1 MB/tile), and the 64-core CMP has 32 MB (512 KB/tile). We use McPAT <ref type=""bibr"" target=""#b23"">[24]</ref> to derive the area and energy numbers of chip components (cores, caches, NoC, and memory controller) at 22 n",0
"pe=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b30"">31,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" target=""#b45"" pe=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b30"">31,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" target=""#b45""",0
"e=""bibr"" target=""#b13"">[14]</ref>, and up to 24% (6.3% gmean) over an idealized shared-private D-NUCA organization that uses twice the cache capacity <ref type=""bibr"" target=""#b15"">[16]</ref>. Jigsaw delivers similar benefits on multithreaded application mixes, demonstrating that, given the right ha dom use them. ASP-NUCA <ref type=""bibr"" target=""#b11"">[12]</ref>, ESP-NUCA <ref type=""bibr"" target=""#b30"">[31]</ref>, and Elastic Cooperative Caching <ref type=""bibr"" target=""#b15"">[16]</ref> use way-partitioning to divide cache banks between private and shared levels. However, this division does no caches. These schemes often size partitions using hill-climbing (e.g., shadow tags <ref type=""bibr"" target=""#b11"">[12]</ref> or LRU way hit counters <ref type=""bibr"" target=""#b15"">[16]</ref>), which can get stuck in local optima, whereas Jigsaw captures full miss curves to make global decisions.</p artition the LLC between shared and private regions, but gives the full LLC capacity to both the private (L3) and shared (L4) regions. Herrero et al. <ref type=""bibr"" target=""#b15"">[16]</ref> show that this idealized scheme always outperforms several state-of- the-art private-baseline D-NUCA schemes ng, selective replication, and adaptive spilling (DCC <ref type=""bibr"" target=""#b14"">[15]</ref>, ASR <ref type=""bibr"" target=""#b1"">[2]</ref>, and ECC <ref type=""bibr"" target=""#b15"">[16]</ref>), often by significant margins (up to 30%).</p><p>Vantage and Jigsaw both use 512-line (4 KB) UMONs with 128",0
"ative multithreading architecture.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2."">Background and Related Work</head><p>Previous work <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b37"">38]</ref> describes support mechanisms for migrating register state in order to ore cores.</p><p>The cores of our CMP feature hardware support for thread activation and deactivation, as found in prior studies of thread scheduling <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b37"">38]</ref>. While those works used hardware support to implement scheduling and t",1
"et al., explore the complementary problem of branch prediction for short-lived threads <ref type=""bibr"" target=""#b7"">[8]</ref>.</p><p>Stream buffers <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b23"">24]</ref> introduce small associative structures which track data access patte",0
"hitectures. Focusing on misses, these schemes target the subset of the future working set which is not currently cached. Dependence-following schemes <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b9"">10]</ref> prefetch by following dependence chains through memory. While valuable,",0
"f cache state impedes performance as execution migrates across cores <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b25"">26]</ref>. This is a well-documented and longstanding problem. Execution that would ordinarily reside on a single core ng-set prediction on SpMT architectures.</p><p>We obtained the Multithreading framework to evaluate a number of Hardware Transactional Memory designs <ref type=""bibr"" target=""#b25"">[26]</ref> and modified it to include our working migration techniques. In this framework, loops and function calls are #b20"">[21]</ref>. Memory dependencies are addressed via a modified form of Hardware Transactional Memory, specifically the OFWI design recommended by <ref type=""bibr"" target=""#b25"">[26]</ref>. This memory design is aware of thread ordering, forwards values between threads, and detects conflicts at w ction simulations based on SimPoint <ref type=""bibr"" target=""#b29"">[30]</ref>. We model dual-core execution using architectural parameters similar to <ref type=""bibr"" target=""#b25"">[26]</ref>; these parameters include a shared L2 cache, which decreases the penalty for transferring data between cores",0
"m of branch prediction for short-lived threads <ref type=""bibr"" target=""#b7"">[8]</ref>.</p><p>Stream buffers <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b23"">24]</ref> introduce small associative structures which track data access patterns. Additional work <ref type=""bibr"" tar",0
"del we developed and synthesized for a commercial process with Synopsys tools. We provide all numbers and formulas of our model in a technical report <ref type=""bibr"" target=""#b15"">[16]</ref>.</p><p>One parameter of our model is the sensitivity of design to static power. The parameters in our model or. Many of these workloads have multiple checkpoints at multiple representative regions as provided by PinPoints [4S]. All checkpoints are listed in <ref type=""bibr"" target=""#b15"">[16]</ref>, along with their individual performance and energy consumption on each of the evaluated core models. Baseli points).</p><p>We analyzed HBA and other baselines extensively but can report only some analyses below due to space constraints. Our technical report <ref type=""bibr"" target=""#b15"">[16]</ref> provides additional results, including sensitivity studies, power model details, individual benchmark result",0
"ial) peifor mance. This is important for many algorithms and for any application with serialized code sections <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"">15,</ref><ref type=""bibr"">28,</ref><ref type=""bibr"">29,</ref><ref type=""bibr"">59]</ref>. Second exploit this diversity, past works have proposed core-level heterogeneity. These heterogeneous designs either combine multiple separate cores (e.g., <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"">10,</ref><ref type IW frame does not exist for the code block.</p><p>Coarse-grained Heterogeneous Cores: Several works propose the use of statically heterogeneous cores <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"">10,</ref><ref type",0
"type=""bibr"" target=""#b15"">[16]</ref>, wish branches <ref type=""bibr"" target=""#b14"">[15]</ref>, dynamic predication based on frequently executed paths <ref type=""bibr"" target=""#b13"">[14]</ref>, and predicate prediction <ref type=""bibr"" target=""#b25"">[26]</ref>, to name a few. In this paper, predicati",1
"tency (number of cycles) for modern processors from different vendors. The latency ranges from 13 to 20 cycles <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b6"">7]</ref>. We conservatively use 10 cycle",0
"nditional moves, a commonly available predication primitive in commercial instruction-set architectures (ISA), is generally profitable for this class <ref type=""bibr"" target=""#b1"">[2]</ref>. For completeness, we analyze why the gcc compiler did not if-convert such branches and manually do so at the -execution. We focus on the most closely related work. Various ingenious techniques for predication have been proposed, such as: software predication <ref type=""bibr"" target=""#b1"">[2]</ref>, predication using hyperblocks <ref type=""bibr"" target=""#b21"">[22]</ref>, dynamic hammock predication <ref typ",0
"target=""#b14"">[15]</ref>, dynamic predication based on frequently executed paths <ref type=""bibr"" target=""#b13"">[14]</ref>, and predicate prediction <ref type=""bibr"" target=""#b25"">[26]</ref>, to name a few. In this paper, predication (i.e., ifconversion) is a key enabling mechanism for applying CFD d to exist in heavily if-converted code such as hyperblocks as these large scheduling regions yield more flexibility for code motion. Quinones et al. <ref type=""bibr"" target=""#b25"">[26]</ref> adapted the predicate register file for an OOO processor, and in so doing resorted to moving it into the ren",0
"nchmark suites: SPEC2006 <ref type=""bibr"" target=""#b31"">[32]</ref> (engineering, scientific, and other workstation type benchmarks), NU-MineBench-3.0 <ref type=""bibr"" target=""#b23"">[24]</ref> (data mining), BioBench <ref type=""bibr"" target=""#b0"">[1]</ref> (bioinformatics), and cBench-1.1 <ref type=""",0
"urate than simple static hardware predictors.  </p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>C. Encoding Hints in the PC</head><p>Jim?nez <ref type=""bibr"" target=""#b12"">[14]</ref> proposes to encode branch hints in the program counter. By carefully aligning the code, it is possible to ma bias. The code is aligned by inserting no-ops, which are placed as much as possible on cold code paths. Overall, placement is not exact. We refer to <ref type=""bibr"" target=""#b12"">[14]</ref> for details of the algorithm.</p><p>Figure <ref type=""figure"" target=""#fig_2"">5</ref> demonstrates the error",1
"fying one cannot modify the other. It turns out that commercial instruction sets do not respect the properties of interchangeability and independence <ref type=""bibr"" target=""#b8"">[10]</ref>. To solve this problem, registers are grouped in register classes such that all registers in the same class a",0
"a real practical need for encoding hints in instructions. Many processors already contain mechanisms to convey hints on, e.g., branch directions [2], <ref type=""bibr"" target=""#b1"">[3]</ref>, branch targets <ref type=""bibr"" target=""#b2"">[4]</ref> and memory locality <ref type=""bibr"" target=""#b3"">[5]<",0
"e must do (the semantics of the instruction) the hint bits give directions on how the architecture can best accomplish a task. The recent Itanium ISA <ref type=""bibr"" target=""#b0"">[1]</ref>, contains hint bits to steer branch prediction and hint bits to predict locality of memory reference.</p><p>Hi",0
". Many processors already contain mechanisms to convey hints on, e.g., branch directions [2], <ref type=""bibr"" target=""#b1"">[3]</ref>, branch targets <ref type=""bibr"" target=""#b2"">[4]</ref> and memory locality <ref type=""bibr"" target=""#b3"">[5]</ref>. On the other hand, researchers have indicated tha",0
"efficient and high performing cache hierarchy. One of the key design choices for a multilevel cache hierarchy is whether or not to enforce inclusion <ref type=""bibr"" target=""#b4"">[6,</ref><ref type=""bibr"" target=""#b25"">27,</ref><ref type=""bibr"" target=""#b15"">17]</ref>. While inclusion greatly simpl /ref><ref type=""bibr"" target=""#b15"">17]</ref>. While inclusion greatly simplifies the cache coherence protocol <ref type=""bibr"" target=""#b7"">[9,</ref><ref type=""bibr"" target=""#b4"">6]</ref>, it limits performance when the size of the largest cache is not significantly larger than the sum of the small he inclusion property requires that the contents of all the smaller caches of a multi-level cache hierarchy be a subset of the last-level cache (LLC) <ref type=""bibr"" target=""#b4"">[6]</ref>. When a line is evicted from the LLC, inclusion is enforced by removing that line from all the caches in the h inclusion eliminates the natural snoop filter benefit that an inclusive LLC provides, thus breaking the coherence benefits that come with inclusivity <ref type=""bibr"" target=""#b4"">[6]</ref>. While snoop filters <ref type=""bibr"" target=""#b3"">[5,</ref><ref type=""bibr"" target=""#b20"">22,</ref><ref type=",1
"ng duplicated in the LLC. In doing so, non-inclusion increases the effective capacity of the cache hierarchy <ref type=""bibr"" target=""#b25"">[27,</ref><ref type=""bibr"" target=""#b27"">29]</ref>. Unfortunately, non-inclusion eliminates the natural snoop filter benefit that an inclusive LLC provides, thu che hierarchy come from the increase in the effective caching capacity <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b25"">27,</ref><ref type=""bibr"" target=""#b27"">29]</ref>. However, we show that the first order benefit of non-inclusion is the elimination of inclusion victims and n r"" target=""#b20"">22,</ref><ref type=""bibr"" target=""#b21"">23]</ref> can be used in addition to the LLC, such structures increase the hardware overhead <ref type=""bibr"" target=""#b27"">[29]</ref> and verification complexity <ref type=""bibr"" target=""#b7"">[9]</ref>. It would be ideal to design a cache hie",0
"replicated their results for single-threaded workloads, however, our results indicate that global temporal locality indeed benefits CMPs. Garde et al <ref type=""bibr"" target=""#b11"">[13]</ref> followed up on Zahran's work and deconstructed global replacement for single-core and multi-core processors.",0
"/p><p>It is a widely held belief that the primary benefits of a noninclusive cache hierarchy come from the increase in the effective caching capacity <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b25"">27,</ref><ref type=""bibr"" target=""#b27"">29]</ref>. However, we show that the fir tio and implement inclusive LLCs [4] while AMD microprocessors (like the Phenom II processor) have a 1:4 cache ratio and implement non-inclusive LLCs <ref type=""bibr"" target=""#b0"">[1]</ref>. The figure shows that inclusive LLCs with 1:8 ratio have an average of 3% (max 12%) worse performance than no is especially important since chip designers today are willing to sacrifice coherence benefits for the performance improvement of non-inclusive LLCs <ref type=""bibr"" target=""#b0"">[1]</ref>. Using QBS, we report 10-33% performance improvement for 25 of the 105 workloads on 2, 4 and 8 core systems wi",0
"e from the time of its placement in the cache to the time of its last reference. From the last reference until the block is evicted the block is dead <ref type=""bibr"" target=""#b12"">[13]</ref>. Cache blocks are dead on average 86.2% of the time over a set of memory-intensive benchmarks. Cache efficie ock Predictors</head><p>Previous work introduced several dead block predictors and applied them to problems such as prefetching and block replacement <ref type=""bibr"" target=""#b12"">[13]</ref>, <ref type=""bibr"" target=""#b14"">[15]</ref>, <ref type=""bibr"" target=""#b10"">[11]</ref>, <ref type=""bibr"" targ "" target=""#b4"">[5]</ref>, <ref type=""bibr"" target=""#b0"">[1]</ref>.</p><p>1) Trace Based Predictor: Dead block prediction was introduced by Lai et al. <ref type=""bibr"" target=""#b12"">[13]</ref>. The Lai et al. predictor is used to prefetch data into dead blocks in the L1 data cache. This reference tra imu-late a 2MB LLC for the single-thread workloads. In keeping with the methodology of recent cache papers <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" target=""#b12"">[13]</ref>, <ref type=""bibr"" target=""#b19"">[20]</ref>, <ref type=""bibr"" target=""#b18"">[19]</ref>, <ref type=""bibr"" targ",1
"s problem extends to other recently proposed improvements to caches, including adaptive insertion policies <ref type=""bibr"" target=""#b18"">[19]</ref>, <ref type=""bibr"" target=""#b6"">[7]</ref>.</p><p>? Due to the large number of memory and instruction references tracked by these predictors, they must b rmance than parallel misses <ref type=""bibr"" target=""#b19"">[20]</ref>. Thread Aware Dynamic Insertion Policy (TADIP) uses DIP in a multi-core context <ref type=""bibr"" target=""#b6"">[7]</ref>. It takes into account the memory requirement of each executing program. It has a dedicated leader set for eac get=""#b18"">[19]</ref>, <ref type=""bibr"" target=""#b14"">[15]</ref>, <ref type=""bibr"" target=""#b8"">[9]</ref>, <ref type=""bibr"" target=""#b10"">[11]</ref>, <ref type=""bibr"" target=""#b6"">[7]</ref>, <ref type=""bibr"" target=""#b7"">[8]</ref>, we choose a memory-intensive subset of the benchmarks. We use the fo",0
"sum of these instruction addresses, is used to index a prediction table. A trace based predictor is also used to optimize a cache coherence protocol <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" target=""#b22"">[23]</ref> and perform dynamic self-invalidation <ref type=""bibr"" target=""#b tion and dead block predictor accuracy. The experiments model a 16-way set-associative last-level cache to remain consistent with other previous work <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" target=""#b14"">[15]</ref>, <ref type=""bibr"" target=""#b18"">[19]</ref>, <ref type=""bibr"" targ re simulates one billion instructions. We simu-late a 2MB LLC for the single-thread workloads. In keeping with the methodology of recent cache papers <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" target=""#b12"">[13]</ref>, <ref type=""bibr"" target=""#b19"">[20]</ref>, <ref type=""bibr"" targ",0
"get=""#b12"">[13]</ref>, <ref type=""bibr"" target=""#b14"">[15]</ref>, <ref type=""bibr"" target=""#b10"">[11]</ref>, <ref type=""bibr"" target=""#b4"">[5]</ref>, <ref type=""bibr"" target=""#b0"">[1]</ref>.</p><p>1) Trace Based Predictor: Dead block prediction was introduced by Lai et al. <ref type=""bibr"" target=""# wice that number of cycles. This predictor is used to prefetch into the L1 cache and filter a victim cache. Abella et al. propose a similar predictor <ref type=""bibr"" target=""#b0"">[1]</ref> based on number of references rather than cycles for reducing cache leakage.</p><p>3) Cache Burst Predictor: C",0
"r is used to identify a pool of dead blocks in the L2 cache to be used as a ""virtual victim cache"" into which LRU victims from hot sets can be stored <ref type=""bibr"" target=""#b9"">[10]</ref>.</p><p>2) Time-Based Predictor: Hu et al. propose a time-based dead block predictor <ref type=""bibr"" target=""",0
"the reporting systems do not contain reliable measures of the common covariates. These factors limit the benefit of applying stratification routinely <ref type=""bibr"" target=""#b8"">(8)</ref>.</p><p>Our new method (i) accomplishes the goals of stratification, dampening or removing the effect of covari",1
"tion using a fixed set of covariates reduces power by dividing up the available data across unimportant strata <ref type=""bibr"" target=""#b4"">(4,</ref><ref type=""bibr"" target=""#b15"">15)</ref>. Our approach does not divide data across strata and can correct for the effects of confounders even if those",0
"for improved patient care <ref type=""bibr"" target=""#b21"">(21)</ref><ref type=""bibr"" target=""#b22"">(22)</ref><ref type=""bibr"" target=""#b23"">(23)</ref><ref type=""bibr"" target=""#b24"">(24)</ref><ref type=""bibr"" target=""#b25"">(25)</ref><ref type=""bibr"" target=""#b26"">(26)</ref>. Clinical trials do not ro",0
". org Web site.</p><p>Identification and prediction of DDIs is a critical activity for improved patient care <ref type=""bibr"" target=""#b21"">(21)</ref><ref type=""bibr"" target=""#b22"">(22)</ref><ref type=""bibr"" target=""#b23"">(23)</ref><ref type=""bibr"" target=""#b24"">(24)</ref><ref type=""bibr"" target=""#b",0
"TRODUCTION</head><p>Drug development has stagnated over the past 20 years as new drug approvals lag behind, ballooning research and development costs <ref type=""bibr"" target=""#b0"">(1)</ref>. An increasing proportion of experimental drugs are failing preclinical and clinical trials, with the two larg",1
"data for hundreds of thousands of compound-protein pairs (Table <ref type=""table"" target=""#tab_0"">1</ref>). Genetic knowledge bases, such as PharmGKB <ref type=""bibr"" target=""#b46"">(47)</ref>, provide curated data on the genetic variation of drug response (Table <ref type=""table"" target=""#tab_0"">1</",0
"led to advances in our understanding of some notoriously complex diseases, including autism <ref type=""bibr"" target=""#b15"">(16)</ref>, schizophrenia <ref type=""bibr"" target=""#b16"">(17)</ref>, type 2 diabetes <ref type=""bibr"" target=""#b17"">(18)</ref>, and cardiovascular disease <ref type=""bibr"" targ ain how genetic perturbations of a common molecular network involved in both autism and schizophrenia can have very different functional consequences <ref type=""bibr"" target=""#b16"">(17)</ref>. The shift in focus from the study of individual proteins to the systems level has led to advances in other",0
"argets and explain side effects <ref type=""bibr"" target=""#b7"">(8)</ref><ref type=""bibr"" target=""#b8"">(9)</ref><ref type=""bibr"" target=""#b9"">(10)</ref><ref type=""bibr"" target=""#b10"">(11)</ref>. However, few of these approaches have been vetted in a prospective clinical trial setting and therefore are",0
"ped to use network analysis and chemical systems biology data to identify off-targets and explain side effects <ref type=""bibr"" target=""#b7"">(8)</ref><ref type=""bibr"" target=""#b8"">(9)</ref><ref type=""bibr"" target=""#b9"">(10)</ref><ref type=""bibr"" target=""#b10"">(11)</ref>. However, few of these approa",0
"target.</p><p>While other stable methods exist for training neural networks in the reinforcement learning setting, such as neural fitted Q-iteration <ref type=""bibr"" target=""#b18"">24</ref> , these methods involve the repeated training of networks de novo on hundreds of iterations. Consequently, the me algorithm, network architecture and hyperparameters on each game, privy only to the inputs a human player would have. In contrast to previous work <ref type=""bibr"" target=""#b18"">24,</ref><ref type=""bibr"" target=""#b20"">26</ref> , our approach incorporates 'end-to-end' reinforcement learning that u ion pairs to scalar estimates of their Q-value, the history and the action have been used as inputs to the neural network by some previous approaches <ref type=""bibr"" target=""#b18"">24,</ref><ref type=""bibr"" target=""#b20"">26</ref> . The main drawback of this type of architecture is that a separate fo",1
"yperparameters on each game, privy only to the inputs a human player would have. In contrast to previous work <ref type=""bibr"" target=""#b18"">24,</ref><ref type=""bibr"" target=""#b20"">26</ref> , our approach incorporates 'end-to-end' reinforcement learning that uses reward to continuously shape represe -value, the history and the action have been used as inputs to the neural network by some previous approaches <ref type=""bibr"" target=""#b18"">24,</ref><ref type=""bibr"" target=""#b20"">26</ref> . The main drawback of this type of architecture is that a separate forward pass is required to compute the Q-",1
"ith fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks <ref type=""bibr"" target=""#b3"">[9]</ref><ref type=""bibr"" target=""#b4"">[10]</ref><ref type=""bibr"" target=""#b5"">[11]</ref> to develop a novel artificial agent, termed a deep Q-network, that ca =""bibr"" target=""#b10"">16</ref> known as deep neural networks. Notably, recent advances in deep neural networks <ref type=""bibr"" target=""#b3"">[9]</ref><ref type=""bibr"" target=""#b4"">[10]</ref><ref type=""bibr"" target=""#b5"">[11]</ref> , in which several layers of nodes are used to build up progressively",0
"rn concepts such as object categories directly from raw sensory data. We use one particularly successful architecture, the deep convolutional network <ref type=""bibr"" target=""#b11"">17</ref> , which uses hierarchical layers of tiled convolutional filters to mimic the effects of receptive fields-inspi",0
"nd temporal difference reinforcement learning algorithms 3 . While reinforcement learning agents have achieved some successes in a variety of domains <ref type=""bibr"" target=""#b0"">[6]</ref><ref type=""bibr"" target=""#b1"">[7]</ref><ref type=""bibr"" target=""#b2"">[8]</ref> , their applicability has previo",0
"earning, which uses two key ideas. First, we used a biologically inspired mechanism termed experience replay <ref type=""bibr"" target=""#b15"">[21]</ref><ref type=""bibr"" target=""#b16"">[22]</ref><ref type=""bibr"" target=""#b17"">[23]</ref> that randomizes over the data, thereby removing correlations in the learning with deep network architectures was critically dependent on our incorporation of a replay algorithm <ref type=""bibr"" target=""#b15"">[21]</ref><ref type=""bibr"" target=""#b16"">[22]</ref><ref type=""bibr"" target=""#b17"">[23]</ref> involving the storage and representation of recently experienced tr alian brain, with the timecompressed reactivation of recently experienced trajectories during offline periods <ref type=""bibr"" target=""#b15"">21,</ref><ref type=""bibr"" target=""#b16"">22</ref> (for example, waking rest) providing a putative mechanism by which value functions may be efficiently updated example, waking rest) providing a putative mechanism by which value functions may be efficiently updated through interactions with the basal ganglia <ref type=""bibr"" target=""#b16"">22</ref> . In the future, it will be important to explore the potential use of biasing the content of experience replay",0
"instabilities with a novel variant of Q-learning, which uses two key ideas. First, we used a biologically inspired mechanism termed experience replay <ref type=""bibr"" target=""#b15"">[21]</ref><ref type=""bibr"" target=""#b16"">[22]</ref><ref type=""bibr"" target=""#b17"">[23]</ref> that randomizes over the d successful integration of reinforcement learning with deep network architectures was critically dependent on our incorporation of a replay algorithm <ref type=""bibr"" target=""#b15"">[21]</ref><ref type=""bibr"" target=""#b16"">[22]</ref><ref type=""bibr"" target=""#b17"">[23]</ref> involving the storage and ealization of such a process in the mammalian brain, with the timecompressed reactivation of recently experienced trajectories during offline periods <ref type=""bibr"" target=""#b15"">21,</ref><ref type=""bibr"" target=""#b16"">22</ref> (for example, waking rest) providing a putative mechanism by which val",0
"e spaces. Here we use recent advances in training deep neural networks <ref type=""bibr"" target=""#b3"">[9]</ref><ref type=""bibr"" target=""#b4"">[10]</ref><ref type=""bibr"" target=""#b5"">[11]</ref> to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly fro deep neural networks. Notably, recent advances in deep neural networks <ref type=""bibr"" target=""#b3"">[9]</ref><ref type=""bibr"" target=""#b4"">[10]</ref><ref type=""bibr"" target=""#b5"">[11]</ref> , in which several layers of nodes are used to build up progressively more abstract representations of the da",0
"utional filters to mimic the effects of receptive fields-inspired by Hubel and Wiesel's seminal work on feedforward processing in early visual cortex <ref type=""bibr"" target=""#b12"">18</ref> -thereby exploiting the local spatial correlations present in images, and building in robustness to natural tr",0
"goal of general artificial intelligence <ref type=""bibr"" target=""#b7"">13</ref> that has eluded previous efforts <ref type=""bibr"" target=""#b2"">8,</ref><ref type=""bibr"" target=""#b8"">14,</ref><ref type=""bibr"" target=""#b9"">15</ref> . To achieve this, we developed a novel agent, a deep Q-network (DQN), w",0
"le, image classification <ref type=""bibr"" target=""#b17"">17</ref> , face recognition <ref type=""bibr"" target=""#b18"">18</ref> , and playing Atari games <ref type=""bibr"" target=""#b19"">19</ref> . They use many layers of neurons, each arranged in overlapping tiles, to construct increasingly abstract, loc",1
"predicting expert moves in the game of Go using supervised learning <ref type=""bibr"" target=""#b13"">13,</ref><ref type=""bibr"" target=""#b21"">[21]</ref><ref type=""bibr"" target=""#b22"">[22]</ref><ref type=""bibr"" target=""#b23"">[23]</ref><ref type=""bibr"" target=""#b24"">[24]</ref> . The SL policy network p",0
"f type=""bibr"" target=""#b9"">9</ref> , and weak amateur level play in Go <ref type=""bibr"" target=""#b10"">10</ref> .</p><p>Monte Carlo tree search (MCTS) <ref type=""bibr"" target=""#b11"">11,</ref><ref type=""bibr"" target=""#b12"">12</ref> uses Monte Carlo rollouts to estimate the value of each state in a sea erformance in small-board Go 28,29,47 using convolutional networks.</p><p>An alternative approach to minimax search is Monte Carlo tree search (MCTS) <ref type=""bibr"" target=""#b11"">11,</ref><ref type=""bibr"" target=""#b12"">12</ref> , which estimates the optimal value of interior nodes by a double appr",0
"work has focused on a linear combination v θ (s) = ϕ(s) • θ of features ϕ(s) with weights θ. Weights were trained using temporal-difference learning <ref type=""bibr"" target=""#b41"">41</ref> in chess <ref type=""bibr"" target=""#b42"">42,</ref><ref type=""bibr"" target=""#b43"">43</ref> , checkers <ref type=",0
"on an Elo scale 37 : a 230 point gap corresponds to a 79% probability of winning, which roughly corresponds to one amateur dan rank advantage on KGS <ref type=""bibr"" target=""#b38"">38</ref> ; an approximate correspondence to human ranks is also shown, horizontal lines show KGS ranks achieved online d on an Elo scale 37 : a 230 point gap corresponds to a 79% probability of winning, which roughly corresponds to one amateur dan rank advantage on KGS<ref type=""bibr"" target=""#b38"">38</ref> ; an approximate correspondence to human ranks is also shown,</figDesc></figure> <figure xmlns=""http://www.tei",0
"ological features was introduced in <ref type=""bibr"" target=""#b34"">[35]</ref>, which was then used to develop a multimodal deception detection system <ref type=""bibr"" target=""#b1"">[2]</ref>. An extensive review of approaches for evaluating human credibility using physiological, visual, acoustic, and s into the semantic categories of words that represent useful clues for deception, their performance is often similar to that of the n-grams features <ref type=""bibr"" target=""#b1"">[2]</ref>. Since in our current work we are not focusing on the insights that can be gained from linguistic analyses, we",1
"found to correlate with deceptive behavior <ref type=""bibr"" target=""#b8"">[9]</ref>. The gesture annotation is performed using the MUMIN coding scheme <ref type=""bibr"" target=""#b2"">[3]</ref>, which is a standard multimodal annotation scheme for interpersonal interactions.</p><p>In the MUMIN scheme, f",1
"fe trial data using text and gestures modalities. While there is research work that has used court trial transcripts to identify deceptive statements <ref type=""bibr"" target=""#b13"">[14]</ref>, we are not aware of any previous work that took into consideration modalities other than text for deception ta. To our knowledge, there is very little work focusing on real-life high-stake data. The work closest to ours is presented by Fornaciari and Poesio <ref type=""bibr"" target=""#b13"">[14]</ref>, which targets the identification of deception in statements issued by witnesses and defendants using a corp",1
"heart rate, respiration rate, and skin temperature. Several studies <ref type=""bibr"" target=""#b40"">[41,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b9"">10]</ref> indicated that relying solely on such physiological measurements can be biased and misleading. Chittaranjan et",0
"e explored the identification of deceptive content in a variety of domains, including online dating websites <ref type=""bibr"" target=""#b38"">[39,</ref><ref type=""bibr"" target=""#b17"">18]</ref>, forums <ref type=""bibr"" target=""#b41"">[42,</ref><ref type=""bibr"" target=""#b22"">23]</ref>, social networks <r",0
"cations such as deception detection and social behavior.</p><p>Facial expressions also play a critical role in the identification of deception. Ekman <ref type=""bibr"" target=""#b10"">[11]</ref> defined micro-expressions as relatively short involuntary expressions, which can be indicative of deceptive",0
"ainly on data collected from deceivers and truth-tellers. The data is usually elicited from human contributors, in a lab setting or via crowdsourcing <ref type=""bibr"" target=""#b27"">[28,</ref><ref type=""bibr"" target=""#b32"">33]</ref>, for instance by asking subjects to narrate stories in deceptive and inguistic Inquiry and Word Count (LIWC) lexicon <ref type=""bibr"" target=""#b33"">[34]</ref> to build deception models using machine learning approaches <ref type=""bibr"" target=""#b27"">[28,</ref><ref type=""bibr"" target=""#b3"">4]</ref> and showed that the use of psycholinguistic information was helpful fo target=""#b27"">[28,</ref><ref type=""bibr"" target=""#b32"">33]</ref>, for instance by asking subjects to narrate stories in deceptive and truthful manner <ref type=""bibr"" target=""#b27"">[28]</ref>, by performing one-on-one interviews, or by participating in ""Mock crime"" scenarios <ref type=""bibr"" target= ness of features derived from text analysis, which frequently includes basic linguistic representations such as n-grams and sentence count statistics <ref type=""bibr"" target=""#b27"">[28]</ref>, and also more complex linguistic features derived from syntactic CFG trees and part of speech tags <ref typ",0
"eased rhythmic pulsing gestures were associated with truthful behavior. Also related is the taxonomy of hand gestures developed by Maricchiolo et al. <ref type=""bibr"" target=""#b25"">[26]</ref> for applications such as deception detection and social behavior.</p><p>Facial expressions also play a criti",0
"from suspects, witnesses, and innocents.</p><p>For hand gestures, blob analysis was used to detect deceit by tracking the hand movements of subjects <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b39"">40]</ref>, or using geometric features related to the hand and head motion <re",0
". Cohen et al. <ref type=""bibr"" target=""#b7"">[8]</ref> found that fewer iconic hand gestures were a sign of a deceptive narration, and Hillman et al. <ref type=""bibr"" target=""#b18"">[19]</ref> determined that increased speech prompting gestures were associated with deception while increased rhythmic",0
"to 4.29 between 2000 and 2013. The National Registry of Exonerations reported on 873 exonerations from 1989 to 2012, with a tragedy behind each case <ref type=""bibr"" target=""#b16"">[17]</ref>. Hence, the need arises for a reliable and efficient system to detect deceptive behavior and discriminate be",0
"ect lies by using thermal variations in the periorbital area and by deducing the respiration rate from the thermal nostril areas. Granhag and Hartwig <ref type=""bibr"" target=""#b15"">[16]</ref> proposed a methodology using psychologically informed mind-reading to evaluate statements from suspects, wit",0
"o predict the subjects' decisions in the game. In order to improve lie detection in criminalsuspect interrogations, Sumriddetchkajorn and Somboonkaew <ref type=""bibr"" target=""#b36"">[37]</ref> developed an infrared system to detect lies by using thermal variations in the periorbital area and by deduc",0
"arget=""#b5"">[6]</ref> identified several hand gestures that can be related to the act of deception using data from simulated interviews. Cohen et al. <ref type=""bibr"" target=""#b7"">[8]</ref> found that fewer iconic hand gestures were a sign of a deceptive narration, and Hillman et al. <ref type=""bibr",0
"deed a difficult task for humans and further verifies previous findings where human ability to spot liars was found to be slightly better than chance <ref type=""bibr"" target=""#b0"">[1]</ref>. Moreover, the performance of the human annotators appears to be significantly below that of our system.</p></",0
"the cost is a 29% performance degradation relative to a conventional design due to an increase in the access time on a filter cache miss. The L-Cache <ref type=""bibr"" target=""#b5"">[6]</ref> similarly reduces switching activity by holding loop-nested basic blocks designated by the compiler and provid",1
"pacing circuit design attempts to keep power dissipation to reasonable levels. Power projections for the next implementation of the Alpha line (21364 <ref type=""bibr"" target=""#b4"">[5]</ref>) indicate that this trend is expected to continue. Similarly, while the UltraSparc I microprocessor <ref type=",0
"raction of the functionality in a modern microprocessor, and are often partitioned for performance reasons.</p><p>Like complexity-adaptive processors <ref type=""bibr"" target=""#b0"">[1]</ref>, the approach described in this paper exploits the fact that applications may drastically differ in their hard",0
"m 30W to 60W, despite the fact that the energy per transition (calculated in the last column by dividing the power dissipation by the clock frequency <ref type=""bibr"" target=""#b8"">[9]</ref>) has declined during this period. Thus, rapid increases in both clock frequency and chip functionality (and th",0
"has demonstrated the power of networkbased approaches in drug discovery <ref type=""bibr"" target=""#b0"">[1]</ref><ref type=""bibr"" target=""#b1"">[2]</ref><ref type=""bibr"" target=""#b2"">[3]</ref>. We have shown previously that a large semantic network of drugtarget interactions provides a powerful framewo t preference <ref type=""bibr"" target=""#b4"">[5]</ref>.</p><p>In this work, we apply a random walk-based link prediction algorithm based on Chen et al. <ref type=""bibr"" target=""#b2"">[3]</ref> to a more extensive drug-target network and evaluated its performance using an external dataset. We combine th o target transition matrix and W TD is target to drug transition matrix. The calculation of each of the transition matrix in discussed in Chen et al. <ref type=""bibr"" target=""#b2"">[3]</ref>. The random walk is implemented on the heterogeneous network using the Eq. ( <ref type=""formula"">5</ref>) give target=""#b13"">[15]</ref><ref type=""bibr"" target=""#b14"">[16]</ref>. Because of these evidences, we here simply adopt the previously used value of 0.3 <ref type=""bibr"" target=""#b2"">[3]</ref>. Second, the robustness of ? (jumping probability) has already been discussed <ref type=""bibr"" target=""#b13"">[ [16]</ref><ref type=""bibr"" target=""#b15"">[17]</ref>. It has been shown that the weight parameters w d and w t are robust among the prediction results <ref type=""bibr"" target=""#b2"">[3]</ref>.</p><p>In our drug target network 684 (94%) drugs have at least one target. We prepare a test network of 684 d fer a general approach that takes the whole drug target network into account without separating protein categories, in contrast to the previous study <ref type=""bibr"" target=""#b2"">[3]</ref>. The following estimation corroborates our approach. Our drug-target dataset contains 727 drugs and 3,519 prot /K a , where S ab is the adjacency matrix of the network and (S ab equals 1 if node a and b are connected, 0 otherwise) K a denotes the degree of a. <ref type=""bibr"" target=""#b2"">(3)</ref> With the probability c, the walker goes back to a. (4) After many time steps the probability of finding the ra nd<ref type=""table"" target=""#tab_4"">4</ref>. This is the first time that the random walk-based method is evaluated using a binding assay dataset (cf. <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b4"">5]</ref>).</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>Case study: p",1
"obe, cation, anion, and ring) [http://docs.eyesopen.com/rocs/shape_theory.html] defined by SMARTS. Conformers for the data set is created using OMEGA <ref type=""bibr"" target=""#b11"">[13]</ref>, about 250 conformers with RMSD threshold of 0.6 is generated. ROCS performs shape-based overlay of conforme",0
"div xmlns=""http://www.tei-c.org/ns/1.0""><head>Background</head><p>Recent work has demonstrated the power of networkbased approaches in drug discovery <ref type=""bibr"" target=""#b0"">[1]</ref><ref type=""bibr"" target=""#b1"">[2]</ref><ref type=""bibr"" target=""#b2"">[3]</ref>. We have shown previously that a is the set of nodes and E is the set of links. For each pair of nodes a, b ? V we can assign a proximity score by executing the following procedure: <ref type=""bibr"" target=""#b0"">(1)</ref> we start a random walker from a. <ref type=""bibr"" target=""#b1"">(2)</ref> At each time step, with the probabili",0
"ts for each drug.</p><p>We took some random drugs and tried to find known binding associations to protein targets. We searched three databases ChEMBL <ref type=""bibr"" target=""#b16"">[19]</ref>, PDSP <ref type=""bibr"" target=""#b17"">[20]</ref>, and Pubchem <ref type=""bibr"" target=""#b18"">[21]</ref> using",0
"[10] and ROCS program which uses Tanimoto combo similarity-which combines shape and color measures of a compound, we calculate them with ROCS program <ref type=""bibr"" target=""#b9"">[11]</ref>.</p><p>ECFP (extended connectivity fingerprint) encodes information on atom-centered fragments that is derive",0
"ension, but recursive convolutions resulted in worse performances than a single convolution due to overfitting. To overcome overfitting, Liang and Hu <ref type=""bibr"" target=""#b16"">[17]</ref> uses a recurrent layer that takes feed-forward inputs into all unfolded layers. They show that performance i g a deeply-recursive network very difficult. This is in accordance with the limited success of previous methods using at most three recursions so far <ref type=""bibr"" target=""#b16"">[17]</ref>. Among many reasons, two severe problems are vanishing and exploding gradients <ref type=""bibr"" target=""#b0""",1
"olutions are applied only two times. Similar dimension reduction occurs in the recurrent convolutional neural networks used for semantic segmentation <ref type=""bibr"" target=""#b21"">[22]</ref>. As SR methods predict full-sized images, dimension reduction is not allowed.</p><formula xml:id=""formula_0""",1
"ient descent based on backpropagation (LeCun et al. <ref type=""bibr"" target=""#b14"">[15]</ref>). We implement our model using the MatConvNet 1 package <ref type=""bibr"" target=""#b29"">[30]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4."">Experimental Results</head><p>In this secti",0
"st interpolations but yield poor results. Some of the more powerful methods utilize statistical image priors <ref type=""bibr"" target=""#b26"">[27,</ref><ref type=""bibr"" target=""#b11"">12]</ref> or internal patch recurrence <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b9"">10]</ref>.",0
"nd that DRCN optimized with the widely-used stochastic gradient descent method does not easily converge. This is due to exploding/vanishing gradients <ref type=""bibr"" target=""#b0"">[1]</ref>. Learning long-range dependencies between pixels with a single weight layer is very difficult.</p><p>We propos most three recursions so far <ref type=""bibr"" target=""#b16"">[17]</ref>. Among many reasons, two severe problems are vanishing and exploding gradients <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b20"">21]</ref>.</p><p>Exploding gradients refer to the large increase in the norm of",0
"bor embedding <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b18"">19]</ref>, sparse coding <ref type=""bibr"" target=""#b30"">[31,</ref><ref type=""bibr"" target=""#b31"">32,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b28"">29]</ref>, convolutional neural network r"" target=""#b30"">[31]</ref> for all experiments. For testing, we use four datasets. Datasets Set5 <ref type=""bibr"" target=""#b18"">[19]</ref> and Set14 <ref type=""bibr"" target=""#b31"">[32]</ref> are often used for benchmark <ref type=""bibr"" target=""#b28"">[29,</ref><ref type=""bibr"" target=""#b27"">28,</re",0
"type=""bibr"" target=""#b2"">[3]</ref> from input to the reconstruction net. Adding layer skips is successfully used for a semantic segmentation network <ref type=""bibr"" target=""#b17"">[18]</ref> and we employ a similar idea. Now input image is directly fed into the reconstruction net whenever it is use",0
"learned during training.</p><p>A similar but a different concept of supervising intermediate layers for a convolutional network is used in Lee et al <ref type=""bibr"" target=""#b15"">[16]</ref>. Their method simultaneously minimizes classification error while improving the directness and transparency he hidden layer learning process. There are two significant differences between our recursive-supervision and deep-supervision proposed in Lee et al. <ref type=""bibr"" target=""#b15"">[16]</ref>. They associate a unique classifier for each hidden layer. For each additional layer, a new classifier has t more layers. In addition, using different reconstruction nets no longer effectively regularizes the network. The second difference is that Lee et al. <ref type=""bibr"" target=""#b15"">[16]</ref> discards all intermediate classifiers during testing. However, an ensemble of all intermediate predictions s",0
"s do not increase while more recursions are performed. Our network has the receptive field of 41 by 41 and this is relatively large compared to SRCNN <ref type=""bibr"" target=""#b4"">[5]</ref>  <ref type=""bibr"">(13 by 13)</ref>. While DRCN has good properties, we find that DRCN optimized with the widel pe=""bibr"" target=""#b31"">32,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b28"">29]</ref>, convolutional neural network (CNN) <ref type=""bibr"" target=""#b4"">[5]</ref> and random forest <ref type=""bibr"" target=""#b22"">[23]</ref>.</p><p>Among several recent learning-based success nd random forest <ref type=""bibr"" target=""#b22"">[23]</ref>.</p><p>Among several recent learning-based successes, convolutional neural network (SRCNN) <ref type=""bibr"" target=""#b4"">[5]</ref> demonstrated the feasibility of an end-to-end approach to SR. One possibility to improve SRCNN is to simply st tical Formulation</head><p>The network takes an interpolated input image (to the desired size) as input x and predicts the target image y as in SRCNN <ref type=""bibr"" target=""#b4"">[5]</ref>. Our goal is to learn a model f that predicts values ŷ = f (x), where ŷ is its estimate of ground truth output te the performance of our method on several datasets. We first describe datasets used Ground Truth A+ <ref type=""bibr"" target=""#b28"">[29]</ref> SRCNN <ref type=""bibr"" target=""#b4"">[5]</ref> RFL <ref type=""bibr"" target=""#b22"">[23]</ref> SelfEx   for training and testing our method. Next, our training ead><p>We provide quantitative and qualitative comparisons. For benchmark, we use public code for A+ <ref type=""bibr"" target=""#b28"">[29]</ref>, SRCNN <ref type=""bibr"" target=""#b4"">[5]</ref>, RFL <ref type=""bibr"" target=""#b22"">[23]</ref> and SelfEx <ref type=""bibr"" target=""#b9"">[10]</ref>. We deal wi ef type=""bibr"" target=""#b31"">[32]</ref> are often used for benchmark <ref type=""bibr"" target=""#b28"">[29,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b4"">5]</ref>. Dataset B100 consists of natural images in the Berkeley Segmentation Dataset <ref type=""bibr"" target=""#b19"">[2",0
"olutional networks (DCN) succeeding in various computer vision tasks often use very large receptive fields (224x224 common in ImageNet classification <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b23"">24]</ref>). Among many approaches to widen the receptive field, increasing net",0
"rization is carried out via weight decay and dropout <ref type=""bibr"" target=""#b16"">[17]</ref>.</p><p>When the graph structure of the input is known, <ref type=""bibr"" target=""#b1"">[2]</ref> introduced a model to generalize ConvNets using low learning complexity similar to that of a ConvNet, and whic connected networks trained with dropout using fewer parameters. Our main contributions can be summarized as follows:</p><p>• We extend the ideas from <ref type=""bibr"" target=""#b1"">[2]</ref> to large-scale classification problems, specifically Imagenet Object Recognition, text categorization and bioi asure of similarity that is obtained in an unsupervised fashion. However, it does not attempt to exploit any weight-sharing strategy.</p><p>Recently, <ref type=""bibr"" target=""#b1"">[2]</ref> proposed a generalization of convolutions to graphs via the Graph Laplacian. By identifying a linear, translat Generalizing Convolutions to Graphs</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.1"">Spectral Networks</head><p>Our work builds upon <ref type=""bibr"" target=""#b1"">[2]</ref> which introduced spectral networks. We recall the definition here and its main properties. A spectral network o the spatial decay, since</p><formula xml:id=""formula_4"">∂ k x(ξ) ∂ξ k ≤ C |u| k |x(u)|du ,</formula><p>where x(ξ) is the Fourier transform of x. In <ref type=""bibr"" target=""#b1"">[2]</ref> it was suggested to use the same principle in a general graph, by considering a smoothing kernel K ∈ R N ×N0 , ://www.tei-c.org/ns/1.0""><head n=""4"">Graph Construction</head><p>Whereas some recognition tasks in non-Euclidean domains, such as those considered in <ref type=""bibr"" target=""#b1"">[2]</ref> or <ref type=""bibr"" target=""#b11"">[12]</ref>, might have a prior knowledge of the graph structure of the input",1
"s O(N 2 ) operations. This is a major difference with respect to traditional Con-vNets, which require only O(N ). Fourier implementations of Convnets <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b18"">19]</ref> bring the complexity to O(N log N ) thanks again to the specific sym",0
"product.</p><p>Here, the unitary matrix U plays the role of the Fourier Transform in R d . There are several ways of computing the graph Laplacian L <ref type=""bibr"" target=""#b0"">[1]</ref>. In this paper, we choose the normalized version L =</p><formula xml:id=""formula_1"">I −D −1/2 W D −1/2</formul elation of squares of previously whitened features), or the mutual information.</p><p>This distance is then used to build a Gaussian diffusion Kernel <ref type=""bibr"" target=""#b0"">[1]</ref> </p><formula xml:id=""formula_7"">ω(i, j) = exp − d(i,j) σ 2 .<label>(1)</label></formula><p>In our experiments, stance</p><formula xml:id=""formula_10"">d sup (i, j) = W 1,i − W 1,j 2 ,<label>(2)</label></formula><p>that is then fed into the Gaussian kernel as in <ref type=""bibr"" target=""#b0"">(1)</ref>. The interpretation is that the supervised criterion will extract through W 1 a collection of linear measureme",0
"ing the graph by P graph .</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""5.1"">Reuters</head><p>We used the Reuters dataset described in <ref type=""bibr"" target=""#b17"">[18]</ref>, which consists of training and test sets each containing 201,369 documents from 50 mutually exclusive class es. Each document is represented as a log-normalized bag of words for 2000 common non-stop words. As a baseline we used the fullyconnected network of <ref type=""bibr"" target=""#b17"">[18]</ref> with two hidden layers consisting of 2000 and 1000 hidden units regularized with dropout.</p><p>We chose hyp",0
"es are reduced to learning with fully-connected layers, which have O(N 2 ) parameters, and regularization is carried out via weight decay and dropout <ref type=""bibr"" target=""#b16"">[17]</ref>.</p><p>When the graph structure of the input is known, <ref type=""bibr"" target=""#b1"">[2]</ref> introduced a",0
"ted very good performance in classification tasks, they have recently been shown to be particularly unstable to adversarial perturbations of the data <ref type=""bibr"" target=""#b17"">[18]</ref>. In fact, very small and often imperceptible perturbations of the data samples are sufficient to fool state- les. Though adversarial attacks are specific to the classifier, it seems that the adversarial perturbations are generalizable across different models <ref type=""bibr"" target=""#b17"">[18]</ref>. This can actually become a real concern from a security point of view.</p><p>An accurate method for finding f its influence factors.</p><p>We now review some of the relevant work. The phenomenon of adversarial instability was first introduced and studied in <ref type=""bibr"" target=""#b17"">[18]</ref>. The authors estimated adversarial examples by solving penalized optimization problems and presented an anal h complexity of neural networks might be a reason explaining the presence of adversarial examples. Unfortunately, the optimization method employed in <ref type=""bibr"" target=""#b17"">[18]</ref> is time-consuming and therefore does not scale to large datasets. In <ref type=""bibr"" target=""#b13"">[14]</re b4"">[5]</ref> introduced a smoothness penalty in the training procedure that allows to boost the robustness of the classifier. Notably, the method in <ref type=""bibr"" target=""#b17"">[18]</ref> was applied in order to generate adversarial perturbations. We should finally mention that the phenomenon of ype=""foot"" target=""#foot_3"">4</ref> . We compare the proposed DeepFool approach to stateof-the-art techniques to compute adversarial perturbations in <ref type=""bibr"" target=""#b17"">[18]</ref> and <ref type=""bibr"" target=""#b3"">[4]</ref>. The method in <ref type=""bibr"" target=""#b17"">[18]</ref> solves chniques to compute adversarial perturbations in <ref type=""bibr"" target=""#b17"">[18]</ref> and <ref type=""bibr"" target=""#b3"">[4]</ref>. The method in <ref type=""bibr"" target=""#b17"">[18]</ref> solves a series of penalized optimization problems to find the minimal perturbation, whereas <ref type=""bibr the fast gradient method. It should be noted moreover that the proposed approach also yields slightly smaller perturbation vectors than the method in <ref type=""bibr"" target=""#b17"">[18]</ref>. The proposed approach is hence more accurate in detecting directions that can potentially fool neural netwo tei-c.org/ns/1.0""><head>Classifier</head><p>the complexity aspect, the proposed approach is substantially faster than the standard method proposed in <ref type=""bibr"" target=""#b17"">[18]</ref>. In fact, while the approach <ref type=""bibr"" target=""#b17"">[18]</ref> involves a costly minimization of a s proposed approach is substantially faster than the standard method proposed in <ref type=""bibr"" target=""#b17"">[18]</ref>. In fact, while the approach <ref type=""bibr"" target=""#b17"">[18]</ref> involves a costly minimization of a series of objective functions, we observed empirically that DeepFool con",1
"ion performance in many research areas such as bioinformatics <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b15"">16]</ref>, speech <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b5"">6]</ref>, and computer vision <ref type=""bibr"" target=""#b9"">[10,</ref><ref type",0
"ral networks are powerful learning models that achieve state-of-the-art pattern recognition performance in many research areas such as bioinformatics <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b15"">16]</ref>, speech <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" targ al perturbation as the minimal perturbation r that is sufficient to change the estimated label k(x): ∆(x; k) := min r r 2 subject to k(x + r) = k(x), <ref type=""bibr"" target=""#b0"">(1)</ref> where x is an image and k(x) is the estimated label. We call ∆(x; k) the robustness of k at point x. The robus",0
") architecture <ref type=""bibr"" target=""#b10"">[11]</ref>.</p><p>• ILSVRC 2012: We used CaffeNet <ref type=""bibr"" target=""#b6"">[7]</ref> and GoogLeNet <ref type=""bibr"" target=""#b16"">[17]</ref> pre-trained models.</p><p>In order to evaluate the robustness to adversarial perturbations of a classifier f",0
"ls that achieve state-of-the-art pattern recognition performance in many research areas such as bioinformatics <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b15"">16]</ref>, speech <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b5"">6]</ref>, and computer vision",0
"problems.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2."">Related Work</head><p>Residual Representations. In image recognition, VLAD <ref type=""bibr"" target=""#b17"">[18]</ref> is a representation that encodes by the residual vectors with respect to a dictionary, and Fisher Vector <re sidual vectors with respect to a dictionary, and Fisher Vector <ref type=""bibr"" target=""#b29"">[30]</ref> can be formulated as a probabilistic version <ref type=""bibr"" target=""#b17"">[18]</ref> of VLAD. Both of them are powerful shallow representations for image retrieval and classification <ref type=",1
"shion, and the ""levels"" of features can be enriched by the number of stacked layers (depth). Recent evidence <ref type=""bibr"" target=""#b39"">[40,</ref><ref type=""bibr"" target=""#b42"">43]</ref> reveals that network depth is of crucial importance, and the leading results <ref type=""bibr"" target=""#b39"">[ =""bibr"" target=""#b42"">43]</ref> reveals that network depth is of crucial importance, and the leading results <ref type=""bibr"" target=""#b39"">[40,</ref><ref type=""bibr"" target=""#b42"">43,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b15"">16]</ref> on the challenging ImageNet d d a linear layer connected from the network input to the output <ref type=""bibr"" target=""#b32"">[33,</ref><ref type=""bibr"" target=""#b47"">48]</ref>. In <ref type=""bibr"" target=""#b42"">[43,</ref><ref type=""bibr"" target=""#b23"">24]</ref>, a few intermediate layers are directly connected to auxiliary class ibr"" target=""#b45"">46]</ref> propose methods for centering layer responses, gradients, and propagated errors, implemented by shortcut connections. In <ref type=""bibr"" target=""#b42"">[43]</ref>, an ""inception"" layer is composed of a shortcut branch and a few deeper branches.</p><p>Concurrent with our",1
"Fig. <ref type=""figure"" target=""#fig_1"">2</ref>). Shortcut connections <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" target=""#b47"">48]</ref> are those skipping one or more layers. In our case, the shortcut connections simply perform identity mapping, Connections. Practices and theories that lead to shortcut connections <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" target=""#b47"">48]</ref> have been studied for a long time. An early practice of training multi-layer perceptrons (MLPs) is to add a l ining multi-layer perceptrons (MLPs) is to add a linear layer connected from the network input to the output <ref type=""bibr"" target=""#b32"">[33,</ref><ref type=""bibr"" target=""#b47"">48]</ref>. In <ref type=""bibr"" target=""#b42"">[43,</ref><ref type=""bibr"" target=""#b23"">24]</ref>, a few intermediate lay",0
"nce, and the leading results <ref type=""bibr"" target=""#b39"">[40,</ref><ref type=""bibr"" target=""#b42"">43,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b15"">16]</ref> on the challenging ImageNet dataset <ref type=""bibr"" target=""#b34"">[35]</ref> all exploit ""very deep"" <ref ty all exploit ""very deep"" <ref type=""bibr"" target=""#b39"">[40]</ref> models, with a depth of sixteen <ref type=""bibr"" target=""#b39"">[40]</ref> to thirty <ref type=""bibr"" target=""#b15"">[16]</ref>. Many other nontrivial visual recognition tasks <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" targ pe=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" target=""#b11"">12]</ref> and intermediate normalization layers <ref type=""bibr"" target=""#b15"">[16]</ref>, which enable networks with tens of layers to start converging for stochastic gradient descent (SGD) with ba br"" target=""#b20"">[21]</ref>. The standard color augmentation in <ref type=""bibr"" target=""#b20"">[21]</ref> is used. We adopt batch normalization (BN) <ref type=""bibr"" target=""#b15"">[16]</ref> right after each convolution and before activation, following <ref type=""bibr"" target=""#b15"">[16]</ref>. We f> is used. We adopt batch normalization (BN) <ref type=""bibr"" target=""#b15"">[16]</ref> right after each convolution and before activation, following <ref type=""bibr"" target=""#b15"">[16]</ref>. We initialize the weights as in <ref type=""bibr"" target=""#b11"">[12]</ref> and train all plain/residual nets ns. We use a weight decay of 0.0001 and a momentum of 0.9. We do not use dropout <ref type=""bibr"" target=""#b12"">[13]</ref>, following the practice in <ref type=""bibr"" target=""#b15"">[16]</ref>.</p><p>In testing, for comparison studies we adopt the standard 10-crop testing <ref type=""bibr"" target=""#b2 layer one.</p><p>We argue that this optimization difficulty is unlikely to be caused by vanishing gradients. These plain networks are trained with BN <ref type=""bibr"" target=""#b15"">[16]</ref>, which ensures forward propagated signals to have non-zero variances. We also verify that the backward propa rpart, the 34-layer 6.66 VGG <ref type=""bibr"" target=""#b39"">[40]</ref> (v5) 6.8 PReLU-net <ref type=""bibr"" target=""#b11"">[12]</ref> 4.94 BN-inception <ref type=""bibr"" target=""#b15"">[16]</ref> 4.82 ResNet (ILSVRC <ref type=""bibr"">'15)</ref> 3.57</p><p>Table <ref type=""table"">5</ref>. Error rates (%) erparts. We use a weight decay of 0.0001 and momentum of 0.9, and adopt the weight initialization in <ref type=""bibr"" target=""#b11"">[12]</ref> and BN <ref type=""bibr"" target=""#b15"">[16]</ref> but with no dropout. These models are trained with a minibatch size of 128 on two GPUs. We start with a lear",0
"+x can be realized by feedforward neural networks with ""shortcut connections"" (Fig. <ref type=""figure"" target=""#fig_1"">2</ref>). Shortcut connections <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" target=""#b47"">48]</ref> are those skipping one or more a good reformulation or preconditioning can simplify the optimization. Shortcut Connections. Practices and theories that lead to shortcut connections <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" target=""#b47"">48]</ref> have been studied for a long t",0
"target=""#tab_8"">7</ref> and 8 show the object detection baseline results on PASCAL VOC 2007 and 2012 <ref type=""bibr"" target=""#b4"">[5]</ref> and COCO <ref type=""bibr"" target=""#b25"">[26]</ref>. We adopt Faster R-CNN <ref type=""bibr"" target=""#b31"">[32]</ref> as the detection method. Here we are intere",0
"rk input to the output <ref type=""bibr"" target=""#b32"">[33,</ref><ref type=""bibr"" target=""#b47"">48]</ref>. In <ref type=""bibr"" target=""#b42"">[43,</ref><ref type=""bibr"" target=""#b23"">24]</ref>, a few intermediate layers are directly connected to auxiliary classifiers for addressing vanishing/exploding l cases (i.e., option A), method error (%) Maxout <ref type=""bibr"" target=""#b8"">[9]</ref> 9.38 NIN <ref type=""bibr"" target=""#b24"">[25]</ref> 8.81 DSN <ref type=""bibr"" target=""#b23"">[24]</ref> 8.22 # layers # params FitNet <ref type=""bibr"">[</ref> so our residual models have exactly the same depth, w 48k iterations, and terminate training at 64k iterations, which is determined on a 45k/5k train/val split. We follow the simple data augmentation in <ref type=""bibr"" target=""#b23"">[24]</ref> for training: 4 pixels are padded on each side, a n da3 2 ×32 crop is randomly sampled from the padded image",0
"r"" target=""#b7"">8]</ref>, which hamper convergence from the beginning. This problem, however, has been largely addressed by normalized initialization <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" target=""#b11"">",0
"nd classification <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b46"">47]</ref>. For vector quantization, encoding residual vectors <ref type=""bibr"" target=""#b16"">[17]</ref> is shown to be more effective than encoding original vectors.</p><p>In low-level vision and computer graphic",0
"ighway networks"" <ref type=""bibr"" target=""#b40"">[41,</ref><ref type=""bibr"" target=""#b41"">42]</ref> present shortcut connections with gating functions <ref type=""bibr"" target=""#b14"">[15]</ref>. These gates are data-dependent and have parameters, in contrast to our identity shortcuts that are paramete",0
"tion, post-processing is used to refine the bounding boxes, eliminate duplicate detections, and rescore the boxes based on other objects in the scene <ref type=""bibr"" target=""#b12"">[13]</ref>. These complex pipelines are slow and hard to optimize because each individual component must be trained sep features <ref type=""bibr"" target=""#b5"">[6]</ref>). Then, classifiers <ref type=""bibr"" target=""#b34"">[35,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b9"">10]</ref> or localizers <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr""",1
"we use dropout and extensive data augmentation. A dropout layer with rate = .5 after the first connected layer prevents co-adaptation between layers <ref type=""bibr"" target=""#b17"">[18]</ref>. For data augmentation we introduce random scaling and translations of up to 20% of the original image size.",0
"framework by sharing computation and using neural networks to propose regions instead of Selective Search <ref type=""bibr"" target=""#b13"">[14]</ref>  <ref type=""bibr"" target=""#b26"">[27]</ref>. While they offer speed and accuracy improvements over R-CNN, both still fall short of real-time performance",0
"ct detection is a core problem in computer vision. Detection pipelines generally start by extracting a set of robust features from input images (Haar <ref type=""bibr"" target=""#b24"">[25]</ref>, SIFT <ref type=""bibr"" target=""#b22"">[23]</ref>, HOG <ref type=""bibr"" target=""#b3"">[4]</ref>, convolutional",0
"y a week and achieve a single crop top-5 accuracy of 88% on the ImageNet 2012 validation set, comparable to the GoogLeNet models in Caffe's Model Zoo <ref type=""bibr"" target=""#b23"">[24]</ref>.</p><p>We then convert the model to perform detection. Ren et al. show that adding both convolutional and co",0
"training and test time so it implicitly encodes contextual information about classes as well as their appearance. Fast R-CNN, a top detection method <ref type=""bibr"" target=""#b13"">[14]</ref>, mistakes background patches in an image for objects because it can't see the larger context. YOLO makes les this complex pipeline must be precisely tuned independently and the resulting system is very slow, taking more than 40 seconds per image at test time <ref type=""bibr"" target=""#b13"">[14]</ref>.</p><p>YOLO shares some similarities with R-CNN. Each grid cell proposes potential bounding boxes and scores Faster R-CNN focus on speeding up the R-CNN framework by sharing computation and using neural networks to propose regions instead of Selective Search <ref type=""bibr"" target=""#b13"">[14]</ref>  <ref type=""bibr"" target=""#b26"">[27]</ref>. While they offer speed and accuracy improvements over R-CNN, bot rences between YOLO and R-CNN variants we explore the errors on VOC 2007 made by YOLO and Fast R-CNN, one of the highest performing versions of R-CNN <ref type=""bibr"" target=""#b13"">[14]</ref>. Based on the different error profiles we show that YOLO can be used to rescore Fast R-CNN detections and re",0
"verge from what the system has seen before <ref type=""bibr"" target=""#b2"">[3]</ref>. We compare YOLO to other detection systems on the Picasso Dataset <ref type=""bibr"" target=""#b11"">[12]</ref> and the People-Art Dataset <ref type=""bibr"" target=""#b2"">[3]</ref>, two datasets for testing person detectio",0
"ng. We propose a simple yet effective training procedure. We learn residuals only and use extremely high learning rates (10 4 times higher than SRCNN <ref type=""bibr"" target=""#b8"">[6]</ref>) enabled by adjustable gradient clipping. Our proposed method performs better than existing methods in accurac tionary based on sparse signal representation. Lately, random forest <ref type=""bibr"" target=""#b20"">[18]</ref> and convolutional neural network (CNN) <ref type=""bibr"" target=""#b8"">[6]</ref> have also been used with large improvements in accuracy.</p><p>Among them, Dong et al. <ref type=""bibr"" target l neural network (CNN) <ref type=""bibr"" target=""#b8"">[6]</ref> have also been used with large improvements in accuracy.</p><p>Among them, Dong et al. <ref type=""bibr"" target=""#b8"">[6]</ref> has demonstrated that a CNN can be used to learn a mapping from LR to HR in an end-to-end manner. Their method ture for effi-cient learning when input and output are highly correlated. Moreover, our initial learning rate is 10 4 times higher than that of SRCNN <ref type=""bibr"" target=""#b8"">[6]</ref>. This is enabled by residual-learning and gradient clipping.</p><p>Scale Factor We propose a single-model SR a h extraction/representation, non-linear mapping and reconstruction. Filters of spatial sizes 9 × 9, 1 × 1, and 5 × 5 were used respectively.</p><p>In <ref type=""bibr"" target=""#b8"">[6]</ref>, Dong et al. attempted to prepare deeper models, but failed to observe superior performance after a week of tr at increasing depth significantly boosts performance. We successfully use 20 weight layers (3 × 3 for each layer). Our network is very deep (20 vs. 3 <ref type=""bibr"" target=""#b8"">[6]</ref>) and information used for reconstruction (receptive field) is much larger (41 × 41 vs. 13 × 13).</p><p>Trainin y multiplied by 0.0001).</p><p>High Learning Rates for Very Deep Networks Training deep models can fail to converge in realistic limit of time. SRCNN <ref type=""bibr"" target=""#b8"">[6]</ref> fails to show superior performance with more than three weight layers. While there can be various reasons, one verged. Their learning rate 10 −5 is too small for a network to converge within a week on a common GPU. Looking at Fig. <ref type=""figure"">9</ref> of <ref type=""bibr"" target=""#b8"">[6]</ref>, it is not easy to say their deeper networks have converged and their performances were saturated. While more nd the second one uses 291 images with the addition of 200 images from Berkeley Segmentation Dataset <ref type=""bibr"" target=""#b18"">[16]</ref>. SRCNN <ref type=""bibr"" target=""#b8"">[6]</ref> uses a very large ImageNet dataset.</p><p>We use 291 images as in <ref type=""bibr"" target=""#b20"">[18]</ref> fo tasets. Moreover, our methods are relatively fast. The public code of SRCNN based on a CPU implementation is slower than the code used by Dong et. al <ref type=""bibr"" target=""#b8"">[6]</ref> in their paper based on a GPU implementation.</p><p>In Figures <ref type=""figure"">6 and 7</ref>, we compare ou",1
"ghbor embedding <ref type=""bibr"" target=""#b6"">[4,</ref><ref type=""bibr"" target=""#b17"">15]</ref> methods interpolate the patch subspace. Sparse coding <ref type=""bibr"" target=""#b27"">[25,</ref><ref type=""bibr"" target=""#b28"">26,</ref><ref type=""bibr"" target=""#b23"">21,</ref><ref type=""bibr"" target=""#b24 LR to HR in an end-to-end manner. Their method, termed SRCNN, does not require any engineered features that are typically necessary in other methods <ref type=""bibr"" target=""#b27"">[25,</ref><ref type=""bibr"" target=""#b28"">26,</ref><ref type=""bibr"" target=""#b23"">21,</ref><ref type=""bibr"" target=""#b24 rent training images. For example, RFL <ref type=""bibr"" target=""#b20"">[18]</ref> has two methods, where the first one uses 91 images from Yang et al. <ref type=""bibr"" target=""#b27"">[25]</ref> and the second one uses 291 images with the addition of 200 images from Berkeley Segmentation Dataset <ref t",0
"image, commonly referred as single image super-resolution (SISR) <ref type=""bibr"" target=""#b14"">[12]</ref>, <ref type=""bibr"" target=""#b10"">[8]</ref>, <ref type=""bibr"" target=""#b11"">[9]</ref>. SISR is widely used in computer vision applications ranging from security and surveillance imaging to medica ods utilizing statistical image priors <ref type=""bibr"" target=""#b22"">[20,</ref><ref type=""bibr"" target=""#b15"">13]</ref> or internal patch recurrence <ref type=""bibr"" target=""#b11"">[9]</ref>.</p><p>Currently, learning methods are widely used to model a mapping from LR to HR patches. Neighbor embeddi olation to color components of an image and sophisticated models to luminance components as in other methods <ref type=""bibr"" target=""#b6"">[4]</ref>, <ref type=""bibr"" target=""#b11"">[9]</ref>, <ref type=""bibr"" target=""#b28"">[26]</ref>. This is because human vision is more sensitive to details in inte",0
"h many weight layers, this becomes an end-to-end relation requiring very long-term memory. For this reason, the vanishing/exploding gradients problem <ref type=""bibr"" target=""#b4"">[2]</ref> can be critical. We can solve this problem simply with residual-learning.</p><p>As the input and output images asic rule of thumb to make learning rate high to boost training. But simply setting learning rate high can also lead to vanishing/exploding gradients <ref type=""bibr"" target=""#b4"">[2]</ref>. For the reason, we suggest an adjustable gradient clipping for maximal boost in speed while suppressing explo",0
"t=""#b17"">[18]</ref>. Previous work explored DNN properties that could be used to craft adversarial samples <ref type=""bibr"" target=""#b17"">[18]</ref>, <ref type=""bibr"" target=""#b28"">[30]</ref>, <ref type=""bibr"" target=""#b34"">[36]</ref>. Simply put, these techniques exploit gradients computed by netwo en et al., who presented a method for producing images that are unrecognizable to humans, but are nonetheless labeled as recognizable objects by DNNs <ref type=""bibr"" target=""#b28"">[30]</ref>. For instance, they demonstrated how a DNN will classify a noise-filled image constructed using their techni cture and Training Tools threat model, based on the backpropagation procedure used during network training <ref type=""bibr"" target=""#b17"">[18]</ref>, <ref type=""bibr"" target=""#b28"">[30]</ref>, <ref type=""bibr"" target=""#b34"">[36]</ref>. This approach creates adversarial samples by defining an optimiz",1
"rafting much easier for input domains like malware executables, which are not as easy to perturb as images <ref type=""bibr"" target=""#b10"">[11]</ref>, <ref type=""bibr"" target=""#b15"">[16]</ref>. This distortion reduction comes with a performance cost. Indeed, more elaborate but accurate saliency map f",0
"mples can make DNNs more robust.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>VII. RELATED WORK</head><p>The security of machine learning <ref type=""bibr"" target=""#b1"">[2]</ref> is an active research topic within the security and machine learning communities. A broad taxonomy of attacks",0
", and • is a norm appropriate to compare the DNN inputs. Solving this problem is non-trivial, as properties of DNNs make it non-linear and non-convex <ref type=""bibr"" target=""#b24"">[25]</ref>. Thus, we craft adversarial samples by constructing a mapping from input perturbations to output variations. ple heuristics, or even brute force. However such solutions are hard to implement for deep neural networks because of non-convexity and non-linearity <ref type=""bibr"" target=""#b24"">[25]</ref>. Instead, we propose a systematic approach stemming from the forward derivative.</p><p>We define the forward",0
"#b30"">[32]</ref>, <ref type=""bibr"" target=""#b31"">[33]</ref>, language processing <ref type=""bibr"" target=""#b12"">[13]</ref>, financial fraud detection <ref type=""bibr"" target=""#b22"">[23]</ref>, and recently malware detection <ref type=""bibr"" target=""#b13"">[14]</ref>.</p><p>This increasing use of deep",0
"on-sequential outputs. We will describe sequential outputs in the next section. The biggest modification of GNNs is that we use Gated Recurrent Units <ref type=""bibr"" target=""#b5"">(Cho et al., 2014)</ref> and unroll the recurrence for a fixed number of steps T and use backpropagation through time in",1
"e bases. In this work, we study feature learning techniques for graph-structured inputs. Our starting point is previous work on Graph Neural Networks <ref type=""bibr"" target=""#b24"">(Scarselli et al., 2009)</ref>, which we modify to use gated recurrent units and modern optimization techniques and the s of the internal state during the process of producing a sequence of outputs. Here, (1) is mostly achieved by previous work on Graph Neural Networks <ref type=""bibr"" target=""#b24"">(Scarselli et al., 2009)</ref>; we make several minor adaptations of this framework, including changing it to use moder goal in this work are methods that learn features on graphs, including Graph Neural Networks <ref type=""bibr"" target=""#b10"">(Gori et al., 2005;</ref><ref type=""bibr"" target=""#b24"">Scarselli et al., 2009)</ref>, spectral networks <ref type=""bibr"" target=""#b4"">(Bruna et al., 2013)</ref> and recent wo n=""2"">GRAPH NEURAL NETWORKS</head><p>In this section, we review Graph Neural Networks (GNNs) <ref type=""bibr"" target=""#b10"">(Gori et al., 2005;</ref><ref type=""bibr"" target=""#b24"">Scarselli et al., 2009)</ref> and introduce notation and concepts that will be used throughout.</p><p>GNNs are a genera work on directed graphs, so (v, v ) represents a directed edge v → v , but we note that the framework can easily be adapted to undirected graphs; see <ref type=""bibr"" target=""#b24"">Scarselli et al. (2009)</ref>. The node vector (or node representation or node embedding) for node v is denoted by h v he timestep:</p><formula xml:id=""formula_2"">h (t) v = f * (l v , l CO(v) , l NBR(v) , h (t−1) NBR(v)</formula><p>). Several variants are discussed in <ref type=""bibr"" target=""#b24"">Scarselli et al. (2009)</ref> including positional graph forms, node-specific updates, and alternative representations >Scarselli et al. (2009)</ref> including positional graph forms, node-specific updates, and alternative representations of neighborhoods. Concretely, <ref type=""bibr"" target=""#b24"">Scarselli et al. (2009)</ref> suggest decomposing f * (•) to be a sum of per-edge terms:</p><formula xml:id=""formula_3"" odel is defined per node and is a differentiable function g(h v , l v ) that maps to an output. This is generally a linear or neural network mapping. <ref type=""bibr"" target=""#b24"">Scarselli et al. (2009)</ref> focus on outputs that are independent per node, which are implemented by mapping the fina ave been applied in several domains <ref type=""bibr"" target=""#b10"">(Gori et al., 2005;</ref><ref type=""bibr"" target=""#b6"">Di Massa et al., 2006;</ref><ref type=""bibr"" target=""#b24"">Scarselli et al., 2009;</ref><ref type=""bibr"">Uwents et al., 2011)</ref>, but they do not appear to be in widespread us",1
"e no null pointer dereferences in a program), a core problem is to find mathematical descriptions of the data structures used in a program. Following <ref type=""bibr"" target=""#b3"">Brockschmidt et al. (2015)</ref>, we have phrased this as a machine learning problem where we will learn to map from a s ap from a set of input graphs, representing the state of memory, to a logical description of the data structures that have been instantiated. Whereas <ref type=""bibr"" target=""#b3"">Brockschmidt et al. (2015)</ref> relied on a large amount of hand-engineering of features, we show that the system can b osed to single output single graph training.</p><p>A more complex scenario allowing for nested data structures (e.g., list of lists) was discussed in <ref type=""bibr"" target=""#b3"">Brockschmidt et al. (2015)</ref>. We have also successfully extended the GGS-NN model to this case. More details on this ng names and order of the formulas and then comparing for exact equality.</p><p>We compared our GGS-NN-based model with a method we developed earlier <ref type=""bibr"" target=""#b3"">(Brockschmidt et al., 2015)</ref>. The earlier approach treats each prediction step as standard classification, and requ",0
"classification can be handled in the same manner as node-level regression or classification.</p><p>Learning is done via the Almeida-Pineda algorithm <ref type=""bibr"" target=""#b0"">(Almeida, 1990;</ref><ref type=""bibr"" target=""#b21"">Pineda, 1987)</ref>, which works by running the propagation to conve",0
"rent dimension.</p><p>GGS-NNs are related to soft alignment and attentional models (e.g., <ref type=""bibr"" target=""#b1"">Bahdanau et al. (2014)</ref>; <ref type=""bibr"" target=""#b16"">Kumar et al. (2015)</ref>; <ref type=""bibr"">Sukhbaatar et al. (2015)</ref>) in two respects: first, the graph represent",0
"ural Networks <ref type=""bibr"" target=""#b10"">(Gori et al., 2005;</ref><ref type=""bibr"" target=""#b24"">Scarselli et al., 2009)</ref>, spectral networks <ref type=""bibr"" target=""#b4"">(Bruna et al., 2013)</ref> and recent work on learning graph fingerprints for classification tasks on graph representati classify using an output neural network. There are several models that make use of similar propagation of node representations on a graph structure. <ref type=""bibr"" target=""#b4"">Bruna et al. (2013)</ref> generalize convolutions to graph structures. The difference between their work and GNNs is ana",0
"s paper of assembling problem-specific neural networks as a composition of learned components has a long history, dating back at least to the work of <ref type=""bibr"" target=""#b12"">Hinton (1988)</ref> on assembling neural networks according to a family tree structure in order to predict relations be",0
"b4"">(Bruna et al., 2013)</ref> and recent work on learning graph fingerprints for classification tasks on graph representations of chemical molecules <ref type=""bibr"" target=""#b8"">(Duvenaud et al., 2015)</ref>.</p><p>Our main contribution is an extension of Graph Neural Networks that outputs sequenc olutions to graph structures. The difference between their work and GNNs is analogous to the difference between convolutional and recurrent networks. <ref type=""bibr"" target=""#b8"">Duvenaud et al. (2015)</ref> also consider convolutional like operations on graphs, building a learnable, differentiable",0
"gy for obtaining better generalization from small amounts of supervision. However, even systems that have relied extensively on unsupervised features <ref type=""bibr"" target=""#b7"">(Collobert et al., 2011;</ref><ref type=""bibr"" target=""#b36"">Turian et al., 2010;</ref><ref type=""bibr"" target=""#b23"">Li <p>We provide a brief description of LSTMs and CRFs, and present a hybrid tagging architecture. This architecture is similar to the ones presented by <ref type=""bibr"" target=""#b7"">Collobert et al. (2011)</ref> and <ref type=""bibr"" target=""#b19"">Huang et al. (2015)</ref>.</p></div> <div xmlns=""http:/ elationship between words and their characters.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.2"">Pretrained embeddings</head><p>As in <ref type=""bibr"" target=""#b7"">Collobert et al. (2011)</ref>, we use pretrained word embeddings to initialize our lookup table. We observe significant F, the character-level representations, pretraining of our    Several other neural architectures have previously been proposed for NER. For instance, <ref type=""bibr"" target=""#b7"">Collobert et al. (2011)</ref> uses a CNN over a sequence of word embeddings with a CRF layer on top. This can be thought",1
"a hybrid tagging architecture. This architecture is similar to the ones presented by <ref type=""bibr"" target=""#b7"">Collobert et al. (2011)</ref> and <ref type=""bibr"" target=""#b19"">Huang et al. (2015)</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2.1"">LSTM</head><p>Recurrent neu This can be thought of as our first model without character-level embeddings and with the bidirectional LSTM being replaced by a CNN. More recently, <ref type=""bibr"" target=""#b19"">Huang et al. (2015)</ref> presented a model similar to our LSTM-CRF, but using hand-crafted spelling features. <ref typ",1
"om/ glample/tagger and https://github.com/clab/ stack-lstm-ner</note> 			<note xmlns=""http://www.tei-c.org/ns/1.0"" place=""foot"" n=""2"" xml:id=""foot_1""><ref type=""bibr"" target=""#b15"">(Graff, 2011;</ref><ref type=""bibr"" target=""#b4"">Biemann et al., 2007;</ref> Callison-Burch et al.,  2010;<ref type=""bi",0
"(where does the word being tagged tend to occur in a corpus?). To capture orthographic sensitivity, we use character-based word representation model <ref type=""bibr"" target=""#b25"">(Ling et al., 2015b)</ref> to capture distributional sensitivity, we combine these representations with distributional ><p>A very simple-but surprisingly effective-tagging model is to use the h t 's as features to make independent tagging decisions for each output y t <ref type=""bibr"" target=""#b25"">(Ling et al., 2015b)</ref>. Despite this model's success in simple problems like POS tagging, its independent classific ound useful for morphologically rich languages and to handle the outof-vocabulary problem for tasks like part-of-speech tagging and language modeling <ref type=""bibr"" target=""#b25"">(Ling et al., 2015b)</ref> or dependency parsing <ref type=""bibr"" target=""#b2"">(Ballesteros et al., 2015)</ref>. Figure efined to be the dot product between the embedding of a wordin-context computed with a bidirectional LSTMexactly the same as the POS tagging model of <ref type=""bibr"" target=""#b25"">Ling et al. (2015b)</ref> and these are combined with bigram compatibility scores (i.e., the A y,y 's). This architectu",0
"nsively on unsupervised features <ref type=""bibr"" target=""#b7"">(Collobert et al., 2011;</ref><ref type=""bibr"" target=""#b36"">Turian et al., 2010;</ref><ref type=""bibr"" target=""#b23"">Lin and Wu, 2009;</ref><ref type=""bibr"">Ando and Zhang, 2005b, inter alia)</ref> have used these to augment, rather tha elling features. <ref type=""bibr"" target=""#b40"">Zhou and Xu (2015)</ref> also used a similar model and adapted it to the semantic role labeling task. <ref type=""bibr"" target=""#b23"">Lin and Wu (2009)</ref> used a linear chain CRF with L 2 regularization, they added phrase cluster features extracted f",0
"ndependence assumptions.</p><p>Therefore, instead of modeling tagging decisions independently, we model them jointly using a conditional random field <ref type=""bibr"" target=""#b22"">(Lafferty et al., 2001)</ref>. For an input sentence</p><formula xml:id=""formula_2"">X = (x 1 , x 2 , . . . , x n ),</fo",0
"backward LSTM. These are two distinct networks with different parameters. This forward and backward LSTM pair is referred to as a bidirectional LSTM <ref type=""bibr"" target=""#b16"">(Graves and Schmidhuber, 2005)</ref>.</p><p>The representation of a word using this model is obtained by concatenating",0
"model significantly outperforms all previous methods, including the ones using external labeled data. The only exception is Dutch, where the model of <ref type=""bibr"" target=""#b14"">Gillick et al. (2015)</ref> can perform better by leveraging the information from other NER datasets. The Stack-LSTM al ron and aggregating context information.</p><p>Finally, there is currently a lot of interest in models for NER that use letter-based representations. <ref type=""bibr"" target=""#b14"">Gillick et al. (2015)</ref> model the task of sequencelabeling as a sequence to sequence learning problem and incorpora",0
"the problem of galaxy morphology prediction by rotating feature maps, effectively learning an equivariant representation. This work was later extended<ref type=""bibr"" target=""#b8"">(Dieleman et al., 2016)</ref> and evaluated on various computer vision problems that have cyclic symmetry.<ref type=""bib",0
"</ref> show that useful representations can be learned in an unsupervised manner by training a convolutional network to be equivariant to ego-motion. <ref type=""bibr"" target=""#b1"">Anselmi et al. (2014;</ref><ref type=""bibr"">2015)</ref> use the theory of locally compact topological groups to develop",0
"=""#b25"">Lee et al., 2015b;</ref><ref type=""bibr"" target=""#b38"">Srivastava et al., 2015;</ref><ref type=""bibr"" target=""#b4"">Clevert et al., 2015;</ref><ref type=""bibr"" target=""#b24"">Lee et al., 2015a)</ref>. However, due to radical differences in model sizes and architectures, it is difficult to infe",0
"separate pass for each parameter and would make deep learning entirely infeasible.</p><p>Applying RMD to hyperparameter optimization was proposed by <ref type=""bibr"" target=""#b3"">Bengio (2000)</ref> and <ref type=""bibr"" target=""#b2"">Baydin &amp; Pearlmutter (2014)</ref>, and applied to small proble et al. (1998</ref><ref type=""bibr"" target=""#b13"">), Eigenmann &amp; Nossek (1999)</ref>, <ref type=""bibr"" target=""#b9"">Chen &amp; Hagan (1999)</ref>, <ref type=""bibr"" target=""#b3"">Bengio (2000)</ref>, <ref type=""bibr"" target=""#b0"">Abdel-Gawad &amp; Ratner (2007)</ref>, and <ref type=""bibr"" target=""#",1
"ype=""bibr"" target=""#b3"">Bengio (2000)</ref> and <ref type=""bibr"" target=""#b2"">Baydin &amp; Pearlmutter (2014)</ref>, and applied to small problems by <ref type=""bibr"" target=""#b12"">Domke (2012)</ref>. However, the naïve approach fails for real-sized problems because of memory constraints. RMD requir se the number of hidden units.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""5."">Related work</head><p>The most closely-related work is <ref type=""bibr"" target=""#b12"">Domke (2012)</ref>, who derived algorithms to compute reverse-mode derivatives of gradient descent with momentum and L-",1
"example, evaluations of Long Short-Term Memory <ref type=""bibr"" target=""#b18"">(Hochreiter &amp; Schmidhuber, 1997)</ref> or a Neural Turing Machines <ref type=""bibr"" target=""#b16"">(Graves et al., 2014)</ref> rely on long chains of mostly-small updates of parameters. Exactly reversing these dynamics",0
"iv xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.6."">Implementation Details</head><p>Automatic differentiation (AD) software packages such as Theano <ref type=""bibr"" target=""#b1"">(Bastien et al., 2012;</ref><ref type=""bibr"" target=""#b5"">Bergstra et al., 2010)</ref> are mainstays of deep learning, s",0
"oach relied on naïve caching of all parameter vectors w 1 , w 2 , . . . , w T , making it impractical for large models with many training iterations. <ref type=""bibr"" target=""#b23"">Larsen et al. (1998</ref><ref type=""bibr"" target=""#b13"">), Eigenmann &amp; Nossek (1999)</ref>, <ref type=""bibr"" target",0
"ning these softened architectures feasible.</p><p>We illustrate this ""architecture learning"" with a multitask learning problem, the Omniglot data set <ref type=""bibr"" target=""#b22"">(Lake, 2014)</ref>. This data set consists of 28x28 pixel greyscale images of characters from 50 alphabets with up to 5",0
"s 11 and 12 both require a Hessian-vector product, but these can be computed exactly by applying RMD to the dot product of the gradient with a vector <ref type=""bibr"" target=""#b31"">(Pearlmutter, 1994)</ref>. Thus the time complexity of reverse SGD is O(T ), the same as forward SGD.</p></div> <div xm",0
"procedures often employ various heuristics to set learning rate schedules, or set their shape using one or two hyperparameters set by crossvalidation <ref type=""bibr"" target=""#b11"">(Dahl et al., 2014;</ref><ref type=""bibr"" target=""#b36"">Sutskever et al., 2013)</ref>. These schedule choices are suppo",0
"eed to continuously adapt to the new distribution. When the input distribution to a learning system changes, it is said to experience covariate shift <ref type=""bibr"" target=""#b17"">(Shimodaira, 2000)</ref>. This is typically handled via domain adaptation <ref type=""bibr"" target=""#b7"">(Jiang, 2008)</",1
"rval, either by modifying the network directly or by changing the parameters of the optimization algorithm to depend on the network activation values <ref type=""bibr"" target=""#b22"">(Wiesler et al., 2014;</ref><ref type=""bibr"" target=""#b14"">Raiko et al., 2012;</ref><ref type=""bibr"" target=""#b13"">Pove",1
"shift on training, and the ability of Batch Normalization to combat it, we considered the problem of predicting the digit class on the MNIST dataset <ref type=""bibr"" target=""#b8"">(LeCun et al., 1998a)</ref>. We used a very simple network, with a 28x28 binary image as input, and 3 fully-connected hi",0
"et=""#b11"">(Nair &amp; Hinton, 2010)</ref> ReLU (x) = max(x, 0), careful initialization <ref type=""bibr"" target=""#b0"">(Bengio &amp; Glorot, 2010;</ref><ref type=""bibr"" target=""#b16"">Saxe et al., 2013)</ref>, and small learning rates. If, however, we could ensure that the distribution of nonlinearity her conjecture that Batch Normalization may lead the layer Jacobians to have singular values close to 1, which is known to be beneficial for training <ref type=""bibr"" target=""#b16"">(Saxe et al., 2013)</ref>. Consider two consecutive layers with normalized inputs, and the transformation between these",0
"n to a new variant of the Inception network <ref type=""bibr"" target=""#b20"">(Szegedy et al., 2014)</ref>, trained on the Im-ageNet classification task <ref type=""bibr"" target=""#b15"">(Russakovsky et al., 2014)</ref>. The network has a large number of convolutional and pooling layers, with a softmax la rt a test error of 4.82% on test server. This improves upon the previous best result, and exceeds the estimated accuracy of human raters according to <ref type=""bibr"" target=""#b15"">(Russakovsky et al., 2014)</ref>.</p><p>For our ensemble, we used 6 networks. Each was based on BN-x30, modified via so",0
"etition are reached by the Deep Image ensemble of traditional models <ref type=""bibr"" target=""#b23"">(Wu et al., 2015)</ref> and the ensemble model of <ref type=""bibr"" target=""#b5"">(He et al., 2015)</ref>. The latter reports the top-5 error of 4.94%, as evaluated by the ILSVRC test server. Here we re",0
"range of possibilities that Batch Normalization potentially enables. Our future work includes applications of our method to Recurrent Neural Networks <ref type=""bibr"" target=""#b12"">(Pascanu et al., 2013)</ref>, where the internal covariate shift and the vanishing or exploding gradients may be especi",0
"f type=""bibr"" target=""#b15"">16]</ref> have significantly improved image classification <ref type=""bibr"" target=""#b13"">[14]</ref> and object detection <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b18"">19]</ref> accuracy. Compared to image classification, object detection is a more ification, object detection is a more challenging task that requires more complex methods to solve. Due to this complexity, current approaches (e.g., <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b24""> compromise speed, accuracy, or simplicity.</p><p>In this paper, we streamline the training process for stateof-the-art ConvNet-based object detectors <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b10"">11]</ref>. We propose a single-stage training algorithm that jointly learns to c optimizes a softmax classifier and bounding-box regressors, rather than training a softmax classifier, SVMs, and regressors in three separate stages <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b10"">11]</ref>. The components of this procedure (the loss, mini-batch sampling strat locations.</p><p>The resulting method can train a very deep detection network (VGG16 <ref type=""bibr"" target=""#b19"">[20]</ref>) 9× faster than R-CNN <ref type=""bibr"" target=""#b8"">[9]</ref> and 3× faster than SPPnet <ref type=""bibr"" target=""#b10"">[11]</ref>. At runtime, the detection network process ef></p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""1.1."">R-CNN and SPPnet</head><p>The Region-based Convolutional Network method (R-CNN) <ref type=""bibr"" target=""#b8"">[9]</ref> achieves excellent object detection accuracy by using a deep ConvNet to classify object proposals. R-CNN, howe egression offsets, t k = t k x , t k y , t k w , t k h , for each of the K object classes, indexed by k. We use the parameterization for t k given in <ref type=""bibr"" target=""#b8"">[9]</ref>, in which t k specifies a scale-invariant translation and log-space height/width shift relative to an object p 6]</ref> advocates for a two-network system that separates localization and classification. OverFeat <ref type=""bibr"" target=""#b18"">[19]</ref>, R-CNN <ref type=""bibr"" target=""#b8"">[9]</ref>, and SPPnet <ref type=""bibr"" target=""#b10"">[11]</ref> also train classifiers and bounding-box localizers, howe common practice, we actually iterate over permutations of the dataset). We use mini-batches of size R = 128, sampling 64 RoIs from each image. As in <ref type=""bibr"" target=""#b8"">[9]</ref>, we take 25% of the RoIs from object proposals that have intersection over union (IoU) overlap with a groundtr bability Pr(class = k | r) ∆ = p k . We then perform non-maximum suppression independently for each class using the algorithm and settings from R-CNN <ref type=""bibr"" target=""#b8"">[9]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.1."">Truncated SVD for faster detection</head><p ned ImageNet models that are available online. 2 The first is the CaffeNet (essentially AlexNet <ref type=""bibr"" target=""#b13"">[14]</ref>) from R-CNN <ref type=""bibr"" target=""#b8"">[9]</ref>. We alternatively refer to this CaffeNet as model S, for ""small."" The second network is VGG CNN M 1024 from <r",1
"task losses. We normalize the ground-truth regression targets v i to have zero mean and unit variance. All experiments use λ = 1.</p><p>We note that <ref type=""bibr"" target=""#b5"">[6]</ref> uses a related loss to train a classagnostic object proposal network. Different from our approach, <ref type="" note that <ref type=""bibr"" target=""#b5"">[6]</ref> uses a related loss to train a classagnostic object proposal network. Different from our approach, <ref type=""bibr"" target=""#b5"">[6]</ref> advocates for a two-network system that separates localization and classification. OverFeat <ref type=""bibr"" t",0
"lt is difficult to predict without actually running the experiment. The state-of-the-art for measuring object proposal quality is Average Recall (AR) <ref type=""bibr"" target=""#b11"">[12]</ref>. AR correlates well with mAP for several proposal methods using R-CNN, when using a fixed number of proposal",0
"uth bounding-box regression target v. We use a multi-task loss L on each labeled RoI to jointly train for classification and bounding-box regression: <ref type=""bibr"" target=""#b0"">(1)</ref> in which L cls (p, u) = − log p u is log loss for true class u.</p><formula xml:id=""formula_1"">L(p, u, t u , v",0
"limited to using a single scale by implementation details. Yet it achieves a mAP of 66.9%, which is slightly higher than the 66.0% reported for R-CNN <ref type=""bibr"" target=""#b9"">[10]</ref>, even though R-CNN uses ""infinite"" scales in the sense that each proposal is warped to a canonical size.</p><",0
"(CA) can service without snooping.</p><p>Synthetic benchmarks are an important tool to analyze specific aspects of computer systems. In previous work <ref type=""bibr"" target=""#b6"">[7]</ref>, <ref type=""bibr"" target=""#b7"">[8]</ref> we presented micro-benchmarks to measure the characteristics of distr ion 2.3]</ref>. It provides data from its appendent L3 slice or obtains a copy from another core's L1 or L2 cache as indicated by the core valid bits <ref type=""bibr"" target=""#b6"">[7]</ref>. In case of an L3 miss, the caching agent forwards the request to the home agent which provides the data from xmlns=""http://www.tei-c.org/ns/1.0""><head>B. Microbenchmark Design</head><p>We use an extended version of the synthetic microbenchmarks presented in <ref type=""bibr"" target=""#b6"">[7]</ref>. They include data placement and coherence state control mechanisms that place cache lines in a fully specifie",1
"es are restricted <ref type=""bibr"" target=""#b10"">[11]</ref>. Furthermore, the uncore frequency is adapted to the workload dynamically by the hardware <ref type=""bibr"" target=""#b11"">[12]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>B. Uncore Design</head><p>Haswell-EP is available /p></note> 			<note xmlns=""http://www.tei-c.org/ns/1.0"" place=""foot"" n=""4"" xml:id=""foot_2""><p>Uncore frequency scaling automatically adjusts frequency<ref type=""bibr"" target=""#b11"">[12]</ref> </p></note> 		</body> 		<back> 			<div type=""references"">  				<listBibl>  <biblStruct xml:id=""b0""> 	<analyt",0
"hy and avoid limitations caused by memory accesses.</p><p>Contemporary multi-socket x86 servers use point-to-point connections between the processors <ref type=""bibr"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b1"">[2]</ref>. The available memory bandwidth scales with the number of processors",0
"nce on the performance of parallel applications.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>II. RELATED WORK</head><p>Kottapalli et al. <ref type=""bibr"" target=""#b3"">[4]</ref> describe the directory assisted snoop broadcast protocol (DAS)-an extension of the MESIF protocol <ref type=""b mented using source snooping (see Section IV-B) or home snooping (see Section IV-C).</p><p>The MESIF protocol can be augmented with directory support <ref type=""bibr"" target=""#b3"">[4]</ref>. The ""directory assisted snoop broadcast protocol"" stores 2-bit of directory information for each cache line i forward in another node. Consequently, the in-memory state is changed to snoop all instead of shared which would be used without the directory cache <ref type=""bibr"" target=""#b3"">[4]</ref>. Thus, the home node broadcasts a snoop request which adds between 78 and 89 ns to the latency.  </p></div> <d",0
"al.</ref> was designed to maximize performance (minimal group miss ratio) and required the assumption that the individual miss ratio curve be convex <ref type=""bibr"" target=""#b8"">[9]</ref>. Our algorithm uses dynamic programming to examine the entire solution space. It generalizes the optimization oth for single-threaded programs by partitioning the cache between instructions and data, and for multiprogramming by giving each process a partition <ref type=""bibr"" target=""#b8"">[9]</ref>.</p><p>In the way of optimizing their algorithm, they proved that the average miss ratio for two or more unifo ext cache block to the process with the highest miss-rate derivative. The allocation is optimal if the miss-rate derivatives are as equal as possible <ref type=""bibr"" target=""#b8"">[9]</ref>. The optimality depends on several assumptions. One is that the miss-rate derivative must be monotonic. In oth",1
"s the issues of validation.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>A. Methodology</head><p>Following the methodology of Wang et al. <ref type=""bibr"" target=""#b11"">[12]</ref>, we randomly choose 16 programs from SPEC 2006: perlbench, bzip2, mcf, zeusmp, namd, dealII, soplex, povray, p.</p><p>The footprint measurement we use is the same as the implementation by Xiang et al. <ref type=""bibr"" target=""#b15"">[16]</ref> and Wang et al. <ref type=""bibr"" target=""#b11"">[12]</ref> Xiang et al. reported on average 23 times slowdown from the full-trace footprint analysis. Wang et al. devel =""bibr"" target=""#b15"">[16]</ref>. Independent validation can be found in the use of the footprint theory in optimal program symbiosis in shared cache <ref type=""bibr"" target=""#b11"">[12]</ref>, optimal memory allocation in Memcached <ref type=""bibr"" target=""#b1"">[2]</ref>, and a study from the OS com nt study by Wang et al. shows that the HOTL-based miss ratio prediction has a linear relationship between execution time, with a coefficient of 0.938 <ref type=""bibr"" target=""#b11"">[12]</ref>. They measure execution times and miss ratios of all 1820 4-programs co-run groups from a set of 16 SPEC pro >[16]</ref>. Wang et al. developed a sampling method called adaptive bursty footprint (ABF) profiling, which takes on average 0.09 second per program <ref type=""bibr"" target=""#b11"">[12]</ref>. To have reproducible results, our implementation uses the full-trace footprint. We assume that in practice, accuracy of the footprint analysis in modeling fully-associative LRU cache. Wang et al. tested the analysis on program execution traces for CPU cache <ref type=""bibr"" target=""#b11"">[12]</ref>, Hu et al. on keyvalue access traces for Memcached <ref type=""bibr"" target=""#b1"">[2]</ref>, and Wires et al. target=""#b1"">[2]</ref>. Finally, Wang et al. showed strong correlation (coefficient 0.938) between the predicted miss ratio and measured co-run speed <ref type=""bibr"" target=""#b11"">[12]</ref>. The correlation means that if we minimize the miss ratio in shared cache, we minimize the execution time of",0
". For example, Hsu et al. used a cache simulator called CASPER to measure the miss ratio curves from cache sizes 16KB to 1024KB in increments of 16KB <ref type=""bibr"" target=""#b0"">[1]</ref>. They noted that ""miss rate errors would have been unacceptably high with larger cache sizes"" because they ""we",0
"a. 2 Parihar et al. presented a counter-based hardware mechanism to provide programs the protection of partitioning, without the risk of unused space <ref type=""bibr"" target=""#b3"">[4]</ref>. and free-for-all sharing can be seen as opposite edge cases of partition-sharing. In this paper, we investiga",0
"allocation in Memcached <ref type=""bibr"" target=""#b1"">[2]</ref>, and a study from the OS community on server cache performance for disk access traces <ref type=""bibr"" target=""#b12"">[13]</ref>.</p><p>Random Phase Interaction: We assume that programs interact in their average behavior. Programs may ha et al. on keyvalue access traces for Memcached <ref type=""bibr"" target=""#b1"">[2]</ref>, and Wires et al.</p><p>on disk access traces for server cache <ref type=""bibr"" target=""#b12"">[13]</ref>. The three studies re-implemented the footprint analysis independently and reported high accuracy through ex",0
"for the particular classification task).</p><p>The proposed method naturally extends our previous work of unsupervised information network embedding <ref type=""bibr"" target=""#b26"">[27]</ref> and first learns a low dimensional embedding for words through a heterogeneous text network. The network enc rds, words and documents, and words and labels. The network is embedded into a low dimensional vector space that preserves the second-order proximity <ref type=""bibr"" target=""#b26"">[27]</ref> between the vertices in the network. The representation of an arbitrary piece of text (e.g., a sentence or a network embedding model called the ""LINE,"" which is suitable for arbitrary types of information networks: undirected or directed, binary or weighted <ref type=""bibr"" target=""#b26"">[27]</ref>. The LINE model optimizes an objective function which aims to preserve both the local and global network str 1"">Bipartite Network Embedding</head><p>In our previous work, we introduced the LINE model to learn the embedding of large-scale information networks <ref type=""bibr"" target=""#b26"">[27]</ref>. LINE is mainly designed for homogeneous networks, i.e., networks with the same types of nodes. LINE cannot not comparable. Here, we first adapt the LINE model for embedding bipartite networks. The essential idea is to make use of the second-order proximity <ref type=""bibr"" target=""#b26"">[27]</ref> between vertices, which assumes vertices with similar neighbors are similar to each other and thus should be log p(vj|vi).<label>(3)</label></formula><p>The objective (3) can be optimized with stochastic gradient descent using the techniques of edge sampling <ref type=""bibr"" target=""#b26"">[27]</ref> and negative sampling <ref type=""bibr"" target=""#b17"">[18]</ref>. In each step, a binary edge e = (i, j) is s ess significant deficiency of stochastic gradient descent in learning network embeddings. For the detailed optimization process, readers can refer to <ref type=""bibr"" target=""#b26"">[27]</ref>.</p><p>The embeddings of the word-word, word-document, and word-label network can all be learned by the abov straightforward solution to optimize the objective (4) is to merge the all the edges in the three sets Eww, E wd , E wl and then deploy edge sampling <ref type=""bibr"" target=""#b26"">[27]</ref>, which samples an edge for model updating in each step, with the sampling probability proportional to its we rder of the words <ref type=""bibr"" target=""#b9"">[10]</ref>.</p><p>• LINE: the large-scale information network embedding model proposed by Tang et al. <ref type=""bibr"" target=""#b26"">[27]</ref>. We use the LINE model to learn unsupervised embeddings with the wordword network, word-document network or",1
"figDesc>Test(PTE(G wl )) Figure5: Document visualization using unsupervised and predictive embeddings on 20ng data set, visualized with the t-SNE tool<ref type=""bibr"" target=""#b28"">[29]</ref>.</figDesc></figure> <figure xmlns=""http://www.tei-c.org/ns/1.0""><head></head><label></label><figDesc></figDe",0
"ween the training and test data sets, we randomly shuffle the training and test data sets; (4) RCV1, a large benchmark corpus for text classification <ref type=""bibr"" target=""#b11"">[12]</ref> <ref type=""foot"" target=""#foot_3"">4</ref> . Four subsets including Corporate, Economics, Government and Mark",0
"aph embedding algorithms such as MDS <ref type=""bibr"" target=""#b8"">[9]</ref>, IsoMap <ref type=""bibr"" target=""#b27"">[28]</ref> and Laplacian eigenmap <ref type=""bibr"" target=""#b0"">[1]</ref> are not applicable for embedding large-scale networks that contain millions of vertices and billions of edges. ce nodes, VB as the set of target nodes. Therefore, we can define the conditional probabilities p(vi|vj), p(vi|dj) and p(vi|lj) according to equation <ref type=""bibr"" target=""#b0"">(1)</ref>, and then learn the embeddings by optimizing objective function <ref type=""bibr"" target=""#b2"">(3)</ref>. Next,",0
"so compare with classical semisupervised approaches, Naive Bayes with EM (NB+EM) <ref type=""bibr"" target=""#b24"">[25]</ref> and label propagation (LP) <ref type=""bibr"" target=""#b30"">[31]</ref>. Fig. <ref type=""figure"">2</ref> reports the performance on both long and short documents. Overall, both CNN",0
"features was introduced in (Pérez- <ref type=""bibr"">Rosas et al., 2014)</ref>, which was then used to develop a multimodal deception detection system <ref type=""bibr"" target=""#b1"">(Abouelenien et al., 2014)</ref>. An extensive review of approaches for evaluating human credibility using physiological",1
"with deceptive behavior <ref type=""bibr"" target=""#b8"">(Depaulo et al., 2003)</ref>. The gesture annotation is performed using the MUMIN coding scheme <ref type=""bibr"" target=""#b2"">(Allwood et al., 2007)</ref>.</p><p>In the MUMIN scheme, facial displays consist of several different facial expressions",1
". specifically focus on the annotation of facial displays and hand movements, as they have been previously found to correlate with deceptive behavior <ref type=""bibr"" target=""#b8"">(Depaulo et al., 2003)</ref>. The gesture annotation is performed using the MUMIN coding scheme <ref type=""bibr"" target= n truthful and deceptive clips. This set of features is motivated by previous research that has suggested that deceivers' speech has lower complexity <ref type=""bibr"" target=""#b8"">(Depaulo et al., 2003)</ref>. We use the tool described in <ref type=""bibr"" target=""#b22"">(Lu, 2010)</ref>, which genera eceivers seem to make more eye contact (Interlocutor gaze) and nod (Side-Turn-R) more frequently than truth-tellers. This agrees with the findings in <ref type=""bibr"" target=""#b8"">(Depaulo et al., 2003)</ref> that liars who are more motivated to get away with their lies (i.e., trials) are likely to",1
"ion rate, skin temperature. Several studies <ref type=""bibr"" target=""#b39"">(Vrij, 2001;</ref><ref type=""bibr"" target=""#b13"">Gannon et al., 2009;</ref><ref type=""bibr"" target=""#b9"">Derksen, 2012)</ref> indicated that relying solely on physiological measurements can be biased and misleading. <ref type",0
"address the deception detection task using a number of modalities, including text <ref type=""bibr"" target=""#b12"">(Feng et al., 2012)</ref> and speech <ref type=""bibr"" target=""#b16"">(Hirschberg et al., 2005;</ref><ref type=""bibr"" target=""#b28"">Newman et al., 2003)</ref>. Unlike the polygraph methods,",0
"r"">(Maricchiolo et al., )</ref> for deception and social behavior. Facial expressions also played a critical role in the identification of deception. <ref type=""bibr"" target=""#b10"">(Ekman, 2001)</ref> defined micro-expressions as relatively short involuntary expressions, which can be indicative of d",0
"an important component of human deception.</p><p>We also analyze the contribution of the linguistic features. Using the linguistic ethnography method <ref type=""bibr"" target=""#b26"">(Mihalcea and Pulman, 2009)</ref>, we obtain the most dominant LIWC word classes associated with deceptive and truthful",0
"u et al., 2005;</ref><ref type=""bibr"" target=""#b38"">Tsechpenakis et al., 2005)</ref>, or using geometric features related to the hand and head motion <ref type=""bibr"" target=""#b24"">(Meservy et al., 2005)</ref>. Caso et al. <ref type=""bibr"" target=""#b5"">(Caso et al., 2006)</ref> identified particular",0
"d interviews. <ref type=""bibr"" target=""#b7"">Cohen et al. (2010)</ref> found that fewer iconic hand gestures were a sign of a deceptive narration, and <ref type=""bibr"" target=""#b15"">Hillman et al. (2012)</ref> determined that increased speech prompting gestures were associated with deception while in",0
"ification of deceptive content in a variety of domains, including online dating websites <ref type=""bibr"" target=""#b37"">(Toma and Hancock, 2010;</ref><ref type=""bibr"" target=""#b14"">Guadagno et al., 2012)</ref>, forums <ref type=""bibr"" target=""#b40"">(Warkentin et al., 2010;</ref><ref type=""bibr"" targ",0
"=""1"">Introduction</head><p>As deceptive behavior occurs on a daily basis in different areas of life <ref type=""bibr"" target=""#b25"">(Meyer, 2010;</ref><ref type=""bibr"" target=""#b35"">Smith et al., 2014)</ref>, the need arises for automated methodologies to detect deception in an efficient, yet reliabl",0
"09;</ref><ref type=""bibr"" target=""#b9"">Derksen, 2012)</ref> indicated that relying solely on physiological measurements can be biased and misleading. <ref type=""bibr"" target=""#b6"">Chittaranjan et al. (Chittaranjan and Hung, 2010</ref>) created an audio visual recording of the ""Are you a Werewolf?"" g",0
"deed a difficult task for humans and further verifies previous findings where human ability to spot liars was found to be slightly better than chance <ref type=""bibr"" target=""#b0"">(Aamodt and Custer, 2006)</ref>. Moreover, the performance of the human annotators appears to be significantly below tha",0
"er> 	<text xml:lang=""en""> 		<body> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""1"">Introduction</head><p>Knowledge bases (KBs), such as Freebase <ref type=""bibr"" target=""#b1"">(Bollacker et al., 2008)</ref>, NELL <ref type=""bibr"" target=""#b19"">(Mitchell et al., 2015)</ref>, and DBPedia <ref type",1
"a knowledge base. Markov logic networks <ref type=""bibr"" target=""#b23"">(Richardson and Domingos, 2006)</ref> fall into this category, as does ProPPR <ref type=""bibr"" target=""#b28"">(Wang et al., 2013)</ref> and many other logic-based systems. PRA, the main subject of this paper, also fits in this li",0
"are useful for various tasks, including training relation extractors and semantic parsers <ref type=""bibr"" target=""#b11"">(Hoffmann et al., 2011;</ref><ref type=""bibr"" target=""#b12"">Krishnamurthy and Mitchell, 2012)</ref>, and question answering <ref type=""bibr"" target=""#b0"">(Berant et al., 2013)</re",0
"type=""bibr"" target=""#b11"">(Hoffmann et al., 2011;</ref><ref type=""bibr"" target=""#b12"">Krishnamurthy and Mitchell, 2012)</ref>, and question answering <ref type=""bibr"" target=""#b0"">(Berant et al., 2013)</ref>. While these knowledge bases may be very large, they are still quite incomplete, missing lar",0
". While these knowledge bases may be very large, they are still quite incomplete, missing large percentages of facts about common or popular entities <ref type=""bibr"" target=""#b30"">(West et al., 2014;</ref><ref type=""bibr"" target=""#b5"">Choi et al., 2015)</ref>. The task of knowledge base completion-",0
"anaging contention among applications through explicit partitioning of the cache among co-running applications for throughput or fairness improvement <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b31",1
"the destructive interference of high-demand low-reuse threads on other threads, nor are they adept in handling thrashing or scan-type access streams <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" target=""#b31",0
"igm does not work well for task-parallel programs running on higher number of on-chip cores with fine-grained tasks and dynamic task-core assignments <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b8"">9]</ref>. Tasks have much shorter lifetimes",0
"ultithreaded programs with static thread assignments and mapping, with an aim to ensure balanced progress for all threads while optimizing throughput <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b26"">27]</ref>.</p><p>In this work we focus on shared LLC managment for an alternat ttempt to partition the ways of the shared LLC among co-running threads in order to maximize throughput and achieve balanced progress for all threads <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" target=""#b31"">32]</ref>. We find that such thread-ba ded Workloads</head><p>Muralidhara et al. investigate the problem of partitioning a shared L2 cache among the threads of a multi-threaded application <ref type=""bibr"" target=""#b25"">[26]</ref>. The authors proposed a dynamic partitioning scheme that focuses on making the slowest running thread (criti a dynamic partitioning scheme that focuses on making the slowest running thread (critical path thread) faster so that the application becomes faster <ref type=""bibr"" target=""#b25"">[26]</ref>. The proposed technique involves dividing the entire execution time into equally spaced intervals of dynamic",0
"oited the information tracked by the runtime to improve the efficiency of hardware optimizations such as prefetching and coherence for private caches <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b27"">28]</ref>.</p><p>Figure <ref type=""figure"">1</ref> illustrates the two competi the runtime to guide coherence optimizations such as downgrading and self-invalidation in order to improve performance of task-parallel applications <ref type=""bibr"" target=""#b23"">[24]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""9."">CONCLUSION</head><p>In this paper, we propo",0
"ving the efficiency of paging in the following three ways. 1. Multipage mappings use one TLB entry to map multiple pages (e.g., 8-16 pages per entry) <ref type=""bibr"" target=""#b36"">[38,</ref><ref type=""bibr"" target=""#b37"">39,</ref><ref type=""bibr"" target=""#b45"">47]</ref>. Mapping multiple pages per huge pages must have size-aligned physical addresses, and thus the OS can only allocate them when the available memory is size-aligned and contiguous <ref type=""bibr"" target=""#b36"">[38,</ref><ref type=""bibr"" target=""#b37"">39]</ref>. In addition, many commodity processors provide limited numbers of l le eager paging algorithm generates large range translations for a variety of block sizes and exploits the clustering behavior of the buddy allocator <ref type=""bibr"" target=""#b36"">[38,</ref><ref type=""bibr"" target=""#b37"">39]</ref>.</p><p>Finally, eager paging is only effective when memory fragmenta ing approaches, such as sub-blocked TLBs <ref type=""bibr"" target=""#b45"">[47]</ref>, CoLT <ref type=""bibr"" target=""#b37"">[39]</ref> and Clustered TLBs <ref type=""bibr"" target=""#b36"">[38]</ref>, pack multiple Page  multiple of translations (e.g., 8-16) per entry, which limits their potential to reduce ages with libhugetlbfs using hardware performance counters. (ii) We emulate multipage mappings in BadgerTrap. We implement the Clustered TLB approach <ref type=""bibr"" target=""#b36"">[38]</ref> of Pham et al., configured with 512 fullyassociative entries. Each entry indexes up to an 8-page cluster, sh b36"">[38]</ref> of Pham et al., configured with 512 fullyassociative entries. Each entry indexes up to an 8-page cluster, shown best by Clustered TLB <ref type=""bibr"" target=""#b36"">[38]</ref>. We use eager paging to increase the opportunities to form multipages, improving on the original implementat d overhead for the three different page sizes available on x86-64 processors. All other configurations are emulated. The CTLB bars show Clustered TLB <ref type=""bibr"" target=""#b36"">[38]</ref> results. The DS bars show direct segments <ref type=""bibr"" target=""#b8"">[10]</ref> results and the RMM bars ork on multipage mappings (sub-blocked TLBs <ref type=""bibr"" target=""#b45"">[47]</ref>, CoLT <ref type=""bibr"" target=""#b37"">[39]</ref>, Clustered TLBs <ref type=""bibr"" target=""#b36"">[38]</ref>), huge pages <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b4"">6,</ref><ref type=""bibr"" t",1
"ly-associative copies of direct segment's base/limit/offset logic <ref type=""bibr"" target=""#b8"">[10]</ref> or a simplified version of the range cache <ref type=""bibr"" target=""#b46"">[48]</ref>: it performs two comparisons per entry instead of a single equality test. Our design can achieve this perfor the system.</p><p>Finally, our proposed architecture resembles prior works in fine-grained memory protection <ref type=""bibr"" target=""#b22"">[24,</ref><ref type=""bibr"" target=""#b46"">48,</ref><ref type=""bibr"" target=""#b47"">49]</ref>, in the sense that both exploit range behavior. However, instead of e",0
"ef type=""bibr"" target=""#b8"">[10,</ref><ref type=""bibr"" target=""#b37"">39]</ref>, or partial compaction with techniques adapted from garbage collection <ref type=""bibr"" target=""#b15"">[17,</ref><ref type=""bibr"" target=""#b16"">18]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""7."">Met",0
"segmentation to implement virtual memory. The Burroughs B5000 <ref type=""bibr"" target=""#b31"">[33]</ref> was an early user of pure segments. The 8086 <ref type=""bibr"" target=""#b1"">[2]</ref> and iAPX 432 <ref type=""bibr"" target=""#b24"">[26]</ref> processors also supported pure segmentation without pag",0
"ation.</p><p>Prior work also proposes virtual caches to reduce the performance and energy overheads of the TLB by only translating after a cache miss <ref type=""bibr"" target=""#b9"">[11,</ref><ref type=""bibr"" target=""#b27"">29,</ref><ref type=""bibr"" target=""#b48"">50]</ref>. However for those workloads",0
"s not designed for networks (e.g., <ref type=""bibr"" target=""#b27"">[1]</ref>) or lack a clear objective function tailored for network embedding (e.g., <ref type=""bibr"" target=""#b42"">[16]</ref>). We anticipate that a new model with a carefully designed objective function that preserves properties of t ected graphs while the proposed model is applicable for both undirected and directed graphs.</p><p>The most recent work related with ours is DeepWalk <ref type=""bibr"" target=""#b42"">[16]</ref>, which deploys a truncated random walk for social network embedding. Although empirically effective, the Dee kr and Youtube<ref type=""foot"" target=""#foot_0"">2</ref> . The Flickr network is denser than the Youtube network (the same network as used in DeepWalk <ref type=""bibr"" target=""#b42"">[16]</ref>). (3) Citation Networks. Two types of citation networks are used: an author citation network and a paper cit or graph factorization.</p><p>An information network can be represented as an affinity matrix, and is able to represent each vertex with a • DeepWalk <ref type=""bibr"" target=""#b42"">[16]</ref>. DeepWalk is an approach recently proposed for social network embedding, which is only applicable for networ t to 200, as used in word embedding <ref type=""bibr"" target=""#b39"">[13]</ref>. For other networks, the dimension is set as 128 by default, as used in <ref type=""bibr"" target=""#b42"">[16]</ref>. Other default settings include: the number of negative samples K = 5 for LINE and </p></div> <div xmlns=""ht",1
"ve been conducted for Facebook<ref type=""foot"" target=""#foot_0"">3</ref>  <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b12"">13]</ref>. They analyzed the relationship between internal properties such as self-construction <ref type=""bibr"" target l properties such as self-construction <ref type=""bibr"" target=""#b4"">[5]</ref>, narcissism <ref type=""bibr"" target=""#b6"">[7]</ref>, self-presentation <ref type=""bibr"" target=""#b12"">[13]</ref> and the corresponding user profiles on Facebook.</p><p>We assume that a profile image is influenced by a use",1
"ciate Twitter users to a number of topics and based on the results recommended users with similar interests on the Twitter network to the target user <ref type=""bibr"" target=""#b8"">[9]</ref>. Ramage et al. improved the accuracy of LDA based user recommendation by considering hashtags(#), replies (@),",0
"racteristics of Twitter are unique; therefore, it has attracted considerable research interest in recent years <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b9"">10]</ref>, which are known as user profiling studies.</p><p>When users join Twitte so on. Researchers have profiled users based on their introductory information or the content of their tweets <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref>, or based on the name, location, and sex of the user <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" ta",0
"al media services, the features and characteristics of Twitter are unique; therefore, it has attracted considerable research interest in recent years <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b9"">10]</ref>, which are known as user profilin mage, some introductory information and so on. Researchers have profiled users based on their introductory information or the content of their tweets <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref>, or based on the name, location, and sex of the user <ref type=""bibr"" ta nt of their tweets <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref>, or based on the name, location, and sex of the user <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b3"">4]</ref>. The reason behind using the aforementioned features for user profiling",0
"itter users, and stated that links on Twitter do not represent social relationships in the real world but represent information sharing relationships <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b7"">8]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3."">WHY PRO",0
"ources <ref type=""bibr"" target=""#b19"">[21]</ref>. Usage-based pricing can affect overall demand levels, but does not even out short-term fluctuations <ref type=""bibr"" target=""#b11"">[13]</ref>. To manage these fluctuations in demand for a fixed amount of available datacenter capacity, cloud providers is less likely to be interrupted and will therefore have a shorter expected running time. Job interruptibility. We can use the expected running time <ref type=""bibr"" target=""#b11"">(13)</ref> to observe the effect of the recovery time parameter, t r , on a job's feasibility for spot instances. Intui t; min p t k 1−Fπ(p) = t k , and a spot instance is feasible at any price.</p><p>The optimal bid price. We can now multiply the expected running time <ref type=""bibr"" target=""#b11"">(13)</ref> with the expected spot price <ref type=""bibr"" target=""#b7"">(9)</ref> to find that the cost of a job with a p ptible. We use p to denote the optimal bid price to <ref type=""bibr"" target=""#b13"">(15)</ref>.</p><p>We now observe that the expected running time in <ref type=""bibr"" target=""#b11"">(13)</ref> decreases with the bid price, while the expected spot price increases with the bid price. We find that the e +t o ,</formula><p>i.e., it is the sum of the recovery, execution, and overhead times. Hence, we can extend the result for a single persistent bid in <ref type=""bibr"" target=""#b11"">(13)</ref> as</p><formula xml:id=""formula_32"">M i=1 T i F π (p) = t s + t o − M t r 1 − tr t k 1 − F π (p) . (<label>17",1
"al understanding of Amazon's prevailing spot prices by studying the two-month history made available by Amazon <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b13"">15]</ref>. Though these statistics alone are sufficient for the user to deduce a bid price, they also give us some insi -specific works on both provider and user actions are limited to statistical studies of historical spot prices <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b13"">15]</ref>.</p><p>Game theoretic pricing. Spot pricing is a distributed solution to the problem of allocating cloud reso prices offered by the provider. Complicating this tradeoff is the fact that user jobs can have long runtimes spanning many changes in the spot price <ref type=""bibr"" target=""#b13"">[15]</ref>. Users then face two key challenges: 1) Users must predict the spot prices in order to optimize their bids f b on an on-demand instance, and the second constraint ensures that the job is sufficiently interruptible. We use p to denote the optimal bid price to <ref type=""bibr"" target=""#b13"">(15)</ref>.</p><p>We now observe that the expected running time in <ref type=""bibr"" target=""#b11"">(13)</ref> decreases Proposition 5. If the probability density function of the spot price monotonically decreases, i.e., F π (p) is concave, the optimal bid price solving <ref type=""bibr"" target=""#b13"">(15)</ref> </p><formula xml:id=""formula_29"">is p = ψ −1 t k t r − 1 ,<label>(16)</label></formula><p>where ψ −1 (•) is n that of running the job on an on-demand instance. Comparing <ref type=""bibr"" target=""#b17"">(19)</ref> to bidding for a single persistent request in <ref type=""bibr"" target=""#b13"">(15)</ref>, we see that <ref type=""bibr"" target=""#b17"">(19)</ref> can be solved similarly to <ref type=""bibr"" target=""# gle persistent request in <ref type=""bibr"" target=""#b13"">(15)</ref>, we see that <ref type=""bibr"" target=""#b17"">(19)</ref> can be solved similarly to <ref type=""bibr"" target=""#b13"">(15)</ref> in Proposition 5.</p><p>By comparing the costs for multiple bids and for a single bid at the optimal bid pri easible set is the larger one of π and F −1 π 1 − t k ts .</p><p>Proof of Proposition 5.</p><p>Proof. By taking the first-order derivative of Φ(p) in <ref type=""bibr"" target=""#b13"">(15)</ref>  <ref type=""figure"" target=""#fig_3"">3</ref>), F π (p) is concave and F π (p)− pf π (p) ≥ 0. We then have ∂g(",1
"r smart grid electricity <ref type=""bibr"" target=""#b7"">[9]</ref>, secondary spectrum access <ref type=""bibr"" target=""#b14"">[16]</ref>, grid computing <ref type=""bibr"" target=""#b17"">[19]</ref>, or Internet data <ref type=""bibr"" target=""#b22"">[24]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1 ""formula_41"">)</formula><p>where the first constraint ensures that the cost is lower than that of running the job on an on-demand instance. Comparing <ref type=""bibr"" target=""#b17"">(19)</ref> to bidding for a single persistent request in <ref type=""bibr"" target=""#b13"">(15)</ref>, we see that <ref ty paring <ref type=""bibr"" target=""#b17"">(19)</ref> to bidding for a single persistent request in <ref type=""bibr"" target=""#b13"">(15)</ref>, we see that <ref type=""bibr"" target=""#b17"">(19)</ref> can be solved similarly to <ref type=""bibr"" target=""#b13"">(15)</ref> in Proposition 5.</p><p>By comparing th p m and p v respectively as the optimal bid prices for a one-time single instance request (Proposition 4) and for multiple persistent requests (as in <ref type=""bibr"" target=""#b17"">(19)</ref>). The first constraint is satisfied if the user submits sufficiently many simultaneous requests for the slav",0
"esources are priced according to real-time market demand <ref type=""bibr"" target=""#b38"">[40]</ref>. Amazon's Elastic Compute Cloud (EC2) spot pricing <ref type=""bibr"" target=""#b1"">[2]</ref> is an example of such a strategy.</p><p>Spot pricing creates an auction-based market for available cloud compu onsider Infrastructure-asa-service (IaaS) cloud services, which are essentially remote virtual machines (VMs) with CPU, memory, and storage resources <ref type=""bibr"" target=""#b1"">[2]</ref>. We follow Amazon's terminology and use the term ""instance"" to denote the use of a single VM. Instances can be",0
"ch less studied than provider strategies. While some works have shown that users can reduce their costs by using spot rather than on-demand instances <ref type=""bibr"" target=""#b26"">[28,</ref><ref type=""bibr"" target=""#b35"">37]</ref>, they only consider heuristic bidding strategies for single-instance",0
"e it to empirical bid prices and use it to develop bidding strategies for users. Joint userprovider interactions for cloud services are considered in <ref type=""bibr"" target=""#b24"">[26]</ref>, but auction-specific works on both provider and user actions are limited to statistical studies of historic",0
"'s revenue <ref type=""bibr"" target=""#b8"">[10,</ref><ref type=""bibr"" target=""#b15"">17,</ref><ref type=""bibr"" target=""#b32"">34]</ref> or social welfare <ref type=""bibr"" target=""#b23"">[25,</ref><ref type=""bibr"" target=""#b36"">38]</ref>. We construct a similar model but relate it to empirical bid prices users' qualityof-service. We could more explicitly account for user satisfaction by taking the social welfare to be the provider's objective function <ref type=""bibr"" target=""#b23"">[25,</ref><ref type=""bibr"" target=""#b36"">38]</ref>. While our current formulation matches well with the observed spot p",0
"ids in a resource auction. More generally, auctions have been proposed as a solution to generic distributed allocation games over multiple time slots <ref type=""bibr"" target=""#b18"">[20]</ref>. Users' optimal bidding strategies in such auctions can be quite complex, particularly if multiple users try m π (•) and F v π (•) denote the spot prices' cumulative distribution functions for the master and slave node instance types. The first constraint in <ref type=""bibr"" target=""#b18"">(20)</ref> ensures that the master node runs longer than any of the slave nodes (cf. ( <ref type=""formula"" target=""#for",0
"uling jobs within a datacenter <ref type=""bibr"" target=""#b6"">[8,</ref><ref type=""bibr"" target=""#b12"">14,</ref><ref type=""bibr"" target=""#b20"">22,</ref><ref type=""bibr"" target=""#b31"">33]</ref>, most have taken user demands as a given input. Yet user demand can be partially controlled by cloud provider purely operational perspective <ref type=""bibr"" target=""#b6"">[8,</ref><ref type=""bibr"" target=""#b12"">14,</ref><ref type=""bibr"" target=""#b20"">22,</ref><ref type=""bibr"" target=""#b31"">33]</ref>. Others incorporate pricing considerations, e.g., dynamically allocating cloud resources, so as to maximize t",0
"emand</head><p>While many works have considered the operational problem of scheduling jobs within a datacenter <ref type=""bibr"" target=""#b6"">[8,</ref><ref type=""bibr"" target=""#b12"">14,</ref><ref type=""bibr"" target=""#b20"">22,</ref><ref type=""bibr"" target=""#b31"">33]</ref>, most have taken user demands and pricing. Many works have considered resource allocation in the cloud from a purely operational perspective <ref type=""bibr"" target=""#b6"">[8,</ref><ref type=""bibr"" target=""#b12"">14,</ref><ref type=""bibr"" target=""#b20"">22,</ref><ref type=""bibr"" target=""#b31"">33]</ref>. Others incorporate pricing c",0
", this GMM-free approach still requires iterative procedures such as generating forced alignments and decision trees. Meanwhile, another line of work <ref type=""bibr"" target=""#b6"">[6,</ref><ref type=""bibr"" target=""#b7"">7,</ref><ref type=""bibr"" target=""#b8"">8,</ref><ref type=""bibr"" target=""#b9"">9,</r ionist temporal classification (CTC) objective function to infer speech-label alignments automatically. This CTC technique is further investigated in <ref type=""bibr"" target=""#b6"">[6,</ref><ref type=""bibr"" target=""#b7"">7,</ref><ref type=""bibr"" target=""#b8"">8,</ref><ref type=""bibr"" target=""#b14"">14]< two major obstacles. First, it is challenging to incorporate lexicons and language models into decoding. When decoding CTC-trained models, past work <ref type=""bibr"" target=""#b6"">[6,</ref><ref type=""bibr"" target=""#b8"">8,</ref><ref type=""bibr"" target=""#b10"">10]</ref> has successfully constrained sea r experiments with the Wall Street Journal (WSJ) benchmark show that Eesen results in superior performance than the existing end-to-end ASR pipelines <ref type=""bibr"" target=""#b6"">[6,</ref><ref type=""bibr"" target=""#b8"">8]</ref>. The WERs of Eesen are on a par with strong hybrid HMM/DNN baselines. Mo ch, the RNN model in our Eesen framework is not trained using frame-level labels with respect to the cross-entropy (CE) criterion. Instead, following <ref type=""bibr"" target=""#b6"">[6,</ref><ref type=""bibr"" target=""#b8"">8,</ref><ref type=""bibr"" target=""#b10"">10]</ref>, we adopt the CTC objective <ref G</head></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.1."">Decoding with WFSTs</head><p>Previous work has introduced a variety of methods <ref type=""bibr"" target=""#b6"">[6,</ref><ref type=""bibr"" target=""#b8"">8,</ref><ref type=""bibr"" target=""#b10"">10]</ref> to decode CTC-trained models. Th WSJ standard pruned trigram language model in the ARPA format (which we will consistently refer to as standard). To be consistent with previous work <ref type=""bibr"" target=""#b6"">[6,</ref><ref type=""bibr"" target=""#b8"">8]</ref>, we report our results on the eval92 set. Our experimental setup has bee m is reduced to 7.34%.</p><p>Table <ref type=""table"">3</ref> lists the results of end-to-end ASR systems that have been reported in the previous work <ref type=""bibr"" target=""#b6"">[6,</ref><ref type=""bibr"" target=""#b8"">8]</ref> and on the same dataset. Our Eesen framework outperforms both <ref type= marking. End-to-end systems described in the literature differ not only in their model architectures but also in their decoding methods. For example, <ref type=""bibr"" target=""#b6"">[6]</ref> and <ref type=""bibr"" target=""#b8"">[8]</ref> adopt two distinct versions of beam search for decoding CTC models evel language models <ref type=""bibr"" target=""#b10"">[10]</ref> or achieve the integration under constrained conditions (e.g., nbest list rescoring in <ref type=""bibr"" target=""#b6"">[6]</ref>). In this work, we propose a generalized decoding approach based on WFSTs <ref type=""bibr"" target=""#b28"">[27,< %, with both the lexicon and the language model used in decoding. When only the lexicon is used, our decoding behaves similarly as the beam search in <ref type=""bibr"" target=""#b6"">[6]</ref>. In this case, the WER rises quickly to 26.92%. This obvious degradation reveals the effectiveness of our deco <ref type=""table"">3</ref> shows that with the standard language model, the character-based system gets the WER of 9.07%. CTC experiments in past work <ref type=""bibr"" target=""#b6"">[6]</ref> have adopted an expanded vocabulary, and re-trained the language model using text data released together with ious work <ref type=""bibr"" target=""#b6"">[6,</ref><ref type=""bibr"" target=""#b8"">8]</ref> and on the same dataset. Our Eesen framework outperforms both <ref type=""bibr"" target=""#b6"">[6]</ref> and <ref type=""bibr"" target=""#b8"">[8]</ref> in terms of WERs on the testing set. It is worth pointing out that b6"">[6]</ref> and <ref type=""bibr"" target=""#b8"">[8]</ref> in terms of WERs on the testing set. It is worth pointing out that the 8.7% WER reported in <ref type=""bibr"" target=""#b6"">[6]</ref> is obtained not in a purely end-to-end manner. Instead, the authors of <ref type=""bibr"" target=""#b6"">[6]</ref> ting out that the 8.7% WER reported in <ref type=""bibr"" target=""#b6"">[6]</ref> is obtained not in a purely end-to-end manner. Instead, the authors of <ref type=""bibr"" target=""#b6"">[6]</ref> generate a nbest list of hypotheses from a hybrid DNN model, and apply the CTC model to rescore the hypotheses >In our future work, we plan to further improve the WERs of Eesen systems via more advanced learning techniques (e.g., expected transcription loss in <ref type=""bibr"" target=""#b6"">[6]</ref>) and alternative decoding approach (e.g., dynamic decoders <ref type=""bibr"" target=""#b32"">[31]</ref>). Also, w",1
"s. Meanwhile, another line of work <ref type=""bibr"" target=""#b6"">[6,</ref><ref type=""bibr"" target=""#b7"">7,</ref><ref type=""bibr"" target=""#b8"">8,</ref><ref type=""bibr"" target=""#b9"">9,</ref><ref type=""bibr"" target=""#b10"">10,</ref><ref type=""bibr"" target=""#b11"">11,</ref><ref type=""bibr"" target=""#b12"">1",1
"ur Eesen framework. Acoustic modeling in Eesen is viewed as a sequence-to-sequence learning problem. We exploit deep recurrent neural networks (RNNs) <ref type=""bibr"" target=""#b16"">[15,</ref><ref type=""bibr"" target=""#b17"">16]</ref> as the acoustic models, and the Long Short-Term Memory (LSTM) units",1
"ce of ASR has been improved dramatically by the introduction of deep neural networks (DNNs) as acoustic models <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b2"">3]</ref>. In the hybrid HMM/DNN approach, DNNs are used to classify speech frames",0
"d architecture as the acoustic model. However, combing LSTMs with other network structures, e.g., time-delay <ref type=""bibr"" target=""#b24"">[23,</ref><ref type=""bibr"" target=""#b25"">24]</ref> or convolutional neural networks <ref type=""bibr"" target=""#b26"">[25,</ref><ref type=""bibr"" target=""#b20"">19]<",0
"ation. Fig. <ref type=""figure"" target=""#fig_0"">1</ref> depicts the structure of the LSTM units we use. The blue curves represent peephole connections <ref type=""bibr"" target=""#b23"">[22]</ref> that link the memory cells to the gates to learn precise timing of the outputs. The computation at the time",0
"gation through time (BPTT). In practice, training RNNs to learn long-term temporal dependency can be difficult due to the vanishing gradients problem <ref type=""bibr"" target=""#b22"">[21]</ref>. To overcome this issue, we apply the LSTM units <ref type=""bibr"" target=""#b18"">[17]</ref> as the building b",0
"Eesen is viewed as a sequence-to-sequence learning problem. We exploit deep recurrent neural networks (RNNs) <ref type=""bibr"" target=""#b16"">[15,</ref><ref type=""bibr"" target=""#b17"">16]</ref> as the acoustic models, and the Long Short-Term Memory (LSTM) units <ref type=""bibr"" target=""#b18"">[17,</ref>",0
". Our decoding method represents the CTC labels, lexicons and language models as separate WFSTs. Using highly-optimized FST libraries such as OpenFST <ref type=""bibr"" target=""#b30"">[29]</ref>, we can fuse the WFSTs efficiently into a single search graph. Building of the individual WFSTs is described",0
"type=""bibr"" target=""#b7"">7,</ref><ref type=""bibr"" target=""#b8"">8,</ref><ref type=""bibr"" target=""#b9"">9,</ref><ref type=""bibr"" target=""#b10"">10,</ref><ref type=""bibr"" target=""#b11"">11,</ref><ref type=""bibr"" target=""#b12"">12]</ref> has focused on end-to-end ASR, i.e., modeling the mapping between spe",0
"type=""bibr"" target=""#b6"">[6]</ref>). In this work, we propose a generalized decoding approach based on WFSTs <ref type=""bibr"" target=""#b28"">[27,</ref><ref type=""bibr"" target=""#b29"">28]</ref>. A WFST is a finite-state acceptor (FSA) in which each transition has an input symbol, an output symbol and a representation, CTC decoding in principle can leverage any language models that can be converted into WFSTs. Following conventions in the literature <ref type=""bibr"" target=""#b29"">[28]</ref>, the language model WFST is denoted as G.</p><p>Lexicon. A lexicon WFST encodes the mapping from sequences o 0"">1</ref> shows a comparison between Eesen and a hybrid HMM/DNN system. The hybrid system is constructed by following the standard Kaldi recipe ""s5"" <ref type=""bibr"" target=""#b29"">[28]</ref>. Inputs of the DNN model are 11 neighboring frames of filterbank features. The DNN has 6 hidden layers and 1",0
"acoustic modeling in Eesen cannot leverage speaker adapted front-ends. We will study new speaker adaptation <ref type=""bibr"" target=""#b36"">[35,</ref><ref type=""bibr"" target=""#b37"">36]</ref> and adaptive training <ref type=""bibr"" target=""#b38"">[37,</ref><ref type=""bibr"" target=""#b39"">38]</ref> techn",0
"Gaussians in the GMM models.</p><p>Previous work has made various attempts to reduce the complexity of ASR. In <ref type=""bibr"" target=""#b4"">[4,</ref><ref type=""bibr"" target=""#b5"">5]</ref>, researchers propose to flatstart DNNs and thus get ride of GMM models. However, this GMM-free approach still r",0
"f type=""bibr"" target=""#b6"">[6,</ref><ref type=""bibr"" target=""#b7"">7,</ref><ref type=""bibr"" target=""#b8"">8,</ref><ref type=""bibr"" target=""#b9"">9,</ref><ref type=""bibr"" target=""#b10"">10,</ref><ref type=""bibr"" target=""#b11"">11,</ref><ref type=""bibr"" target=""#b12"">12]</ref> has focused on end-to-end ASR nguage models into decoding. When decoding CTC-trained models, past work <ref type=""bibr"" target=""#b6"">[6,</ref><ref type=""bibr"" target=""#b8"">8,</ref><ref type=""bibr"" target=""#b10"">10]</ref> has successfully constrained search paths with lexicons. However, how to integrate word-level language models els with respect to the cross-entropy (CE) criterion. Instead, following <ref type=""bibr"" target=""#b6"">[6,</ref><ref type=""bibr"" target=""#b8"">8,</ref><ref type=""bibr"" target=""#b10"">10]</ref>, we adopt the CTC objective <ref type=""bibr"" target=""#b13"">[13]</ref> to automatically learn the alignments b ng with WFSTs</head><p>Previous work has introduced a variety of methods <ref type=""bibr"" target=""#b6"">[6,</ref><ref type=""bibr"" target=""#b8"">8,</ref><ref type=""bibr"" target=""#b10"">10]</ref> to decode CTC-trained models. These methods, however, either fail to integrate word-level language models <re successfully constrained search paths with lexicons. However, how to integrate word-level language models efficiently still is an unanswered question <ref type=""bibr"" target=""#b10"">[10]</ref>. Second, the community lacks a shared experimental platform for the purpose of benchmarking. End-to-end syst ef><ref type=""bibr"" target=""#b10"">10]</ref> to decode CTC-trained models. These methods, however, either fail to integrate word-level language models <ref type=""bibr"" target=""#b10"">[10]</ref> or achieve the integration under constrained conditions (e.g., nbest list rescoring in <ref type=""bibr"" targ",0
"more than 2x <ref type=""bibr"" target=""#b1"">[1]</ref>.</p><p>To address this disparity, researchers have designed heterogeneous multi-core processors <ref type=""bibr"" target=""#b2"">[2]</ref> in which an application is mapped to the most efficient core that meets its performance requirements. ARM's bi nularity of application migration amongst them. Higher switching costs of coarse-grained heterogeneous systems <ref type=""bibr"" target=""#b1"">[1,</ref><ref type=""bibr"" target=""#b2"">2]</ref> enforce switching granularities of the order of milli-seconds. Novel architectures minimize migration costs by le has the same issue width and functional units as big</p></note> 			<note xmlns=""http://www.tei-c.org/ns/1.0"" place=""foot"" n=""2"" xml:id=""foot_1""><p><ref type=""bibr"" target=""#b2"">2</ref> OinO stands for an InO core appearing to be OoO</p></note> 			<note xmlns=""http://www.tei-c.org/ns/1.0"" place=""f",1
"InO core, provided it used a bestcase OoO schedule for each trace and perfectly predictable control-flow. These results corroborate with recent work <ref type=""bibr"" target=""#b8"">[8]</ref> that credit the OoO's ability to create good static schedules as the main reason for the performance advantage",1
"r all interleaving conditional forward branches (TraceID). This definition is synonymous with previous works <ref type=""bibr"" target=""#b14"">[14,</ref><ref type=""bibr"" target=""#b15"">15]</ref> and represents a sequence of instructions or basic blocks that have a high likelihood of appearing together.",0
"n-Order (InO) cores have lower complexity, allowing them to be significantly more energy efficient (3.5x) than OoO, but at a slowdown of more than 2x <ref type=""bibr"" target=""#b1"">[1]</ref>.</p><p>To address this disparity, researchers have designed heterogeneous multi-core processors <ref type=""bib =""bibr"" target=""#b2"">[2]</ref> in which an application is mapped to the most efficient core that meets its performance requirements. ARM's big.LITTLE <ref type=""bibr"" target=""#b1"">[1]</ref> combines a high-performance big (OoO) core and a lowperformance but energy-efficient little (InO) core. This a f type=""bibr"" target=""#b24"">24]</ref>, and heterogeneous ISAs <ref type=""bibr"" target=""#b25"">[25]</ref>. Commercial products include ARM's big.LITTLE <ref type=""bibr"" target=""#b1"">[1]</ref> and NVidia's Kal-El <ref type=""bibr"" target=""#b26"">[26]</ref> which combine high and low performance general p eterogeneous multicore dictates the granularity of application migration amongst them. Higher switching costs of coarse-grained heterogeneous systems <ref type=""bibr"" target=""#b1"">[1,</ref><ref type=""bibr"" target=""#b2"">2]</ref> enforce switching granularities of the order of milli-seconds. Novel arc",0
"f>, specialized hardware for specific codes <ref type=""bibr"" target=""#b23"">[23,</ref><ref type=""bibr"" target=""#b24"">24]</ref>, and heterogeneous ISAs <ref type=""bibr"" target=""#b25"">[25]</ref>. Commercial products include ARM's big.LITTLE <ref type=""bibr"" target=""#b1"">[1]</ref> and NVidia's Kal-El <r",0
"3]</ref>, thereby reducing the overall energy consumption. Prior work has proposed heterogeneous architectures <ref type=""bibr"" target=""#b4"">[4,</ref><ref type=""bibr"" target=""#b5"">5,</ref><ref type=""bibr"" target=""#b6"">6,</ref><ref type=""bibr"" target=""#b7"">7]</ref> that enable fast switching between in energy of little. Overall, DynaMOS contributes energy savings of 32% as compared to having only a big core, a 2.2x increase over state-of-the-art <ref type=""bibr"" target=""#b5"">[5]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2."">MOTIVATION</head><p>A superscalar big's abili table schedules and stores them in a Schedule Trace-Cache (STC) <ref type=""bibr"" target=""#b14"">[14]</ref> (Section 3.3).</p><p>? An online controller <ref type=""bibr"" target=""#b5"">[5]</ref> (Section 3.4) observes a sequence of traces in the current context and migrates execution on the little if it sulting in high hit rates.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.4"">Controller</head><p>We adopt the controller of Padmanabha <ref type=""bibr"" target=""#b5"">[5]</ref> to decide which core, big or little, should execute the current trace to maximize efficiency. The controller m sy; a linear regression model (trained offline using SPEC's train data set) is used to estimate its performance on the other core, similar to that in <ref type=""bibr"" target=""#b5"">[5]</ref>. Performance metrics of a trace, such as cache misses, branch mispredicts, ILP, MLP, its memoizability quotien ><p>We see that with practical constraints, DynaMOS is able to run 37% of its instructions on little, which is a  2.9x increase over prior work (13%) <ref type=""bibr"" target=""#b5"">[5]</ref>. This is due to OinO mode's comparable performance to the big, allowing more instructions to run on little wit aMOS as compared to running only on big. Overheads due to leakage from the inactive core (which is clockgated) and those imposed by the CC controller <ref type=""bibr"" target=""#b5"">[5]</ref> are included. Including OinO mode on little adds around 8% energy overhead to an InO, from accessing a bigger chitecture is targeted toward low-power cores, we use low-leakage technology with low operating power, minimizing this overhead to &lt;4%. Prior work <ref type=""bibr"" target=""#b5"">[5]</ref> estimates the controller to incur overheads of 1.875kB storage and 0.033W power, while the performance estimat g/ns/1.0""><head n=""5."">RESULTS</head><p>This section provides quantitative benefits of DynaMOS and compares them to oracular knowledge and prior work <ref type=""bibr"" target=""#b5"">[5,</ref><ref type=""bibr"" target=""#b10"">10]</ref>, along with intuitions as to why some benchmarks are more amenable to",0
"ne <ref type=""bibr"">[9,</ref><ref type=""bibr"" target=""#b10"">10]</ref> or on different, specialized pipelines <ref type=""bibr"" target=""#b11"">[11,</ref><ref type=""bibr"" target=""#b12"">12]</ref>.</p><p>We overcome several design challenges en route to enabling DynaMOS . First, false register dependencie cient execution on the InO mode.</p><p>Other works propose using two pipelines: one for trace generation and another for trace execution. Turboscalar <ref type=""bibr"" target=""#b12"">[12]</ref> has a thin cold pipeline that discovers ILP over a long instruction window and a fat hot pipeline to exploit",0
"ltage and frequency of the core (DVFS) to improve the core's energy efficiency at the expense of performance <ref type=""bibr"" target=""#b29"">[29,</ref><ref type=""bibr"" target=""#b30"">30]</ref>. Recent work <ref type=""bibr"" target=""#b31"">[31]</ref> shows that microarchitectural heterogeneity, which can",0
"rmance benefits either on the same pipeline <ref type=""bibr"">[9,</ref><ref type=""bibr"" target=""#b10"">10]</ref> or on different, specialized pipelines <ref type=""bibr"" target=""#b11"">[11,</ref><ref type=""bibr"" target=""#b12"">12]</ref>.</p><p>We overcome several design challenges en route to enabling Dy correct data-flow across traces seamlessly, the OinO performs Level-2 renaming, using a  modified version of the rotational remap scheme used by DIF <ref type=""bibr"" target=""#b11"">[11]</ref>. Each AR is mapped to a fixed size circular buffer of PRs, shown as the Physical Register File (PRF) in Figu scovers ILP over a long instruction window and a fat hot pipeline to exploit this knowledge. Perhaps the work that most closely resembles ours is DIF <ref type=""bibr"" target=""#b11"">[11]</ref>. They use a primary OoO engine to create and cache schedules, which are then formatted as a VLIW and made ac",0
"/ref><ref type=""bibr"" target=""#b23"">23]</ref> has primarily focused on intra-line compression to minimize decompression latency. Even the recent work <ref type=""bibr"" target=""#b24"">[24]</ref> which does compress across cache lines, is optimized for single-stream performance.</p><p>In contrast, this type=""bibr"" target=""#b13"">[13]</ref>.</p><p>Another metric to measure area overhead is dictionary size. C-Pack in Adaptive and Decoupled requires    <ref type=""bibr"" target=""#b24"">[24]</ref>. As MORC allocates 512-bytes for each compression and decompression engine, the basic single-log implementat ompressed caches, we also compare to Adaptive <ref type=""bibr"" target=""#b18"">[18]</ref>, Decoupled <ref type=""bibr"" target=""#b19"">[19]</ref>, and SC2 <ref type=""bibr"" target=""#b24"">[24]</ref> compressed caches. These schemes are evaluated with perfect LRU replacement policy. For fairness, both Adapt ork has shown that compressed caches are more efficient than larger uncompressed caches thanks to significantly lower static and dynamic energy power <ref type=""bibr"" target=""#b24"">[24]</ref>. Figure <ref type=""figure"" target=""#fig_15"">9a</ref> compares the memory subsystem energy, including compres ref><ref type=""bibr"" target=""#b19"">19,</ref><ref type=""bibr"" target=""#b20"">20,</ref><ref type=""bibr"" target=""#b22"">22]</ref>. While one recent scheme <ref type=""bibr"" target=""#b24"">[24]</ref> compresses inter-line, it still prioritizes single-stream performance over throughput. In contrast, MORC is -of-breed cache compression schemes: Adaptive <ref type=""bibr"" target=""#b18"">[18]</ref>, Decoupled <ref type=""bibr"" target=""#b19"">[19]</ref>, and SC2 <ref type=""bibr"" target=""#b24"">[24]</ref>.</p><p>Adaptive <ref type=""bibr"" target=""#b18"">[18]</ref> uses a basic cache organization with sets and ways 2"">[42]</ref> has similar performance to Decoupled, but is designed to be easier to implement <ref type=""bibr"" target=""#b42"">[42]</ref>. Finally, SC2 <ref type=""bibr"" target=""#b24"">[24]</ref> is most similar to MORC because it maintains a system-wide dictionary which can compress data across cache l",1
"ncy. To sustain the growth of these applications, manycore architectures <ref type=""bibr"" target=""#b3"">[3,</ref><ref type=""bibr"" target=""#b4"">4,</ref><ref type=""bibr"" target=""#b5"">5,</ref><ref type=""bibr"" target=""#b6"">6,</ref><ref type=""bibr"" target=""#b7"">7,</ref><ref type=""bibr"" target=""#b8"">8,</re",0
"r"" target=""#b13"">[13]</ref> 4pJ 2x 64b floating point op (45nm) <ref type=""bibr"" target=""#b14"">[14]</ref> 45pJ 22.5x 64b transfer across 15mm on-chip <ref type=""bibr"" target=""#b15"">[15]</ref> 375pJ 185x 64b transfer across main-board <ref type=""bibr"" target=""#b16"">[16]</ref> 2.5nJ 1250x 64b access t",0
"<body> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""1."">INTRODUCTION</head><p>Throughput-oriented computing is becoming increasingly important <ref type=""bibr"" target=""#b1"">[1,</ref><ref type=""bibr"" target=""#b2"">2]</ref>. Emerging examples of throughput computing occur in the enterprise (batc",0
"uction. In the bandwidth-exhausted regime, an 80% decrease in bandwidth usage can increase throughput up to 5 times. These results echo another study <ref type=""bibr"" target=""#b25"">[25]</ref> on the limits of cache compression where caches can be ideally compressed up to 64x.</p><p>We evaluate MORC",0
"n stream, to analyze the miss instruction stream of an LIP cache. We classify instruction cache misses into three categories as originally defined in <ref type=""bibr"" target=""#b48"">[51]</ref>. Non-repetitive misses do not belong to any recurring pattern. New misses are those instructions misses that curately match instruction misses to their corresponding patterns.</p><p>We propose to use the Temporal Instruction Fetch Streaming (TIFS) prefetcher <ref type=""bibr"" target=""#b48"">[51]</ref> to prefetch recurring missing instructions. TIFS predicts and prefetches future instruction misses through r "" target=""#b46"">[49]</ref>. Because of the additional hardware cost, we choose LIP instead of BIP. TIFS is implemented as described by Ferdman et al. <ref type=""bibr"" target=""#b48"">[51]</ref>. We find that it is sufficient for the IML to keep track of 8 K instruction misses. More IML entries only le",1
"ode.js, which is the most popular server-side event-driven platform based on JavaScript <ref type=""bibr"" target=""#b10"">[11]</ref>. Numerous companies <ref type=""bibr"" target=""#b21"">[23]</ref> such as eBay, PayPal, and LinkedIn have adopted it to improve the efficiency and scalability of their applic",0
"rge instruction footprints with little intraevent code reuse. Recent studies on client-side event-driven applications also derive similar conclusions <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b12"">13]</ref>. We take this research a step further to make the key observation th -driven applications.</p><p>Asynchronous/Event Execution Analysis Prior work on event-driven applications primarily focus on client-side applications <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b12"">13]</ref> whereas we study server-side applications. While prior art also attr",0
"y focus in TLBrelated studies due to their extremely low miss rates <ref type=""bibr"" target=""#b38"">[41]</ref><ref type=""bibr"" target=""#b39"">[42]</ref><ref type=""bibr"" target=""#b40"">[43]</ref>. However, I-TLB performance is crucial because every instruction fetch requires a TLB lookup, and TLB misses </ref> compares the I-TLB MPKI under 4 KB and 2 MB (i.e., superpage) page sizes. Although superpages are traditionally used for reducing D-TLB misses <ref type=""bibr"" target=""#b40"">[43,</ref><ref type=""bibr"" target=""#b45"">48]</ref>, our results indicate that large pages would be helpful for improvin",0
"D do not provide the necessary details to mimic the actual implementation. However, they do provide sufficient information about program optimization <ref type=""bibr"" target=""#b37"">[40]</ref> that indirectly indicate reasonable predictor parameters. Based on those informational resources, we mimic b",0
"rarely have true data dependences among tasks, and their maximum achievable parallelism exceeds 100?. It may seem that thread-level speculation (TLS) <ref type=""bibr"" target=""#b27"">[28,</ref><ref type=""bibr"" target=""#b59"">60,</ref><ref type=""bibr"" target=""#b65"">66,</ref><ref type=""bibr"" target=""#b67 d selective aborts of dependent tasks. Commit serialization: Prior TLS schemes enforce inorder commits by passing a token among ready-to-commit tasks <ref type=""bibr"" target=""#b27"">[28,</ref><ref type=""bibr"" target=""#b60"">61,</ref><ref type=""bibr"" target=""#b65"">66,</ref><ref type=""bibr"" target=""#b67 p>Much prior work has investigated thread-level speculation (TLS) schemes to parallelize sequential programs <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b59"">60,</ref><ref type=""bibr"" target=""#b60"">61,</ref><ref type=""bibr"" target=""#b65"" ing mispeculation, most TLS schemes abort the task that caused the violation and all later speculative tasks <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b60"">61,</ref><ref type=""bibr"" target=""#b65"">66,</ref><ref type=""bibr"" target=""#b67"" fering speculative data in caches) or more expensive multiversioning <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b28"">29,</ref><ref type=""bibr"" target=""#b55"">56,</ref><ref type=""bibr"" target=""#b59""",1
"enges in parallelizing these applications, consider Dijkstra's single-source shortest paths (sssp) algorithm <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b21"">22]</ref>. sssp finds the shortest distance between some source node and all other nodes in a graph with weighted edges",0
"Second, they are important in simulating systems whose state evolves over time, such as circuits <ref type=""bibr"" target=""#b46"">[47]</ref>, computers <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b58"">59]</ref>, networks <ref type=""bibr"" target=""#b36"">[37,</ref><ref type=""bibr""",0
"e=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b41"">42,</ref><ref type=""bibr"" target=""#b48"">49,</ref><ref type=""bibr"" target=""#b56"">57,</ref><ref type=""bibr"" target=""#b73"" ese analyses find that parallelism is often plentiful (&gt;1000?), but very large instruction windows are needed to exploit it (&gt;100K instructions <ref type=""bibr"" target=""#b41"">[42,</ref><ref type=""bibr"" target=""#b56"">57]</ref>). Our oracle tool focuses on task-level parallelism, so it misses in",0
"f type=""bibr"" target=""#b58"">59]</ref>, networks <ref type=""bibr"" target=""#b36"">[37,</ref><ref type=""bibr"" target=""#b71"">72]</ref>, healthcare systems <ref type=""bibr"" target=""#b38"">[39]</ref>, and systems of partial differential equations <ref type=""bibr"" target=""#b31"">[32,</ref><ref type=""bibr"" tar",0
"ecently, Pugsley et al. introduced a new sort of prefetcher, offset prefetchers, and the sandbox method for selecting the prefetch offset dynamically <ref type=""bibr"" target=""#b25"">[26]</ref>. Offset prefetching is a generalization of next-line prefetching. Unlike a stream prefetcher, an offset pref ""#b11"">12,</ref><ref type=""bibr"" target=""#b16"">17]</ref> (this list is not exhaustive).</p><p>Recently, Pugsley et al. introduced Sandbox prefetching <ref type=""bibr"" target=""#b25"">[26]</ref>. The Sandbox prefetcher prefetches line X + D when line X is requested at the L2, where D is adjusted dynami on application behavior. To the best of our knowledge, the first published full-fledged offset prefetcher is the Sandbox prefetcher by Pugsley et al. <ref type=""bibr"" target=""#b25"">[26]</ref>. However, the offset selection mechanism in the Sandbox prefetcher ignores prefetch timeliness. The Best-Off Sandbox prefetching</head><p>To the best of our knowledge, the SBP prefetcher of Pugsley et al. is the first published full-fledged offset prefetcher <ref type=""bibr"" target=""#b25"">[26]</ref>. The SBP prefetcher is cost-effective and was shown to outperform significantly the FDP prefetcher <ref type ith fake prefetches, while the RR table is updated with actual prefetches.</p><p>We implemented the SBP prefetcher as described in the original paper <ref type=""bibr"" target=""#b25"">[26]</ref>, but with a few modifications to make the comparison with BO prefetching meaningful. Our SBP uses the same l",1
"rd in a table some history about past memory accesses and use that history to predict future memory accesses <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b35""",0
"""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b34"">35,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b11"">1",0
"chitecture in a very complex way. This is one of the reasons why it is sometimes difficult to reproduce published experimental results on prefetching <ref type=""bibr"" target=""#b24"">[25]</ref>.</p><p>This section provides a detailed description of our baseline microarchitecture, with a focus on parts",0
"mory accesses and use that history to predict future memory accesses <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" target=""#b1"">",0
"p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2."">RELATED WORK</head><p>Simultaneous multithreading (SMT) was proposed by Tullsen et al. <ref type=""bibr"" target=""#b22"">[22]</ref> as a way to improve the utilization and throughput of a single core. Enabling SMT increases the area and pow",1
"nly due to replicating architectural and performance-critical structures, but it can significantly improve throughput. Recently, Eyerman and Eeckhout <ref type=""bibr"" target=""#b7"">[7]</ref> show that a multicore processor consisting of SMT cores has an additional benefit other than increasing throug",0
"on and throughput of a single core. Enabling SMT increases the area and power consumption of a core (5% to 20% <ref type=""bibr"" target=""#b2"">[2,</ref><ref type=""bibr"" target=""#b11"">11]</ref>), mainly due to replicating architectural and performance-critical structures, but it can significantly impro",0
"ning on the processor, and not so much by the way these applications are scheduled onto the cores. This observation is also made by Radojkovi? et al. <ref type=""bibr"" target=""#b16"">[16]</ref>. As a result, with a fixed set of runnable applications, scheduling has no impact on the inter-core interfer",0
"specialized hardware <ref type=""bibr"" target=""#b1"">[2]</ref>, phase-based sampling <ref type=""bibr"" target=""#b2"">[3]</ref>, and statistical sampling <ref type=""bibr"" target=""#b3"">[4]</ref>. Of these techniques, the sampling based approaches typically provide the best trade-Copyright: 978-1-4673-921 d, but Perelman et al. <ref type=""bibr"" target=""#b8"">[9]</ref> extended SimPoint to provide statistical confidence measures.</p><p>Wunderlich, et al. <ref type=""bibr"" target=""#b3"">[4]</ref> developed the SMARTS framework, which applies statistical sampling theory to computer architecture simulation. field of inferential statistics provides well known techniques for inferring statistics about a population given a sample of that population. SMARTS <ref type=""bibr"" target=""#b3"">[4]</ref> demonstrated that systematic sampling can be used to approximate random sampling when used with microarchitect ple characterization. For speed and accuracy we compared LiveSim with no sampling simulation and with a sampling mode that was very similar to SMARTS <ref type=""bibr"" target=""#b3"">[4]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""5.1"">Speed</head><p>Our primary goal for LiveSim seminal work related to profile based sampling <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b8"">9]</ref> and statistical sampling <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b10"">11]</ref> for microarchitecture simulatio",1
"he execution time constraints of LiveSim.</p><p>However, LiveSim is able to take advantage of the correlation between code signatures and performance <ref type=""bibr"" target=""#b15"">[16]</ref> and use this information to group the checkpoints into clusters. When results are calculated, LiveSim ensure",0
"ques have been developed to reduce simulation time including: benchmarks size reduction <ref type=""bibr"" target=""#b0"">[1]</ref>, specialized hardware <ref type=""bibr"" target=""#b1"">[2]</ref>, phase-based sampling <ref type=""bibr"" target=""#b2"">[3]</ref>, and statistical sampling <ref type=""bibr"" targe ts <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b0"">1]</ref>. Another approach is to accelerate the timing simulation using FPGAs <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b6"">7]</ref>, but this requires custom hardware",0
"e and branch predictor warmup was unnecessary, and they proposed ways to determine when to begin warmup prior to simulating a sample. Eeckhout et al. <ref type=""bibr"" target=""#b24"">[25]</ref> proposed a similar technique that further reduced the amount of warmup required. Luo <ref type=""bibr"" target",0
"lts in 5 seconds or less. However, there are a variety of other techniques that have been proposed for accelerating warmup.</p><p>Haskins and Skadron <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b23"">24]</ref> demonstrated that continuous cache and branch predictor warmup was u",0
"on motivated us to base our models on two representative CNN architectures, which we chose because of their nice tradeoffs between speed and accuracy <ref type=""bibr"" target=""#b2"">[2]</ref>:</p><p>1. The ILSVRC-2012 <ref type=""bibr"" target=""#b19"">[19]</ref> winning entry of <ref type=""bibr"" target="" 12 <ref type=""bibr"" target=""#b19"">[19]</ref> winning entry of <ref type=""bibr"" target=""#b12"">[12]</ref> (AlexNet).</p><p>2. The CNN-M-2048 model from <ref type=""bibr"" target=""#b2"">[2]</ref> (VGG-CNN-M-2048), which is a variant of the model introduced in <ref type=""bibr"" target=""#b28"">[28]</ref>.</p> omplex model such as a CNN using only a small amount of training data without overfitting. Our approach to tackling this problem follows recent works <ref type=""bibr"" target=""#b2"">[2,</ref><ref type=""bibr"" target=""#b8"">8,</ref><ref type=""bibr"" target=""#b9"">9,</ref><ref type=""bibr"" target=""#b26"">26]< <ref type=""bibr"" target=""#b19"">[19]</ref> using much smaller datasets, we set an initial learning rate of 0.001, which is lower than the typical 0.01 <ref type=""bibr"" target=""#b2"">[2,</ref><ref type=""bibr"" target=""#b12"">12]</ref>, so as not to drastically alter the pre-trained weights. We found that",1
"(AlexNet).</p><p>2. The CNN-M-2048 model from <ref type=""bibr"" target=""#b2"">[2]</ref> (VGG-CNN-M-2048), which is a variant of the model introduced in <ref type=""bibr"" target=""#b28"">[28]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.3"">Dataset and Training</head><p>As noted abo",1
"[18]</ref>. Fewer works follow the dimensional approach, according to which facial expressions are treated as regression in the Arousal-Valence space <ref type=""bibr"" target=""#b29"">[29]</ref>. A very detailed and recent review can be found in <ref type=""bibr"" target=""#b21"">[21]</ref>.</p><p>The Emot",0
"ef type=""bibr"" target=""#b13"">[13]</ref> convolutional and audio features were fused using Partial Least Squares and multiple classifiers. Finally, in <ref type=""bibr"" target=""#b11"">[11]</ref> multiple Deep Convolutional Neural Network (CNN) were introduced for different data modalities (video frames ithms for this task, as well as winning related competitions <ref type=""bibr"" target=""#b10"">[10]</ref>, particularly previous years' EmotiW challenge <ref type=""bibr"" target=""#b11"">[11,</ref><ref type=""bibr"" target=""#b14"">14]</ref>.</p><p>However, due to the small dataset size for the EmotiW 2015 im",0
"follow the categorical approach of the 7 basic expressions. Images are selected from movies, in a semi-automated way, via a system based on subtitles <ref type=""bibr"" target=""#b5"">[5,</ref><ref type=""bibr"" target=""#b6"">6]</ref>. The challenging characteristics of SFEW are twofold. First, imaging con",0
"humans is holistic rather than feature-based, especially in the case of photographs (but not for schematics) <ref type=""bibr"" target=""#b15"">[15,</ref><ref type=""bibr"" target=""#b16"">16]</ref>. As a result, even people with impaired high frequency vision can distinguish successfully between different",0
"ts that facial expression recognition in humans is holistic rather than feature-based, especially in the case of photographs (but not for schematics) <ref type=""bibr"" target=""#b15"">[15,</ref><ref type=""bibr"" target=""#b16"">16]</ref>. As a result, even people with impaired high frequency vision can di",0
"es were detected with OpenCV's Viola &amp; Jones face detector (frontal and profile) <ref type=""bibr"" target=""#b25"">[25]</ref>. The Intraface library <ref type=""bibr"" target=""#b4"">[4]</ref> was used in order to detect 49 facial points. The fit of the alignment model, provided by Intraface, was used",0
"tween speed and accuracy <ref type=""bibr"" target=""#b2"">[2]</ref>:</p><p>1. The ILSVRC-2012 <ref type=""bibr"" target=""#b19"">[19]</ref> winning entry of <ref type=""bibr"" target=""#b12"">[12]</ref> (AlexNet).</p><p>2. The CNN-M-2048 model from <ref type=""bibr"" target=""#b2"">[2]</ref> (VGG-CNN-M-2048), whic t EmotiW dataset.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>Angry</head><p>The training procedure for our CNNs closely follows that of <ref type=""bibr"" target=""#b12"">[12]</ref>. They are trained using stochastic gradient descent with hyperparameters (momentum=0.9, weight decay=0.0005, f> using much smaller datasets, we set an initial learning rate of 0.001, which is lower than the typical 0.01 <ref type=""bibr"" target=""#b2"">[2,</ref><ref type=""bibr"" target=""#b12"">12]</ref>, so as not to drastically alter the pre-trained weights. We found that doing so delays the onset of overfitti",0
"aph can be accurately captured.</p><p>Recently, there has been a surge of interest in learning graph representations from data. For example, DeepWalk <ref type=""bibr"" target=""#b19"">[20]</ref>, one recent model, transforms a graph structure into a sample collection of linear sequences consisting of v =""#b0"">[1]</ref> proposed a graph factorization method, which used stochastic gradient descent to optimize matrices from large graphs. Perozzi et al. <ref type=""bibr"" target=""#b19"">[20]</ref> presented an approach, which transformed graph structure into several linear vertex sequences by using a tru concatenating the representation of 1step and 2-step relational information and tuning the threshold of maximum number of vertices.</p><p>2. DeepWalk <ref type=""bibr"" target=""#b19"">[20]</ref>. DeepWalk is a method that learns the representation of social networks. The original model only works for u o form the representations and employ the reconstruction strategy for vertices with small degrees to achieve the optimal performance. As mentioned in <ref type=""bibr"" target=""#b19"">[20]</ref>, for DeepWalk and E-SGNS, we set window size as 10, walk length as 40, walks per vertex as 80. According to ss of different graph representations through a multi-label classification task by regarding the learned representations as features.</p><p>Following <ref type=""bibr"" target=""#b19"">[20,</ref><ref type=""bibr"" target=""#b26"">27]</ref>, we use the LibLinear package <ref type=""bibr"" target=""#b9"">[10]</re",1
"roposed model through the preservation of different k-step relational information in distinct subspaces.</p><p>Another recently proposed work is LINE <ref type=""bibr"" target=""#b24"">[25]</ref>, which has a loss function to capture both 1-step and 2-step local relational information. To capture certai ated vertex representations by using skip-gram model. This is considered as an equally weighted linear combination of k-step information. Tang et al. <ref type=""bibr"" target=""#b24"">[25]</ref> later proposed a large-scale information network embedding, which optimizes a loss function where both 1-ste i-c.org/ns/1.0""><head n=""6.2"">Baseline Algorithms</head><p>We use the following methods of graph representation as baseline algorithms.</p><p>1. LINE <ref type=""bibr"" target=""#b24"">[25]</ref>. LINE is a recently proposed method for learning graph representations on large-scale information networks. of each k-step information is averaged.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""6.3"">Parameter Settings</head><p>As suggested in <ref type=""bibr"" target=""#b24"">[25]</ref>, for LINE, we set the mini-batch size of stochastic gradient descent (SGD) as 1, learning rate of starting v <ref type=""bibr"" target=""#b19"">[20]</ref>, for DeepWalk and E-SGNS, we set window size as 10, walk length as 40, walks per vertex as 80. According to <ref type=""bibr"" target=""#b24"">[25]</ref>, LINE yielded better results when the learned graph representations are L2 normalized, while DeepWalk and E- each system accordingly. For a fair comparison, the dimension d of representations is set as 128 for Blogcatalog network and DBLP network as used in <ref type=""bibr"" target=""#b24"">[25]</ref> and is set as 64 for 20-NewsGroup network as used in <ref type=""bibr"" target=""#b28"">[29]</ref>. For GraRep, und, we randomly sample 10% to 90% of the vertices and use these samples for training, and use the remaining vertices for evaluation. As suggested in <ref type=""bibr"" target=""#b24"">[25]</ref>, we set k-max as 0, 200, 500 and 1000, respectively, and we report the best performance with k-max=500. </p>",1
"C k gives the representations of context vertices as its column vectors <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b29"">30]</ref>. The final matrix W k is returned from the algorithm as the low-d representations of the vertices which captu",0
"/ref>, independent component analysis (ICA) <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b12"">13]</ref>, and deep neural networks <ref type=""bibr"" target=""#b11"">[12]</ref>. Our focus in this work is on the novel model for learning graph representations, so we do not pursue any al",0
"presented methods for learning latent representational vectors of the graphs which can then be applied to social network classification. Ahmed et al. <ref type=""bibr"" target=""#b0"">[1]</ref> proposed a graph factorization method, which used stochastic gradient descent to optimize matrices from large",0
"urrence counts <ref type=""bibr"" target=""#b18"">[19]</ref>. On the other hand, the family of matrix factorization methods can utilize global statistics <ref type=""bibr"" target=""#b4"">[5]</ref>. Previous work include Latent Semantic Analysis (LSA) <ref type=""bibr"" target=""#b14"">[15]</ref>, which decompo nt vertices as its column vectors, and C k gives the representations of context vertices as its column vectors <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b29"">30]</ref>. The final matrix W k is returned from the algorithm as the low-d repre",0
"nalysis (LSA) <ref type=""bibr"" target=""#b14"">[15]</ref>, which decomposes termdocument matrix and yields latent semantic representations. Lund et al. <ref type=""bibr"" target=""#b16"">[17]</ref> put forward Hyperspace Analogue to Language (HAL), factorizing a word-word co-occurrence counts matrix to ge",0
"rning word representations and showed that the Skip-Gram model with Negative Sampling (SGNS) can be regarded as a model that implicitly such a matrix <ref type=""bibr"" target=""#b15"">[16]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2.2"">Graph Representation Approaches</head><p>T s proposed by Gutmann et al. <ref type=""bibr"" target=""#b10"">[11]</ref>, to define our objective function. Following a similar discussion presented in <ref type=""bibr"" target=""#b15"">[16]</ref>, we first introduce our k-step loss function defined over the complete graph as follows:</p><formula xml:id= ) = 1/N . This leads to:</p><formula xml:id=""formula_9"">L k (w, c) = A k w,c • log σ( w • c) + λ N w A k w ,c • log σ(− w • c)</formula><p>Following <ref type=""bibr"" target=""#b15"">[16]</ref>, we define e = w • c, and setting ∂L k ∂e = 0. This yields the following:</p><formula xml:id=""formula_10"">w m.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.3"">Optimization with Matrix Factorization</head><p>Following the work of Levy et al. <ref type=""bibr"" target=""#b15"">[16]</ref>, to reduce noise, we replace all negative entries in Y k with 0. This gives us a positive k-step log probabi pe=""bibr"" target=""#b13"">14]</ref>, and is regarded as one of the important methods that can be used for dimensionality reduction. It was also used in <ref type=""bibr"" target=""#b15"">[16]</ref>.</p><p>For the matrix X k , SVD factorizes it as:</p><formula xml:id=""formula_13"">X k = U k Σ k (V k ) T</fo ue to our novel model, or comes from any non-linearity introduced in this dimensionality reduction step. To maintain the consistency with Levy et al. <ref type=""bibr"" target=""#b15"">[16]</ref>, we only employed SVD in this work.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4."">ALGORITH the collection of all observed pairs in sequences, that is, |D| = γK. This matrix Y E−SGN S becomes exactly the same as that of SGNS as described in <ref type=""bibr"" target=""#b15"">[16]</ref>. This shows SGNS is essentially a special version of our GraRep model that deals with linear sequences which",0
"e popular SVD can also be exploited. Examples include incremental SVD <ref type=""bibr"" target=""#b21"">[22]</ref>, independent component analysis (ICA) <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b12"">13]</ref>, and deep neural networks <ref type=""bibr"" target=""#b11"">[12]</ref>. O",0
"amples include incremental SVD <ref type=""bibr"" target=""#b21"">[22]</ref>, independent component analysis (ICA) <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b12"">13]</ref>, and deep neural networks <ref type=""bibr"" target=""#b11"">[12]</ref>. Our focus in this work is on the novel m",0
"nerating the noisy observations conditioned on the unknown true labels and some behavior assumptions, with examples of the Dawid-Skene (DS) estimator <ref type=""bibr"" target=""#b4"">[5]</ref>, the minimax entropy (Entropy) estimator<ref type=""foot"" target=""#foot_0"">1</ref>  <ref type=""bibr"" target=""#b arget=""#b10"">11]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2.2"">Dawid-Skene Estimator</head><p>The method of Dawid and Skene <ref type=""bibr"" target=""#b4"">[5]</ref> is a generative approach by considering worker confusability. It posits that the performance of a worker is co , including majority voting (MV), iterative weighted majority voting (IWMV) <ref type=""bibr"" target=""#b10"">[11]</ref>, the Dawid-Skene (DS) estimator <ref type=""bibr"" target=""#b4"">[5]</ref>, and the minimax entropy (Entropy) estimator <ref type=""bibr"" target=""#b24"">[25]</ref>. For Entropy estimator, e regularization parameters (c, ) are selected from c = 2ˆ[−8 : 0] and = <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b4"">5]</ref> by the method in Sec. 6.2. As for Gibbs-CrowdSVM, we generate 50 samples in each run and discard the first 10 s",1
"b = 0. All the possible decision boundaries with the same orientation are equivalent. Inspired by the generalized notion of margin in multi-class SVM <ref type=""bibr"" target=""#b3"">[4]</ref>, we define the crowdsourcing margin as the minimal difference between the aggregated score of the potential tr rmula xml:id=""formula_6"">0≤ω d i ≤c − 1 2 η η + i d ω d i ∆ i (d),<label>(6)</label></formula><p>which is exactly the QP dual problem in standard SVM <ref type=""bibr"" target=""#b3"">[4]</ref>. So it can be efficiently solved by welldeveloped SVM solvers like LIBSVM <ref type=""bibr"" target=""#b1"">[2]</r",0
">[11]</ref>. We can also put a prior over worker confusion matrices and transform the inference into a standard inference problem in graphical models <ref type=""bibr"" target=""#b11"">[12]</ref>. Recently, spectral methods have also been applied to better initialize the DS model <ref type=""bibr"" target 18"">)</formula><p>where μ is the mean of q * (η). Then we can make predictions by maximize this function.</p><p>Apparently, the discriminant function <ref type=""bibr"" target=""#b11"">(12)</ref> represents a strong coupling between the generative model and the discriminative margin constraints. Therefo",0
"ng the regularization parameter to its extreme values (i.e., 0 or ∞). We investigate two choices on defining the max-margin posterior regularization: <ref type=""bibr"" target=""#b0"">(1)</ref> an averaging model with a variational inference algorithm; and (2) a Gibbs model with a Gibbs sampler under a e results are insensitive to them. For M 3 V, CrowdSVM and Gibbs-CrowdSVM, the regularization parameters (c, ) are selected from c = 2ˆ[−8 : 0] and = <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b4"">5]</ref> by the method in Sec. 6.2. As for",0
"nce (RegBayes) <ref type=""bibr"" target=""#b26"">[27]</ref> to incorporate max-margin majority voting constraints as posterior regularization on problem <ref type=""bibr"" target=""#b7"">(8)</ref>, and define the Bayesian max-margin estimator (denoted by CrowdSVM) as solving:</p><formula xml:id=""formula_9"" ,665 query-URL pairs on a relevance rating scale from 1 to 5. Each task is labeled by 6 workers on average. In total 15,567 labels are collected. Age <ref type=""bibr"" target=""#b7"">[8]</ref>: It consists of 10,020 labels of age estimations for 1,002 face images. Each image was labeled by 10 workers.",0
"=""#b11"">[12]</ref>.</p><p>On the other hand, many researchers have found convolutional networks (ConvNets) <ref type=""bibr"" target=""#b16"">[17]</ref>  <ref type=""bibr"" target=""#b17"">[18]</ref> are useful in extracting information from raw signals, ranging from computer vision applications to speech r",1
"ssifiers <ref type=""bibr"" target=""#b14"">[15]</ref>, and incorporating character-level features to ConvNets <ref type=""bibr"" target=""#b27"">[28]</ref>  <ref type=""bibr"" target=""#b28"">[29]</ref>. In particular, these ConvNet approaches use words as a basis, in which character-level features extracted a ConvNet approaches use words as a basis, in which character-level features extracted at word <ref type=""bibr"" target=""#b27"">[28]</ref> or word n-gram <ref type=""bibr"" target=""#b28"">[29]</ref> level form a distributed representation. Improvements for part-of-speech tagging and information retrieval w",0
"es of text classification are based on words, in which simple statistics of some ordered word combinations (such as n-grams) usually perform the best <ref type=""bibr"" target=""#b11"">[12]</ref>.</p><p>On the other hand, many researchers have found convolutional networks (ConvNets) <ref type=""bibr"" tar",0
"In particular, time-delay networks used in the early days of deep learning research are essentially convolutional networks that model sequential data <ref type=""bibr"" target=""#b0"">[1]</ref>  <ref type=""bibr"" target=""#b30"">[31]</ref>.</p><p>In this article we explore treating text as a kind of raw si",0
"n the early days of deep learning research are essentially convolutional networks that model sequential data <ref type=""bibr"" target=""#b0"">[1]</ref>  <ref type=""bibr"" target=""#b30"">[31]</ref>.</p><p>In this article we explore treating text as a kind of raw signal at character level, and applying tem",0
"n by the success of region proposal methods (e.g., <ref type=""bibr"" target=""#b21"">[22]</ref>) and region-based convolutional neural networks (R-CNNs) <ref type=""bibr"" target=""#b5"">[6]</ref>. Although region-based CNNs were computationally expensive as originally developed in <ref type=""bibr"" target= eural networks (R-CNNs) <ref type=""bibr"" target=""#b5"">[6]</ref>. Although region-based CNNs were computationally expensive as originally developed in <ref type=""bibr"" target=""#b5"">[6]</ref>, their cost has been drastically reduced thanks to sharing convolutions across proposals <ref type=""bibr"" targ "">20]</ref> generate region proposals from a network whose last fc layer simultaneously predicts multiple (e.g., 800) boxes, which are used for R-CNN <ref type=""bibr"" target=""#b5"">[6]</ref> object detection. Their proposal network is applied on a single image or multiple large image crops (e.g., 224 eg , and a balancing weight λ. <ref type=""foot"" target=""#foot_3"">3</ref>For regression, we adopt the parameterizations of the 4 coordinates following <ref type=""bibr"" target=""#b5"">[6]</ref>:</p><formula xml:id=""formula_2"">t x = (x − x a )/w a , t y = (y − y a )/h a , t w = log(w/w a ), t h = log(h/h ed conv layers) are initialized by pretraining a model for ImageNet classification <ref type=""bibr"" target=""#b16"">[17]</ref>, as is standard practice <ref type=""bibr"" target=""#b5"">[6]</ref>. We tune all layers of the ZF net, and conv3 1 and up for the VGG net to conserve memory <ref type=""bibr"" targ",1
"-convolutional network <ref type=""bibr"" target=""#b13"">[14]</ref>, can be trained end-to-end by back-propagation and stochastic gradient descent (SGD) <ref type=""bibr"" target=""#b11"">[12]</ref>. We follow the ""imagecentric"" sampling strategy from <ref type=""bibr"" target=""#b4"">[5]</ref> to train this n",0
"tion with standard deviation 0.01. All other layers (i.e., the shared conv layers) are initialized by pretraining a model for ImageNet classification <ref type=""bibr"" target=""#b16"">[17]</ref>, as is standard practice <ref type=""bibr"" target=""#b5"">[6]</ref>. We tune all layers of the ZF net, and conv",0
"g algorithms have been shown to be vulnerable to adversarial samples <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b18"">19]</ref>; adversaries subtly alter legitimate inputs (call input perturbation) to induce the trained model to produce lassifiers learned using these techniques. Building on previous work <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b18"">19]</ref> describing how adversaries can efficiently select perturbations leading deep neural networks to misclassify t ries of targeted classifiers alleviates the need of previous attacks <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b18"">19]</ref> for knowledge of the target architecture and parameters. We generalized this method and showed that it can ta vely misleading non-linear and non-convex models like neural networks<ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b18"">19]</ref>. In addition, we introduce new techniques to craft adversarial samples against support vector machines and de ne such attack, Papernot et al. trained a local deep neural network (DNN) using crafted inputs and output labels generated by the target ""victim"" DNN <ref type=""bibr"" target=""#b18"">[19]</ref>. Thereafter, the local network was used to generate adversarial samples that were highly effective on the or θ can use the fast gradient sign method introduced in <ref type=""bibr"" target=""#b11"">[12]</ref> or the Jacobian-based iterative approach proposed in <ref type=""bibr"" target=""#b18"">[19]</ref>. We only provide here a brief description of the fast gradient sign method, which is the one we use in this",1
"inary Support Vector Machine classifier f k is trained with samples of class k labeled as positive and samples from other classes labeled as negative <ref type=""bibr"" target=""#b7"">[8]</ref>. To classify a sample, each binary linear SVM classifier f k makes a prediction and the overall multiclass cla",0
"/head><p>Many classes of machine learning algorithms have been shown to be vulnerable to adversarial samples <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b18"">19]</ref>; adversaries subtly alter legitimate inputs (call input perturbation) roduced to mislead a specific model f can mislead other models f -even if their architectures greatly differ <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b19"">20]</ref>. A practical impact of this property is that it leads to oracle-based has primarily studied the case where at least one of the models involved in the transfer is a neural network <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b23"">24]</ref>, while we aim to more generally characterize the transferability betw d to craft adversarial samples against classifiers learned using these techniques. Building on previous work <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b18"">19]</ref> describing how adversaries can efficiently select perturbations leadi models approximating the decision boundaries of targeted classifiers alleviates the need of previous attacks <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b18"">19]</ref> for knowledge of the target architecture and parameters. We generaliz . They yield adversarial samples effectively misleading non-linear and non-convex models like neural networks<ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b18"">19]</ref>. In addition, we introduce new techniques to craft adversarial sample mely vulnerable to intra-technique and to a lesser extend vulnerable to cross-technique transferability. In fact, as pointed out by Goodfellow et al. <ref type=""bibr"" target=""#b11"">[12]</ref>, shallow models like logistic regression are unable to cope with adversarial samples and learn a classifier al samples misclassified by DNNs, an adversary with knowledge of the model f and its parameters θ can use the fast gradient sign method introduced in <ref type=""bibr"" target=""#b11"">[12]</ref> or the Jacobian-based iterative approach proposed in <ref type=""bibr"" target=""#b18"">[19]</ref>. We only prov e one we use in this work. To find an adversarial sample x * approximatively solving the optimization problem stated in Equation 1, Goodfellow et al. <ref type=""bibr"" target=""#b11"">[12]</ref> proposed to compute the following perturbation:</p><formula xml:id=""formula_10"">δ x = ε sgn(∇ x c(f, x, y)) hod.</p><p>Adversaries can also craft adversarial samples misclassified by multi-class logistic regression models using the fast gradient sign method <ref type=""bibr"" target=""#b11"">[12]</ref>. In the case of logistic regression, the method finds the most damaging perturbation δ x (according to the m <ref type=""bibr"" target=""#b16"">[17]</ref>. Unfortunately, we found that defenses proposed in the literature-such as training with adversarial samples <ref type=""bibr"" target=""#b11"">[12]</ref>-were noneffective, or we were unable to deploy them because of our lack of access to the machine learning mo",0
"m that statement in Google's case using available documentation. This work is part of a series of security evaluations of machine learning algorithms <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b4"">5]</ref>. Unlike us, previous work in this field assumed knowledge of the model a",0
"ployed remotely, e.g., by machine learning as a service platforms. The existence of such a threat vector calls for the design of defensive mechanisms <ref type=""bibr"" target=""#b16"">[17]</ref>. Unfortunately, we found that defenses proposed in the literature-such as training with adversarial samples",0
"ed the promise of using FPGA overlays to improve designer's productivity in developing hardware accelerators <ref type=""bibr"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b2"">[2]</ref>. While such hardware accelerators can often deliver significant performance improvement over their software co as ZUMA <ref type=""bibr"" target=""#b7"">[7]</ref>, QUKU <ref type=""bibr"" target=""#b8"">[8]</ref>, or QuickDough <ref type=""bibr"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b2"">[2]</ref> have demonstrated the benefits of overlay by improving designers' productivity while maintaining excellent per e existing overlay works, diverse choice of soft/ hard processors <ref type=""bibr"" target=""#b9"">[9]</ref>, <ref type=""bibr"" target=""#b10"">[10]</ref>, <ref type=""bibr"" target=""#b2"">[2]</ref> are used and the integration between the processor and accelerator varies from one work to another.</p><p>The",1
"suffer from patent and trademark issues when being employed commercially.</p><p>Moreover, there exists some lightweight RV32I designs such as zscale <ref type=""bibr"" target=""#b28"">[28]</ref>, GRVI <ref type=""bibr"" target=""#b29"">[29]</ref> or ORCA <ref type=""bibr"" target=""#b6"">[6]</ref> which are si",0
"the processor and accelerator varies from one work to another.</p><p>The closest work that is designed to resolve the above coupling problem is ADRES <ref type=""bibr"" target=""#b11"">[11]</ref>. Mei et al. proposed an architecture that contains a VLIW processor tightly-coupled with a coarse-grained re",0
"many early works have already demonstrated the promise of using FPGA overlays to improve designer's productivity in developing hardware accelerators <ref type=""bibr"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b2"">[2]</ref>. While such hardware accelerators can often deliver significant perfo ensive applications. Research works such as ZUMA <ref type=""bibr"" target=""#b7"">[7]</ref>, QUKU <ref type=""bibr"" target=""#b8"">[8]</ref>, or QuickDough <ref type=""bibr"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b2"">[2]</ref> have demonstrated the benefits of overlay by improving designers' pro",0
"ghtlycoupled with the hardware accelerator as illustrated in Figure 1. In the proposed framework, Multiple Runtime Architecture Computer(MURAC) model <ref type=""bibr"" target=""#b4"">[4]</ref> is adopted to handle the transfer of control when the execution is switched from one architecture to another.<",0
"utputs that a father is to a doctor as a mother is to a nurse. The primary embedding studied in this paper is the popular publicly-available word2vec <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b24"">25]</ref> embedding trained on a corpus of Google News texts consisting of 3 m p>Embedding. Unless otherwise stated, the embedding we refer to in this paper is the aforementioned w2vNEWS embedding, a d = 300-dimensional word2vec <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b24"">25]</ref> embedding, which has proven to be immensely useful since it is high",1
"notions of fairness have been described in a number of works, see, e.g., <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b9"">10]</ref> and a recent survey <ref type=""bibr"" target=""#b40"">[41]</ref>.</p><p>Feldman et al. <ref type=""bibr"" target=""# ibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b9"">10]</ref> and a recent survey <ref type=""bibr"" target=""#b40"">[41]</ref>.</p><p>Feldman et al. <ref type=""bibr"" target=""#b9"">[10]</ref> distinguish classification algorithms that achieve fairness by modifying the underlying data from those that",0
"of papers written about word embeddings and their applications, from Web search <ref type=""bibr"" target=""#b26"">[27]</ref> to parsing Curriculum Vitae <ref type=""bibr"" target=""#b15"">[16]</ref>. However, none of these papers have recognized how blatantly sexist the embeddings are and hence risk introd",0
""">[39]</ref>). In Wikipedia, Wager et al. <ref type=""bibr"" target=""#b38"">[39]</ref> found that, as suggested by prior work on gender bias in language <ref type=""bibr"" target=""#b1"">[2]</ref>, articles about women more often emphasize their gender, their husbands and their husbands' jobs, and other to",0
"the vector differences between words in embeddings have been shown to represent relationships between words <ref type=""bibr"" target=""#b31"">[32,</ref><ref type=""bibr"" target=""#b25"">26]</ref>. For example given an analogy puzzle, ""man is to king as woman is to x"" (denoted as man:king :: woman:x), sim ring coherence and analogy-solving abilities: RG <ref type=""bibr"" target=""#b31"">[32]</ref>, WS <ref type=""bibr"" target=""#b11"">[12]</ref>, MSR-analogy <ref type=""bibr"" target=""#b25"">[26]</ref>. Higher is better. The results show that the performance does not degrade after debiasing. Note that we use x, it is equivalent to the most common approach to finding single word analogies, namely maximizing cos( y, x + b − a) called cosAdd in earlier work <ref type=""bibr"" target=""#b25"">[26]</ref> since we assume all vectors are unit length. This works well in some cases, but a weakness can be seen that,",0
"[12]</ref> the latency.</p><p>These features have motivated the development and release of high performance stacked DRAM from several leading vendors <ref type=""bibr"" target=""#b12"">[13]</ref><ref type=""bibr"" target=""#b13"">[14]</ref><ref type=""bibr"" target=""#b14"">[15]</ref><ref type=""bibr"" target=""#b ecture targeted for the datacenter market. The bottom layer of the chip has many-core compute engine and the top layer has wide I/O DRAMs (similar to <ref type=""bibr"" target=""#b12"">[13]</ref>) which are used as LLC shared by all cores. To optimize the total chip throughput, the bottom layer allocate",1
"tails on DRAM architecture <ref type=""bibr"" target=""#b19"">[21]</ref><ref type=""bibr"" target=""#b20"">[22]</ref><ref type=""bibr"" target=""#b21"">[23]</ref><ref type=""bibr"" target=""#b22"">[24]</ref> and die-stacking technology <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b5"">6,</ref><re "">[29]</ref> at the expense of other parameter. DRAM also requires refresh which consume significant amount of energy and reduces device availability <ref type=""bibr"" target=""#b22"">[24]</ref>. Due to these tradeoffs, increasing the size of DRAM beyond a limit may not be feasible.</p><p>Architecture ef type=""bibr"" target=""#b65"">[67]</ref>. At architecture level, multiple techniques such as data compression, rank subsetting, access scheduling etc. <ref type=""bibr"" target=""#b22"">[24]</ref> can be synergistically integrated to bring the best of them together. Similarly, compiler and OS techniques",0
"bandwidth than off-chip DRAM while incurring only half <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b10"">11]</ref> or one-third <ref type=""bibr"" target=""#b11"">[12]</ref> the latency.</p><p>These features have motivated the development and release of high performance stacked DRA that combining sectored cache with partial tags provides the highest performance improvement while also reducing the tag overhead.</p><p>Jiang et al. <ref type=""bibr"" target=""#b11"">[12]</ref> present filter-based techniques to avoid bandwidth-wastage in page-based DRAM caches. Their first technique dditional complexity of organizing the data in bi-modal manner.</p><p>A comparison of bandwidth-saving approaches: It is noteworthy that Jiang et al. <ref type=""bibr"" target=""#b11"">[12]</ref> optimize bandwidth by using adaptive page allocation in DRAM, which is different from Meza et al. <ref type= ""bibr"" target=""#b57"">[59,</ref><ref type=""bibr"" target=""#b59"">61]</ref> Dynamically optimizing bandwidth usage <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b19"">21,</ref><ref type=""bibr"" target=""#b30"">32,</ref><ref type=""bibr"" target=""#b32"" pe=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b58"">60</ref>  <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b26"">28,</ref><ref type=""bibr"" target=""#b48"">50,</ref><ref type=""bibr"" target=""#b49"" br"" target=""#b49"">51,</ref><ref type=""bibr"" target=""#b50"">52,</ref><ref type=""bibr"" target=""#b56"">58]</ref> Predictor for selecting useful/hot blocks <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b49"">51,</ref><ref type=""bibr"" target=""#b51"">53]</ref> Prefetching <ref type=""bibr""",0
"3,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b28"">30,</ref><ref type=""bibr"" target=""#b46"">48]</ref> Cache reconfiguration <ref type=""bibr"" target=""#b41"">[43,</ref><ref type=""bibr"" target=""#b47"">49,</ref><ref type=""bibr"" target=""#b52"">54</ref>] Cache mapping scheme <ref ty while Chang et al. <ref type=""bibr"" target=""#b52"">[54]</ref> use selective-set approach <ref type=""bibr"" target=""#b24"">[26]</ref>.</p><p>Inoue et al. <ref type=""bibr"" target=""#b41"">[43]</ref> present an SRAM/DRAM hybrid cache architecture. They assume a processor designed with small SRAM L2 cache in e and for such applications, the line is placed in DRAM cache with low probability. This avoids unnecessary insertions into DRAM. Unlike Inoue et al. <ref type=""bibr"" target=""#b41"">[43]</ref>, they do not power-gate the DRAM for saving energy, but instead focus on reducing inter-core interference in",0
"stochastic gradient descent (SGD) akin to backpropogation on just single hidden-layer feedforward neural networks. Recent attempts in this direction <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b27"">28]</ref> propose efficient algorithms but rely on a rigid notion of a network lting algorithm is flexible, giving us control over the search space through tunable parameters, in contrast to rigid search procedures in prior work <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b27"">28]</ref>. Consequently, our method generalizes prior work and can model the f we predict the existence of an edge given a pair of nodes. We contrast the performance of node2vec with state-of-the-art feature learning algorithms <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b27"">28]</ref>. We experiment with several real-world networks from diverse domains d neighborhoods.</p><p>Inspired by the Skip-gram model, recent research established an analogy for networks by representing a network as a ""document"" <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b27"">28]</ref>. The same way as a document is an ordered sequence of words, one cou networks and all prediction tasks. This is a major shortcoming of prior work which fail to offer any flexibility in sampling of nodes from a network <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b27"">28]</ref>. Our algorithm node2vec overcomes this limitation by designing a fle ugh a neighborhood sampling strategy S.</p><p>We proceed by extending the Skip-gram architecture to networks <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b23"">24]</ref>. We seek to optimize the following objective function, which maximizes the log-probability of observing a net ach in which we take the top d eigenvectors of the normalized Laplacian matrix of graph G as the feature vector representations for nodes. • DeepWalk <ref type=""bibr"" target=""#b23"">[24]</ref>: This approach learns d-dimensional feature representations by simulating uniform random walks. The sampling at a 2-hop distance from the source nodes. We exclude other matrix factorization approaches which have already been shown to be inferior to DeepWalk <ref type=""bibr"" target=""#b23"">[24]</ref>. We also exclude a recent approach, GraRep <ref type=""bibr"" target=""#b5"">[6]</ref>, that generalizes LINE to rom prior work serve as useful pointers in making the sampling procedure computationally efficient. We showed how random walks, also used in DeepWalk <ref type=""bibr"" target=""#b23"">[24]</ref>, allow the sampled nodes to be reused as neighborhoods for different source nodes appearing in the walk. Ali i-c.org/ns/1.0""><head n=""5."">DISCUSSION AND CONCLUSION</head><p>Both DeepWalk and LINE can be seen as rigid search strategies over networks. DeepWalk <ref type=""bibr"" target=""#b23"">[24]</ref> proposes search using uniform random walks. The obvious limitation with such a strategy is that it gives us",1
"t work for supervised feature learning based on existing and novel graph-specific deep network architectures <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b30"">31,</ref><ref type=""bibr"" target=""#b38""",0
"domains; for instance, in genomics, it helps us discover novel interactions between genes, and in social networks, it can identify real-world friends <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b33"">34]</ref>.</p><p>Any supervised machine learning algorithm requires a set of inf",0
"tion of random walks. The optimization phase is made efficient using negative sampling <ref type=""bibr"" target=""#b21"">[22]</ref> and asynchronous SGD <ref type=""bibr"" target=""#b25"">[26]</ref>. Many ideas from prior work serve as useful pointers in making the sampling procedure computationally effici",0
", it results in task-independent features that closely match task-specific approaches in predictive accuracy <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b22"">23]</ref>.</p><p>However, current techniques fail to satisfactorily define and optimize a reasonable objective required",0
"Description</head></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.1"">Stressor Event and Subject Dictionaries</head><p>The word embeddings <ref type=""bibr"" target=""#b8"">[Mikolov et al., 2013]</ref> have been found effective in estimating the semantic similarities among different words. In",1
"s Measurement</head><p>The measurement of psychological stress has been well studied by psychologists in the past decades. Most of these measurements <ref type=""bibr"" target=""#b9"">[Rowlison and Felner, 1988;</ref><ref type=""bibr"" target=""#b1"">Brantley and Jones, 1993]</ref> are based on questionnair br"" target=""#b5"">[Friedman, 1999]</ref>: It trains a gradient boosted decision tree model with features associated with each tweet.</p><p>• Lasso-MTL <ref type=""bibr"" target=""#b9"">[Nie et al., 2010]</ref>: It is the Lasso regularized MTL which can achieve the goal of reducing model complexity and fe ensure a fair comparison, we fed all the comparison methods with the same feature settings. For the SVM, SR and GBDT methods, we use the scikit-learn <ref type=""bibr"" target=""#b9"">[Pedregosa et al., 2011]</ref> implementation. As for the comparison methods of MTL, we use the MALSAR package <ref type",0
"has been well studied by psychologists in the past decades. Most of these measurements <ref type=""bibr"" target=""#b9"">[Rowlison and Felner, 1988;</ref><ref type=""bibr"" target=""#b1"">Brantley and Jones, 1993]</ref> are based on questionnaires and interviews. Among them, the Social Readjustment Rating S nd Lin, 2011]</ref>: It is a popular binary classifier that is found to effective in several classification problems.</p><p>• Softmax Regression (SR) <ref type=""bibr"" target=""#b1"">[Böhning, 1992]</ref>: It is a model that is used to predict the probabilities of the different possible outcomes of a c",0
"atures.</p><p>• cASO-MTL <ref type=""bibr"" target=""#b3"">[Chen et al., 2009]</ref>: It is a convex relaxation of the alternating structure optimization <ref type=""bibr"" target=""#b0"">[Ando and Zhang, 2005]</ref>, which decomposes the predictive model of each task into the task-specific feature mapping",0
"d patterns to strengthen the learning performance, we regard each event or subject detection as a task and propose to use a multi-task learning (MTL) <ref type=""bibr"" target=""#b2"">[Caruana, 1997]</ref> model. The MTL model is able to adaptively capture the relatedness among tasks, as well as to lear ith some state-of-the-art models. All the reported results in this paper were based on 10-fold cross validation:</p><p>• Support Vector Machine (SVM) <ref type=""bibr"" target=""#b2"">[Chang and Lin, 2011]</ref>: It is a popular binary classifier that is found to effective in several classification prob",0
"lays for primary-users in cognitive radio networks, an optimal relay selection strategy is developed in <ref type=""bibr"" target=""#b10"">[11]</ref>. In <ref type=""bibr"" target=""#b11"">[12]</ref>, the dynamism of electricity price and the deferrability of some home applications are exploited to design a k N = [0, ..., 0, P i k N ]. • Compute the corresponding Nash value V i k N (XN , RN ) using ( <ref type=""formula"" target=""#formula_15"">10</ref>) and <ref type=""bibr"" target=""#b11"">(12)</ref>.</p><formula xml:id=""formula_21"">End For 1 ≤ n ≤ N − 1</formula><p>For each possible realization of Xn and R d construct Pi k n using (11). • Compute the corresponding Nash value V i k n (Xn, Rn) using ( <ref type=""formula"" target=""#formula_15"">10</ref>) and <ref type=""bibr"" target=""#b11"">(12)</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>End End</head><p>In the following, the developed M",1
"ations to users' access behaviors is highly desirable to DT-ORS-Net applications. To this end, the two-player Markov stopping game (MSG) developed in <ref type=""bibr"" target=""#b16"">[17]</ref> can serve as a good starting point, which extends the classic optimal stopping theory into a game theoretic nds the classic optimal stopping theory into a game theoretic setting so as to handle the potential conflicts between the two players.</p><p>However, <ref type=""bibr"" target=""#b16"">[17]</ref> does not provide a systematic method to deal with a general number of players and hence is still not readily ic resource and contend with each other. To derive the optimal strategy for each player in such situations, the two-player MSG framework developed in <ref type=""bibr"" target=""#b16"">[17]</ref> may serve as a basis. Particularly, in the two-player MSG, two rational and selfish players monitor the same em (with time horizon N − n) for the remaining player. However, a systematic method for handling a general number of players in a MSG is missing from <ref type=""bibr"" target=""#b16"">[17]</ref>. Considering this, a general M-MSG is proposed in the next section so as to address the resource access prob G. For this procedure, several new definitions are needed. Particularly, when two players coexist in the MSG, the randomized stopping time is used in <ref type=""bibr"" target=""#b16"">[17]</ref> to deal with the potential competition from the other player. To further handle the dynamism in the number o ion from the other player. To further handle the dynamism in the number of players as the game evolving, the concept of selection time is proposed in <ref type=""bibr"" target=""#b16"">[17]</ref>. However, constructing the selection time essentially requires a full enumeration of all possible dynamics i",1
"a><p>where • denotes the concatenation operation and P i k n is an NE of player i k in the auxiliary game at timeslot n with payoff function given by <ref type=""bibr"" target=""#b8"">(9)</ref>. Consequently,</p><formula xml:id=""formula_19"">V i k n (X n , R n ) = Ṽ i k n (X n , R n ). (<label>12</label> uted resource allocation problems<ref type=""bibr"" target=""#b5"">[6]</ref><ref type=""bibr"" target=""#b6"">[7]</ref><ref type=""bibr"" target=""#b7"">[8]</ref><ref type=""bibr"" target=""#b8"">[9]</ref> in which the luxury of resource selection over time is usually not available.</note> 			<note xmlns=""http://ww",0
"some home applications are exploited to design an optimal electricity utilization rule that can balance the electricity expense and waiting time. In <ref type=""bibr"" target=""#b12"">[13]</ref>, as a more economic-friendly alternative to the conventional direct cellular communications, vehicleassisted ent capability of that user. The task accomplishment capabilities Q n 's of different mobile users are assumed to follow uniform i.i.d. distributions <ref type=""bibr"" target=""#b12"">[13]</ref>, and so are X n 's. Similarly to the previous problem, a recruiter can invoke Algorithm 1 and <ref type=""bib",0
"he network users for further performance enhancement. For instance, besides the opportunistic spectrum resource exploited in cognitive radio networks <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref>, idle virtual machines may be offered by a cloud provider as an opportun .org/ns/1.0"" place=""foot"" n=""5"" xml:id=""foot_4"">If further given that P k is optimal, then T (P k ) in (2) equals the optimal stopping time defined in<ref type=""bibr"" target=""#b0"">(1)</ref>.</note> 			<note xmlns=""http://www.tei-c.org/ns/1.0"" place=""foot"" n=""8"" xml:id=""foot_5"">Here, the expectation",0
"nts within the same shared pool of passing mobile users. For example, in a vehicle based crowdsourcing network <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b18"">19]</ref>, companies can set up access points near a highway and ask passing vehicles to advertise their products at a",0
"cloud provider can offer two types of computing resources: the regular resource and the opportunistic resource <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b17"">18]</ref>. Particularly, the opportunistic resources can be a set of virtual machines that are currently idle. To incre",0
"-tolerant feature makes the resource access problem considered in this work distinctive from the conventional distributed resource allocation problems<ref type=""bibr"" target=""#b5"">[6]</ref><ref type=""bibr"" target=""#b6"">[7]</ref><ref type=""bibr"" target=""#b7"">[8]</ref><ref type=""bibr"" target=""#b8"">[9]",0
"ing delay).</p><p>Nonetheless, these prior works mainly focus on the single-user cases and build their results on the classic optimal stopping theory <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b14"">15]</ref> that can provide the optimal online strategy for accessing the seque d>C. Background on Two-player Markov Stopping Game</head><p>Before introducing the two-player MSG, some basics of the classic Markov stopping problem <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b14"">15]</ref>  </p><formula xml:id=""formula_0"">T * = inf{1 ≤ n ≤ N |f (X n ) = γ n |X n , R n . (<label>5</label></formula><formula xml:id=""formula_7"">)</formula><p>Remark 4: Note that when K = 1, the value at timeslot n is given by <ref type=""bibr"" target=""#b13"">[14]</ref> V 1 n (X n , 1) = ess sup</p><formula xml:id=""formula_8"">P 1 ∈{P|n≤T (P)≤N } f X T (P 1 ) |X n .<label>(6)</ ision does not depend on future observations). Under a mild condition for the boundedness of X n , the single player optimal stopping time is given by<ref type=""bibr"" target=""#b13"">[14]</ref> </figDesc><table><row><cell>stopping time T is a random variable taking values in the set</cell></row><row><",0
"formula><p>Now it will be illustrated below how to recursively compute an NE strategy for each player. For ease of presentation, only symmetric games <ref type=""bibr"" target=""#b20"">[21]</ref> will be considered in this work, in which the reward functions f of all players are identical; while nonsymm",0
"r"" target=""#b18"">[19,</ref><ref type=""bibr"" target=""#b30"">31,</ref><ref type=""bibr"" target=""#b42"">43]</ref>, and fine-grain reconfigurable processors <ref type=""bibr"" target=""#b64"">[64]</ref> are all examples.</p><p>Highly configurable processors have unique benefits for the IaaS Cloud as resources any of the benefits of heterogeneous multicores while maintaining a homogeneous fabric. The CASH architecture is inspired by the Sharing Architecture <ref type=""bibr"" target=""#b64"">[64]</ref>, but improves on it with fast reconfiguration, a well defined software-hardware interface which is needed fo figuration by running the x264 video encoder <ref type=""bibr"" target=""#b4"">[5]</ref> on the CASH Architecture (which extends the Sharing Architecture <ref type=""bibr"" target=""#b64"">[64]</ref>). The video encoder is an excellent example of our target application as it has a clear QoS requirement: to </head></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>A. Architecture Overview</head><p>The CASH architecture extends the Sharing Architecture <ref type=""bibr"" target=""#b64"">[64]</ref> -a prior configurable core architecture -by <ref type=""bibr"" target=""#b0"">(1)</ref> innovating in fast recon of physical registers, etc.) <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b40"">41,</ref><ref type=""bibr"" target=""#b57"">57,</ref><ref type=""bibr"" target=""#b64"">64]</ref> allow fine grain control over resource scheduling. As fine-grain configurability is adopted in data centers, static architectures or even coarse-grain configurable architectures <ref type=""bibr"" target=""#b40"">[41,</ref><ref type=""bibr"" target=""#b57"">57,</ref><ref type=""bibr"" target=""#b64"">64]</ref>. The one drawback of these approaches is that fine-grain configurability creates complicated, non-convex opti",1
"ad><p>CASH is influenced by prior research that has effectively deployed control systems to meet QoS demands <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b34"" runtime treats the speedup as a timevarying parameter, observes the achieved QoS q(t) and uses a Q-learning approach to learn the true speedup online <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b52"">53]</ref>. This approach has the advantage that it is computationally cheap, b rchitecture.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>D. Control-based Resource Management</head><p>Like many prior approaches (e.g., <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b34 ]</ref> or split the modeling process into a piece that is provided by the application developer and a piece that is provided by the system developer <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b24"">25]</ref>. CASH differs as it does not",0
"PARSEC benchmarks <ref type=""bibr"" target=""#b4"">[5]</ref>, the apache web server <ref type=""bibr"" target=""#b2"">[3]</ref>, and the postal mail server <ref type=""bibr"" target=""#b11"">[12]</ref> to explore the CASH architecture. They are representative benchmarks that provide a measure of performance a",0
"pipeline architectures (modifiable issue width, instruction window size, number of physical registers, etc.) <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b40"">41,</ref><ref type=""bibr"" target=""#b57"">57,</ref><ref type=""bibr"" target=""#b64"">64]</ref> allow fine grain control over es have the potential to produce greater energy efficiency and cost savings than static architectures or even coarse-grain configurable architectures <ref type=""bibr"" target=""#b40"">[41,</ref><ref type=""bibr"" target=""#b57"">57,</ref><ref type=""bibr"" target=""#b64"">64]</ref>. The one drawback of these a",0
"bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b34"">35,</ref><ref type=""bibr"" target=""#b37"">38,</ref><ref type=""bibr"" target=""#b47"">48,</ref><ref type=""bibr"" target=""#b50"">51,</ref><ref type=""bibr"" target=""#b63"" b20"">21]</ref>. While the technique is general, implementations are often highly specific to individual applications (e.g., an embedded video encoder <ref type=""bibr"" target=""#b37"">[38]</ref>). Some prior work has provided more general implementations by implementing control systems at the middlewar",0
"mplement and integrate our design in GEM5 <ref type=""bibr"" target=""#b3"">[3]</ref>, on which the OS kernel (Linux 4.2) and the embedded network GARNET <ref type=""bibr"" target=""#b1"">[1]</ref> are enhanced according to our technique. The details of the simulation platform setup are shown in Table <ref",1
"various scaling delimiters on multi-threaded applications such as LLC, memory subsystem interference, workload imbalance and cache coherency, etc. In <ref type=""bibr"" target=""#b7"">[7]</ref>, a metric for assessing thread criticality, which considers both how much time a thread is performing useful w",0
"e, meaning that only one thread can get access to a critical section at a time. As shown in previous studies <ref type=""bibr"" target=""#b21"">[21,</ref><ref type=""bibr"" target=""#b12"">12,</ref><ref type=""bibr"" target=""#b13"">13,</ref><ref type=""bibr"" target=""#b17"">17,</ref><ref type=""bibr"" target=""#b22"" on by exploiting various mechanisms such as running serialized codes on the fat cores in an asymmetric CMP ( <ref type=""bibr"" target=""#b21"">[21,</ref><ref type=""bibr"" target=""#b12"">12,</ref><ref type=""bibr"" target=""#b13"">13,</ref><ref type=""bibr"" target=""#b17"">17,</ref><ref type=""bibr"" target=""#b22"" ge supply technique is exploited to speed up threads that hold locks.</p><p>In contrast to the above studies <ref type=""bibr"" target=""#b21"">[21,</ref><ref type=""bibr"" target=""#b12"">12,</ref><ref type=""bibr"" target=""#b13"">13,</ref><ref type=""bibr"" target=""#b17"">17,</ref><ref type=""bibr"" target=""#b22"" workloads. If different critical sections frequently appear at the same time, they are not sent to the large core for false serialization prevention. <ref type=""bibr"" target=""#b12"">[12]</ref> proposes a cooperative software-hardware mechanism to identify and accelerate the most critical bottlenecks,",0
"As shown in previous studies <ref type=""bibr"" target=""#b21"">[21,</ref><ref type=""bibr"" target=""#b12"">12,</ref><ref type=""bibr"" target=""#b13"">13,</ref><ref type=""bibr"" target=""#b17"">17,</ref><ref type=""bibr"" target=""#b22"">22,</ref><ref type=""bibr"">8,</ref><ref type=""bibr"" target=""#b23"">23,</ref><ref cores in an asymmetric CMP ( <ref type=""bibr"" target=""#b21"">[21,</ref><ref type=""bibr"" target=""#b12"">12,</ref><ref type=""bibr"" target=""#b13"">13,</ref><ref type=""bibr"" target=""#b17"">17,</ref><ref type=""bibr"" target=""#b22"">22]</ref>), or by predicting and prioritizing threads that are executing serial ontrast to the above studies <ref type=""bibr"" target=""#b21"">[21,</ref><ref type=""bibr"" target=""#b12"">12,</ref><ref type=""bibr"" target=""#b13"">13,</ref><ref type=""bibr"" target=""#b17"">17,</ref><ref type=""bibr"" target=""#b22"">22,</ref><ref type=""bibr"">8,</ref><ref type=""bibr"" target=""#b23"">23,</ref><ref",0
"struct visual and textual views. To highlight the sentiment information in the text, we introduce an external sentiment knowledge base, Senti-WordNet <ref type=""bibr"" target=""#b17"">[10]</ref>, which forms the sentiment view. Then, using a framework of multi-view canonical correlation analysis (CCA) res: The third view aims to characterize the sentiment aspect of the associate text. For this, we use an external knowledge base, called SentiWordNet <ref type=""bibr"" target=""#b17"">[10]</ref>. It is based on the well-known English lexical dictionary WordNet <ref type=""bibr"" target=""#b33"">[26]</ref>, , a method that concatenates low-level visual features with the mid-level features (denoted as Low&amp;SentiBank), and a textual feature-based method <ref type=""bibr"" target=""#b17"">[10]</ref> (denoted as SentiStrength<ref type=""foot"" target=""#foot_3"">3</ref> ). Note that for Low <ref type=""bibr"" tar",1
"<ref type=""bibr"" target=""#b17"">[10]</ref>, which forms the sentiment view. Then, using a framework of multi-view canonical correlation analysis (CCA) <ref type=""bibr"" target=""#b18"">[11]</ref>, we calculate a latent embedding space in which correlations among the three views are maximized. Specifical he alignments of multiple views, but it only models the linear relationship between random variables. Several nonlinear extensions such as kernel CCA <ref type=""bibr"" target=""#b18"">[11]</ref> and Deep CCA <ref type=""bibr"" target=""#b29"">[22]</ref> have been proposed to reveal nonlinear relationship b s subsection describes how to find latent correlations among multiple views using a framework of the generalization of canonical correlation analysis <ref type=""bibr"" target=""#b18"">[11]</ref>. Let X i (i ∈ {v, t, s}) denote the feature matrix of the i-th view, and the similarity between two feature can be transformed into a distance problem such that the distances in the resulting space between each pair of views for the same image are minimized <ref type=""bibr"" target=""#b18"">[11]</ref>. The objective function to learn the latent space is as follows: min Wv,Wt,Ws i, j∈{v,t,s}</p><formula xml:i i j is a covariance matrix between ϕ i (X i ) and ϕ j (X j ), and w ik represents the k-th column of the matrix W i . In the conventional kernel CCA <ref type=""bibr"" target=""#b18"">[11]</ref>, kernel trick is used in Eq. (1). To reduce the computation complexity, one can use explicit feature maps <r",1
"lations among the three views are maximized. Specifically, to capture the nonlinear relationship between features, we introduce explicit feature maps <ref type=""bibr"" target=""#b19"">[12,</ref><ref type=""bibr"" target=""#b20"">13]</ref> to CCA. Finally, using the features that are projected to the latent intractable for large-scale datasets due to their high computational complexity and memory use. In contrast, recent advances of explicit feature maps <ref type=""bibr"" target=""#b19"">[12,</ref><ref type=""bibr"" target=""#b20"">13]</ref> can convert nonlinear problems to linear problems, which can be solv A <ref type=""bibr"" target=""#b18"">[11]</ref>, kernel trick is used in Eq. (1). To reduce the computation complexity, one can use explicit feature maps <ref type=""bibr"" target=""#b19"">[12,</ref><ref type=""bibr"" target=""#b20"">13]</ref>. Let φ(x) denote an explicit feature mapping such that K i (x, x ) = f type=""bibr"" target=""#b12"">[5]</ref>.</p><p>For GIST features, attribute features, and SentiBank features, we use the random Fourier feature mapping <ref type=""bibr"" target=""#b19"">[12]</ref> to approximate the Gaussian kernel. All other histogram-based features were mapped using the exact Bhattacha",1
"ed. Specifically, to capture the nonlinear relationship between features, we introduce explicit feature maps <ref type=""bibr"" target=""#b19"">[12,</ref><ref type=""bibr"" target=""#b20"">13]</ref> to CCA. Finally, using the features that are projected to the latent embedding space, we train a sentiment cl to their high computational complexity and memory use. In contrast, recent advances of explicit feature maps <ref type=""bibr"" target=""#b19"">[12,</ref><ref type=""bibr"" target=""#b20"">13]</ref> can convert nonlinear problems to linear problems, which can be solved by linear frameworks with a low comput f>, kernel trick is used in Eq. (1). To reduce the computation complexity, one can use explicit feature maps <ref type=""bibr"" target=""#b19"">[12,</ref><ref type=""bibr"" target=""#b20"">13]</ref>. Let φ(x) denote an explicit feature mapping such that K i (x, x ) = φ(x) φ(x). Instead of using the kernel t t=""#b19"">[12]</ref> to approximate the Gaussian kernel. All other histogram-based features were mapped using the exact Bhattacharyya kernel map- ping <ref type=""bibr"" target=""#b20"">[13]</ref>. Finally, similar to <ref type=""bibr"" target=""#b16"">[9]</ref>, we reduce each kernelmapped feature to 500 di",1
"eported that the collaborative use of textual features around training images (e.g., tags and descriptions) can improve the image content recognition <ref type=""bibr"" target=""#b15"">[8,</ref><ref type=""bibr"" target=""#b16"">9]</ref>. Inspired from these studies, to bridge images and sentiment, we shoul p>The use of correlations among visual and textual features associated to images has improved several image annotation and crossmodal retrieval tasks <ref type=""bibr"" target=""#b15"">[8,</ref><ref type=""bibr"" target=""#b16"">9,</ref><ref type=""bibr"" target=""#b23"">[16]</ref><ref type=""bibr"" target=""#b24"" textual features of an image using a traditional bagof-words approach, which counts how many times a word appears in text around the image. Following <ref type=""bibr"" target=""#b15"">[8,</ref><ref type=""bibr"" target=""#b16"">9]</ref>, we use the linear kernel for the textual features, which counts the n",0
"alysis. Thus, this paper aims to use the latent correlations among multiple views for better sentiment analysis. Canonical correlation analysis (CCA) <ref type=""bibr"" target=""#b28"">[21]</ref> is one of the techniques typically used to learn the alignments of multiple views, but it only models the li",0
"ibr"" target=""#b16"">9,</ref><ref type=""bibr"" target=""#b23"">[16]</ref><ref type=""bibr"" target=""#b24"">[17]</ref><ref type=""bibr"" target=""#b25"">[18]</ref><ref type=""bibr"" target=""#b26"">[19]</ref><ref type=""bibr"" target=""#b27"">[20]</ref>, but its effectiveness has not been fully demonstrated in image sen owing the feature design used in recent visual classification methods <ref type=""bibr"" target=""#b16"">[9,</ref><ref type=""bibr"" target=""#b25"">18,</ref><ref type=""bibr"" target=""#b26"">19]</ref>, we represent image appearance using a combination of different visual descriptors: a 3× 256 dimensional hist his multi-view formulation has recently proven to be effective for cross-modal retrieval and image annotation <ref type=""bibr"" target=""#b16"">[9,</ref><ref type=""bibr"" target=""#b26"">19]</ref>. In the following subsection, we describe how to use the latent space learned from multiple views for image s the eigenvalues of each dimension in the embedding space. p is a weighting parameter, which is set to 4 as in <ref type=""bibr"" target=""#b16"">[9,</ref><ref type=""bibr"" target=""#b26"">19]</ref>. Using Eq. ( <ref type=""formula"" target=""#formula_3"">3</ref>) for each view, we represent the final feature m",0
"10 billion photos in 2015 <ref type=""bibr"" target=""#b8"">[1]</ref>, and Instagram, which has grown to have more than 400 million monthly active users <ref type=""bibr"" target=""#b9"">[2]</ref>. These images uploaded by Internet users can be considered to reflect visual aspects of their daily lives. Suc",0
"c inference of the sentiment implied in the images has received increasing research attention in recent years <ref type=""bibr"" target=""#b11"">[4]</ref><ref type=""bibr"" target=""#b12"">[5]</ref><ref type=""bibr"" target=""#b13"">[6]</ref><ref type=""bibr"" target=""#b14"">[7]</ref>.</p><p>Conventional methods o sentiment analysis have aimed to design effective visual features for training sentiment polarity classifiers <ref type=""bibr"" target=""#b11"">[4]</ref><ref type=""bibr"" target=""#b12"">[5]</ref><ref type=""bibr"" target=""#b13"">[6]</ref>. However, due to the affective gap between lowlevel visual features a while recent works have started to analyze the sentiments of unconstrained real-world images on social media <ref type=""bibr"" target=""#b11"">[4]</ref><ref type=""bibr"" target=""#b12"">[5]</ref><ref type=""bibr"" target=""#b13"">[6]</ref><ref type=""bibr"" target=""#b14"">[7]</ref>. Typically, the goal is to de To train a sen-timent polarity classifier, color histogram and SIFT-based features of images are used in <ref type=""bibr"" target=""#b11"">[4]</ref>. In <ref type=""bibr"" target=""#b12"">[5]</ref>, emotion-related adjective-noun pairs were selected for image sentiment analysis, and their classifiers, call following mid-level features: 2,000-dimensional attribute features <ref type=""bibr"" target=""#b31"">[24]</ref> and 1,200-dimensional SentiBank outputs <ref type=""bibr"" target=""#b12"">[5]</ref>.</p><p>For GIST features, attribute features, and SentiBank features, we use the random Fourier feature mappi tual view: a low-level visual feature-based method <ref type=""bibr"" target=""#b11"">[4]</ref> (denoted as Low), a mid-level visual feature-based method <ref type=""bibr"" target=""#b12"">[5]</ref> (denoted as SentiBank), a method that concatenates low-level visual features with the mid-level features (den lickr dataset Instagram dataset Random 49.78 ± 1.05% 50.06 ± 1.09% Low <ref type=""bibr"" target=""#b11"">[4]</ref> 69.44 ± 0.85% 67.16 ± 1.28% SentiBank <ref type=""bibr"" target=""#b12"">[5]</ref> 70.01 ± 0.63% 67.26 ± 1.12% Low&amp;SentiBank 70.54 ± 1.00% 68.03 ± 1.36% SentiStrength <ref type=""bibr"" targ rity classifier.</p><p>In this paper, we exploit a linear SVM, which is also used in the conventional methods <ref type=""bibr"" target=""#b11"">[4,</ref><ref type=""bibr"" target=""#b12"">5]</ref>. Note that although this paper focuses on binary classification as well as the conventional methods <ref type= >5]</ref>. Note that although this paper focuses on binary classification as well as the conventional methods <ref type=""bibr"" target=""#b11"">[4,</ref><ref type=""bibr"" target=""#b12"">5]</ref>, our method can be easily extended to multi-class sentiment classification (e.g., positive, negative, and neur each dataset. Since this experiment targets on the binary classification problem following the previous works <ref type=""bibr"" target=""#b11"">[4,</ref><ref type=""bibr"" target=""#b12"">5]</ref>, we discarded the images labeled by ""neutral"" and the images resulting in disagreement among workers. Note tha",0
"ernal knowledge base, called SentiWordNet <ref type=""bibr"" target=""#b17"">[10]</ref>. It is based on the well-known English lexical dictionary WordNet <ref type=""bibr"" target=""#b33"">[26]</ref>, and has been utilized in text-based opinion mining tasks <ref type=""bibr"" target=""#b34"">[27]</ref>.</p><p>I",0
"target=""#b10"">[3]</ref>. Thus, automatic inference of the sentiment implied in the images has received increasing research attention in recent years <ref type=""bibr"" target=""#b11"">[4]</ref><ref type=""bibr"" target=""#b12"">[5]</ref><ref type=""bibr"" target=""#b13"">[6]</ref><ref type=""bibr"" target=""#b14"" f>.</p><p>Conventional methods of image sentiment analysis have aimed to design effective visual features for training sentiment polarity classifiers <ref type=""bibr"" target=""#b11"">[4]</ref><ref type=""bibr"" target=""#b12"">[5]</ref><ref type=""bibr"" target=""#b13"">[6]</ref>. However, due to the affectiv ref type=""bibr"" target=""#b22"">15]</ref>, while recent works have started to analyze the sentiments of unconstrained real-world images on social media <ref type=""bibr"" target=""#b11"">[4]</ref><ref type=""bibr"" target=""#b12"">[5]</ref><ref type=""bibr"" target=""#b13"">[6]</ref><ref type=""bibr"" target=""#b14"" arity of images, i.e., positive or negative. To train a sen-timent polarity classifier, color histogram and SIFT-based features of images are used in <ref type=""bibr"" target=""#b11"">[4]</ref>. In <ref type=""bibr"" target=""#b12"">[5]</ref>, emotion-related adjective-noun pairs were selected for image se mbedding-based approach with the following conventional methods, which exploit either visual or textual view: a low-level visual feature-based method <ref type=""bibr"" target=""#b11"">[4]</ref> (denoted as Low), a mid-level visual feature-based method <ref type=""bibr"" target=""#b12"">[5]</ref> (denoted a eature-based method <ref type=""bibr"" target=""#b17"">[10]</ref> (denoted as SentiStrength<ref type=""foot"" target=""#foot_3"">3</ref> ). Note that for Low <ref type=""bibr"" target=""#b11"">[4]</ref>, we use the same Table <ref type=""table"">2</ref>. Average and standard deviation of the classification accura le"">2</ref>. Average and standard deviation of the classification accuracy of image sentiment polarity for 10 runs in each dataset. Note that for Low <ref type=""bibr"" target=""#b11"">[4]</ref>, we use the same visual feature set as those described in Sec. 3.1, except for SentiBank outputs.</p><p>Metho et as those described in Sec. 3.1, except for SentiBank outputs.</p><p>Method Flickr dataset Instagram dataset Random 49.78 ± 1.05% 50.06 ± 1.09% Low <ref type=""bibr"" target=""#b11"">[4]</ref> 69.44 ± 0.85% 67.16 ± 1.28% SentiBank <ref type=""bibr"" target=""#b12"">[5]</ref> 70.01 ± 0.63% 67.26 ± 1.12% Lo timent labels, we learn a sentiment polarity classifier.</p><p>In this paper, we exploit a linear SVM, which is also used in the conventional methods <ref type=""bibr"" target=""#b11"">[4,</ref><ref type=""bibr"" target=""#b12"">5]</ref>. Note that although this paper focuses on binary classification as wel >[4,</ref><ref type=""bibr"" target=""#b12"">5]</ref>. Note that although this paper focuses on binary classification as well as the conventional methods <ref type=""bibr"" target=""#b11"">[4,</ref><ref type=""bibr"" target=""#b12"">5]</ref>, our method can be easily extended to multi-class sentiment classifica bels of images via crowdsourcing. Conventional methods exploited pseudo sentiment labels using the automatic annotation algorithm based on image tags <ref type=""bibr"" target=""#b11"">[4,</ref><ref type=""bibr"" target=""#b14"">7]</ref>, but it is unreliable due to the noisy tags or lack of tags. To the be mber of positive and negative images in each dataset. Since this experiment targets on the binary classification problem following the previous works <ref type=""bibr"" target=""#b11"">[4,</ref><ref type=""bibr"" target=""#b12"">5]</ref>, we discarded the images labeled by ""neutral"" and the images resulting",0
"ord dictionary with a 2-layer spatial pyramid and max pooling. We also extract the following mid-level features: 2,000-dimensional attribute features <ref type=""bibr"" target=""#b31"">[24]</ref> and 1,200-dimensional SentiBank outputs <ref type=""bibr"" target=""#b12"">[5]</ref>.</p><p>For GIST features, a",0
"ns <ref type=""bibr"" target=""#b13"">[6]</ref>. In addition, we will introduce the deep learning-based features <ref type=""bibr"" target=""#b37"">[30,</ref><ref type=""bibr"" target=""#b38"">31]</ref>, which have significantly improved many computer vision tasks, into the proposed framework. Furthermore, we w",0
"entiment, which enables several applications including opinion mining about social events, product marketing, and affective human-machine interaction <ref type=""bibr"" target=""#b10"">[3]</ref>. Thus, automatic inference of the sentiment implied in the images has received increasing research attention",0
"linear relationship between random variables. Several nonlinear extensions such as kernel CCA <ref type=""bibr"" target=""#b18"">[11]</ref> and Deep CCA <ref type=""bibr"" target=""#b29"">[22]</ref> have been proposed to reveal nonlinear relationship between the variables. However, these methods are intrac",0
"nd crossmodal retrieval tasks <ref type=""bibr"" target=""#b15"">[8,</ref><ref type=""bibr"" target=""#b16"">9,</ref><ref type=""bibr"" target=""#b23"">[16]</ref><ref type=""bibr"" target=""#b24"">[17]</ref><ref type=""bibr"" target=""#b25"">[18]</ref><ref type=""bibr"" target=""#b26"">[19]</ref><ref type=""bibr"" target=""#b",0
"rious situations and share them on the Web. Two pertinent examples that are currently popular are Flickr, which hosted over 10 billion photos in 2015 <ref type=""bibr"" target=""#b8"">[1]</ref>, and Instagram, which has grown to have more than 400 million monthly active users <ref type=""bibr"" target=""#b",0
"in routing have also been explored on the supply side, where individual routing nodes may act strategically <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b14"">15]</ref>. We instead consider consumer incentives and assume a single provider coordinates the entire network.</p><p>P",1
"s of business data, which typically have deadlines. Centralized traffic engineering (TE) techniques have been proposed to improve network utilization <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b19"">20]</ref> without affecting low latency traffic and with explicit support for ble to an auction. The need to combine traffic engineering with pricing. Pricing can guard existing TE techniques that improve WAN utilization (e.g., <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b19"">20]</ref>) against strategic users. By quoting lower prices for more flexible he timestep corresponding to its deadline. A sizeable portion of inter-datacenter transfers have deadlines, and can be modeled using this abstraction <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b21"">22]</ref>. For example, periodic index refreshes are expected to be fully avai , but note that they can be handled as a special case of our solution ( ?4.4). Other portions of the WAN traffic may not be governed by any TE scheme <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" target=""#b21"">22]</ref>. For example, there could be ink costs and (2) optimizes for social welfare. Further, by using prices, SAM is mostly protected from strategic users ( ?4.2). Similar to prior work <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b19"">20]</ref>, we execute SAM once every few minutes. This leaves enough time to p l high-pri requests in our simulations. We assume that the bandwidth required for such requests is known a priori (e.g., from historical usage, as in <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b21"">22]</ref>), and is appropriately reserved on all links of the network. The lin ef type=""bibr"" target=""#b10"">11]</ref>.</p><p>Traffic engineering for datacenter WANs has been drawn recent attention from both industry and academia <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" target=""#b23"">24]</ref>. SWAN <ref type=""bibr"" targe lier, Pretium sets aside some capacity to account for ad hoc high priority traffic; the volume to be set aside is estimated based on historical usage <ref type=""bibr"" target=""#b17"">[18]</ref>. When unexpected congestion occurs, perhaps because of more high-pri traffic or network faults, Pretium's sc industry and academia <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" target=""#b23"">24]</ref>. SWAN <ref type=""bibr"" target=""#b17"">[18]</ref> and B4 <ref type=""bibr"" target=""#b19"">[20]</ref> aim to improve the utilization of inter-DC WAN. However, th",1
"are not directly applicable; but there is a rich and relevant literature on incentive-aware online scheduling <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b16"">17]</ref> that can possibly be adapted to WAN allocation. Regardless of the feasibility of auctions, we advocate a pric",0
"saturated -each byte is priced accordingly -then along the path Price Menu: Transfer from S to T time interval <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref> time interval <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b0"">1]</ref> Network (all capaci idely studied problem in networking. Notable works include adaptive congestion avoidance <ref type=""bibr"" target=""#b20"">[21]</ref>, oblivious routing <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b13"">14]</ref>, and finding suitable routing parameters for given protocols (e.g., OS",0
"d interface are different in that requests specify deadlines and demands, and the provider uses traffic engineering to accommodate multiple requests. <ref type=""bibr"" target=""#b31"">[32]</ref> proposes destination-based tiered pricing for transit ISPsbased on both the traffic demand, as well the cost",0
"recent advances in combinatorial market design and statistical learning <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b18"">19]</ref>. Second, Pretium plans well into the future so it can effectively balance price and service guarantees. It de dow, the price selection will be approximately optimal for the upcoming window as well. See <ref type=""bibr"" target=""#b5"">[6]</ref> and Theorem 52 of <ref type=""bibr"" target=""#b18"">[19]</ref> for a formalization. Impact of dynamic prices on users. In Pretium, the price of a request is unknown until ype=""bibr"" target=""#b2"">[3]</ref>, and given sufficient data it is possible to learn those prices <ref type=""bibr"" target=""#b5"">[6]</ref>. Hsu et al. <ref type=""bibr"" target=""#b18"">[19]</ref> point out that such prices can be used to guide online allocation. An alternative line of work <ref type=""bi",0
"given protocols (e.g., OSPF <ref type=""bibr"" target=""#b13"">[14]</ref>). Recent papers consider the objective of imposing fairness in a shared network <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b10"">11]</ref>.</p><p>Traffic engineering for datacenter WANs has been drawn recent",0
"e observed requests, closing the loop. Updates are based on dual pricing, and recent advances in combinatorial market design and statistical learning <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b18"">19]</ref>. Second, Pretium plans well into and if there are suf-ficiently many requests per time window, the price selection will be approximately optimal for the upcoming window as well. See <ref type=""bibr"" target=""#b5"">[6]</ref> and Theorem 52 of <ref type=""bibr"" target=""#b18"">[19]</ref> for a formalization. Impact of dynamic prices on u et"" prices is a classical result in economics <ref type=""bibr"" target=""#b2"">[3]</ref>, and given sufficient data it is possible to learn those prices <ref type=""bibr"" target=""#b5"">[6]</ref>. Hsu et al. <ref type=""bibr"" target=""#b18"">[19]</ref> point out that such prices can be used to guide online a",0
"n for future work.</p><p>Incentive issues in routing have also been explored on the supply side, where individual routing nodes may act strategically <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b14"">15]</ref>. We instead consider consumer incentives and assume a single provide",0
"Updates are based on dual pricing, and recent advances in combinatorial market design and statistical learning <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b18"">19]</ref>. Second, Pretium plans well into the future so it can effectively balan b18"">[19]</ref> point out that such prices can be used to guide online allocation. An alternative line of work <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b11"">12]</ref> employs online learning methods to find near-optimal prices, rather tha",0
"=""#foot_0"">1</ref> via a first-order approximation of localized spectral filters on graphs <ref type=""bibr"" target=""#b11"">(Hammond et al., 2011;</ref><ref type=""bibr"" target=""#b5"">Defferrard et al., 2016)</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2.1"">SPECTRAL GRAPH CONVOLUT imum K steps away from the central node (K th -order neighborhood). The complexity of evaluating Eq. 5 is O(|E|), i.e. linear in the number of edges. <ref type=""bibr"" target=""#b5"">Defferrard et al. (2016)</ref> use this K-localized convolution to define a convolutional neural network on graphs.</p>< in a pre-processing step.Our method is based on spectral graph convolutional neural networks, introduced in Bruna et al. (2014) and later extended by<ref type=""bibr"" target=""#b5"">Defferrard et al. (2016)</ref> with fast localized convolutions. In contrast to these works, we consider here the task o number of simplifications (see Section 2.2) can be introduced to the original frameworks of<ref type=""bibr"" target=""#b3"">Bruna et al. (2014)</ref> and<ref type=""bibr"" target=""#b5"">Defferrard et al. (2016)</ref> that improve scalability and classification performance in large-scale networks.</note></",1
"ultistep pipeline including random walk generation and semi-supervised training is required where each step has to be optimized separately. Planetoid <ref type=""bibr"" target=""#b27"">(Yang et al., 2016)</ref> alleviates this by injecting label information in the process of learning embeddings.</p></di #b22"">Sen et al. (2008)</ref>) are chosen based on validation set performance for each dataset separately.</p><p>Lastly, we compare against Planetoid <ref type=""bibr"" target=""#b27"">(Yang et al., 2016)</ref>, where we always choose their bestperforming model variant (transductive vs. inductive) as a or ICA, we report the mean accuracy of 100 runs with random node orderings. Results for all other baseline methods are taken from the Planetoid paper <ref type=""bibr"" target=""#b27"">(Yang et al., 2016)</ref>. Planetoid* denotes the best model for the respective dataset out of the variants presented i ysis on random graphs.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""5.1"">DATASETS</head><p>We closely follow the experimental setup in <ref type=""bibr"" target=""#b27"">Yang et al. (2016)</ref>  <ref type=""bibr"">et al., 2008)</ref>. The datasets contain sparse bag-of-words feature vector </p><p>A knowledge graph is a set of entities connected with directed, labeled edges (relations). We follow the pre-processing scheme as described in <ref type=""bibr"" target=""#b27"">Yang et al. (2016)</ref>. We assign separate relation nodes r 1 and r 2 for each entity pair (e 1 , r, e 2 ) as (e 1 , gularization).</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""5.3"">BASELINES</head><p>We compare against the same baseline methods as in <ref type=""bibr"" target=""#b27"">Yang et al. (2016)</ref>, i.e. label propagation (LP) <ref type=""bibr"" target=""#b30"">(Zhu et al., 2003)</ref>, semi-sup rget=""#foot_2"">3</ref> and trained on the same hardware (with GPU) as our GCN model. We trained and tested our model on the same dataset splits as in <ref type=""bibr"" target=""#b27"">Yang et al. (2016)</ref> and report mean accuracy of 100 runs with random weight initializations. We used the following on) and 64 (number of hidden units).</p><p>In addition, we report performance of our model on 10 randomly drawn dataset splits of the same size as in <ref type=""bibr"" target=""#b27"">Yang et al. (2016)</ref>, denoted by GCN (rand. splits). Here, we report mean and standard error of prediction accuracy >) is a bipartite graph dataset extracted from a knowledge graph with 55,864 relation nodes and 9,891 entity nodes. Dataset statistics, as reported in<ref type=""bibr"" target=""#b27"">Yang et al. (2016)</ref>.Citation networks We consider three citation network datasets: Citeseer, Cora and Pubmed(Sen nodes that are used for training divided by the total number of nodes in each dataset. NELL<ref type=""bibr"" target=""#b4"">(Carlson et al., 2010;</ref><ref type=""bibr"" target=""#b27"">Yang et al., 2016</ref>) is a bipartite graph dataset extracted from a knowledge graph with 55,864 relation nodes and 9",0
"ural network models which operate directly on graphs and show how it can be motivated from a first-order approximation of spectral graph convolutions <ref type=""bibr"" target=""#b11"">(Hammond et al., 2011)</ref>. Secondly, we demonstrate how this form of a graph-based neural network model can be used is propagation rule can be motivated<ref type=""foot"" target=""#foot_0"">1</ref> via a first-order approximation of localized spectral filters on graphs <ref type=""bibr"" target=""#b11"">(Hammond et al., 2011;</ref><ref type=""bibr"" target=""#b5"">Defferrard et al., 2016)</ref>.</p></div> <div xmlns=""http:// ing the eigendecomposition of L in the first place might be prohibitively expensive for large graphs. To circumvent this problem, it was suggested in <ref type=""bibr"" target=""#b11"">Hammond et al. (2011)</ref> that g θ (Λ) can be well-approximated by a truncated expansion in terms of Chebyshev polyno defined as</p><formula xml:id=""formula_4"">T k (x) = 2xT k−1 (x) − T k−2 (x), with T 0 (x) = 1 and T 1 (x) = x.</formula><p>The reader is referred to <ref type=""bibr"" target=""#b11"">Hammond et al. (2011)</ref> for an in-depth discussion of this approximation.</p><p>Going back to our definition of a c",0
"cent for future work.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.2"">IMPLEMENTATION</head><p>In practice, we make use of TensorFlow <ref type=""bibr"" target=""#b0"">(Abadi et al., 2015)</ref> for an efficient GPU-based implementation<ref type=""foot"" target=""#foot_1"">2</ref> of Eq. 9 u ataset used in these experiments. We compare results on a GPU and on a CPU-only implementation<ref type=""foot"" target=""#foot_3"">4</ref> in TensorFlow <ref type=""bibr"" target=""#b0"">(Abadi et al., 2015)</ref>. Figure <ref type=""figure"" target=""#fig_1"">2</ref> summarizes the results.</p></div> <div xml",0
"ignificantly larger scale. We show that in this setting, a number of simplifications (see Section 2.2) can be introduced to the original frameworks of<ref type=""bibr"" target=""#b3"">Bruna et al. (2014)</ref> and<ref type=""bibr"" target=""#b5"">Defferrard et al. (2016)</ref> that improve scalability and c",0
"dividuals within a network. For all their benefits, the widespread use of such tools raises legitimate privacy concerns. For instance, Mislove et al. <ref type=""bibr"" target=""#b22"">[24]</ref> demonstrated how, by analysing Facebook's social network structure, as well as the attributes of some users,",1
":1608.00375v1 [cs.SI] 1 Aug 2016 cial networking sites and other internet content is policed, and anti-governmental blogs and activities are censored <ref type=""bibr"" target=""#b15"">[17,</ref><ref type=""bibr"" target=""#b16"">18]</ref>.</p><p>Against this background, we ask the question: can individuals",0
"2]</ref> is denoted by c degr , the closeness centrality <ref type=""bibr"" target=""#b3"">[5]</ref> is denoted by c clos , and the betweeness centrality <ref type=""bibr"" target=""#b1"">[3,</ref><ref type=""bibr"" target=""#b8"">10]</ref> is denoted by c betw . Specifically, given a node v i ∈ V and an undire",0
"in understanding how criminals and terrorists could escape detection, especially given their increasing reliance of social-media survival strategies <ref type=""bibr"" target=""#b25"">[27,</ref><ref type=""bibr"" target=""#b12"">14]</ref>. To date, however, this fundamental question has received little att",0
"work, we experiment with seven communitydetection algorithms implemented in the igraph package of the R language (version 1.0.1), namely: Eigenvector <ref type=""bibr"" target=""#b23"">[25]</ref>, Betweenness <ref type=""bibr"" target=""#b24"">[26]</ref>, Walktrap <ref type=""bibr"" target=""#b27"">[29]</ref>,",0
"n accuracy in fully labeled cases as well, and provides tolerance against incorrect labels.</p><p>The recently introduced transform/stability loss of <ref type=""bibr"" target=""#b20"">Sajjadi et al. (2016b)</ref> is based on the same principle as our work, and the Π-model can be seen as a special case pair of evaluations, which according to our measurements is ∼0.5 percentage points better than independent flips.</p><p>A principled comparison with <ref type=""bibr"" target=""#b20"">Sajjadi et al. (2016b)</ref> is difficult due to several reasons. They provide results only for a fairly extreme set of c nonlinearity and denoising, having two corrupted paths, and comparing the outputs of the network instead of pre-activation data of the final layer. <ref type=""bibr"" target=""#b20"">Sajjadi et al. (2016b)</ref> recently introduced a new loss function for semi-supervised learning, so called transform/",1
"results only for a fairly extreme set of augmentations (translations, flipping, rotations, stretching, and shearing) on top of fractional max pooling <ref type=""bibr"" target=""#b4"">(Graham, 2014)</ref>, which introduces random, local stretching inside the network, and is known to improve classificati",0
"ations). Given that in a separate experiment our network matched the best published result for non-augmented SVHN when extra data is used (1.69% from <ref type=""bibr"" target=""#b11"">Lee et al. (2015)</ref>), this gap is quite surprising, and leads us to conclude that fractional max pooling leads to a r traditional supervised training, our network approximately matches the state-of-the-art error rate for a single model in CIFAR-10 with augmentation <ref type=""bibr"" target=""#b11"">(Lee et al., 2015;</ref><ref type=""bibr"">Mishkin &amp; Matas, 2016</ref>) at 6.05%, and without augmentation <ref type=",0
"nces between the obtained n network outputs. As such, their technique follows the general pseudo-ensemble agreement (PEA) regularization framework of <ref type=""bibr"" target=""#b0"">Bachman et al. (2014)</ref>. In addition, they employ a mutual exclusivity loss term <ref type=""bibr"" target=""#b19"">(Saj",0
"network through dropout <ref type=""bibr"">(Srivastava et al., 2014)</ref>, dropconnect <ref type=""bibr"">(Wan et al., 2013)</ref>, or stochastic depth <ref type=""bibr"" target=""#b7"">(Huang et al., 2016)</ref> regularization methods, and in swapout networks <ref type=""bibr"" target=""#b24"">(Singh et al.,",0
"""bibr"" target=""#b33"">34,</ref><ref type=""bibr"" target=""#b40"">41]</ref> and modelling short-text similarities <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b28"">29,</ref><ref type=""bibr"" target=""#b34"">[35]</ref><ref type=""bibr"" target=""#b35 f><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b30"">31]</ref> or representation-focused <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b34"">[35]</ref><ref type=""bibr"" target=""#b35"">[36]</ref><ref type=""bibr"" target=""#b3 ar, we found thatunsurprisingly-the performance of the distributed model improves drastically in the presence of more data. Unlike some previous work <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" target=""#b36"">37]</ref> that train on clickthrough d epresentation of each term in the query and document. Our n-graph based input encoding is motivated by the trigraph encoding proposed by Huang et al. <ref type=""bibr"" target=""#b15"">[16]</ref>, but unlike their approach we don't limit our input representation to n-graphs of a fixed length. For each t l, which was reported to have the best performance on the retrieval task, as a baseline in this paper. Both the deep structured semantic model (DSSM) <ref type=""bibr"" target=""#b15"">[16]</ref> and its convolutional variant CDSSM <ref type=""bibr"" target=""#b36"">[37]</ref> consider only the document tit rk with document body text, but we can point to related papers that use short text such as title, for document ranking or related tasks. Huang et al. <ref type=""bibr"" target=""#b15"">[16]</ref> learn a distributed representation of query and title, for document ranking. The input representation is cha",1
"ent DNN models for short-text matching as either interaction-focused <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b30"">31]</ref> or representation-focused <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b15"">16,</ref><r captures both the exact term matches and the match positions. It is also similar to the indicator matching matrix proposed previously by Pang et al. <ref type=""bibr"" target=""#b30"">[31]</ref>. While the interaction matrix X perfectly captures every query term match in the document, it does not retai s include <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b33"">34]</ref>.</p><p>Pang et al. <ref type=""bibr"" target=""#b30"">[31]</ref> propose the use of matching matrices to represent the similarity of short texts, then apply a convolutional",1
"is paper considers local and distributed representations of queries and documents for use in Web page ranking. Our measure of ranking quality is NDCG <ref type=""bibr"" target=""#b16"">[17]</ref>, which rewards a ranker for returning documents with higher gain nearer to the top, where gain is determined",0
"target=""#b4"">[5]</ref>, probabilistic latent semantic analysis (PLSA) <ref type=""bibr"" target=""#b13"">[14]</ref> and latent Dirichlet allocation (LDA) <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b38"">39]</ref> learn lowdimensional vector representations of terms, and match the qu",0
"s been explored in the context of other IR scenarios, including query classification <ref type=""bibr"" target=""#b20"">[21]</ref>, query auto-completion <ref type=""bibr"" target=""#b25"">[26]</ref>, next query prediction <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b37"">38]</ref>, an",0
"tch problem that arises with local representations, it is possible to do document ranking using a distributed representation of terms. Mikolov et al. <ref type=""bibr"" target=""#b23"">[24]</ref> developed the popular word2vec embedding approach that has been used in several retrieval studies. Zheng and",0
"alized near the beginning of the relevant document. Match proximity serves as a foundation for effective methods such as sequential dependence models <ref type=""bibr"" target=""#b22"">[23]</ref>.</p><p>Finally, inexact term matches between the query and the document refer to techniques for addressing t al"". 2  In practice, the most effective IR methods leverage combinations of these techniques. Dependence models combine exact matching with proximity <ref type=""bibr"" target=""#b22"">[23]</ref>. LDA-based document models combine exact matching with inexact matching <ref type=""bibr"" target=""#b38"">[39]< target=""#foot_1"">4</ref> for indexing and retrieval.</p><p>Match positions are handled by substantially fewer models. Metzler's dependence model (DM) <ref type=""bibr"" target=""#b22"">[23]</ref> provides an inference network approach to modeling term proximity. We used the Indri implementation for our new terms can be incorporated with no adjustments to the underlying model. They can also be extended to reward matches of query phrases and proximity <ref type=""bibr"" target=""#b22"">[23]</ref>.</p><p>To deal with the vocabulary mismatch problem that arises with local representations, it is possible t",0
"luation</head><p>All evaluation and empirical analysis used the normalized discounted cumulative gain (NDCG) metric computed at positions one and ten <ref type=""bibr"" target=""#b17"">[18]</ref>. All performance metrics were averaged over queries for each run. Whenever testing for significant differenc",0
"=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b33"">34,</ref><ref type=""bibr"" target=""#b40"">41]</ref> and modelling short-text similarities <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b28"">29,</ref><ref type=""bibr"" target=""#b34 ng these lines, Guo et al. <ref type=""bibr"" target=""#b11"">[12]</ref> classify recent DNN models for short-text matching as either interaction-focused <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b30"">31]</ref> or representation-focused <r d <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b30"">31]</ref> or representation-focused <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b34"">[35]</ref><ref type=""bibr"" target=""#b3 ion of the model. These are our DSSM and CDSSM baselines. Other convolutional models that match short texts using distributed representations include <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b34"">35]</ref>, also showing good performance on short text ranking tasks.</p><p>Ou",0
"tween the matches. In contrast, latent semantic analysis (LSA) <ref type=""bibr"" target=""#b4"">[5]</ref>, probabilistic latent semantic analysis (PLSA) <ref type=""bibr"" target=""#b13"">[14]</ref> and latent Dirichlet allocation (LDA) <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b38"">",0
"entirely sidesteps the complicated machinery developed for classical ASR <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b5"">6]</ref>. It is able to do this because it i ver, has relied on simple neural network encoder and decoder models using recurrent models with LSTMs <ref type=""bibr"" target=""#b5"">[6]</ref> or GRUs <ref type=""bibr"" target=""#b3"">[4]</ref>. However, their use of hierarchy in the encoders demonstrates that better encoder networks in the model should tation per parameter. In this paper, we use very deep CNN techniques to significantly improve over previous shallow seq2seq speech recognition models <ref type=""bibr"" target=""#b3"">[4]</ref>. Our best model achieves a WER of 10.53% where our baseline acheives a WER of 14.76%. We present detailed anal p>We first begin by investigating the acoustic encoder depth of the baseline model without using any convolutional layers. Our baseline model follows <ref type=""bibr"" target=""#b3"">[4]</ref> using the skip connection technique in its time reduction. The baseline L × 3 or 3 layer BLSTM acoustic encode he number of parameters. On the WSJ ASR task, we obtained 10.5% WER without a language model, an 8.5% absolute improvement over published best result <ref type=""bibr"" target=""#b3"">[4]</ref>. While we demonstrated our results only on the seq2seq task, we believe this architecture should also signific",1
"ly yield better generalized and more robust models compared to DNNs <ref type=""bibr"" target=""#b12"">[13]</ref>. Recently, very deep CNNs architectures <ref type=""bibr"" target=""#b13"">[14]</ref> have also been shown to be successful in ASR <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" targe ery deep CNNs have been successfully applied to ASR, recently there have been several advancements in the computer vision community on very deep CNNs <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b17"">18]</ref> that have not been * Work done as Google Brain interns. explored in how how ConvLSTMs can be beneficial and replace LSTMs.</p><p>We are driven by same motivation that led to the success of very deep networks in vision <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b22",1
"ied to ASR, recently there have been several advancements in the computer vision community on very deep CNNs <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b17"">18]</ref> that have not been * Work done as Google Brain interns. explored in the speech community. We explore and appl arameters that would have been needed otherwise to build such deeper models. NiN has seen great success in computer vision, building very deep models <ref type=""bibr"" target=""#b17"">[18]</ref>. We show how to apply NiN principles in hierarchical Recurrent Neural Networks (RNNs) <ref type=""bibr"" targe eplace LSTMs.</p><p>We are driven by same motivation that led to the success of very deep networks in vision <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b22"">23]</ref> -add depth of processing usin",1
"0.1), and used 32 output channels. Gradient norm clipping to 1 was applied, together with Gaussian weight noise N (0, 0.075) and L2 weight decay 1e−5 <ref type=""bibr"" target=""#b29"">[30]</ref>. We used ADAM with the default hyperparameters described in <ref type=""bibr"" target=""#b30"">[31]</ref>, howev",0
"type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b10"">11]</ref>. Unlike Deep Neural Networks (DNNs) <ref type=""bibr"" target=""#b11"">[12]</ref>, CNNs explicitly exploit structural locality in the spectral feature space. CNNs use shared weight filters a",0
"ng=""en""> 		<body> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""1."">INTRODUCTION</head><p>The sequence-to-sequence (seq2seq) model with attention <ref type=""bibr"" target=""#b0"">[1]</ref> has recently demonstrated a promising new direction for ASR that entirely sidesteps the complicated machinery d acts as an regularizer. BN has also seen success in endto-end CTC models <ref type=""bibr"" target=""#b21"">[22]</ref>. The seq2seq attention mechanism <ref type=""bibr"" target=""#b0"">[1]</ref> has high variance in the gradient (especially from random initialization); without BN we were unable to train rget=""#b2"">[3]</ref>. In our work, we replace Listen with a network of very deep CNNs and BLSTMs. The AttendAndSpell is an attention-based transducer <ref type=""bibr"" target=""#b0"">[1]</ref>, which generates one character yi at a time:</p><formula xml:id=""formula_3"">si = DecodeRNN([yi−1, ci−1], si−1) ion without peephole connections.</p><p>The AttentionContext function generates ci with a contentbased Multi-Layer Perceptron (MLP) attention network <ref type=""bibr"" target=""#b0"">[1]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2.2."">Network in Network</head><p>In our study, w",0
"ns. explored in the speech community. We explore and apply some of these techniques in our end-to-end speech model:</p><p>1. Network-in-Network (NiN) <ref type=""bibr"" target=""#b18"">[19]</ref> increases network depth through the use of 1x1 convolutions. This allows us to increase the depth and expres",0
"arget=""#b13"">[14]</ref> have also been shown to be successful in ASR <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b16"">17]</ref>, using more non-linearities, but fewer parameters. Such a strategy can lead to more expressive models with be",0
"hinery developed for classical ASR <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b5"">6]</ref>. It is able to do this because it is not restricted by the classical inde models. To our knowledge, the previous best reported WER on WSJ without an LM was the seq2seq model with Task Loss Estimation achieving 18.0% WER in <ref type=""bibr"" target=""#b4"">[5]</ref>. Our baseline, also a seq2seq model, achieved 14.76% WER. Our model is different from that of <ref type=""bibr"" ng 18.0% WER in <ref type=""bibr"" target=""#b4"">[5]</ref>. Our baseline, also a seq2seq model, achieved 14.76% WER. Our model is different from that of <ref type=""bibr"" target=""#b4"">[5]</ref> in that we did not use location-based priors on the attention model and we used weight noise. Our best model,",0
"to give the model better spectral and temporal invariance properties, thus typically yield better generalized and more robust models compared to DNNs <ref type=""bibr"" target=""#b12"">[13]</ref>. Recently, very deep CNNs architectures <ref type=""bibr"" target=""#b13"">[14]</ref> have also been shown to be using weight sharing with convolutional filters. CNNs have shown improvement over traditional fully-connected deep neural networks on many ASR tasks <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b10"">11]</ref>, we investigate the effect of convolutional layers in seq2seq models",0
"vision community.</p><p>Convolutional Neural Networks (CNNs) <ref type=""bibr"" target=""#b7"">[8]</ref> have been successfully applied to many ASR tasks <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b10"">11]</ref>. Unlike Deep Neural Networks (D",0
"n and slower to converge when the model becomes very deep. Several architectures have been proposed recently to enable training of very deep networks <ref type=""bibr"" target=""#b26"">[27,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b28",0
"target=""#b7"">[8]</ref> have been successfully applied to many ASR tasks <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b10"">11]</ref>. Unlike Deep Neural Networks (DNNs) <ref type=""bibr"" target=""#b11"">[12]</ref>, CNNs explicitly exploit struct filters. CNNs have shown improvement over traditional fully-connected deep neural networks on many ASR tasks <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b10"">11]</ref>, we investigate the effect of convolutional layers in seq2seq models. In a hybrid system, convolutions requir",0
"have been proposed recently to enable training of very deep networks <ref type=""bibr"" target=""#b26"">[27,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b28"">29]</ref>. The idea behind these approaches is similar to the LSTM innovation -",0
"l deep model applied in image super-resolution (SR), the Super-Resolution Convolutional Neural Network (SRCNN) <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref> has demonstrated superior performance to the previous hand-crafted models either in speed and restoration quali ping between the LR and HR image spaces. Among them, the Super-Resolution Convolutional Neural Network (SRCNN) <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref> has drawn considerable attention due to its simple network structure and excellent restoration quality. Though he pioneer work is termed as the Super-Resolution Convolutional Neural Network (SRCNN) proposed by Dong et al. <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref>. Motivated by SRCNN, some problems such as face hallucination <ref type=""bibr"" target=""#b15"">[16]</ref> and dep .0""><head n=""3"">Fast Super-Resolution by CNN</head><p>We first briefly describe the network structure of SRCNN <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref>, and then we detail how we reformulate the network layer by layer. The differences between FSRCNN and SRCNN are w.tei-c.org/ns/1.0""><head n=""4.4"">Experiments for Different Upscaling Factors</head><p>Unlike existing methods <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref> that need to train a network from scratch for a different scaling factor, the proposed FSRCNN enjoys the flexib from a given low-resolution (LR) one. Recent SR algorithms are mostly learning-based (or patch-based) methods <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b4"">5,</re es are projected on a high-dimensional LR feature space, then followed by a complex mapping to another high-dimensional HR feature space. Dong et al. <ref type=""bibr"" target=""#b1"">[2]</ref> show that the mapping accuracy can be substantially improved by adopting a wider mapping layer, but at the cos ccuracy can be substantially improved by adopting a wider mapping layer, but at the cost of the running time. For example, the large SRCNN (SRCNN-Ex) <ref type=""bibr"" target=""#b1"">[2]</ref> has 57,184 parameters, which are six times larger than that for SRCNN <ref type=""bibr"">(8,032 parameters)</ref end mapping between the original LR and HR images with no pre-processing. 2) The proposed model achieves a speed up of at least 40× than the SRCNN-Ex <ref type=""bibr"" target=""#b1"">[2]</ref> while still keeping its exceptional performance. One of its small-size version can run in real-time (&gt;24 fp ing factors are the width (i.e., the number of filters in a layer) and depth (i.e., the number of layers) of the mapping layer. As indicated in SRCNN <ref type=""bibr"" target=""#b1"">[2]</ref>, a 5 × 5 layer achieves much better results than a 1 × 1 layer. But they are lack of experiments on very deep that the smallest network FSRCNN (48,12,2) achieves an average PSNR of 32.87 dB, which is already higher than that of SRCNN-Ex (32.75 dB) reported in <ref type=""bibr"" target=""#b1"">[2]</ref>. The FSRCNN (48,12,2) contains only 8,832 parameters, then the acceleration compared with SRCNN-Ex is 57184/88 databases, namely the super-resolution forest (SRF) <ref type=""bibr"" target=""#b6"">[7]</ref>, SRCNN <ref type=""bibr"" target=""#b0"">[1]</ref>, SRCNN-Ex <ref type=""bibr"" target=""#b1"">[2]</ref> and the sparse coding based network (SCN) <ref type=""bibr"" target=""#b7"">[8]</ref>. The implementations of thes",1
"h map super-resolution <ref type=""bibr"" target=""#b16"">[17]</ref> have achieved state-of-the-art results. Deeper structures have also been explored in <ref type=""bibr"" target=""#b17"">[18]</ref> and <ref type=""bibr"" target=""#b18"">[19]</ref>. Different from the conventional learning-based methods, SRCNN hard to shrink the sparse coding sub-network with no loss of mapping accuracy. Furthermore, all these networks <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b18"">19]</ref> need to process the bicubic-upscaled LR images. The proposed FS-RCNN",1
"nseTime Group Limited.  Table <ref type=""table"">5</ref>. The results of PSNR (dB), SSIM and IFC <ref type=""bibr"" target=""#b28"">[29]</ref> on the Set5 <ref type=""bibr"" target=""#b29"">[30]</ref>, Set14 <ref type=""bibr"" target=""#b8"">[9]</ref> and BSD200 <ref type=""bibr"" target=""#b24"">[25]</ref>   </p></",0
"al LR image. It is worth noting that the deconvolution layer is not equal to a simple substitute of the conventional interpolation kernel like in FCN <ref type=""bibr"" target=""#b12"">[13]</ref>, or 'unpooling+convolution' like <ref type=""bibr"" target=""#b13"">[14]</ref>. Instead, it consists of diverse t the deconvolution layer as DeConv(9, 1, d).</p><p>Different from inserting traditional interpolation kernels (e.g., bicubic or bilinear) in-network <ref type=""bibr"" target=""#b12"">[13]</ref> or having 'unpooling+convolution' <ref type=""bibr"" target=""#b13"">[14]</ref>, the deconvolution layer learns",0
"www.tei-c.org/ns/1.0""><p>As a successful deep model applied in image super-resolution (SR), the Super-Resolution Convolutional Neural Network (SRCNN) <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref> has demonstrated superior performance to the previous hand-crafted model recovering a high-resolution (HR) image from a given low-resolution (LR) one. Recent SR algorithms are mostly learning-based (or patch-based) methods <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b3"">4,</r target=""#b7"">8]</ref> that learn a mapping between the LR and HR image spaces. Among them, the Super-Resolution Convolutional Neural Network (SRCNN) <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref> has drawn considerable attention due to its simple network structure and have been successfully applied on SR. The pioneer work is termed as the Super-Resolution Convolutional Neural Network (SRCNN) proposed by Dong et al. <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref>. Motivated by SRCNN, some problems such as face hallucination <ref type= > <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3"">Fast Super-Resolution by CNN</head><p>We first briefly describe the network structure of SRCNN <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref>, and then we detail how we reformulate the network layer by layer. The d /ref>).</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.4"">Experiments for Different Upscaling Factors</head><p>Unlike existing methods <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref> that need to train a network from scratch for a different scaling factor cessing speed on large images is still unsatisfactory. For example, to upsample an 240 × 240 image by a factor of 3, the speed of the original SR-CNN <ref type=""bibr"" target=""#b0"">[1]</ref> is about 1.32 fps, which is far from real-time (24 fps). To approach real-time, we should accelerate SRCNN for exceptional performance. One of its small-size version can run in real-time (&gt;24 fps) on a generic CPU with better restoration quality than SRCNN <ref type=""bibr"" target=""#b0"">[1]</ref>. 3) We transfer the convolution layers of the proposed networks for fast training and testing across different the speed of FSRCNN (32,5,1) reaches 24.7 fps, satisfying the real-time requirement. Furthermore, the FSRCNN (32,5,1) even outperforms SRCNN (9-1-5) <ref type=""bibr"" target=""#b0"">[1]</ref> (see Table <ref type=""table"" target=""#tab_4"">3 and 4</ref>).</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0 learning-based SR algorithms that rely on external databases, namely the super-resolution forest (SRF) <ref type=""bibr"" target=""#b6"">[7]</ref>, SRCNN <ref type=""bibr"" target=""#b0"">[1]</ref>, SRCNN-Ex <ref type=""bibr"" target=""#b1"">[2]</ref> and the sparse coding based network (SCN) <ref type=""bibr"" t dataset is widely used as the training set in learningbased SR methods <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b0"">1]</ref>. As deep models generally benefit from big data, studies have found that 91 images are not enough to push a dee",0
"ef type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b7"">8]</ref> that learn a mapping between the LR",0
"=""#b16"">[17]</ref> have achieved state-of-the-art results. Deeper structures have also been explored in <ref type=""bibr"" target=""#b17"">[18]</ref> and <ref type=""bibr"" target=""#b18"">[19]</ref>. Different from the conventional learning-based methods, SRCNN directly learns an end-to-end mapping between work with no loss of mapping accuracy. Furthermore, all these networks <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b18"">19]</ref> need to process the bicubic-upscaled LR images. The proposed FS-RCNN does not only perform on the original LR",0
"""#b1"">2]</ref>. Motivated by SRCNN, some problems such as face hallucination <ref type=""bibr"" target=""#b15"">[16]</ref> and depth map super-resolution <ref type=""bibr"" target=""#b16"">[17]</ref> have achieved state-of-the-art results. Deeper structures have also been explored in <ref type=""bibr"" target",0
"f type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b7"">8]</re or most learning-based SR methods <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b7"">8]</ref>. If the network was learned directl head><p>Training dataset. The 91-image dataset is widely used as the training set in learningbased SR methods <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b0"">1]</ref>. As deep models generally benefit from big data, studies have found that in Table <ref type=""table"" target=""#tab_4"">4</ref>. We also add another two competitive methods -KK <ref type=""bibr"" target=""#b27"">[28]</ref> and A+ <ref type=""bibr"" target=""#b4"">[5]</ref> for comparison. Note that these results are obtained using different datasets, and our models are trained on t",0
"o a simple substitute of the conventional interpolation kernel like in FCN <ref type=""bibr"" target=""#b12"">[13]</ref>, or 'unpooling+convolution' like <ref type=""bibr"" target=""#b13"">[14]</ref>. Instead, it consists of diverse automatically learned upsampling kernels (see Figure <ref type=""figure"" tar traditional interpolation kernels (e.g., bicubic or bilinear) in-network <ref type=""bibr"" target=""#b12"">[13]</ref> or having 'unpooling+convolution' <ref type=""bibr"" target=""#b13"">[14]</ref>, the deconvolution layer learns a set of upsampling kernel for the input feature maps. As shown in Figure <r",0
"we dynamically analyze the sensitivity of the teachers' noisy votes; for this purpose, we use the state-of-the-art moments accountant technique from <ref type=""bibr"" target=""#b0"">Abadi et al. (2016)</ref>, which tightens the privacy bound when the topmost vote has a large quorum. As a result, for M for a restricted class of student classifiers-in effect, limiting applicability to logistic regression with convex loss. Also, unlike the methods of <ref type=""bibr"" target=""#b0"">Abadi et al. (2016)</ref>, which represent the state-of-the-art in differentiallyprivate deep learning, our techniques m emble.</p><p>MNIST and SVHN, our techniques provide a privacy/utility tradeoff that equals or improves upon bespoke learning methods such as those of <ref type=""bibr"" target=""#b0"">Abadi et al. (2016)</ref>.</p><p>Section 5 further discusses the related work. Building on this related work, our contri eatly reduce the privacy loss by radically reducing the need for supervision. • We present a new application of the moments accountant technique from <ref type=""bibr"" target=""#b0"">Abadi et al. (2016)</ref> for improving the differential-privacy analysis of knowledge transfer, which allows the traini ial-privacy bound of (2.04, 10 −5 ) for MNIST and (8.19, 10 −6 ) for SVHN, respectively with accuracy of 98.00% and 90.66%. In comparison, for MNIST, <ref type=""bibr"" target=""#b0"">Abadi et al. (2016)</ref> obtain a looser (8, 10 −5 ) privacy bound and 97% accuracy. For SVHN, <ref type=""bibr"" target= ALYSIS</head><p>To better keep track of the privacy cost, we use recent advances in privacy cost accounting. The moments accountant was introduced by <ref type=""bibr"" target=""#b0"">Abadi et al. (2016)</ref>, building on previous work <ref type=""bibr"" target=""#b8"">(Bun &amp; Steinke, 2016;</ref><ref t la><p>] is the moment generating function of the privacy loss random variable.</p><p>The following properties of the moments accountant are proved in <ref type=""bibr"" target=""#b0"">Abadi et al. (2016)</ref>.</p><p>Theorem 1. 1. [Composability] Suppose that a mechanism M consists of a sequence of adap −5 ) and (8.19, 10 −6 ) achieve accuracies of 98.00% and 90.66%. These results improve the differential privacy state-of-the-art for these datasets. <ref type=""bibr"" target=""#b0"">Abadi et al. (2016)</ref> previously obtained 97% accuracy with a (8, 10 −5 ) bound on MNIST, starting from an inferior its privacy bounds are given per-parameter, and the large number of parameters prevents the technique from providing a meaningful privacy guarantee. <ref type=""bibr"" target=""#b0"">Abadi et al. (2016)</ref> provided stricter bounds on the privacy loss induced by a noisy SGD by introducing the moments l privacy guarantees of our PATE approach. Namely, we keep track of the privacy budget throughout the student's training using the moments accountant <ref type=""bibr"" target=""#b0"">(Abadi et al., 2016)</ref>. When teachers reach a strong quorum, this allows us to bound privacy costs more strictly.</p and private SVHN model. However, this comes at the cost of assuming that nonprivate unlabeled data is available, an assumption that is not shared by <ref type=""bibr"" target=""#b0"">(Abadi et al., 2016;</ref><ref type=""bibr"" target=""#b33"">Shokri &amp; Shmatikov, 2015)</ref>. <ref type=""bibr"" target=""#",1
"<div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.1"">DIFFERENTIAL PRIVACY PRELIMINARIES AND A SIMPLE ANALYSIS OF PATE</head><p>Differential privacy <ref type=""bibr"" target=""#b16"">(Dwork et al., 2006b;</ref><ref type=""bibr"" target=""#b12"">Dwork, 2011)</ref>  </p><formula xml:id=""formula_2"">Pr[M(d) ∈ "">(Aggarwal, 2005)</ref>. An alternative definition, differential privacy, established itself as a rigorous standard for providing privacy guarantees <ref type=""bibr"" target=""#b16"">(Dwork et al., 2006b)</ref>. In contrast to k-anonymity, differential privacy is a property of the randomized algorithm",0
"s can be surprisingly hard to reason about; for example, even a single data item can greatly impact machine learning models trained on a large corpus <ref type=""bibr"" target=""#b10"">(Chaudhuri et al., 2011)</ref>. Therefore, to limit the effect of any single sensitive data item on the student's learn",0
"cal records or genetic sequences <ref type=""bibr"" target=""#b2"">(Alipanahi et al., 2015;</ref><ref type=""bibr"" target=""#b23"">Kannan et al., 2016;</ref><ref type=""bibr"" target=""#b25"">Kononenko, 2001;</ref><ref type=""bibr"" target=""#b35"">Sweeney, 1997)</ref>. Ideally, in those cases, the learning algori",0
"acts, private photographs or correspondence, or even medical records or genetic sequences <ref type=""bibr"" target=""#b2"">(Alipanahi et al., 2015;</ref><ref type=""bibr"" target=""#b23"">Kannan et al., 2016;</ref><ref type=""bibr"" target=""#b25"">Kononenko, 2001;</ref><ref type=""bibr"" target=""#b35"">Sweeney,",0
"onal information to solve the ill-posed reconstruction problem.</p><p>Learning upscaling filters was briefly suggested in the footnote of Dong et.al. <ref type=""bibr"" target=""#b5"">[6]</ref>. However, the importance of integrating it into the CNN as part of the SR operation was not fully recognised a f integrating it into the CNN as part of the SR operation was not fully recognised and the option not explored. Additionally, as noted by Dong et al. <ref type=""bibr"" target=""#b5"">[6]</ref>, there are no efficient implementations of a convolution layer whose output size is larger than the input size as well as tanh activation function. We first evaluate the power of the sub-pixel convolution layer by comparing against SRCNN's standard 9-1-5 model <ref type=""bibr"" target=""#b5"">[6]</ref>. Here, we follow the approach in <ref type=""bibr"" target=""#b5"">[6]</ref>, using relu as the activation functio ub-pixel convolution layer by comparing against SRCNN's standard 9-1-5 model <ref type=""bibr"" target=""#b5"">[6]</ref>. Here, we follow the approach in <ref type=""bibr"" target=""#b5"">[6]</ref>, using relu as the activation function for our models in this experiment, and training a set of models with 91 is selected here in order to allow a straight-forward comparison with results from previous published results<ref type=""bibr"" target=""#b30"">[31,</ref><ref type=""bibr"" target=""#b5"">6]</ref>.</note> 		</body> 		<back> 			<div type=""references"">  				<listBibl>  <biblStruct xml:id=""b0""> 	<analytic> 		<",1
"based <ref type=""bibr"" target=""#b34"">[35]</ref>, image statisticsbased <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b45"">46,</ref><ref type=""bibr"" target=""#b11"">12]</ref> and patch-based <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bib",0
"#b43"">[44]</ref> trained a cascaded sparse coding network from end to end inspired by LISTA (Learning iterative shrinkage and thresholding algorithm) <ref type=""bibr"" target=""#b15"">[16]</ref> to fully exploit the natural sparsity of images. The network structure is not limited to neural networks, fo",0
"b8"">[9,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b45"">46,</ref><ref type=""bibr"" target=""#b11"">12]</ref> and patch-based <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b42"">43,</ref><ref type=""bibr"" target=""#b51"">52,</ref><ref type=""bibr"" target=""#b12""> ed our best model's run time on Set14<ref type=""foot"" target=""#foot_2"">3</ref> with an upscale factor of 3. We evaluate the run time of other methods <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b50"">51,</ref><ref type=""bibr"" target=""#b38"">39]</ref> from the Matlab codes provided",0
""">33]</ref>, satellite imaging <ref type=""bibr"" target=""#b37"">[38]</ref>, face recognition <ref type=""bibr"" target=""#b16"">[17]</ref> and surveillance <ref type=""bibr"" target=""#b52"">[53]</ref>. The global SR problem assumes LR data to be a low-pass filtered (blurred), downsampled and noisy version of",0
"fits of two state-of-the art compressed caches-Decoupled Compressed Cache (DCC) [Sardashti and Wood 2013a,  2013b]  and Skewed Compressed Cache (SCC) <ref type=""bibr"" target=""#b26"">[Sardashti et al. 2014</ref>]-with a more practical and simpler design. YACC's cache layout is similar to conventional hes-Decoupled Compressed Cache (DCC) <ref type=""bibr"">[Sardashti and</ref><ref type=""bibr"">Wood 2013a, 2013b]</ref> and Skewed Compressed Cache (SCC) <ref type=""bibr"" target=""#b26"">[Sardashti et al. 2014</ref>]-try to achieve some of these goals, although at extra costs and complexities. DCC <ref ty n overheads when a block size grows but requires the additional area and complexity of backward pointers to maintain the decoupled mapping.</p><p>SCC <ref type=""bibr"" target=""#b26"">[Sardashti et al. 2014</ref>] also uses super-block tags but eliminates DCC's backward pointers. SCC makes tag-data map complexity due to the extra level of indirection (i.e., backward pointers) and the need to separately manage block and super-block replacements. SCC <ref type=""bibr"" target=""#b26"">[Sardashti et al. 2014</ref>] eliminates DCC's backward pointers and simplifies cache replacement but adds the complexi licit data alignment network.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2.3."">SCC: A State-of-the-Art Compressed Cache</head><p>SCC <ref type=""bibr"" target=""#b26"">[Sardashti et al. 2014</ref>] is a state-of-the-art compressed cache design that picks the best of the alternatives dis einhardt 2005;</ref><ref type=""bibr"" target=""#b17"">Kim et al. 2002;</ref><ref type=""bibr"">Sardashti and</ref><ref type=""bibr"">Wood 2013a, 2013b;</ref><ref type=""bibr"" target=""#b26"">Sardashti et al. 2014]</ref>.</p><p>A compressed cache organization must provide tags to map additional blocks, support a relatively simple change. We will next discuss YACC design in more detail.</p><p>3.1.1 YACC Tag Format. Like SCC, YACC uses sparse super-block tags <ref type=""bibr"" target=""#b26"">[Sardashti et al. 2014]</ref> to map the blocks that are compressed in the corresponding data. In Figure <ref type=""fig t YACC and other compressed caches at L3. Table <ref type=""table"">II</ref> shows the main parameters. We use 64-byte cache block sizes. For YACC, SCC <ref type=""bibr"" target=""#b26"">[Sardashti et al. 2014]</ref> and DCC <ref type=""bibr"">[Sardashti and</ref><ref type=""bibr"">Wood 2013a, 2013b]</ref>, w logic <ref type=""bibr"">[Sardashti and</ref><ref type=""bibr"">Wood 2013a, 2013b]</ref>. We consider this extra overhead in our simulation. ? SCC models <ref type=""bibr"" target=""#b26"">[Sardashti et al. 2014]</ref> with four-block super-blocks and 16-byte subblocks. We use an LRU-based replacement algor Our previous proposals, DCC and SCC, reduce the extra hardware complexity induced for storing and retrieving compressed data blocks. Compared to SCC <ref type=""bibr"" target=""#b26"">[Sardashti et al. 2014]</ref>, we addressed most of the issues of the compaction in caches: very limited tag and metada",1
"be differentially important in different context ( §3.5). To include sensitivity to this fact, our model includes two levels of attention mechanisms <ref type=""bibr"" target=""#b0"">(Bahdanau et al., 2014;</ref><ref type=""bibr"" target=""#b29"">Xu et al., 2015)</ref> -one at the word level and one at the nt components in the following sections.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2.1"">GRU-based sequence encoder</head><p>The GRU <ref type=""bibr"" target=""#b0"">(Bahdanau et al., 2014)</ref> uses a gating mechanism to track the state of sequences without using separate memory cell th words w it , t ∈ [0, T ], we first embed the words to vectors through an embedding matrix W e , x ij = W e w ij .</p><p>We use a bidirectional GRU <ref type=""bibr"" target=""#b0"">(Bahdanau et al., 2014)</ref> to get annotations of words by summarizing information from both directions for words, and >(Li et al., 2015)</ref> and language modeling <ref type=""bibr"" target=""#b14"">(Lin et al., 2015)</ref>.</p><p>The attention mechanism was proposed by <ref type=""bibr"" target=""#b0"">(Bahdanau et al., 2014)</ref> in machine translation. The encoder decoder framework is used and an attention mechanism i",1
"it also provides insight into which words and sentences contribute to the classification decision which can be of value in applications and analysis <ref type=""bibr"" target=""#b21"">(Shen et al., 2014;</ref><ref type=""bibr"">Gao et al., 2014)</ref>.</p><p>The key difference to previous work is that ou",1
"To include sensitivity to this fact, our model includes two levels of attention mechanisms <ref type=""bibr"" target=""#b0"">(Bahdanau et al., 2014;</ref><ref type=""bibr"" target=""#b29"">Xu et al., 2015)</ref> -one at the word level and one at the sentence level -that let the model to pay more or less att ework is used and an attention mechanism is used to select the reference words in original language for words in foreign language before translation. <ref type=""bibr"" target=""#b29"">Xu et al. (2015)</ref> uses the attention mechanism in image caption generation to select the relevant image regions wh",1
"</ref><ref type=""bibr"" target=""#b10"">Kumar et al., 2015;</ref><ref type=""bibr"" target=""#b4"">Hermann et al., 2015)</ref>, and image question answering <ref type=""bibr"" target=""#b30"">(Yang et al., 2015)</ref>. Unlike these works, we explore a hierarchical attention mechanism (to the best of our knowle",0
"rget=""#b8"">Kim (2014)</ref> use neural networks for text classification. The architecture is a direct application of CNNs, as used in computer vision <ref type=""bibr"" target=""#b12"">(LeCun et al., 1998)</ref>, albeit with NLP interpretations. <ref type=""bibr"" target=""#b7"">Johnson and Zhang (2014)</re",0
"ing, 2012;</ref><ref type=""bibr"" target=""#b6"">Joachims, 1998)</ref>. More recent approaches used deep learning, such as convolutional neural networks <ref type=""bibr"" target=""#b1"">(Blunsom et al., 2014)</ref> and recurrent neural networks based on long short-term memory (LSTM) <ref type=""bibr"" targe",0
"ral language question answering <ref type=""bibr"" target=""#b23"">(Sukhbaatar et al., 2015;</ref><ref type=""bibr"" target=""#b10"">Kumar et al., 2015;</ref><ref type=""bibr"" target=""#b4"">Hermann et al., 2015)</ref>, and image question answering <ref type=""bibr"" target=""#b30"">(Yang et al., 2015)</ref>. Unli",0
"ns/1.0""><head n=""3.3"">Model configuration and training</head><p>We split documents into sentences and tokenize each sentence using Stanford's CoreNLP <ref type=""bibr"" target=""#b17"">(Manning et al., 2014)</ref>. We only retain words appearing more than 5 times in building the vocabulary and replace t",0
"s to text. It has broad applications including topic labeling <ref type=""bibr"" target=""#b28"">(Wang and Manning, 2012)</ref>, sentiment classification <ref type=""bibr"" target=""#b16"">(Maas et al., 2011;</ref><ref type=""bibr"" target=""#b19"">Pang and Lee, 2008)</ref>, and spam detection <ref type=""bibr""",0
"s, such as n-grams, and then use a linear model or kernel methods on this representation <ref type=""bibr"" target=""#b28"">(Wang and Manning, 2012;</ref><ref type=""bibr"" target=""#b6"">Joachims, 1998)</ref>. More recent approaches used deep learning, such as convolutional neural networks <ref type=""bibr""",0
"network to compose the sentence vectors to get a document vectors. There are some other works that use hierarchical structure in sequence generation <ref type=""bibr"" target=""#b13"">(Li et al., 2015)</ref> and language modeling <ref type=""bibr"" target=""#b14"">(Lin et al., 2015)</ref>.</p><p>The attent",0
"ime I in Phoenix, I will go back here. || Highly recommend. Although neural-network-based approaches to text classification have been quite effective <ref type=""bibr"" target=""#b8"">(Kim, 2014;</ref><ref type=""bibr"" target=""#b31"">Zhang et al., 2015;</ref><ref type=""bibr"" target=""#b7"">Johnson and Zhang get=""#b26"">(Tang et al., 2015)</ref> and <ref type=""bibr"" target=""#b31"">(Zhang et al., 2015)</ref>.</p><p>CNN-word Word based CNN models like that of <ref type=""bibr"" target=""#b8"">(Kim, 2014)</ref> are used. CNN-char Character level CNN models are reported in <ref type=""bibr"" target=""#b31"">(Zhang et corresponding sentences. Note that this happens in a multiclass setting, that is, detection happens before the selection of the topic! 4 Related Work <ref type=""bibr"" target=""#b8"">Kim (2014)</ref> use neural networks for text classification. The architecture is a direct application of CNNs, as used",0
"br"" target=""#b8"">(Kim, 2014;</ref><ref type=""bibr"" target=""#b31"">Zhang et al., 2015;</ref><ref type=""bibr"" target=""#b7"">Johnson and Zhang, 2014;</ref><ref type=""bibr"" target=""#b26"">Tang et al., 2015)</ref>, in this paper we test the hypothesis that better representations can be obtained by incorpora cter-based CNN, and Conv-GRNN, LSTM-GRNN. These baseline methods and results are reported in <ref type=""bibr"" target=""#b31"">(Zhang et al., 2015;</ref><ref type=""bibr"" target=""#b26"">Tang et al., 2015)</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.2.1"">Linear methods</head><p>Li tion, and the remaining 10% for test, unless stated otherwise.</p><p>Yelp reviews are obtained from the Yelp Dataset Challenge in 2013, 2014 and 2015 <ref type=""bibr"" target=""#b26"">(Tang et al., 2015)</ref>. There are five levels of ratings from 1 to 5 (higher is better). IMDB reviews are obtained f 13</ref>) is used as feature set.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.2.2"">SVMs</head><p>SVMs-based methods are reported in <ref type=""bibr"" target=""#b26"">(Tang et al., 2015)</ref>, including SVM+Unigrams, Bigrams, Text Features, AverageSG, SSWE. In detail, Unigrams and Big .</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.2.3"">Neural Network methods</head><p>The neural network based methods are reported in <ref type=""bibr"" target=""#b26"">(Tang et al., 2015)</ref> and <ref type=""bibr"" target=""#b31"">(Zhang et al., 2015)</ref>.</p><p>CNN-word Word based CNN s a single sequence and the average of the hidden states of all words is used as feature for classification. Conv-GRNN and LSTM-GRNN were proposed by <ref type=""bibr"" target=""#b26"">(Tang et al., 2015)</ref>. They also explore the hierarchical structure: a CNN or LSTM provides a sentence vector, and ucture to for sentence classification <ref type=""bibr"" target=""#b11"">(Lai et al., 2015;</ref><ref type=""bibr"" target=""#b32"">Zhou et al., 2015)</ref>. <ref type=""bibr"" target=""#b26"">Tang et al. (2015)</ref> use hierarchical structure in sentiment classification. They first use a CNN or LSTM to get a",0
"tence and use a treestructured LSTMs for classification. There are also some works that combine LSTM and CNN structure to for sentence classification <ref type=""bibr"" target=""#b11"">(Lai et al., 2015;</ref><ref type=""bibr"" target=""#b32"">Zhou et al., 2015)</ref>. <ref type=""bibr"" target=""#b26"">Tang et",0
"elings, <ref type=""bibr"" target=""#b31"">Zhang et al. (2015)</ref> apply a character-level CNN for text classification and achieve competitive results. <ref type=""bibr"" target=""#b22"">Socher et al. (2013)</ref>   explore the structure of a sentence and use a treestructured LSTMs for classification. The",0
"d query ""what is the informative word"" over the words like that used in memory networks <ref type=""bibr"" target=""#b23"">(Sukhbaatar et al., 2015;</ref><ref type=""bibr"" target=""#b10"">Kumar et al., 2015)</ref>. The word context vector u w is randomly initialized and jointly learned during the training =""bibr"" target=""#b27"">(Vinyals et al., 2014)</ref>, natural language question answering <ref type=""bibr"" target=""#b23"">(Sukhbaatar et al., 2015;</ref><ref type=""bibr"" target=""#b10"">Kumar et al., 2015;</ref><ref type=""bibr"" target=""#b4"">Hermann et al., 2015)</ref>, and image question answering <ref t",0
"text classification have been quite effective <ref type=""bibr"" target=""#b8"">(Kim, 2014;</ref><ref type=""bibr"" target=""#b31"">Zhang et al., 2015;</ref><ref type=""bibr"" target=""#b7"">Johnson and Zhang, 2014;</ref><ref type=""bibr"" target=""#b26"">Tang et al., 2015)</ref>, in this paper we test the hypothe a direct application of CNNs, as used in computer vision <ref type=""bibr"" target=""#b12"">(LeCun et al., 1998)</ref>, albeit with NLP interpretations. <ref type=""bibr"" target=""#b7"">Johnson and Zhang (2014)</ref> explores the case of directly using a high-dimensional one hot vector as input. They find",0
"onal word vectors using word2vec and the average word embeddings of each document are used. SSWE uses sentiment specific word embeddings according to <ref type=""bibr"" target=""#b25"">(Tang et al., 2014)</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.2.3"">Neural Network methods</h",0
"WE. In detail, Unigrams and Bigrams uses bag-of-unigrams and bagof-bigrams as features respectively.</p><p>Text Features are constructed according to <ref type=""bibr"" target=""#b9"">(Kiritchenko et al., 2014)</ref>, including word and character n-grams, sentiment lexicon features etc. AverageSG constr",0
"lations between candidate instances and domain human knowledge about the extraction. For a general introduction of first-order logic, please refer to <ref type=""bibr"" target=""#b11"">[12]</ref>.</p><p>Complete consistency describes the fact that the values of two latent variables y i and y j should be",1
"in the two stages are around 90.0%, respectively. For example, F1 score is reported as 92% for the task of homepage finding conducted by Tang et al. <ref type=""bibr"" target=""#b8"">[9]</ref>, and when 87% for extracting profile attributes from the homepage <ref type=""bibr"" target=""#b4"">[5]</ref>. Fro m the pages. In both steps, the state-of-the-art performances achieved by traditional methods are around 90% <ref type=""bibr"" target=""#b4"">[5]</ref>, <ref type=""bibr"" target=""#b8"">[9]</ref>. However, the overall accuracy by combining the two steps inevitably drops to 80% due to error propagation bet eriment Setup</head><p>Dataset. To construct a ground-truth dataset for quantitative evaluation, we randomly choose 2,000 researchers from AMiner.org <ref type=""bibr"" target=""#b8"">[9]</ref>. Specifically, for extracting Email of each researcher, we search the Web using search engine by querying the",0
"attributes, we can combine the defined factor functions and define the following log-likelihood objective function by following the Markov assumption <ref type=""bibr"" target=""#b12"">[13]</ref>:</p><formula xml:id=""formula_7"">log P (Y |X, θ) = yi∈Y k α k φ k (y i , x i ) + ei∼ej m β m ψ m (y i , y j )",0
"and user profile mining, has long been viewed as an important and challenging problem in Web mining and natural language processing. Related studies <ref type=""bibr"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b2"">[3]</ref>, <ref type=""bibr"" target=""#b",0
"rget=""#b5"">[6]</ref>. <ref type=""bibr"">Merler et al.</ref> propose a method to extract user attributes from the pictures posted in social media feeds <ref type=""bibr"" target=""#b17"">[18]</ref>, especially gender information. <ref type=""bibr"" target=""#b18"">[19]</ref> and inferred user's profile by ana",0
"tional convolutional neural networks have a major limitation that they just handle spatial information. For example, in the EmotiW 2014 winner's work <ref type=""bibr"" target=""#b3"">[4]</ref>, all video frames are extracted from videos and regarded as the static images for further process. The aggrega nd C3D network 1 .</p><p>A particular type of recurrent neural networks, the Long Short-Term Memory (LSTM) recurrent neural network is widely adopted <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b7"">8]</ref>. LSTM has memory ability and suits ]</ref>, and so on.</p><p>Unlike C3D networks, a few works are given for video-based emotion recognition using CNN or RNN structures in recent papers <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b18"">19]</ref>. Such deep networks reach top co",1
"focus on facial graph analysis <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b22"">23]</ref> or designing specific CNN-RNN networks <ref type=""bibr"" target=""#b4"">[5]</ref>. Such classifier that takes video sequences as input is becoming more and more important for video event detec ype of recurrent neural networks, the Long Short-Term Memory (LSTM) recurrent neural network is widely adopted <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b7"">8]</ref>. LSTM has memory ability and suits for processing sequences with contexts tworks, a few works are given for video-based emotion recognition using CNN or RNN structures in recent papers <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b18"">19]</ref>. Such deep networks reach top competitive results in the history of Emo "" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b18"">19]</ref>. Such deep networks reach top competitive results in the history of EmotiW challenges <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b26"">27]</ref>. Therefore we take similar LSTM models in our work while applying C3D ybrid CNN-RNN and C3D Networks</head><p>Previous work shows that either CNN-RNN or C3D model alone can achieve good performance in action recognition <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b7"">8]</ref>. And we found that the CNN-RNN and",1
"d motion information simultaneously and the C3D features with a linear classifier can achieve good performance on different video analysis benchmarks <ref type=""bibr"" target=""#b5"">[6]</ref>.</p><p>As for task of video-based emotion recognition, few work based on C3D structures is given in existing p patially applied to 2D static images. While in our 3D ConvNets, the operations are performed spatio-temporally by adding an additional time dimension <ref type=""bibr"" target=""#b5"">[6]</ref>. Hence such C3D networks preserve the temporal information of the input signals, resulting in a more distincti ts. The C3D net has 8 convolutions, 5 max-poolings, and 2 fully connected layers, followed by a softmax output layer. Other parameters are similar to <ref type=""bibr"" target=""#b5"">[6]</ref>. The specific C3D structure used in our implementation is shown in Figure <ref type=""figure"" target=""#fig_5"">4 >Previous work shows that either CNN-RNN or C3D model alone can achieve good performance in action recognition <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b7"">8]</ref>. And we found that the CNN-RNN and C3D hybrid network can further improve",1
"o-based emotion recognition using CNN or RNN structures in recent papers <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b18"">19]</ref>. Such deep networks reach top competitive results in the history of EmotiW challenges <ref type=""bibr"" target E model is trained on face images only, which is originally used for a face verification task. We also evaluate DCN network architecture described in <ref type=""bibr"" target=""#b18"">[19]</ref> without any pre-training. The faces are resized to a fixed size of 256*256, which is the same as the size of",1
"d and shown to be efficient to deal with various sequences versus sequences problems, such as audio analysis <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b28"">29]</ref>, video captioning <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b24"">25]</ref>, video ac",0
"br"" target=""#b9"">[10]</ref>, GoogLeNet <ref type=""bibr"" target=""#b10"">[11]</ref>, VGG <ref type=""bibr"" target=""#b12"">[13]</ref>, and Residual Network <ref type=""bibr"" target=""#b11"">[12]</ref>, which perform well for general objects recognition. Deep-face VGG model (VGG-FACE) <ref type=""bibr"" target=",0
"s/1.0""><head n=""3."">EXPERIMENTS</head><p>We participate in the EmotiW 2016 challenge and mainly evaluate the proposed method on the AFEW 6.0 database <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b26"">27]</ref>.</p><p>In this section, the EmotiW 2016 challenge is briefly introduce EmotiW) 2016 challenge <ref type=""bibr"" target=""#b26"">[27]</ref> has two sub-challenges: (1) video based emotion recognition on the AFEW 6.0 database <ref type=""bibr"" target=""#b0"">[1]</ref>; (2) group level emotion recognition on the HAPPEI database <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""",0
"a sigmoid or hyperbolic tangent, and t h is the hidden state.</p><p>Although RNNs have been widely used in many tasks such as handwriting recognition <ref type=""bibr"" target=""#b16"">[17]</ref> or speech recognition <ref type=""bibr"" target=""#b17"">[18]</ref>, they have difficulties in learning long-ter",0
"ures of the faces are taken from fc6 layer of VGG16-Face model fine-tuned with FER2013 face emotion database <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b15"">16]</ref>. For each iteration of RNN training, 16 face features are randomly selected as inputs. One layer LSTM network",0
"or a special To select a basis for the fine-tuned models, several mainstream CNN architectures trained with ImageNet are examined, including CaffeNet <ref type=""bibr"" target=""#b9"">[10]</ref>, GoogLeNet <ref type=""bibr"" target=""#b10"">[11]</ref>, VGG <ref type=""bibr"" target=""#b12"">[13]</ref>, and Resi",0
"N network used in the paper, the CNN features of the faces are taken from fc6 layer of VGG16-Face model fine-tuned with FER2013 face emotion database <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b15"">16]</ref>. For each iteration of RNN training, 16 face features are randomly s ned with ImageNet are examined, including CaffeNet <ref type=""bibr"" target=""#b9"">[10]</ref>, GoogLeNet <ref type=""bibr"" target=""#b10"">[11]</ref>, VGG <ref type=""bibr"" target=""#b12"">[13]</ref>, and Residual Network <ref type=""bibr"" target=""#b11"">[12]</ref>, which perform well for general objects reco",0
"dels, several mainstream CNN architectures trained with ImageNet are examined, including CaffeNet <ref type=""bibr"" target=""#b9"">[10]</ref>, GoogLeNet <ref type=""bibr"" target=""#b10"">[11]</ref>, VGG <ref type=""bibr"" target=""#b12"">[13]</ref>, and Residual Network <ref type=""bibr"" target=""#b11"">[12]</re",0
"target=""#b28"">29]</ref>, video captioning <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b24"">25]</ref>, video action recognition <ref type=""bibr"" target=""#b21"">[22]</ref>, and so on.</p><p>Unlike C3D networks, a few works are given for video-based emotion recognition using CNN o r temporal models on optical flow images and perform late fusion akin to the two-stream hypothesis following <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b21"">22]</ref>. The performance of the system is around 25% for emotion classification, better than only using frames as inp",0
"Long Short-Term Memory (LSTM) recurrent neural network is widely adopted <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b7"">8]</ref>. LSTM has memory ability and suits for processing sequences with contexts well. An encoder LSTM can be used to NN or C3D model alone can achieve good performance in action recognition <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b7"">8]</ref>. And we found that the CNN-RNN and C3D hybrid network can further improve the performance.</p><p>Considering th to remember, when the unit should continue to remember or forget the value, and when the unit should output the value. We use LSTM unit described in <ref type=""bibr"" target=""#b7"">[8]</ref> in our method (Figure <ref type=""figure"" target=""#fig_3"">2</ref>), which iterates between the following operat  ,</formula><p>where  is the sigmoid function, and  is the hyperbolic tangent function.</p><p>We use a similar framework with the one described in <ref type=""bibr"" target=""#b7"">[8]</ref>, which combines LSTMs with deep convolutional networks to train a model spatially and temporally for video seq",0
"made great influences in the emotion recognition area. Previous winners usually focus on facial graph analysis <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b22"">23]</ref> or designing specific CNN-RNN networks <ref type=""bibr"" target=""#b4"">[5]</ref>. Such classifier that takes vi",0
"ased emotion recognition on the AFEW 6.0 database <ref type=""bibr"" target=""#b0"">[1]</ref>; (2) group level emotion recognition on the HAPPEI database <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b26"">27]</ref>.</p><p>We only participate in the video-based emotion recognition sub-",0
"n. Recently, LSTMs are also well developed and shown to be efficient to deal with various sequences versus sequences problems, such as audio analysis <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b28"">29]</ref>, video captioning <ref type=""bibr"" target=""#b23"">[24,</ref><ref type",0
"held for four years since 2013 and has made great influences in the emotion recognition area. Previous winners usually focus on facial graph analysis <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b22"">23]</ref> or designing specific CNN-RNN networks <ref type=""bibr"" target=""#b4"">[ CNN-RNN and C3D hybrid network can further improve the performance.</p><p>Considering that audio modality can bring ~3% gain in recognition accuracy <ref type=""bibr"" target=""#b2"">[3]</ref>, we also trained a SVM with the linear kernel using audio features extracted with the OpenSmile toolkit <ref t y be that the training data is too limited.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.5"">C3D Network</head><p>Considering that in <ref type=""bibr"" target=""#b2"">[3]</ref>, the images with two face scales are used to further improve the performance, we adopt two kinds of face image",0
"uracy <ref type=""bibr"" target=""#b2"">[3]</ref>, we also trained a SVM with the linear kernel using audio features extracted with the OpenSmile toolkit <ref type=""bibr"" target=""#b6"">[7]</ref>.</p><p>The CNN-RNN, C3D and audio SVM model were trained separately and their predication scores were combined",0
"hlets on relatively small biological networks (i.e., few hundreds/thousands of nodes/edges) and uses such counts as features for graph classification <ref type=""bibr"" target=""#b37"">(Vishwanathan, Schraudolph, Kondor and Borgwardt, 2010)</ref>. Previous work showed that graphlet counting is computati type=""bibr"" target=""#b31"">(Schaeffer, 2007)</ref>, role discovery <ref type=""bibr"" target=""#b30"">(Rossi and Ahmed, 2015b)</ref>, graph classification <ref type=""bibr"" target=""#b37"">(Vishwanathan et al., 2010)</ref>, and relational learning <ref type=""bibr"" target=""#b7"">(Getoor and Taskar, 2007)</ref tation of most graph kernels (including the graphlet kernel) is that they scale poorly to large graphs with more than few hundreds/thousands of nodes <ref type=""bibr"" target=""#b37"">(Vishwanathan et al., 2010)</ref>. Thus, our fast algorithms would speedup the computations of these methods and their proach on protein graphs (D&amp; D collection of 1178 protein graphs) and chemical compound graphs (MUTAG collection of 188 chemical compound graphs) <ref type=""bibr"" target=""#b37"">(Vishwanathan et al., 2010)</ref>. We extract the graphlet features using Algorithm 2. Then, we learn a model using SVM leinberg, 2013)</ref>. Moreover, a variety of graph kernels have been proposed in machine learning (e.g., graphlet, subtree, and random walk kernels) <ref type=""bibr"" target=""#b37"">(Vishwanathan et al., 2010;</ref><ref type=""bibr"" target=""#b3"">Costa and De Grave, 2010;</ref><ref type=""bibr"" target=""",1
"=""#fig_21"">7</ref>. Visualization of the human diseasome network: A network of disorders and disease genes linked by known disorder-gene associations <ref type=""bibr"" target=""#b8"">(Goh et al., 2007)</ref>. Edges are weighted/colored by their number of incident star graphlets of size 4 nodes, nodes a cancer, leukemia, and deafness) correspond to hubs (large stars) that are connected to a large number of distinct disorders, which is consistent with <ref type=""bibr"" target=""#b8"">(Goh et al., 2007)</ref>.</p><p>and other related queries, we leverage the proposed parallel graphlet counting method in ef type=""figure"" target=""#fig_21"">7</ref>. Figure <ref type=""figure"" target=""#fig_21"">7</ref> provides a visualization of the human diseasome network <ref type=""bibr"" target=""#b8"">(Goh et al., 2007)</ref>, where we used Alg. 2 to rank (weight) all the edges in the network by the number of star patte t counting method can help to quickly highlight such large stars by using the counts (of stars of size 4 nodes) as edge weights or colors. Notably,   <ref type=""bibr"" target=""#b8"">(Goh et al., 2007)</ref>.</p><p>Note that the same approach is also applicable for finding cliques and other interesting a, and deafness) correspond to hubs (large stars) that are connected to a large number of distinct disorders, which is consistent with the findings in<ref type=""bibr"" target=""#b8"">(Goh et al., 2007)</ref>.Note that the same approach is also applicable for finding cliques and other interesting patter",0
"structure, it has been shown that graphlet frequencies often carry significant information about the local network structure in a variety of domains <ref type=""bibr"" target=""#b15"">(Holland and Leinhardt, 1976;</ref><ref type=""bibr"" target=""#b4"">Faust, 2010;</ref><ref type=""bibr"" target=""#b6"">Frank, #b25"">(Milo et al., 2002)</ref>. In social science, graphlet analysis (typically known as k-subgraph census) is widely adopted in sociometric studies <ref type=""bibr"" target=""#b15"">(Holland and Leinhardt, 1976;</ref><ref type=""bibr"" target=""#b6"">Frank, 1988)</ref>. Much of the work in this vein focu",0
"s in computer networking <ref type=""bibr"" target=""#b5"">(Feldman and Shavitt, 2008;</ref><ref type=""bibr"" target=""#b12"">Hales and Arteconi, 2008;</ref><ref type=""bibr"" target=""#b1"">Becchetti, Boldi, Castillo and Gionis, 2008</ref>) (e.g., for web spam detection, analysis of peer-to-peer protocols and",0
"/ref>, graphlets were widely used for protein function prediction <ref type=""bibr"" target=""#b32"">(Shervashidze et al., 2009)</ref>, network alignment <ref type=""bibr"" target=""#b24"">(Milenkovi?, Ng, Hayes and Pr?ulj, 2010)</ref>, and phylogeny <ref type=""bibr"" target=""#b19"">(Kuchaiev, Milenkovi?, Mem",0
"e high-resolution maps have low-level features that harm their representational capacity for object recognition.</p><p>The Single Shot Detector (SSD) <ref type=""bibr"" target=""#b21"">[22]</ref> is one of the first attempts at using a ConvNet's pyramidal feature hierarchy as if it were a featurized ima arget=""#b1"">[2]</ref>) concatenate features of multiple layers before computing predictions, which is equivalent to summing transformed features. SSD <ref type=""bibr"" target=""#b21"">[22]</ref> and MS-CNN <ref type=""bibr"" target=""#b2"">[3]</ref> predict objects at multiple layers of the feature hierarc ard negative mining <ref type=""bibr"" target=""#b34"">[35]</ref>, context modeling <ref type=""bibr"" target=""#b15"">[16]</ref>, stronger data augmentation <ref type=""bibr"" target=""#b21"">[22]</ref>, etc. These improvements are complementary to FPNs and should boost accuracy further.</p><p>Recently, FPN ha",1
"""#b32"">[33]</ref> and COCO <ref type=""bibr"" target=""#b20"">[21]</ref> detection challenges use multi-scale testing on featurized image pyramids (e.g., <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b34"">35]</ref>). The principle advantage of featurizing each level of an image pyra ory, and so, if exploited, image pyramids are used only at test time <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b34"">35]</ref>, which creates an inconsistency between train/test-time inference. Fo the COCO-style Average Precision (AP) by 2.3 points and PASCAL-style AP by 3.8 points, over a strong single-scale baseline of Faster R-CNN on ResNets <ref type=""bibr"" target=""#b15"">[16]</ref>. Our method is also easily extended to mask proposals and improves both instance segmentation AR and speed o #b18"">[19,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" target=""#b15"">16]</ref>), and in this paper we present results using ResNets <ref type=""bibr"" target=""#b15"">[16]</ref>. The construction of our pyramid involves a bottom-up pathway, a top-down pathway, and lateral connections, create our pyramid. This choice is natural since the deepest layer of each stage should have the strongest features.</p><p>Specifically, for ResNets <ref type=""bibr"" target=""#b15"">[16]</ref> we use the feature activations output by each stage's last residual block. We denote the output of these las ve found that our model is robust to many design choices. We have experimented with more sophisticated blocks (e.g., using multilayer residual blocks <ref type=""bibr"" target=""#b15"">[16]</ref> as the connections) and observed marginally better results. Designing better connection modules is not the f raining size, and k 0 is the target level on which an RoI with w × h = 224 2 should be mapped into. Analogous to the ResNet-based Faster R-CNN system <ref type=""bibr"" target=""#b15"">[16]</ref> that uses C 4 as the single-scale feature map, we set k 0 to 4. Intuitively, Eqn. (1) means that if the RoI' ss-specific classifiers and bounding box regressors) to all RoIs of all levels. Again, the heads all share parameters, regardless of their levels. In <ref type=""bibr"" target=""#b15"">[16]</ref>, a ResNet's conv5 layers (a 9-layer deep subnetwork) are adopted as the head on top of the conv4 features, b network) are adopted as the head on top of the conv4 features, but our method has already harnessed conv5 to construct the feature pyramid. So unlike <ref type=""bibr"" target=""#b15"">[16]</ref>, we simply adopt RoI pooling to extract 7×7 features, and attach two hidden 1,024-d fully-connected (fc) lay target=""#b28"">[29]</ref>, we run two baselines (Table <ref type=""table"" target=""#tab_0"">1</ref>(a, b)) using the single-scale map of C 4 (the same as <ref type=""bibr"" target=""#b15"">[16]</ref>) or C 5 , both using the same hyper-parameters as ours, including using 5 scale anchors of {32 2 , 64 2 , 12 or. For simplicity we do not share features between Fast R-CNN and RPN, except when specified.</p><p>As a ResNet-based Fast R-CNN baseline, following <ref type=""bibr"" target=""#b15"">[16]</ref>, we adopt RoI pooling with an output size of 14×14 and attach all conv5 layers as the hidden layers of the h for RPN and Fast R-CNN. Table <ref type=""table"" target=""#tab_2"">3</ref>(a) shows our reproduction of the baseline Faster R-CNN system as described in <ref type=""bibr"" target=""#b15"">[16]</ref>. Under controlled settings, our FPN (Table <ref type=""table"" target=""#tab_2"">3</ref>(c)) is better than this ><p>Note that Table <ref type=""table"" target=""#tab_2"">3</ref>(a) and (b) are baselines that are much stronger than the baseline provided by He et al. <ref type=""bibr"" target=""#b15"">[16]</ref> in Table 3(*). We find the following implementations contribute to the gap: (i) We use an image scale of 800 ontrast to 64 RoIs in <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b15"">16]</ref>; (iii) We use 5 scale anchors instead of 4 in <ref type=""bibr"" target=""#b15"">[16]</ref> (adding 32 2 ); (iv) At test time we use 1000 proposals per image instead of 300 in <ref type=""bibr"" target= anchors instead of 4 in <ref type=""bibr"" target=""#b15"">[16]</ref> (adding 32 2 ); (iv) At test time we use 1000 proposals per image instead of 300 in <ref type=""bibr"" target=""#b15"">[16]</ref>. So comparing with He et al.'s ResNet-50 Faster R-CNN baseline in Table <ref type=""table"" target=""#tab_2"">3< uch as iterative regression <ref type=""bibr"" target=""#b8"">[9]</ref>, hard negative mining <ref type=""bibr"" target=""#b34"">[35]</ref>, context modeling <ref type=""bibr"" target=""#b15"">[16]</ref>, stronger data augmentation <ref type=""bibr"" target=""#b21"">[22]</ref>, etc. These improvements are complemen for RPN are consistent with Fast R-CNN. Models are trained on the trainval35k set and use ResNet-50.</figDesc><table /><note>† Provided by authors of<ref type=""bibr"" target=""#b15"">[16]</ref>.</note></figure> <figure xmlns=""http://www.tei-c.org/ns/1.0"" type=""table"" xml:id=""tab_3""><head>Table 2 (</he ss is independent of the backbone convolutional architectures (e.g., <ref type=""bibr"" target=""#b18"">[19,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" target=""#b15"">16]</ref>), and in this paper we present results using ResNets <ref type=""bibr"" target=""#b15"">[16]</ref>. The construct e following implementations contribute to the gap: (i) We use an image scale of 800 pixels instead of 600 in <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b15"">16]</ref>; (ii) We train with 512 RoIs per image which accelerate convergence, in contrast to 64 RoIs in <ref type=""bib 15"">16]</ref>; (ii) We train with 512 RoIs per image which accelerate convergence, in contrast to 64 RoIs in <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b15"">16]</ref>; (iii) We use 5 scale anchors instead of 4 in <ref type=""bibr"" target=""#b15"">[16]</ref> (adding 32 2 ); (iv)",0
"gmentation. Several other approaches (HyperNet <ref type=""bibr"" target=""#b17"">[18]</ref>, ParseNet <ref type=""bibr"" target=""#b22"">[23]</ref>, and ION <ref type=""bibr"" target=""#b1"">[2]</ref>) concatenate features of multiple layers before computing predictions, which is equivalent to summing transfor detection dataset <ref type=""bibr"" target=""#b20"">[21]</ref>. We train using the union of 80k train images and a 35k subset of val images (trainval35k <ref type=""bibr"" target=""#b1"">[2]</ref>), and report ablations on a 5k subset of val images (minival). We also report final results on the standard te",0
"are popular in recent research <ref type=""bibr"" target=""#b27"">[28,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b25"">26]</ref>. Their goals are to produce a single high-level feature map of a fine resolution on which the predictions are #b27"">[28]</ref> for segmentation, Recombinator networks <ref type=""bibr"" target=""#b16"">[17]</ref> for face detection, and Stacked Hourglass networks <ref type=""bibr"" target=""#b25"">[26]</ref> for keypoint estimation. Ghiasi et al. <ref type=""bibr"" target=""#b7"">[8]</ref> present a Laplacian pyramid p",0
"ts.</p><p>Methods using multiple layers. A number of recent approaches improve detection and segmentation by using different layers in a ConvNet. FCN <ref type=""bibr"" target=""#b23"">[24]</ref> sums partial scores for each category over multiple scales to compute semantic segmentations. Hypercolumns <",0
"to temporal networks. In static networks, network motifs or graphlets are defined as small induced subgraphs occurring in a bigger network structure <ref type=""bibr"" target=""#b4"">[4,</ref><ref type=""bibr"" target=""#b19"">19,</ref><ref type=""bibr"" target=""#b29"">29]</ref>. We extend static mo-tifs to t /ref>. Furthermore, motifs are critical for understanding the higher-order organizational patterns in networks <ref type=""bibr"" target=""#b3"">[3,</ref><ref type=""bibr"" target=""#b4"">4]</ref>. On the algorithmic side, a large amount of research has been devoted simply to counting triangles in undirecte",1
"curring within ? time units that correspond to instances of M .</p><p>The first step can use known algorithms for enumerating motifs in static graphs <ref type=""bibr"" target=""#b27"">[27]</ref>, and the second step is a simple matter of fetching the appropriate temporal edges. To perform the third ste",0
"h 6, 2016. BITCOIN. Bitcoin is a decentralized digital currency and payment system. This dataset consists of all payments made up to October 19, 2014 <ref type=""bibr"" target=""#b11"">[11]</ref>. Nodes in the network correspond to Bitcoin addresses, and an individual may have several addresses. An edge",0
"ref><ref type=""bibr"" target=""#b29"">29]</ref>. Furthermore, motifs are critical for understanding the higher-order organizational patterns in networks <ref type=""bibr"" target=""#b3"">[3,</ref><ref type=""bibr"" target=""#b4"">4]</ref>. On the algorithmic side, a large amount of research has been devoted si",0
"owledge network built from MEDLINE and other NCBI data sources. We use natural language processing methods, such as Latent Dirichlet Allocation (LDA) <ref type=""bibr"" target=""#b5"">[8]</ref> and topical phrase mining <ref type=""bibr"" target=""#b12"">[16]</ref>, along with other data mining techniques t nigrams based on their co-occurrence rate as well as their topical similarity using a process they call Phrase LDA.</p><p>Latent Dirichlet Allocation <ref type=""bibr"" target=""#b5"">[8]</ref> is the most common topic modeling process and PLDA+ is a scalable implementation of this algorithm <ref type=""",1
"r knowledge, MOLIERE is the rst hypothesis generation system to utilize the entire MEDLINE data set. By using state-of-the-art tools, such as ToPMine <ref type=""bibr"" target=""#b12"">[16]</ref> and FastText <ref type=""bibr"" target=""#b6"">[9]</ref>, we are able to nd novel hypotheses without restricting use natural language processing methods, such as Latent Dirichlet Allocation (LDA) <ref type=""bibr"" target=""#b5"">[8]</ref> and topical phrase mining <ref type=""bibr"" target=""#b12"">[16]</ref>, along with other data mining techniques to conceptually link together abstracts and biomedical objects (suc n this corpus. en we use ToPMine to identify multiword phrases from that corpus such as ""asthma a ack,"" allowing us to treat phrases as single tokens <ref type=""bibr"" target=""#b12"">[16]</ref>. Next, we send the corpus through FastText, the most recent word2vec implementation, which maps each unique eighbors in the A layer. In order to do this, we turned to the UMLS SPECIALIST NLP toolset <ref type=""bibr"" target=""#b0"">[1]</ref> as well as ToPMine <ref type=""bibr"" target=""#b12"">[16]</ref> and FastText <ref type=""bibr"" target=""#b6"">[9,</ref><ref type=""bibr"" target=""#b19"">23]</ref>. Our process fo component, we accept this quality vs. time trade o . It is also important to note that we modify the version of ToP-Mine distributed by El-Kishky in <ref type=""bibr"" target=""#b12"">[16]</ref> to allow phrases containing numbers, such as gene names like p53.</p><p>Next, FastText maps each token in ou are helpful when general language may signi cantly over weigh a specialized language. However, phrase mining approaches that recover n-grams, such as <ref type=""bibr"" target=""#b12"">[16]</ref>, produce accurate methods without limiting the dictionary. Incorporating the Semantic Layer S: In section 2.",1
"n nd clouds of documents around these pathways which contain knowledge representative of the path as a whole. PLDA+, a scalable implementation of LDA <ref type=""bibr"" target=""#b24"">[28]</ref>, allows us to quickly nd topic models in these clouds. Unlike similar systems, we do not restrict PLDA+ to a ains many documents which, due to our network construction process, all share common topics. We perform topic modeling on these documents using PLDA+ <ref type=""bibr"" target=""#b24"">[28]</ref>. e result is a set of plain text topics which represent di erent concepts which likely connect the two queri [8]</ref> is the most common topic modeling process and PLDA+ is a scalable implementation of this algorithm <ref type=""bibr"" target=""#b16"">[20,</ref><ref type=""bibr"" target=""#b24"">28]</ref>. Developed by Zhiyuan Liu et al., PLDA+ quickly identi es groups of words and phrases which all relate to a s",1
"l data in both their work on the disease network <ref type=""bibr"" target=""#b15"">[19]</ref> as well as their more generalized symptoms-disease network <ref type=""bibr"" target=""#b47"">[51]</ref>. In the former <ref type=""bibr"" target=""#b15"">[19]</ref>, the authors construct a bipartite network of disea themselves. ey use the Diseasome to create two projected networks, the human disease network (HDN), and the Disease Gene Network (DGN). In the la er <ref type=""bibr"" target=""#b47"">[51]</ref>, they construct a more generalized human symptoms disease network (HSDN) by using both UMLS keywords and bib",0
"long rarely occurring words.</p><p>ToPMine, a project from El-Kishky et al., is focused on discovering multi-word phrases from a large corpus of text <ref type=""bibr"" target=""#b16"">[20]</ref>. is project intelligently groups unigrams together to create n-gram phrases for later use in text mining alg Allocation <ref type=""bibr"" target=""#b5"">[8]</ref> is the most common topic modeling process and PLDA+ is a scalable implementation of this algorithm <ref type=""bibr"" target=""#b16"">[20,</ref><ref type=""bibr"" target=""#b24"">28]</ref>. Developed by Zhiyuan Liu et al., PLDA+ quickly identi es groups of",0
"nitially discovered. e prediction of new processes is especially important for repurposing existing drugs (or drug target genes) to a new application <ref type=""bibr"" target=""#b1"">[4,</ref><ref type=""bibr"" target=""#b2"">5,</ref><ref type=""bibr"" target=""#b29"">33]</ref>. As an example, the drugs develo",0
"16]</ref>. Next, we send the corpus through FastText, the most recent word2vec implementation, which maps each unique token in the corpus to a vector <ref type=""bibr"" target=""#b26"">[30]</ref>. We can then t a centroid to each publication and use the Fast Library for Approximate Nearest Neighbors (FL imensionality of our vector space d to 500. is is consistent with published examples of similar size, for example the Google news corpus processed in <ref type=""bibr"" target=""#b26"">[30]</ref>. Lastly, we increase the word neighborhood and number of possible sub-words from ve to eight in order to inc ext is the most recent implementation of word2vec from Milkolov et al. <ref type=""bibr"" target=""#b6"">[9,</ref><ref type=""bibr"" target=""#b19"">23,</ref><ref type=""bibr"" target=""#b26"">30,</ref><ref type=""bibr"" target=""#b27"">31]</ref>. Word2vec is a method which utilizes the skip-gram model to identify low support words, we accept that any n-gram seen by FastText is important. Following examples presented in <ref type=""bibr"" target=""#b19"">[23,</ref><ref type=""bibr"" target=""#b26"">30,</ref><ref type=""bibr"" target=""#b27"">31]</ref> and others, we set the dimensionality of our vector space d to 500. i",0
"Connect, an online system that allows researchers to query for concepts intersecting two keywords, is a notable contribution to hypothesis generation <ref type=""bibr"" target=""#b23"">[27]</ref>. is system, proposed by Liu et al., is similar to both our system and Arrowsmith <ref type=""bibr"" target=""#b",0
"on and large scale suite of historical hypotheses. We are currently evaluating whether a ground-truth network, like the drug-side-e ect network SIDER <ref type=""bibr"" target=""#b22"">[26]</ref>, can be a good source of such hypotheses. For example, if we identify a set of recently added connections wi",0
"sing the Dynamic Topic Modeling (DTM) <ref type=""bibr"" target=""#b4"">[7]</ref>. Our preliminary experiments with scalable time-dependent clustered LDA <ref type=""bibr"" target=""#b17"">[21]</ref> that signi cantly accelerates DTM demonstrate a potential to discover dynamic topics in MED-LINE. e dynamic",0
"g. Unlike our system, Liu et al. restrict DiseaseConnect to simply 3 of the 130 semantic types. ey supplement this subset with concepts from the OMIM <ref type=""bibr"" target=""#b15"">[19]</ref> and GWAS <ref type=""bibr"" target=""#b3"">[6]</ref> databases, two genome speci c data sets. Still, their netwo sualization.</p><p>Barabási et al. improve upon the network analytic approach to understand biomedical data in both their work on the disease network <ref type=""bibr"" target=""#b15"">[19]</ref> as well as their more generalized symptoms-disease network <ref type=""bibr"" target=""#b47"">[51]</ref>. In the e=""bibr"" target=""#b15"">[19]</ref> as well as their more generalized symptoms-disease network <ref type=""bibr"" target=""#b47"">[51]</ref>. In the former <ref type=""bibr"" target=""#b15"">[19]</ref>, the authors construct a bipartite network of disease phonemes and genomes to which they refer to as the Dis",0
"n of interest.</p><p>In 1986 Swanson hypothesized that novel discoveries could be found by carefully studying the existing body of scienti c research <ref type=""bibr"" target=""#b41"">[45]</ref>. Since then, many groups have a empted to mine the wealth of public knowledge. E orts such as Swanson's own .org/ns/1.0""><head n=""1.3"">Related Work</head><p>e study and exploration of undiscovered public knowledge began in 1986 with Swanson's landmark paper <ref type=""bibr"" target=""#b41"">[45]</ref>. Swanson hypothesized that fragments of information from the set of public knowledge could be connected in s",0
"factors which altered the steps in a scientist's research pipeline. Several promising observations in this direction have been done by Foster et al. <ref type=""bibr"" target=""#b14"">[18]</ref> who examined this through Bourdieu's eld theory of science by developing a typology of research strategies o",0
"n to hypothesis generation <ref type=""bibr"" target=""#b23"">[27]</ref>. is system, proposed by Liu et al., is similar to both our system and Arrowsmith <ref type=""bibr"" target=""#b35"">[39]</ref> in its focus on UMLS keywords and MEDLINE literature mining. Unlike our system, Liu et al. restrict DiseaseC",0
"/p><p>More recently, DDX3 activity was found to be involved cancer development and progression mainly through regulation of the Wnt signaling pathway <ref type=""bibr"" target=""#b10"">[13,</ref><ref type=""bibr"" target=""#b46"">50]</ref> and associated regulation of Cellcell and Cell-matrix adhesion, tumo",0
"(SCGs) provide a formalism for specifying input-output mappings, potentially stochastic, with learnable parameters using directed acyclic graphs (see <ref type=""bibr"" target=""#b38"">Schulman et al. (2015)</ref> for a review). The state of each non-input node in such a graph is obtained from the state",1
"target=""#b33"">Ranganath et al., 2014;</ref><ref type=""bibr"">Mnih &amp; Gregor, 2014;</ref><ref type=""bibr"">Titsias &amp; Lázaro-Gredilla, 2015;</ref><ref type=""bibr"" target=""#b15"">Gu et al., 2016)</ref>, which differ primarily in the variance reduction techniques used.</p></div> <div xmlns=""http://",0
"ong independence properties, and a great deal of work has been done to generalize it <ref type=""bibr"" target=""#b5"">(Connor &amp; Mosimann, 1969;</ref><ref type=""bibr"" target=""#b1"">Aitchison, 1985;</ref><ref type=""bibr"" target=""#b34"">Rayens &amp; Srinivasan, 1994;</ref><ref type=""bibr"" target=""#b6"">F",0
"<p>To motivate the construction of Concrete random variables, we review a method for sampling from discrete distributions called the Gumbel-Max trick <ref type=""bibr"" target=""#b24"">(Luce, 1959;</ref><ref type=""bibr"">Yellott, 1977;</ref><ref type=""bibr"" target=""#b31"">Papandreou &amp; Yuille, 2011;</r",0
"sc> 		<profileDesc> 			<abstract> <div xmlns=""http://www.tei-c.org/ns/1.0""><p>We present a variational approximation to the information bottleneck of <ref type=""bibr"" target=""#b33"">Tishby et al. (1999)</ref>. This variational approach allows us to parameterize the information bottleneck model using controls the tradeoff. <ref type=""foot"" target=""#foot_2"">3</ref> This approach is known as the information bottleneck (IB), and was first proposed in <ref type=""bibr"" target=""#b33"">Tishby et al. (1999)</ref>. Intuitively, the first term in R IB encourages Z to be predictive of Y ; the second term en utual information is, in general, computationally challenging. There are two notable exceptions: the first is when X, Y and Z are all discrete, as in <ref type=""bibr"" target=""#b33"">Tishby et al. (1999)</ref>; this can be used to cluster discrete data, such as words. The second case is when X, Y and",1
"ication; single-step gradient-based attacks <ref type=""bibr"" target=""#b31"">(Goodfellow et al., 2014;</ref><ref type=""bibr"">Kurakin et al., 2016;</ref><ref type=""bibr"" target=""#b15"">Huang et al., 2015)</ref>, which choose a gradient direction of the image pixels at some loss and then take a single st",0
"s.</p><p>Variational inference is a natural way to approximate the problem. Variational bounds on mutual information have previously been explored in <ref type=""bibr"" target=""#b1"">Agakov (2004)</ref>, though not in conjunction with the information bottleneck objective. <ref type=""bibr"" target=""#b20""",0
"on of the network. This form of the information bottleneck is known as predictive information<ref type=""bibr"" target=""#b4"">(Bialek et al., 2001;</ref><ref type=""bibr"" target=""#b24"">Palmer et al., 2015)</ref>.</note> 			<note xmlns=""http://www.tei-c.org/ns/1.0"" place=""foot"" n=""11"" xml:id=""foot_8"">The",0
"a learned mean and covariance. But the idea remained somewhat implicit until residual networks <ref type=""bibr"" target=""#b5"">(He et al. (2015)</ref>; <ref type=""bibr"" target=""#b6"">He et al. (2016)</ref>) explicitly introduced a reparameterization of the convolutional layers such that when all traina ww.tei-c.org/ns/1.0""><head n=""1.2"">RELATED WORK</head><p>Since the advent of residual networks <ref type=""bibr"" target=""#b5"">(He et al. (2015)</ref>; <ref type=""bibr"" target=""#b6"">He et al. (2016)</ref>), most state-of-the-art networks for image classification have adopted a residual parameterizatio . (3.1)</formula><p>A residual network is composed of a sequence of such residual blocks. In comparison with the full pre-activation architecture in <ref type=""bibr"" target=""#b6"">He et al. (2016)</ref>, we remove two batch normalization layers and one ReLU layer in each building block.</p><p>We ass",1
"ar residual networks have no critical points other than the global optimum. In contrast, for standard linear neural networks we only know, by work of <ref type=""bibr"" target=""#b11"">Kawaguchi (2016)</ref> that these networks don't have local optima except the global optimum, but it doesn't rule out o minima, but they didn't prove that a good differentiable local minimum does exist. <ref type=""bibr"" target=""#b1"">Baldi &amp; Hornik (1989)</ref> and <ref type=""bibr"" target=""#b11"">Kawaguchi (2016)</ref> show that linear neural networks have no bad local minima.</p><p>In contrast, we show that the o",0
"=""#b2"">Choromanska et al. (2015)</ref> draws an analogy between the optimization landscape of neural nets and that of the spin glass model in physics <ref type=""bibr"" target=""#b0"">(Auffinger et al. (2013)</ref>). <ref type=""bibr"" target=""#b13"">Soudry &amp; Carmon (2016)</ref> showed that 2-layer neu",0
"as been recent progress on understanding the optimization landscape of neural networks, though a comprehensive answer remains elusive. Experiments in <ref type=""bibr"" target=""#b4"">Goodfellow et al. (2014)</ref> and <ref type=""bibr"" target=""#b3"">Dauphin et al. (2014)</ref> suggest that the training o",0
"s=""http://www.tei-c.org/ns/1.0""><head n=""1"">INTRODUCTION</head><p>Traditional convolutional neural networks for image classification, such as AlexNet <ref type=""bibr"" target=""#b12"">(Krizhevsky et al. (2012)</ref>), are parameterized in such a way that when all trainable weights are 0, a convolutiona er 700, 000 steps (about one week of training). While no longer state-of-the-art, this performance is significantly better than the 40.7% reported by <ref type=""bibr"" target=""#b12"">Krizhevsky et al. (2012)</ref>, as well as the best all-convolutional architecture by <ref type=""bibr"" target=""#b14"">Sp",0
"bibr"" target=""#b18"">[16,</ref><ref type=""bibr"" target=""#b17"">15,</ref><ref type=""bibr"" target=""#b19"">17,</ref><ref type=""bibr"" target=""#b28"">26,</ref><ref type=""bibr"" target=""#b9"">7]</ref>. Unlike traditional supervised learning, dense vectorized representations <ref type=""bibr"" target=""#b18"">[16,</ =""bibr"" target=""#b19"">[17,</ref><ref type=""bibr"" target=""#b28"">26,</ref><ref type=""bibr"" target=""#b27"">25,</ref><ref type=""bibr"" target=""#b8"">6,</ref><ref type=""bibr"" target=""#b9"">7]</ref>. Many of these methods are technically inspired by word embedding <ref type=""bibr"" target=""#b18"">[16,</ref><ref",1
"Hence, many traditional methods under network settings heavily rely on problem specific feature engineering <ref type=""bibr"" target=""#b14"">[12,</ref><ref type=""bibr"" target=""#b15"">13,</ref><ref type=""bibr"" target=""#b36"">34,</ref><ref type=""bibr"" target=""#b11"">9,</ref><ref type=""bibr"" target=""#b35""> Supervised feature-based baselines. As widely used in similar author identification/disambiguation problems <ref type=""bibr"" target=""#b14"">[12,</ref><ref type=""bibr"" target=""#b15"">13,</ref><ref type=""bibr"" target=""#b36"">34,</ref><ref type=""bibr"" target=""#b11"">9,</ref><ref type=""bibr"" target=""#b35""> =""#b13"">[11]</ref>. And we also notice KDD Cup 2013 has similar author identification/disambiguation problem <ref type=""bibr"" target=""#b14"">[12,</ref><ref type=""bibr"" target=""#b15"">13,</ref><ref type=""bibr"" target=""#b36"">34,</ref><ref type=""bibr"" target=""#b11"">9,</ref><ref type=""bibr"" target=""#b35"">",0
"n=""1."">INTRODUCTION</head><p>Heterogeneous networks are ubiquitous. Examples include bibliographic networks <ref type=""bibr"" target=""#b22"">[20,</ref><ref type=""bibr"" target=""#b24"">22]</ref>, movie recommendation networks <ref type=""bibr"" target=""#b34"">[32]</ref> and many online social networks cont tem, where the model is asked to match queried document with certain target, such as reviewer recommendation <ref type=""bibr"" target=""#b31"">[29,</ref><ref type=""bibr"" target=""#b24"">22]</ref>.</p><p>To tackle the author identification problem, as well as many other network mining problems, good repre br"" target=""#b21"">19]</ref>. Many existing work on mining heterogeneous networks rely on feature engineering <ref type=""bibr"" target=""#b22"">[20,</ref><ref type=""bibr"" target=""#b24"">22]</ref>, while we adopt embedding methods for automatic feature learning.</p><p>Network embedding also attracts lots tion. Similar problems in social and information networks are also studied, such as collaboration prediction <ref type=""bibr"" target=""#b22"">[20,</ref><ref type=""bibr"" target=""#b24"">22]</ref>. The major difference between those work and ours is the methodology, their methods are mostly based on heavy",0
"xmlns=""http://www.tei-c.org/ns/1.0""><head n=""1."">INTRODUCTION</head><p>Heterogeneous networks are ubiquitous. Examples include bibliographic networks <ref type=""bibr"" target=""#b22"">[20,</ref><ref type=""bibr"" target=""#b24"">22]</ref>, movie recommendation networks <ref type=""bibr"" target=""#b34"">[32]</ ibr"" target=""#b26"">24,</ref><ref type=""bibr"" target=""#b21"">19]</ref>. Many existing work on mining heterogeneous networks rely on feature engineering <ref type=""bibr"" target=""#b22"">[20,</ref><ref type=""bibr"" target=""#b24"">22]</ref>, while we adopt embedding methods for automatic feature learning.</p of the most important sources of information. Similar problems in social and information networks are also studied, such as collaboration prediction <ref type=""bibr"" target=""#b22"">[20,</ref><ref type=""bibr"" target=""#b24"">22]</ref>. The major difference between those work and ours is the methodology",0
">1</ref> . This allows us to define quantities such as the element-wise mean E p [z] = [π 1 , ..., π k ] of these vectors.</p><p>The Gumbel-Max trick <ref type=""bibr"" target=""#b8"">(Gumbel, 1954;</ref><ref type=""bibr"" target=""#b12"">Maddison et al., 2014)</ref> provides a simple and efficient way to d",1
"(y 1 , ..., y k ) = Γ(k)τ k−1 k i=1 π i /y τ i −k k i=1 π i /y τ +1 i<label>(3)</label></formula><p>This distribution was independently discovered by <ref type=""bibr"" target=""#b13"">Maddison et al. (2016)</ref>, where it is referred to as the concrete distribution. As the softmax temperature τ approa",1
">The score function estimator (SF, also referred to as REINFORCE <ref type=""bibr"" target=""#b26"">(Williams, 1992)</ref> and likelihood ratio estimator <ref type=""bibr"" target=""#b3"">(Glynn, 1990</ref>)) uses the identity ∇ θ p θ (z) = p θ (z)∇ θ log p θ (z) to derive the following unbiased estimator:<",0
"et=""#b7"">Gu et al., 2016;</ref><ref type=""bibr"" target=""#b6"">Gregor et al., 2013)</ref>, or biased path derivative estimators for Bernoulli variables <ref type=""bibr"" target=""#b0"">(Bengio et al., 2013)</ref>. However, no existing gradient estimator has been formulated specifically for categorical va where m is a differentiable proxy for the stochastic sample.</p><p>For Bernoulli variables with mean parameter θ, the Straight-Through (ST) estimator <ref type=""bibr"" target=""#b0"">(Bengio et al., 2013)</ref> approximates m = µ θ (z), implying ∇ θ m = 1. For k = 2 (Bernoulli), ST Gumbel-Softmax is si ing ∇ θ z ≈ ∇ θ y. We call this the Straight-Through (ST) Gumbel Estimator, as it is reminiscent of the biased path derivative estimator described in <ref type=""bibr"" target=""#b0"">Bengio et al. (2013)</ref>. ST Gumbel-Softmax allows samples to be sparse even when the temperature τ is high.</p></div>",0
"n annealed via a fixed schedule), this scheme can be interpreted as entropy regularization <ref type=""bibr"" target=""#b25"">(Szegedy et al., 2015;</ref><ref type=""bibr"" target=""#b17"">Pereyra et al., 2016)</ref>, where the Gumbel-Softmax distribution can adaptively adjust the ""confidence"" of proposed s",0
"stochastic gradient estimation has traditionally focused on either score function estimators augmented with Monte Carlo variance reduction techniques <ref type=""bibr"" target=""#b16"">(Paisley et al., 2012;</ref><ref type=""bibr"" target=""#b14"">Mnih &amp; Gregor, 2014;</ref><ref type=""bibr"" target=""#b7"">",0
", image regions <ref type=""bibr"" target=""#b27"">(Xu et al., 2015)</ref>, and memory locations <ref type=""bibr"" target=""#b5"">(Graves et al., 2014;</ref><ref type=""bibr"" target=""#b4"">Graves et al., 2016)</ref>. Discrete representations are often more interpretable <ref type=""bibr"" target=""#b1"">(Chen et",0
"h as the element-wise mean E p [z] = [π 1 , ..., π k ] of these vectors.</p><p>The Gumbel-Max trick <ref type=""bibr"" target=""#b8"">(Gumbel, 1954;</ref><ref type=""bibr"" target=""#b12"">Maddison et al., 2014)</ref> provides a simple and efficient way to draw samples z from a categorical distribution with",0
"variance reduction techniques <ref type=""bibr"" target=""#b16"">(Paisley et al., 2012;</ref><ref type=""bibr"" target=""#b14"">Mnih &amp; Gregor, 2014;</ref><ref type=""bibr"" target=""#b7"">Gu et al., 2016;</ref><ref type=""bibr"" target=""#b6"">Gregor et al., 2013)</ref>, or biased path derivative estimators for the image (14×28). This is a common benchmark for training stochastic binary networks (SBN) <ref type=""bibr"" target=""#b19"">(Raiko et al., 2014;</ref><ref type=""bibr"" target=""#b7"">Gu et al., 2016;</ref><ref type=""bibr"" target=""#b15"">Mnih &amp; Rezende, 2016)</ref>. The minimization objective for thi log p θ (z)] + µ b (7)</formula><p>We briefly summarize recent stochastic gradient estimators that utilize control variates. We direct the reader to <ref type=""bibr"" target=""#b7"">Gu et al. (2016)</ref> for further detail on these techniques.</p><p>• NVIL <ref type=""bibr"" target=""#b14"">(Mnih &amp; G ulli variables, which makes the estimator biased for non-quadratic f , since it ignores the correction term µ b in the estimator expression. • MuProp <ref type=""bibr"" target=""#b7"">(Gu et al., 2016</ref>) also models the baseline as a first-order Taylor expansion:</p><formula xml:id=""formula_9"">b = f",0
"et=""#b23"">24]</ref> where the idea is to shift the loss from the image-space to a higher-level feature space of an object recognition system like VGG <ref type=""bibr"" target=""#b48"">[49]</ref>, resulting in sharper results despite lower PSNR values.</p><p>CNNs have also been found useful for the task the LR image only needs to be passed through the network once to get the result. The exclusive use of 3×3 filters is inspired by the VGG architecture <ref type=""bibr"" target=""#b48"">[49]</ref> and allows for deeper models at a low number of parameters in the network.</p><p>As the LR input is smaller lar feature representations.</p><p>For the feature map φ, we use a pre-trained implementation of the popular VGG-19 network <ref type=""bibr"">[1,</ref><ref type=""bibr"" target=""#b48"">49]</ref>. It consists of stacked convolutions coupled with pooling layers to gradually decrease the spatial dimension",1
"exploit recurrent patches of different scales within a single image <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b55"">56]</ref> or learn mappings between low and high resolution pairs of image patc Glasner <ref type=""bibr"" target=""#b16"">[17]</ref> Kim <ref type=""bibr"" target=""#b26"">[27]</ref> SCSR <ref type=""bibr"" target=""#b59"">[60]</ref> SelfEx <ref type=""bibr"" target=""#b21"">[22]</ref> SRCNN <ref type=""bibr"" target=""#b7"">[8]</ref> PSyCo <ref type=""bibr"" target=""#b39"">[40]</ref> VDSR <ref type",0
"that super-resolution algorithms can be used as a preprocessing step to improve the performance of other image-related tasks such as face recognition <ref type=""bibr"" target=""#b11"">[12]</ref>. We propose to use the performance of state-of-the-art object recognition models as a metric to evaluate ima",0
"n shown to produce sharp results in a number of image generation tasks <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b38"">39,</ref><ref type=""bibr"" target=""#b40"">41,</ref><ref type=""bibr"" target=""#b65"">66]</ref> but have so far only been applied in the context of super-resolution image I est .</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>Bicubic</head><p>ENet-E ENet-P ENet-PA ENet-PAT IHR Following common practice <ref type=""bibr"" target=""#b40"">[41]</ref>, we apply leaky ReLU activations <ref type=""bibr"" target=""#b33"">[34]</ref> and use strided convolutions to g",0
"tput, ne- cessitating an additional regularization term in the output such as total variation <ref type=""bibr"" target=""#b42"">[43]</ref>. Odena et al. <ref type=""bibr"" target=""#b37"">[38]</ref> replace the convolution transpose layers with nearest-neighbor upsampling of the feature activations in the",0
"by low dimensional manifolds. It is then unlikely that the model manifold and the true distribution's support have a non-negligible intersection (see <ref type=""bibr"" target=""#b0"">[1]</ref>), and this means that the KL distance is not defined (or simply infinite).</p><p>The typical remedy is to add br"" target=""#b5"">[6]</ref>. On the other hand, training GANs is well known for being delicate and unstable, for reasons theoretically investigated in <ref type=""bibr"" target=""#b0"">[1]</ref>.</p><p>In this paper, we direct our attention on the various ways to measure how close the model distribution adient. intersection contained in a set of measure zero. This happens to be the case when two low dimensional manifolds intersect in general position <ref type=""bibr"" target=""#b0"">[1]</ref>.</p><p>Since the Wasserstein distance is much weaker than the JS distance<ref type=""foot"" target=""#foot_2"">3</ ince the JS is locally saturated and we get vanishing gradients, as can be seen in Figure <ref type=""figure"">1</ref> of this paper and Theorem 2.4 of <ref type=""bibr"" target=""#b0"">[1]</ref>. In Figure <ref type=""figure"" target=""#fig_1"">2</ref> we show a proof of concept of this, where we train a GAN other cases collapse to a single nonsensical image <ref type=""bibr"" target=""#b3"">[4]</ref>. This last phenomenon has been theoretically explained in <ref type=""bibr"" target=""#b0"">[1]</ref> and highlighted in <ref type=""bibr"" target=""#b10"">[11]</ref>. When using the − log D trick <ref type=""bibr"" ta ing likelihood is around 0.1 to each pixel in a generated image, when the pixels were already normalized to be in the range <ref type=""bibr"">[0,</ref><ref type=""bibr"" target=""#b0"">1]</ref>. This is a very high amount of noise, so much that when papers report the samples of their models, they don't a",1
"er Φ : (Prob(X ), δ) → (C b (X ) * , • )</p><p>where Φ(P)(f ) := E x∼P [f (x)] is a linear function over C b (X ). The Riesz Representation theorem ( <ref type=""bibr"" target=""#b6"">[7]</ref>, Theorem 10) tells us that Φ is an isometric immersion. This tells us that we can effectively consider Prob(X completeness. In that case, we define D * (x) = m 2 f * (x) + m 2 . We then have 0 ≤ D(x) ≤ m and For completeness, we now show a proof for equation <ref type=""bibr"" target=""#b6"">(7)</ref> and the existence of said f * that attains the value of the infimum. Take µ = P r − P θ , which is a signed me",0
"bility. We therefore switched to RMSProp <ref type=""bibr"" target=""#b20"">[21]</ref> which is known to perform well even on very nonstationary problems <ref type=""bibr"" target=""#b12"">[13]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.3"">Improved stability</head><p>One of the ben",0
"-c.org/ns/1.0""><head>Proof. See Appendix C</head><p>Now comes the question of finding the function f that solves the maximization problem in equation <ref type=""bibr"" target=""#b1"">(2)</ref>. To roughly approximate this, something that we can do is train a neural network parameterized with weights w very useful, but they also have worse sample complexity.</p><p>• Generative Moment Matching Networks (GMMNs) <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b1"">2]</ref> are the generative counterpart of MMD. By backproping through the kernelized formula for equation ( <ref type=""",0
"</ref> which in a lot of cases makes MMD very useful, but they also have worse sample complexity.</p><p>• Generative Moment Matching Networks (GMMNs) <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b1"">2]</ref> are the generative counterpart of MMD. By backproping through the kerne ring empirical sample complexity of GANs with the theoretical sample complexity of MMDs, which tends to be worse. However, in the original GMMN paper <ref type=""bibr"" target=""#b9"">[10]</ref> they indeed used a minibatch of size 1000, much larger than the standard 32 or 64 (even when this incurred in",0
"tion</head><p>Recently, Convolutional Neural Networks (CNNs) <ref type=""bibr"" target=""#b0"">[1]</ref> have achieved great success in acoustic modeling <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b3"">4]</ref>. In the context of Automatic Speec type=""bibr"" target=""#b5"">[5,</ref><ref type=""bibr"" target=""#b6"">6]</ref>, like regular Deep Neural Networks (DNNs), which results in a hybrid system <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b3"">4]</ref>. In the typical hybrid system, the es on different time/frequency scales and provide the required non-linear modeling capabilities.</p><p>Unlike the time windows applied in DNN systems <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b3"">4]</ref>, the temporal modeling is deployed for the TIMIT dataset.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2."">Convolutional Neural Networks</head><p>Most of the CNN models <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b3"">4]</ref> in the speech domain have large fi bibr"" target=""#b0"">(1)</ref> The training speed can be very slow due to the iterative multiplications over time when the input sequence is very long; <ref type=""bibr"" target=""#b1"">(2)</ref> The training process is sometimes tricky due to the well-known problem of gradient vanishing/exploding <ref ty",1
"work that provides a distribution over sequences directly. Two popular neural network sequence models are Connectionist Temporal Classification (CTC) <ref type=""bibr"" target=""#b10"">[10]</ref> and recurrent models for sequence generation <ref type=""bibr"" target=""#b8"">[8,</ref><ref type=""bibr"" target=",1
"long; <ref type=""bibr"" target=""#b1"">(2)</ref> The training process is sometimes tricky due to the well-known problem of gradient vanishing/exploding <ref type=""bibr"" target=""#b17"">[17,</ref><ref type=""bibr"" target=""#b18"">18]</ref>. Although various approaches have been proposed to address these iss",0
"al Classification (CTC) <ref type=""bibr"" target=""#b10"">[10]</ref> and recurrent models for sequence generation <ref type=""bibr"" target=""#b8"">[8,</ref><ref type=""bibr"" target=""#b11"">11]</ref>.</p><p>To the best of our knowledge, all end-to-end neural speech recognition systems employ recurrent neural",0
"e task of speech recognition <ref type=""bibr"" target=""#b16"">[16,</ref><ref type=""bibr"" target=""#b24"">24,</ref><ref type=""bibr"" target=""#b25"">25,</ref><ref type=""bibr"" target=""#b26"">26]</ref> is the maxout function <ref type=""bibr"" target=""#b27"">[27]</ref>. Following the same computational process as",0
"r type of activation function which has been shown to improve the results for the task of speech recognition <ref type=""bibr"" target=""#b16"">[16,</ref><ref type=""bibr"" target=""#b24"">24,</ref><ref type=""bibr"" target=""#b25"">25,</ref><ref type=""bibr"" target=""#b26"">26]</ref> is the maxout function <ref t",0
"e motivated a recent surge of interests in training 'end-to-end' systems <ref type=""bibr"" target=""#b7"">[7,</ref><ref type=""bibr"" target=""#b8"">8,</ref><ref type=""bibr"" target=""#b9"">9]</ref>. End-to-end neural systems for speech recognition typically replace the HMM with a neu-ral network that provide",0
"chitecture used in this context is the Long Short-Term Memory (LSTM) <ref type=""bibr"" target=""#b12"">[12,</ref><ref type=""bibr"" target=""#b13"">13,</ref><ref type=""bibr"" target=""#b14"">14,</ref><ref type=""bibr"" target=""#b15"">15]</ref>. For example, a model with multiple layers of bi-directional LSTMs an",0
"r more details.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.3."">Training and Evaluation</head><p>To optimize the model, we use Adam <ref type=""bibr"" target=""#b31"">[31]</ref> with learning rate 10 −4 . Stochastic gradient descent with learning rate 10 −5 is then used for fine-tuning",0
"t successful recurrent neural network architecture used in this context is the Long Short-Term Memory (LSTM) <ref type=""bibr"" target=""#b12"">[12,</ref><ref type=""bibr"" target=""#b13"">13,</ref><ref type=""bibr"" target=""#b14"">14,</ref><ref type=""bibr"" target=""#b15"">15]</ref>. For example, a model with mu ith multiple layers of bi-directional LSTMs and CTC on top which is pre-trained with the transducer networks <ref type=""bibr"" target=""#b12"">[12,</ref><ref type=""bibr"" target=""#b13"">13]</ref> obtained the state-of-the-art on the TIMIT dataset. After these successes on phoneme recognition, similar sys",0
"irregular access patterns <ref type=""bibr"" target=""#b5"">[6]</ref>, <ref type=""bibr"" target=""#b6"">[7]</ref>, <ref type=""bibr"" target=""#b7"">[8]</ref>, <ref type=""bibr"" target=""#b8"">[9]</ref>, <ref type=""bibr"" target=""#b9"">[10]</ref>. While these methods show significant benefit, because they are inhe e hits. On the other hand, lbm suffers from performance degradation as the prefetching depth grows. Since lbm has a variety of memory access patterns <ref type=""bibr"" target=""#b8"">[9]</ref>, deeper prefetching with a simple delta predictor wastes bandwidth, and pollutes the cache. While deeper specu s adapting speculation relative to its accuracy.</p><p>To address both prefetching coverage and accuracy, prior work has adopted lookahead mechanisms <ref type=""bibr"" target=""#b8"">[9]</ref>, <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" target=""#b12"">[13]</ref>. These studies, however, igh hardware complexity <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" target=""#b12"">[13]</ref>, or do not implement adaptive throttling <ref type=""bibr"" target=""#b8"">[9]</ref>. Moreover, most hardware prefetchers work in the physical address space <ref type=""bibr"" target=""#b3"">[4]</ref f type=""bibr"" target=""#b8"">[9]</ref>. Moreover, most hardware prefetchers work in the physical address space <ref type=""bibr"" target=""#b3"">[4]</ref>, <ref type=""bibr"" target=""#b8"">[9]</ref>, <ref type=""bibr"" target=""#b9"">[10]</ref>, <ref type=""bibr"" target=""#b10"">[11]</ref>, where the mapping betwee ithout prefetching. Moreover, SPP outperforms recent, best of class, lookahead and non-lookahead prefetchers <ref type=""bibr"" target=""#b7"">[8]</ref>, <ref type=""bibr"" target=""#b8"">[9]</ref>, <ref type=""bibr"" target=""#b9"">[10]</ref>, including the winner of the most recent data prefetching competitio t are commonly observed during program execution, offset prefetchers take longer to train or fail to select the optimal offset. Lookahead prefetchers <ref type=""bibr"" target=""#b8"">[9]</ref>, <ref type=""bibr"" target=""#b11"">[12]</ref> efficiently encode the relationship between accesses to yield futur recursively uses that prediction to again index into the pattern table and generate further predictions. This recursion allows lookahead prefetchers <ref type=""bibr"" target=""#b8"">[9]</ref>, <ref type=""bibr"" target=""#b11"">[12]</ref> to prefetch far ahead of the current program execution, and generat true desired prefetching depth must be limited. To avoid over-prefetching, existing lookahead prefetchers <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" target=""#b8"">[9]</ref> globally and statically limit the depth to which lookahead is pursued ahead of the current demand access strea predict when they are broken up by a physical page transition. Previously proposed history-based techniques <ref type=""bibr"" target=""#b3"">[4]</ref>, <ref type=""bibr"" target=""#b8"">[9]</ref> address this by making predictions using the first offset in a page, or with very short delta histories. Altho dress spaces.</p><p>We compare SPP against three top performing recently proposed prefetching algorithms: the Variable Length Delta Prefetcher (VLDP) <ref type=""bibr"" target=""#b8"">[9]</ref>, the Best Offset Prefetcher (BOP) <ref type=""bibr"" target=""#b9"">[10]</ref>, and the DRAM-Aware Access Map Patt",1
"ing depth on a simple PC-based delta prefetcher.</p><p>attempt to predict complex, irregular access patterns <ref type=""bibr"" target=""#b5"">[6]</ref>, <ref type=""bibr"" target=""#b6"">[7]</ref>, <ref type=""bibr"" target=""#b7"">[8]</ref>, <ref type=""bibr"" target=""#b8"">[9]</ref>, <ref type=""bibr"" target=""#b re demand accesses. Later efforts extended this approach to detect the temporal order between delta patterns <ref type=""bibr"" target=""#b5"">[6]</ref>, <ref type=""bibr"" target=""#b6"">[7]</ref>. While these prefetchers achieve good performance, they require megabytes of hardware state storage, which is",0
"curacy.</p><p>To address both prefetching coverage and accuracy, prior work has adopted lookahead mechanisms <ref type=""bibr"" target=""#b8"">[9]</ref>, <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" target=""#b12"">[13]</ref>. These studies, however, suffer from high hardware complexity <re >, <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" target=""#b12"">[13]</ref>. These studies, however, suffer from high hardware complexity <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" target=""#b12"">[13]</ref>, or do not implement adaptive throttling <ref type=""bibr"" target= lue is used to throttle prefetching depth dynamically in order to balance prefetch coverage with accuracy. ? Unlike prior lookahead based prefetchers <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" target=""#b12"">[13]</ref>, SPP does not require deep hooks into the core microarchitecture ecution, offset prefetchers take longer to train or fail to select the optimal offset. Lookahead prefetchers <ref type=""bibr"" target=""#b8"">[9]</ref>, <ref type=""bibr"" target=""#b11"">[12]</ref> efficiently encode the relationship between accesses to yield future predictions, enabling further speculati n index into the pattern table and generate further predictions. This recursion allows lookahead prefetchers <ref type=""bibr"" target=""#b8"">[9]</ref>, <ref type=""bibr"" target=""#b11"">[12]</ref> to prefetch far ahead of the current program execution, and generate timely prefetches for as long as their it is unlikely to persist forever, thus the true desired prefetching depth must be limited. To avoid over-prefetching, existing lookahead prefetchers <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" target=""#b8"">[9]</ref> globally and statically limit the depth to which lookahead is pursu",0
"</profileDesc> 	</teiHeader> 	<text xml:lang=""en""> 		<body> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>I. INTRODUCTION</head><p>The ""Memory Wall"" <ref type=""bibr"" target=""#b0"">[1]</ref>, the vast gulf between processor execution speed and memory latency, has led to the development of large and d",0
"p periods, and it can prefetch complex patterns in new physical pages sooner, resulting in higher prefetch coverage. Unlike the Global History Buffer <ref type=""bibr"" target=""#b16"">[17]</ref>, which records all observed delta patterns, the GHR only stores delta patterns that cross page boundaries. N",0
"pe=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b6"">7]</ref>. This important finding attracts great interests in edge partitioning recently <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b14"">15 s all existing methods with large margins. Among existing partitioners, METIS gives the lowest replication factor which is consistent with literature <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b12"">13]</ref>. However, METIS runs out of memory when partitioning the Twitter, Frie ication factor. The notable multi-level vertex partitioning algorithm METIS <ref type=""bibr"" target=""#b7"">[8]</ref> is extended for edge partitioning <ref type=""bibr"" target=""#b2"">[3]</ref>, which makes full access to the graph structure by partitioning the graph entirely in memory. This does lead t i ∩ E j = ∅ for i j. A p-edge partitioning is α-balanced if max i ∈[p] {|E i |} ≤ ⌈ α |E | /p⌉.<label>(1)</label></formula><p>The replication factor <ref type=""bibr"" target=""#b2"">[3]</ref> of a partitioning is defined as</p><formula xml:id=""formula_3"">RF(E 1 , . . . ,E p ) := 1 |V | i ∈[p] |V (E i p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2.3"">NP-Hardness</head><p>The p-edge partitioning problem has been proved to be NP-hard in <ref type=""bibr"" target=""#b2"">[3]</ref> when p grows with n = |V |. To our best knowledge, it has not been proved elsewhere that this problem is NP-ha nimizes edge cut and balances user-defined vertex weight. One can turn a vertex-partitioner into an edge-partitioner while preserving its performance <ref type=""bibr"" target=""#b2"">[3]</ref>. To transform METIS to an edge-partitioner, we first call METIS with the vertex weight equal to its degree. Th s=""2"">2.83 3.00 1.65</cell><cell>1.94</cell></row></table></figure> 			<note xmlns=""http://www.tei-c.org/ns/1.0"" place=""foot"" n=""1"" xml:id=""foot_0"">In<ref type=""bibr"" target=""#b2"">[3]</ref>, the NP-hardness is proved by a reduction from 3-partition problem. When p is constant, the 3-partition proble",1
"important finding attracts great interests in edge partitioning recently <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b16"">17]</ref>. Edge partitioning has been w power-law degree distribution, like DBH. Both HDRF and Oblivious rely only on historical data, and thus use the graph structure only partially. Sheep <ref type=""bibr"" target=""#b12"">[13]</ref> partitions the graph in a divide and conquer manner, which uses more graph structure than Oblivious and HDRF graph structure by partitioning the graph entirely in memory. This does lead to the state-of-the-art replication factors on a great number of graphs <ref type=""bibr"" target=""#b12"">[13]</ref>, but it is not applicable to large graphs.</p><p>In this paper, we present a heuristic called NE (Neighbor E ef type=""bibr"" target=""#b16"">[17]</ref>, Oblivious <ref type=""bibr"" target=""#b5"">[6]</ref>, HDRF <ref type=""bibr"" target=""#b14"">[15]</ref>, and Sheep <ref type=""bibr"" target=""#b12"">[13]</ref>. METIS is the state-of-the-art method for vertex partitioning which minimizes edge cut and balances user-def s out of memory when partitioning the Twitter, Friendster and UK-union graphs on our 125 GB RAM machine. This result also echoes the fact reported by <ref type=""bibr"" target=""#b12"">[13]</ref>.</p><p>Running time. Fig. <ref type=""figure"" target=""#fig_6"">5</ref> plots the running time of all partition ilar to DBH since they all scan the input graph once sequentially <ref type=""bibr"" target=""#b16"">[17]</ref>. Using only a single thread, NE and Sheep <ref type=""bibr"" target=""#b12"">[13]</ref> have similar running time. But Sheep can be speeded up significantly by multi-threading, which is not direct ge partitioner Neighbor Expansion (NE) that outperforms other state-of-the-art ones including METIS <ref type=""bibr"" target=""#b7"">[8]</ref> and Sheep <ref type=""bibr"" target=""#b12"">[13]</ref> in terms of replication factors. Applying the NE algorithm to distributed graph mining effectively reduces c ns. Among existing partitioners, METIS gives the lowest replication factor which is consistent with literature <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b12"">13]</ref>. However, METIS runs out of memory when partitioning the Twitter, Friendster and UK-union graphs on our 125 G",1
"world graphs, edge partitioning (Fig. <ref type=""figure"" target=""#fig_0"">1</ref>) is found to be more effective in advanced distributed graph engines <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b15"">16]</ref>, which evenly assigns edges to m h vertex partition can vary widely. Not surprisingly, researchers demonstrated that edge partitioning performs better on many large real-world graphs <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b6"">7]</ref>. This important finding attracts great interests in edge partitioning re pe=""bibr"" target=""#b6"">7]</ref>. This important finding attracts great interests in edge partitioning recently <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b16"">1 rget=""#b14"">15,</ref><ref type=""bibr"" target=""#b16"">17]</ref>. Edge partitioning has been widely adopted in recent graph systems including PowerGraph <ref type=""bibr"" target=""#b5"">[6]</ref>, Spark GraphX <ref type=""bibr"" target=""#b6"">[7]</ref>, and Chaos <ref type=""bibr"" target=""#b15"">[16]</ref>, to machines should be minimized, which is often a bottleneck in graph computing due to the intrinsic dependency and expensive random access in the graph <ref type=""bibr"" target=""#b5"">[6]</ref>. While it is relatively easy to solve the first issue by partitioning the edges evenly, the second issue can b of smaller degree. DBH can exploit the skewed degree distribution of power-law graphs, but it leverages little graph structure, like RAND. Oblivious <ref type=""bibr"" target=""#b5"">[6]</ref> is a streaming algorithm that considers the distribution of previously assigned edges when assigning an incomi o evaluate whether the given partitioner can reduce the communication cost and execution time on distributed graph processing systems like PowerGraph <ref type=""bibr"" target=""#b5"">[6]</ref> and PowerLyra <ref type=""bibr"" target=""#b3"">[4]</ref> in Section 5.2.</p><p>Testbed. We evaluate all partition ompeting partitioners. We compare our NE algorithm with six existing edge partitioners, including METIS <ref type=""bibr"" target=""#b7"">[8]</ref>, RAND <ref type=""bibr"" target=""#b5"">[6]</ref>, DBH <ref type=""bibr"" target=""#b16"">[17]</ref>, Oblivious <ref type=""bibr"" target=""#b5"">[6]</ref>, HDRF <ref t METIS <ref type=""bibr"" target=""#b7"">[8]</ref>, RAND <ref type=""bibr"" target=""#b5"">[6]</ref>, DBH <ref type=""bibr"" target=""#b16"">[17]</ref>, Oblivious <ref type=""bibr"" target=""#b5"">[6]</ref>, HDRF <ref type=""bibr"" target=""#b14"">[15]</ref>, and Sheep <ref type=""bibr"" target=""#b12"">[13]</ref>. METIS is",0
"mory, while other methods like Sheep or DBH only store vertex set. For instance, our implementation of NE takes about 90 GB RAM to partition uk-union <ref type=""bibr"" target=""#b1"">[2]</ref>. We have made some preliminary attempts to extend the NE algorithm to a streaming algorithm via sampling metho liminary attempts to extend the NE algorithm to a streaming algorithm via sampling methods (Appendix B), which is able to partition the clueweb graph <ref type=""bibr"" target=""#b1"">[2]</ref> (|V | = 978M, |E| = 42.5B) whose edge set exceeds the volume of main memory.</p><p>Since x +y + s = k we know aming algorithm (Alg. 3) in Table <ref type=""table"" target=""#tab_7"">4</ref>. To evaluate the scalability of the streaming we add a new graph clue-web <ref type=""bibr"" target=""#b1"">[2]</ref> of 978, 408, 098 vertices and 42, 574, 107, 469 edges. Comparing with NE, the streaming algorithm enables us t",0
"</ref>; when 2 &lt; τ &lt; 3.4785 and κ → ∞, besides small connected components the graph contains a giant component of constant fraction of vertices <ref type=""bibr"" target=""#b0"">[1]</ref>. To give a quantitative argument, we define the following terms. If we pick an edge and go to one of its end v",0
"ref type=""figure"" target=""#fig_0"">1</ref>) is found to be more effective in advanced distributed graph engines <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b15"">16]</ref>, which evenly assigns edges to machines in a way that minimizes the num surprisingly, researchers demonstrated that edge partitioning performs better on many large real-world graphs <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b6"">7]</ref>. This important finding attracts great interests in edge partitioning recently <ref type=""bibr"" target=""#b2"">[3 ]</ref>. Edge partitioning has been widely adopted in recent graph systems including PowerGraph <ref type=""bibr"" target=""#b5"">[6]</ref>, Spark GraphX <ref type=""bibr"" target=""#b6"">[7]</ref>, and Chaos <ref type=""bibr"" target=""#b15"">[16]</ref>, to divide the graph across machines. It turns out that t that</p><formula xml:id=""formula_44"">(1 − β )e (H ) = |W | + 3 k 2 .</formula><p>Thus e (H ) − 3 k 2 &gt; βe (H ). However, if this is the case, then <ref type=""bibr"" target=""#b6"">(7)</ref> </p><formula xml:id=""formula_45"">tells us that |E (V 1 ) ∪ E (V 1 ,S )| &gt; βe (H ). This is a contradiction.",0
"apply Theorem 4.1, we need to estimate the expectation of |E | /n and σ /n. Here we apply the elegant treatment of random power-law graphs by Newman <ref type=""bibr"" target=""#b13"">[14]</ref> using generating functions, and derive the expected number of connected components and the expected number o G ′ 0 (1). Computing E [ σ /n]. We now estimate the term E[ σ /n]. When τ &gt; 2,</formula><p>it is almost sure that the graph is not fully connected <ref type=""bibr"" target=""#b13"">[14]</ref>; when 2 &lt; τ &lt; 3.4785 and κ → ∞, besides small connected components the graph contains a giant componen edge leads to the giant component, and t k is the probability that this edge leads to a small component of size k. Here we follow Newman's assumption <ref type=""bibr"" target=""#b13"">[14]</ref> that there is no loop in the small components.</p><p>Hence we have</p><formula xml:id=""formula_38"">H 1 (x )",0
".tei-c.org/ns/1.0""><head n=""1"">INTRODUCTION</head><p>To handle large-scale graphs, distributed graph engines <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b15"">16]</ref> partition the input graph and parallelize the computation on a cluste",0
"arator problem with β = α /2. We show that</p><p>, and this will infer that two problems are equivalent.</p><p>Given Proof. The proof is motivated by <ref type=""bibr"" target=""#b4"">[5]</ref> which proves that a vertex constraint MIN-separator problem is NP-hard. To the best of our knowledge, our work",0
"ang=""en""> 		<body> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""1"">INTRODUCTION</head><p>To handle large-scale graphs, distributed graph engines <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b15"">16]</ref> partition the input graph an",0
"idea that if there is a DDI between drug A and drug B, and drug C has a similar structure to drug A, there is likely a DDI between drug C and drug B <ref type=""bibr"" target=""#b5"">[8]</ref>. Vilar et al. predicted DDIs with a matrix transformation approach using structural similarities of drugs with ""#b5"">[8]</ref>. Vilar et al. predicted DDIs with a matrix transformation approach using structural similarities of drugs with molecular fingerprints <ref type=""bibr"" target=""#b5"">[8]</ref>. In subsequent studies, the authors reported prediction methods using integrated similarity measures including f the top ten predicted drugs were supported by reports in literature and databases. Comparing to the prediction results in the study by Vilar et al. <ref type=""bibr"" target=""#b5"">[8]</ref>, which also used drug structural similarities, two of the top ten drugs predicted by our model (i.e. salicycli top ten drugs (i.e. testosterone, prednisolone, prednisone, and lovastatin) were predicted to have DDI with simvastatin in the study by Vilar et al. <ref type=""bibr"" target=""#b5"">[8]</ref>.</p><p>These two case studies suggested that our approach could also be used to predict possible enzymes and t",1
"odel for predicting DDI was developed through integrated clinical side effect information from the drug labels and FDA adverse event reporting system <ref type=""bibr"" target=""#b15"">[18]</ref>. Electronic health records (EHRs) were also used to identify or prioritize drug-drug-adverse events <ref typ",0
"to a drug, which includes absorption, distribution, metabolism, and excretion (ADME). DDI occurs when two drugs share the same mechanism of excretion <ref type=""bibr"" target=""#b1"">[4]</ref>. A significant number of studies on PK-based DDI have been done at the molecular level involving enzymes and t br"" target=""#b2"">[5]</ref>. For example, changes in gastric pH caused by a drug can affect the gastro-intestinal absorption of a co-administered drug <ref type=""bibr"" target=""#b1"">[4]</ref>. If two drugs both binding to a same plasma protein are co-administered, the concentration of the free drugs i get=""#b1"">[4]</ref>. If two drugs both binding to a same plasma protein are co-administered, the concentration of the free drugs in plasma may change <ref type=""bibr"" target=""#b1"">[4]</ref>. Also, various drugs are substrates, inhibitors, or inducers of the CYP enzymes, the dominant metabolic enzyme",0
"rfarin <ref type=""bibr"" target=""#b23"">[27]</ref>. It is reported that special precautions are necessary when taking dronabinol together with warfarin <ref type=""bibr"" target=""#b24"">[28]</ref>. Overall, DDI between warfarin and eight out of the top ten predicted drugs were supported by reports in lit",0
"se dangerous DDI and should be used with cautions and close monitoring. It is reported that quercetin displaces warfarin bound to human serum albumin <ref type=""bibr"" target=""#b22"">[26]</ref> due to competitive binding and that genistein also shares the binding sites in human serum albumin with warf association data (pharmacogenetic data) were retrieved from PharmGKB <ref type=""bibr"" target=""#b34"">[38]</ref> (3262 associations, downloaded on Sep. <ref type=""bibr"" target=""#b22"">26,</ref><ref type=""bibr"">2014)</ref>. Protein-protein physical interaction data was retrieved from BioGRID <ref type=""",0
"s to have no online adaptation of the network. Recent works have focused on learning deep embeddings that can be used as universal object descriptors <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b16""> ><p>We present an extensive investigation into the effect of incorporating the CF into the fully-convolutional Siamese framework of Bertinetto et al. <ref type=""bibr"" target=""#b2"">[3]</ref>. We find that the CF does not improve results for networks that are sufficiently deep. However, our method ena king community for their simplicity and competitive performance. For our method, we prefer to build upon the fully-convolutional Siamese architecture <ref type=""bibr"" target=""#b2"">[3]</ref>, as it enforces the prior that the appearance similarity function should commute with translation.</p><p>At it v xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.1."">Fully-convolutional Siamese networks</head><p>Our starting point is a network similar to that of <ref type=""bibr"" target=""#b2"">[3]</ref>, which we later modify in order to allow the model to be interpreted as a Correlation Filter tracker. The full tches. To apply this network to object tracking, it is necessary to combine this with a procedure that describes the logic of the tracker. Similar to <ref type=""bibr"" target=""#b2"">[3]</ref>, we employ a simplistic tracking algorithm to assess the utility of the similarity function.</p><p>Online trac the effect of incorporating the Correlation Filter during training. We first compare against the symmetric Siamese architecture of Bertinetto et al. <ref type=""bibr"" target=""#b2"">[3]</ref>. We then compare the endto-end trained CFNet to a variant where the features are replaced with features that w works to convergence multiple times with different random seeds, this would require significantly more resources. Our baseline diverges slightly from <ref type=""bibr"" target=""#b2"">[3]</ref> in two ways. Firstly, we reduce the total stride of the network from 8 to 4 (2 at conv1, 2 at pool1) to avoid he most promising singlelayer network (CFNet-conv1). We compare our methods against state-of-the-art trackers that can operate in realtime: SiamFC-3s <ref type=""bibr"" target=""#b2"">[3]</ref>, Staple <ref type=""bibr"" target=""#b1"">[2]</ref> and LCT <ref type=""bibr"" target=""#b21"">[22]</ref>. We also inc earning and domain adaptation.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>A. Implementation details</head><p>We follow the procedure of <ref type=""bibr"" target=""#b2"">[3]</ref> to minimize the loss (equation 2) through SGD, with the Xavier-improved parameters initialization and using mi terminated after the n-th linear layer, including the following ReLU but not the following pooling layer (if any).Our baseline diverges slightly from<ref type=""bibr"" target=""#b2"">[3]</ref> in two ways. Firstly, we reduce the total stride of the network from 8 to 4 (2 at conv1, 2 at pool1) to avoid cell><cell>-</cell></row></table></figure> 			<note xmlns=""http://www.tei-c.org/ns/1.0"" place=""foot"" n=""1"" xml:id=""foot_0"">Note that this differs from<ref type=""bibr"" target=""#b2"">[3]</ref>, in which the target object and search area were instead denoted z and x respectively.</note> 			<note xmlns="" ntly, several methods based on Siamese networks have been introduced <ref type=""bibr"" target=""#b27"">[28,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b2"">3]</ref>, raising interest in the tracking community for their simplicity and competitive performance. For our method, w ation of a standard CF tracker <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b2"">3]</ref> with CNN features, as proposed in previous work <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" targe",1
"t algorithm that learns to discriminate an image patch from the surrounding patches by solving a large ridge regression problem extremely efficiently <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b12"">13]</ref>. It has proved to be highly successful in object tracking (e.g. <ref t in practice we did not find learning this parameter to improve the tracking accuracy compared to the conventional choice of a fixed Gaussian response <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b12"">13]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4."">Exper ning at high framerates.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2."">Related work</head><p>Since the seminal work of Bolme et al. <ref type=""bibr"" target=""#b3"">[4]</ref>, the Correlation Filter has enjoyed great popularity within the tracking community. Notable efforts have been et al. <ref type=""bibr"" target=""#b14"">[15]</ref>. To reduce the effect of circular boundaries, the feature map x is pre-multiplied by a cosine window <ref type=""bibr"" target=""#b3"">[4]</ref> and the final template is cropped <ref type=""bibr"" target=""#b28"">[29]</ref>.</p><p>Notice that the forward pas",1
"fthe-art results.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.1."">Evaluation criteria</head><p>Popular tracking benchmarks like VOT <ref type=""bibr"" target=""#b15"">[16]</ref> and OTB <ref type=""bibr"" target=""#b31"">[32,</ref><ref type=""bibr"" target=""#b32"">33]</ref> have made all grou",0
"ssful in object tracking (e.g. <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b1"">2]</ref>), where its efficiency enables a tracker to adapt its internal model of the object on the fly at every frame. I erature (e.g. bounding box regression <ref type=""bibr"" target=""#b24"">[25]</ref>, ensembling of multiple cues <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b1"">2]</ref>, optical flow <ref type=""bibr"" target=""#b27"">[28]</ref>).</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><h v1). We compare our methods against state-of-the-art trackers that can operate in realtime: SiamFC-3s <ref type=""bibr"" target=""#b2"">[3]</ref>, Staple <ref type=""bibr"" target=""#b1"">[2]</ref> and LCT <ref type=""bibr"" target=""#b21"">[22]</ref>. We also include the recent SAMF <ref type=""bibr"" target=""#b",0
"ion feature maps <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b8"">9]</ref> and augmenting the objective with a more robust loss <ref type=""bibr"" target=""#b25"">[26]</ref>. For the sake of simplicity, in this work we adopt the basic formulation of the Correlation Filter.</p><p>Re",0
"rity within the tracking community. Notable efforts have been devoted to its improvement, for example by mitigating the effect of periodic boundaries <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b7"">8]</ref>, incorporating multi-resolution",0
"CF with the discriminative power of CNN features trained offline. This has been done in several works (e.g. <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b30"">31]</ref>), which have shown that CNNs and arget=""#b21"">22,</ref><ref type=""bibr"" target=""#b2"">3]</ref> with CNN features, as proposed in previous work <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b6"">7]</ref>. However, these earlier networks were not trained end-to-end. The novelty is to compute the derivative of the C allenge <ref type=""bibr"" target=""#b26"">[27]</ref>. The results show that these features, which are often the first choice for combining CFs with CNNs <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b24"">25 ine, requiring only 600kB to store. This may be of particular interest for embedded devices with limited memory. In contrast, methods like Deep-SRDCF <ref type=""bibr"" target=""#b6"">[7]</ref> and C-COT <ref type=""bibr"" target=""#b8"">[9]</ref>, which use out-of-the-box deep features for the Correlation",0
"an image patch from the surrounding patches by solving a large ridge regression problem extremely efficiently <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b12"">13]</ref>. It has proved to be highly successful in object tracking (e.g. <ref type=""bibr"" target=""#b5"">[6,</ref><ref t is parameter to improve the tracking accuracy compared to the conventional choice of a fixed Gaussian response <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b12"">13]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4."">Experiments</head><p>The principal aim of ou k w = ω(x) computes a standard CF template w from the training feature map x = f ρ (x ′ ) by solving a ridge regression problem in the Fourier domain <ref type=""bibr"" target=""#b12"">[13]</ref>. Its effect can be understood as crafting a discriminative template that is robust against translations. It ilter is the template w ∈ R m×m whose inner product with each circular shift of the image x * δ −u is as close as possible to a desired response y[u] <ref type=""bibr"" target=""#b12"">[13]</ref>, minimizing u∈U</p><formula xml:id=""formula_3"">( x * δ −u , w − y[u]) 2 = w ⋆ x − y 2 . (<label>4</label></f ultipliers of a constrained optimization problem that is equivalent to eq. 5. The solution to eq. 6 can be computed efficiently in the Fourier domain <ref type=""bibr"" target=""#b12"">[13]</ref>,</p><formula xml:id=""formula_9"">     k = 1 n ( x * • x) + λ✶ α = 1 n k −1 • y w = α * • x (7a) (7b) (7c p>Notice that the forward pass of the architecture in Figure <ref type=""figure"">1</ref> corresponds exactly to the operation of a standard CF tracker <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b2"">3",0
".tei-c.org/ns/1.0""><head n=""4.1."">Evaluation criteria</head><p>Popular tracking benchmarks like VOT <ref type=""bibr"" target=""#b15"">[16]</ref> and OTB <ref type=""bibr"" target=""#b31"">[32,</ref><ref type=""bibr"" target=""#b32"">33]</ref> have made all ground truth annotations available and do not enforce he same set of ""natural"" hyperparameters, which are reasonable for all methods and not tuned for any particular method.</p><p>As in the OTB benchmark <ref type=""bibr"" target=""#b31"">[32,</ref><ref type=""bibr"" target=""#b32"">33]</ref>, we quantify the performance of the tracker on a sequence in terms o 013/50/100 benchmarks to confirm that our results are on par with the state-of-theart. All numbers in this section are obtained using the OTB toolkit <ref type=""bibr"" target=""#b31"">[32]</ref>. We report the results for the three best instantiations of CFNet from Figure <ref type=""figure"" target=""#fi",0
"etwork. Recent works have focused on learning deep embeddings that can be used as universal object descriptors <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b4""> on of the Correlation Filter.</p><p>Recently, several methods based on Siamese networks have been introduced <ref type=""bibr"" target=""#b27"">[28,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b2"">3]</ref>, raising interest in the tracking community for their simplicity and co",0
"veloped to minimize this problem <ref type=""bibr"" target=""#b14"">(Rusu et al., 2016;</ref><ref type=""bibr"" target=""#b6"">Kirkpatrick et al., 2017;</ref><ref type=""bibr"" target=""#b2"">Fernando et al., 2017)</ref>. Generative models, on the other hand, are a more natural fit for this kind of setup since",1
"s at a time, and specialized techniques are actively being developed to minimize this problem <ref type=""bibr"" target=""#b14"">(Rusu et al., 2016;</ref><ref type=""bibr"" target=""#b6"">Kirkpatrick et al., 2017;</ref><ref type=""bibr"" target=""#b2"">Fernando et al., 2017)</ref>. Generative models, on the oth except for the word embedding matrix. A promising method to prevent catastrophic forgetting in discriminative models is elastic weight consolidation <ref type=""bibr"" target=""#b6"">(Kirkpatrick et al., 2017)</ref>. However, the method requires computing a Fisher information matrix, and it is not clea",1
"n learning sequentially from examples from a single class at a time, and specialized techniques are actively being developed to minimize this problem <ref type=""bibr"" target=""#b14"">(Rusu et al., 2016;</ref><ref type=""bibr"" target=""#b6"">Kirkpatrick et al., 2017;</ref><ref type=""bibr"" target=""#b2"">Fer",1
"rget=""#b8"">(Morin &amp; Bengio, 2005)</ref>, noise contrastive estimation <ref type=""bibr"" target=""#b7"">(Mnih &amp; Teh, 2012)</ref>, sampled softmax <ref type=""bibr"" target=""#b4"">(Jean et al., 2015)</ref>, or one-vs-each approximation <ref type=""bibr"" target=""#b15"">(Titsias, 2017)</ref>. However, e",0
"e=""foot"" target=""#foot_0"">1</ref> For the generative model, the dimension of the class embedding is also set to 100. We train our model using AdaGrad <ref type=""bibr"" target=""#b1"">(Duchi et al., 2012)</ref> and tune the learning rate on development sets. We also use the development sets to decide wh",0
"> or convolutional neural networks <ref type=""bibr"" target=""#b18"">(Zhang et al., 2015;</ref><ref type=""bibr"" target=""#b16"">Xiao &amp; Cho, 2016;</ref><ref type=""bibr"" target=""#b0"">Conneau et al., 2016)</ref>. All of the generative models have lower classification accuracies. These results agree with",0
"tive LSTM model is competitive with other discriminative models based on logistic regression <ref type=""bibr"" target=""#b18"">(Zhang et al., 2015;</ref><ref type=""bibr"" target=""#b5"">Joulin et al., 2016)</ref> or convolutional neural networks <ref type=""bibr"" target=""#b18"">(Zhang et al., 2015;</ref><re",0
"the label embeddings v y using standard word embedding techniques for all y ∈ Y. In order to do this, we use pretrained GloVe word embedding vectors <ref type=""bibr"" target=""#b12"">(Pennington et al., 2014)</ref>. <ref type=""foot"" target=""#foot_4"">5</ref> In cases where the class labels consist of m",0
"ibr"" target=""#b7"">(Mnih &amp; Teh, 2012)</ref>, sampled softmax <ref type=""bibr"" target=""#b4"">(Jean et al., 2015)</ref>, or one-vs-each approximation <ref type=""bibr"" target=""#b15"">(Titsias, 2017)</ref>. However, even with these approximations, discriminative models are still much faster.</p><p>Data",0
"in sample complexity and ability to adapt to shifting data distributions.</p><p>While neural networks are traditionally used as discriminative models <ref type=""bibr"" target=""#b9"">(Ney, 1995;</ref><ref type=""bibr"" target=""#b13"">Rubinstein &amp; Hastie, 1997)</ref>, their flexibility makes them well",0
"tion behavior of complex neural networks is difficult, with findings from convex problems failing to account for empirical facts about generalization <ref type=""bibr"" target=""#b17"">Zhang et al. (2017)</ref>. As such, this result is remarkable for being one domain in which generalization behavior of",0
"<head n=""3"">Experiments</head></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.1"">Datasets</head><p>We use publicly available datasets from <ref type=""bibr"" target=""#b18"">Zhang et al. (2015)</ref> to evaluate our models (http: //goo.gl/JyCnZq). They are standard text classification dataset ults from previous work on these datasets. Our discriminative LSTM model is competitive with other discriminative models based on logistic regression <ref type=""bibr"" target=""#b18"">(Zhang et al., 2015;</ref><ref type=""bibr"" target=""#b5"">Joulin et al., 2016)</ref> or convolutional neural networks <re on <ref type=""bibr"" target=""#b18"">(Zhang et al., 2015;</ref><ref type=""bibr"" target=""#b5"">Joulin et al., 2016)</ref> or convolutional neural networks <ref type=""bibr"" target=""#b18"">(Zhang et al., 2015;</ref><ref type=""bibr"" target=""#b16"">Xiao &amp; Cho, 2016;</ref><ref type=""bibr"" target=""#b0"">Conne",0
"upplementary material. Furthermore, utilizing more negative links in each mini-batch can lower the expected stochastic gradient variance. As shown in <ref type=""bibr"" target=""#b34"">[35,</ref><ref type=""bibr"" target=""#b35"">36]</ref>, the reduction of variance can lead to faster convergence. This sugg ce of stochastic gradient estimator, or (2) distributing the training over multiple workers. Several sampling techniques, such as stratified sampling <ref type=""bibr"" target=""#b34"">[35]</ref> and importance sampling <ref type=""bibr"" target=""#b35"">[36]</ref> are proposed to achieve the variance reduc",1
"zing more negative links in each mini-batch can lower the expected stochastic gradient variance. As shown in <ref type=""bibr"" target=""#b34"">[35,</ref><ref type=""bibr"" target=""#b35"">36]</ref>, the reduction of variance can lead to faster convergence. This suggests that Negative Sharing (w./wo. Strati ing over multiple workers. Several sampling techniques, such as stratified sampling <ref type=""bibr"" target=""#b34"">[35]</ref> and importance sampling <ref type=""bibr"" target=""#b35"">[36]</ref> are proposed to achieve the variance reduction. Different from their work, we improve sampling strategies in",1
"and Computational Challenges</head><p>To train the model, we use stochastic gradient descent based algorithms <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b15"">16]</ref>, which are widely used for training matrix factorization and neural networks. The main flow of the training a all Time.</p><p>Parameter Settings. The key parameters are tuned with validation set, while others are simply set to reasonable values. We adopt Adam <ref type=""bibr"" target=""#b15"">[16]</ref> as the stochastic optimizer. We use the same batch size b = 512 for all sampling strategies, we use the numb et=""#b3"">4,</ref><ref type=""bibr"" target=""#b29"">30]</ref>.</p><p>Stochastic Gradient Descent <ref type=""bibr"" target=""#b2"">[3]</ref> and its variants <ref type=""bibr"" target=""#b15"">[16]</ref> have been widely adopted in training machine learning models, including neural networks. Samples are drawn u",0
"of computation. This assumption is shattered by models that are defined under graph structure, with applications in social and knowledge graph mining <ref type=""bibr"" target=""#b1"">[2]</ref>, image caption ranking <ref type=""bibr"" target=""#b19"">[20]</ref>, and so on. For those scenarios, we believe b",0
"in news recommendation. To overcome this problem, hybrid methods are proposed to incorporate side information <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b27"">28]</ref>, or item content information <ref type=""bibr"" target=""#b10"">[11,</ref performance, hybrid methods are proposed to incorporate side information <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b37"">38]</ref>, as well as content informati",0
"ir preferences, while negative signals are usually implicit. This is usually referred as ""implicit feedback"" <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b25"">26]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>Table 1: Ex ating prediction, implicit feedback is found in many real-world scenarios and studied by many papers as well <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b25"">26]</ref>. Although collaborative filtering techniques are powerful, they suffe",0
"ef>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>Table 1: Examples of loss functions for recommendation.</head><p>Pointwise loss SG-loss <ref type=""bibr"" target=""#b21"">[22]</ref>:</p><formula xml:id=""formula_1"">-(u,v )∈D log σ (f T u g v ) + λE v ′ ∼Pn log σ (−f T u g v ′ )</formula><p> MSE) has been applied where ""negative terms"" are weighted less. And skip-gram (SG) loss has been successfully utilized to learn robust word embedding <ref type=""bibr"" target=""#b21"">[22]</ref>. These two loss functions are summarized in Table <ref type=""table"">1</ref>. Note that we use a weighted exp vector equals to the true gradient in expectation. In the setting where negative examples are overwhelming, such as in word embedding (e.g., Word2Vec <ref type=""bibr"" target=""#b21"">[22]</ref>) and network embedding (e.g., LINE <ref type=""bibr"" target=""#b28"">[29]</ref>) tasks, negative sampling is ut negative links (with a different distribution).</p><p>Many existing algorithms with graph-based loss functions <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b28"">29]</ref> adopt the ""Negative Sampling"" strategy, in which k negative samples a",0
"UCTION</head><p>Collaborative Filtering (CF) has been one of the most effective methods in recommender systems, and methods like matrix factorization <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b26"">27]</ref> are widely adopted. However, ly extracting high-level features from content data. However, it also comes with a price. Traditional CF methods, such as sparse matrix factorization <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b26"">27]</ref>, are usually fast to train, while the deep neural networks in genera t.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2.2"">Functional Embedding</head><p>In most of existing matrix factorization techniques <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b26"">27]</ref>, each user/item ID is associ ng <ref type=""bibr"" target=""#b17"">[18]</ref> has been one of the most effective methods in recommender systems, and methods like matrix factorization <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b26"">27]</ref> are widely adopted. While many papers focus on the explicit feedback",0
"ine deep learning with traditional collaborative filtering for recommendation tasks, as seen in recent studies <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b31"">32,</ref><ref type=""bibr"" target=""#b36"">37]</ref>.</p><p>In this work, we general ref>.</p><p>In this work, we generalize several state-of-the-art neural networkbased recommendation algorithms <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b29"">30]</ref>, and propose a more general framework that combines both collaborative g/ns/1.0""><head n=""2.1"">Text Recommendation Problem</head><p>In this work, we use the text recommendation task <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b30"">31,</ref><ref type=""bibr"" target=""#b31"">32]</ref> as an illustrative application g, against IID sampling and Negative Sampling. It is worth noting that several existing stateof-the-art models <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b29"">30]</ref> are special cases of our framework (e.g. using MSE-loss/Log-loss with C DNNs for recommender systems, recent efforts are made in combining collaborative filtering and neural networks <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b29"">30,</ref><ref type=""bibr"" target=""#b31"">32]</ref>. <ref type=""bibr"" target=""#b31"" unsupervised text embedding. The general functional embedding framework in this work subsumes existing models <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b29"">30]</ref>.</p><p>Stochastic Gradient Descent <ref type=""bibr"" target=""#b2"">[3]</r h functions such as decision trees <ref type=""bibr"" target=""#b37"">[38]</ref> and some specific neural networks <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b3"">4]</ref>. Generalizing the existing work, we propose to replace the original embedding vectors u and v with general diff of 204,986 positive interactions in CiteULike data. As for Yahoo! News data, there are 10,000 users, 58,579 items and 515,503 interactions. Following <ref type=""bibr"" target=""#b3"">[4]</ref>, we select a portion (20%) of items to form the pool of test items. All user interactions with those test item <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b29"">30]</ref> can be improved by others such as SG-loss and pairwise loss functions <ref type=""bibr"" target=""#b3"">[4]</ref>.</p><p>To further investigate the superior performance brought by Negative Sharing. We study the number of neg item-side text information for article recommendation, <ref type=""bibr"" target=""#b0"">[1]</ref> adopts RNN/GRU to better understand the text content. <ref type=""bibr"" target=""#b3"">[4]</ref> proposes to use CNN and pairwise loss functions, and also incorporate unsupervised text embedding. The general f type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b37"">38]</ref>, as well as content information <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b30"">31,</ref><ref type=""bibr"" target=""#b31"">",0
"filters with filter size of 3. Regularization is added using both weight decay on user embedding and dropout on item embedding. For RNN, we use LSTM <ref type=""bibr"" target=""#b11"">[12]</ref> with 50 hidden units. For both models, the dimensions of user and word embedding are set to 50. Early stop i",0
"</div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4."">Experiments</head><p>For the experiments, we follow the protocol used in previous papers <ref type=""bibr"" target=""#b19"">[20,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b14"">15]</ref>, which uses unseen classes f ut FANNG <ref type=""bibr"" target=""#b4"">[5]</ref>), (2) lifted structured embedding <ref type=""bibr"" target=""#b20"">[21]</ref>, (3) N-pairs metric loss <ref type=""bibr"" target=""#b19"">[20]</ref>, (4) clustering <ref type=""bibr"" target=""#b14"">[15]</ref>, and (5) triplet combined with global loss <ref ty",1
"/ref> is at the core of many recently proposed computer vision methods <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b29"">30]</ref>. The main advantage of such m s/1.0""><head n=""4."">Experiments</head><p>For the experiments, we follow the protocol used in previous papers <ref type=""bibr"" target=""#b19"">[20,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b14"">15]</ref>, which uses unseen classes from the CUB-200-2011 <ref type=""bibr"" tar samples per class (in [10<ref type=""foot"" target=""#foot_0"">1</ref> , 10 2 ]), where the implementation of traditional classifiers becomes challenging <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b14"">15]</ref>.</p><p>Arguably, the most explored deep learning model that can esti ning set. An efficient computation of the full matrix of pairwise distances of a subset of the training set (i.e., the mini-batch) allows Song et al. <ref type=""bibr"" target=""#b20"">[21]</ref> to design of a new loss function that integrates all positive and negative samples to form a lifted structur ve mining <ref type=""bibr"" target=""#b15"">[16]</ref> (with and without FANNG <ref type=""bibr"" target=""#b4"">[5]</ref>), (2) lifted structured embedding <ref type=""bibr"" target=""#b20"">[21]</ref>, (3) N-pairs metric loss <ref type=""bibr"" target=""#b19"">[20]</ref>, (4) clustering <ref type=""bibr"" target="" or the remaining approaches (i.e. our proposed method, and ( <ref type=""formula"">5</ref>) ), we use the same training and test set split described in <ref type=""bibr"" target=""#b20"">[21]</ref> across all datasets. Specifically, the means CUB200-2011 <ref type=""bibr"" target=""#b24"">[25]</ref> has 11, 7 twork with pre-trained GoogLeNet <ref type=""bibr"" target=""#b22"">[23]</ref> weights and randomly initialize the final fully connected layer similar to <ref type=""bibr"" target=""#b20"">[21]</ref>. We set the embedding size to 64 <ref type=""bibr"" target=""#b20"">[21]</ref> and the learning rate for the ran weights and randomly initialize the final fully connected layer similar to <ref type=""bibr"" target=""#b20"">[21]</ref>. We set the embedding size to 64 <ref type=""bibr"" target=""#b20"">[21]</ref> and the learning rate for the randomly initialized fully connected layer is multiplied by 10 to achieve fast b20"">[21]</ref> and the learning rate for the randomly initialized fully connected layer is multiplied by 10 to achieve faster convergence similar to <ref type=""bibr"" target=""#b20"">[21]</ref>.</p><p>For the experiments using triplet combined with global loss <ref type=""bibr"" target=""#b9"">[10]</ref> etwork with pretrained GoogleNet weights <ref type=""bibr"" target=""#b22"">[23]</ref> and randomly initialize the final fully connected layer similar to <ref type=""bibr"" target=""#b20"">[21]</ref> . The learning rate for the randomly initialized fully connected layer is multiplied by 10 to achieve faster ""#b20"">[21]</ref> . The learning rate for the randomly initialized fully connected layer is multiplied by 10 to achieve faster convergence similar to <ref type=""bibr"" target=""#b20"">[21]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""6.1."">Effect of Scaling Parameter κ on the Embe",1
"=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b29"">30]</ref>. The main advantage of such models lies in their ability to automatically learn metric spaces, where samples",0
"on of effective feature embedding <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b18""",0
"ed on triplet networks <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b25"">26]</ref>, which are an extension of the siamese network <ref type=""bibr"" target=""#b0"">[1]</ref>. Triplet networks are composed of three identical convolutional neural networks (ConvNets) that are trained us esents a normalisation function, and r l (.) denotes a non-linear activation function (e.g., ReLU <ref type=""bibr"" target=""#b13"">[14]</ref>). Also in <ref type=""bibr"" target=""#b0"">(1)</ref>, note that f l = [f l,1 , ..., f l,n l ] represents an array of n l pre-activation functions.</p></div> <div x",0
"""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" target=""#b28"">29,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b5"">6]</ref> is at the core of many recently proposed computer vision methods <ref type=""bibr"" target=""#b2"">[3,</ref><ref ty",0
"ype=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b14"" f the embedding space, and the approximate nearest neighbour search that forms the basis of our proposed method. As pointed out by Shrivastava et al. <ref type=""bibr"" target=""#b16"">[17]</ref>, hard negative and positive mining is a relabeling of the problem of bootstrappping <ref type=""bibr"" target=",0
",</ref><ref type=""bibr"" target=""#b5"">6]</ref> is at the core of many recently proposed computer vision methods <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b29"" ethod</head><p>We first describe the architecture of a triplet network <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b26"">27]</ref> and the loss function used in its training. Then, we describe the sam g/ns/1.0""><head n=""3.1."">Triplet Networks</head><p>The triplet network <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b26"">27]</ref> is trained with an input composed of an anchor point x i (from class mation is infeasible even for datasets of modest sizes (e.g., N = 10 5 ). This issue has lead to the implementation of importance sampling techniques <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b25"">26]</ref> that stochastically under-sa ere required). During the first two epochs, triplet mining was completely disabled to allow for batches comprised of only random triplets. Similar to <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b9"">10]</ref>, we set the margin for the triplet and global loss to 0.2 and 0.01 re select positive and negative samples, but they do not apply any type of importance sampling to select challenging samples. In FaceNet, Schroff et al. <ref type=""bibr"" target=""#b15"">[16]</ref> introduced a triplet training approach, where pairs of anchor and positive samples are randomly selected, an rg/ns/1.0""><head n=""3.2."">Smart Mining</head><p>As discussed in Sec. 2, semi-hard mining has proved an effective method for training triplet networks <ref type=""bibr"" target=""#b15"">[16]</ref> with the primary aim of finding sets of triplets that will continue to progress the training of the network. ptive controller) is compared with the following state-of-the-art deep metric learning approaches: (1) triplet learning with semihard negative mining <ref type=""bibr"" target=""#b15"">[16]</ref> (with and without FANNG <ref type=""bibr"" target=""#b4"">[5]</ref>), (2) lifted structured embedding <ref type= d Cars196 <ref type=""bibr"" target=""#b8"">[9]</ref>. From these tables, we can first see that Triplet + FANNG significantly improves upon the Semi-hard <ref type=""bibr"" target=""#b15"">[16]</ref> results with respect to all measures, and showing that the smart mining process using FANNGs is more effecti 5]</ref>. Our proposals are highlighted.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>Method</head><p>NMI R@1 R@2 R@4 R@8</p><p>Semi-hard <ref type=""bibr"" target=""#b15"">[16]</ref> 55   </p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.2."">Qualitative Results</head><p>Figures clustering and recall performances. Furthermore, it is worth noting that Triplet + FANNG performs substantially better than its counterpart Semi-hard <ref type=""bibr"" target=""#b15"">[16]</ref> with respect to the clustering and recall performances, thus highlighting the importance of the smart mining .</figDesc></figure> <figure xmlns=""http://www.tei-c.org/ns/1.0"" xml:id=""fig_3""><head>2 )</head><label>2</label><figDesc>For semi-hard mining, such as<ref type=""bibr"" target=""#b15"">[16]</ref>, algorithmic complexity is reduced by limiting triplet selection to a brute force search within each minibat random triplet</note></figure> <figure xmlns=""http://www.tei-c.org/ns/1.0"" type=""table"" xml:id=""tab_1""><head></head><label></label><figDesc>Semi-hard<ref type=""bibr"" target=""#b15"">[16]</ref> 53.35 51.54 63.78 73.52 82.41 Lifted Structure [21] 56.88 52.98 65.70 76.01 84.27 N-pairs [20] 57.79 53.90 6 semi-hard mining is instead commonly performed over the stochastic subset of samples used in each minibatch <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b15"">16]</ref>. This method has the additional advantage of avoiding repeated attempts at learning from the hardest triplets",0
"ref type=""bibr"" target=""#b25"">26]</ref> computed using training samples acquired from a smart sampling method that has low computational com- plexity <ref type=""bibr"" target=""#b4"">[5]</ref> and can find effective training samples that produce gradients with large magnitude (see Fig. <ref type=""figur in samples from at least two difference classes (due to training with very few samples per class). A FANNG (Fast Approximate Nearest Neighbour Graph) <ref type=""bibr"" target=""#b4"">[5]</ref> is a graph based index that can find these neighbourhoods quickly and with a very high rate of recall. Additio ng epoch we perform a full forward pass of the training set T to generate the current feature embedding f (x, θ f ). The indexing graph used in FANNG <ref type=""bibr"" target=""#b4"">[5]</ref> is then constructed using the traverse-add algorithm (Alg. 4 in <ref type=""bibr"" target=""#b4"">[5]</ref>) with f (x, θ f ). The indexing graph used in FANNG <ref type=""bibr"" target=""#b4"">[5]</ref> is then constructed using the traverse-add algorithm (Alg. 4 in <ref type=""bibr"" target=""#b4"">[5]</ref>) with the embedding of each element of T forming a vertex in the graph. At each vertex, a list of outbound edg The newly formed traversable graph enables the computationally efficient collection of the approximate nearest neighbour set S.</p><p>As described in <ref type=""bibr"" target=""#b4"">[5]</ref>, the traverse-add algorithm can be repeatedly applied until a specified percentage success rate is reached. On centage of 98% is achieved, our approach diverges from the original building process for FANNGs. Rather than applying the backtrack search (Alg. 3 in <ref type=""bibr"" target=""#b4"">[5]</ref>) to further refine the graph, we instead use the same backtrack search algorithm to immediately generate the a uality and k nearest neighbour retrieval <ref type=""bibr"" target=""#b7"">[8]</ref>. Our proposed method combining triplet and global losses using FANNG <ref type=""bibr"" target=""#b4"">[5]</ref> with and without automated hyper-parameter selection (i.e., the adaptive controller) is compared with the foll deep metric learning approaches: (1) triplet learning with semihard negative mining <ref type=""bibr"" target=""#b15"">[16]</ref> (with and without FANNG <ref type=""bibr"" target=""#b4"">[5]</ref>), (2) lifted structured embedding <ref type=""bibr"" target=""#b20"">[21]</ref>, (3) N-pairs metric loss <ref type",0
"""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" target=""#b28"">29,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b5"">6]</ref> is at the core of many recently proposed computer vision methods <ref t",0
"e. In large part, these advances have been driven by powerful baseline systems, such as the Fast/Faster R-CNN <ref type=""bibr"" target=""#b14"">[9,</ref><ref type=""bibr"" target=""#b34"">29]</ref> and Fully Convolutional Network (FCN) <ref type=""bibr"" target=""#b29"">[24]</ref> frameworks for object detecti stack of four consecutive convs. <ref type=""bibr"" target=""#b14"">[9]</ref>. N is 64 for the C4 backbone (as in <ref type=""bibr"" target=""#b14"">[9,</ref><ref type=""bibr"" target=""#b34"">29]</ref>) and 512 for FPN (as in <ref type=""bibr"" target=""#b27"">[22]</ref>). We train on 8 GPUs (so effective minibatc mple, flexible, and fast system can surpass prior state-of-the-art instance segmentation results. Our method, called Mask R-CNN, extends Faster R-CNN <ref type=""bibr"" target=""#b34"">[29]</ref> by adding a branch for predicting segmentation masks on each Region of Interest (RoI), in parallel with the type=""bibr"" target=""#b14"">9]</ref> to allow attending to RoIs on feature maps using RoIPool, leading to fast speed and better accuracy. Faster R-CNN <ref type=""bibr"" target=""#b34"">[29]</ref> advanced this stream by learning the attention mechanism with a Region Proposal Network (RPN). Faster R-CNN to-pixel alignment, which is the main missing piece of Fast/Faster R-CNN.</p><p>Faster R-CNN: We begin by briefly reviewing the Faster R-CNN detector <ref type=""bibr"" target=""#b34"">[29]</ref>. Faster R-CNN consists of two stages. The first stage, called a Region Proposal Network (RPN), proposes cand nd Mask R-CNN have the same backbones and so they are shareable.</p><p>Inference: At test time, the proposal number is 300 for the C4 backbone (as in <ref type=""bibr"" target=""#b34"">[29]</ref>) and 1000 for FPN (as in <ref type=""bibr"" target=""#b27"">[22]</ref>). We run the box prediction branch on the >Inference: We train a ResNet-101-FPN model that shares features between the RPN and Mask R-CNN stages, following the 4-step training of Faster R-CNN <ref type=""bibr"" target=""#b34"">[29]</ref>. This model runs at 195ms per image on an Nvidia Tesla M40 GPU (plus 15ms CPU time resizing the outputs to t ad n=""3.1."">Implementation Details</head><p>We set hyper-parameters following existing Fast/Faster R-CNN work <ref type=""bibr"" target=""#b14"">[9,</ref><ref type=""bibr"" target=""#b34"">29,</ref><ref type=""bibr"" target=""#b27"">22]</ref>. Although these decisions were made for object detection in original pe=""bibr"" target=""#b27"">22]</ref>. Although these decisions were made for object detection in original papers <ref type=""bibr"" target=""#b14"">[9,</ref><ref type=""bibr"" target=""#b34"">29,</ref><ref type=""bibr"" target=""#b27"">22]</ref>, we found our instance segmentation system is robust to them.</p><p>T",1
"e future research.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2."">Related Work R-CNN:</head><p>The Region-based CNN (R-CNN) approach <ref type=""bibr"" target=""#b15"">[10]</ref> to bounding-box object detection is to attend to a manageable number of candidate object regions <ref type="" that applies bounding-box classification and regression in parallel (which turned out to largely simplify the multi-stage pipeline of original R-CNN <ref type=""bibr"" target=""#b15"">[10]</ref>).</p><p>Formally, during training, we define a multi-task loss on each sampled RoI as L = L cls + L box + L Instance Segmentation: Driven by the effectiveness of R-CNN, many approaches to instance segmentation are based on segment proposals. Earlier methods <ref type=""bibr"" target=""#b15"">[10,</ref><ref type=""bibr"" target=""#b17"">12,</ref><ref type=""bibr"" target=""#b18"">13,</ref><ref type=""bibr"" target=""#b11",0
"=""bibr"" target=""#b38"">[33,</ref><ref type=""bibr"" target=""#b21"">16]</ref> and evaluate convolutional networks <ref type=""bibr"" target=""#b25"">[20,</ref><ref type=""bibr"" target=""#b24"">19]</ref> independently on each RoI. R-CNN was extended <ref type=""bibr"" target=""#b19"">[14,</ref><ref type=""bibr"" targe",0
"r R-CNN is flexible and robust to many follow-up improvements (e.g., <ref type=""bibr"" target=""#b35"">[30,</ref><ref type=""bibr"" target=""#b27"">22,</ref><ref type=""bibr"" target=""#b22"">17]</ref>), and is the current leading framework in several benchmarks.</p><p>Instance Segmentation: Driven by the effe box and performs classification and bounding-box regression. The features used by both stages can be shared for faster inference. We refer readers to <ref type=""bibr"" target=""#b22"">[17]</ref> for latest, comprehensive comparisons between Faster R-CNN and other frameworks.</p><p>Mask R-CNN: Mask R-CN ef type=""foot"" target=""#foot_2"">3</ref> . We note that not all frameworks automatically benefit from deeper or advanced networks (see benchmarking in <ref type=""bibr"" target=""#b22"">[17]</ref>).    Multinomial vs. Independent Masks: Mask R-CNN decouples mask and class prediction: as the existing box d). Mask R-CNN using ResNet-101-FPN outperforms the base variants of all previous state-ofthe-art models, including the single-model variant of G-RMI <ref type=""bibr"" target=""#b22"">[17]</ref>, the winner of the COCO 2016 Detection Challenge. Using ResNeXt-101-FPN, Mask R-CNN further improves results ractice.</p><p>Although Mask R-CNN is fast, we note that our design is not optimized for speed, and better speed/accuracy tradeoffs could be achieved <ref type=""bibr"" target=""#b22"">[17]</ref>, e.g., by varying image sizes and proposal numbers, which is beyond the scope of this paper.</p><p>Training: example, is denoted by ResNet-50-C4. This is a common choice used in <ref type=""bibr"" target=""#b20"">[15,</ref><ref type=""bibr"" target=""#b12"">7,</ref><ref type=""bibr"" target=""#b22"">17,</ref><ref type=""bibr"" target=""#b36"">31]</ref>.</p><p>We also explore another more effective backbone recently propo",0
"f>. However, recent research showed that an attacker could generate adversarial examples to fool classifiers <ref type=""bibr"" target=""#b33"">[34,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b18"">19]</ref>. Their algorithms perturbed ben e specifically, researchers showed that it was possible to generate adversarial examples to fool classifiers <ref type=""bibr"" target=""#b33"">[34,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b18"">19]</ref>. Their algorithms perturbed nor target=""#b0"">(1)</ref> Training the target classifier with adversarial examples, called adversarial training <ref type=""bibr"" target=""#b33"">[34,</ref><ref type=""bibr"" target=""#b4"">5]</ref>; <ref type=""bibr"" target=""#b1"">(2)</ref> Training a classifier to distinguish between normal and adversarial ex st of which leveraged gradient based optimization from normal examples <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b33"">34,</ref><ref type=""bibr"" target=""#b4"">5]</ref>. Moosavi et al. showed that it was even possible to find one effective universal adversarial perturbation that, <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2.3.1"">Fast gradient sign method(FGSM). Given a normal image</head><p>x, fast gradient sign method <ref type=""bibr"" target=""#b4"">[5]</ref> looks for a similar image x ′ in the L ∞ neighborhood of x that fools the classifier. It defines a loss functi "" target=""#b33"">[34,</ref><ref type=""bibr"" target=""#b21"">22]</ref>, or mix the adversarial objective with the classification objective as regularizer <ref type=""bibr"" target=""#b4"">[5]</ref>. Though this idea is promising, it is hard to reason about what attacks to train on and how important the adve",1
"g <ref type=""bibr"" target=""#b15"">[16]</ref>. However, recent research showed that an attacker could generate adversarial examples to fool classifiers <ref type=""bibr"" target=""#b33"">[34,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b18""> ollow three approaches: <ref type=""bibr"" target=""#b0"">(1)</ref> Training the target classifier with adversarial examples, called adversarial training <ref type=""bibr"" target=""#b33"">[34,</ref><ref type=""bibr"" target=""#b4"">5]</ref>; <ref type=""bibr"" target=""#b1"">(2)</ref> Training a classifier to dist ef type=""bibr"" target=""#b7"">8]</ref>. More specifically, researchers showed that it was possible to generate adversarial examples to fool classifiers <ref type=""bibr"" target=""#b33"">[34,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b18""> we refer to as adversarial training. For example, one may use a mixture of normal and adversarial examples in the training set for data augmentation <ref type=""bibr"" target=""#b33"">[34,</ref><ref type=""bibr"" target=""#b21"">22]</ref>, or mix the adversarial objective with the classification objective > <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2.3"">Existing attacks</head><p>Since the discovery of adversarial examples for neural networks in <ref type=""bibr"" target=""#b33"">[34]</ref>, researchers have found adversarial examples on various network architectures. For example, feedforward conv for generating adversarial examples, most of which leveraged gradient based optimization from normal examples <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b33"">34,</ref><ref type=""bibr"" target=""#b4"">5]</ref>. Moosavi et al. showed that it was even possible to find one effective",1
"monstrated impressive performance on many tasks, such as image classification <ref type=""bibr"" target=""#b8"">[9]</ref> and natural language processing <ref type=""bibr"" target=""#b15"">[16]</ref>. However, recent research showed that an attacker could generate adversarial examples to fool classifiers <r",0
"et classifier with adversarial examples, called adversarial training <ref type=""bibr"" target=""#b33"">[34,</ref><ref type=""bibr"" target=""#b4"">5]</ref>; <ref type=""bibr"" target=""#b1"">(2)</ref> Training a classifier to distinguish between normal and adversarial examples <ref type=""bibr"" target=""#b19"">[2 s. For (3), <ref type=""bibr"">Carlini et al.</ref> showed that defensive distillation did not significantly increase the robustness of neural networks <ref type=""bibr"" target=""#b1"">[2]</ref>. Moreover, this approach requires changing and retraining the target classifier, which adds engineering comple model human perception, researchers proposed three popular metrics to approximate human's perception of visual difference, namely L 0 , L 2 , and L ∞ <ref type=""bibr"" target=""#b1"">[2]</ref>. These metrics are special cases of the L p norm:</p><formula xml:id=""formula_0"">∥x ∥ p = n i=1 |x i | p 1 p</ ]</ref>, researchers have found adversarial examples on various network architectures. For example, feedforward convolutional classification networks <ref type=""bibr"" target=""#b1"">[2]</ref>, generative networks <ref type=""bibr"" target=""#b13"">[14]</ref>, and recurrent networks <ref type=""bibr"" target 0""><head n=""2.3.4"">Carlini attack.</head><p>Carlini recently introduced a powerful attack that generates adversarial examples with small perturbation <ref type=""bibr"" target=""#b1"">[2]</ref>. The attack can be targeted or untargeted for all three metrics L 0 , L 2 , and L ∞ . We take the untargeted L ining techniques <ref type=""bibr"" target=""#b9"">[10]</ref> and hides the gradient between the pre-softmax layer (logits) and softmax outputs. However, <ref type=""bibr"" target=""#b1"">[2]</ref> showed that it is easy to bypass the defense by adopting one of the three following strategies: (1) choose a m s all the parameters of the classifier, it would be difficult for her to find adversarial example <ref type=""bibr"" target=""#b24"">[25]</ref>. However, <ref type=""bibr"" target=""#b1"">[2]</ref> showed that it was actually easy to find adversarial examples for the classifier hardened in <ref type=""bibr"" selected 55 000 examples for the training set, 5 000 for the validation set, and 1 000 for the test set. We trained a classifier using the setting in <ref type=""bibr"" target=""#b1"">[2]</ref> and got an accuracy of 99.4%. On CIFAR-10, we selected 45 000 examples for training set, 5 000 for the validat se study on Carlini attack, why does MagNet work?</head><p>Carlini showed that it was viable to mount transfer attack with higher confidence on MNIST <ref type=""bibr"" target=""#b1"">[2]</ref>. Among the attacks that we evaluated, Carlini's attack is the most interesting because it is the most effectiv <p>We evaluated the impact of different confidence levels in Carlini's attack on MagNet. For MNIST, we used the same classifier as in Carlini's paper <ref type=""bibr"" target=""#b1"">[2]</ref> for generating adversarial examples and as the target classifier in our evaluation. We generated adversarial e MagNet contains at least two networks, a reformer and one (or more) detector, that make independent decisions. Therefore, the attack as described in <ref type=""bibr"" target=""#b1"">[2]</ref> cannot handle MagNet. To overcome this obstacle, we removed the detectors from MagNet and kept only the reform parameters. In this setting, we evaluated MagNet on popular attacks <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b1"">2]</ref>. On the MNIST dataset, Mag-Net achieved more than 99% classification accuracy on adversarial examples generated eted attack is easier to succeed, results in smaller perturbations, and transfers better to different models <ref type=""bibr"" target=""#b18"">[19,</ref><ref type=""bibr"" target=""#b1"">2]</ref>. Since untargeted attack is more difficult to defend against, we evaluate MagNet on untargeted attack to show i get=""#b25"">[26]</ref>. For DeepFool and Carlini's attack, we used their authors' open source implementations <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b1"">2]</ref>. In principle, MagNet works better when we deploy several instances of both reconstruction error based detector ecurity perspective.</p><p>Recent work has demonstrated the feasibility of attacking such systems with carefully crafted input for real-world systems <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b7"">8]</ref>. More specifically, researchers . Researchers developed several methods for generating adversarial examples, most of which leveraged gradient based optimization from normal examples <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b33"">34,</ref><ref type=""bibr"" target=""#b4"">5]</ref>. Moosavi et al. showed that it w",0
"ref type=""bibr"" target=""#b19"">[20]</ref>; and (3) Making target classifiers hard to attack by blocking gradient pathway, e.g., defensive distillation <ref type=""bibr"" target=""#b24"">[25]</ref>.</p><p>However, all these approaches have limitations. Both (1) and (2) require adversarial examples to trai is independent from the target classifier, and is therefore faster and more flexible. MagNet may benefit from a robust target classifier (section 5). <ref type=""bibr"" target=""#b24"">[25]</ref> trains the classifier in a certain way such that it is nearly impossible for gradient based attacks to gener obust classifier such that even if the attacker knows all the parameters of the classifier, it would be difficult for her to find adversarial example <ref type=""bibr"" target=""#b24"">[25]</ref>. However, <ref type=""bibr"" target=""#b1"">[2]</ref> showed that it was actually easy to find adversarial examp </ref>. However, <ref type=""bibr"" target=""#b1"">[2]</ref> showed that it was actually easy to find adversarial examples for the classifier hardened in <ref type=""bibr"" target=""#b24"">[25]</ref>. We do not know how to find such robust classifiers, or even if they exist.</p><p>We take a different approa [2]</ref>. Among the attacks that we evaluated, Carlini's attack is the most interesting because it is the most effective on the distillation defense <ref type=""bibr"" target=""#b24"">[25]</ref> and there is no known effective defense prior to our work. This attack is also interesting because the attac",0
"</p><p>Researchers speculate that for many AI tasks, their relevant data lie on a manifold that is of much lower dimension than the full sample space <ref type=""bibr"" target=""#b22"">[23]</ref>. This suggests that the normal examples for a classification task are on a manifold, and adversarial example bability p(x). x occurs naturally if p(x) is non-negligible. Researchers believe that N t constitute a manifold that is of much lower dimension than S<ref type=""bibr"" target=""#b22"">[23]</ref>. Since we do not know the data generation process, we approximate N t by the union of natural datasets for t",0
"or gradient based attacks to generate adversarial examples directly on the network. Defensive distillation leverages distillation training techniques <ref type=""bibr"" target=""#b9"">[10]</ref> and hides the gradient between the pre-softmax layer (logits) and softmax outputs. However, <ref type=""bibr""",0
"type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b28"">29]</ref>, and human-computer interaction <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b12"">13]</ref>. These security-critical domains require better understanding of neural networks from the security perspectiv",0
"odern world. They are used in autonomous control for robots and vehicles <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b3"">4]</ref>, financial systems <ref type=""bibr"" target=""#b31"">[32]</ref>, medical treatments <ref type=""bibr"" target=""#b30""",0
"ref>, financial systems <ref type=""bibr"" target=""#b31"">[32]</ref>, medical treatments <ref type=""bibr"" target=""#b30"">[31]</ref>, information security <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b28"">29]</ref>, and human-computer interaction <ref type=""bibr"" target=""#b10"">[11,<",0
"al network accuracy <ref type=""bibr"" target=""#b41"">(Simonyan &amp; Zisserman, 2015;</ref><ref type=""bibr"" target=""#b44"">Srivastava et al., 2015;</ref><ref type=""bibr"" target=""#b18"">He et al., 2016;</ref><ref type=""bibr"" target=""#b21"">Huang et al., 2016;</ref><ref type=""bibr"" target=""#b4"">2017)</ref> get=""#fig_1"">1</ref>, which compares a 5-layer LeNet (left) <ref type=""bibr"" target=""#b34"">(LeCun et al., 1998)</ref> with a 110-layer ResNet (right) <ref type=""bibr"" target=""#b18"">(He et al., 2016)</ref> on the CIFAR-100 dataset. The top row shows the distribution of prediction confidence (i.e. pro on. den layers. Recent research suggests that these normalization techniques have enabled the development of very deep architectures, such as ResNets <ref type=""bibr"" target=""#b18"">(He et al., 2016)</ref> and DenseNets <ref type=""bibr"" target=""#b22"">(Huang et al., 2017)</ref>. It has been shown that l networks has increased at a dramatic pace over the past few years.</p><p>It is now common to see networks with hundreds, if not thousands of layers <ref type=""bibr"" target=""#b18"">(He et al., 2016;</ref><ref type=""bibr"" target=""#b21"">Huang et al., 2016)</ref> and hundreds of convolutional filters p weight decay, if any at all. The top performing ImageNet models of 2015 all use an order of magnitude less weight decay than models of previous years <ref type=""bibr"" target=""#b18"">(He et al., 2016;</ref><ref type=""bibr"" target=""#b41"">Simonyan &amp; Zisserman, 2015)</ref>.</p><p>We find that trainin",1
"ning modern neural networks. Learning theory suggests that regularization is necessary to prevent overfitting, especially as model capacity increases <ref type=""bibr"" target=""#b46"">(Vapnik, 1998)</ref>. However, due to the apparent regularization effects of Batch Normalization, recent research seems",0
"or perfectly calibrated classifiers, MCE and ECE both equal 0.</p><p>Negative log likelihood is a standard measure of a probabilistic model's quality <ref type=""bibr"" target=""#b15"">(Friedman et al., 2001)</ref>. It is also referred to as the cross entropy loss in the context of deep learning <ref ty and n samples, NLL is defined as:</p><formula xml:id=""formula_7"">L = − n i=1 log(π(y i |x i ))<label>(6)</label></formula><p>It is a standard result <ref type=""bibr"" target=""#b15"">(Friedman et al., 2001)</ref> that, in expectation, NLL is minimized if and only if π(Y |X) recovers the ground truth c",0
"ork shows that very deep or wide models are able to generalize better than smaller ones, while exhibiting the capacity to easily fit the training set <ref type=""bibr"" target=""#b52"">(Zhang et al., 2017)</ref>.</p><p>Although increasing depth and width may reduce classification error, we observe that at the expense of well-modeled probabilities.</p><p>We can connect this finding to recent work examining the generalization of large neural networks. <ref type=""bibr"" target=""#b52"">Zhang et al. (2017)</ref> observe that deep neural networks seemingly violate the common understanding of learning theo",0
"etworks (RNNs) both read and freely generate text, has made abstractive summarization viable <ref type=""bibr"" target=""#b3"">(Chopra et al., 2016;</ref><ref type=""bibr"" target=""#b17"">Nallapati et al., 2016;</ref><ref type=""bibr"" target=""#b20"">Rush et al., 2015;</ref><ref type=""bibr"" target=""#b28"">Zeng et=""#b6"">(Gu et al., 2016;</ref><ref type=""bibr"" target=""#b7"">Gulcehre et al., 2016;</ref><ref type=""bibr"" target=""#b15"">Miao and Blunsom, 2016;</ref><ref type=""bibr"" target=""#b17"">Nallapati et al., 2016;</ref><ref type=""bibr"" target=""#b28"">Zeng et al., 2016)</ref>.</p><p>Our approach is close to th ore useful. Therefore we apply our model to the recently-introduced CNN/ Daily Mail dataset <ref type=""bibr"" target=""#b8"">(Hermann et al., 2015;</ref><ref type=""bibr"" target=""#b17"">Nallapati et al., 2016)</ref>, which contains news articles (39 sentences on average) paired with multi-sentence summar http://www.tei-c.org/ns/1.0""><head n=""4"">Dataset</head><p>We use the CNN/Daily Mail dataset <ref type=""bibr"" target=""#b8"">(Hermann et al., 2015;</ref><ref type=""bibr"" target=""#b17"">Nallapati et al., 2016)</ref>, which contains online news articles (781 tokens on average) paired with multi-sentence s /div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2.1"">Sequence-to-sequence attentional model</head><p>Our baseline model is similar to that of <ref type=""bibr"" target=""#b17"">Nallapati et al. (2016)</ref>, and is depicted in Figure <ref type=""figure"" target=""#fig_0"">2</ref>. The tokens of the et al., 2016)</ref>, further improving performance on those datasets.</p><p>However, large-scale datasets for summarization of longer text are rare. <ref type=""bibr"" target=""#b17"">Nallapati et al. (2016)</ref> adapted the DeepMind question-answering dataset <ref type=""bibr"" target=""#b8"">(Hermann et es of it in the source text.</p><p>Our approach is considerably different from that of <ref type=""bibr"" target=""#b7"">Gulcehre et al. (2016)</ref> and <ref type=""bibr"" target=""#b17"">Nallapati et al. (2016)</ref>. Those works train their pointer components to activate only for out-of-vocabulary words line news articles (781 tokens on average) paired with multi-sentence summaries (3.75 sentences or 56 tokens on average). We used scripts supplied by <ref type=""bibr"" target=""#b17"">Nallapati et al. (2016)</ref> to obtain the same version of the the data, which has 287,226 training pairs, 13,368 vali tor adds 1153 extra parameters (w h * , w s , w x and b ptr in equation 8), and coverage adds 512 extra parameters (w c in equation 11).</p><p>Unlike <ref type=""bibr"" target=""#b17"">Nallapati et al. (2016)</ref>, we do not pretrain the word embeddings -they are learned from scratch during training. W ss, given that the disparity in the lead-3 scores is (+1.1 ROUGE-1, +2.0 ROUGE-2, +1.1 ROUGE-L) points respectively, and our best model scores exceed <ref type=""bibr"" target=""#b17"">Nallapati et al. (2016)</ref> by (+4.07 ROUGE-1, +3.98 ROUGE-2, +3.73 ROUGE-L) points, we may estimate that we outperfo ancies in the reports of the first officers on the scene. (...) Summary: more questions than answers emerge in controversial s.c. police shooting. of <ref type=""bibr"" target=""#b17"">Nallapati et al. (2016)</ref> by several ROUGE points. Despite the brevity of the coverage training phase (about 1% of ""#b3"">(Chopra et al., 2016)</ref>, Abstract Meaning Representations <ref type=""bibr"" target=""#b24"">(Takase et al., 2016)</ref>, hierarchical networks <ref type=""bibr"" target=""#b17"">(Nallapati et al., 2016)</ref>, variational autoencoders <ref type=""bibr"" target=""#b15"">(Miao and Blunsom, 2016)</ref>, r text.</p><p>Temporal attention is a related technique that has been applied to <ref type=""bibr"">NMT (Sankaran et al., 2016)</ref> and summarization <ref type=""bibr"" target=""#b17"">(Nallapati et al., 2016)</ref>. In this approach, each attention distribution is divided by the sum of the previous, wh s our ROUGE scores (see Table <ref type=""table"" target=""#tab_2"">1</ref>), compared to the smaller boost given by temporal attention for the same task <ref type=""bibr"" target=""#b17"">(Nallapati et al., 2016)</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4"">Dataset</head><p>We use , we also report the lead-3 baseline (which uses the first three sentences of the article as a summary), and compare to the only existing abstractive <ref type=""bibr"" target=""#b17"">(Nallapati et al., 2016)</ref> and extractive <ref type=""bibr"" target=""#b16"">(Nallapati et al., 2017)</ref> models on t e same version of the the data, which has 287,226 training pairs, 13,368 validation pairs and 11,490 test pairs. Both the dataset's published results <ref type=""bibr"" target=""#b17"">(Nallapati et al., 2016</ref><ref type=""bibr"" target=""#b16"">(Nallapati et al., , 2017) )</ref> use the anonymized versi full dataset. The output of our models is available online. <ref type=""foot"" target=""#foot_5"">6</ref>Given that we generate plain-text summaries but <ref type=""bibr"" target=""#b17"">Nallapati et al. (2016;</ref><ref type=""bibr"" target=""#b16"">2017)</ref> generate anonymized summaries (see Section 4),",1
"(2016)</ref> Forced-Attention Sentence Compression, that were applied to short-text summarization. We propose a novel variant of the coverage vector <ref type=""bibr"" target=""#b25"">(Tu et al., 2016)</ref> from Neural Machine Translation, which we use to track and control coverage of the source docum div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2.3"">Coverage mechanism</head><p>Repetition is a common problem for sequenceto-sequence models <ref type=""bibr"" target=""#b25"">(Tu et al., 2016;</ref><ref type=""bibr"" target=""#b14"">Mi et al., 2016;</ref><ref type=""bibr"">Sankaran et al., 2016;</re 2016)</ref>, and is especially pronounced when generating multi-sentence text (see Figure <ref type=""figure"">1</ref>). We adapt the coverage model of <ref type=""bibr"" target=""#b25"">Tu et al. (2016)</ref> to solve the problem. In our coverage model, we maintain a coverage vector c t , which is the su </p><p>Coverage. Originating from Statistical Machine Translation <ref type=""bibr"" target=""#b10"">(Koehn, 2009)</ref>, coverage was adapted for NMT by <ref type=""bibr"" target=""#b25"">Tu et al. (2016)</ref> and <ref type=""bibr"" target=""#b14"">Mi et al. (2016)</ref>, who both use a GRU to update the cove",1
"abstractive system by at least 2 ROUGE points.</p><p>Our hybrid pointer-generator network facilitates copying words from the source text via pointing <ref type=""bibr"" target=""#b26"">(Vinyals et al., 2015)</ref>, which improves accuracy and handling of OOV words, while retaining the ability to generat i-c.org/ns/1.0""><head n=""2.2"">Pointer-generator network</head><p>Our pointer-generator network is a hybrid between our baseline and a pointer network <ref type=""bibr"" target=""#b26"">(Vinyals et al., 2015)</ref>, as it allows both copying words via pointing, and generating words from a fixed vocabular to create summaries, and Cheung and Penn (2014) explore sentence fusion using dependency trees.</p><p>Pointer-generator networks. The pointer network <ref type=""bibr"" target=""#b26"">(Vinyals et al., 2015)</ref> is a sequence-tosequence model that uses the soft attention distribution of <ref type=""bib",1
"e evaluated). We obtain our ROUGE scores using the pyrouge package. <ref type=""foot"" target=""#foot_3"">4</ref> We also evaluate with the METEOR metric <ref type=""bibr"" target=""#b4"">(Denkowski and Lavie, 2014)</ref>, both in exact match mode (rewarding only exact matches between words) and full mode (",0
"type=""figure"" target=""#fig_2"">5</ref>).</p><p>Due to the difficulty of abstractive summarization, the great majority of past work has been extractive <ref type=""bibr"" target=""#b11"">(Kupiec et al., 1995;</ref><ref type=""bibr"" target=""#b18"">Paice, 1990;</ref><ref type=""bibr"" target=""#b21"">Saggion and",0
"summary; at test time it is the previous word emitted by the decoder), and has decoder state s t . The attention distribution a t is calculated as in <ref type=""bibr"" target=""#b0"">Bahdanau et al. (2015)</ref>:</p><formula xml:id=""formula_0"">e t i = v T tanh(W h h i +W s s t + b attn )<label>(1)</lab inter network <ref type=""bibr"" target=""#b26"">(Vinyals et al., 2015)</ref> is a sequence-tosequence model that uses the soft attention distribution of <ref type=""bibr"" target=""#b0"">Bahdanau et al. (2015)</ref> to produce an output sequence consisting of elements from the input sequence. The pointer n",0
", resulting in the CNN/Daily Mail dataset, and provided the first abstractive baselines. The same authors then published a neural extractive approach <ref type=""bibr"" target=""#b16"">(Nallapati et al., 2017)</ref>, which uses hierarchical RNNs to select sentences, and found that it significantly outpe the article as a summary), and compare to the only existing abstractive <ref type=""bibr"" target=""#b17"">(Nallapati et al., 2016)</ref> and extractive <ref type=""bibr"" target=""#b16"">(Nallapati et al., 2017)</ref> models on the full dataset. The output of our models is available online. <ref type=""foo rget=""#fig_1"">4</ref>). However, our best model does not quite surpass the ROUGE scores of the lead-3 baseline, nor the current best extractive model <ref type=""bibr"" target=""#b16"">(Nallapati et al., 2017)</ref>. We discuss this issue in section 7.1.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0 airs, 13,368 validation pairs and 11,490 test pairs. Both the dataset's published results <ref type=""bibr"" target=""#b17"">(Nallapati et al., 2016</ref><ref type=""bibr"" target=""#b16"">(Nallapati et al., , 2017) )</ref> use the anonymized version of the data, which has been pre-processed to replace each <ref type=""foot"" target=""#foot_5"">6</ref>Given that we generate plain-text summaries but <ref type=""bibr"" target=""#b17"">Nallapati et al. (2016;</ref><ref type=""bibr"" target=""#b16"">2017)</ref> generate anonymized summaries (see Section 4), our ROUGE scores are not strictly comparable. There is evide",0
", which can be viewed as a balance between extractive and abstractive approaches, is similar to <ref type=""bibr"">Gu et al.'s (2016)</ref> CopyNet and <ref type=""bibr"" target=""#b15"">Miao and Blunsom's (2016)</ref> Forced-Attention Sentence Compression, that were applied to short-text summarization. W get=""#b24"">(Takase et al., 2016)</ref>, hierarchical networks <ref type=""bibr"" target=""#b17"">(Nallapati et al., 2016)</ref>, variational autoencoders <ref type=""bibr"" target=""#b15"">(Miao and Blunsom, 2016)</ref>, and direct optimization of the performance metric <ref type=""bibr"" target=""#b19"">(Ranza et al., 2016)</ref>, and summarization <ref type=""bibr"" target=""#b6"">(Gu et al., 2016;</ref><ref type=""bibr"" target=""#b7"">Gulcehre et al., 2016;</ref><ref type=""bibr"" target=""#b15"">Miao and Blunsom, 2016;</ref><ref type=""bibr"" target=""#b17"">Nallapati et al., 2016;</ref><ref type=""bibr"" target=""#b28"" 2016;</ref><ref type=""bibr"" target=""#b28"">Zeng et al., 2016)</ref>.</p><p>Our approach is close to the Forced-Attention Sentence Compression model of <ref type=""bibr"" target=""#b15"">Miao and Blunsom (2016)</ref> and the CopyNet model of <ref type=""bibr"" target=""#b6"">Gu et al. (2016)</ref>, with some",0
"t=""#b17"">Nallapati et al. (2016)</ref>, we do not pretrain the word embeddings -they are learned from scratch during training. We train using Adagrad <ref type=""bibr"" target=""#b5"">(Duchi et al., 2011)</ref> with learning rate 0.15 and an initial accumulator value of 0.1. (This was found to work best",0
"reliminaries</head><p>Our results are given in Table <ref type=""table"" target=""#tab_2"">1</ref>. We evaluate our models with the standard ROUGE metric <ref type=""bibr"" target=""#b12"">(Lin, 2004b)</ref>, reporting the F 1 scores for ROUGE-1, ROUGE-2 and ROUGE-L (which respectively measure the word-over ROUGE is exacerbated by only having one reference summary, which has been shown to lower ROUGE's reliability compared to multiple reference summaries <ref type=""bibr"" target=""#b12"">(Lin, 2004a)</ref>.</p><p>Due to the subjectivity of the task and thus the diversity of valid summaries, it seems that",0
"of abstraction while avoiding repetition) and ultimately more useful. Therefore we apply our model to the recently-introduced CNN/ Daily Mail dataset <ref type=""bibr"" target=""#b8"">(Hermann et al., 2015;</ref><ref type=""bibr"" target=""#b17"">Nallapati et al., 2016)</ref>, which contains news articles ( 17"">(Nallapati et al., 2016)</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4"">Dataset</head><p>We use the CNN/Daily Mail dataset <ref type=""bibr"" target=""#b8"">(Hermann et al., 2015;</ref><ref type=""bibr"" target=""#b17"">Nallapati et al., 2016)</ref>, which contains online news art r summarization of longer text are rare. <ref type=""bibr"" target=""#b17"">Nallapati et al. (2016)</ref> adapted the DeepMind question-answering dataset <ref type=""bibr"" target=""#b8"">(Hermann et al., 2015)</ref> for summarization, resulting in the CNN/Daily Mail dataset, and provided the first abstract",0
"find that a simpler approach -summing the attention distributions to obtain the coverage vector -suffices. In this respect our approach is similar to <ref type=""bibr"" target=""#b27"">Xu et al. (2015)</ref>, who apply a coverage-like method to image cap-tioning, and <ref type=""bibr"" target=""#b1"">Chen e",0
"e language model and copy mechanism to work together to perform abstractive copying.</p><p>Coverage. Originating from Statistical Machine Translation <ref type=""bibr"" target=""#b10"">(Koehn, 2009)</ref>, coverage was adapted for NMT by <ref type=""bibr"" target=""#b25"">Tu et al. (2016)</ref> and <ref typ",0
"majority of past work has been extractive <ref type=""bibr"" target=""#b11"">(Kupiec et al., 1995;</ref><ref type=""bibr"" target=""#b18"">Paice, 1990;</ref><ref type=""bibr"" target=""#b21"">Saggion and Poibeau, 2013)</ref>. However, the recent success of sequence-to-sequence models (Sutskever   <ref type=""bi",0
"> to produce an output sequence consisting of elements from the input sequence. The pointer network has been used to create hybrid approaches for NMT <ref type=""bibr"" target=""#b7"">(Gulcehre et al., 2016)</ref>, language modeling <ref type=""bibr"" target=""#b13"">(Merity et al., 2016)</ref>, and summari language modeling <ref type=""bibr"" target=""#b13"">(Merity et al., 2016)</ref>, and summarization <ref type=""bibr"" target=""#b6"">(Gu et al., 2016;</ref><ref type=""bibr"" target=""#b7"">Gulcehre et al., 2016;</ref><ref type=""bibr"" target=""#b15"">Miao and Blunsom, 2016;</ref><ref type=""bibr"" target=""#b17"">N anism often copies a word while attending to multiple occurrences of it in the source text.</p><p>Our approach is considerably different from that of <ref type=""bibr"" target=""#b7"">Gulcehre et al. (2016)</ref> and <ref type=""bibr"" target=""#b17"">Nallapati et al. (2016)</ref>. Those works train their p",0
"ion, we develop an encoder model for entities in the relational graph and apply it to both tasks.</p><p>Our entity classification model, similarly to <ref type=""bibr"" target=""#b16"">Kipf and Welling (2017)</ref>, uses softmax classifiers at each node in the graph. The classifiers take node representa to be a (message-specific) neural network-like function or simply a linear transformation g m (h i , h j ) = W h j with a weight matrix W such as in <ref type=""bibr"" target=""#b16"">Kipf and Welling (2017)</ref>. This type of transformation has been shown to be very effective at accumulating and enco representation through a single linear transformation. While we only consider such a featureless approach in this work, we note that it was shown in <ref type=""bibr"" target=""#b16"">Kipf and Welling (2017)</ref> that it is possible for this class of models to make use of predefined feature vectors (e el is primarily motivated as an extension of GCNs that operate on local graph neighborhoods <ref type=""bibr"" target=""#b8"">(Duvenaud et al. 2015;</ref><ref type=""bibr"" target=""#b16"">Kipf and Welling 2017)</ref> to large-scale relational data. These and related methods such as graph neural networks <r . 2014;</ref><ref type=""bibr"" target=""#b8"">Duvenaud et al. 2015;</ref><ref type=""bibr"" target=""#b6"">Defferrard, Bresson, and Vandergheynst 2016;</ref><ref type=""bibr"" target=""#b16"">Kipf and Welling 2017)</ref> for large-scale and highly multi-relational data, characteristic of realistic knowledge ba mprovements in areas such as graph classification <ref type=""bibr"" target=""#b8"">(Duvenaud et al. 2015)</ref> and graph-based semi-supervised learning <ref type=""bibr"" target=""#b16"">(Kipf and Welling 2017)</ref>.</p><p>Motivated by these architectures, we define the following simple propagation model",1
"h dropout rate 0.2 for self-loops and 0.4 for other edges. Using edge droupout makes our training objective similar to that of denoising autoencoders <ref type=""bibr"" target=""#b28"">(Vincent et al. 2008)</ref>. We apply l2 regularization to the decoder with a penalty of 0.01.</p><p>We use the Adam op",0
"on and entity classification problems. To address the scalability of our method, it would be worthwhile to explore subsampling techniques, such as in <ref type=""bibr"" target=""#b11"">Hamilton, Ying, and Leskovec (2017)</ref>. Lastly, it would be promising to replace the current form of summation over",0
"rganize and store factual knowledge, enabling a multitude of applications including question answering <ref type=""bibr"">(Yao and Van Durme 2014;</ref><ref type=""bibr"" target=""#b0"">Bao et al. 2014;</ref><ref type=""bibr"" target=""#b24"">Seyler, Yahya, and Berberich 2015;</ref><ref type=""bibr"" target=""#b",0
"rely on any type of factorization (or generally any scoring function), we use one of the simplest and most effective factorization methods: DistMult <ref type=""bibr"" target=""#b30"">(Yang et al. 2014)</ref>. We observe that our method achieves competitive results on standard benchmarks, outperforming div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""6.1"">Relational modeling</head><p>Our encoder-decoder approach to link prediction relies on DistMult <ref type=""bibr"" target=""#b30"">(Yang et al. 2014)</ref> in the decoder, a special and simpler case of the RESCAL factorization <ref type=""bibr"" target 00</cell><cell>59,071</cell><cell>20,466</cell></row></table><note>Baselines A common baseline for both experiments is direct optimization of DistMult<ref type=""bibr"" target=""#b30"">(Yang et al. 2014)</ref>. This factorization strategy is known to perform well on standard</note></figure> <figure xmln type=""bibr"" target=""#b25"">(Socher et al. 2013;</ref><ref type=""bibr"">Lin et al. 2015;</ref><ref type=""bibr"" target=""#b27"">Toutanova et al. 2016;</ref><ref type=""bibr"" target=""#b30"">Yang et al. 2014;</ref><ref type=""bibr"">Trouillon et al. 2016</ref>)) can be interpreted under this framework. The cruc ll link prediction model is schematically depicted in Figure <ref type=""figure"">3b</ref>.</p><p>In our experiments, we use the DistMult factorization <ref type=""bibr"" target=""#b30"">(Yang et al. 2014</ref>) as the scoring function, which is known to perform well on standard link prediction benchmarks nd a triple (s, r, o) is scored as</p><formula xml:id=""formula_11"">f (s, r, o) = e T s R r e o .</formula><p>(6) As in previous work on factorization <ref type=""bibr"" target=""#b30"">(Yang et al. 2014;</ref><ref type=""bibr"">Trouillon et al. 2016)</ref>, we train the model with negative sampling. For e",0
"f>. <ref type=""bibr"">Denton et al.</ref> propose a genera-tive model based on a Laplacian pyramid framework (LAP-GAN) to generate realistic images in <ref type=""bibr"" target=""#b8"">[6]</ref>, which is the most related to our work. However, the proposed LapSRN differs from LAPGAN in three aspects.</p>",1
"t=""#b32"">[30,</ref><ref type=""bibr"" target=""#b38"">36]</ref>, and random forest <ref type=""bibr"" target=""#b28"">[26]</ref>.</p><p>Recently, Dong et al. <ref type=""bibr"" target=""#b9"">[7]</ref> propose a Super-Resolution Convolutional Neural Network (SRCNN) to learn a nonlinear LR-to-HR mapping. The net igh capacity of deep networks. Experimental results demonstrate that our method is faster than several CNN based super-resolution models, e.g., SRCNN <ref type=""bibr"" target=""#b9"">[7]</ref>, SCN <ref type=""bibr"" target=""#b35"">[33]</ref>, VDSR <ref type=""bibr"" target=""#b19"">[17]</ref>, and DRCN <ref ally linear regressors for each cluster. Convolutional neural networks based SR. In contrast to modeling the LR-HR mapping in the patch space, SR-CNN <ref type=""bibr"" target=""#b9"">[7]</ref> jointly optimize all the steps and learn the nonlinear mapping in the image space. The VDSR network <ref type= the nonlinear mapping in the image space. The VDSR network <ref type=""bibr"" target=""#b19"">[17]</ref> demonstrates significant improvement over SRCNN <ref type=""bibr"" target=""#b9"">[7]</ref> by increasing the network depth from 3 to 20 convolutional layers. To facilitate training a deeper model with onvolutional layers. To facilitate training a deeper model with a fast Table <ref type=""table"">1</ref>: Comparisons of CNN based SR algorithms: SRCNN <ref type=""bibr"" target=""#b9"">[7]</ref>, FSRCNN <ref type=""bibr"" target=""#b10"">[8]</ref>, SCN <ref type=""bibr"" target=""#b35"">[33]</ref>, ESPCN <ref ty e state-of-the-arts</head><p>We compare the proposed LapSRN with 8 state-of-theart SR algorithms: A+ <ref type=""bibr"" target=""#b32"">[30]</ref>, SRCNN <ref type=""bibr"" target=""#b9"">[7]</ref>, FSRCNN <ref type=""bibr"" target=""#b10"">[8]</ref>, SelfExSR <ref type=""bibr"" target=""#b17"">[15]</ref>, RFL <ref s the stroke with the letter ""O"". On the right image, our method reconstructs the rails without the ringing artifacts.</p><p>Ground-truth HR HR SRCNN <ref type=""bibr"" target=""#b9"">[7]</ref> VDSR <ref type=""bibr"" target=""#b19"">[17]</ref> LapSRN (ours)</p><p>Figure <ref type=""figure"">8</ref>: Visual c div><figure xmlns=""http://www.tei-c.org/ns/1.0"" xml:id=""fig_0""><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Network architectures of SRCNN<ref type=""bibr"" target=""#b9"">[7]</ref>, FSRCNN<ref type=""bibr"" target=""#b10"">[8]</ref>, VDSR<ref type=""bibr"" target=""#b19"">[17]</ref>, DRCN<ref type= ame training dataset is used in <ref type=""bibr"" target=""#b19"">[17,</ref><ref type=""bibr"" target=""#b28"">26]</ref>    the protocol of existing methods <ref type=""bibr"" target=""#b9"">[7,</ref><ref type=""bibr"" target=""#b19"">17]</ref>, we generate the LR training patches using the bicubic downsampling. W windows and the stripes on tigers. We observe that methods using the bicubic upsampling for pre-processing generate results with noticeable artifacts <ref type=""bibr"" target=""#b9"">[7,</ref><ref type=""bibr"" target=""#b19"">17,</ref><ref type=""bibr"" target=""#b28"">26,</ref><ref type=""bibr"" target=""#b32""> results on BSDS100 and URBAN100 in Figure <ref type=""figure"">5</ref>. For 8× SR, it is challenging to predict HR images from bicubicupsampled images <ref type=""bibr"" target=""#b9"">[7,</ref><ref type=""bibr"" target=""#b19"">17,</ref><ref type=""bibr"" target=""#b32"">30]</ref> or using one-step upsampling < ects the 3D scene geometry and uses self-similarity to hallucinate the regular structure. This is a common limitation shared by parametric SR methods <ref type=""bibr"" target=""#b9"">[7,</ref><ref type=""bibr"" target=""#b10"">8,</ref><ref type=""bibr"" target=""#b19"">17,</ref><ref type=""bibr"" target=""#b20"">1",0
"LapSRN, each convolutional layer consists of 64 filters with the size of 3 × 3. We initialize the convolutional filters using the method of He et al. <ref type=""bibr"" target=""#b15"">[13]</ref>. The size of the transposed convolutional filters is 4 × 4 and the weights are initialized from a bilinear f",0
"ttp://www.tei-c.org/ns/1.0""><head n=""4.5."">Super-resolving video sequences</head><p>We conduct frame-based SR experiments on two video sequences from <ref type=""bibr"" target=""#b24"">[22]</ref> with a spatial resolution of 1200 × 800 pixels. 2 We downsample each frame by 8×, and then apply super-resol",0
"p>The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU <ref type=""bibr"" target=""#b19"">[20]</ref>, ByteNet <ref type=""bibr"" target=""#b14"">[15]</ref> and ConvS2S <ref type=""bibr"" target=""#b7"">[8]</ref>, all of which use convolutional neural networks as basic ions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels, or O(log k (n)) in the case of dilated convolutions <ref type=""bibr"" target=""#b14"">[15]</ref>, increasing the length of the longest paths between any two positions in the network. Convolutional layers a ns, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b14"">15]</ref> and <ref type=""bibr"" target=""#b7"">[8]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3"">M",1
"ous tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b15"">16]</ref>. In all but a few cases <ref type=""bibr"" target=""#b21"">[22]</ref>, however, such attention mechanisms are use",1
"and decoder stacks. For the base model, we use a rate of P drop = 0.1. Label Smoothing During training, we employed label smoothing of value ls = 0.1 <ref type=""bibr"" target=""#b29"">[30]</ref>. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.</p><p>6",0
"xml:lang=""en""> 		<body> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""1"">Introduction</head><p>Recurrent neural networks, long short-term memory <ref type=""bibr"" target=""#b11"">[12]</ref> and gated recurrent <ref type=""bibr"" target=""#b6"">[7]</ref> neural networks in particular, have been firmly",0
"network. We employ a residual connection <ref type=""bibr"" target=""#b9"">[10]</ref> around each of the two sub-layers, followed by layer normalization <ref type=""bibr"" target=""#b0"">[1]</ref>. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implem",0
"ndent sentence representations <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b18"">19]</ref>.</p><p>End-to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned rec",0
"onstraints limit batching across examples. Recent work has achieved significant improvements in computational efficiency through factorization tricks <ref type=""bibr"" target=""#b17"">[18]</ref> and conditional computation <ref type=""bibr"" target=""#b25"">[26]</ref>, while also improving model performanc",0
"ransduction problems such as language modeling and machine translation <ref type=""bibr"" target=""#b28"">[29,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b4"">5]</ref>. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder /www.tei-c.org/ns/1.0""><head n=""3"">Model Architecture</head><p>Most competitive neural sequence transduction models have an encoder-decoder structure <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b28"">29]</ref>. Here, the encoder maps an input",0
"aries of recurrent language models and encoder-decoder architectures <ref type=""bibr"" target=""#b30"">[31,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b12"">13]</ref>.</p><p>Recurrent models typically factor computation along the symbol positions of the input and output seque",0
"e made. There are also recent approaches to learning over graph structures using convolution operators that offer promise as an embedding methodology <ref type=""bibr"" target=""#b16"">[17]</ref>. So far, graph convolutional networks (GCNs) have only been applied in the transductive setting with fixed g ew aggregator architectures we propose provide significant gains (7.4% on average) compared to an aggregator inspired by graph convolutional networks <ref type=""bibr"" target=""#b16"">[17]</ref>. Lastly, we probe the expressive capability of our approach and show, through theoretical analysis, that Gra etwork (GCN), introduced by Kipf et al. <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b17"">18]</ref>. The original GCN algorithm <ref type=""bibr"" target=""#b16"">[17]</ref> is designed for semi-supervised learning in a transductive setting, and the exact algorithm requires that th tors in {h k−1 u , ∀u ∈ N (v)}. The mean aggregator is nearly equivalent to the convolutional propagation rule used in the transductive GCN framework <ref type=""bibr"" target=""#b16"">[17]</ref>. In particular, we can derive an inductive variant of the GCN approach by replacing lines 4 and 5 in Algorit ></formula><p>We call this modified mean-based aggregator convolutional since it is a rough, linear approximation of a localized spectral convolution <ref type=""bibr"" target=""#b16"">[17]</ref>. An important distinction between this convolutional aggregator and our other proposed aggregators is that i egator functions (Section 3.3). Since, the ""convolutional"" variant of GraphSAGE is an extended, inductive version of Kipf et al's semi-supervised GCN <ref type=""bibr"" target=""#b16"">[17]</ref>, we term this variant GraphSAGE-GCN. We test unsupervised variants of GraphSAGE trained according to the los is far more competitive when tested in the transductive setting, where it can be extensively trained on a single, fixed graph. (That said, Kipf et al <ref type=""bibr"" target=""#b16"">[17]</ref> <ref type=""bibr"" target=""#b17"">[18]</ref> found that GCN-based approach consistently outperformed DeepWalk, p://www.tei-c.org/ns/1.0"" place=""foot"" n=""4"" xml:id=""foot_2"">Note that this differs from Kipf et al's exact equation by a minor normalization constant<ref type=""bibr"" target=""#b16"">[17]</ref>.</note> 			<note xmlns=""http://www.tei-c.org/ns/1.0"" place=""foot"" n=""5"" xml:id=""foot_3"">Code and links to th ype=""bibr"" target=""#b16"">[17]</ref>. So far, graph convolutional networks (GCNs) have only been applied in the transductive setting with fixed graphs <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b17"">18]</ref>. In this work we both extend GCNs to the task of inductive unsupervi ref type=""bibr"" target=""#b23"">24]</ref>. However, our approach is closely related to the graph convolutional network (GCN), introduced by Kipf et al. <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b17"">18]</ref>. The original GCN algorithm <ref type=""bibr"" target=""#b16"">[17]</ref worth noting, however, that Kipf et al's ""featureless"" GCN approach has parameter dimension O(|V|), so this requirement is not entirely unreasonable <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b17"">18]</ref>.</p><p>Following Theorem 1, we let x v ∈ U, ∀v ∈ V denote the featur r graphs have been proposed (e.g., <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b23"">24]</ref>). The majority of these methods do not scale to large graphs or are d",1
"ar, graph convolutional networks (GCNs) have only been applied in the transductive setting with fixed graphs <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b17"">18]</ref>. In this work we both extend GCNs to the task of inductive unsupervised learning and propose a framework that However, our approach is closely related to the graph convolutional network (GCN), introduced by Kipf et al. <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b17"">18]</ref>. The original GCN algorithm <ref type=""bibr"" target=""#b16"">[17]</ref> is designed for semi-supervised learnin ""featureless"" GCN approach has parameter dimension O(|V|), so this requirement is not entirely unreasonable <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b17"">18]</ref>.</p><p>Following Theorem 1, we let x v ∈ U, ∀v ∈ V denote the feature inputs for Algorithm 1 on graph G = (V, transductive setting, where it can be extensively trained on a single, fixed graph. (That said, Kipf et al <ref type=""bibr"" target=""#b16"">[17]</ref> <ref type=""bibr"" target=""#b17"">[18]</ref> found that GCN-based approach consistently outperformed DeepWalk, even in the transductive setting on link p",1
"h respect to each other (Appendix D).</p><p>All models were implemented in TensorFlow <ref type=""bibr"" target=""#b0"">[1]</ref> with the Adam optimizer <ref type=""bibr"" target=""#b15"">[16]</ref> (except DeepWalk, which performed better with the vanilla gradient descent optimizer). We designed our exper",0
"e 2005 data for testing (with 30% used for validation). For features, we used node degrees and processed the paper abstracts according Arora et al.'s <ref type=""bibr"" target=""#b1"">[2]</ref> sentence embedding approach, with 300-dimensional word vectors trained using the GenSim word2vec implementatio",0
"and the remaining days for testing (with 30% used for validation). For features, we use off-the-shelf 300-dimensional GloVe CommonCrawl word vectors <ref type=""bibr"" target=""#b26"">[27]</ref>; for each post, we concatenated (i) the average embedding of the post title, (ii) the average embedding of a",0
"ing <ref type=""bibr"" target=""#b22"">[23]</ref>, multi-dimensional scaling <ref type=""bibr"" target=""#b18"">[19]</ref>, as well as the PageRank algorithm <ref type=""bibr"" target=""#b24"">[25]</ref>. Since these embedding algorithms directly train node embeddings for individual nodes, they are inherently t",0
"type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b23"">24]</ref>). The majority of these methods do not scale to large graphs or are designed for whole-graph classification ( ole-graph classification (or both) <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b23"">24]</ref>. However, our approach is closely related to the graph convolutional network (GCN), introduced by Kipf et al.",0
"also bear close relationships to more classic approaches to spectral clustering <ref type=""bibr"" target=""#b22"">[23]</ref>, multi-dimensional scaling <ref type=""bibr"" target=""#b18"">[19]</ref>, as well as the PageRank algorithm <ref type=""bibr"" target=""#b24"">[25]</ref>. Since these embedding algorith",0
"ct identifiable in fewer iterations, or with less restrictions, than we impose. Moreover, due to our reliance on two universal approximation theorems <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b28"">29]</ref>, the required dimensionality is in principle O(|V|). We can provide and θ σ its parameters, we have that ∀ , ∃θ σ such that |f θσ (x) − g(x)| &lt; |, ∀x ∈ U .</p><p>Proof. This is a direct consequence of Theorem 2 in <ref type=""bibr"" target=""#b14"">[15]</ref>.</p><p>Lemma 3. Let A be the adjacency matrix of G, let N 2 (v) denote the 2-hop neighborhood of a node, v, single-layer MLP (or equivalently, one more iteration of Algorithm 1, ignoring neighborhood information). Again this last step follows directly from <ref type=""bibr"" target=""#b14"">[15]</ref>. Corollary 2 is a direct consequence of Theorem 1 and the fact that, for any probability distribution that i",0
"eads to significant gains in performance (Section 4).</p><p>LSTM aggregator. We also examined a more complex aggregator based on an LSTM architecture <ref type=""bibr"" target=""#b13"">[14]</ref>. Compared to the mean aggregator, LSTMs have the advantage of larger expressive capability. However, it is i",0
"ces' lengths.</p><p>Because soft-DTW can be used with kernel machines, one typically observes an increase in performance when using soft-DTW over DTW <ref type=""bibr"" target=""#b6"">(Cuturi, 2011)</ref> for classification.</p><p>Our contributions. We explore in this paper another important benefit of",1
"in a discriminative framework (with a k-NN or SVM classifier) to predict a real or a class label output, and engineered to run faster in that context <ref type=""bibr"" target=""#b27"">(Yi et al., 1998)</ref>. Recent works by <ref type=""bibr"" target=""#b17"">Petitjean et al. (2011)</ref>; <ref type=""bibr""",0
"4"">[5,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b17"">18]</ref> and machine learning <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b3"">4]</ref> communities exposed the vulnerability of classifiers to integrity atta en, adversarial examples are crafted using this substitute. We expect the target DNN to misclassify them due to transferability between architectures <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b3"">4]</ref> To understand the difficulty of conducting the attack under this threa he DNN architecture and parameters <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b13"">14]</ref>, or (b) an independently collected training set to fit an auxiliary model <ref type=""bibr"" target=""#b1"">[2,</ or (b) an independently collected training set to fit an auxiliary model <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b13"">14]</ref>. This limited their applicability to strong adversaries capable of gaining insider knowledge of the targeted inputs with adversarial examples showed this is not the case in practice <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b13"">14]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3."">THREAT MODEL</head><p>A taxonomy of adversar non-convex ML model: e.g., a DNN. The basis for most adversarial attacks <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b13"">14]</ref> is to approximate its solution using gradient-based optimization on functions defined by a DNN. Because evalu ense mechanism is known, but we study the two with the greatest empirical success so far: adversarial training <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b13"">14]</ref>, and defensive distillation for DNNs <ref type=""bibr"" target=""#b9"">[10]</ref>.</p><p>Adversarial training: It ity</head><p>Previous work started explaining why adversarial samples transfer between different architectures <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b13"">14]</ref>. Here, we build an intuition behind transferability based on statistical hypothesis testing <ref type=""bibr"" of-the-art DNNs. In contrast, previous work failed to simultaneously provide all of these three key properties <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b17"" sarial examples throughout training increases the robustness of significantly descriptive models, such as DNNs <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b16"">17]</ref>. We implemented an approximation of this defense using the Google Pre beled training set from the same population distribution than the oracle could train a model with a different architecture and use it as a substitute <ref type=""bibr"" target=""#b13"">[14]</ref>: adversarial examples designed to manipulate the substitute are often misclassified by the targeted model. H",1
"ork failed to simultaneously provide all of these three key properties <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b17"">18]</ref>. Our contributions are:</p><p VMs, it is unlikely the approach would scale to DNNs.</p><p>Srndic et al. explored the strategy of training a substitute model to find evading inputs <ref type=""bibr"" target=""#b11"">[12]</ref>. They do so using labeled data, which is expensive to collect, especially for models like DNNs. In fact, the",0
"chs, confirming the result.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""8."">DEFENSE STRATEGIES</head><p>The two types of defense are: <ref type=""bibr"" target=""#b0"">(1)</ref> reactive where one seeks to detect adversarial examples, and (2) proactive where one makes the model itself mo",0
"instance, a malware detector is a classifier taking executables as inputs and assigning them to the benign or malware class. Efforts in the security <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b17"">18]<",0
"while remaining correctly classified by a human observer. To illustrate, consider the following images, potentially consumed by an autonomous vehicle <ref type=""bibr"" target=""#b12"">[13]</ref>:</p><p>To humans, these images appear to be the same: our biological classifiers (vision) identify each imag >:</p><p>To humans, these images appear to be the same: our biological classifiers (vision) identify each image as a stop sign. The image on the left <ref type=""bibr"" target=""#b12"">[13]</ref> is indeed an ordinary image of a stop sign. We produced the image on the right by adding a precise perturbat h a perturbation not affecting human recognition.</p><p>? A second oracle trained locally with the German Traffic Signs Recognition Benchmark (GTSRB) <ref type=""bibr"" target=""#b12"">[13]</ref>, can be forced to misclassify more than 64.24% of altered inputs without affecting human recognition.</p></d at lower distortions compared to the MNIST oracle.</p><p>Oracle Description: The GTSRB dataset is an image collection consisting of 43 traffic signs <ref type=""bibr"" target=""#b12"">[13]</ref>. Images vary in size and are RGB-encoded. To simplify, we resize images to 32x32 pixels, recenter them by su figDesc>Figure 2: Adversarial samples (misclassified) in the bottom row are created from the legitimate samples<ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b12"">13]</ref> in the top row. The DNN outputs are identified below the samples.</figDesc></figure> <figure xmlns=""http://ww",0
"sing heuristics based on observations of common-case access patterns <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b19"" s break accesses into many classes, often using the requesting PC, and then adapt their policy to each class <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b38"">39]</ref>. However, with thousands of c of accesses. IRGD <ref type=""bibr"" target=""#b34"">[35]</ref> computes a statistical cost function from sampled reuse distance histograms. And Hawkeye <ref type=""bibr"" target=""#b15"">[16]</ref> emulates MIN's past decisions. Without a theoretical foundation, it is unclear if any of these policies take ss, PC, or instruction sequence. Likewise, SDBP <ref type=""bibr"" target=""#b20"">[21]</ref>, PRP <ref type=""bibr"" target=""#b10"">[11]</ref>, and Hawkeye <ref type=""bibr"" target=""#b15"">[16]</ref> learn the behavior of different PCs. And PDP <ref type=""bibr"" target=""#b13"">[14]</ref> and IRGD <ref type=""b",1
"licies at equal area, closing 57% of the gap from random replacement to MIN vs. 47% for SHiP <ref type=""bibr"" target=""#b38"">[39]</ref>, 41% for DRRIP <ref type=""bibr"" target=""#b16"">[17]</ref>, and 42% for PDP <ref type=""bibr"" target=""#b13"">[14]</ref>.</p><p>Fewer misses translate into large area sav at were previously reused: e.g., LFU uses frequency alone, and ""scan-resistant"" policies like ARC <ref type=""bibr"" target=""#b25"">[26]</ref> and SRRIP <ref type=""bibr"" target=""#b16"">[17]</ref> favor candidates that have been reused at least once.</p><p>Second, recent high-performance policies adapt t IP <ref type=""bibr"" target=""#b29"">[30]</ref> detects thrashing with set dueling, and thereafter inserts most lines at LRU to prevent thrashing. DRRIP <ref type=""bibr"" target=""#b16"">[17]</ref> inserts lines at medium priority, promoting them only upon reuse, and avoids thrashing using the same mechan . We sweep configurations for each policy and select the one that is most area-efficient at iso-performance. DRRIP uses M = 2 bits per tag and = 1/32 <ref type=""bibr"" target=""#b16"">[17]</ref>. SHiP uses M = 2 bits and PC signatures with idealized, large history counter tables <ref type=""bibr"" target common-case access patterns <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" target=""#b20"" re. This longstanding approach takes inspiration from theory (i.e., <ref type=""bibr"">MIN)</ref> and has been implemented in recent empirical policies <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b19"">20]</ref>. (However, Sec. III shows that this strategy is suboptimal.)</p><p>Y SDBP <ref type=""bibr"" target=""#b20"">[21]</ref> and PRP <ref type=""bibr"" target=""#b10"">[11]</ref> predict which lines are unlikely to be reused. RRIP <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b38"">39]</ref> and IbRDP <ref type=""bibr"" target=""#b26"">[27]</ref> try to predict c b27"">[28]</ref>. Most policies employ some form of recency, favoring candidates that were referenced recently: e.g., LRU uses recency alone, and RRIP <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b38"">39]</ref> predicts a longer time until reference for older candidates. Similar MIN does not easily generalize under uncertainty: obvious generalizations like evicting the candidate with the highest expected time until reference <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" target=""#b34"">35]</ref> or the lowest expected hit p xample: Example: Example: Example: Inspired by MIN, several recent policies predict time until reference and evict the candidate with the longest one <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" target=""#b34"">35]</ref>. A simple counterexample sho s: lines that have hit at least once, and newly inserted lines that have not. We use this simple scheme because it has proven effective in prior work <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b25"">26]</ref>. (In the above example, the ardware-only implementation in Sec. VI-D.</p><p>Unlike prior policies, EVA does not devote a fraction of sets to monitoring alternative policies (cf. <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b29"">30]</ref>), nor does it require auxiliary tags to monitor properties independe",0
"dently with static, known probabilities. In this model, the optimal policy is to evict the candidate with the lowest reference probability, i.e., LFU <ref type=""bibr"" target=""#b1"">[2]</ref>. Though useful in other areas (e.g., web caches <ref type=""bibr"" target=""#b3"">[4]</ref>), the IRM is inadequat efine a reference model that is simple enough to solve for the optimal policy, yet accurate enough that this policy is effective. In 1971, Aho et al. <ref type=""bibr"" target=""#b1"">[2]</ref> studied page replacement within the independent reference model (IRM), which assumes that pages are accessed n have argued are the fundamental constraints (Sec. III), it gives broad optimality conditions. For example, both perfect information (MIN) and the IRM <ref type=""bibr"" target=""#b1"">[2]</ref> are special cases.</p><p>EVA is easy to extend to many different contexts. For instance, with small modificati",0
"ehavior is through frequency, favoring candidates that were previously reused: e.g., LFU uses frequency alone, and ""scan-resistant"" policies like ARC <ref type=""bibr"" target=""#b25"">[26]</ref> and SRRIP <ref type=""bibr"" target=""#b16"">[17]</ref> favor candidates that have been reused at least once.</p use this simple scheme because it has proven effective in prior work <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b25"">26]</ref>. (In the above example, the ""reused class"" is the small array and the fraction of the big array that fits.) H",0
"<div xmlns=""http://www.tei-c.org/ns/1.0""><head>I. INTRODUCTION</head><p>Last-level caches consume significant resources, often over 50% of chip area <ref type=""bibr"" target=""#b23"">[24]</ref>, so it is crucial to manage them efficiently. Prior work has approached cache replacement from both theoreti",0
"nclusivity properties or even form an explicit hierarchy.</p><p>To separate the metadata and data hierarchies, we build upon the Direct-to-Data (D2D) <ref type=""bibr"" target=""#b0"">[1]</ref> cache hierarchy, which replaces TLB lookups with access to location tracking information to identify the locat t metadata hierarchy. This enables many previously proposed cache optimizations under one common framework. These include:</p><p>? Direct data access <ref type=""bibr"" target=""#b0"">[1]</ref> to lower latency by using the metadata information to skip level searches, way searches and directory indirect a wide range of existing and future cache optimizations. As a first step, we extend the energy-efficient, but single-core only, Direct-to-Data (D2D) <ref type=""bibr"" target=""#b0"">[1]</ref> cache framework to support multiple cores.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>A. Private t address multicore issues. Boettcher et al. <ref type=""bibr"" target=""#b38"">[39]</ref> and Sembrant et al. <ref type=""bibr"" target=""#b39"">[40]</ref>, <ref type=""bibr"" target=""#b0"">[1]</ref> extend the TLB with cacheline way information to reduce L1 cache energy. Boettcher et al. <ref type=""bibr"" tar onding active MD entry needs to be updated. This is handled by adding tracking pointers to each cacheline (e, f). More details on this can be found in<ref type=""bibr"" target=""#b0"">[1]</ref>.</p></note> 			<note xmlns=""http://www.tei-c.org/ns/1.0"" place=""foot"" n=""2"" xml:id=""foot_1""><p>There are separ",1
"r"" target=""#b2"">[3]</ref>, <ref type=""bibr"" target=""#b3"">[4]</ref>, <ref type=""bibr"" target=""#b4"">[5]</ref>, <ref type=""bibr"" target=""#b5"">[6]</ref>, <ref type=""bibr"" target=""#b6"">[7]</ref>. (Leverage the metadata hierarchy's decoupling of cacheline addresses from placement, skipped directory indire r"" target=""#b2"">[3]</ref>, <ref type=""bibr"" target=""#b3"">[4]</ref>, <ref type=""bibr"" target=""#b4"">[5]</ref>, <ref type=""bibr"" target=""#b5"">[6]</ref>, <ref type=""bibr"" target=""#b6"">[7]</ref>. To optimize for varying access latencies, many policies have been suggested to migrate or replicate data clos lify efficient NUCA implementations, while the placement and replication policies we investigated are simple, with much room for improvement. DDCache <ref type=""bibr"" target=""#b6"">[7]</ref> provides data replication, but adds a sharer list to each L1 cacheline whereas D2M has one sharer list per reg",0
"t=""#b12"">[13]</ref>, <ref type=""bibr"" target=""#b13"">[14]</ref>, <ref type=""bibr"" target=""#b14"">[15]</ref>, <ref type=""bibr"" target=""#b15"">[16]</ref>, <ref type=""bibr"" target=""#b16"">[17]</ref>, <ref type=""bibr"" target=""#b17"">[18]</ref>, <ref type=""bibr"" target=""#b18"">[19]</ref>. (The metadata provide",0
"hared data classification <ref type=""bibr"" target=""#b7"">[8]</ref>, <ref type=""bibr"" target=""#b8"">[9]</ref>, <ref type=""bibr"" target=""#b9"">[10]</ref>, <ref type=""bibr"" target=""#b10"">[11]</ref>.</p><p>(Using classification from the sharing information in the metadata.) ? Cache bypassing to improve per",0
"tions, etc. </p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>V. EVALUATION A. Methodology</head><p>We use the gem5 ARM full-system simulator <ref type=""bibr"" target=""#b25"">[26]</ref> configured to be similar to a contemporary energy-efficient processor as shown in Table <ref type=""table"">II",0
"usted chain <ref type=""bibr"" target=""#b4"">[4,</ref><ref type=""bibr"" target=""#b5"">5]</ref>,and trusted measurement is a key problem of this technology <ref type=""bibr"" target=""#b6"">[6,</ref><ref type=""bibr"" target=""#b7"">7]</ref>. Trusted computing treats the integrity as a fundamental attribute of tr",1
"nced Control Flow Checking Using Assertions (ECCA) <ref type=""bibr"" target=""#b10"">[10]</ref> and Control Flow Checking by Software Signatures (CFCSS) <ref type=""bibr"" target=""#b11"">[11]</ref> and On-line Control Flow Error Detection Using Relationship Signatures Among Basic Blocks(RSCFC) <ref type="" is able to detect this new class of attack. CFCBS is a software-only solution and does not require any special hardware. This solution is inspired by <ref type=""bibr"" target=""#b11"">[11]</ref> and <ref type=""bibr"" target=""#b12"">[12,</ref><ref type=""bibr"" target=""#b13"">13,</ref><ref type=""bibr"" target lied to the assembly codes. We considered four versions for each benchmark:</p><p>The original code.</p><p>A safe one, obtained by applying the CFCSS <ref type=""bibr"" target=""#b11"">[11]</ref> technique to the original code.</p><p>A safe one, obtained by applying the RSCFC <ref type=""bibr"" target=""#b",0
"s transactions over open networks, such as the Internet, increasingly demand for secure communication and secure operation due to rising online fraud <ref type=""bibr"" target=""#b0"">[1]</ref> and software attacks <ref type=""bibr"" target=""#b2"">[2]</ref>. Some of these vulnerabilities are due to the com",0
"he integrity of executable content through remote attestation. The cores of trusted computing technology are trusted computing base and trusted chain <ref type=""bibr"" target=""#b4"">[4,</ref><ref type=""bibr"" target=""#b5"">5]</ref>,and trusted measurement is a key problem of this technology <ref type=""b",0
"CBS is a software-only solution and does not require any special hardware. This solution is inspired by <ref type=""bibr"" target=""#b11"">[11]</ref> and <ref type=""bibr"" target=""#b12"">[12,</ref><ref type=""bibr"" target=""#b13"">13,</ref><ref type=""bibr"" target=""#b14"">14]</ref> and absorbs their advantages k does not contain any other effective instructions except an unconditional jump instruction and has only one successor node and one predecessor node <ref type=""bibr"" target=""#b12"">[12]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>Program Flow Graph</head><p>A program P can be rep ed(vi) as the set of nodes predecessor of vi. If and only if bri,j E, then the node vj suc(vi). Similarly, only if brj,i E, then the node vj pred(vi) <ref type=""bibr"" target=""#b12"">[12]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>Branch sequence</head><p>A program takes different",0
"=""bibr"" target=""#b14"">[Elgammal and Lee 2004]</ref>, as well as independent component analysis <ref type=""bibr"" target=""#b3"">[Cao et al. 2003]</ref>. <ref type=""bibr"" target=""#b4"">Cao et al. [2005]</ref> extract emotions using support vector machines, and synthesize 3D animations based on speech and at it can be interpolated and extrapolated. They also present a user study rating the level of realism in emotion synthesis, covering several methods <ref type=""bibr"" target=""#b4"">[Cao et al. 2005;</ref><ref type=""bibr"" target=""#b27"">Liu and Ostermann 2011;</ref><ref type=""bibr"" target=""#b33"">Melenc would be to manually label or categorize the training samples based on the apparent emotion <ref type=""bibr"" target=""#b0"">[Anderson et al. 2013;</ref><ref type=""bibr"" target=""#b4"">Cao et al. 2005;</ref><ref type=""bibr"" target=""#b11"">Deng et al. 2006;</ref><ref type=""bibr"" target=""#b47"">Wampler et al",1
"a bilinear model for separating the apparent emotional state from speech visemes. This was later generalized in closely related topics to multilinear <ref type=""bibr"" target=""#b46"">[Vasilescu and Terzopoulos 2003;</ref><ref type=""bibr"" target=""#b47"">Wampler et al. 2007</ref>] and non-linear models <",0
"br"" target=""#b9"">Deena et al. 2013]</ref>, hidden semi-Markov model <ref type=""bibr"" target=""#b37"">[Schabus et al. 2014]</ref>, or recurrent networks <ref type=""bibr"" target=""#b15"">[Fan et al. 2016]</ref>.</p><p>Alternatively, machine learning has been used for learning coarticulation <ref type=""bib . 2016]</ref>.</p><p>Alternatively, machine learning has been used for learning coarticulation <ref type=""bibr"" target=""#b11"">[Deng et al. 2006;</ref><ref type=""bibr"" target=""#b15"">Ezzat et al. 2002]</ref>, followed by a concatenation stage to synthesize animation, or for mapping between various sta ping. Such image-based methods (e.g., <ref type=""bibr"" target=""#b0"">[Anderson et al. 2013;</ref><ref type=""bibr"" target=""#b9"">Deena et al. 2013;</ref><ref type=""bibr"" target=""#b15"">Ezzat et al. 2002;</ref><ref type=""bibr"" target=""#b15"">Fan et al. 2016;</ref><ref type=""bibr"" target=""#b27"">Liu and Ost target=""#b0"">[Anderson et al. 2013;</ref><ref type=""bibr"" target=""#b9"">Deena et al. 2013;</ref><ref type=""bibr"" target=""#b15"">Ezzat et al. 2002;</ref><ref type=""bibr"" target=""#b15"">Fan et al. 2016;</ref><ref type=""bibr"" target=""#b27"">Liu and Ostermann 2011;</ref><ref type=""bibr"" target=""#b48"">Wang a lying on explicit trajectory optimization (e.g. <ref type=""bibr"" target=""#b2"">[Brand 1999;</ref><ref type=""bibr"" target=""#b9"">Deena et al. 2013;</ref><ref type=""bibr"" target=""#b15"">Fan et al. 2016]</ref>) have substantially higher latency.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""",0
"ndow to a compact 2D representation for the subsequent convolutional layers. Our approach is inspired by the source-filter model of speech production <ref type=""bibr"" target=""#b1"">[Benzeghiba et al. 2007;</ref><ref type=""bibr"" target=""#b24"">Lewis 1991]</ref>, where the audio signal is modeled as a c pically based on a specialized techniques such as Mel-frequency cepstral coefficients (MFCC), perceptual linear prediction (PLP), and rasta filtering <ref type=""bibr"" target=""#b1"">[Benzeghiba et al. 2007</ref>]. These techniques have enjoyed wide adoption mainly because they lead to good linear sepa",0
"oximation, with alternative representations such as Gaussian process latent variable model <ref type=""bibr"" target=""#b8"">[Deena and Galata 2009;</ref><ref type=""bibr"" target=""#b9"">Deena et al. 2013]</ref>, hidden semi-Markov model <ref type=""bibr"" target=""#b37"">[Schabus et al. 2014]</ref>, or recurr red video frames with concatenation, blending, and warping. Such image-based methods (e.g., <ref type=""bibr"" target=""#b0"">[Anderson et al. 2013;</ref><ref type=""bibr"" target=""#b9"">Deena et al. 2013;</ref><ref type=""bibr"" target=""#b15"">Ezzat et al. 2002;</ref><ref type=""bibr"" target=""#b15"">Fan et al. method therefore seems to be around 100ms. Methods relying on explicit trajectory optimization (e.g. <ref type=""bibr"" target=""#b2"">[Brand 1999;</ref><ref type=""bibr"" target=""#b9"">Deena et al. 2013;</ref><ref type=""bibr"" target=""#b15"">Fan et al. 2016]</ref>) have substantially higher latency.</p></d",0
"nother solution is to design a ML model to learn feature interactions from raw data automatically. A popular approach is factorization machines (FMs) <ref type=""bibr"" target=""#b26"">[27]</ref>, which embeds features into a latent space and models the interactions between features via inner product of real-world data that have complex and non-linear underlying structure, FM may not be expressive enough. Although higher-order FMs have been proposed <ref type=""bibr"" target=""#b26"">[27]</ref>, they still belong to the family of linear models and are claimed to be di cult to estimate <ref type=""bibr"" ly, it enhances linear/logistic regression (LR) using the second-order factorized interactions between features. By specifying input features, Rendle <ref type=""bibr"" target=""#b26"">[27]</ref> showed that FM can mimic many speci c factorization models such as the standard MF, parallel factor analysis rescaled an embedding vector by its input feature value, rather than simply an embedding table lookup, so as to account for the real valued features <ref type=""bibr"" target=""#b26"">[27]</ref>.</p><p>Bi-Interaction Layer. We then feed the embedding set V x into the Bi-Interaction layer, which is a po h SGD (or its variants).</p><p>-HOFM. is is the TensorFlow implementation<ref type=""foot"" target=""#foot_7"">7</ref> of higherorder FM, as described in <ref type=""bibr"" target=""#b26"">[27]</ref>. We experimented with order size 3, since the MovieLens data concerns the ternary relationship between users //www.tei-c.org/ns/1.0""><head n=""2.1"">Factorization Machines</head><p>Factorization machines are originally proposed for collaborative recommendation <ref type=""bibr"" target=""#b26"">[27,</ref><ref type=""bibr"" target=""#b29"">30]</ref>. Given a real valued feature vector x ∈ R n , FM estimates the targe",1
"e been proposed <ref type=""bibr"" target=""#b26"">[27]</ref>, they still belong to the family of linear models and are claimed to be di cult to estimate <ref type=""bibr"" target=""#b27"">[28]</ref>. Moreover, they are known to have only marginal improvements over FM, which we suspect the reason might be d ks for context-aware prediction and personalized tag recommendation. With one hidden layer only, our NFM signi cantly outperforms FM (the ocial LibFM <ref type=""bibr"" target=""#b27"">[28]</ref> implementation) with a 7.3% improvement. Compared to the state-of-the-art deep learning methods -the 3-layer that FM still belongs to the family of (multivariate) linear models. In other words, the predicted target ˆ (x) is linear w.r.t. each model parameter <ref type=""bibr"" target=""#b27"">[28]</ref>. Formally, for each model parameter θ ∈ {w 0 , {w i }, { if }}, we can have ˆ (x) = + hθ , where and h are e 5"">5</ref> . We compared with the following competitive embedding-based models that are speci cally designed for sparse data prediction:</p><p>-LibFM <ref type=""bibr"" target=""#b27"">[28]</ref>. is is the o cial implementation <ref type=""foot"" target=""#foot_6"">6</ref> of FM released by Rendle. It has",1
"a common solution is to convert them to a set of binary features (a.k.a. feature vector) via one-hot encoding <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b29"">30,</ref><ref type=""bibr"" target=""#b30"">3 employ DNNs for e ectively learning feature interactions under sparse se ings. Until very recently, some work <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b30"">31,</ref><ref type=""bibr"" target=""#b43"">4 type=""bibr"" target=""#b30"">31]</ref>. Many successful solutions in both industry and academia largely rely on manually cra ing combinatorial features <ref type=""bibr"" target=""#b8"">[9]</ref>, i.e., constructing new features by combining multiple predictor variables, also known as cross features. For "" target=""#b27"">[28]</ref> implementation) with a 7.3% improvement. Compared to the state-of-the-art deep learning methods -the 3-layer Wide&amp;Deep <ref type=""bibr"" target=""#b8"">[9]</ref> and 10-layer DeepCross <ref type=""bibr"" target=""#b30"">[31]</ref> -our 1-layer NFM shows consistent improvement get=""#b43"">[44]</ref> developed a FM-supported Neural Network (FNN), which uses the feature embeddings learned by FM to initialize DNNs. Cheng et al. <ref type=""bibr"" target=""#b8"">[9]</ref> proposed Wide&amp;Deep for App recommendation, where the deep part is a multi-layer perceptron (MLP) on the co >. We experimented with order size 3, since the MovieLens data concerns the ternary relationship between users, movies and tags.</p><p>-Wide&amp;Deep <ref type=""bibr"" target=""#b8"">[9]</ref>. As introduced in Section 2.2, the deep part rst concatenates feature embeddings, followed by a MLP to model u nd nonlinear feature interactions e ectively to improve FM's expressiveness. In contrast to traditional deep learning methods that simply concatenate <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b30"">31,</ref><ref type=""bibr"" target=""#b43"">44]</ref> or average <ref type=""bibr"" ta red in the training data. In recent years, embedding-based methods become increasingly popular, which try to learn feature interactions from raw data <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b29""> deep layers to learn meaningful interaction function. While it is claimed that multiple non-linear layers are able to learn feature interactions well <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b30"">31]</ref>, such a deep architecture can be di cult to optimize in practice due t of higher-order interactions in a linear way. As for the structure of fully connected layers (i.e., size of each layer), one can freely choose tower <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b15"">16]</ref>, constant <ref type=""bibr"" target=""#b30"">[31]</ref>, and diamond <ref p><p>3.1.2 Relation to Wide&amp;Deep and DeepCross. NFM has a similar multi-layered neural architecture with several existing deep learning solutions <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b30"">31]</ref>.</p><p>e key di erence is in t",0
"techniques for many information retrieval (IR) and data mining (DM) tasks, ranging from recommendation systems <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b15"">16]</ref>, targeted advertising <ref type=""bibr"" target=""#b20"">[21]</ref>, to search ranking <ref type=""bibr"" target=""# ar way. As for the structure of fully connected layers (i.e., size of each layer), one can freely choose tower <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b15"">16]</ref>, constant <ref type=""bibr"" target=""#b30"">[31]</ref>, and diamond <ref type=""bibr"" target=""#b43"">[44]</ref>, a to a set of binary features (a.k.a. feature vector) via one-hot encoding <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b29"">30,</ref><ref type=""bibr"" target=""#b30"">31]</ref>. erea er, standard machine le eature interactions under sparse se ings. Until very recently, some work <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b30"">31,</ref><ref type=""bibr"" target=""#b43"">44]</ref> started to explore DNNs for s d DeepCross. NFM has a similar multi-layered neural architecture with several existing deep learning solutions <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b30"">31]</ref>.</p><p>e key di erence is in the Bi-Interaction pooling component, wh simply concatenate <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b30"">31,</ref><ref type=""bibr"" target=""#b43"">44]</ref> or average <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b35"">36]</ref> embedding vectors in the low level, our use of Bi-Interaction poolin time.</p><p>is property is the same with average/max pooling and concatenation that are rather simple but commonly used in neural network approaches <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b30"">31,</ref><ref type=""bibr"" target=""#b35"">36]</ref>. To show the linear time com =""bibr"" target=""#b30"">31,</ref><ref type=""bibr"" target=""#b43"">44]</ref> started to explore DNNs for some scenarios of sparse predictive analytics. In <ref type=""bibr"" target=""#b15"">[16]</ref>, He et al. presented a neural collaborative ltering (NCF) framework to learn interactions between users and re embedding vectors carries too li le information about feature interactions in the low level. An empirical evidence is from He et al.'s recent work <ref type=""bibr"" target=""#b15"">[16]</ref>, which shows that simply concatenating user and item embedding vectors leads to very poor results for collab es in neural network modelling such as dropout can well prevent NFM from over ing. For classi cation task, we can optimize the hinge loss or log loss <ref type=""bibr"" target=""#b15"">[16]</ref>. For ranking task, we can optimize pairwise personalized ranking loss <ref type=""bibr"" target=""#b28"">[29,</r this work concerns higher-order interactions between features, we study the task of personalized tag recommendation rather than collaborative ltering <ref type=""bibr"" target=""#b15"">[16]</ref> that considers the second-order interactions only. e tagging part of the data includes 668, 953 tag applicat elded immense success on speech processing and computer vision, their performance is still unsatisfactory for IR tasks, such as collaborative ltering <ref type=""bibr"" target=""#b15"">[16]</ref>. In our view, one reason is that most data of IR and DM tasks are naturally sparse; and to date, there still",0
"analytics is one of the most important techniques for many information retrieval (IR) and data mining (DM) tasks, ranging from recommendation systems <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b15"">16]</ref>, targeted advertising <ref type=""bibr"" target=""#b20"">[21]</ref>, to se these categorical predictor variables, a common solution is to convert them to a set of binary features (a.k.a. feature vector) via one-hot encoding <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b29"">30 es via inner product of their embedding vectors. While FM has yielded great promise<ref type=""foot"" target=""#foot_0"">1</ref> in many prediction tasks <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b23"">24",0
"ine advertising <ref type=""bibr"" target=""#b20"">[21]</ref>, microblog retrieval <ref type=""bibr"" target=""#b25"">[26]</ref>, to open relation extraction <ref type=""bibr"" target=""#b24"">[25]</ref>.</p><p>2.1.1 Expressiveness Limitation of FM. Despite e ectiveness, we point out that FM still belongs to th",0
"build e ective ML models with such sparse data, it is crucial to account for the interactions between features <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b30"">31]</ref>. Many successful solutions in both industry and academia largely rely s to existing methods for higher-order interaction learning, such as higher-Order FM <ref type=""bibr"" target=""#b2"">[3]</ref> and Exponential Machines <ref type=""bibr"" target=""#b22"">[23]</ref>, which only support the learning of higher-order interactions in a linear way. As for the structure of fully",0
"ibr"" target=""#b26"">[27]</ref> showed that FM can mimic many speci c factorization models such as the standard MF, parallel factor analysis, and SVD++ <ref type=""bibr"" target=""#b21"">[22]</ref>, Owing to such genericity, FM has been recognized as one of the most e ective embedding methods for sparse d",0
"sidered.</p><p>One main power of FM stems from its generality -in contrast to matrix factorization (MF) that models the relation of two entities only <ref type=""bibr"" target=""#b16"">[17]</ref>, FM is a general predictor working with any real valued feature vector for supervised learning. Clearly, it",0
"perspective, another viable solution for incorporating nonlinearities is to extend the objective function with regularizers like the graph Laplacian <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b33"">34]</ref>. Lastly, we are interested in exploring the Bi-Interaction pooling f",0
"definitions mutually depend on each other.</p><p>Release Consistency (RC) are often mixed with the concept of ""SC for data-race-free (DRF) programs"" <ref type=""bibr"" target=""#b27"">[28]</ref>. It should be noted that ""SC for DRF"" is inadequate for an ISA memory model, which must specify behaviors of",1
"arge amount of research has also been devoted to specifying the memory models of high-level languages: C++ <ref type=""bibr"" target=""#b25"">[26]</ref>, <ref type=""bibr"" target=""#b46"">[47]</ref>- <ref type=""bibr"" target=""#b49"">[50]</ref>, Java <ref type=""bibr"" target=""#b50"">[51]</ref>- <ref type=""bibr"" v &lt;mo Ld a ? St a v &lt;po Ld a}</formula></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>C. Compiling C++11 to WMM</head><p>C++ primitives <ref type=""bibr"" target=""#b46"">[47]</ref> can be mapped to WMM instructions in an efficient way as shown in Figure <ref type=""figure"">10</ref>. For th",0
"fit these observations <ref type=""bibr"" target=""#b6"">[7]</ref>- <ref type=""bibr"" target=""#b9"">[10]</ref>, <ref type=""bibr"" target=""#b11"">[12]</ref>- <ref type=""bibr"" target=""#b15"">[16]</ref>.</p><p>The newly designed open-source RISC-V ISA <ref type=""bibr"" target=""#b16"">[17]</ref> offers a unique o",0
"bstract machine. We observe a growing interest in operational definitions: memory models of x86, ARM and POWER have all been formalized operationally <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b6"">[7]</ref>, <ref type=""bibr"" target=""#b7"">[8]</ref>, <ref type=""bibr"" target=""#b antaneous Instruction Execution (I 2 E), which is the style used in the operational definitions of SC <ref type=""bibr"" target=""#b0"">[1]</ref> and TSO <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b24"">[25]</ref>. An I 2 E abstract machine consists of n atomic processors and an n processors perhaps due to their hardware complexity. Instead the manufactures and researchers have chosen to present weaker memory models, e.g., TSO <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b2"">[3]</ref>, <ref type=""bibr"" target=""#b24"">[25]</ref>, <ref type=""bibr"" target="" ntaneously and increments the PC.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>A. TSO Model</head><p>The TSO abstract machine proposed in <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b24"">[25]</ref> (Figure <ref type=""figure"" target=""#fig_4"">6b</ref>) contains a sto",0
"25"">[26]</ref>, <ref type=""bibr"" target=""#b46"">[47]</ref>- <ref type=""bibr"" target=""#b49"">[50]</ref>, Java <ref type=""bibr"" target=""#b50"">[51]</ref>- <ref type=""bibr"" target=""#b52"">[53]</ref>, etc. We will provide compilation schemes from C++ to WMM and WMM-S.</p><p>Recently, Lustig et al. have used",0
"ter ShallowCNN), which can use only short-range associations. Moreover, DPCNN can be regarded as a deep extension of ShallowCNN, which we proposed in <ref type=""bibr"" target=""#b6"">(Johnson and Zhang, 2015b</ref>) and later tested with large datasets in <ref type=""bibr"" target=""#b7"">(Johnson and Zhan nabling training of deep networks.</p><p>• Text region embedding enhanced with unsupervised embeddings (embeddings trained in an unsupervised manner) <ref type=""bibr"" target=""#b6"">(Johnson and Zhang, 2015b)</ref> for improving accuracy.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2.1 text categorization typically starts with converting each word in the text to a word vector (word embedding). We take a more general viewpoint as in <ref type=""bibr"" target=""#b6"">(Johnson and Zhang, 2015b)</ref> and consider text region embedding -embedding of a region of text covering one or more yer, which is an embedding function that takes view-1 as input, serves as an unsupervised embedding function in the model for text categorization. In <ref type=""bibr"" target=""#b6"">(Johnson and Zhang, 2015b)</ref> unsupervised embeddings obtained this way are useful for classification.</p><p>For use dding enhancement with diverse unsupervised embeddings. Note that ShallowCNN enhanced with unsupervised embeddings (row 2) was originally proposed in <ref type=""bibr"" target=""#b6"">(Johnson and Zhang, 2015b)</ref> as a semi-supervised extension of <ref type=""bibr"" target=""#b5"">(Johnson and Zhang, 201",1
"acter-level CNNs were shown to outperform deep 9-layer character-level CNNs of <ref type=""bibr"" target=""#b14"">(Zhang et al., 2015)</ref>. However, in <ref type=""bibr"" target=""#b7"">(Johnson and Zhang, 2016)</ref>, very shallow 1-layer word-level CNNs were shown to be more accurate and much faster tha extension of ShallowCNN, which we proposed in <ref type=""bibr"" target=""#b6"">(Johnson and Zhang, 2015b</ref>) and later tested with large datasets in <ref type=""bibr"" target=""#b7"">(Johnson and Zhang, 2016)</ref>.</p><p>We show that DPCNN with 15 weight layers outperforms the previous best models on r of stars. Sogou is in Romanized Chinese, and the others are in English. Classes are balanced on all the datasets. Data preprocessing was done as in <ref type=""bibr"" target=""#b7"">(Johnson and Zhang, 2016)</ref>. That is, upper-case letters were converted to lower-case letters. Unlike <ref type=""bib handled as variable-sized without any shortening or padding; however, the vocabulary size was limited to 30K words. For example, as also mentioned in <ref type=""bibr"" target=""#b7"">(Johnson and Zhang, 2016)</ref>, the complete vocabulary of the Ama.p training set contains 1.3M words. A vocabulary of unsupervised embedding training</head><p>To facilitate comparison with ShallowCNN, we matched our unsupervised embedding setting exactly with that of <ref type=""bibr"" target=""#b7"">(Johnson and Zhang, 2016)</ref>. That is, we trained the same four types of tvembeddings, which are embeddings of 5-and ef> as a semi-supervised extension of <ref type=""bibr"" target=""#b5"">(Johnson and Zhang, 2015a)</ref>, and then it was tested on the large datasets in <ref type=""bibr"" target=""#b7"">(Johnson and Zhang, 2016)</ref>. The performance improvements of DPCNN over ShallowCNN indicates that the added depth is ows error rates of DPCNNs with 3, 7, and 15 weight layers (blue circles from left to right). For comparison, the Shal-lowCNN results (green 'x') from <ref type=""bibr"" target=""#b7"">(Johnson and Zhang, 2016)</ref> are also shown. The x-axis represents the computation time (seconds for categorizing 10K ate among a number of variations presented in the respective papers.</figDesc><table><row><cell>-dim bow or 200K-dim</cell></row></table><note>[JZ16]:<ref type=""bibr"" target=""#b7"">Johnson and Zhang (2016)</ref>, [YYDHSH16]: Yang et al. (2016), [CSBL16]: Conneau et al. (2016), [ZZL15]: Zhang et al. (",1
"rvised embeddings (row 2) was originally proposed in <ref type=""bibr"" target=""#b6"">(Johnson and Zhang, 2015b)</ref> as a semi-supervised extension of <ref type=""bibr"" target=""#b5"">(Johnson and Zhang, 2015a)</ref>, and then it was tested on the large datasets in <ref type=""bibr"" target=""#b7"">(Johnson",0
"of the internal representation of each document by half.</p><p>A number of models <ref type=""bibr"" target=""#b11"">(Simonyan and Zisserman, 2015;</ref><ref type=""bibr"" target=""#b1"">He et al., 2015</ref><ref type=""bibr"" target=""#b2"">He et al., , 2016;;</ref><ref type=""bibr"" target=""#b0"">Conneau et al. es b (unique to each layer) are the parameters to be trained. The number of W's rows is the number of feature maps (also called the number of filters <ref type=""bibr"" target=""#b1"">(He et al., 2015)</ref>) of this layer. We set activation σ(•) to the rectifier σ(x) = max(x, 0). In our implementation, d was used also in(Johnson and Zhang, 2015a,b, 2016). It was meant to reduce learning rate when error plateaus, as is often done on image tasks, e.g.,<ref type=""bibr"" target=""#b1"">(He et al., 2015)</ref>, though for simplicity, the timing of reduction was fixed for each dataset.</note> 			<note xmln",0
"ounts of training data (e.g., one million documents). Examples are deep character-level CNNs <ref type=""bibr"" target=""#b14"">(Zhang et al., 2015;</ref><ref type=""bibr"" target=""#b0"">Conneau et al., 2016)</ref>, a complex combination of CNNs and recurrent neural networks (RNNs) <ref type=""bibr"" target= =""#b11"">(Simonyan and Zisserman, 2015;</ref><ref type=""bibr"" target=""#b1"">He et al., 2015</ref><ref type=""bibr"" target=""#b2"">He et al., , 2016;;</ref><ref type=""bibr"" target=""#b0"">Conneau et al., 2016)</ref> increase the number of feature maps whenever downsampling is performed, causing the total co s were converted to lower-case letters. Unlike <ref type=""bibr"" target=""#b9"">(Kim, 2014;</ref><ref type=""bibr"" target=""#b14"">Zhang et al., 2015;</ref><ref type=""bibr"" target=""#b0"">Conneau et al., 2016)</ref>, variable-sized documents were handled as variable-sized without any shortening or padding; utational challenges.</p><p>There have been several recent studies of CNN for text categorization in the large training data setting. For example, in <ref type=""bibr"" target=""#b0"">(Conneau et al., 2016)</ref>, very deep 32-layer character-level CNNs were shown to outperform deep 9-layer character-le and Zhang, 2016)</ref>, very shallow 1-layer word-level CNNs were shown to be more accurate and much faster than the very deep characterlevel CNNs of <ref type=""bibr"" target=""#b0"">(Conneau et al., 2016)</ref>. Although character-level approaches have merit in not having to deal with millions of dist entation on a GPU. The right figure is a close-up of x ∈ [0, 20] of the left figure. It stands out in the left figure that the character-level CNN of <ref type=""bibr"" target=""#b0"">(Conneau et al., 2016)</ref> is much slower than DPCNNs. This is partly because it increases the number of feature maps",0
"hang et al., 2015;</ref><ref type=""bibr"" target=""#b0"">Conneau et al., 2016)</ref>, a complex combination of CNNs and recurrent neural networks (RNNs) <ref type=""bibr"" target=""#b12"">(Tang et al., 2015)</ref>, and RNNs in a wordsentence hierarchy <ref type=""bibr"" target=""#b13"">(Yang et al., 2016)</ref ord level and the sentence level. It is more complex than DPCNN due to the use of RNNs and linguistic knowledge for sentence segmentation. Similarly, <ref type=""bibr"" target=""#b12"">Tang et al. (2015)</ref> proposed to use CNN or LSTM to represent each sentence in documents and then use RNNs. Althoug",0
"le triplets (stride 1). This 2-stride downsampling reduces the size of the internal representation of each document by half.</p><p>A number of models <ref type=""bibr"" target=""#b11"">(Simonyan and Zisserman, 2015;</ref><ref type=""bibr"" target=""#b1"">He et al., 2015</ref><ref type=""bibr"" target=""#b2"">He",0
"o be a meta-parameter. The minibatch size was fixed to 100. Regularization was done by weight decay with the parameter 0.0001 and by optional dropout <ref type=""bibr"" target=""#b3"">(Hinton et al., 2012)</ref> with 0.5 applied to the input to the top layer. In some cases overfitting was observed, and",0
"mbination of CNNs and recurrent neural networks (RNNs) <ref type=""bibr"" target=""#b12"">(Tang et al., 2015)</ref>, and RNNs in a wordsentence hierarchy <ref type=""bibr"" target=""#b13"">(Yang et al., 2016)</ref>.</p><p>A CNN is a feedforward network with convolution layers interleaved with pooling layers 016)</ref>. The performance improvements of DPCNN over ShallowCNN indicates that the added depth is indeed useful, capturing more global information. <ref type=""bibr"" target=""#b13"">Yang et al. (2016)</ref>'s hierarchical attention network (row 3) consists of RNNs in the word level and the sentence l",0
"premise and hypothesis through tree-LSTM <ref type=""bibr"" target=""#b35"">(Zhu et al., 2015;</ref><ref type=""bibr"" target=""#b30"">Tai et al., 2015;</ref><ref type=""bibr"" target=""#b15"">Le and Zuidema, 2015)</ref>, which extends the chain LSTM to a recursive network <ref type=""bibr"" target=""#b28"">(Socher",1
"s end, we will also encode syntactic parse trees of a premise and hypothesis through tree-LSTM <ref type=""bibr"" target=""#b35"">(Zhu et al., 2015;</ref><ref type=""bibr"" target=""#b30"">Tai et al., 2015;</ref><ref type=""bibr"" target=""#b15"">Le and Zuidema, 2015)</ref>, which extends the chain LSTM to a re",1
"orporated into the best-performing models.</p><p>To this end, we will also encode syntactic parse trees of a premise and hypothesis through tree-LSTM <ref type=""bibr"" target=""#b35"">(Zhu et al., 2015;</ref><ref type=""bibr"" target=""#b30"">Tai et al., 2015;</ref><ref type=""bibr"" target=""#b15"">Le and Zui ted network architectures <ref type=""bibr"" target=""#b20"">(Munkhdalai and Yu, 2016b)</ref>.</p><p>We ensemble our ESIM model with syntactic tree-LSTMs <ref type=""bibr"" target=""#b35"">(Zhu et al., 2015)</ref> based on syntactic parse trees and achieve significant improvement over our best sequential en nary tree, where padding nodes are inserted when there are no enough leaves to form a full tree. Each tree node is implemented with a tree-LSTM block <ref type=""bibr"" target=""#b35"">(Zhu et al., 2015)</ref> same as in model ( <ref type=""formula"" target=""#formula_16"">17</ref>). Table <ref type=""table""",1
"rget=""#b2"">(Bowman et al., 2016;</ref><ref type=""bibr"" target=""#b31"">Vendrov et al., 2015;</ref><ref type=""bibr"" target=""#b19"">Mou et al., 2016;</ref><ref type=""bibr"" target=""#b16"">Liu et al., 2016;</ref><ref type=""bibr"" target=""#b20"">Munkhdalai and Yu, 2016a;</ref><ref type=""bibr"" target=""#b25"">Roc augmented parser-interpreter neural network (SPINN) which combines parsing and interpretation within a single tree-sequence hybrid model. The work by <ref type=""bibr"" target=""#b16"">Liu et al. (2016)</ref> uses BiLSTM to generate sentence representations, and then replaces average pooling with intra- 3.3 82.1 (5) 300D SPINN-PI encoders <ref type=""bibr"" target=""#b2"">(Bowman et al., 2016)</ref> 3.7M 89.2 83.2 (6) 600D BiLSTM intra-attention encoders <ref type=""bibr"" target=""#b16"">(Liu et al., 2016)</ref> 2.8M 84.5 84.2 (7) 300D NSE encoders <ref type=""bibr"" target=""#b20"">(Munkhdalai and Yu, 2016a)",0
"central to human and artificial intelligence. Modeling inference in human language is very challenging. With the availability of large annotated data <ref type=""bibr"" target=""#b1"">(Bowman et al., 2015)</ref>, it has recently become feasible to train neural network based inference models, which have guage inference. An important contribution is the creation of a much larger annotated dataset, the Stanford Natural Language Inference (SNLI) dataset <ref type=""bibr"" target=""#b1"">(Bowman et al., 2015)</ref>. The corpus has 570,000 human-written English sentence pairs manually labeled by multiple hu /p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4"">Experimental Setup</head><p>Data The Stanford Natural Language Inference (SNLI) corpus <ref type=""bibr"" target=""#b1"">(Bowman et al., 2015)</ref> focuses on three basic relationships between a premise and a potential hypothesis: the premi ral network models, which often need relatively large annotated data to estimate their parameters, have shown to achieve the state of the art on SNLI <ref type=""bibr"" target=""#b1"">(Bowman et al., 2015</ref><ref type=""bibr"" target=""#b2"">(Bowman et al., , 2016;;</ref><ref type=""bibr"" target=""#b20"">Mun e=""bibr"" target=""#b9"">(Dagan et al., 2005;</ref><ref type=""bibr"" target=""#b12"">Iftene and Balahur-Dobrescu, 2007)</ref>, among others. More recently, <ref type=""bibr"" target=""#b1"">Bowman et al. (2015)</ref> made available the SNLI dataset with 570,000 human annotated sentence pairs. They also experi s the sentence pairs lacking consensus among multiple human annotators. As in the related work, we remove this category. We used the same split as in <ref type=""bibr"" target=""#b1"">Bowman et al. (2015)</ref> and other previous work.</p><p>The parse trees used in this paper are produced by the Stanfor ead><p>Overall performance Table <ref type=""table"">1</ref> shows the results of different models. The first row is a baseline classifier presented by <ref type=""bibr"" target=""#b1"">Bowman et al. (2015)</ref> that considers handcrafted features such as BLEU score of the hypothesis with respect to the",0
"s of the sentences.</p><p>A variety of more advanced networks have been developed since then <ref type=""bibr"" target=""#b2"">(Bowman et al., 2016;</ref><ref type=""bibr"" target=""#b31"">Vendrov et al., 2015;</ref><ref type=""bibr"" target=""#b19"">Mou et al., 2016;</ref><ref type=""bibr"" target=""#b16"">Liu et ding. The model of <ref type=""bibr"" target=""#b2"">Bowman et al. (2016)</ref> encodes the premise and hypothesis with two different LSTMs. The model in <ref type=""bibr"" target=""#b31"">Vendrov et al. (2015)</ref> uses unsupervised ""skip-thoughts"" pre-training in GRU encoders. The approach proposed by <r d Yu (2016a)</ref>  (2) 300D LSTM encoders <ref type=""bibr"" target=""#b2"">(Bowman et al., 2016)</ref> 3.0M 83.9 80.6 (3) 1024D pretrained GRU encoders <ref type=""bibr"" target=""#b31"">(Vendrov et al., 2015)</ref> 15M 98.8 81.4 (4) 300D tree-based CNN encoders <ref type=""bibr"" target=""#b19"">(Mou et al.,",0
"NLI) is concerned with determining whether a naturallanguage hypothesis h can be inferred from a premise p, as depicted in the following example from <ref type=""bibr"" target=""#b17"">MacCartney (2009)</ref>, where the hypothesis is regarded to be entailed from the premise. p: Several airlines polled s "">Related Work</head><p>Early work on natural language inference has been performed on rather small datasets with more conventional methods (refer to <ref type=""bibr"" target=""#b17"">MacCartney (2009)</ref> for a good literature survey), which includes a large bulk of work on recognizing textual entai",0
""" target=""#b17"">MacCartney (2009)</ref> for a good literature survey), which includes a large bulk of work on recognizing textual entailment, such as <ref type=""bibr"" target=""#b9"">(Dagan et al., 2005;</ref><ref type=""bibr"" target=""#b12"">Iftene and Balahur-Dobrescu, 2007)</ref>, among others. More re",0
"model.</p><p>Future work interesting to us includes exploring the usefulness of external resources such as Word-Net and contrasting-meaning embedding <ref type=""bibr"" target=""#b5"">(Chen et al., 2015)</ref> to help increase the coverage of wordlevel inference relations. Modeling negation more closely",0
"ature survey), which includes a large bulk of work on recognizing textual entailment, such as <ref type=""bibr"" target=""#b9"">(Dagan et al., 2005;</ref><ref type=""bibr"" target=""#b12"">Iftene and Balahur-Dobrescu, 2007)</ref>, among others. More recently, <ref type=""bibr"" target=""#b1"">Bowman et al. (201",0
"p and its context. Note that we used LSTM memory blocks in our models. We examined other recurrent memory blocks such as GRUs (Gated Recurrent Units) <ref type=""bibr"" target=""#b7"">(Cho et al., 2014)</ref> and they are inferior to LSTMs on the heldout set for our NLI task.</p><p>As discussed above, i",0
"j), ∀j ∈ [1, . . . , b ].</formula><p>(2)</p><p>Due to the space limit, we will skip the description of the basic chain LSTM and readers can refer to <ref type=""bibr"" target=""#b11"">Hochreiter and Schmidhuber (1997)</ref> for details. Briefly, when modeling a sequence, an LSTM employs a set of soft g",0
"n et al., 2015)</ref> to help increase the coverage of wordlevel inference relations. Modeling negation more closely within neural network frameworks <ref type=""bibr"" target=""#b29"">(Socher et al., 2013;</ref><ref type=""bibr"" target=""#b34"">Zhu et al., 2014)</ref> may help contradiction detection.</p>",0
"irs. They also experimented with simple classification models as well as simple neural networks that encode the premise and hypothesis independently. <ref type=""bibr"" target=""#b25"">Rocktäschel et al. (2015)</ref> proposed neural attention-based models for NLI, which captured the attention informatio the previous models.</p><p>(8)-( <ref type=""formula"" target=""#formula_14"">15</ref>), are inter-sentence attention-based model. The model marked with <ref type=""bibr"" target=""#b25"">Rocktäschel et al. (2015)</ref> is LSTMs enforcing the so called word-by-word attention. The model of <ref type=""bibr"" get=""#b19"">Mou et al., 2016;</ref><ref type=""bibr"" target=""#b16"">Liu et al., 2016;</ref><ref type=""bibr"" target=""#b20"">Munkhdalai and Yu, 2016a;</ref><ref type=""bibr"" target=""#b25"">Rocktäschel et al., 2015;</ref><ref type=""bibr"" target=""#b32"">Wang and Jiang, 2016;</ref><ref type=""bibr"" target=""#b6""> .8M 84.5 84.2 (7) 300D NSE encoders <ref type=""bibr"" target=""#b20"">(Munkhdalai and Yu, 2016a)</ref> 3.0M 86.2 84.6</p><p>(8) 100D LSTM with attention <ref type=""bibr"" target=""#b25"">(Rocktäschel et al., 2015)</ref> 250K 85.3 83.5 (9) 300D mLSTM <ref type=""bibr"" target=""#b32"">(Wang and Jiang, 2016)</r",0
"could well involve both, which has been discussed in the context of recognizing textual entailment (RTE) <ref type=""bibr"">(Mehdad et al., 2010;</ref><ref type=""bibr"" target=""#b10"">Ferrone and Zanzotto, 2014)</ref>. In this paper, we are interested in exploring this within the neural network framewo",0
"naturally applied to network representation learning, most notably the group of NLP models known as word2vec <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b17"">18]</ref>. A number of recent research publications have proposed word2vec-based network representation learning framew text corpus, Mikolov et al. proposed word2vec to learn the distributed representations of words in a corpus <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b17"">18]</ref>. Inspired by it, DeepWalk <ref type=""bibr"" target=""#b21"">[22]</ref> and node2vec <ref type=""bibr"" target=""#b7 tapath2vec and metapath2vec++ methods can be parallelized by using the same mechanism as word2vec and node2vec <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b17"">18]</ref>. All codes are implemented in C and C++ and our experiments are conducted in a computing server with ad 12 (4 c framework-a two-layer neural network-to learn the distributed representations of words in natural language <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b17"">18]</ref>. Building on word2vec, Perozzi et al. suggested that the ""context"" of a node can be denoted by their co-occur terogeneous networks to generate heterogeneous neighborhoods with network semantics for various types of nodes. Second, we extend the skip-gram model <ref type=""bibr"" target=""#b17"">[18]</ref> to facilitate the modeling of geographically and semantically close nodes. Finally, we develop a heterogeneo zations (CMU &amp; MIT), as well as papers (e.g., p 2 &amp; p 3 ).</p><p>To achieve e cient optimization, Mikolov et al. introduced negative sampling <ref type=""bibr"" target=""#b17"">[18]</ref>, in which a relatively small set of words (nodes) are sampled from the corpus (network) for the construction given a network G = (V , E), the objective is to maximize the network probability in terms of local structures <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b21"">22]</ref>, that is:</p><formula xml:id=""formula_2"">arg max θ ∈V c ∈N ( ) p(c | type of nodes and p(c t | ; θ ) is commonly de ned as a so max function <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b23"">24]</ref>, that is:</p><formula xml:id=""formula_4"">p(c t | ; θ ) = e Xc t •X u",1
"t a large number of social and information networks are heterogeneous in nature, involving diversity of node types and/or relationships between nodes <ref type=""bibr"" target=""#b24"">[25]</ref>. ese heterogeneous networks present unique challenges that cannot be handled by representation learning mode ilarity search <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b35"">35]</ref>. In contrast to conventional meta-path-based methods <ref type=""bibr"" target=""#b24"">[25]</ref>, the advantage of latent-space representation learning lies in its ability to model similarities between nod o maximize the likelihood of preserving both the structures and semantics of a given heterogeneous network. In metapath2vec, we rst propose meta-path <ref type=""bibr"" target=""#b24"">[25]</ref> based random walks in heterogeneous networks to generate heterogeneous neighborhoods with network semantics → V t +1 • • • R l −1 − −−− → V l , wherein R = R 1 • R 2 • • • • • R l −1 de</formula><p>nes the composite relations between node types V 1 and V l <ref type=""bibr"" target=""#b24"">[25]</ref>. Take Figure <ref type=""figure"">2</ref>(a) as an example, a meta-path ""APA"" represents the coauthor relation most commonly and e ectively used meta-path schemes in heterogeneous academic networks are ""APA"" and ""APVPA"" <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b24"">[25]</ref><ref type=""bibr"" target=""#b25"">[26]</ref><ref type=""bibr"" target=""#b26"">[27]</ref>. Notice that ""APA"" denotes rks, which was rst brie y introduced in <ref type=""bibr"" target=""#b20"">[21]</ref>. In speci c, we leverage the de nition of heterogeneous networks in <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b26"">27]</ref> and present the learning problem with its inputs and outputs.</p><p> re-de ned meta-path P. In addition, meta-paths are commonly used in a symmetric way, that is, its rst node type V 1 is the same with the last one V l <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b27"">28]</ref>, facilitating its recursive n that many data mining tasks in heterogeneous information networks can bene t from the modeling of meta-paths <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b26"">27]</ref>.   </p><formula xml:id=""formula_7"">V = V V ∪ V A ∪ V O ∪ V P . k t sp",1
"e=""bibr"" target=""#b34"">34]</ref>, such as the application of factorization models for recommendation systems <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b15"">16]</ref>, node classi cation <ref type=""bibr"" target=""#b32"">[32]</ref>, relational mining <ref type=""bibr"" target=""#b1",0
"e-venue) and restrain it as an unsupervised embedding method; (4) Spectral Clustering <ref type=""bibr"" target=""#b33"">[33]</ref> / Graph Factorization <ref type=""bibr"" target=""#b1"">[2]</ref>: With the same treatment to these methods in node2vec <ref type=""bibr"" target=""#b7"">[8]</ref>, we exclude them",0
"k embeddings can be further applied to various network mining tasks, such as node classi cation <ref type=""bibr"" target=""#b12"">[13]</ref>, clustering <ref type=""bibr"" target=""#b26"">[27,</ref><ref type=""bibr"" target=""#b27"">28]</ref>, and similarity search <ref type=""bibr"" target=""#b25"">[26,</ref><ref ref type=""bibr"" target=""#b20"">[21]</ref>. In speci c, we leverage the de nition of heterogeneous networks in <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b26"">27]</ref> and present the learning problem with its inputs and outputs.</p><p>De nition 2.1. A Heterogeneous Network is eneous information networks can bene t from the modeling of meta-paths <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b26"">27]</ref>.   </p><formula xml:id=""formula_7"">V = V V ∪ V A ∪ V O ∪ V P . k t speci</formula><formula xml:id=""formula_8"" orks are ""APA"" and ""APVPA"" <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b24"">[25]</ref><ref type=""bibr"" target=""#b25"">[26]</ref><ref type=""bibr"" target=""#b26"">[27]</ref>. Notice that ""APA"" denotes the coauthor semantic, that is, the traditional (homogeneous) collaboration links ree classical heterogeneous network mining tasks, including multi-class node classi cation <ref type=""bibr"" target=""#b12"">[13]</ref>, node clustering <ref type=""bibr"" target=""#b26"">[27]</ref>, and similarity search <ref type=""bibr"" target=""#b25"">[26]</ref>. In addition, we also use the embedding pro",0
"TED WORK</head><p>Network representation learning can be traced back to the usage of latent factor models for network analysis and graph mining tasks <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b34"">34]</ref>, such as the application of factorization models for recommendation s",0
"denotes 's neighborhood with the t t h type of nodes and p(c t | ; θ ) is commonly de ned as a so max function <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b23"">24]</ref>, that is:</p><formula xml:id=""f",0
"ving these challenges, the latent heterogeneous network embeddings can be further applied to various network mining tasks, such as node classi cation <ref type=""bibr"" target=""#b12"">[13]</ref>, clustering <ref type=""bibr"" target=""#b26"">[27,</ref><ref type=""bibr"" target=""#b27"">28]</ref>, and similarit latent representations learned by di erent methods over three classical heterogeneous network mining tasks, including multi-class node classi cation <ref type=""bibr"" target=""#b12"">[13]</ref>, node clustering <ref type=""bibr"" target=""#b26"">[27]</ref>, and similarity search <ref type=""bibr"" target=""#",0
"eworks for heterogeneous network representation learning. Data. We use two heterogeneous networks, including the AMiner Computer Science (CS) dataset <ref type=""bibr"" target=""#b31"">[31]</ref> and the Database and Information Systems (DBIS) dataset <ref type=""bibr"" target=""#b25"">[26]</ref>. Both data",0
"meta-path-based work and found that the most commonly and e ectively used meta-path schemes in heterogeneous academic networks are ""APA"" and ""APVPA"" <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b24"">[25]</ref><ref type=""bibr"" target=""#b25"">[26]</ref><ref type=""bibr"" target=""#b",0
"mating the probability of the current player winning from position s. This neural network combines the roles of both policy network and value network <ref type=""bibr"" target=""#b11"">12</ref> into a single architecture. The neural network consists of many residual blocks <ref type=""bibr"" target=""#b3""> is most directly applicable to zero-sum games of perfect information. We follow the formalism of alternating Markov games described in previous work <ref type=""bibr"" target=""#b11"">12</ref> , noting that algorithms based on value or policy iteration extend naturally to this setting <ref type=""bibr"" c.org/ns/1.0""><head>AlphaGo versions</head><p>We compare three distinct versions of AlphaGo:</p><p>1. AlphaGo Fan is the previously published program <ref type=""bibr"" target=""#b11"">12</ref> that played against Fan Hui in October 2015. This program was distributed over many machines using 176 GPUs.</ rcement learning algorithm, and MCTS algorithm as described in this paper. However, it uses the same handcrafted features and rollouts as AlphaGo Lee <ref type=""bibr"" target=""#b11"">12</ref> and training was initialised by supervised learning from human data.</p><p>4. AlphaGo Zero is the program desc architecture, and by using a low weight on the value component, it was possible to avoid overfitting to the values (a problem described in prior work <ref type=""bibr"" target=""#b11"">12</ref> ). After 72 hours the move prediction accuracy exceeded the state of the art reported in previous work <ref ty ibr"" target=""#b32"">[33]</ref> , reaching 60.4% on the KGS test set; the value prediction error was also substantially better than previously reported <ref type=""bibr"" target=""#b11"">12</ref> . The validation set was composed of professional games from GoKifu. Accuracies and mean squared errors are re also included in the tournament. The Elo ratings of AlphaGo Fan, Crazy Stone, Pachi and GnuGo were anchored to the tournament values from prior work <ref type=""bibr"" target=""#b11"">12</ref> , and correspond to the players reported in that work. The results of the matches of AlphaGo Fan against Fan H ing self-play training. Further evaluations were also performed against baseline players with Elo ratings anchored to the previously published values <ref type=""bibr"" target=""#b11"">12</ref> .</p><p>We measured the head-to-head performance of AlphaGo Zero against AlphaGo Lee, and the 40 block instanc rom the root state and iteratively selects moves that maximise an upper confidence bound Q(s, a) + U (s, a), where U (s, a) ∝ P (s, a)/(1 + N (s, a)) <ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b23"">24</ref> , until a leaf node s is encountered. This leaf position is expanded a r work <ref type=""bibr"" target=""#b11"">12</ref> ). After 72 hours the move prediction accuracy exceeded the state of the art reported in previous work <ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b29"">[30]</ref><ref type=""bibr"" target=""#b30"">[31]</ref><ref type=""bibr"" target=""#b3",1
"reinforcement learning algorithm is to use these search operators repeatedly in a policy iteration procedure <ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b22"">23</ref> : the neural network's parameters are updated to make the move probabilities and value (p, v) = f θ (s) more c In large state spaces, approximations are necessary to evaluate each policy and to represent its improvement <ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b22"">23</ref> .</p><p>Classification-based reinforcement learning <ref type=""bibr"" target=""#b36"">37</ref> improves the polic",0
"ict the winner of games played by the policy network against itself. Once trained, these networks were combined with a Monte-Carlo Tree Search (MCTS) <ref type=""bibr"" target=""#b12"">[13]</ref><ref type=""bibr"" target=""#b13"">[14]</ref><ref type=""bibr"" target=""#b14"">[15]</ref> to provide a lookahead sea",0
"icated architecture based on Go knowledge regarding connectivity, territory and eyes. This neural network was trained by temporal-difference learning <ref type=""bibr"" target=""#b41"">42</ref> to predict territory in games of self-play, building on prior work <ref type=""bibr"" target=""#b42"">43</ref> . A",0
"in previous work <ref type=""bibr"" target=""#b11"">12</ref> , noting that algorithms based on value or policy iteration extend naturally to this setting <ref type=""bibr"" target=""#b38"">39</ref> .</p><p>Self-play reinforcement learning has previously been applied to the game of Go. NeuroGo 40, 41 used a",0
"rget=""#b21"">23,</ref><ref type=""bibr"" target=""#b29"">31]</ref>. As the pioneer CNN model for SR, Super-Resolution Convolutional Neural Network (SRCNN) <ref type=""bibr"" target=""#b1"">[2]</ref> predicts the nonlinear LR-HR mapping via a fully convolutional network, and significantly outperforms classica m LR to HR only at the end of the network. One commonality among the above CNN models is that their networks contain fewer than 5 layers, e.g., SRCNN <ref type=""bibr"" target=""#b1"">[2]</ref> uses 3 convolutional layers. Their deeper structures with 4 or 5 layers do not achieve better performance, whi </ref> introduces GRL, i.e., residual learning between the input ILR image and the output HR image. There are three notes for VDSR: (1) Un-like SRCNN <ref type=""bibr"" target=""#b1"">[2]</ref> that only uses 3 layers, VDSR stacks 20 weight layers (3 × 3 for each layer) in the residual branch, which lea eptual scores for SR evaluation <ref type=""bibr"" target=""#b32"">[34]</ref>. The results are presented in Tab. 3. Note that Dataset Scale Bicubic SRCNN <ref type=""bibr"" target=""#b1"">[2]</ref> SelfEx <ref type=""bibr"" target=""#b9"">[10]</ref> RFL <ref type=""bibr"" target=""#b21"">[23]</ref>   the results of plementation achieves similar benchmark performance as <ref type=""bibr"" target=""#b12"">[13]</ref> reported in Tab. Qualitative comparisons among SRCNN <ref type=""bibr"" target=""#b1"">[2]</ref>, SelfEx <ref type=""bibr"" target=""#b9"">[10]</ref>, VDSR <ref type=""bibr"" target=""#b12"">[13]</ref> and DRRN are , NBSRF <ref type=""bibr"" target=""#b20"">[22]</ref>, PSyCo [20] and IA <ref type=""bibr"" target=""#b28"">[30]</ref>. The deep models (d ≤ 8) include SRCNN <ref type=""bibr"" target=""#b1"">[2]</ref>, DJSR <ref type=""bibr"" target=""#b31"">[33]</ref>, CSCN <ref type=""bibr"" target=""#b30"">[32]</ref>, ESPCN <ref ty rained end-to-end to fully exploit the natural sparsity of images. Shi et al. <ref type=""bibr"" target=""#b23"">[25]</ref> observe that the prior models <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b30"">32]</ref> increase LR image's resolution via bicubic interpolation be-fore CNN l in DRRN, Fig. <ref type=""figure"" target=""#fig_0"">1</ref> shows the Peak Signal-to-Noise Ratio (PSNR) performance of several recent CNN models for SR <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b16""> comparison, we also construct a DRRN B1U9 (d = 20, k = 297K) structure, which has the same depth as VDSR and DRCN, but fewer parameters. Both the DL <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b13"">14]</ref> and non-DL <ref type=""bibr"" ta all models are applied to its luminance component only. Therefore, the input and output images are of the same size. For fair comparison, similar to <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b21""> ref type=""bibr"" target=""#b1"">[2]</ref> SelfEx <ref type=""bibr"" target=""#b9"">[10]</ref> RFL <ref type=""bibr"" target=""#b21"">[23]</ref>   the results of <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"">20,</ref><ref type=""bibr"" target=""#b21"">23]</ref> are c",1
"ng deeper networks and led to the observation that ""the deeper the better"" might not be the case in SR. Inspired by the success of very deep networks <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b25"">27,</ref><ref type=""bibr"" target=""#b26"">28]</ref> on ImageNet <ref type=""bibr"" t dopt GRL in our identity branch. Further, very deep networks could suffer from the performance degradation problem, as observed in visual recognition <ref type=""bibr"" target=""#b7"">[8]</ref> and image restoration <ref type=""bibr"" target=""#b16"">[17]</ref>. The reason may be a significant amount of ima .org/ns/1.0""><head n=""2."">Related Work</head><p>Since Sec. 1 overviews DL-based SISR, this section focuses on three most related work to ours: ResNet <ref type=""bibr"" target=""#b7"">[8]</ref>, VDSR <ref type=""bibr"" target=""#b12"">[13]</ref> and DRCN <ref type=""bibr"" target=""#b13"">[14]</ref>. Fig. <ref ""#b18"">[19]</ref>, are omitted for clarity.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2.1."">ResNet</head><p>The main idea of ResNet <ref type=""bibr"" target=""#b7"">[8]</ref> is to use a residual learning framework to ease the training of very deep networks. Instead of hoping every fe lobal residual learning + recursive learning (multiple weight layers in the residual unit) Table <ref type=""table"">1</ref>. Strategies used in ResNet <ref type=""bibr"" target=""#b7"">[8]</ref>, VDSR <ref type=""bibr"" target=""#b12"">[13]</ref>, DRCN <ref type=""bibr"" target=""#b13"">[14]</ref> and DRRN. U, d :id=""formula_1"">x = U (x) = σ(F(x, W ) + h(x)),<label>(1)</label></formula><p>where x is the output of the residual unit, h(x) is an identity mapping <ref type=""bibr"" target=""#b7"">[8]</ref> : h(x) = x, W is a set of weights (the biases are omitted to simplify notations), function σ denotes ReLU, F(x ve learning into the residual branch by constructing the recursive block structure, in which several residual units are stacked. Noted that in ResNet <ref type=""bibr"" target=""#b7"">[8]</ref>, different residual units use different inputs for the identity branch (green dashed boxes in Fig. <ref type="" ive block and finally the whole network structure.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.1."">Residual Unit</head><p>In ResNet <ref type=""bibr"" target=""#b7"">[8]</ref>, the basic residual unit is formulated as Eq. 1 and the activation functions (BN <ref type=""bibr"" target=""#b10 igure xmlns=""http://www.tei-c.org/ns/1.0"" xml:id=""fig_1""><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Simplified structures of (a) ResNet<ref type=""bibr"" target=""#b7"">[8]</ref>. The green dashed box means a residual unit. (b) VDSR<ref type=""bibr"" target=""#b12"">[13]</ref>. The purple lin",1
"ch 9 residual units are stacked. With the same depth but far fewer parameters, DRRN B1U9 achieves better performance than the state-of-theart methods <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b13"">14]</ref>. After increasing the depth without adding any parameters, the 52-la ef><ref type=""bibr"" target=""#b25"">27,</ref><ref type=""bibr"" target=""#b26"">28]</ref> on ImageNet <ref type=""bibr"" target=""#b19"">[21]</ref>, Kim et al. <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b13"">14]</ref> propose two very deep convolutional networks for SR, both stacking 2 ://www.tei-c.org/ns/1.0""><head n=""4."">Experiments</head></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.1."">Datasets</head><p>By following <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b21"">23]</ref>, we use a training dataset of 291 images, where 91 images are from Y ough this is unnecessary for DRRN.</p><p>Tab. 2 summarizes quantitative results on the four testing sets, by citing the results of prior methods from <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b13"">14]</ref>. The two DRRN models outperforms all existing methods in all dataset ich 9 residual units are stacked. With the same depth but far fewer parameters, DRRN B1U9 achieves better performance than the state-of-theart methods<ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b13"">14]</ref>. After increasing the depth without adding any parameters, the 52-la After increasing the depth without adding any parameters, the 52-layer DRRN B1U25 further improves the performance and significantly outperforms VDSR <ref type=""bibr"" target=""#b12"">[13]</ref>, DRCN <ref type=""bibr"" target=""#b13"">[14]</ref> and RED30 <ref type=""bibr"" target=""#b16"">[17]</ref> by 0.37, the viewpoints of training efficiency and storage, respectively. On the one hand, to accelerate the convergence speed of very deep networks, the VDSR <ref type=""bibr"" target=""#b12"">[13]</ref> is trained with a very high learning rate (10 −1 , instead of 10 −4 in SRCNN) and the authors further use re p>Since Sec. 1 overviews DL-based SISR, this section focuses on three most related work to ours: ResNet <ref type=""bibr"" target=""#b7"">[8]</ref>, VDSR <ref type=""bibr"" target=""#b12"">[13]</ref> and DRCN <ref type=""bibr"" target=""#b13"">[14]</ref>. Fig. <ref type=""figure"" target=""#fig_1"">2</ref> illustra ping, which is assumed to be easier for optimization. Denoting the input as x and the underlying mapping as H(x), the residual mapping is defined as  <ref type=""bibr"" target=""#b12"">[13]</ref>. The purple line refers to a global identity mapping. (c) DRCN <ref type=""bibr"" target=""#b13"">[14]</ref>. Th multiple weight layers in the residual unit) Table <ref type=""table"">1</ref>. Strategies used in ResNet <ref type=""bibr"" target=""#b7"">[8]</ref>, VDSR <ref type=""bibr"" target=""#b12"">[13]</ref>, DRCN <ref type=""bibr"" target=""#b13"">[14]</ref> and DRRN. U, d, T, and B are the numbers of residual units i xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2.2."">VDSR</head><p>Differing from ResNet that uses residual learning in every few stacked layers, VDSR <ref type=""bibr"" target=""#b12"">[13]</ref> introduces GRL, i.e., residual learning between the input ILR image and the output HR image. There are three pping from the input LR image. The entire network structure of DRRN is illustrated in Fig. <ref type=""figure"" target=""#fig_4"">5</ref>. Actually, VDSR <ref type=""bibr"" target=""#b12"">[13]</ref> can be viewed as a special case of DRRN, i.e., when U = 0, DRRN becomes VDSR.</p><p>DRRN has two key paramet • , 180 • , 270 • and flip them horizontally. After that, for each original image, we have 7 additional augmented versions. Besides, inspired by VDSR <ref type=""bibr"" target=""#b12"">[13]</ref>, we also use scale augmentation to train our model, and images with different scales (×2, ×3 and ×4) are all is set to 0.1 and then decreased to half every 10 epochs. Since a large learning rate is used in our work, we adopt the adjustable gradient clipping <ref type=""bibr"" target=""#b12"">[13]</ref> to boost the convergence rate while suppressing exploding gradients. Specifically, the gradients are clipped e from our re-implementation. Similar to DRRN, the VDSR re-implementation also uses BN and ReLU as the activation functions, unlike the original VDSR <ref type=""bibr"" target=""#b12"">[13]</ref> that does not use BN. These results are faithful since our VDSR re-implementation achieves similar benchmark target=""#b12"">[13]</ref> that does not use BN. These results are faithful since our VDSR re-implementation achieves similar benchmark performance as <ref type=""bibr"" target=""#b12"">[13]</ref> reported in Tab. Qualitative comparisons among SRCNN <ref type=""bibr"" target=""#b1"">[2]</ref>, SelfEx <ref ty > reported in Tab. Qualitative comparisons among SRCNN <ref type=""bibr"" target=""#b1"">[2]</ref>, SelfEx <ref type=""bibr"" target=""#b9"">[10]</ref>, VDSR <ref type=""bibr"" target=""#b12"">[13]</ref> and DRRN are illustrated in Fig. <ref type=""figure"" target=""#fig_6"">7</ref>. For SRCNN and SelfEx, we use th lts.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.5."">Discussions</head><p>Since global residual learning has been well discussed in <ref type=""bibr"" target=""#b12"">[13]</ref>, in this section, we mainly focus on local residual learning (LRL), recursive learning and multi-path struct rning (LRL), recursive learning and multi-path structure. Local Residual Learning To demonstrate the effectiveness of LRL, DRRN is compared with VDSR <ref type=""bibr"" target=""#b12"">[13]</ref>, which has no LRL. For fair comparison, the depth and number of parameters are kept the same for both method 32]</ref>, ESPCN <ref type=""bibr"" target=""#b23"">[25]</ref> and FSRCNN <ref type=""bibr"" target=""#b2"">[3]</ref>. Very deep models (d ≥ 20) include VDSR <ref type=""bibr"" target=""#b12"">[13]</ref>, DRCN <ref type=""bibr"" target=""#b13"">[14]</ref>, RED <ref type=""bibr"" target=""#b16"">[17]</ref> and DRRN with After increasing the depth without adding any parameters, the 52-layer DRRN B1U25 further improves the performance and significantly outperforms VDSR<ref type=""bibr"" target=""#b12"">[13]</ref>, DRCN<ref type=""bibr"" target=""#b13"">[14]</ref> and RED30<ref type=""bibr"" target=""#b16"">[17]</ref> by 0.37, 0 el><figDesc>Figure 2. Simplified structures of (a) ResNet<ref type=""bibr"" target=""#b7"">[8]</ref>. The green dashed box means a residual unit. (b) VDSR<ref type=""bibr"" target=""#b12"">[13]</ref>. The purple line refers to a global identity mapping. (c) DRCN<ref type=""bibr"" target=""#b13"">[14]</ref>. The t=""#fig_0"">1</ref> shows the Peak Signal-to-Noise Ratio (PSNR) performance of several recent CNN models for SR <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b23"" 1U9 (d = 20, k = 297K) structure, which has the same depth as VDSR and DRCN, but fewer parameters. Both the DL <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b13"">14]</ref> and non-DL <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"">2 e component only. Therefore, the input and output images are of the same size. For fair comparison, similar to <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b21"">23]</ref>, we crop pixels near image bo",1
"same depth but far fewer parameters, DRRN B1U9 achieves better performance than the state-of-theart methods <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b13"">14]</ref>. After increasing the depth without adding any parameters, the 52-layer DRRN B1U25 further improves the perfo f><ref type=""bibr"" target=""#b26"">28]</ref> on ImageNet <ref type=""bibr"" target=""#b19"">[21]</ref>, Kim et al. <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b13"">14]</ref> propose two very deep convolutional networks for SR, both stacking 20 convolutional layers, from the viewpoin has the same depth as VDSR and DRCN, but fewer parameters. Both the DL <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b13"">14]</ref> and non-DL <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"">20,</ref><ref type=""bibr"" target=""#b21"">2 Tab. 2 summarizes quantitative results on the four testing sets, by citing the results of prior methods from <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b13"">14]</ref>. The two DRRN models outperforms all existing methods in all datasets and scale factors, in both PSNR and Str e same depth but far fewer parameters, DRRN B1U9 achieves better performance than the state-of-theart methods<ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b13"">14]</ref>. After increasing the depth without adding any parameters, the 52-layer DRRN B1U25 further improves the perfo rameters, the 52-layer DRRN B1U25 further improves the performance and significantly outperforms VDSR <ref type=""bibr"" target=""#b12"">[13]</ref>, DRCN <ref type=""bibr"" target=""#b13"">[14]</ref> and RED30 <ref type=""bibr"" target=""#b16"">[17]</ref> by 0.37, 0.21 and 0.21 dB respectively.</p><p>posed inve t clipping to solve gradient explosion problem. On the other hand, to control the model parameters, the Deeply-Recursive Convolutional Network (DRCN) <ref type=""bibr"" target=""#b13"">[14]</ref> introduces a very deep recursive layer via a chain structure with up to 16 recursions. To mitigate the diffi mages, i.e., DRRN has many LRLs and only 1 GRL.</p><p>(2) Recursive learning of residual units is proposed in DRRN to keep our model compact. In DRCN <ref type=""bibr"" target=""#b13"">[14]</ref>, a deep recursive layer (up to 16 convolutional recursions) is learned and the weights are shared in the 16 n focuses on three most related work to ours: ResNet <ref type=""bibr"" target=""#b7"">[8]</ref>, VDSR <ref type=""bibr"" target=""#b12"">[13]</ref> and DRCN <ref type=""bibr"" target=""#b13"">[14]</ref>. Fig. <ref type=""figure"" target=""#fig_1"">2</ref> illustrates these models via simplified network structures as H(x), the residual mapping is defined as  <ref type=""bibr"" target=""#b12"">[13]</ref>. The purple line refers to a global identity mapping. (c) DRCN <ref type=""bibr"" target=""#b13"">[14]</ref>. The blue dashed box refers to a recursive layer, among which the convolutional layers (with light green col le <ref type=""table"">1</ref>. Strategies used in ResNet <ref type=""bibr"" target=""#b7"">[8]</ref>, VDSR <ref type=""bibr"" target=""#b12"">[13]</ref>, DRCN <ref type=""bibr"" target=""#b13"">[14]</ref> and DRRN. U, d, T, and B are the numbers of residual units in ResNet, convolutional layers in VDSR, recursio e of DRRN, when there's no residual unit in our recursive block.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2.3."">DRCN</head><p>DRCN <ref type=""bibr"" target=""#b13"">[14]</ref> is motivated by the observation that adding more weight layers introduces more parameters, where the model i respectively. The embedding net f 1 (x) represents a given image x as feature maps H 0 . The inference net f 2 (H 0 ) stacks T recursions (T = 16 in <ref type=""bibr"" target=""#b13"">[14]</ref>) in a recursive layer, with shared weights among these recursions. Finally, the reconstruction net f Rec (H [25]</ref> and FSRCNN <ref type=""bibr"" target=""#b2"">[3]</ref>. Very deep models (d ≥ 20) include VDSR <ref type=""bibr"" target=""#b12"">[13]</ref>, DRCN <ref type=""bibr"" target=""#b13"">[14]</ref>, RED <ref type=""bibr"" target=""#b16"">[17]</ref> and DRRN with d = 20 and 52. Fig. <ref type=""figure"" target="" parameters, the 52-layer DRRN B1U25 further improves the performance and significantly outperforms VDSR<ref type=""bibr"" target=""#b12"">[13]</ref>, DRCN<ref type=""bibr"" target=""#b13"">[14]</ref> and RED30<ref type=""bibr"" target=""#b16"">[17]</ref> by 0.37, 0.21 and 0.21 dB respectively.</figDesc></figure een dashed box means a residual unit. (b) VDSR<ref type=""bibr"" target=""#b12"">[13]</ref>. The purple line refers to a global identity mapping. (c) DRCN<ref type=""bibr"" target=""#b13"">[14]</ref>. The blue dashed box refers to a recursive layer, among which the convolutional layers (with light green col -to-Noise Ratio (PSNR) performance of several recent CNN models for SR <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b23"">25,</ref><ref type=""bibr"" target=""#b30"" nd output images are of the same size. For fair comparison, similar to <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b21"">23]</ref>, we crop pixels near image boundary before evaluation, although this",1
"uctures with only 6 convolutional layers, where the activation functions, batch normalization (BN) <ref type=""bibr"" target=""#b10"">[11]</ref> and ReLU <ref type=""bibr"" target=""#b18"">[19]</ref>, are omitted for clarity.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2.1."">ResNet</head><p> t=""#b7"">[8]</ref>, the basic residual unit is formulated as Eq. 1 and the activation functions (BN <ref type=""bibr"" target=""#b10"">[11]</ref> and ReLU <ref type=""bibr"" target=""#b18"">[19]</ref>) are performed after the weight layers. In contrast to such a ""post-activation"" structure, He et al. <ref ty",0
") models, especially Convolutional Neural Networks (CNN), are widely used to address the ill- . PSNR of recent CNN models for scale factor ×3 on Set5 <ref type=""bibr"" target=""#b0"">[1]</ref>. Red points are our models. △, ✩, and • are models with less than 5 layers, 20 layers, and more than 30 layers s are from Berkeley Segmentation Dataset <ref type=""bibr"" target=""#b17"">[18]</ref>. For testing, we utilize four widely used benchmark datasets, Set5 <ref type=""bibr"" target=""#b0"">[1]</ref>, Set14 <ref type=""bibr"" target=""#b34"">[36]</ref>, BSD100 <ref type=""bibr"" target=""#b17"">[18]</ref> and Urban10 tp://www.tei-c.org/ns/1.0"" xml:id=""fig_0""><head>Figure 1</head><label>1</label><figDesc>Figure1. PSNR of recent CNN models for scale factor ×3 on Set5<ref type=""bibr"" target=""#b0"">[1]</ref>. Red points are our models. △, ✩, and • are models with less than 5 layers, 20 layers, and more than 30 layers",0
"(IFC) <ref type=""bibr"" target=""#b22"">[24]</ref> for comparison, which claims to have the highest correlation with perceptual scores for SR evaluation <ref type=""bibr"" target=""#b32"">[34]</ref>. The results are presented in Tab. 3. Note that Dataset Scale Bicubic SRCNN <ref type=""bibr"" target=""#b1"">[2",0
"ery deep networks <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b25"">27,</ref><ref type=""bibr"" target=""#b26"">28]</ref> on ImageNet <ref type=""bibr"" target=""#b19"">[21]</ref>, Kim et al. <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b13"">14]</ref> propose two ve",0
"ef type=""bibr"" target=""#b0"">[1]</ref>, Set14 <ref type=""bibr"" target=""#b34"">[36]</ref>, BSD100 <ref type=""bibr"" target=""#b17"">[18]</ref> and Urban100 <ref type=""bibr"" target=""#b9"">[10]</ref>, which have 5, 14, 100 and 100 images respectively.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head ctors, in both PSNR and Structural SIMilarity (SSIM) <ref type=""foot"" target=""#foot_0"">1</ref> . Especially on the recent difficult Ur-ban100 dataset <ref type=""bibr"" target=""#b9"">[10]</ref>, DRRN significantly advances the state of the art, with the improvement margin of 0.47, 0.38, and 0.26 dB on "" target=""#b32"">[34]</ref>. The results are presented in Tab. 3. Note that Dataset Scale Bicubic SRCNN <ref type=""bibr"" target=""#b1"">[2]</ref> SelfEx <ref type=""bibr"" target=""#b9"">[10]</ref> RFL <ref type=""bibr"" target=""#b21"">[23]</ref>   the results of <ref type=""bibr"" target=""#b1"">[2,</ref><ref ty nce as <ref type=""bibr"" target=""#b12"">[13]</ref> reported in Tab. Qualitative comparisons among SRCNN <ref type=""bibr"" target=""#b1"">[2]</ref>, SelfEx <ref type=""bibr"" target=""#b9"">[10]</ref>, VDSR <ref type=""bibr"" target=""#b12"">[13]</ref> and DRRN are illustrated in Fig. <ref type=""figure"" target=""# that report PSNR for scale factor ×3 on datasets Set5 and Set14. Shallow (non-DL) models include A+ <ref type=""bibr"" target=""#b29"">[31]</ref>, SelfEx <ref type=""bibr"" target=""#b9"">[10]</ref>, RFL <ref type=""bibr"" target=""#b21"">[23]</ref>, NBSRF <ref type=""bibr"" target=""#b20"">[22]</ref>, PSyCo [20] a eters. Both the DL <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b13"">14]</ref> and non-DL <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"">20,</ref><ref type=""bibr"" target=""#b21"">23]</ref> methods in recent years are used for benchm SelfEx <ref type=""bibr"" target=""#b9"">[10]</ref> RFL <ref type=""bibr"" target=""#b21"">[23]</ref>   the results of <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"">20,</ref><ref type=""bibr"" target=""#b21"">23]</ref> are cited from [20]<ref type=""foot"" target=""",0
"alth of external examples and the power of self examples unique to the input. Inspired by the learning iterative shrinkage and thresholding algorithm <ref type=""bibr"" target=""#b4"">[5]</ref>, Cascaded Sparse Coding Network (CSCN) <ref type=""bibr"" target=""#b30"">[32]</ref> is trained end-to-end to full",0
"same input for the identity branch (green dashed boxes in Fig. <ref type=""figure"" target=""#fig_1"">2(d)</ref>), which further facilitates the learning <ref type=""bibr"" target=""#b15"">[16]</ref>. We highlight the differences of the network structures between DRRN and the related models in Tab. 1. Now, p gradient backpropagation during training. Compared to the chain mode, this multipath mode facilitates the learning and is less prone to overfitting <ref type=""bibr"" target=""#b15"">[16]</ref>. Therefore, we formulate our residual unit as</p><formula xml:id=""formula_5"">H u = G(H u−1 ) = F(H u−1 , W ) s of our recursive block. First, we illustrate the structure of our recursive blocks in Fig. <ref type=""figure"" target=""#fig_3"">4</ref>. Motivated by <ref type=""bibr"" target=""#b15"">[16]</ref>, we introduce a convolutional layer at the beginning of the recursive block, and then several residual units n limited training set (e.g., 291 images) is used, the recursive learning is indeed effective under the same structure, and less prone to overfitting <ref type=""bibr"" target=""#b15"">[16]</ref>.</p><p>Multi-Path Structure To demonstrate the effectiveness of multi-path structure, we compare DRRN with t",0
"nd weight decay to 10 −4 . Every weight layer has 128 filters of the size 3 × 3.</p><p>For weight initialization, we use the same method as He et al. <ref type=""bibr"" target=""#b6"">[7]</ref>, which is shown to be suitable for networks utilizing ReLU. The initial learning rate is set to 0.1 and then d",0
"a><p>where Θ denotes the parameter set. The objective function is optimized via the mini-batch stochastic gradient descent (SGD) with backpropagation <ref type=""bibr"" target=""#b14"">[15]</ref>. We implement DRRN via Caffe <ref type=""bibr"" target=""#b11"">[12]</ref>.</p></div> <div xmlns=""http://www.tei",0
"nspired by the learning iterative shrinkage and thresholding algorithm <ref type=""bibr"" target=""#b4"">[5]</ref>, Cascaded Sparse Coding Network (CSCN) <ref type=""bibr"" target=""#b30"">[32]</ref> is trained end-to-end to fully exploit the natural sparsity of images. Shi et al. <ref type=""bibr"" target=""# #b28"">[30]</ref>. The deep models (d ≤ 8) include SRCNN <ref type=""bibr"" target=""#b1"">[2]</ref>, DJSR <ref type=""bibr"" target=""#b31"">[33]</ref>, CSCN <ref type=""bibr"" target=""#b30"">[32]</ref>, ESPCN <ref type=""bibr"" target=""#b23"">[25]</ref> and FSRCNN <ref type=""bibr"" target=""#b2"">[3]</ref>. Very de natural sparsity of images. Shi et al. <ref type=""bibr"" target=""#b23"">[25]</ref> observe that the prior models <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b30"">32]</ref> increase LR image's resolution via bicubic interpolation be-fore CNN learning, which increases the computatio ""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b23"">25,</ref><ref type=""bibr"" target=""#b30"">32]</ref> versus the number of parameters, denoted as k. Compared to the prior CNN models, DRRN achieves the best perfo",0
"he high-frequency information, it is widely used in applications such as medical imaging <ref type=""bibr"" target=""#b24"">[26]</ref>, satellite imaging <ref type=""bibr"" target=""#b27"">[29]</ref>, security and surveillance <ref type=""bibr"" target=""#b35"">[37]</ref>, where high-frequency details are great",0
"ized via the mini-batch stochastic gradient descent (SGD) with backpropagation <ref type=""bibr"" target=""#b14"">[15]</ref>. We implement DRRN via Caffe <ref type=""bibr"" target=""#b11"">[12]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4."">Experiments</head></div> <div xmlns=""http:/",0
"://www.tei-c.org/ns/1.0""><head n=""4.2."">Implementation Details</head><p>Data augmentation is performed on the 291-image training dataset. Inspired by <ref type=""bibr"" target=""#b28"">[30]</ref>, the flipped and rotated versions of the training images are considered. Specifically, we rotate the origina ype=""bibr"" target=""#b9"">[10]</ref>, RFL <ref type=""bibr"" target=""#b21"">[23]</ref>, NBSRF <ref type=""bibr"" target=""#b20"">[22]</ref>, PSyCo [20] and IA <ref type=""bibr"" target=""#b28"">[30]</ref>. The deep models (d ≤ 8) include SRCNN <ref type=""bibr"" target=""#b1"">[2]</ref>, DJSR <ref type=""bibr"" target",0
"rse Coding Network (CSCN) <ref type=""bibr"" target=""#b30"">[32]</ref> is trained end-to-end to fully exploit the natural sparsity of images. Shi et al. <ref type=""bibr"" target=""#b23"">[25]</ref> observe that the prior models <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b30"">32]</ref SRCNN <ref type=""bibr"" target=""#b1"">[2]</ref>, DJSR <ref type=""bibr"" target=""#b31"">[33]</ref>, CSCN <ref type=""bibr"" target=""#b30"">[32]</ref>, ESPCN <ref type=""bibr"" target=""#b23"">[25]</ref> and FSRCNN <ref type=""bibr"" target=""#b2"">[3]</ref>. Very deep models (d ≥ 20) include VDSR <ref type=""bibr"" =""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b23"">25,</ref><ref type=""bibr"" target=""#b30"">32]</ref> versus the number of parameters, denoted as k. Compared to the prior",0
"illustrates these models via simplified network structures with only 6 convolutional layers, where the activation functions, batch normalization (BN) <ref type=""bibr"" target=""#b10"">[11]</ref> and ReLU <ref type=""bibr"" target=""#b18"">[19]</ref>, are omitted for clarity.</p></div> <div xmlns=""http://ww dual Unit</head><p>In ResNet <ref type=""bibr"" target=""#b7"">[8]</ref>, the basic residual unit is formulated as Eq. 1 and the activation functions (BN <ref type=""bibr"" target=""#b10"">[11]</ref> and ReLU <ref type=""bibr"" target=""#b18"">[19]</ref>) are performed after the weight layers. In contrast to su",0
"d ReLU <ref type=""bibr"" target=""#b18"">[19]</ref>) are performed after the weight layers. In contrast to such a ""post-activation"" structure, He et al. <ref type=""bibr"" target=""#b8"">[9]</ref> propose a ""preactivation"" structure, which performs the activation before the weight layers. They claim that t",0
"of BestConfig, we implement two PO algorithms, adopting the machine learning approach.</p><p>One is based on the COMT (Co-Training Model Tree) method <ref type=""bibr"" target=""#b16"">[17]</ref>, which assumes a linear relation between parameters and the performance. COMT divides the parameter space in",1
"rget=""#b32"">34,</ref><ref type=""bibr"" target=""#b33"">35]</ref>. It can also result in unsatisfactory performances under atypical application workloads <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b39""> do not fully address all the above challenges. Though sporadic proposals are found on automatically suggesting configuration settings for Web servers <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b39"">41,</ref><ref type=""bibr"" target=""#b41"">43]</ref>, databases <ref type=""bibr"" ta mples requires to test the SUT for many times. This is a highly costly process. A related work uses reinforcement learning in the same tuning problem <ref type=""bibr"" target=""#b5"">[6]</ref>. It formulates the performance optimization process as a finite Markov decision process (MDP), which consists",0
"onfiguration tuning involves solving a problem with a high-dimensional parameter space, thus a large sample set is commonly needed to find a solution <ref type=""bibr"" target=""#b15"">[16]</ref>. However, collecting a large set of performance-configuration samples is impractical for configuration tunin with high-dimensional parameters <ref type=""bibr"" target=""#b20"">[21]</ref>, but previous research typically studies the problem based on simulations <ref type=""bibr"" target=""#b15"">[16]</ref>; the overhead aspect is rarely considered to the extent as required by configuration tuning for general syst spaces generally assume the abundance of samples. For example, some solve the optimization problem with around 10 parameters using about 2000 samples <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b41"">43]</ref>. Except through simulations, it is too costly to collect such an amo model-based <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b37"">39]</ref> and search-based <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b39"">41,</ref><ref type=""bibr"" target=""#b41"">43]</ref>. In the design of BestConfig small number of parameters to tune, e.g., smart hill climbing <ref type=""bibr"" target=""#b39"">[41]</ref>, or require a huge number of initial testings <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b42"">44]</ref>, e.g., simulated annealing <ref type=""bibr"" target=""#b38"">[40]</ref>",0
"0, 2), to which the DDS method can be directly applied. We can map a sampled value within ranges of [0, 1) and <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2)</ref> respectively to the values of 0 or 1, which are equal to false and true respectively. Similar mappings can be c performance gained from the long tuning process is still worthwhile. Besides, as the benchmarking community has proved theoretically and practically <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b29"">30,</ref><ref type=""bibr"" target=""#b31"">",0
"=""bibr"" target=""#b39"">41]</ref>. In fact, configuration settings have strong impacts on the system performance <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"">32,</ref><ref type=""bibr"" target=""#b34"">36]</ref>. To tap the performance potential of a syst",0
"attractive solution to long-tail recognition is to transfer knowledge from data-rich head classes to data-poor tail classes. While transfer learning <ref type=""bibr"" target=""#b16"">[16,</ref><ref type=""bibr"" target=""#b17"">17,</ref><ref type=""bibr"" target=""#b18"">18]</ref> from a source to target task 16"">[16,</ref><ref type=""bibr"" target=""#b17"">17,</ref><ref type=""bibr"" target=""#b18"">18]</ref> from a source to target task is a well studied problem <ref type=""bibr"" target=""#b16"">[16,</ref><ref type=""bibr"" target=""#b19"">19]</ref>, by far the most common approach is fine-tuning a model pre-trained",1
"""bibr"" target=""#b43"">43,</ref><ref type=""bibr"" target=""#b22"">22,</ref><ref type=""bibr"" target=""#b44"">44,</ref><ref type=""bibr"" target=""#b45"">45,</ref><ref type=""bibr"" target=""#b46"">46,</ref><ref type=""bibr"" target=""#b39"">39,</ref><ref type=""bibr"" target=""#b47"">47,</ref><ref type=""bibr"" target=""#b48""",0
"""bibr"" target=""#b11"">11,</ref><ref type=""bibr"" target=""#b12"">12,</ref><ref type=""bibr"" target=""#b13"">13,</ref><ref type=""bibr"" target=""#b14"">14,</ref><ref type=""bibr"" target=""#b15"">15]</ref>.</p><p>Head-to-tail knowledge transfer: An attractive solution to long-tail recognition is to transfer knowle ceable improvement since pre-training on the head is quite similar to training on the unbalanced long-tailed dataset (which is dominated by the head) <ref type=""bibr"" target=""#b15"">[15]</ref>.</p><p>Transferring meta-knowledge: Inspired by the recent work on meta-learning <ref type=""bibr"" target=""#b",0
"rks (CNNs) have revolutionized the landscape of visual recognition, through the ability to learn ""big models"" with hundreds of millions of parameters <ref type=""bibr"" target=""#b1"">[1,</ref><ref type=""bibr"" target=""#b2"">2,</ref><ref type=""bibr"" target=""#b3"">3,</ref><ref type=""bibr"" target=""#b4"">4]</r underlying chain.</p><p>Training: Given the network structure defined above, we now describe an efficient method for training based on two insights. <ref type=""bibr"" target=""#b1"">(1)</ref> The recursive definition of MetaModelNet suggests a recursive strategy for training. We begin with the last bl",0
"""bibr"" target=""#b48"">48,</ref><ref type=""bibr"" target=""#b49"">49,</ref><ref type=""bibr"" target=""#b50"">50,</ref><ref type=""bibr"" target=""#b51"">51,</ref><ref type=""bibr"" target=""#b52"">52]</ref>) through estimating a generic model transformation. To do so, <ref type=""bibr"" target=""#b21"">[21]</ref> learn",0
"diction of those interactions, and the interfaces through which they occur, are important and challenging problems that have attracted much attention <ref type=""bibr"" target=""#b9"">[10]</ref>. This paper focuses on predicting protein interfaces. Despite the plethora of available methods for interface ently noted that ""The field in its current state appears to be saturated. This calls for new methodologies or sources of information to be exploited"" <ref type=""bibr"" target=""#b9"">[10]</ref>. Most machine learning methods for interface prediction use hand-crafted features that come from the domain e",1
"rface accessibility, sequence conservation, residue properties such as hydrophobicity and charge, and various shape descriptors (see Aumentado et al. <ref type=""bibr"" target=""#b5"">[6]</ref> for a review of the most commonly used features for this task).</p><p>The task of object recognition in images",0
"machine learning tasks from computer vision <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b14"">15]</ref> and speech recognition <ref type=""bibr"" target=""#b11"">[12]</ref> to machine translation <ref type=""bibr"" target=""#b23"">[24]</ref> and computational biology <ref type=""bibr""",0
"osest in space to a residue of interest, and 21 yielded the best performance in our validation experiments. We implemented our networks in TensorFlow <ref type=""bibr"" target=""#b0"">[1]</ref> v1.0.1 to make use of rapid training on GPUs. Training times vary from roughly 17-102 minutes depending on con",0
", remarking on the aspects which resemble or helped inspire our implementation.</p><p>In their Molecular Fingerprint Networks (MFNs), Duvenaud et al. <ref type=""bibr"" target=""#b8"">[9]</ref> proposed a spatial graph convolution approach similar to Equation ( <ref type=""formula"" target=""#formula_0"">1< target=""#b4"">[5]</ref> 0.790 (0.014) ---Diffusion (DCNN) (5 hops) <ref type=""bibr"" target=""#b4"">[5]</ref>) 0.828 (0.018) ---Single Weight Matrix (MFN <ref type=""bibr"" target=""#b8"">[9]</ref>) 0.865 (0.007) 0.871 (0.013) 0.873 (0.017) 0.869 (0.017) Node Average (Equation <ref type=""formula"" target=""#f",0
"r local semantic features alone are inadequate to represent the text comprehensively. Some ensemble approaches <ref type=""bibr"" target=""#b7"">[8]</ref><ref type=""bibr"" target=""#b8"">[9]</ref><ref type=""bibr"" target=""#b9"">[10]</ref> fused different text features and achieved promising results, but they el, we adapt the basic n-gram by setting up rules which can guarantee a linear increase of the words.</p><p>Assuming it follows the Markov assumption <ref type=""bibr"" target=""#b8"">[9]</ref> that the selection probability of any word only depends on the previous N-1 words, which can be considered as",1
"dequate to represent the text comprehensively. Some ensemble approaches <ref type=""bibr"" target=""#b7"">[8]</ref><ref type=""bibr"" target=""#b8"">[9]</ref><ref type=""bibr"" target=""#b9"">[10]</ref> fused different text features and achieved promising results, but they are still limited because of the overl",1
"tion cannot be modeled. GRU is a novel recurrent neural network which can be used in time series analysis and avoid the problem of gradient vanishing <ref type=""bibr"" target=""#b11"">[12]</ref>.</p><p>In our model, GRU extract long-term dependency features from the forward 1-gramcorpus and backward 1-",0
"aracteristics of text structure, how to extract text features more effectively and optimize the algorithm for higher accuracy are the main challenges <ref type=""bibr"" target=""#b0"">[1]</ref>. Some conventional machine learning models are simple but have yield strong baselines. For example, Pang et al n Level Fusion Approach</head><p>Our model utilizes decision-level fusion to concatenate the complementary information of different kinds of features <ref type=""bibr"" target=""#b0"">[1]</ref>. The decision vector [x 1 , ⋯ , x k−1 , x k ] represents the corpus from GRU, and the decision vector</p><form",0
"on.</p><p>Compared to the conventional approaches, neural network has gained significant popularity since it can extract deep level semantic features <ref type=""bibr"" target=""#b3"">[4]</ref>. Both long short-term memory (LSTM) <ref type=""bibr"" target=""#b4"">[5]</ref> and gated recurrent unit (GRU) <re tilized to improve the robustness of the classifier by this strategy. It can effectively extract useful and rich local features although it is simple <ref type=""bibr"" target=""#b3"">[4]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2.3"">Global Dependency Feature Extraction</head>< n in l-th layer, and b l j is j-th neurons bias in l-th layer. Our model uses cross entropy as the loss function to minimize the categorization error <ref type=""bibr"" target=""#b3"">[4]</ref>. The loss function is as follows.</p><formula xml:id=""formula_6"">L ( {x, y} M C ) = argmin ( ∑ M m=1 ∑ C c=1 y es feature, and the categorization accuracy is 94.1% in categories comp.graphics. Global average pooling focuses on extracting local semantic feature <ref type=""bibr"" target=""#b3"">[4]</ref> and the accuracy is 88.3% in the same categories. The large gap shows that the global feature is better than l",0
"t analysis, and text categorization. Table <ref type=""table"" target=""#tab_0"">1</ref> is a summary. The IMDB<ref type=""foot"" target=""#foot_2"">1</ref>  <ref type=""bibr"" target=""#b12"">[13]</ref> data set consists of numerous film movie reviews. It is commonly used for emotional categorization. The ELEC",0
"either the global dependency features or local semantic features alone are inadequate to represent the text comprehensively. Some ensemble approaches <ref type=""bibr"" target=""#b7"">[8]</ref><ref type=""bibr"" target=""#b8"">[9]</ref><ref type=""bibr"" target=""#b9"">[10]</ref> fused different text features a",0
"es <ref type=""bibr"" target=""#b3"">[4]</ref>. Both long short-term memory (LSTM) <ref type=""bibr"" target=""#b4"">[5]</ref> and gated recurrent unit (GRU) <ref type=""bibr"" target=""#b5"">[6]</ref> can capture the long-term dependencies of text sequences, thus they can deal with the information that depends",0
"tracted by the gated recurrent unit, while the local feature with short term semantic within a sliding context is extracted by global average pooling <ref type=""bibr"" target=""#b10"">[11]</ref>. Then two kinds of complementary features are fused to get a more comprehensive understanding of the languag corpuses. As shown in Fig. <ref type=""figure"" target=""#fig_1"">3</ref>, it directly calculates the average value of the word vectors on each dimension <ref type=""bibr"" target=""#b10"">[11]</ref>. Here the variable a t,n denotes the word vector of t-th word and X ′ denotes result matrix with size T × N,",0
"ef type=""bibr"" target=""#b0"">[1]</ref>. Some conventional machine learning models are simple but have yield strong baselines. For example, Pang et al. <ref type=""bibr"" target=""#b1"">[2]</ref> proposed a SVM categorization model based on n-gram approach and achieved good performance. Wang et al. <ref t",0
"LP is inherently limited by true data dependencies. Value prediction was proposed to address this limitation <ref type=""bibr"" target=""#b12"">[12,</ref><ref type=""bibr"" target=""#b18"">20]</ref>. By predicting the value(s) produced by an instruction (producer), instructions that consume the value(s) (co rg/ns/1.0""><head n=""2"">RELATED WORK 2.1 Value Prediction</head><p>Since the introduction of value prediction <ref type=""bibr"" target=""#b12"">[12,</ref><ref type=""bibr"" target=""#b18"">20]</ref>, there has been a plethora of work on this subject. In general, value predictors can be classified into two b flicting store is not committed yet. For the loads that belong to the first sequence, a conventional value predictor (e.g., Last-Value-Predictor, LVP <ref type=""bibr"" target=""#b18"">[20]</ref>) might mispredict the second load's value because the value has been changed by the interleaving store. The",1
"loads and previously committed stores. This can effectively mitigate Challenge #1.</p><p>Similar to early work <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b9"">9,</ref><ref type=""bibr"" target=""#b13"">13]</ref>, we observed that load memory addresses exhibit similar temporal locali y used for hiding the multi-cycle access latency of the memory hierarchy <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b9"">9,</ref><ref type=""bibr"" target=""#b10"">10,</ref><ref type=""bibr"" target=""#b32"">34]</ref>, and to prefetch the data to be",0
"target=""#b3"">[4]</ref>, and various Javascript benchmarks <ref type=""bibr"" target=""#b8"">[8,</ref><ref type=""bibr"">14,</ref><ref type=""bibr"">15,</ref><ref type=""bibr"" target=""#b15"">17,</ref><ref type=""bibr"" target=""#b33"">35,</ref><ref type=""bibr"" target=""#b38"">40]</ref>.</p><p>Table <ref type=""table",0
"rs <ref type=""bibr"" target=""#b2"">[3]</ref>. Address prediction has been mostly used for hiding the multi-cycle access latency of the memory hierarchy <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b9"">9,</ref><ref type=""bibr"" target=""#b10"">10,<",0
"lows for reordering most memory operations with one exception: dependent loads are not allowed to be reordered <ref type=""bibr"" target=""#b6"">[6,</ref><ref type=""bibr"" target=""#b25"">27]</ref>. Value prediction can violate this rule. To avoid violating the memory consistency model, we employ a techniq",0
"te that the skip-gram model with negative sampling for word embedding has been shown to be an implicit factorization of a certain word-context matrix <ref type=""bibr"" target=""#b25"">[24]</ref>, and there is recent effort to theoretically explaining the word embedding models from geometric perspective _7"">ℓ(i, j) = A i, j log д x ⊤ i y j + b d i d j vol(G) log д −x ⊤ i y j .</formula><p>Let us define z i, j = x ⊤ i y j . Following Levy and Goldberg <ref type=""bibr"" target=""#b25"">[24]</ref>, where the authors suggested that for a sufficient large embedding dimension, each individual z i, j can ass a T -sized window</p><formula xml:id=""formula_18"">w i−T , • • • , w i−1 , w i+1 , • • • , w i+T .</formula><p>Following the work by Levy and Goldberg <ref type=""bibr"" target=""#b25"">[24]</ref>, SGNS is implicitly factorizing</p><formula xml:id=""formula_19"">log #(w, c) | D | #(w ) • #(c) − log b,<labe ue to the element-wise matrix logarithm. The matrix is not only ill-defined (since log 0 = −∞), but also dense. Inspired by the Shifted PPMI approach <ref type=""bibr"" target=""#b25"">[24]</ref>, we define M ′ such that M ′ i, j = max(M i, j , 1) (Line 3). In this way, log M ′ is a sparse and consisten ampling (SGNS) <ref type=""bibr"" target=""#b30"">[29]</ref>. Recently, there has been effort in understanding this model. For example, Levy and Goldberg <ref type=""bibr"" target=""#b25"">[24]</ref> prove that SGNS is actually conducting an implicit matrix factorization, which provides us with a tool to an ding models; and Hashimoto et al. <ref type=""bibr"" target=""#b19"">[18]</ref> frame word embedding as a metric learning problem. Built upon the work in <ref type=""bibr"" target=""#b25"">[24]</ref>, we theoretically analyze popular skip-gram based network embedding models and connect them with spectral gr",1
"-c.org/ns/1.0""><head n=""5"">RELATED WORK</head><p>The story of network embedding stems from Spectral Clustering <ref type=""bibr"" target=""#b6"">[5,</ref><ref type=""bibr"" target=""#b46"">45]</ref>, a data clustering technique which selects eigenvalues/eigenvectors of a data affinity matrix to obtain repre",0
"ed from 10% to 90%. For Flickr, the training ratio is varied from 1% to 10%. We use the onevs-rest logistic regression model implemented by LIBLINEAR <ref type=""bibr"" target=""#b15"">[14]</ref> for the multi-label classification task. In the test phase, the onevs-rest model yields a ranking of labels",0
"target=""#b44"">[43,</ref><ref type=""bibr"" target=""#b48"">47,</ref><ref type=""bibr"" target=""#b50"">49]</ref>, network embedding with high order structure <ref type=""bibr"" target=""#b7"">[6,</ref><ref type=""bibr"" target=""#b17"">16]</ref>, signed network embedding <ref type=""bibr"" target=""#b11"">[10]</ref>, d",0
"arning <ref type=""bibr"" target=""#b42"">[43]</ref>, manifold learning algorithms <ref type=""bibr"" target=""#b37"">[38]</ref>, and geometric deep learning <ref type=""bibr"" target=""#b6"">[7]</ref>-all of which involve representation learning with graph-structured data. We refer the reader to <ref type=""bib er the reader to <ref type=""bibr"" target=""#b32"">[33]</ref>, <ref type=""bibr"" target=""#b42"">[43]</ref>, <ref type=""bibr"" target=""#b37"">[38]</ref>, and <ref type=""bibr"" target=""#b6"">[7]</ref> for comprehensive overviews of these areas.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""1.1"">N (e.g. , using Chebyshev polynomials) are conceptually similar to Algorithm 1, with some minor variations, and we refer the reader to Bronstein et al. <ref type=""bibr"" target=""#b6"">[7]</ref> for a thorough discussion of these techniques.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.1",1
"e, there are other lines of closely related and relevant work, which we do not review in detail here-including latent space models of social networks <ref type=""bibr"" target=""#b32"">[33]</ref>, embedding methods for statistical relational learning <ref type=""bibr"" target=""#b42"">[43]</ref>, manifold l deep learning <ref type=""bibr"" target=""#b6"">[7]</ref>-all of which involve representation learning with graph-structured data. We refer the reader to <ref type=""bibr"" target=""#b32"">[33]</ref>, <ref type=""bibr"" target=""#b42"">[43]</ref>, <ref type=""bibr"" target=""#b37"">[38]</ref>, and <ref type=""bibr"" projecting, nodes into a latent space, where geometric relations in this latent space correspond to interactions (e.g., edges) in the original graph <ref type=""bibr"" target=""#b32"">[33]</ref>. Figure <ref type=""figure"">2</ref> visualizes an example embedding of the famous Zachary Karate Club social",1
"ies that exist in the network. B, Twodimensional visualization of node embeddings generated from this graph using the DeepWalk method (Section 2.2.2) <ref type=""bibr"" target=""#b46"">[47]</ref>. The distances between nodes in the embedding space reflect similarity in the original graph, and the node e e=""bibr"" target=""#b32"">[33]</ref>. Figure <ref type=""figure"">2</ref> visualizes an example embedding of the famous Zachary Karate Club social network <ref type=""bibr"" target=""#b46"">[47]</ref>, where two dimensional node embeddings capture the community structure implicit in the social network.</p></ earning tasks. For example, one could feed the learned embeddings to a logistic regression classifier to predict the community that a node belongs to <ref type=""bibr"" target=""#b46"">[47]</ref>, or one could use distances between the embeddings to recommend friendship links in a social network <ref ty 5] z i z j general DEC(z i , z j ) -s G (v i , v j ) 2 2</formula></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>Random walk</head><p>DeepWalk <ref type=""bibr"" target=""#b46"">[47]</ref> </p><formula xml:id=""formula_5"">e z i z j k?V e z i z k p G (v j |v i ) -s G (v i , v j ) log(DEC(z i , z j >. DeepWalk employs a ""hierarchical softmax"" technique to compute the normalizing factor, using a binary-tree structure to accelerate the computation <ref type=""bibr"" target=""#b46"">[47]</ref>. In contrast, node2vec approximates Equation (11) using ""negative sampling"": instead of normalizing over the he original graph, and the node embeddings are spatially clustered according to the different color-coded communities. Reprinted with permission from <ref type=""bibr"" target=""#b46"">[47,</ref><ref type=""bibr"" target=""#b48"">49]</ref>.</p><p>parate lines of research that have drawn significant attentio iques such as t-SNE <ref type=""bibr"" target=""#b56"">[57]</ref> or principal components analysis (PCA) in order to generate 2D visualizations of graphs <ref type=""bibr"" target=""#b46"">[47,</ref><ref type=""bibr"" target=""#b53"">54]</ref>, which can be useful for discovering communities and other hidden st s G according to the probability of v i and v j co-occurring on a fixed-length random walk over the graph G <ref type=""bibr"" target=""#b27"">[28,</ref><ref type=""bibr"" target=""#b46"">47]</ref>. In practice, most approaches realize the reconstruction objective (Equation <ref type=""formula"" target=""#for eos, web pages, or individuals into different categories/communities <ref type=""bibr"" target=""#b27"">[28,</ref><ref type=""bibr"" target=""#b34"">35,</ref><ref type=""bibr"" target=""#b46"">47,</ref><ref type=""bibr"" target=""#b53"">54]</ref>. Recently, Hamilton et al. <ref type=""bibr"" target=""#b28"">[29]</ref>",0
"1"">z S = v i ?S z i ,<label>(21)</label></formula><p>where the embeddings, {z i , ?v i ? S}, are generated using a variant of Algorithm 1. Dai et al. <ref type=""bibr"" target=""#b15"">[16]</ref> employ an analogous sum-based approach but note that it has conceptual connections to meanfield inference: i nce where the message-passing operations have been replaced with differentiable neural network alternatives. Motivated by this connection, Dai et al. <ref type=""bibr"" target=""#b15"">[16]</ref> also propose a modified encoder based on Loopy Belief Propagation <ref type=""bibr"" target=""#b41"">[42]</ref>. 3"">z i = ?(W k V ? COMBINE(x i , AGGREGATE({? K i,l , ?v l ? N (v i )})).<label>(23)</label></formula><p>Once the embeddings are computed, Dai et al. <ref type=""bibr"" target=""#b15"">[16]</ref>, use a simple element-wise sum to combine the node embeddings for a subgraph, as in Equation ( <ref type=""fo to subgraphs: Kearnes et al. aggregate sets of nodes using ""fuzzy"" histograms instead of a sum, and they also employ edge embedding layers similar to <ref type=""bibr"" target=""#b15"">[16]</ref>. Neipart et al. define an ordering on the nodes-e.g. using a problem specific ordering or by employing an of ddings can be used to classify or predict various properties of molecular graphs, including predicting the efficacy of potential solar cell materials <ref type=""bibr"" target=""#b15"">[16]</ref>, or predicting the therapeutic effect of candidate drugs <ref type=""bibr"" target=""#b33"">[34]</ref>. More gen ications in a number of areas. The most prominent application domain is for classifying the properties of graphs corresponding to different molecules <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b43"">44,</ref><ref type=""bibr"" target=""#b33",0
"biquity, graphs are the backbone of countless systems, allowing relational knowledge about interacting entities to be efficiently stored and accessed <ref type=""bibr"" target=""#b1"">[2]</ref>.</p><p>However, graphs are not only useful as structured knowledge repositories: they also play a key role in",0
"s) <ref type=""bibr"" target=""#b28"">[29]</ref>, or assign each node a one-hot indicator vector as an attribute <ref type=""bibr"" target=""#b35"">[36,</ref><ref type=""bibr"" target=""#b52"">53]</ref>. These methods are often called convolutional because they represent a node as a function of its surrounding 9,</ref><ref type=""bibr"" target=""#b34"">35]</ref> and link prediction <ref type=""bibr"" target=""#b55"">[56,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" target=""#b52"">53]</ref> benchmarks. At a high level, these approaches solve the four main limitations of shallow embeddings, noted at es <ref type=""bibr"" target=""#b11"">[12]</ref> and (ii) extend pairwise decoders with type-specific parameters <ref type=""bibr"" target=""#b42"">[43,</ref><ref type=""bibr"" target=""#b52"">53]</ref>. For example, in graphs with varying The embeddings were generated using the multi-layer OhmNet method and pr ecoder (i.e., z i z j ? A i,j ) can be replaced with a bilinear form <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b42"">43,</ref><ref type=""bibr"" target=""#b52"">53]</ref>:</p><formula xml:id=""formula_25"">DEC ? (z i , z j ) = z A ? z,<label>(17)</label></formula><p>where ? indexes utlined in Algorithm 1, including graph convolutional networks (GCN) <ref type=""bibr"" target=""#b34"">[35,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" target=""#b52"">53,</ref><ref type=""bibr"" target=""#b55"">56]</ref>, column networks <ref type=""bibr"" target=""#b49"">[50]</ref>, and the G nted in Section 2.3.2-can also incorporate task-specific supervision <ref type=""bibr"" target=""#b28"">[29,</ref><ref type=""bibr"" target=""#b34"">35,</ref><ref type=""bibr"" target=""#b52"">53,</ref><ref type=""bibr"" target=""#b59"">60]</ref>. In particular, it is common for methods incorporate supervision from matrix, A ? , in Equation ( <ref type=""formula"" target=""#formula_25"">17</ref>) can be regularized in various ways (e.g., constrained to be diagonal) <ref type=""bibr"" target=""#b52"">[53]</ref>, which can be especially useful when there are a large number of edge types, as in the case for embedding kn",0
"ble weight matrix and GRU denotes the Gated Recurrent Unit introduced by Cho et al. <ref type=""bibr"" target=""#b13"">[14]</ref>. Finally, Gilmer et al. <ref type=""bibr"" target=""#b24"">[25]</ref> discuss another abstraction of GNNs, considering models of the form</p><formula xml:id=""formula_37"">h k i =",0
"a task that has countless applications from computational biology (e.g., discovering related drugs) to marketing (e.g., discovering related products) <ref type=""bibr"" target=""#b22"">[23]</ref>. Again, because each node is associated with real-valued vector embedding, it is possible to apply any gener",0
"eal-valued vector embedding, it is possible to apply any generic clustering algorithm to the set of learned node embeddings (e.g., k-means or DB-scan <ref type=""bibr"" target=""#b21"">[22]</ref>).</p><p>This offers an open-ended and powerful alternative to traditional community detection techniques, an",0
"y. The problem of visualizing graphs in a 2D interface has a long history, with applications throughout data mining, the social sciences, and biology <ref type=""bibr"" target=""#b16"">[17]</ref>. Node embeddings offer a powerful new paradigm for graph visualization: because nodes are mapped to real-val",0
". In the second classification stage, sampling heuristics, such as a fixed foreground-to-background ratio (1:3), or online hard example mining (OHEM) <ref type=""bibr"" target=""#b29"">[30]</ref>, are performed to maintain a manageable balance between foreground and background.</p><p>In contrast, a one- can effectively discount the effect of easy negatives, focusing all attention on the hard negative examples.</p><p>Online Hard Example Mining (OHEM): <ref type=""bibr"" target=""#b29"">[30]</ref> proposed to improve training of two-stage detectors by constructing minibatches using high-loss examples. Sp ,</ref><ref type=""bibr"" target=""#b27"">28]</ref> or hard example mining <ref type=""bibr"" target=""#b35"">[36,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b29"">30]</ref>.</p><p>In this paper, we propose a new loss function that acts as a more effective alternative to previous ap rk <ref type=""bibr"" target=""#b26"">[27]</ref>. Numerous extensions to this framework have been proposed, e.g. <ref type=""bibr"" target=""#b18"">[19,</ref><ref type=""bibr"" target=""#b29"">30,</ref><ref type=""bibr"" target=""#b30"">31,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b12"" e form of hard negative mining <ref type=""bibr"" target=""#b31"">[32,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b29"">30,</ref><ref type=""bibr"" target=""#b20"">21]</ref> that samples hard examples during training or more complex sampling/r ratio anchors yields good results after which point performance saturates. (d) FL outperforms the best variants of online hard example mining (OHEM) <ref type=""bibr"" target=""#b29"">[30,</ref><ref type=""bibr"" target=""#b20"">21]</ref> by over 3 points AP. (e) Accuracy/Speed trade-off of RetinaNet on te",1
"st simply by reducing input image resolution and the number of proposals, but one-stage methods trailed in accuracy even with a larger compute budget <ref type=""bibr"" target=""#b15"">[16]</ref>. In contrast, the aim of this work is to understand if one-stage detectors can match or surpass the accuracy",0
"arget=""#b29"">30,</ref><ref type=""bibr"" target=""#b20"">21]</ref> that samples hard examples during training or more complex sampling/reweighing schemes <ref type=""bibr"" target=""#b1"">[2]</ref>. In contrast, we show that our proposed focal loss naturally handles the class imbalance faced by a one-stage",0
"detectors are applied over a regular, dense sampling of object locations, scales, and aspect ratios. Recent work on one-stage detectors, such as YOLO <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b25"">26]</ref> and SSD <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" ta ge object detector based on deep networks. More recently SSD <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b8"">9]</ref> and YOLO <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b25"">26]</ref> have renewed interest in one-stage methods. These detectors have bee",0
". The proposal stage (e.g., Selective Search <ref type=""bibr"" target=""#b33"">[34]</ref>, EdgeBoxes <ref type=""bibr"" target=""#b36"">[37]</ref>, DeepMask <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b23"">24]</ref>, RPN <ref type=""bibr"" target=""#b26"">[27]</ref>) rapidly narrows down e=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b9"">10]</ref> and by using learned object proposals <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b26"">27]</ref>. Region Proposal Networks (RPN) integrated proposal generation with t two-stage cascade and (2) biased minibatch sampling. The first cascade stage is an object proposal mechanism <ref type=""bibr"" target=""#b33"">[34,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b26"">27]</ref> that reduces the nearly infinite set of possible object locations dow (FCN) <ref type=""bibr"" target=""#b21"">[22]</ref>, as shown by its gains for RPN <ref type=""bibr"" target=""#b26"">[27]</ref> and DeepMask-style proposals <ref type=""bibr"" target=""#b22"">[23]</ref>, as well at two-stage detectors such as Fast R-CNN <ref type=""bibr"" target=""#b9"">[10]</ref> or Mask R-CNN <r",0
"ms of speed and accuracy for SISR, which operates on LR images directly and progressively reconstruct the sub-band residuals of HR images. Tai et al. <ref type=""bibr"" target=""#b33"">[34]</ref> proposed deep recursive residual network (DRRN) to address the problems of model parameters and accuracy, wh ches with a stride of 21. The output of MemNet is the estimated high-quality patch with the same resolution as the input low-quality patch. We follow <ref type=""bibr"" target=""#b33"">[34]</ref> to do data augmentation. For each task, we train a single model for all different levels of corruption. E.g.",1
"(H r−1 m , W m ) = W 2 m τ (W 1 m τ (H r−1 m )),<label>(6)</label></formula><p>where τ denotes the activation function, including batch normalization <ref type=""bibr"" target=""#b15"">[16]</ref> followed by ReLU <ref type=""bibr"" target=""#b29"">[30]</ref>, and W i m , i = 1, 2 are the weights of the i-th",0
"e gate unit. the additive noise. With this mathematical model, extensive studies are conducted on many image restoration tasks, e.g., image denoising <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b36"">37]< //www.tei-c.org/ns/1.0""><head>Dataset</head><p>Noise BM3D <ref type=""bibr"" target=""#b4"">[5]</ref> EPLL <ref type=""bibr"" target=""#b40"">[41]</ref> PCLR <ref type=""bibr"" target=""#b1"">[2]</ref> PGPD <ref type=""bibr"" target=""#b36"">[37]</ref> WNNM <ref type=""bibr"" target=""#b8"">[9]</ref> RED <ref type=""bib n lead to better performance. However, in our MemNet, we do not perform any post-processing. For qualitative comparisons, we use public codes of PCLR <ref type=""bibr"" target=""#b1"">[2]</ref>, PGPD <ref type=""bibr"" target=""#b36"">[37]</ref> and WNNM <ref type=""bibr"" target=""#b8"">[9]</ref>. The results",0
"he key component of MemNet, a memory block contains a recursive unit and a gate unit. Inspired by neuroscience <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b24"">25]</ref> that recursive connections ubiquitously exist in the neocortex, the recursive unit learns multi-level represe te unit. Recursive Unit is used to model a non-linear function that acts like a recursive synapse in the brain <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b24"">25]</ref>. Here, we use a residual building block, which is introduced in ResNet <ref type=""bibr"" target=""#b11"">[12]</r",0
"type=""bibr"" target=""#b26"">[27]</ref> proposed a very deep convolutional auto-encoder network for image denoising and SISR. Very Recently, Lai et al. <ref type=""bibr"" target=""#b22"">[23]</ref> proposed LapSRN to address the problems of speed and accuracy for SISR, which operates on LR images directly",0
"bibr"" target=""#b5"">(Cheng et al., 2016)</ref> and learning sentence representations <ref type=""bibr"" target=""#b25"">(Lin et al., 2017)</ref>. However, <ref type=""bibr"" target=""#b38"">Vaswani et al. (2017)</ref> showed that not only self-attention can improve a method based on RNNs or convolutions, but stabilize the learning process of self-attention, we have found extending our mechanism to employ multi-head attention to be beneficial, similarly to <ref type=""bibr"" target=""#b38"">Vaswani et al. (2017)</ref>. Specifically, K independent attention mechanisms execute the transformation of Equation <r",1
"et al. (2015)</ref> introduced a parameterization of the spectral filters with smooth coefficients in order to make them spatially localized. Later, <ref type=""bibr"" target=""#b8"">Defferrard et al. (2016)</ref> proposed to approximate the filters by means of a Chebyshev expansion of the graph Laplac so directly compare our model against GCNs (Kipf &amp; Welling, 2017), as well as graph convolutional models utilising higher-order Chebyshev filters <ref type=""bibr"" target=""#b8"">(Defferrard et al., 2016)</ref>, and the MoNet model presented in <ref type=""bibr"" target=""#b28"">Monti et al. (2016)</re 7) and <ref type=""bibr"" target=""#b28"">Monti et al. (2016)</ref> for state-of-the-art techniques. Specifically, for the Chebyshev filterbased approach <ref type=""bibr"" target=""#b8"">(Defferrard et al., 2016)</ref>, we provide the maximum reported performance for filters of orders K = 2 and K = 3. In o ""#b26"">(Lu &amp; Getoor, 2003)</ref> 75.1% 69.1% 73.9% Planetoid <ref type=""bibr"" target=""#b41"">(Yang et al., 2016)</ref> 75.7% 64.7% 77.2% Chebyshev <ref type=""bibr"" target=""#b8"">(Defferrard et al., 2016)</ref>  0.973 ± 0.002 the other techniques. Specifically, as our setup is supervised, we compar",0
"b12"">(Frasconi et al., 1998;</ref><ref type=""bibr"" target=""#b35"">Sperduti &amp; Starita, 1997)</ref>. Graph Neural Networks (GNNs) were introduced in <ref type=""bibr"" target=""#b15"">Gori et al. (2005)</ref> and <ref type=""bibr"" target=""#b33"">Scarselli et al. (2009)</ref> as a generalization of recurs",0
"proven to be useful for tasks such as machine reading <ref type=""bibr"" target=""#b5"">(Cheng et al., 2016)</ref> and learning sentence representations <ref type=""bibr"" target=""#b25"">(Lin et al., 2017)</ref>. However, <ref type=""bibr"" target=""#b38"">Vaswani et al. (2017)</ref> showed that not only self",0
"en successfully applied to tackle problems such as image classification <ref type=""bibr"" target=""#b17"">(He et al., 2016)</ref>, semantic segmentation <ref type=""bibr"" target=""#b21"">(Jégou et al., 2017)</ref> or machine translation <ref type=""bibr"" target=""#b13"">(Gehring et al., 2016)</ref>, where th",0
"ve non-spectral approaches <ref type=""bibr"" target=""#b11"">(Duvenaud et al., 2015;</ref><ref type=""bibr"" target=""#b1"">Atwood &amp; Towsley, 2016;</ref><ref type=""bibr"" target=""#b16"">Hamilton et al., 2017)</ref>, which define convolutions directly on the graph, operating on groups of spatially close n /ref> presented mixture model CNNs (MoNet), a spatial approach which provides a unified generalization of CNN architectures to graphs. More recently, <ref type=""bibr"" target=""#b16"">Hamilton et al. (2017)</ref> introduced GraphSAGE, a method for computing node representations in an inductive manner. testing. Critically, testing graphs remain completely unobserved during training. To construct the graphs, we used the preprocessed data provided by <ref type=""bibr"" target=""#b16"">Hamilton et al. (2017)</ref>. The average number of nodes per graph is 2372. Each node has 50 features that are compose .</p><p>Inductive learning For the inductive learning task, we compare against the four different supervised GraphSAGE inductive methods presented in <ref type=""bibr"" target=""#b16"">Hamilton et al. (2017)</ref>. These provide a variety of approaches to aggregating features within a sampled neighborho we report the micro-averaged F 1 score on the nodes of the two unseen test graphs, averaged after 10 runs, and reuse the metrics already reported in <ref type=""bibr"" target=""#b16"">Hamilton et al. (2017)</ref> for  <ref type=""bibr"" target=""#b3"">(Belkin et al., 2006)</ref> 59.5% 60.1% 70.7% SemiEmb <",0
"her designs using ASIC <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b1"">2]</ref> and FPGAs <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b25"">26]</ref>.</p><p>Activation unit is a non-linear function that some layer's outputs go through. Some examples are: rect",1
"=""bibr"" target=""#b20"">[21]</ref> and other designs using ASIC <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b1"">2]</ref> and FPGAs <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b25"">26]</ref>.</p><p>Activation unit is a non-linear function that some layer's outp",0
"ing based on network sparsity are techniques that lower memory bandwidth requirement for this type of workload <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b11"">12]</ref>.</p><p>[5] presents a compiler for a custom CNN accelerator using Torch5 models. Their approach is to map Tor",0
"are and instruction generation software was developed for Caffe <ref type=""bibr"" target=""#b10"">[11]</ref> in <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b0"">1]</ref>. Snowball is the first to generate to custom instructions for hardware accelerator from Torch7 <ref type=""bibr"" riendly computation tiling for CNN accelerators was explored in <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b18"">19]</ref>. In <ref type=""bibr"" target=""#b0"">[1]</ref>, block tiling with xy axis ordering was used. They store tiles with extra overlap regions, called augmented-ti",0
"vely achieved the state-of-the-art accuracy for classification tasks <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b8"">9]</ref> on large image datasets <ref type=""bibr"" target=""#b19"">[20]</ref>. Those models are embarrassingly parallel, bu XC7Z045 FPGA <ref type=""bibr"" target=""#b23"">[24]</ref>. The system was benchmarked with AlexNet and ResNet18 <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b8"">9]</ref> pre-trained models. At 250 MHz, AlexNet arXiv:1708.00117v1 [cs.DC] 1 Aug 2017 achieved in 93.6 frames/s and 1.2 t, manually crafting assembly like instructions can be cumbersome and error prone specially when a model is composed of several layers like in ResNet <ref type=""bibr"" target=""#b8"">[9]</ref>. Even if one was patient enough to manually write code for some of state-ofthe-art deep learning models, furth accumulating all values in a window with this weight gives the average value of a window.</p><p>Residual addition or bypass is used in ResNet models <ref type=""bibr"" target=""#b8"">[9]</ref>. The output values of a CONV are element-wise added with a previous layer's input. In hardware, we want to add flake will process each element in the list in sequence.</p><p>In some models, such as GoogLeNet <ref type=""bibr"" target=""#b21"">[22]</ref> and ResNet <ref type=""bibr"" target=""#b8"">[9]</ref>, not all the layers are sequential. Some layers share their input and output, thus some layers in the list are",0
"tured or graphical model problem and predict keypoint locations based on hand-crafted features. Recent works <ref type=""bibr"" target=""#b26"">[27,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b38"">3 Lifshitz et al. <ref type=""bibr"" target=""#b22"">[23]</ref> uses deep consensus voting to vote the most probable location of keypoints. Gkioxary et al. <ref type=""bibr"" target=""#b13"">[14]</ref> and Zisserman et al. <ref type=""bibr"" target=""#b1"">[2]</ref> apply RNN-like architectures to sequentially re",1
"e=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b38"">39,</ref><ref type=""bibr"" target=""#b41"">42]</ref> mostly rely on the development of convolutional neural network(CNN) <ref type=""bibr"" target=""#b21"">[22,</ref>",1
"NN-like architectures to sequentially refine the results. Our work is partly inspired by the works on generating and refining score maps. Yang et al. <ref type=""bibr"" target=""#b42"">[43]</ref> adopts pyramid features as inputs of the network in the process of pose estimation, which is good exploratio",1
"earch topic since for decades. Classical approaches tackling the problem of human pose estimation mainly adopt the techniques of pictorial structures <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b0"">1]</ref> or graphical models <ref type=""bibr"" target=""#b6"">[7]</ref>. More speci",0
"deep consensus voting to vote the most probable location of keypoints. Gkioxary et al. <ref type=""bibr"" target=""#b13"">[14]</ref> and Zisserman et al. <ref type=""bibr"" target=""#b1"">[2]</ref> apply RNN-like architectures to sequentially refine the results. Our work is partly inspired by the works on g",0
"two categories: bottom-up approaches and top-down approaches.</p><p>Bottom-Up Approaches. Bottom-up approaches <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b29"">30,</ref><ref type=""bibr"" target=""#b18"">19]</ref> directly predict all keypoint map the relationship between keypoints into part affinity fields (PAFs) and assemble detected keypoints into different poses of people. Newell et al. <ref type=""bibr"" target=""#b25"">[26]</ref> simultaneously produce score maps and pixel-wise embedding to group the candidate keypoints to different peo",0
"two stages in our network architecture: GlobalNet and RefineNet. Our GlobalNet learns a good feature representation based on feature pyramid network <ref type=""bibr"" target=""#b23"">[24]</ref>. More importantly, the pyramid feature representation can provide sufficient context information, which is i iv> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.1."">Human Detector</head><p>We adopt the state-of-art object detector algorithms based on FPN <ref type=""bibr"" target=""#b23"">[24]</ref>. ROIAlign from Mask RCNN <ref type=""bibr"" target=""#b14"">[15]</ref> is adopted to replace the ROIPooling in F usually a U-shape structure is integrated to maintain both the spatial resolution and semantic information for the feature layers. More recently, FPN <ref type=""bibr"" target=""#b23"">[24]</ref> further improves the U-shape structure with deeply supervised information. We apply the similar feature pyra ""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b30"">31]</ref>, the up-to-date detector of which are <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b14"">15]</ref>. These detection approaches are composed of two-stage in general. Fi from the feature map and further refine the proposals to get the final boxes via R-CNN network. The detector used in our methods are mostly based on <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b14"">15]</ref>. </p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3."">Ou",0
"al. <ref type=""bibr"" target=""#b5"">[6]</ref> uses iterative error feedback to get pose estimation and refine the prediction gradually. Lifshitz et al. <ref type=""bibr"" target=""#b22"">[23]</ref> uses deep consensus voting to vote the most probable location of keypoints. Gkioxary et al. <ref type=""bibr""",0
"o different people to get final multi-person pose estimation.</p><p>Top-Down Approaches. Top-down approaches <ref type=""bibr"" target=""#b27"">[28,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b8"">9]</ref> interpret the process of detect",0
"ti-person pose estimation has been greatly improved by the involvement of deep convolutional neural networks <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b15"">16]</ref>. For example, in <ref type=""bibr"" target=""#b4"">[5]</ref>, convolutional pose machine is utilized to locate th <ref type=""bibr"" target=""#b41"">42]</ref> mostly rely on the development of convolutional neural network(CNN) <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b15"">16]</ref>, which largely improve the performance of pose estimation. In this paper, we mainly focus on the methods base abeled body parts. DeeperCut <ref type=""bibr"" target=""#b18"">[19]</ref> improves DeepCut <ref type=""bibr"" target=""#b29"">[30]</ref> using deeper ResNet <ref type=""bibr"" target=""#b15"">[16]</ref> and employs image-conditioned pairwise terms to get better performance. Zhe Cao et al. <ref type=""bibr"" targ to have a comparable performance compared with the eight-stage stacked hourglass module. <ref type=""bibr"" target=""#b27"">[28]</ref> utilizes a ResNet <ref type=""bibr"" target=""#b15"">[16]</ref> network to estimate pose in the wild achieving promising performance in the COCO 2016 keypoint challenge. Mo",0
"utilized to locate the keypoint joints in the image and part affinity fields (PAFs) is proposed to assemble the joints to different person. Mask-RCNN <ref type=""bibr"" target=""#b14"">[15]</ref> predicts human bounding boxes first and then warps the feature maps based on the human bounding boxes to obt on the heatmaps to the ground truth location, and then uses the heatmaps with offsets to obtain the final predicted location of keypoints. Mask-RCNN <ref type=""bibr"" target=""#b14"">[15]</ref> predicts human bounding boxes first and then crops out the feature map of the corresponding human bounding b etector</head><p>We adopt the state-of-art object detector algorithms based on FPN <ref type=""bibr"" target=""#b23"">[24]</ref>. ROIAlign from Mask RCNN <ref type=""bibr"" target=""#b14"">[15]</ref> is adopted to replace the ROIPooling in FPN. To train the object detector, all eighty categories from the CO rson pose estimation.</p><p>Top-Down Approaches. Top-down approaches <ref type=""bibr"" target=""#b27"">[28,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b8"">9]</ref> interpret the process of detecting keypoints as a twostage pipeline, th ""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b30"">31]</ref>, the up-to-date detector of which are <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b14"">15]</ref>. These detection approaches are composed of two-stage in general. First generate boxes proposals based on def the proposals to get the final boxes via R-CNN network. The detector used in our methods are mostly based on <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b14"">15]</ref>. </p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3."">Our Approach for Multi-perosn Keypoints Est 4"">15]</ref>. </p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3."">Our Approach for Multi-perosn Keypoints Estimation</head><p>Similar to <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b27"">28]</ref>, our algorithm adopts the top-down pipeline: a human detector is fir",0
"8, the performance of second stage achieves the best result for the balanced training between hard keypoints and simple keypoints.  Inspired by OHEM <ref type=""bibr"" target=""#b34"">[35]</ref>, however the method of online hard keypoints mining loss is quite different from it. our method focuses on h",0
"=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b33"">34,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b43"">44,</ref><ref type=""bibr"" target=""#b28"">29,</ref><ref type=""bibr"" target=""#b19"">2",0
"approaches tackling the problem of human pose estimation mainly adopt the techniques of pictorial structures <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b0"">1]</ref> or graphical models <ref type=""bibr"" target=""#b6"">[7]</ref>. More specifically, the classical works <ref type="" >[10,</ref><ref type=""bibr"" target=""#b0"">1]</ref> or graphical models <ref type=""bibr"" target=""#b6"">[7]</ref>. More specifically, the classical works <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b33"">34,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b32"">",0
"ameters and high computational cost in both training and testing phases. Recent studies have investigated the significant redundancy in deep networks <ref type=""bibr"" target=""#b5"">[6]</ref> and reduced the number of neurons and filters <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target="" <head n=""2."">Related Work</head><p>There has been recent interest in reducing the redundancy of deep CNNs to achieve acceleration and compression. In <ref type=""bibr"" target=""#b5"">[6]</ref> the redundancy in the parameterization of deep learning models has been studied and demonstrated. Cheng et al. "">12,</ref><ref type=""bibr"" target=""#b44"">45]</ref>, the redundancy in the parameterization of deep learning models has been studied and demonstrated <ref type=""bibr"" target=""#b5"">[6]</ref>. We present NISP to efficiently propagate the importance scores from final responses to all other neurons to g",1
"up deep network inference by refining the pipelines of certain tasks <ref type=""bibr"" target=""#b32"">[33,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b45"">46,</ref><ref type=""bibr"" target=""#b26"">27]</ref>. Our method prunes a pre-trained network and requires a fast-convergi ""bibr"" target=""#b40"">41,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b47"">48,</ref><ref type=""bibr"" target=""#b45"">46,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b43"">44,</ref><ref type=""bibr"" target=""#b41""",0
"pe=""bibr"" target=""#b22"">[23]</ref>, AlexNet <ref type=""bibr"" target=""#b21"">[22]</ref>, GoogLeNet <ref type=""bibr"" target=""#b36"">[37]</ref> and ResNet <ref type=""bibr"" target=""#b15"">[16]</ref>. Our experiments show that CNNs pruned by our approach outperform those with the same structures but which a e=""foot"" target=""#foot_0"">3</ref> , AlexNet <ref type=""bibr"" target=""#b21"">[22]</ref>, GoogLeNet <ref type=""bibr"" target=""#b36"">[37]</ref> and ResNet <ref type=""bibr"" target=""#b15"">[16]</ref>.</p><p>All experiments and time benchmarks are obtained using Caffe <ref type=""bibr"" target=""#b18"">[19]</ref",0
"ref type=""bibr"" target=""#b5"">[6]</ref> the redundancy in the parameterization of deep learning models has been studied and demonstrated. Cheng et al. <ref type=""bibr"" target=""#b1"">[2]</ref> exploited properties of structured matrices and used circulant matrices to represent FC layers, reducing stora",0
""" target=""#b5"">[6]</ref> and reduced the number of neurons and filters <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b28"">29]</ref> by pruning the unimportant ones. However, most current approaches tha odel to recover its predictive accuracy.</p><p>filters consider only the statistics of one layer (e.g., prune neurons with small magnitude of weights <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b13"">14]</ref>), or two consecutive layers <ref type=""bibr"" target=""#b28"">[29]</ref a neuron. These methods prune the ""least important"" neurons layer-by-layer either independently <ref type=""bibr"" target=""#b13"">[14]</ref> or greedily <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b28"">29]</ref>, without considering all neurons in different layers jointly.</p><p> ude of Weights</head><p>How to define neuron importance is an open problem. Besides using feature ranking to measure neuron importance, other methods <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b28"">29,</ref><ref type=""bibr"" target=""#b13"">14]</ref> measure neuron importance by 27-44% FLOPs and parameter reduction with tiny top-1 accuracy loss, which shows superior performance when compared with the state-of-the-art methods <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b28"">29]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.6."">A s (50%). With almost zero accuracy loss on ResNet-56, we achieve a 43.61% FLOP reduction, significantly higher than the 27.60% reduction by Li et al. <ref type=""bibr"" target=""#b24"">[25]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""1.1."">Contribution</head><p>We introduce a gene expansion. Our work is a different approximation based on the Lipschitz continuity of a neural network.</p><p>Most similar to our approach, Li et al. <ref type=""bibr"" target=""#b24"">[25]</ref> pruned filters by their weight magnitude. Luo et al. <ref type=""bibr"" target=""#b28"">[29]</ref> utilized stat d achieves similar accuracy loss with larger FLOPs reduction (58.34% vs. 51.50%) Using ResNet on Cifar10 dataset, with top-1 accuracy loss similar to <ref type=""bibr"" target=""#b24"">[25]</ref> (56-A, 56-B. 110-A and 110-B), our method reduces more FLOPs and parameters.</p><p>We also conduct our ResNe to existing methods such as <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" target=""#b24"">25]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.1."">Experimental Setting</head><p>We conduct e propagate the importance proportionally to the weights as described in Algorithm 1.</p><p>The propagation matrices used in algorithm 1 are defined in <ref type=""bibr"" target=""#b24"">(25)</ref> and ( <ref type=""formula"" target=""#formula_32"">26</ref>)</p><formula xml:id=""formula_31"">BP f n conv = ? ? ?",0
"on (in the two-stage framework) and classification tasks. However, all kernels in Inception are sampled at the same center. A similar idea appears in <ref type=""bibr"" target=""#b6"">[3]</ref>, where an Atrous Spatial Pyramid Pooling (ASPP) is exploited to capture multi-scale information. It applies se the Inception family <ref type=""bibr"" target=""#b36"">[33,</ref><ref type=""bibr"" target=""#b35"">32,</ref><ref type=""bibr"" target=""#b34"">31]</ref>, ASPP <ref type=""bibr"" target=""#b6"">[3]</ref>, and Deformable CNN <ref type=""bibr"" target=""#b7"">[4]</ref>.</p><p>The Inception block adopts multiple branche ion at a larger area with more context while keeping the same number of parameters. This design has rapidly proved competent at semantic segmentation <ref type=""bibr"" target=""#b6"">[3]</ref>, and has also been adopted in some reputable object detectors, such as SSD <ref type=""bibr"" target=""#b25"">[22] own the inference speed ( Comparison with other architectures: We also compare our RFB with Inception <ref type=""bibr"" target=""#b36"">[33]</ref>, ASPP <ref type=""bibr"" target=""#b6"">[3]</ref> and Deformable CNN <ref type=""bibr"" target=""#b7"">[4]</ref>. For Inception, besides the original version, we ch e its parameters so that it has the same RF size as RFB does (termed ""Inception-L""). For ASPP, its primary parameters are tuned in image segmentation <ref type=""bibr"" target=""#b6"">[3]</ref> and the RFs are too large for detection, and our experiment, we set it at the same size as in RFB as well (ter",1
"ef><ref type=""bibr"" target=""#b35"">32,</ref><ref type=""bibr"" target=""#b34"">31]</ref>, ASPP <ref type=""bibr"" target=""#b6"">[3]</ref>, and Deformable CNN <ref type=""bibr"" target=""#b7"">[4]</ref>.</p><p>The Inception block adopts multiple branches with different kernel sizes to capture multi-scale informa f the same kernel size, which treats the clues at all the positions equally, probably leading to confusion between object and context. Deformable CNN <ref type=""bibr"" target=""#b7"">[4]</ref> learns distinctive resolutions of individual objects, unfortunately it holds the same downside as ASPP. RFB is es: We also compare our RFB with Inception <ref type=""bibr"" target=""#b36"">[33]</ref>, ASPP <ref type=""bibr"" target=""#b6"">[3]</ref> and Deformable CNN <ref type=""bibr"" target=""#b7"">[4]</ref>. For Inception, besides the original version, we change its parameters so that it has the same RF size as RFB",1
"from 10% to 40% relative to state-of-theart two-stage solutions <ref type=""bibr"" target=""#b23"">[20]</ref>. More recently, Deconvolutional SSD (DSSD) <ref type=""bibr"" target=""#b9"">[6]</ref> and RetinaNet <ref type=""bibr"" target=""#b23"">[20]</ref> substantially ameliorate the precision scores, which a r acceleration, while their accuracies apparently trail those of top two-stage methods.</p><p>Recent more advanced single-stage detectors (e.g., DSSD <ref type=""bibr"" target=""#b9"">[6]</ref> and RetinaNet <ref type=""bibr"" target=""#b23"">[20]</ref>) update their original lightweight backbones by the de r"" target=""#b23"">[20]</ref>) update their original lightweight backbones by the deeper ResNet-101 and apply certain techniques, such as deconvolution <ref type=""bibr"" target=""#b9"">[6]</ref> or Focal Loss <ref type=""bibr"" target=""#b23"">[20]</ref>, whose scores are comparable and even superior to the",0
"t results on the Pascal VOC and MS COCO at a real time processing speed, and demonstrate the generalization ability of RFB by linking it to MobileNet <ref type=""bibr"" target=""#b15"">[12]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2"">Related Work</head><p>Two-stage detector: R- ed. Even though many accomplished lightweight networks have recently been proposed (e.g. DarkNet <ref type=""bibr"" target=""#b28"">[25]</ref>, MobileNet <ref type=""bibr"" target=""#b15"">[12]</ref>, and ShuffleNet <ref type=""bibr"" target=""#b42"">[39]</ref>), we focus on this backbone to achieve direct comp e use is a reduced VGG16 version, it still has a large number of parameters compared with those recent advanced lightweight networks, e.g., MobileNet <ref type=""bibr"" target=""#b15"">[12]</ref>, DarkNet <ref type=""bibr"" target=""#b28"">[25]</ref>, and ShuffleNet <ref type=""bibr"" target=""#b42"">[39]</ref> and ShuffleNet <ref type=""bibr"" target=""#b42"">[39]</ref>. To further test the generalization ability of the RFB module, we link RFB to MobileNet-SSD <ref type=""bibr"" target=""#b15"">[12]</ref>. Following <ref type=""bibr"" target=""#b15"">[12]</ref>, we train it on the MS COCO train+val35k dataset with t ref>. To further test the generalization ability of the RFB module, we link RFB to MobileNet-SSD <ref type=""bibr"" target=""#b15"">[12]</ref>. Following <ref type=""bibr"" target=""#b15"">[12]</ref>, we train it on the MS COCO train+val35k dataset with the same schedule and make evaluation on minival. Tabl sts the performance to 80.5%.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>Framework</head><p>Model mAP (%) #parameters SSD 300 MobileNet <ref type=""bibr"" target=""#b15"">[12]</ref> 19.3% 6.8M SSD 300 MobileNet+RFB 20.7% 7.4M</p><p>Table <ref type=""table"">5</ref>. Accuracies on MS COCO min",0
"promoted the performance of object detection on major challenges and benchmarks, such as Pascal VOC <ref type=""bibr"" target=""#b8"">[5]</ref>, MS COCO <ref type=""bibr"" target=""#b24"">[21]</ref>, and ILSVRC <ref type=""bibr"" target=""#b30"">[27]</ref>. They formulate this issue as a two-stage problem and .tei-c.org/ns/1.0""><head n=""4"">Experiments</head><p>We conduct experiments on the Pascal VOC 2007 <ref type=""bibr"" target=""#b8"">[5]</ref> and MS COCO <ref type=""bibr"" target=""#b24"">[21]</ref> datasets, which have 20 and 80 object categories respectively. In VOC 2007, a predicted bounding box is posi",0
"ct features from deeper CNN backbones, like ResNet <ref type=""bibr"" target=""#b14"">[11]</ref> and Inception <ref type=""bibr"" target=""#b34"">[31]</ref>; <ref type=""bibr"" target=""#b22"">[19]</ref> introduces a top-down architecture to construct feature pyramids, integrating low-level and high-level infor a number of effective extensions are proposed to further improve the detection accuracy, such as R-FCN <ref type=""bibr"" target=""#b20"">[17]</ref>, FPN <ref type=""bibr"" target=""#b22"">[19]</ref>, Mask R-CNN <ref type=""bibr"" target=""#b12"">[9]</ref>.</p><p>One-stage detector: The most representative one- tes: (1) to up-sample the conv7 fc feature maps and concat it with the conv4 3 before applying the RFB-s module, sharing a similar strategy as in FPN <ref type=""bibr"" target=""#b22"">[19]</ref>; and (2) to add a branch with a 7 × 7 kernel in all RFB layers. As we can see in Table <ref type=""table"" tar such a simple module, RFB Net delivers relatively decent scores that are comparable to the ones of up-to-date deeper backbone network based detectors <ref type=""bibr"" target=""#b22"">[19,</ref><ref type=""bibr"" target=""#b21"">18,</ref><ref type=""bibr"" target=""#b23"">20]</ref> and retains the fast speed o",0
"latively decent scores that are comparable to the ones of up-to-date deeper backbone network based detectors <ref type=""bibr"" target=""#b22"">[19,</ref><ref type=""bibr"" target=""#b21"">18,</ref><ref type=""bibr"" target=""#b23"">20]</ref> and retains the fast speed of the original lightweight detector. Addi",0
"ociates only 4 default boxes at conv4 3, conv10 2, and conv11 2 feature map locations and 6 default anchors for all the other layers. Recent research <ref type=""bibr"" target=""#b16"">[13]</ref> claims that low level features are critical to detecting small objects. We thus suppose that performance, es",0
"target=""#b39"">[36]</ref> with the permission from J. Winawer and H. Horiguchi (https: //archive.nyu.edu/handle/2451/33887).</p><p>fact. For instance, <ref type=""bibr"" target=""#b14"">[11]</ref> and <ref type=""bibr"" target=""#b18"">[15]</ref> extract features from deeper CNN backbones, like ResNet <ref t ance, <ref type=""bibr"" target=""#b14"">[11]</ref> and <ref type=""bibr"" target=""#b18"">[15]</ref> extract features from deeper CNN backbones, like ResNet <ref type=""bibr"" target=""#b14"">[11]</ref> and Inception <ref type=""bibr"" target=""#b34"">[31]</ref>; <ref type=""bibr"" target=""#b22"">[19]</ref> introduce ch are comparable to the top ones reported by the two-stage detectors. Unfortunately their performance gains are credited to the very deep ResNet-101 <ref type=""bibr"" target=""#b14"">[11]</ref> model as well, which limits the efficiency.</p><p>According to the discussion above, to build a fast yet pow same reason, we use a 1 × n plus an n×1 conv-layer to take place of the original n×n conv-layer. Ultimately, we apply the shortcut design from ResNet <ref type=""bibr"" target=""#b14"">[11]</ref> and Inception-ResNet V2 <ref type=""bibr"" target=""#b34"">[31]</ref>. Dilated pooling or convolution layer: Thi ""http://www.tei-c.org/ns/1.0""><head>Method</head><p>Backbone Data mAP(%) FPS Faster <ref type=""bibr"" target=""#b29"">[26]</ref> VGG 07+12 73.2 7 Faster <ref type=""bibr"" target=""#b14"">[11]</ref> ResNet-101 07+12 76.4 5 R-FCN <ref type=""bibr"" target=""#b20"">[17]</ref> ResNet-101 07+12 80.5 9 YOLOv2 544 <",0
"to understanding the information contained within the activations of deep neural networks. For this, we consider the ""natural pre-image"" technique of <ref type=""bibr"" target=""#b20"">[21]</ref>, whose goal is to characterize the invariants learned by a deep network by inverting it on the set of natura ng it on the set of natural images. We show that an untrained deep convolutional generator can be used to replace the surrogate natural prior used in <ref type=""bibr"" target=""#b20"">[21]</ref> (the TV norm) with dramatically improved results. Since the new regularizer, like the TV norm, is not learne at work so well for recognition tasks (such as semantic segmentation) is highly detrimental.</p><p>Natural pre-image. The natural pre-image method of <ref type=""bibr"" target=""#b20"">[21]</ref> is a diagnostic tool to study the invariances of a lossy function, such as a deep network, that operates on t arbitrarily. More meaningful visualization can be obtained by restricting the pre-image to a set X of natural images, called a natural pre-image in <ref type=""bibr"" target=""#b20"">[21]</ref>.</p><p>In practice, finding points in the natural pre-image can  Inversion with deep image prior Inversion w br"" target=""#b20"">[21]</ref>.</p><p>In practice, finding points in the natural pre-image can  Inversion with deep image prior Inversion with TV prior <ref type=""bibr"" target=""#b20"">[21]</ref> Pre-trained deep inverting network <ref type=""bibr"" target=""#b7"">[8]</ref> Figure <ref type=""figure"">9</ref> rent layers of AlexNet (trained for classification on ImageNet ISLVRC) using three different regularizers: the Deep Image prior, the TV norm prior of <ref type=""bibr"" target=""#b20"">[21]</ref>, and the network trained to invert representations on a hold-out set <ref type=""bibr"" target=""#b7"">[8]</ref> re not biased by the learning process.</p><p>be done by regularizing the data term similarly to the other inverse problems seen above. The authors of <ref type=""bibr"" target=""#b20"">[21]</ref> prefer to use the TV norm, which is a weak natural image prior, but is relatively unbiased. On the contrary,",1
"used to generate images using such approaches as generative adversarial networks <ref type=""bibr"" target=""#b10"">[11]</ref>, variational autoencoders <ref type=""bibr"" target=""#b15"">[16]</ref>, and direct pixelwise error minimization <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b2",0
"nd the most common downsampling operators, such as Lanczos, are differentiable.</p><p>We evaluate super-resolution ability of our approach using Set5 <ref type=""bibr"" target=""#b1"">[2]</ref> and Set14 <ref type=""bibr"" target=""#b31"">[32]</ref> datasets. We use a scaling factor of 4 to compare to other",0
"how the method can be applied to perform restoration based on pairs of images. In particular, we consider flash-no flash image pair-based restoration <ref type=""bibr"" target=""#b25"">[26]</ref>, where the goal is to obtain an image of a scene with the lighting similar to a no-flash image, while using",0
"generative Con-vNet can potentially produce. The connections between ConvNets and convolutional sparse coding run even deeper and are investigated in <ref type=""bibr"" target=""#b23"">[24]</ref> in the context of recognition networks, and more recently in <ref type=""bibr"" target=""#b24"">[25]</ref>, wher",0
"=""#b10"">[11]</ref>, the recurrent neural aligner (RNA) <ref type=""bibr"" target=""#b11"">[12]</ref>, and the recurrent neural network transducer (RNN-T) <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b13"">14]</ref>. In particular, these architectures allow the output to be decoded a arget=""#b14"">[15]</ref> we evaluated a number of end-to-end models including attention-based models <ref type=""bibr"" target=""#b6"">[7]</ref> and RNN-T <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b13"">14]</ref> trained on ∼12,500 hours of transcribed training data; although end- in Section 7 and Section 8.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2."">RNN-TRANSDUCER</head><p>The RNN-T was proposed by Graves <ref type=""bibr"" target=""#b12"">[13]</ref> as an extension to the connectionist temporal classification (CTC) <ref type=""bibr"" target=""#b16"">[17]</ref> r t &lt; j.</p><p>The RNN-T model, depicted in Figure <ref type=""figure"">1</ref>, consists of an encoder (referred to as the transcription network in <ref type=""bibr"" target=""#b12"">[13]</ref>), a prediction network and a joint network; as described in <ref type=""bibr"" target=""#b14"">[15]</ref>, the R the same form for f joint as described in <ref type=""bibr"" target=""#b13"">[14]</ref>. The entire network is trained jointly to optimize the RNN-T loss <ref type=""bibr"" target=""#b12"">[13]</ref>, which marginalizes over all alignments of target labels with blanks as in CTC, and is computed using dynami ated when blank is output at the last frame, T .</p><p>During inference, the most likely label sequence is computed using beam search as described in <ref type=""bibr"" target=""#b12"">[13]</ref>, with a minor alteration which was found to make the algorithm less computationally intensive without degrad nd to make the algorithm less computationally intensive without degrading performance: we skip summation over prefixes in pref(y) (see Algorithm 1 in <ref type=""bibr"" target=""#b12"">[13]</ref>), unless multiple hypotheses are identical.</p><p>Note that unlike other streaming encoder-decoder architect",1
"d-to-end ASR, conventional systems still remain the state-of-the-art in terms of word error rate (WER) performance. For example, in our previous work <ref type=""bibr"" target=""#b14"">[15]</ref> we evaluated a number of end-to-end models including attention-based models <ref type=""bibr"" target=""#b6"">[7 which are used to train LMs in a traditional speech recognizer. A deficiency of end-to-end systems appears to be in their language modeling capacity <ref type=""bibr"" target=""#b14"">[15]</ref> which may be because large text-only data are not utilized in end-to-end systems.</p><p>In this work we expl er (referred to as the transcription network in <ref type=""bibr"" target=""#b12"">[13]</ref>), a prediction network and a joint network; as described in <ref type=""bibr"" target=""#b14"">[15]</ref>, the RNN-T model can be compared to other encoder-decoder architectures such as ""listen, attend, and spell""",1
"d pronunciation data may be included to improve end-to-end ASR performance. Another contribution of this work is to investigate the use of wordpieces <ref type=""bibr"" target=""#b15"">[16]</ref>, which have been explored previously in the context of machine translation, as a sub-word unit for end-to-en t words, inference for RNN-T with that many output labels would be impractically slow. Therefore, as subword units, we use wordpieces as described in <ref type=""bibr"" target=""#b15"">[16]</ref>. We train a statistical wordpiece model with word counts obtained from text data for segmenting each word in",1
"l, P (φ|W ), is typically built from pronunciation dictionaries curated by expert human linguists, with back-off to a grapheme-to-phoneme (G2P) model <ref type=""bibr"" target=""#b5"">[6]</ref> for out of dictionary words. Finally, an N-gram model trained on text data may be used as a language model, P",0
"tectures have been proposed previously, including the neural transducer <ref type=""bibr"" target=""#b10"">[11]</ref>, the recurrent neural aligner (RNA) <ref type=""bibr"" target=""#b11"">[12]</ref>, and the recurrent neural network transducer (RNN-T) <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bib target=""#b12"">[13]</ref>), unless multiple hypotheses are identical.</p><p>Note that unlike other streaming encoder-decoder architectures such as RNA <ref type=""bibr"" target=""#b11"">[12]</ref> and NT <ref type=""bibr"" target=""#b10"">[11]</ref>, the prediction network is not conditioned on the encoder o",0
"x), the likelihood is typically replaced with a scaled likelihood obtained by dividing the posterior with the prior, P (φ), in socalled hybrid models <ref type=""bibr"" target=""#b0"">[1]</ref>. Deep recurrent neural networks with long short-term memory (LSTM) cells <ref type=""bibr"" target=""#b1"">[2]</re",0
"; &lt;space&gt; &lt;and&gt; &lt;space&gt; &lt;the&gt; &lt;space&gt; &lt;ha&gt; &lt;re&gt;. Wordpieces have be shown to benefit end-to-end recognition <ref type=""bibr"" target=""#b19"">[20]</ref> since they offer a balance with longer context than graphemes and a tunable number of labels. Since the word",0
"C has been widely used in previous works to train end-toend ASR models <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b18"">19]</ref>. However, a major limitation of Fig. <ref type=""figure"">1</ref>. The RNN-T model. The model consists of an en",0
"ing separately-trained acoustic, pronunciation and language model components. A particular class of architecures known as sequence-to-sequence models <ref type=""bibr"" target=""#b9"">[10]</ref> are particularly suited for end-to-end ASR as they include an encoder network which corresponds to the acoust",0
"aligner (RNA) <ref type=""bibr"" target=""#b11"">[12]</ref>, and the recurrent neural network transducer (RNN-T) <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b13"">14]</ref>. In particular, these architectures allow the output to be decoded as soon as the first input is encoded, wit mber of end-to-end models including attention-based models <ref type=""bibr"" target=""#b6"">[7]</ref> and RNN-T <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b13"">14]</ref> trained on ∼12,500 hours of transcribed training data; although end-to-end approaches were found to be compar ).</p><formula xml:id=""formula_5"">z t,u = f joint (h enc t , h dec u )<label>(6)</label></formula><p>We use the same form for f joint as described in <ref type=""bibr"" target=""#b13"">[14]</ref>. The entire network is trained jointly to optimize the RNN-T loss <ref type=""bibr"" target=""#b12"">[13]</ref>, een previously shown that initializing RNN-T encoder parameters from a model trained with the CTC loss is beneficial for the phoneme recognition task <ref type=""bibr"" target=""#b13"">[14]</ref>. We experimented with initializing encoder networks from models trained with the CTC loss and with initializ",0
"ere has been considerable interest in training end-to-end models for ASR <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b8"">9]</ref>, which directly output word transcripts given the input audio. <ref type=""foot"" target=""#foot_0"">1</ref> Thus, ich corresponds to the language model.</p><p>One drawback of typical encoder-decoder type architectures (e.g., <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b8"">9]</ref>) is that the entire input sequence is encoded before the output sequence may be decoded and thus these models c",0
"eal-time streaming speech recognition. Several streaming encoder-decoder architectures have been proposed previously, including the neural transducer <ref type=""bibr"" target=""#b10"">[11]</ref>, the recurrent neural aligner (RNA) <ref type=""bibr"" target=""#b11"">[12]</ref>, and the recurrent neural netw eses are identical.</p><p>Note that unlike other streaming encoder-decoder architectures such as RNA <ref type=""bibr"" target=""#b11"">[12]</ref> and NT <ref type=""bibr"" target=""#b10"">[11]</ref>, the prediction network is not conditioned on the encoder output. This allows for the the pre-training of th",0
"acked every 30ms to a single 240-dimensional acoustic feature vector. To achieve noise robustness acoustic training data is distorted as described in <ref type=""bibr"" target=""#b23"">[24]</ref>. The pronunciation model is a dictionary containing hundreds of thousands of human expert transcribed US Eng",0
"uage model, P (W ).</p><p>Recently, there has been considerable interest in training end-to-end models for ASR <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b8"">9]</ref>, which directly output word transcripts given the input audio. <ref type= the probability of outputting no label corresponding to a given input frame. CTC has been widely used in previous works to train end-toend ASR models <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b18"">19]</ref>. However, a major limitation o",0
"expert transcribed US English word pronunciations. Additional word pronunciations are learned from audio data using pronunciation learning techniques <ref type=""bibr"" target=""#b24"">[25]</ref>. For out-ofdictionary words a G2P model is trained using transcribed word pronunciations. A 5-gram language",0
"non-overlapping camera views, and research on this topic has attracted considerable attention in recent years <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b28"">29 to be labeled. It is a tough task even for humans to identify the same person in different camera views among a potentially huge number of imposters <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b19"">20]</ref>. At the same time, pairwise labeled data is required for each pair of (5)</label></formula><p>where L V = D − S V is the Laplacian matrix and D is the degree matrix with each element D ii = j S V (i, j). As discussed in <ref type=""bibr"" target=""#b8"">[9]</ref>, minimizing the pairwise constraint will force the similar representations to be close to each other. Followin er. Following the assumption that visually similar images of a person have a high probability of sharing the similar representation features in re-id <ref type=""bibr"" target=""#b8"">[9]</ref>, this will make early active learning schema more suitable for re-id applications.</p><p>After introducing the",1
"ost representative and/or informative for training. Active learning is widely studied to solve this kind of sample selection problem. As discussed in <ref type=""bibr"" target=""#b17"">[18]</ref>, active learning methods can be divided into two categories. The first category of algorithms select the mos hat is difficult to analyze. They thus require a certain number of labeled samples to evaluate the uncertainty of the unlabeled data or sampling bias <ref type=""bibr"" target=""#b17"">[18]</ref> will result. It is therefore recommended that such methods are only applied in the mid-stage of experiments bibr"" target=""#b26"">[27]</ref>. These kinds of active learning algorithms are referred to as early active learning or early stage experimental design <ref type=""bibr"" target=""#b17"">[18]</ref>. We illustrate the procedures of and example of the traditional active learning algorithm, QUIRE <ref type="" • • , a n ] ∈ R m×n , V ⊂ X, |V| = m. (7)</formula><p>Finding the optimal subset V ⊂ X in Eq. ( <ref type=""formula"">7</ref>) is NP-hard. Inspired by <ref type=""bibr"" target=""#b17"">[18]</ref>, we relax the problem to the following problem by introducing the 2,0 -norm for structure sparsity:</p><form mula"">8</ref>) a non-convex problem. At the same time, the least squared loss used in Eq. ( <ref type=""formula"">8</ref>) is sensitive to the outliers <ref type=""bibr"" target=""#b17"">[18]</ref>, which makes the algorithm not robust.</p><p>We note that in previous researches <ref type=""bibr"" target=""#b /ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b25"">26]</ref>, the 2,1 -norm is used instead of the 2,0 -norm. It is shown in <ref type=""bibr"" target=""#b17"">[18]</ref> that the 2,1 -norm is the minimum convex hull of the 2,0 -norm when row-sparsity is required. In other words ng.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2."">K-means</head><p>We use the K-means algorithm as another baseline algorithm as in <ref type=""bibr"" target=""#b17"">[18]</ref>. In each experiment, samples are ranked by their distances from the K cluster centers in ascending order. 3. of informative samples from a candidate dataset. It formulates a regularized linear regression problem which minimizes reconstruction error. 5. RRSS <ref type=""bibr"" target=""#b17"">[18]</ref> Early active learning via Robust Representation and Structured Sparsity is a early active learning algorithm he kernelized meth- ods is better than the performance of the linear methods with our algorithm. This is consistent with the mathematical analysis in <ref type=""bibr"" target=""#b17"">[18]</ref> that kernelization produces more discriminative representation by mapping data into high-dimensional feature r"" target=""#b17"">[18]</ref>, which makes the algorithm not robust.</p><p>We note that in previous researches <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b25"">26]</ref>, the 2,1 -norm is used instead of the 2,0 -norm. It is shown in <ref -norm when row-sparsity is required. In other words, minimization of A 2,1 will achieve the same result as A 2,0 when A is row-sparse. As analyzed in <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b29"">30]</ref>, the 2,1 -norm can suppress the effect of outlying samples. We there",1
"ustering-based methods <ref type=""bibr"" target=""#b18"">[19,</ref><ref type=""bibr"" target=""#b15"">16]</ref> and transductive experimental design methods <ref type=""bibr"" target=""#b26"">[27]</ref>. These kinds of active learning algorithms are referred to as early active learning or early stage experimen nt the whole data X through the linear transformation matrix A. The selected samples are therefore considered to be the most representative.</p><p>In <ref type=""bibr"" target=""#b26"">[27]</ref>, an early active learning via a Transduction Experimental Design algorithm (TED) is proposed with the aim of , Eq. ( <ref type=""formula"">2</ref>) is an NP-hard problem to solve, thus an approximate solution by a sequential optimization problem is proposed in <ref type=""bibr"" target=""#b26"">[27]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2.2"">Early Active Learning with Pairwise Constr >Examples is an algorithm which queries the most informative and representative examples for labeling using the min-max margin-based approach. 4. TED <ref type=""bibr"" target=""#b26"">[27]</ref> Active learning via Transduction Experimental Design is an algorithm that selects a subset of informative sa",1
"ther words, minimization of A 2,1 will achieve the same result as A 2,0 when A is row-sparse. As analyzed in <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b29"">30]</ref>, the 2,1 -norm can suppress the effect of outlying samples. We therefore reformulate Eq. ( <ref type=""formula s of 385 individuals from two distinct cameras. Camera B records 749 persons and Camera A records 385 persons, 200 of whom are same persons. 3. i-LID <ref type=""bibr"" target=""#b29"">[30]</ref> The i-LID dataset records 119 individuals captured by three different cameras in an airport terminal. It con",0
"2 √ a i 2 2 +η</formula><p>. η is a very small constant. It can be verified that when η → 0 the problem with η reduces to the original problem in Eq. <ref type=""bibr"" target=""#b11"">(12)</ref>.</p><p>Algorithm 1: Algorithm for solving problem in Eq. <ref type=""bibr"" target=""#b9"">(10)</ref> Input: The 39 × 17 to 141 × 72.</p><p>In the experiments, we use the recently proposed Local Maximal Occurrence (LOMO) features for person image representation <ref type=""bibr"" target=""#b11"">[12]</ref>. As in <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b19"">20]</ref>, all person images target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b19"">20]</ref>, all person images are scaled to 128 × 48 pixels. We then use the default setting in <ref type=""bibr"" target=""#b11"">[12]</ref> to produce a 29,960 dimension feature for each image.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><he on Analysis algorithm seeks a common subspace between the proposed images extracted from disjoint cameras and projects them into a new space. 3. XQDA <ref type=""bibr"" target=""#b11"">[12]</ref> Cross-view Quadratic Discriminant Analysis learns a discriminant low dimensional subspace by cross-view quad the most commonly used performance measure for person re-id algorithms <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b11"">12]</ref>. CMC calculates the probability that there exists a candidate image in the rank k gallery set that appears to",0
"lready some labeled samples. They include uncertainty sampling methods <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b0"">1,</ref><ref type=""bibr"" target=""#b21"">22]</ref> query by committee methods <ref type=""bibr"" target=""#b20"">[21,</ref><re",0
"samples for labeling when there are already some labeled samples. They include uncertainty sampling methods <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b0"">1,</ref><ref type=""bibr"" target=""#b21"">22]</ref> query by committee methods <ref t tal design <ref type=""bibr"" target=""#b17"">[18]</ref>. We illustrate the procedures of and example of the traditional active learning algorithm, QUIRE <ref type=""bibr"" target=""#b5"">[6]</ref>, and our early active learning algorithm with pairwise constraint (abbreviated as EALPC) in Fig. <ref type=""fi pe=""bibr"" target=""#b17"">[18]</ref>. In each experiment, samples are ranked by their distances from the K cluster centers in ascending order. 3. QUIRE <ref type=""bibr"" target=""#b5"">[6]</ref> Active learning by Querying Informative and Representative</p><p>Examples is an algorithm which queries the mo sis, etc.</p></div><figure xmlns=""http://www.tei-c.org/ns/1.0"" xml:id=""fig_0""><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. Procedures of QUIRE<ref type=""bibr"" target=""#b5"">[6]</ref> (upper) and our Early Active Learning with Pairwise Constraint (EALPC) (lower). In QUIRE, pre-labeled samples",0
"sensitive to the outliers <ref type=""bibr"" target=""#b17"">[18]</ref>, which makes the algorithm not robust.</p><p>We note that in previous researches <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b25"">26]</ref>, the 2,1 -norm is used inste la_13"">10</ref>).</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4"">Convergence Analysis</head><p>We first introduce a lemma proposed in <ref type=""bibr"" target=""#b16"">[17]</ref>:</p><p>Lemma 1. For any arbitrary vector m and n there is</p><formula xml:id=""formula_23"">m 2 − m 2 2 2 n 2",0
"ing reliable supervised models. In the category of early active learning, there are clustering-based methods <ref type=""bibr"" target=""#b18"">[19,</ref><ref type=""bibr"" target=""#b15"">16]</ref> and transductive experimental design methods <ref type=""bibr"" target=""#b26"">[27]</ref>. These kinds of active",0
">Datasets We analyze performance of active learning for re-id on four widely referred benchmark datasets for person re-identification.</p><p>1. VIPeR <ref type=""bibr"" target=""#b3"">[4]</ref> The VIPeR dataset contains 1,264 images of 632 persons from two non-overlapping camera views. Two images are t",0
"ng Characteristic (CMC) curve, which is the most commonly used performance measure for person re-id algorithms <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b11"">12]</ref>. CMC calculates the probability that there exists a candidate image i",0
"erson from camera shots across pairs of non-overlapping camera views, and research on this topic has attracted considerable attention in recent years <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b14"">15,",0
"the algorithm not robust.</p><p>We note that in previous researches <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b25"">26]</ref>, the 2,1 -norm is used instead of the 2,0 -norm. It is shown in <ref type=""bibr"" target=""#b17"">[18]</ref> tha",0
"of algorithms select the most informative samples for labeling when there are already some labeled samples. They include uncertainty sampling methods <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b0"">1,</ref><ref type=""bibr"" target=""#b21"">22",0
"ude uncertainty sampling methods <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b0"">1,</ref><ref type=""bibr"" target=""#b21"">22]</ref> query by committee methods <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b2"">3]</ref>. M",0
"enerative TTS model based on the sequence-to-sequence (seq2seq) <ref type=""bibr"" target=""#b17"">(Sutskever et al., 2014)</ref> with attention paradigm <ref type=""bibr"" target=""#b3"">(Bahdanau et al., 2014)</ref>. Our model takes characters as input and outputs raw spectrogram, using several techniques /p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3"">MODEL ARCHITECTURE</head><p>The backbone of Tacotron is a seq2seq model with attention <ref type=""bibr"" target=""#b3"">(Bahdanau et al., 2014;</ref><ref type=""bibr"" target=""#b21"">Vinyals et al., 2015)</ref>. Figure <ref type=""figure"" targe",1
"tion errors to accumulate quickly. In this paper, we propose Tacotron, an end-to-end generative TTS model based on the sequence-to-sequence (seq2seq) <ref type=""bibr"" target=""#b17"">(Sutskever et al., 2014)</ref> with attention paradigm <ref type=""bibr"" target=""#b3"">(Bahdanau et al., 2014)</ref>. Our",1
"were used. We compare our model with a parametric (based on LSTM <ref type=""bibr"" target=""#b20"">(Zen et al., 2016)</ref>) and a concatenative system <ref type=""bibr"" target=""#b7"">(Gonzalvo et al., 2016)</ref>, both of which are in production. As shown in Table <ref type=""table"" target=""#tab_3"">2</r",0
"m converges after 50 iterations (in fact, about 30 iterations seems to be enough), which is reasonably fast. We implemented Griffin-Lim in TensorFlow <ref type=""bibr"" target=""#b0"">(Abadi et al., 2016</ref>) hence it's also part of the model. While Griffin-Lim is differentiable (it does not have trai",0
"illustrated in Figure <ref type=""figure"" target=""#fig_1"">2</ref>. CBHG consists of a bank of 1-D convolutional filters, followed by highway networks <ref type=""bibr"" target=""#b16"">(Srivastava et al., 2015)</ref> and a bidirectional gated recurrent unit (GRU) <ref type=""bibr"" target=""#b6"">(Chung et",0
"ask for an end-to-end model: it must cope with large variations at the signal level for a given input. Moreover, unlike end-to-end speech recognition <ref type=""bibr"" target=""#b5"">(Chan et al., 2016</ref>   or machine translation <ref type=""bibr"" target=""#b23"">(Wu et al., 2016)</ref>, TTS outputs ar",0
"bidirectional GRU RNN on top to extract sequential features from both forward and backward context. CBHG is inspired from work in machine translation <ref type=""bibr"" target=""#b12"">(Lee et al., 2016)</ref>, where the main differences from <ref type=""bibr"" target=""#b12"">Lee et al. (2016)</ref> includ d context. CBHG is inspired from work in machine translation <ref type=""bibr"" target=""#b12"">(Lee et al., 2016)</ref>, where the main differences from <ref type=""bibr"" target=""#b12"">Lee et al. (2016)</ref> include using non-causal convolutions, batch normalization, residual connections, and stride=1",0
"ry deep convolutional neural networks (CNN) to perform single image super-resolution (SISR), and significant improvements over shallow CNN structures <ref type=""bibr"" target=""#b1"">[2]</ref> have been observed. One benefit from using deeper networks is that larger receptive field takes more contextua neighbor embedding <ref type=""bibr"" target=""#b3"">[4]</ref>, random forest <ref type=""bibr"" target=""#b19"">[20]</ref> and convolutional neural network <ref type=""bibr"" target=""#b1"">[2]</ref>. Among them, the CNN-based approaches <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b11""> 0"">[11,</ref><ref type=""bibr"" target=""#b11"">12]</ref> have recently set state of the art for SISR. A network with three layers was first developed in <ref type=""bibr"" target=""#b1"">[2]</ref> to learn an end-to-end mapping for SR. Subsequently, a deep network with 20 layers was proposed in <ref type="" DenseNet block.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.2."">Deconvolution layers</head><p>In previous SR methods such as SRCNN <ref type=""bibr"" target=""#b1"">[2]</ref> and VDSR <ref type=""bibr"" target=""#b10"">[11]</ref>, bicubic interpolation is used to upscale LR images to the d the results using the proposed method and those using other SISR methods, including bicubic, Aplus <ref type=""bibr"" target=""#b23"">[24]</ref>, SRCNN <ref type=""bibr"" target=""#b1"">[2]</ref>, VDSR <ref type=""bibr"" target=""#b10"">[11]</ref> and DRCN <ref type=""bibr"" target=""#b11"">[12]</ref>. The implem B-0.8dB over state-of-theart results on different datasets. On average, an increase of about 1.0 dB using the proposed method was achieved over SRCNN <ref type=""bibr"" target=""#b1"">[2]</ref> with 3-layer CNN and an increase of about 0.5 dB over VDSR <ref type=""bibr"" target=""#b10"">[11]</ref> with 20-l lock is to avoid the gradient vanishing problem, allowing us to train very deep Dataset Bicubic Aplus <ref type=""bibr"" target=""#b23"">[24]</ref> SRCNN <ref type=""bibr"" target=""#b1"">[2]</ref> VDSR <ref type=""bibr"" target=""#b10"">[11]</ref> DRCN <ref type=""bibr"" target=""#b11"">[12]</ref>   In addition, t layers to bottom layers. This helps the flow of information and gradient through the network, making it easy to train. In addition, in previous works <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b10"">11]</ref>, only high-level features at top layers were used in the reconstructio f> to speedup the converging speed in training and also to improve the reconstruction performance. Instead of using interpolation for upscaling as in <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b10"">11]</ref>, recent studies <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr <ref type=""figure"" target=""#fig_0"">1</ref>, three different types of network structures were studied and compared in our work. As in previous methods <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b10"">11]</ref>, only the feature maps at the top layer are used as input for reconstr",1
"large regions from LR images so that sufficient knowledge can be captured for recovering the high-frequency details in HR images.</p><p>Recent works <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b11"">12]</ref> have successfully used very deep convolutional neural networks (CNN) f type=""bibr"" target=""#b19"">[20]</ref> and convolutional neural network <ref type=""bibr"" target=""#b1"">[2]</ref>. Among them, the CNN-based approaches <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b11"">12]</ref> have recently set state of the art for SISR. A network with three la flow of information and gradient through the network, making it easy to train. In addition, in previous works <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b10"">11]</ref>, only high-level features at top layers were used in the reconstruction of HR images. The features at low lev raining and also to improve the reconstruction performance. Instead of using interpolation for upscaling as in <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b10"">11]</ref>, recent studies <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b20"">21]</ref> have demonstr ef>, three different types of network structures were studied and compared in our work. As in previous methods <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b10"">11]</ref>, only the feature maps at the top layer are used as input for reconstructing the HR output. We denote this st veloped in <ref type=""bibr"" target=""#b1"">[2]</ref> to learn an end-to-end mapping for SR. Subsequently, a deep network with 20 layers was proposed in <ref type=""bibr"" target=""#b10"">[11]</ref> to improve the reconstruction accuracy of CNN. The residuals between the HR images and the interpolated LR i rget=""#b10"">[11]</ref> to improve the reconstruction accuracy of CNN. The residuals between the HR images and the interpolated LR images were used in <ref type=""bibr"" target=""#b10"">[11]</ref> to speedup the converging speed in training and also to improve the reconstruction performance. Instead of u .tei-c.org/ns/1.0""><head n=""3.2."">Deconvolution layers</head><p>In previous SR methods such as SRCNN <ref type=""bibr"" target=""#b1"">[2]</ref> and VDSR <ref type=""bibr"" target=""#b10"">[11]</ref>, bicubic interpolation is used to upscale LR images to the HR space. After that, the SR process including th ose using other SISR methods, including bicubic, Aplus <ref type=""bibr"" target=""#b23"">[24]</ref>, SRCNN <ref type=""bibr"" target=""#b1"">[2]</ref>, VDSR <ref type=""bibr"" target=""#b10"">[11]</ref> and DRCN <ref type=""bibr"" target=""#b11"">[12]</ref>. The implementations of these methods have been released using the proposed method was achieved over SRCNN <ref type=""bibr"" target=""#b1"">[2]</ref> with 3-layer CNN and an increase of about 0.5 dB over VDSR <ref type=""bibr"" target=""#b10"">[11]</ref> with 20-layer CNN. It should be mentioned that the most significant improvement is obtained on the very chal em, allowing us to train very deep Dataset Bicubic Aplus <ref type=""bibr"" target=""#b23"">[24]</ref> SRCNN <ref type=""bibr"" target=""#b1"">[2]</ref> VDSR <ref type=""bibr"" target=""#b10"">[11]</ref> DRCN <ref type=""bibr"" target=""#b11"">[12]</ref>   In addition, the proposed framework not only achieves impre",1
"ity-based methods, other sophisticated learning techniques have been developed to model the mapping from LR to HR space, including neighbor embedding <ref type=""bibr"" target=""#b3"">[4]</ref>, random forest <ref type=""bibr"" target=""#b19"">[20]</ref> and convolutional neural network <ref type=""bibr"" tar",0
"rops layers to improve the training of deep residual networks, which demonstrates a great amount of redundancy in deep residual networks. FractalNets <ref type=""bibr"" target=""#b13"">[14]</ref> combines several parallel networks with different depths and many short paths are created in the networks. D",0
"atasets for training and testing. Specifically, 50,000 images were randomly selected from ImageNet for the training. During testing, the dataset Set5 <ref type=""bibr"" target=""#b0"">[1]</ref> and Set14 <ref type=""bibr"" target=""#b28"">[29]</ref> are often used for SR benchmark. The B100 from the Berkele",0
"Image SR may benefit from the collective knowledge of features at different levels. Moreover, previous studies <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b6"">7]</ref> have shown that redundant feature maps are learnt in different layers of deep networks. The reuse of feature ma =""bibr"" target=""#b13"">[14]</ref> combines several parallel networks with different depths and many short paths are created in the networks. DenseNets <ref type=""bibr"" target=""#b6"">[7]</ref> links all layers in the networks and tries to fully explore the advantages of skip connections. All these netw learning low-level features, a set of DenseNet blocks are adopted for learning the high-level features. The DenseNet structure was first proposed in <ref type=""bibr"" target=""#b6"">[7]</ref>. Different from ResNets as proposed in <ref type=""bibr"" target=""#b5"">[6]</ref>, the feature maps are concatena seNet can substantially reduce the number of parameters through feature reuse, thus requiring less memory and computation to achieve high performance <ref type=""bibr"" target=""#b6"">[7]</ref>. Here, we employ the DenseNet structure as a building block in our network. The structure of each denseNet blo",0
"ss. Further, adversarial loss using a generative adversarial network (GAN) was added to the loss function in <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b18"">19]</ref> and photo-realistic SR images can be generated. Although the generated high frequency details in SRGAN <ref t",0
"=""#b0"">[1]</ref> and Set14 <ref type=""bibr"" target=""#b28"">[29]</ref> are often used for SR benchmark. The B100 from the Berkeley segmentation dataset <ref type=""bibr"" target=""#b17"">[18]</ref> consisting of 100 natural images were used for testing. In addition, the proposed method was also evaluated",0
"of 128 feature maps from each block. The filter size was set to 3 × 3 in all weight layers. The weights were initialized using the method proposed in <ref type=""bibr"" target=""#b4"">[5]</ref> and the biases were initialized to zero. The rectified linear units (ReLu) was used as the activation function",0
"SR to replace the low-level pixel-wise loss. Further, adversarial loss using a generative adversarial network (GAN) was added to the loss function in <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b18"">19]</ref> and photo-realistic SR images can be generated. Although the generat </ref><ref type=""bibr"" target=""#b18"">19]</ref> and photo-realistic SR images can be generated. Although the generated high frequency details in SRGAN <ref type=""bibr"" target=""#b14"">[15]</ref> may be 'fake' texture patterns, it yields visually pleasing high-resolution images. Note that the proposed m",0
""">Introduction</head><p>Super-Resolution (SR) from a single image has recently received a huge boost in performance using Deep-Learning based methods <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b11"">12, terpolated to the output size. As done in previous CNN-based SR methods <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b3"">4]</ref>, we only learn the residual between the interpolated LR and its HR parent. We use L 1 loss with ADAM optimizer against externally-supervised methods that were exhaustively trained for these conditions. In fact, ZSSR is significantly better than the older SRCNN <ref type=""bibr"" target=""#b3"">[4]</ref>, and in some cases achieves comparable or better results than VDSR <ref type=""bibr"" target=""#b8"">[9]</ref> (wh",1
"n (SR) from a single image has recently received a huge boost in performance using Deep-Learning based methods <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b12"">13 as 64 channels. We use ReLU activations on each layer. The network input is interpolated to the output size. As done in previous CNN-based SR methods <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b3"">4]</ref>, we only learn the residual betwe ZSSR surpasses SotA SR by a large margin. All reported numerical were produced using the evaluation script of <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b9"">10]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.1."">The 'Ideal' Case</head><p>While this is not",1
"ds (supervised <ref type=""bibr"" target=""#b21"">[22]</ref> or unsupervised <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b6"">7]</ref>) by a few dBs -a huge margin! This boost in performance was obtained with very deep and well engineered CNNs, w r many unsupervised image enhancement methods, including unsupervised SR <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b6"">7]</ref>, Blind-SR <ref type=""bibr"" target=""#b14"">[15]</ref> (when the downscaling kernel is unknown), Blind-Deblurring =""bibr"" target=""#b8"">[9]</ref> (which was the SotA until a year ago). Within the unsupervised-SR regime, ZSSR outperforms the leading method SelfExSR <ref type=""bibr"" target=""#b6"">[7]</ref> by a large margin.</p><p>Moreover, in images with very strong internal repetitive structures, ZSSR tends to su",0
"using Deep-Learning based methods <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b12"">13]</ref>. The recent SotA (State of the Art) method <ref type=""bibr"" target=""# while these typically damage SR supervised methods to the point where bicubic interpolation outperforms current SotA SR methods! Comparison to SRGAN <ref type=""bibr"" target=""#b11"">[12]</ref>: SRGAN is also trained for the ideal case. In those cases, SRGAN methods tend to hallucinate visually pleasi",0
"l is more important than sophisticated image priors, and (ii) using the wrong donwscaling kernel leads to oversmoothed SR results. (using the code of <ref type=""bibr"" target=""#b0"">[1]</ref>). SRGAN obtains poor visual quality on 'non-ideal' LR images -Please zoom-in on screen.</p><p>A special case o w.tei-c.org/ns/1.0"" xml:id=""fig_5""><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Visual comparison of ZSSR to SRGAN [12](using the code of<ref type=""bibr"" target=""#b0"">[1]</ref>). SRGAN obtains poor visual quality on 'non-ideal' LR images -Please zoom-in on screen.</figDesc><graphic url=",0
"preference was further shown to be particularly strong under growing uncertainty and image degradations (see <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b16"">17]</ref> for details).</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3."">Image-Specific CNN</head><p>Our",0
"<ref type=""bibr"" target=""#b6"">7]</ref>, Blind-SR <ref type=""bibr"" target=""#b14"">[15]</ref> (when the downscaling kernel is unknown), Blind-Deblurring <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b1"">2]</ref>, Blind-Dehazing <ref type=""bibr"" target=""#b2"">[3]</ref>, and more. Whi",0
"<ref type=""bibr"" target=""#b12"">[13]</ref> exceeds previous non-Deep SR methods (supervised <ref type=""bibr"" target=""#b21"">[22]</ref> or unsupervised <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b6"">7]</ref>) by a few dBs -a huge margin! This of small pieces of information (e.g., small image patches) across scales of a single image, was shown to be a very strong property of natural images <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b23"">24]</ref>. This formed the basis for many unsupervised image enhancement methods "">[5,</ref><ref type=""bibr"" target=""#b23"">24]</ref>. This formed the basis for many unsupervised image enhancement methods, including unsupervised SR <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b6"">7]</ref>, Blind-SR <ref type=""bibr"" target= many times inside a single image, both within the same scale, as well as across different image scales. This observation was empirically verified by <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b23"">24]</ref> using hundreds of natural images, and was shown to be true for almost xamples.</p><p>For the sake of robustness, as well as to allow large SR scale factors s even from very small LR images, the SR is performed gradually <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b20"">21]</ref>. Our algorithm is applied for several intermediate scale-factors (s 1 e.</p><p>Fig. <ref type=""figure"" target=""#fig_2"">3</ref> shows an example of a simple single-image SR based on internal patch recurrence (courtesy of <ref type=""bibr"" target=""#b4"">[5]</ref>). Note that it is able to recover the tiny handrails in the tiny balconies, since evidence to their existence power is exemplified here using a 'fractal-like' image, the internal predictivepower was analyzed and shown to be strong for almost any natural image <ref type=""bibr"" target=""#b4"">[5]</ref>.</p><p>In fact, it was empirically shown by <ref type=""bibr"" target=""#b23"">[24]</ref> that the internal entrop _2""><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Internal predictive power of image-specific information. Simple unsupervised internal-SR<ref type=""bibr"" target=""#b4"">[5]</ref> is able to reconstruct the tiny handrail in the tiny balconies, whereas externally-trained SotA SR methods fai median of these 8 outputs rather than their mean. We further combine it with the back-projection technique of <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b4"">5]</ref>, so that each of the 8 output images undergoes several iterations of back-projection and finally the median ima <ref type=""bibr"" target=""#b12"">[13]</ref> grows quadratically with the image size. While it is fast on small images, for a 800×800 image it performs <ref type=""bibr"" target=""#b4"">5</ref>  </p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.2."">Adapting to the Test Image</head><p>When the",0
"ype=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b12"">13]</ref>. The recent SotA (State of the Art) method <ref type=""bibr"" target=""#b12"">[13]</ref> exceeds previous non-Dee ibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b12"">13]</ref>. The recent SotA (State of the Art) method <ref type=""bibr"" target=""#b12"">[13]</ref> exceeds previous non-Deep SR methods (supervised <ref type=""bibr"" target=""#b21"">[22]</ref> or unsupervised < her reliability of non-synthesized HR examples over synthesize ones.</p><p>Lastly, we use a method similar to the geometric self-ensemble proposed in <ref type=""bibr"" target=""#b12"">[13]</ref> (which generates 8 different outputs for the 8 rotations+flips of the test image I, and then combines them). therefore a tradeoff between runtime and the output quality, which is up to the user to choose.</p><p>For compariosn, the test-time of leading EDSR+ <ref type=""bibr"" target=""#b12"">[13]</ref> grows quadratically with the image size. While it is fast on small images, for a 800×800 image it performs < or low-quality LR images, and for a wide variety of degradation types, the image-specific CNN obtains significantly better SR results than SotA EDSR+ <ref type=""bibr"" target=""#b12"">[13]</ref> (see Sec. 4). Similarly, in the case of non-ideal downscaling kernels, the image-specific CNN obtains a sign d experiments on a variety of settings. Interestingly, ZSSR produces competitive results (although VDSR <ref type=""bibr"" target=""#b8"">[9]</ref> EDSR+ <ref type=""bibr"" target=""#b12"">[13]</ref> Blind-SR <ref type=""bibr"" target=""#b14"">[15]</ref> ZSSR [estimated kernel] (ours) ZSSR [true kernel] (ours) lied to those images. Please see text for more details.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>Ground Truth VDSR [9]</head><p>EDSR+ <ref type=""bibr"" target=""#b12"">[13]</ref> ZSSR (ours) (PSNR, SSIM) (20.11, 0.9136) (25.29 / 0.9627) (25.68 / 0.9546) Figure <ref type=""figure"">5</ref> 'ideal' supervised setting (i.e., bicubic downscaling).</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>Bicubic interpolation</head><p>EDSR+ <ref type=""bibr"" target=""#b12"">[13]</ref> ZSSR (ours) 27.9216 / 0.7504 27.5600 / 0.7135 28.6148 / 0.7809 Table <ref type=""table"">3</ref>: SR in the pr R image was subsampled by a different random kernel. Table <ref type=""table"">2</ref> compares our against the leading externallysupervised SR methods <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b8"">9]</ref>. We also compared our performance to the unsupervised Blind-SR method",0
"I, and then combines them). We take the median of these 8 outputs rather than their mean. We further combine it with the back-projection technique of <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b4"">5]</ref>, so that each of the 8 output images undergoes several iterations of bac",0
"ideo, we employ a Fisher Vector encoding to aggregate a variable number of features to a fixed-length vector. Fisher Vectors were first introduced in <ref type=""bibr"" target=""#b15"">(Jaakkola and Haussler 1999)</ref> to combine the advantages of generative and discriminative models, and are widely us",1
"esent changes in motion rather than constant motion, which is captured in MBH.</p><p>Audio Features We use MFCC (Mel-frequency Cepstral Coefficients) <ref type=""bibr"" target=""#b5"">(Davis and Mermelstein 1980)</ref> features as our audio features. MFCC has been widely used for ASR (Automatic Speech R",0
"much easier to detect the micro-expression in the image sequence when compared to the static image, as shown by the two green dotted lines. deception <ref type=""bibr"" target=""#b11"">(Farah et al. 2014)</ref>. Additionally, the cost of the equipment and the overt nature of the method make the utility eption. Although it achieves high accuracy <ref type=""bibr"">(Kozel et al. 2005;</ref><ref type=""bibr"" target=""#b17"">Langleben and Moriarty 2013;</ref><ref type=""bibr"" target=""#b11"">Farah et al. 2014)</ref>, important questions related to the working mechanism, reliability and the experimental settin echanism, reliability and the experimental setting are still open research problems <ref type=""bibr"" target=""#b17"">(Langleben and Moriarty 2013;</ref><ref type=""bibr"" target=""#b11"">Farah et al. 2014)</ref>. Furthermore, the above mentioned physiological measure based methods are overt and could be d",0
"a deception in a high stakes situation is crucial for personal and public safety.</p><p>The ability of humans to detect deception is very limited. In <ref type=""bibr"" target=""#b0"">(Bond Jr and DePaulo 2006)</ref>, it was reported that the average accuracy of detecting lies without special aids is 54",0
"scanning the brain and finding areas that are correlated with deception. Although it achieves high accuracy <ref type=""bibr"">(Kozel et al. 2005;</ref><ref type=""bibr"" target=""#b17"">Langleben and Moriarty 2013;</ref><ref type=""bibr"" target=""#b11"">Farah et al. 2014)</ref>, important questions related h et al. 2014)</ref>, important questions related to the working mechanism, reliability and the experimental setting are still open research problems <ref type=""bibr"" target=""#b17"">(Langleben and Moriarty 2013;</ref><ref type=""bibr"" target=""#b11"">Farah et al. 2014)</ref>. Furthermore, the above ment",0
"ld integrate information from more than one modality. Taking motivation from prior work <ref type=""bibr"" target=""#b23"">(Pérez-Rosas et al. 2015;</ref><ref type=""bibr"" target=""#b16"">Jaiswal, Tabibu, and Bajpai 2016)</ref>, we also include features from other modalities, specifically audio and text. T vior. We also include simple verbal and audio features, as other multi-modal approaches <ref type=""bibr"" target=""#b23"">(Pérez-Rosas et al. 2015;</ref><ref type=""bibr"" target=""#b16"">Jaiswal, Tabibu, and Bajpai 2016)</ref>, into the overall covert automated system.</p></div> <div xmlns=""http://www.tei",0
">(DePaulo et al. 2003)</ref>. These cues are faint behavioral residues, which are difficult for untrained people to detect. For example, according to <ref type=""bibr"" target=""#b9"">(Ekman et al. 1969;</ref><ref type=""bibr"" target=""#b10"">Ekman 2009)</ref>, facial micro-expressions reflect emotions tha and since the database was small, the methods were prone to overfitting and did not generalize to new subjects. Based on Ekman's psychology research <ref type=""bibr"" target=""#b9"">(Ekman et al. 1969;</ref><ref type=""bibr"" target=""#b10"">Ekman 2009</ref>) that some facial behaviors are involuntary and features discussed above are low-level features. Here, we introduce the high-level features used to represent facial micro-expressions. According to <ref type=""bibr"" target=""#b9"">(Ekman et al. 1969;</ref><ref type=""bibr"" target=""#b10"">Ekman 2009)</ref> </p></div> <div xmlns=""http://www.tei-c.org/ns",0
"rthermore, the above mentioned physiological measure based methods are overt and could be disrupted by the subject's counter preparation and behavior <ref type=""bibr"" target=""#b12"">(Ganis et al. 2011</ref>).</p><p>Among the covert systems, computer vision based methods play an important role. Early",0
"facial expressions and involuntary facial expressions by identifying Deceit Indicators, which were defined by a group of specific Facial Action Units <ref type=""bibr"" target=""#b8"">(Ekman and Friesen 1977)</ref>. However, this method requires people to manually label facial landmarks and input major",0
"gorithms on three types of datasets that involve learning the properties of molecules from their structure:</p><p>1. The Harvard Clean Energy Project <ref type=""bibr"" target=""#b15"">(Hachmann et al., 2011)</ref>, consisting of 2.3 million organic compounds that are candidates for use in solar cells.",0
"f type=""bibr"" target=""#b28"">Niepert et al., 2016)</ref> directly seek inspiration from the type of classical CNNs that are used for image recognition <ref type=""bibr"" target=""#b24"">(LeCun et al., 1998;</ref><ref type=""bibr"">Krizhevsky et al., 2012)</ref>. These methods involve first fixing a vertex",0
"proposed object detectors are based on the two-stage R-CNN framework <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b29"">30,</ref><ref type=""bibr"" target=""#b22"">23]</ref>, where detection is framed as a multi-task learning problem that comb 6"">[17]</ref> and Fast R-CNN <ref type=""bibr"" target=""#b12"">[13]</ref> introduced the idea of region-wise feature extraction. Later, the Faster R-CNN <ref type=""bibr"" target=""#b29"">[30]</ref> achieved further speeds-up by introducing a Region Proposal Network (RPN). Some more recent works have exten the input image once through an efficient backbone network. SSD <ref type=""bibr"" target=""#b24"">[25]</ref> detects objects in a way similar to the RPN <ref type=""bibr"" target=""#b29"">[30]</ref>, but uses multiple feature maps at different resolutions to cover objects at various scales. Their main limi unding box (""B"") are assigned to each hypothesis. We focus on modeling a multistage detection sub-network, and adopt, but are not limited to, the RPN <ref type=""bibr"" target=""#b29"">[30]</ref> for proposal detection.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.1."">Bounding Box Regre lassifier to perform uniformly well over all IoU levels. At inference, since the majority of the hypotheses produced by a proposal detector, e.g. RPN <ref type=""bibr"" target=""#b29"">[30]</ref> or selective search <ref type=""bibr"" target=""#b32"">[33]</ref>, have low quality, the detector must be more d iv xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3."">Object Detection</head><p>In this paper, we extend the two-stage architecture of the Faster R-CNN <ref type=""bibr"" target=""#b29"">[30,</ref><ref type=""bibr"" target=""#b22"">23]</ref>, shown in Figure <ref type=""figure"">3 (a)</ref>. The first stage is eness of multi-task learning, Δ is usually normalized by its mean and variance, i.e. is replaced by ′ =( − )/ . This is widely used in the literature <ref type=""bibr"" target=""#b29"">[30,</ref><ref type=""bibr"" target=""#b0"">1,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b22"">23 re as above, with the changes of Section 5.5 for FPN+.</p><p>Detection Performance: Again, our implementations are better than the original detectors <ref type=""bibr"" target=""#b29"">[30,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b22"">23]</ref>. Still, the Cascade R-CNN impr "">Results on PASCAL VOC</head><p>The Cascade R-CNN was further experimented on PAS-CAL VOC dataset <ref type=""bibr"" target=""#b7"">[8]</ref>. Following <ref type=""bibr"" target=""#b29"">[30,</ref><ref type=""bibr"" target=""#b24"">25]</ref>, the models were trained on VOC2007 and VOC2012 trainval and tested detection with = {0.5, 0.6, 0.7}, unless otherwise noted. The sampling of the first detection stage follows <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b29"">30]</ref>. In the following stages, resampling is implemented by simply using the regressed outputs from the previous s",1
"pe=""bibr"" target=""#b29"">[30,</ref><ref type=""bibr"" target=""#b0"">1,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b15"">16]</ref>.</p><p>Some works <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type="" tion. 256 RoIs were used per image. FPN: Since no source code was publicly available for FPN, our implementation details could be different. RoIAlign <ref type=""bibr"" target=""#b15"">[16]</ref> was used for a stronger baseline. This is denoted as FPN+ and was used in all ablation studies. As usual, Re f>), and the very recent Deformable R-FCN <ref type=""bibr"" target=""#b4"">[5]</ref>, RetinaNet <ref type=""bibr"" target=""#b23"">[24]</ref> and Mask R-CNN <ref type=""bibr"" target=""#b15"">[16]</ref>. Compared to the best multi-stage detector on COCO, AttractioNet <ref type=""bibr"" target=""#b10"">[11]</ref>,",0
"omposed into a sequence of simpler steps, inspired by the works of cascade pose regression <ref type=""bibr"" target=""#b5"">[6]</ref> and face alignment <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b34"">35]</ref>. In the Cascade R-CNN, it is framed as a cascaded regression problem,",0
"s very sparse detection results and enables real time object detection, by forwarding the input image once through an efficient backbone network. SSD <ref type=""bibr"" target=""#b24"">[25]</ref> detects objects in a way similar to the RPN <ref type=""bibr"" target=""#b29"">[30]</ref>, but uses multiple fea ade R-CNN was further experimented on PAS-CAL VOC dataset <ref type=""bibr"" target=""#b7"">[8]</ref>. Following <ref type=""bibr"" target=""#b29"">[30,</ref><ref type=""bibr"" target=""#b24"">25]</ref>, the models were trained on VOC2007 and VOC2012 trainval and tested on VOC2007 test. Faster R-CNN (with AlexN",0
"he two-stage R-CNN framework <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b29"">30,</ref><ref type=""bibr"" target=""#b22"">23]</ref>, where detection is framed as a multi-task learning problem that combines classification and bounding box reg ead n=""3."">Object Detection</head><p>In this paper, we extend the two-stage architecture of the Faster R-CNN <ref type=""bibr"" target=""#b29"">[30,</ref><ref type=""bibr"" target=""#b22"">23]</ref>, shown in Figure <ref type=""figure"">3 (a)</ref>. The first stage is a proposal sub-network (""H0""), applied to nce: Again, our implementations are better than the original detectors <ref type=""bibr"" target=""#b29"">[30,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b22"">23]</ref>. Still, the Cascade R-CNN improves on these baselines consistently by 2∼4 points, independently of their stre accuracy loss, to avoid the heavy region-wise CNN computations of the Faster R-CNN; while the MS-CNN <ref type=""bibr"" target=""#b0"">[1]</ref> and FPN <ref type=""bibr"" target=""#b22"">[23]</ref> detect high-recall proposals at multiple output layers, so as to alleviate the scale mismatch between the RP seline detectors: Faster R-CNN with backbone VGG-Net <ref type=""bibr"" target=""#b31"">[32]</ref>, R-FCN <ref type=""bibr"" target=""#b3"">[4]</ref> and FPN <ref type=""bibr"" target=""#b22"">[23]</ref> with ResNet backbone <ref type=""bibr"" target=""#b17"">[18]</ref>. These baselines have a wide range of detecti R-CNN). All the compared state-of-the-art detectors were trained with =0 .5.I t is noted that our FPN+ implementation is better than the original FPN <ref type=""bibr"" target=""#b22"">[23]</ref>, providing a very strong baseline. In addition, the extension from FPN+ to Cascade R-CNN improved performanc is widely used in the literature <ref type=""bibr"" target=""#b29"">[30,</ref><ref type=""bibr"" target=""#b0"">1,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b15"">16]</ref>.</p><p>Some works <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""",0
"pmental psychology</head><p>Another potential valuable place to look is human cognition <ref type=""bibr"" target=""#b7"">(Davis &amp; Marcus, 2015;</ref><ref type=""bibr"" target=""#b29"">Lake et al., 2016;</ref><ref type=""bibr"" target=""#b42"">Marcus, 2001;</ref><ref type=""bibr"" target=""#b57"">Pinker &amp; P",1
"y extended these points, with a focus on deep reinforcement learning, noting some serious issues in the field related to robustness and replicability <ref type=""bibr"" target=""#b21"">(Henderson et al., 2017)</ref>.</p><p>Although there has been some progress in automating the process of developing mac",0
"that single recurrent neural networks (SRNs; a forerunner to today's more sophisticated deep learning based recurrent neural networks, known as RNNs; <ref type=""bibr"" target=""#b11"">Elman, 1990)</ref> would have trouble systematically representing and extending recursive structure to various kinds of",0
"e. In recent years, there are some researchers pay close attention to map matching techniques on low-samplingrate trajectories. ST-Matching algorithm <ref type=""bibr"" target=""#b3"">[4]</ref> is the first one solving the problem about low-sampling-rate map matching. IVMM <ref type=""bibr"" target=""#b4""> o be matched. Besides, some methods rely on the outer information, such as the data from mobile devices <ref type=""bibr"" target=""#b12"">[13]</ref>. In <ref type=""bibr"" target=""#b3"">[4]</ref>, the author combines the spatial structures of the road network and the temporal/speed constraints of the traj",1
"s than the previous categories. There are some matching algorithms based on hidden Markov model proposed in <ref type=""bibr"" target=""#b10"">[11]</ref> <ref type=""bibr"" target=""#b11"">[12]</ref>. A trajectory is recognized as a Markov chain, in which the observation state is the point sequence and the",0
"S-enabled devices, a large amount of trajectory data are generated by moving objects and the data are applied to several services, such as route plan <ref type=""bibr"" target=""#b0"">[1]</ref>, hot route discover <ref type=""bibr"" target=""#b1"">[2]</ref> and even social relationship infer <ref type=""bibr",0
"tories. ST-Matching algorithm <ref type=""bibr"" target=""#b3"">[4]</ref> is the first one solving the problem about low-sampling-rate map matching. IVMM <ref type=""bibr"" target=""#b4"">[5]</ref> improved ST-Matching algorithm by adding weighted influence model and interactive voting. HRIS <ref type=""bibr eed constraints of the trajectories. It improves the accuracy of map matching for trajectories with time interval no less than 2 minutes. Then, paper <ref type=""bibr"" target=""#b4"">[5]</ref> improves ST-Matching algorithm by considering the weight of different points. After that, more algorithms <ref",0
"ce and the hidden state is the road segments to be matched. Besides, some methods rely on the outer information, such as the data from mobile devices <ref type=""bibr"" target=""#b12"">[13]</ref>. In <ref type=""bibr"" target=""#b3"">[4]</ref>, the author combines the spatial structures of the road network",0
"retrieval (neural IR) is the development of interaction based models <ref type=""bibr"" target=""#b22"">[13,</ref><ref type=""bibr"" target=""#b31"">21,</ref><ref type=""bibr"" target=""#b39"">29]</ref>. Interaction based models thrive with encoding word-word translations using word embeddings, and utilizing ne ethods to be er summarize the word translations into ranking signals <ref type=""bibr"" target=""#b20"">[11,</ref><ref type=""bibr"" target=""#b22"">13,</ref><ref type=""bibr"" target=""#b39"">29]</ref>. Learned end-to-end from user feedbacks <ref type=""bibr"" target=""#b33"">[23,</ref><ref type=""bibr"" target=""#b3 ibr"" target=""#b22"">13,</ref><ref type=""bibr"" target=""#b39"">29]</ref>. Learned end-to-end from user feedbacks <ref type=""bibr"" target=""#b33"">[23,</ref><ref type=""bibr"" target=""#b39"">29]</ref>, the word embeddings can encode so matches tailored for relevance ranking, which has signi cant advantages ov ng-based pooling methods have shown be er performance than score-based ones like mean-pooling or max-pooling <ref type=""bibr"" target=""#b22"">[13,</ref><ref type=""bibr"" target=""#b39"">29]</ref>.</p><p>Kernel-pooling is applied to each M h q ,h d matrix in M to generate the so -TF feature vector ϕ M h q at counting the frequencies of multi-level so matches are more e ective than weight-summing the similarities <ref type=""bibr"" target=""#b22"">[13,</ref><ref type=""bibr"" target=""#b39"">29]</ref>-""similarity does not necessarily mean relevance"" <ref type=""bibr"" target=""#b15"">[6]</ref>.</p><p>Recall that ef>, the word embeddings can encode so matches tailored for relevance ranking, which has signi cant advantages over traditional feature-based methods <ref type=""bibr"" target=""#b39"">[29,</ref><ref type=""bibr"" target=""#b40"">30]</ref>. ese initial successes of neural IR were mainly from so matching ind ings. e current state-of-the-art kernel pooling and learningto-rank techniques are then used to combine the n-gram somatches to the nal ranking score <ref type=""bibr"" target=""#b39"">[29]</ref>.</p><p>e CNN is the key to modeling n-grams. Typical IR approaches treat n-grams as discrete terms and use t in ad hoc ranking <ref type=""bibr"" target=""#b40"">[30]</ref>.</p><p>K-NRM uni ed the progress of IR customized embeddings and interaction based model <ref type=""bibr"" target=""#b39"">[29]</ref>. It rst embeds words and builds the translation matrix using the similarities between query and document wor ether. When trained with user feedback in a search log, K-NRM outperforms both neural IR methods and feature-based learning-to-rank by a large margin <ref type=""bibr"" target=""#b39"">[29]</ref>.</p><p>ough the so matching of n-grams in information retrieval remains an open topic, there has been a larg M uses the kernel-pooling technique and a learning-torank layer to calculate the ranking score using the n-gram translations M. is part extends K-NRM <ref type=""bibr"" target=""#b39"">[29]</ref> to n-grams. Kernel-pooling is a pooling technique that uses K Gaussian kernels to count the so matches of wo p://www.tei-c.org/ns/1.0""><head n=""3.3"">Summary</head><p>Conv-KNRM adds the ability of so matching n-grams to the recent state-of-the-art K-NRM model <ref type=""bibr"" target=""#b39"">[29]</ref> with convolutional neural networks (CNNs). Without CNNs, Conv-KNRM withdraws to K-NRM.</p><p>Matching n-gram =""4"">DOMAIN ADAPTATION</head><p>End-to-end training Conv-KNRM requires large-scale training data, for example, user clicks in a commercial search log <ref type=""bibr"" target=""#b39"">[29]</ref> or industryscale annotations <ref type=""bibr"" target=""#b31"">[21]</ref>. However, for many search domains suc main to absorb the rich relevance signals in the training data. ey are then used in the target domain to generate so -TF features Φ(M). Xiong, et al. <ref type=""bibr"" target=""#b39"">[29]</ref> showed that kernel-pooled so -TF features reveal di erent types of so match. For example, one kernel may cou change over domains. For instance, the synonym kernel is of low importance in search logs as all candidate documents already contain the query words <ref type=""bibr"" target=""#b39"">[29]</ref>; however, synonyms can be a strong signal in a recall-oriented domain.</p><p>Re-training the ranking layer i g), and a TREC dataset (ClueWeb09-B).</p><p>Sogou-Log: Sogou.com is a major Chinese commercial search engine.</p><p>e same se ings as K-NRM were used <ref type=""bibr"" target=""#b39"">[29]</ref>. e same sample of Sogou log and training-testing splits are used (Table <ref type=""table"" target=""#tab_2"">1< ts were crawled, and were used by the traditional IR baselines for stronger baseline performance. Body texts of training documents were not available <ref type=""bibr"" target=""#b39"">[29]</ref>. e Chinese text was segmented by ICTCLASS <ref type=""bibr"" target=""#b41"">[31]</ref>; then Chinese words were ""><head n=""5.2"">In-Domain Training and Testing</head><p>Training and testing labels on Sogou-Log and Bing-Log were generated following prior research <ref type=""bibr"" target=""#b39"">[29]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>Technical Presentation</head><p>WSDM'18, February ibr"" target=""#b36"">[26]</ref>, MatchPyramid (MP) <ref type=""bibr"" target=""#b33"">[23]</ref>, DRMM <ref type=""bibr"" target=""#b22"">[13]</ref>, and K-NRM <ref type=""bibr"" target=""#b39"">[29]</ref>.</p><p>CDSSM <ref type=""bibr"" target=""#b36"">[26]</ref> is uses CNNs to build query and document representati target=""#b36"">[26]</ref> is uses CNNs to build query and document representations on their words' le er-tri-grams (or Chinese characters in Sogou-Log <ref type=""bibr"" target=""#b39"">[29]</ref>). e ranking scores are calculated by the similarity between the representations.</p><p>MP <ref type=""bibr"" t tiple levels of so -TF, and use learning-to-rank a erwards.</p><p>K-NRM is a state-of-the-art neural model previously tested on the Sogou-Log dataset <ref type=""bibr"" target=""#b39"">[29]</ref>. It uses kernel-pooling instead of DRMM's histogram pooling, and learns the word embeddings and the ranking d using cross-validation on the testing data. On search logs, 5-fold cross validation were used to be consistent with the previous study on Sogou-Log <ref type=""bibr"" target=""#b39"">[29]</ref>. On ClueWeb09-B, the 10-fold cross validation splits from the provided baselines were used. All RankSVM's us ain the learning-to-rank layer. Document Fields: On Sogou-Log, traditional IR methods used both title and body, and neural IR methods only used title <ref type=""bibr"" target=""#b39"">[29]</ref>, as discussed in section 5.1. On Bing-Log, all methods used the title and snippets. On ClueWeb09-B, all meth ueWeb corpus. MP, K-NRM, and Conv-KNRM embeddings were all learned end-to-end using the query logs. For Sogou-log, we set embedding dimension L = 300 <ref type=""bibr"" target=""#b39"">[29]</ref> . For Bing-Log, we set L = 100 because our pilot study showed that L = 100 has similar performance with L = split the cosine range [−1, 1]: the µ or bin centers were: µ 1 = 0.9, µ 2 = 0.7, ..., µ 10 = −0.9.</p><p>e σ of the so match bins were set to be 0.1 <ref type=""bibr"" target=""#b39"">[29]</ref>. Model Implementation and E ciency: e model was implemented with Tensor ow.</p><p>e optimization used the Ad of 5 epochs.</p><p>e training of Conv-KNRM took about 12 hours on an AWS GPU machine. e training time is similar with prior work using only unigrams <ref type=""bibr"" target=""#b39"">[29]</ref>. Most computation time was spent on the embedding layer; the convolutional layer was very e cient.</p></div> ngs and matchbased techniques are necessary for current neural IR methods to provide additional improvements <ref type=""bibr"" target=""#b32"">[22,</ref><ref type=""bibr"" target=""#b39"">29,</ref><ref type=""bibr"" target=""#b40"">30]</ref> Comparing the two strong neural IR baselines, K-NRM outperforms MP by",1
"g the similarities <ref type=""bibr"" target=""#b22"">[13,</ref><ref type=""bibr"" target=""#b39"">29]</ref>-""similarity does not necessarily mean relevance"" <ref type=""bibr"" target=""#b15"">[6]</ref>.</p><p>Recall that Conv-KNRM is a richer model than K-NRM only because it leverages convolutional neural netw",0
"how to learn good representation of the query and document, and the ranking was simply done by their representations' similarities, for example, DSSM <ref type=""bibr"" target=""#b24"">[15]</ref> and its convolution version CDSSM <ref type=""bibr"" target=""#b36"">[26]</ref>. A more recent example is the we",0
"their reviews. None of these works is for sequential recommendation.</p><p>Recurrent neural networks (RNN) was used for session-based recommendation <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b9"">10]</ref>. While RNN has shown to have an impressive capability in modeling seque hains and uses Similarity Model instead of LFM for modeling general user preferences. • GRU4Rec. This is the session-based recommendation proposed by <ref type=""bibr"" target=""#b7"">[8]</ref>. This model uses RNN to capture sequential dependencies and make predictions.</p><p>For each method, the grid ll on MovieLens but can easily get biased on training sets of the other three data sets despite the use of regularization and dropout as described in <ref type=""bibr"" target=""#b7"">[8]</ref>. In addition, GRU4Rec's recommendation is session-based, instead of personalized, which enlarge the generaliza",1
"ocal features for image recognition <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b15"">16]</ref> and natural language processing <ref type=""bibr"" target=""#b11"">[12]</ref>.</p><p>The novelty of Caser is to represent the previous L items as an L × d matrix E, where d is the number resent the previous L items as an L × d matrix E, where d is the number of latent dimensions and the rows preserve the order of the items. Similar to <ref type=""bibr"" target=""#b11"">[12]</ref>, we regard this embedding matrix as the ""image"" of the L items in the latent space and search for sequential ocal features for image recognition <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b15"">16]</ref> and natural language processing <ref type=""bibr"" target=""#b11"">[12]</ref>. Borrows the idea of using CNN in text classi cation <ref type=""bibr"" target=""#b11"">[12]</ref>, our approach arget=""#b15"">16]</ref> and natural language processing <ref type=""bibr"" target=""#b11"">[12]</ref>. Borrows the idea of using CNN in text classi cation <ref type=""bibr"" target=""#b11"">[12]</ref>, our approach regards the L × d matrix E as the ""image"" of the previous L items in the latent space and rega",1
"div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""1.1"">Top-N Sequential Recommendation</head><p>To model user's sequential patterns, the work in <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b20"">21]</ref> considers top-N sequential recommendation that recommends N items th othing, Shoes, Jewelry' and 'Video Games' category), in other words, its sequential signals are much weaker than the above data sets.</p><p>Following <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" target=""#b34"">35]</ref>, we hold the rst 70% of acti type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b3"">4]</ref> nd explicit sequential association rules based on statistical co-occurrences <ref type=""bibr"" target=""#b16"">[17]</ref>. This approach depends on  the explicit representation of patterns, thus, could miss patterns in unobserved",0
"f a set of items.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""1.2"">Limitations of Previous Work</head><p>The Markov chain based model <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b29"">30 kelihood estimation. Factorized personalized Markov chains (FPMC) <ref type=""bibr"" target=""#b20"">[21]</ref> proposed by Rendle et al. and its variant <ref type=""bibr"" target=""#b1"">[2]</ref> improved this method by factorizing this transition matrix into two latent and low-rank sub-matrices. Factoriz er is xed to be L × 1. This is because each column of E is latent for us, it is meaningless to interact with multiple successive columns at one time. <ref type=""bibr"" target=""#b1"">(2)</ref> There is no need to apply max pooling operation over the vertical convolution results, as we want to keep the",0
"ework. In fact, we will show that Caser generalizes several state-of-the-art methods.</p><p>A related but di erent problem is temporal recommendation <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b30"">31,</ref><ref type=""bibr"" target=""#b33"">34]</ref>. For example, temporal recom",0
"for sequential recommendation.</p><p>Recurrent neural networks (RNN) was used for session-based recommendation <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b9"">10]</ref>. While RNN has shown to have an impressive capability in modeling sequences <ref type=""bibr"" target=""#b17"">[18",0
"""http://www.tei-c.org/ns/1.0""><head n=""1.2"">Limitations of Previous Work</head><p>The Markov chain based model <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b29"">30]</ref> is an early approach to top-N s y factorizing this transition matrix into two latent and low-rank sub-matrices. Factorized Sequential Prediction with Item Similarity ModeLs (Fossil) <ref type=""bibr"" target=""#b5"">[6]</ref> proposed by He et al. generalizes this method to high-order Markov chains using a weighted sum aggregation ove s actions (blue) in uences the target action (yellow) individually, instead of collectively. FPMC and Fossil fall into this taxonomy. Although Fossil <ref type=""bibr"" target=""#b5"">[6]</ref> considers a high-order Markov chain, the overall in uence is a weighted sum of previous items' latent represen >where E l is the l-th row of E. Therefore, with vertical lters we can learn to aggregate the embeddings of the L previous items, similar to Fossil's <ref type=""bibr"" target=""#b5"">[6]</ref> weighted sum to aggregate the L previous items' latent representations. The di erence is that each lter F k is hours for Gowalla data, 2 hours for Foursquare and 1 hour for Tmall on a 4-cores i7 CPU and 32GB RAM machine. These times are comparable to Fossil's <ref type=""bibr"" target=""#b5"">[6]</ref> running time and can be further reduced by using GPU.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head thods. FPMC allows a basket of several items at each step. For our sequential recommendation problem, each basket has a single item. • Fossil. Fossil <ref type=""bibr"" target=""#b5"">[6]</ref> models high-order Markov chains and uses Similarity Model instead of LFM for modeling general user preferences t ∈ C u i ∈ D u t −log(σ ( (u,t ) i )) + j i −log(1 − σ ( (u,t ) j</formula><p>)). <ref type=""bibr"" target=""#b12"">(13)</ref> Following previous works <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b31"">32]</ref>, for each target item i, we ra ata obtained from IJCAI 2015 competition <ref type=""foot"" target=""#foot_4"">5</ref> , which aims to forecast repeated buyers. Following previous works <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" target=""#b31"">32]</ref>, we converted all numeric rati t users and items of having less than n feedbacks, as dealing with cold-start recommendation is usually treated as a separate issue in the literature <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b31"">32 "">32]</ref>. n is 5,15,10,10 for MovieLens, Gowalla, Foursquare, and Tmall. The Amazon data previously used in <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b5"">6]</ref> was not used due to its SI (0.0026 for 'O ce Products' category, 0.0019 for 'Clothing, Shoes, Jewelry' and 'Vid",0
"The index t for S u t denotes the order in which an action occurs in the sequence S u , not the absolute timestamp as in temporal recommendation like <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b30"">31,</ref><ref type=""bibr"" target=""#b33"">34]</ref>. Given all users' sequences",0
"n which an action occurs in the sequence S u , not the absolute timestamp as in temporal recommendation like <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b30"">31,</ref><ref type=""bibr"" target=""#b33"">34]</ref>. Given all users' sequences S u , the goal is to recommend each user eneralizes several state-of-the-art methods.</p><p>A related but di erent problem is temporal recommendation <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b30"">31,</ref><ref type=""bibr"" target=""#b33"">34]</ref>. For example, temporal recommendation recommends co ee in the morning",0
"as binary cross-entropy loss:</p><formula xml:id=""formula_19"">= u t ∈ C u i ∈ D u t −log(σ ( (u,t ) i )) + j i −log(1 − σ ( (u,t ) j</formula><p>)). <ref type=""bibr"" target=""#b12"">(13)</ref> Following previous works <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref ) are tuned on the validation set via grid search. We adopt an variant of Stochastic Gradient Descent (SGD) called Adaptive Moment Estimation (Adam) <ref type=""bibr"" target=""#b12"">[13]</ref> for faster convergence, with a batch size of 100. To control model complexity and avoid over-tting, we use t",0
"at the cost of hand-crafted features and manual annotations, but lack the ability to generalize to other domains. In contrast, embedding-based models <ref type=""bibr"" target=""#b7"">(Bordes et al., 2014b;</ref><ref type=""bibr"" target=""#b13"">Hao et al., 2017;</ref><ref type=""bibr"" target=""#b29"">Yavuz e onstrained on a specific domain and broken down when executing logical queries on incomplete KBs. Our work follows the line of Embedding-based models <ref type=""bibr"" target=""#b7"">(Bordes et al., 2014b;</ref><ref type=""bibr"" target=""#b10"">Dong et al., 2015;</ref><ref type=""bibr"" target=""#b28"">Xu et ng set. The models need to cope with questions related to these three OOV relations during the test.</p><p>Several baselines are included here: Embed <ref type=""bibr"" target=""#b7"">(Bordes et al., 2014b)</ref> deals with factoid QA over KB by matching a question with an answer in the embedding spaces",1
"ak supervision, but existing methods are not adequate to handle multi-relation QA due to the lack of reasoning ability.</p><p>Recent reasoning models <ref type=""bibr"" target=""#b16"">(Miller et al., 2016;</ref><ref type=""bibr"" target=""#b26"">Wang et al., 2017)</ref> mainly concentrate on Reading Compre ion and the corresponding document in a multi-hop manner during reasoning. MemNN <ref type=""bibr"" target=""#b27"">(Weston et al., 2015)</ref>, KVMemN2N <ref type=""bibr"" target=""#b16"">(Miller et al., 2016)</ref> and EviNet <ref type=""bibr"" target=""#b21"">(Savenkov and Agichtein, 2017)</ref> transferred cal subgraph of the corresponding answer path, where the settings are the same as <ref type=""bibr"" target=""#b8"">(Bordes et al., 2015)</ref>. KVMemN2N <ref type=""bibr"" target=""#b16"">(Miller et al., 2016)</ref> improves the MemN2N for KBQA as it divides the memory into two parts: the key memory stores",1
"ned to comprehend documents. State-of-the-art memory-based Reading Comprehension models <ref type=""bibr"" target=""#b24"">(Sukhbaatar et al., 2015;</ref><ref type=""bibr"" target=""#b15"">Kumar et al., 2015;</ref><ref type=""bibr"" target=""#b22"">Shen et al., 2016;</ref><ref type=""bibr"" target=""#b26"">Wang et",0
"ed a model coupling distributed and symbolic execution with REINFORCE algorithm, however, training such a model is challenging. Neural Module Network <ref type=""bibr"" target=""#b1"">(Andreas et al., 2015;</ref><ref type=""bibr"" target=""#b2"">Andreas et al., 2016)</ref> customized network architectures f",0
"us studies on QA over knowledge bases can be roughly categorized into two lines: semantic parsing and embedding-based models. Semantic parsing models <ref type=""bibr"" target=""#b30"">(Yih et al., 2014;</ref><ref type=""bibr"" target=""#b31"">Yih et al., 2016)</ref> obtain competitive performance at the co",0
"nd execute a query on tables. However, Neural Programmer needs to predefine symbolic operations, while Neural Enquirer lacks explicit interpretation. <ref type=""bibr"" target=""#b17"">Mou et al. (2017)</ref> proposed a model coupling distributed and symbolic execution with REINFORCE algorithm, however,",0
"target=""#b15"">Kumar et al., 2015;</ref><ref type=""bibr"" target=""#b22"">Shen et al., 2016;</ref><ref type=""bibr"" target=""#b26"">Wang et al., 2017;</ref><ref type=""bibr"" target=""#b9"">Celikyilmaz et al., 2017)</ref> make interactions between a question and the corresponding document in a multi-hop manne",0
"on Answering (QA) has always been a hot topic in AI and this task has recently been facilitated by large-scale Knowledge Bases (KBs) such as Freebase <ref type=""bibr"" target=""#b4"">(Bollacker et al., 2008)</ref>. However, due to the variety and complexity of language and knowledge, open-domain questi (Zhang et al., 2016)</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.1"">PathQuestion</head><p>We adopted two subsets of Freebase <ref type=""bibr"" target=""#b4"">(Bollacker et al., 2008)</ref> as Knowledge Bases to construct the PathQuestion (PQ) and the PathQuestion-Large (PQL) da Socher et al., 2013)</ref> with 13 relations and thousands of entities. As for PathQuestion-Large, we adopted another more complex subset of Freebase <ref type=""bibr"" target=""#b4"">(Bollacker et al., 2008)</ref>. First, we extracted all the paths with two hops (&lt; e s , r 1 , e 1 , r 2 , a &gt;), o",0
"task is to find an entity a in KB as the answer.</p><p>In this work, we consider two typical categories of multi-relation questions, a path question <ref type=""bibr"" target=""#b12"">(Guu et al., 2015)</ref> and a conjunctive question <ref type=""bibr"" target=""#b34"">(Zhang et al., 2016)</ref>, while th",0
"eed less grammars as well as annotated data, and are more flexible to deal with incomplete KBs. To make better matching, subgraphs of an entity in KB <ref type=""bibr"" target=""#b6"">(Bordes et al., 2014a)</ref>, answer aspects <ref type=""bibr"" target=""#b10"">(Dong et al., 2015;</ref><ref type=""bibr"" ta bibr"" target=""#b7"">(Bordes et al., 2014b)</ref> deals with factoid QA over KB by matching a question with an answer in the embedding spaces. Subgraph <ref type=""bibr"" target=""#b6"">(Bordes et al., 2014a)</ref> improves the Embed model by enriching the representation of an answer entity with the entit",0
"ticular task can be used for other models on other tasks, with little to no modifications <ref type=""bibr"" target=""#b35"">(Razavian et al., 2014;</ref><ref type=""bibr"" target=""#b47"">Zoph et al., 2016;</ref><ref type=""bibr"" target=""#b29"">Luong et al., 2016)</ref>.</p><p>We empirically show that not on",0
"><head n=""2.3."">Designing Convolutional Networks</head><p>The decision of what previous nodes to connect to allows the model to form skip connections <ref type=""bibr"" target=""#b15"">(He et al., 2016a;</ref><ref type=""bibr"" target=""#b46"">Zoph &amp; Le, 2017)</ref>. Specifically, at layer k, up to k −",0
"ead><p>There is a growing interest in improving the efficiency of NAS. Concurrent to our work are the promising ideas of using performance prediction <ref type=""bibr"" target=""#b1"">(Baker et al., 2017b;</ref><ref type=""bibr"" target=""#b8"">Deng et al., 2017)</ref>, using iterative search method for arc",0
"lar to dropout with a rate of 0.5 on the skip connections, and to drop-path on the operations <ref type=""bibr"" target=""#b48"">(Zoph et al., 2018;</ref><ref type=""bibr"" target=""#b24"">Larsson et al., 2017)</ref>. At convergence, the model has the error rate of 8.92%. On the validation set, an ensemble",0
"pe=""bibr"" target=""#b28"">(Loshchilov &amp; Hutter, 2017)</ref>. Each architecture search is run for 310 epochs. We initialize ω with He initialization <ref type=""bibr"" target=""#b14"">(He et al., 2015)</ref>. We also apply an 2 weight decay of 10 −4 . We train the architectures recommended by the contr",0
"017)</ref>; gradient masking is known to be an incomplete defense to adversarial examples <ref type=""bibr"" target=""#b17"">(Papernot et al., 2017;</ref><ref type=""bibr"" target=""#b27"">Tramèr et al., 2018)</ref>. Despite this, we observe that 7 of the ICLR 2018 defenses rely on this effect.</p><p>To con evious defenses which cause gradient masking by learning to break gradient descent (e.g., by learning to make the gradients point the wrong direction <ref type=""bibr"" target=""#b27"">(Tramèr et al., 2018</ref>)), we refer to the case where defenses are designed in such a way that the constructed defen r"" target=""#b18"">(Raghunathan et al., 2018;</ref><ref type=""bibr"" target=""#b23"">Sinha et al., 2018)</ref> and one that only argues black-box security <ref type=""bibr"" target=""#b27"">(Tramèr et al., 2018)</ref>. We include one paper, <ref type=""bibr"" target=""#b14"">Ma et al. (2018)</ref>, that was not",0
"uation. It is possible to bypass each defense independently (and ensembles of defenses usually are not much stronger than the strongest sub-component <ref type=""bibr"" target=""#b11"">(He et al., 2017)</ref>). We circumvent image cropping and rescaling with a direct application of EOT. To circumvent bi",0
"rd pass using a differentiable approximation of the function. We compute gradients of randomized defenses by applying Expectation Over Transformation <ref type=""bibr"" target=""#b0"">(Athalye et al., 2017)</ref>. We solve vanishing/exploding gradients through reparameterization and optimize over a spac ><p>Expectation over Transformation. For defenses that employ randomized transformations to the input, we apply Expectation over Transformation (EOT) <ref type=""bibr"" target=""#b0"">(Athalye et al., 2017)</ref> to correctly compute the gradient over the expected transformation to the input.</p><p>When",0
"ard pass is either completely ineffective (e.g. with <ref type=""bibr"" target=""#b24"">Song et al. (2018)</ref>) or many times less effective (e.g. with <ref type=""bibr"" target=""#b3"">Buckman et al. (2018)</ref>).</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.2."">Attacking Randomized Cla",0
"h models.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>III. RU-NET AND R2U-NET ARCHITECTURES</head><p>Inspired by the deep residual model <ref type=""bibr"" target=""#b6"">[7]</ref>, RCNN <ref type=""bibr"" target=""#b40"">[41]</ref>, and U-Net <ref type=""bibr"" target=""#b11"">[12]</ref>, we propo <ref type=""bibr"" target=""#b0"">[1]</ref>, VGG <ref type=""bibr"" target=""#b4"">[5]</ref>, GoogleNet <ref type=""bibr"" target=""#b5"">[6]</ref>, Residual Net <ref type=""bibr"" target=""#b6"">[7]</ref>, DenseNet <ref type=""bibr"" target=""#b7"">[8]</ref>, and CapsuleNet [9][65].</p></div> 			</div>  			<div type=""",1
"-c.org/ns/1.0""><head>III. RU-NET AND R2U-NET ARCHITECTURES</head><p>Inspired by the deep residual model <ref type=""bibr"" target=""#b6"">[7]</ref>, RCNN <ref type=""bibr"" target=""#b40"">[41]</ref>, and U-Net <ref type=""bibr"" target=""#b11"">[12]</ref>, we propose two models for segmentation tasks which are operations of the Recurrent Convolutional Layers (RCL) are performed with respect to the discrete time steps that are expressed according to the RCNN <ref type=""bibr"" target=""#b40"">[41]</ref>. Let's consider the 𝑥 𝑙 input sample in the 𝑙 𝑡ℎ layer of the residual RCNN (RRCNN) block and a pixel locate",1
"developed deep learning models. RCNN and its variants have already shown superior performance on object recognition tasks using different benchmarks <ref type=""bibr"" target=""#b41"">[42,</ref><ref type=""bibr"" target=""#b42"">43]</ref>. The recurrent residual convolutional operations can be demonstrated",1
"sed the ""leaveone-out"" approach for STARE dataset. The CHASH_DB1 dataset contains 28 color retina images and the size of each image is 999×960 pixels <ref type=""bibr"" target=""#b47"">[48]</ref>. The images in this dataset were collected from both left and right eyes of 14 school children. The dataset",0
"these target lesion boundaries, we must emphasize the related pixels. Landmark detection in medical imaging <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b15"">16]</ref> is one example of this. There were several traditional machine learning and image processing techniques avail",0
"example, a fully-connected convolutional neural network (FCN) also provides state-of-the-art results for image segmentation tasks in computer vision <ref type=""bibr"" target=""#b1"">[2]</ref>. Another variant of FCN was also proposed which is called SegNet <ref type=""bibr"" target=""#b9"">[10]</ref>. man target=""#b25"">[26]</ref>.</p><p>In addition, CNNs based segmentation methods based on FCN provide superior performance for natural image segmentation <ref type=""bibr"" target=""#b1"">[2]</ref>. One of the image patch-based architectures is called Random architecture, which is very computationally inten .tei-c.org/ns/1.0""><p>OWADAYS DL provides state-of-the-art performance for image classification <ref type=""bibr"" target=""#b0"">[1]</ref>, segmentation <ref type=""bibr"" target=""#b1"">[2]</ref>, detection and tracking <ref type=""bibr"" target=""#b2"">[3]</ref>, and captioning <ref type=""bibr"" target=""#b3"">",0
"e=""bibr"" target=""#b19"">[20,</ref><ref type=""bibr"" target=""#b20"">21]</ref>. A DCNN based brain tumor segmentation and detection method was proposed in <ref type=""bibr"" target=""#b21"">[22]</ref>.</p><p>From an architectural point of view, the CNN model for classification tasks requires an encoding unit",0
"sing techniques available for medical image segmentation tasks before the DL revolution, including amplitude segmentation based on histogram features <ref type=""bibr"" target=""#b16"">[17]</ref>, the region based segmentation method <ref type=""bibr"" target=""#b17"">[18]</ref>, and the graph-cut approach",0
"lesion in identification tasks. To define these target lesion boundaries, we must emphasize the related pixels. Landmark detection in medical imaging <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b15"">16]</ref> is one example of this. There were several traditional machine learn",0
"plementing modern activation functions such as Rectified Linear Units (ReLU) or Exponential Linear Units (ELU) <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b5"">6]</ref>. Another solution to this problem is proposed by He et al., a deep residual model that overcomes the problem ut work (DCNN) models have been proposed such as AlexNet <ref type=""bibr"" target=""#b0"">[1]</ref>, VGG <ref type=""bibr"" target=""#b4"">[5]</ref>, GoogleNet <ref type=""bibr"" target=""#b5"">[6]</ref>, Residual Net <ref type=""bibr"" target=""#b6"">[7]</ref>, DenseNet <ref type=""bibr"" target=""#b7"">[8]</ref>, and C",0
"due to their flexibility. The basic idea of software control flow checking is to partition the program into basic blocks (branch-free parts of code) <ref type=""bibr"" target=""#b14"">[14]</ref>. For each block a deterministic signature is calculated and saved somewhere during compile time; then errors s (CFCSS) <ref type=""bibr"" target=""#b15"">[15]</ref> and On-line control flow error detection using relationship signatures among basic blocks (RSCFC) <ref type=""bibr"" target=""#b14"">[14]</ref>.</p><p>ECCA, firstly, assigns a unique prime number identifier (BID) to each basic block of a program; then obtained by applying the CFCSS <ref type=""bibr"" target=""#b15"">[15]</ref> technique to the original code,  a safe one, obtained by applying the RSCFC <ref type=""bibr"" target=""#b14"">[14]</ref> technique to the original code,  a safe one, obtained by applying the CFCVE technique to the original code.",1
""">[4,</ref><ref type=""bibr"" target=""#b4"">5]</ref>, and trusted measurement is a key problem of this technology <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b6"">7]</ref>. Trusted computing treats the integrity as a fundamental attribute of trust. However, such attestation mechanis",0
"pen networks, such as the Internet, increasingly demand for secure communication and secure operation due to rising online fraud and software attacks <ref type=""bibr"" target=""#b0"">[1]</ref>. Some of these vulnerabilities are due to the complexity and architectural constraints of the underlying execu the least bit of the registers. In CFCVE we assign a binary positive number to each BB, and the length of a signature Len can be obtained by equation <ref type=""bibr"" target=""#b0"">(1)</ref>, where N is the number of total BBs. </p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.1"">Signatu",0
"ng the error here. The detection latency result in Table <ref type=""table"" target=""#tab_0"">2</ref> is calculated according to the equation 2 and 3 of <ref type=""bibr"" target=""#b17"">[17]</ref>. Table <ref type=""table"" target=""#tab_0"">2</ref> shows that the detection latency of CFCVE is comparable to",0
"sted chain <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b4"">5]</ref>, and trusted measurement is a key problem of this technology <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b6"">7]</ref>. Trusted computing treats the integrity as a fundamental attribute of tr",0
"important three software-based solutions proposed in the literature are the techniques called Enhanced Control Flow Checking Using Assertions (ECCA) <ref type=""bibr"" target=""#b12"">[13]</ref> and Control Flow Checking by Software Signatures (CFCSS) <ref type=""bibr"" target=""#b15"">[15]</ref> and On-li be equal to i s . If G contains a number different from the signature associated with the current node, it means an error has occurred in the program <ref type=""bibr"" target=""#b12"">[13]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>Table 1. Definitions of relevant concepts used in anch destinations (except in the first instruction) in which the execution always enters at the first instruction and leaves via the last instruction <ref type=""bibr"" target=""#b12"">[13]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>Control Flow Graph</head><p>A program P can be rep",0
"afts. Soft errors may be explicit bit flips in latches or memories, or glitches in combinational logics that can propagate and be captured in latches <ref type=""bibr"" target=""#b8"">[9]</ref>. If not handled properly, such errors can cause illegal accesses to peripherals, memory overflow, data corrupt",0
"work architecture restricts confidence gaps as little as possible. We will elaborate.</p><p>There are previous works that achieve the first condition <ref type=""bibr"" target=""#b10"">(Cisse et al., 2017;</ref><ref type=""bibr"" target=""#b17"">Hein &amp; Andriushchenko, 2017)</ref> or bound responses to i =""bibr"" target=""#b24"">Raghunathan et al., 2018;</ref><ref type=""bibr"" target=""#b16"">Haber &amp; Ruthotto, 2017)</ref>. For example, Parseval networks <ref type=""bibr"" target=""#b10"">(Cisse et al., 2017)</ref> bound the Lipschitz constant by requiring each linear or convolution layer be composed of or replace two-sided ReLU with regular ReLU, and this leads to degradation in nominal accuracy, average confidence gap and robustness. Parseval networks <ref type=""bibr"" target=""#b10"">(Cisse et al., 2017)</ref> can be viewed as models without L c term, norm-pooling or two-sided ReLU, and with a more re oses a method to regularize the spectral radius of weight matrices and shows its effect in reducing generalization gap. The work on Parseval networks <ref type=""bibr"" target=""#b10"">(Cisse et al., 2017)</ref> shows that it is possible to control Lipschitz constants of neural networks through regulari our weight regularization, which is key in enforcing the first condition, allows greater degrees of freedom in parameter training than the scheme in <ref type=""bibr"" target=""#b10"">Cisse et al. (2017)</ref>; a new loss function is specially designed for the second condition; we adapt various layers min r(W T W ), r(W W T ) , where r (M ) = max i j |M i,j | (2)</formula><p>The above is where our linear and convolution layers differ from those in <ref type=""bibr"" target=""#b10"">Cisse et al. (2017)</ref>: they require W W T to be an identity matrix, and it is straightforward to see that their sch hereby force the Lipschitz constant of these layers to be 1; they also propose to restrict aggregation operations. The reported robustness results of <ref type=""bibr"" target=""#b10"">Cisse et al. (2017)</ref>, however, are much weaker than those by adversarial training in <ref type=""bibr"" target=""#b21 t ∈ [0, 1] is a trainable parameter and could be a common parameter with (9).</p><p>ResNet-like reconvergence is referred to as aggregation layers in <ref type=""bibr"" target=""#b10"">Cisse et al. (2017)</ref> and a different formula was used:</p><formula xml:id=""formula_11"">y = α • x 1 + (1 − α) • f ( _11"">y = α • x 1 + (1 − α) • f (x 2 )<label>(11</label></formula><p>) where α ∈ [0, 1] is a trainable parameter. Because splitting is not modified in <ref type=""bibr"" target=""#b10"">Cisse et al. (2017)</ref>, their scheme may seem approximately equivalent to ours if a common t parameter is used for ( that are not part of f (x 2 ) and therefore better preserve distances. In contrast, because splitting is not modified, at reconvergence the scheme of <ref type=""bibr"" target=""#b10"">Cisse et al. (2017)</ref> must apply the shrinking factor of 1 − α on all outputs of convolution layers, regardless of ies shrinking factors to only the necessary components. We can also have a different t per channel or even per entry.</p><p>To be fair, the scheme of <ref type=""bibr"" target=""#b10"">Cisse et al. (2017)</ref> has an advantage of being nonexpansive with respect to any L p -norm. However, for L 2 -norm,",1
"y proposals have failed. For example, <ref type=""bibr"" target=""#b2"">Athalye et al. (2018)</ref> reported that out of eight recent defense works, only <ref type=""bibr"" target=""#b21"">Madry et al. (2017)</ref> survived strong attacks. So far the mainstream and most successful remedy is that of adversar ion of ( <ref type=""formula"" target=""#formula_3"">3</ref>) is one that combines with adversarial training. Our implementation applies the technique of <ref type=""bibr"" target=""#b21"">Madry et al. (2017)</ref> on the first loss term (4): we use distorted inputs in calculating L a . The results are repo d n=""3.1"">ROBUSTNESS</head><p>This section evaluates robustness of L2NNN classifiers for MNIST and CIFAR-10 and compares against the state of the art <ref type=""bibr"" target=""#b21"">Madry et al. (2017)</ref>. The robustness metric is accuracy under whitebox non-targeted L 2 -bounded attacks. The atta sed in training <ref type=""bibr"" target=""#b29"">(Tramèr et al., 2017;</ref><ref type=""bibr"" target=""#b33"">Zantedeschi et al., 2017)</ref>. The work of <ref type=""bibr"" target=""#b21"">Madry et al. (2017)</ref> has the best results to date and effectively flattens gradients around training data points, ported robustness results of <ref type=""bibr"" target=""#b10"">Cisse et al. (2017)</ref>, however, are much weaker than those by adversarial training in <ref type=""bibr"" target=""#b21"">Madry et al. (2017)</ref>. We differ from Parseval networks in a number of ways. Our linear and convolution layers do n networks in Table <ref type=""table"" target=""#tab_4"">5</ref>, we use two network architectures. The first has 4 layers and is the architecture used in <ref type=""bibr"" target=""#b21"">Madry et al. (2017)</ref>. The second has 22 layers and is the architecture of Models 3 and 4 in Table <ref type=""table of iterations the attacker uses, and 1000x10 indicates 10 runs each with 1000 iterations. Model 1 is an ordinarily network. Model 2 is the model from<ref type=""bibr"" target=""#b21"">Madry et al. (2017)</ref>. Model 3 is L2NNN without adversarial training. Model 4 is L2NNN with adversarial training.</ alues match those used in<ref type=""bibr"" target=""#b24"">Raghunathan et al. (2018);</ref><ref type=""bibr"" target=""#b19"">Kolter &amp; Wong (2017)</ref>;<ref type=""bibr"" target=""#b21"">Madry et al. (2017)</ref>. Our MNIST L ∞ results are on par with<ref type=""bibr"" target=""#b24"">Raghunathan et al. (2018 ar with<ref type=""bibr"" target=""#b24"">Raghunathan et al. (2018);</ref><ref type=""bibr"" target=""#b19"">Kolter &amp; Wong (2017)</ref> but not as good as<ref type=""bibr"" target=""#b21"">Madry et al. (2017)</ref>. Our CIFAR-10 Model 4 is on par with<ref type=""bibr"" target=""#b21"">Madry et al. (2017)</ref> =""#b19"">Kolter &amp; Wong (2017)</ref> but not as good as<ref type=""bibr"" target=""#b21"">Madry et al. (2017)</ref>. Our CIFAR-10 Model 4 is on par with<ref type=""bibr"" target=""#b21"">Madry et al. (2017)</ref> for L ∞ defense.</figDesc><table><row><cell cols=""4"">Table 4: Accuracy of L2NNN classifiers u that training with L2-bounded adversaries resulted in weaker L2 robustness than the L2 robustness results from training with L∞-bounded adversaries in<ref type=""bibr"" target=""#b21"">Madry et al. (2017)</ref>. Therefore we choose to compare against the best available models, even though they were trai br"" target=""#b21"">Madry et al. (2017)</ref> survived strong attacks. So far the mainstream and most successful remedy is that of adversarial training <ref type=""bibr"" target=""#b21"">(Madry et al., 2017)</ref>. However, as will be shown in Tables <ref type=""table"" target=""#tab_1"">1 and 2</ref>, the ro g.</p><p>In contrast, we are able to build MNIST and CIFAR-10 classifiers, without needing any adversarial training, that exceed the state of the art <ref type=""bibr"" target=""#b21"">(Madry et al., 2017)</ref> in robustness against white-box L 2bounded adversarial attacks. The defense is even stronger a into 10 bins and report accuracy separately on each bin in Figure <ref type=""figure"" target=""#fig_1"">2</ref>. We repeat this experiment for Model 2 <ref type=""bibr"" target=""#b21"">(Madry et al., 2017)</ref> and our Model 3 of Tables <ref type=""table"" target=""#tab_1"">1 and 2</ref>. Note that the L2N ample in Figure <ref type=""figure"" target=""#fig_0"">1</ref>. The first image is the original image of a zero. The second image is an attack on Model 2 <ref type=""bibr"" target=""#b21"">(Madry et al., 2017)</ref> found after 1K iterations, with noise L 2 -norm of 4.4. The third is one found after 10K ite ge and github.com/MadryLab/cifar10_challenge. These models (Model 2's in Tables1 and 2) were built by adversarial training with L∞-bounded adversaries<ref type=""bibr"" target=""#b21"">(Madry et al., 2017)</ref>. To the best of our knowledge,<ref type=""bibr"" target=""#b30"">Tsipras et al. (2019)</ref> fro",1
"017a;</ref><ref type=""bibr"" target=""#b15"">Goodfellow et al., 2014)</ref>, speech recognition <ref type=""bibr"" target=""#b20"">(Kreuk et al., 2018;</ref><ref type=""bibr"" target=""#b1"">Alzantot et al., 2018;</ref><ref type=""bibr"">Carlini &amp; Wagner, 2018)</ref>, image captioning <ref type=""bibr"" target",0
"reported by other robustness works including adversarial training<ref type=""bibr"" target=""#b30"">(Tsipras et al., 2019)</ref> and adversarial polytope<ref type=""bibr"" target=""#b31"">(Wong et al., 2018)</ref>. It remains an open question whether such trade-off is a necessary part of life, and please s",0
"euk et al., 2018;</ref><ref type=""bibr"" target=""#b1"">Alzantot et al., 2018;</ref><ref type=""bibr"">Carlini &amp; Wagner, 2018)</ref>, image captioning <ref type=""bibr"" target=""#b9"">(Chen et al., 2017)</ref> and natural language processing <ref type=""bibr"" target=""#b13"">(Gao et al., 2018;</ref><ref ty",0
"pe=""bibr"">Carlini &amp; Wagner, 2018)</ref>, image captioning <ref type=""bibr"" target=""#b9"">(Chen et al., 2017)</ref> and natural language processing <ref type=""bibr"" target=""#b13"">(Gao et al., 2018;</ref><ref type=""bibr"" target=""#b12"">Ebrahimi et al., 2017)</ref>. These issues bring up both theoret",0
"at require a flawless f0 estimation, such as using the output of a pitch tracker to generate reference annotations for melody and multi-f0 estimation <ref type=""bibr"" target=""#b18"">[18,</ref><ref type=""bibr"" target=""#b19"">19]</ref>.</p><p>In this paper, we present a novel, data-driven method for mon nditions, the second dataset we use is a collection of 230 monophonic stems taken from MedleyDB and re-synthesized using the methodology presented in <ref type=""bibr"" target=""#b18"">[18]</ref>, which uses an analysis/synthesis approach to generate a synthesized track with a perfect f0 annotation that",1
"f each algorithm, we also evaluate their pitch tracking performance on degraded versions of MDB-stem-synth, using the Audio Degradation Toolbox (ADT) <ref type=""bibr"" target=""#b28"">[28]</ref>. We use four different noise sources provided by the ADT: pub, white, pink, and brown. The pub noise is an a",0
"target=""#b9"">[9]</ref> and PRAAT <ref type=""bibr"" target=""#b10"">[10]</ref>, and the cumulative mean normalized difference function as proposed by YIN <ref type=""bibr"" target=""#b11"">[11]</ref>. More recent approaches include SWIPE <ref type=""bibr"" target=""#b12"">[12]</ref>, which performs template mat",0
"pitch range for every instrument. In addition to data augmentation, various sources of audio timbre can be obtained from software instruments; NSynth <ref type=""bibr"" target=""#b30"">[30]</ref> is an example where the training dataset is generated from the sound of software instruments.</p><p>Pitch va",0
"t Hidden Markov Model based Deep Neural Network acoustic model with a ""bottleneck"" layer using a frame based criterion on a large multilingual corpus <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b6"">7]</ref>. The network up to the bottleneck r"" target=""#b14"">[15]</ref> showed that these multilingual models can be adapted to the specific language to improve performance further. The work by <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b15"">16]</ref> presented bottleneck features for multi-lingual systems where they sho before being separated into multiple language-specific softmax layers, which are trained using cross-entropy <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b13"">14]</ref>. This architecture can also be used as a ""bottleneck"" feature extractor dim P (p|X) =          softmax(WL1e + bL1) if X ∈ XL1 softmax(WL2e + bL2) if X ∈ XL2 . . . softmax(WLne + bLn) if X ∈ XLn</formula><p>Unlike <ref type=""bibr"" target=""#b4"">[5]</ref>, we do not have any bottleneck layer, and the whole model is sequence trained based on CTC loss. Note that her",1
"ng the focus of the models shifted to learning features across languages which can be mapped to the same space <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b10"">11]</ref>. Authors in <ref type=""bibr"" target=""#b11"">[12]</ref> looked at unsupervised pretraining on different languag",1
"anguage-specific softmax layers, which are trained using cross-entropy <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b13"">14]</ref>. This architecture can also be used as a ""bottleneck"" feature extractor, from which ""language independent"" fe",1
"a series of shared feed-forward layers, before being separated into multiple language-specific softmax layers, which are trained using cross-entropy <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b13"">14]</ref>. This architecture can also be",0
"> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""1."">INTRODUCTION</head><p>State-of-the-art speech recognition systems with human-like performance <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref> are trained on hundreds of hours of well-annotated speech. Since annotat",0
"possible and gave competitive results when compared to systems with mono-lingual features. Other approaches <ref type=""bibr"" target=""#b17"">[17,</ref><ref type=""bibr"" target=""#b18"">18]</ref> constructed a shared language independent phone set, which could then also be adapted to the target language.",0
"tems where they showed feature porting is possible and gave competitive results when compared to systems with mono-lingual features. Other approaches <ref type=""bibr"" target=""#b17"">[17,</ref><ref type=""bibr"" target=""#b18"">18]</ref> constructed a shared language independent phone set, which could the",0
"neck"" layer using a frame based criterion on a large multilingual corpus <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b6"">7]</ref>. The network up to the bottleneck layer can be used as a language-independent feature extractor while adapting",0
"es across languages which can be mapped to the same space <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b10"">11]</ref>. Authors in <ref type=""bibr"" target=""#b11"">[12]</ref> looked at unsupervised pretraining on different languages for a cross lingual recognition. The dominant arch",0
"ngual experiments, because systems built on CTC tend to be significantly easier to train than those that have been trained using hidden Markov models <ref type=""bibr"" target=""#b20"">[20,</ref><ref type=""bibr"" target=""#b21"">21]</ref>. <ref type=""bibr"" target=""#b22"">[22]</ref> shows that multi-lingual </p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>P (z|X) =</head><p>p∈CTC Path(z)</p><formula xml:id=""formula_0"">P (p|X)</formula><p>Like in <ref type=""bibr"" target=""#b20"">[20]</ref> we use this loss along with stacked Bidirectional LSTM layers to encode the acoustic information and make fr ote xmlns=""http://www.tei-c.org/ns/1.0"" place=""foot"" n=""2"" xml:id=""foot_1"">The code to train the multi-lingual model will be released as part of EESEN<ref type=""bibr"" target=""#b20"">[20]</ref>.</note> 		</body> 		<back> 			<div type=""references"">  				<listBibl>  <biblStruct xml:id=""b0""> 	<monogr>",0
".0""><head n=""1."">INTRODUCTION</head><p>State-of-the-art speech recognition systems with human-like performance <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref> are trained on hundreds of hours of well-annotated speech. Since annotation is an expensive and time-consuming",0
"om well-trained scenarios to those where large amounts of training data may not be available, cannot be transcribed, or are otherwise hard to come by <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b3"">4]</ref>.</p><p>The standard approach is to train a context dependent Hidden Mark anguage. With the on-set of deep learning the focus of the models shifted to learning features across languages which can be mapped to the same space <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b10"">11]</ref>. Authors in <ref type=""bibr"" target=""#b11"">[12]</ref> looked at unsupe",0
"of the early works in multi-lingual and cross-lingual speech recognition involved the use of language independent features like articulatory features <ref type=""bibr"" target=""#b7"">[8]</ref> to train HMM based systems. Authors in <ref type=""bibr"" target=""#b8"">[9]</ref> used subspace Gaussian mixture",0
"on CTC tend to be significantly easier to train than those that have been trained using hidden Markov models <ref type=""bibr"" target=""#b20"">[20,</ref><ref type=""bibr"" target=""#b21"">21]</ref>. <ref type=""bibr"" target=""#b22"">[22]</ref> shows that multi-lingual CTC systems with shared phones can improv",0
"everages fetch-directed instruction prefetching (FDIP) <ref type=""bibr"" target=""#b14"">[15]</ref> and extends it with unified prefetching into the BTB <ref type=""bibr"" target=""#b12"">[13]</ref>. The scheme, called Boomerang, discovers BTB misses on the prefetch path and fills them by fetching the appr limited to prefetching of instructions. Recent work has addressed this limitation by adding a BTB prefetch capability in a technique called Boomerang <ref type=""bibr"" target=""#b12"">[13]</ref>. Boomerang uses a basic-block-oriented BTB to detect BTB misses, which it then fills by fetching and decodin he blocks. Furthermore, conditional branches that guide local control flow tend to have very short displacements, typically within a few cache blocks <ref type=""bibr"" target=""#b12"">[13]</ref>, as shown by dashed arrows in Figure <ref type=""figure"" target=""#fig_0"">2</ref>. Thus, even for larger funct ch probes for the corresponding cache blocks. For filling the BTBs, Shotgun takes a hybrid approach by incorporating the features from both Boomerang <ref type=""bibr"" target=""#b12"">[13]</ref> and Confluence <ref type=""bibr"" target=""#b9"">[10]</ref>. Specifically, while prefetching instruction blocks he missing branch and stores it into one the BTBs depending on branch type. The rest of the predecoded branches are stored in the BTB Prefetch Buffer <ref type=""bibr"" target=""#b12"">[13]</ref>. On a hit to the BTB Prefetch Buffer, the accessed branch is moved to the appropriate BTB based on the branc ity, pinning of metadata cache lines in the LLC and the associated system software support, making it an expensive proposition as shown in prior work <ref type=""bibr"" target=""#b12"">[13]</ref>. The LLC tag array extension, for storing index table, costs 240KB of storage overhead, whereas the history nce on Apache, Nutch, and Streaming with 16%-19% additional coverage. Confluence performs poorly on these applications, as also noted by Kumar et al. <ref type=""bibr"" target=""#b12"">[13]</ref>, owing to frequent LLC accesses for loading history metadata. On every misprediction in L1-I access sequence",1
"p><p>Microarchitecture researchers have proposed a number of instruction <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b16"">17]</ref> and BTB <ref type=""bibr"" targ t temporal streaming research has focused on lowering the storage costs <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b11"">12]</ref>; however, even with optimizations, for a many-core CMP running several consolidated workloads, the total stor n the branch type.</p><p>Discussion: Similar to Shotgun, two previously proposed techniques, pTask <ref type=""bibr"" target=""#b6"">[7]</ref> and (RDIP) <ref type=""bibr"" target=""#b11"">[12]</ref>), also leverage global </p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""5"">Methodology</head></d",0
"flush when misspeculation is discovered.</p><p>The front-end bottleneck in servers is a well-established problem, first characterized in the late 90s <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b13"">14]</ref>. Over the years, the problem h",0
"/ns/1.0""><head n=""5"">Methodology</head></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""5.1"">Simulation Infrastructure</head><p>We use Flexus <ref type=""bibr"" target=""#b17"">[18]</ref>, a full system multiprocessor simulator, to evaluate Shotgun on a set of enterprise and open-source scale-ou g the cycles spent executing operating system core) to measure performance. This metric has been shown to be an accurate measure of server throughput <ref type=""bibr"" target=""#b17"">[18]</ref>. Our modeled processor is a 16-core tiled CMP. Each core is 3-way out-of-order that microarchitecturally res",0
"pe=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b16"">17]</ref> and BTB <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b2"">3]</ref> prefetchers over the ye",0
"celerators are needed to maximize performance/watt. This observation has led to a flurry of ASIC proposals for DNN accelerators over the recent years <ref type=""bibr"" target=""#b2"">[3]</ref><ref type=""bibr"" target=""#b3"">[4]</ref><ref type=""bibr"" target=""#b4"">[5]</ref><ref type=""bibr"" target=""#b5"">[6] ""#b41"">[42]</ref><ref type=""bibr"" target=""#b42"">[43]</ref> have proposed meshes due to their flexibility to support all-to-all communication. Diannao <ref type=""bibr"" target=""#b2"">[3]</ref> and Shidiannao <ref type=""bibr"" target=""#b43"">[44]</ref> relied on mesh-based interconnects for data transfer. he trade-off between flexibility and efficiency in accelerator domain with an example of convolution accelerator for image processing domain. Diannao <ref type=""bibr"" target=""#b2"">[3]</ref>, DaDiannao <ref type=""bibr"" target=""#b14"">[15]</ref>, ShiDiannao <ref type=""bibr"" target=""#b43"">[44]</ref> are Reduction and Collection Network: ART</head><p>Binary trees are well-suited for performing reductions and have been used in prior DNN implementations <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b18"">19",1
"be mapped over the underlying hardware substrate. This is the approach most of the early DNN accelerators took <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b14"">15]</ref>. However, recently new partitioning approaches that explore additiona > for an array of 100s of tiny PEs, and as a result almost every DNN accelerator has used a hierarchy of buses <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b33"">34]</ref> and/or trees <ref type=""bibr"" target=""#b33"">[34,</ref><ref type=""bibr",1
"simulation infrastructures <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b45"">[46]</ref><ref type=""bibr"" target=""#b46"">[47]</ref><ref type=""bibr"" target=""#b47"">[48]</ref>. Fused-layer CNN <ref type=""bibr"" target=""#b15"">[16]</ref> and others <ref type=""bibr"" target=""#b34"">[35,</r",0
"explore additional data reuse opportunities by changing the partition have emerged -across layers <ref type=""bibr"" target=""#b15"">[16]</ref>, kernels <ref type=""bibr"" target=""#b16"">[17]</ref> and hybrid (kernel, channel, output) <ref type=""bibr"" target=""#b17"">[18]</ref>. Among these, for e.g., Fused or performing reductions and have been used in prior DNN implementations <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b20"">21]</ref> to implement adder trees with rators, such as finding a better way to map data on to hardware or the best configuration of DNN accelerators, using novel simulation infrastructures <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b45"">[46]</ref><ref type=""bibr"" target=""#b46"">[47]</ref><ref type=""bibr"" target=""#b",0
"ud and IoT platforms -to solve complex regression and classification problems in image <ref type=""bibr"" target=""#b0"">[1]</ref> and speech recognition <ref type=""bibr"" target=""#b1"">[2]</ref> with accuracies surpassing those of humans.</p><p>The microarchitecture of DNN inference engines is currently",0
"gn of RNNs was not as active as that of CNNs. In the last two years, hardware acceleration of LSTMs on FPGAs <ref type=""bibr"" target=""#b52"">[53]</ref><ref type=""bibr"" target=""#b53"">[54]</ref><ref type=""bibr"" target=""#b54"">[55]</ref><ref type=""bibr"" target=""#b55"">[56]</ref><ref type=""bibr"" target=""#b",0
"lving signals with temporal correlation, such as speech recognition, transliteration and so on. The most popular RNN is long short-term memory (LSTM) <ref type=""bibr"" target=""#b26"">[27]</ref>, where the output at a certain time depends on the current input value, three gate values (forget, input, an to construct arbitrary sized VNs given sparse data.</p><p>RNN accelerators: Although the algorithm of RNNs has been discussed for more than 20 years <ref type=""bibr"" target=""#b26"">[27,</ref><ref type=""bibr"" target=""#b49"">[50]</ref><ref type=""bibr"" target=""#b50"">[51]</ref><ref type=""bibr"" target=""#b",0
"image or input activations (the output of a previous convolution layer) and produces output activations by sliding a set of filters over input images <ref type=""bibr"" target=""#b24"">[25]</ref>. Convolutions account for almost 90% of the computations in a CNN <ref type=""bibr"" target=""#b25"">[26]</ref>. layer.</p><p>Optimizing for Spatial Reuse in CNNs. Maeri tries to optimize and get the best of the three kinds of dataflows described in the Eyeriss <ref type=""bibr"" target=""#b24"">[25]</ref> taxonomy. Each multiplier switch acts as a weight stationary node without requiring weights to be forwarded not reuse data within the PE array so it requires to read different data every cycle to each row and column, which results in high energy consumption <ref type=""bibr"" target=""#b24"">[25]</ref>. Therefore, the systolic array need to read 1,323 times from the SRAM in the example.</p><p>In contrast, Mae ization tool for FPGA based accelerators. The design used simple adder trees within their computation engine, which can benefit from our ART. Eyeriss <ref type=""bibr"" target=""#b24"">[25]</ref> analyzed data flow patterns in existing CNN accelerators and proposed a new data flow pattern for CNN accele 2""><head>Figure 12 .Figure 13 .</head><label>1213</label><figDesc>Figure 12. Total latency and compute unit utilization of systolic array(SA), Eyeriss<ref type=""bibr"" target=""#b24"">[25]</ref> style row-stationary accelerator, and Maeri with 64 PEs (multiplier switches) for selected conv layers in Al",0
"ed a remarkable degree of accuracy in recognition and classification of images, exceeding human capabilities <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b23"">24]</ref>. Each convolution layer receives input in form of a raw image or input activations (the output of a previous",0
"rged -across layers <ref type=""bibr"" target=""#b15"">[16]</ref>, kernels <ref type=""bibr"" target=""#b16"">[17]</ref> and hybrid (kernel, channel, output) <ref type=""bibr"" target=""#b17"">[18]</ref>. Among these, for e.g., Fused CNN <ref type=""bibr"" target=""#b15"">[16]</ref> retains intermediate outputs of . In addition, we also discuss other related work across DNN accelerator design.</p><p>DNN accelerators for flexible dataflows: FlexFlow [34] and DNA <ref type=""bibr"" target=""#b17"">[18]</ref> are two recent DNN ASIC proposals with similar motivation as this work. FlexFlow demonstrates a design that",0
"able. Next, we examine several alternative solutions and argue their shortcomings when used in real systems.</p><p>OS-level page coloring Lin's study <ref type=""bibr"" target=""#b26"">[29]</ref> aims to offer cache partitioning by using OS-level page coloring which directly assigns LLC among threads in",1
"he similar improvement with dCat. Elasticsearch <ref type=""bibr"" target=""#b0"">[1]</ref> is a widely-used search and analytics engine. We use the YCSB <ref type=""bibr"" target=""#b8"">[11]</ref> cloud benchmark suite to measure the performance of Elasticsearch. We use workload C which reads from a datab",0
"in kernel so far.</p><p>Performation isolation in I/O Performance interference and isolation in I/O subsystem has been studied extensively. SecondNet <ref type=""bibr"" target=""#b15"">[18]</ref> and Oktopus <ref type=""bibr"" target=""#b4"">[7]</ref> statically allocate the network bandwidth to guarantee t",0
"Redis keeps all datae in memory, cache is critical to performance. We run the Redis server in the target VM and run the client side benchmark Memtier <ref type=""bibr"" target=""#b1"">[3]</ref> in another host within the same LAN. 1 million records (128 bytes each) are inserted to Redis server first as",0
"vel cache allocation mechanisms according to the workloads behaviors <ref type=""bibr"" target=""#b19"">[22,</ref><ref type=""bibr"" target=""#b20"">23,</ref><ref type=""bibr"" target=""#b24"">27,</ref><ref type=""bibr"" target=""#b33"">36,</ref><ref type=""bibr"" target=""#b34"">37,</ref><ref type=""bibr"" target=""#b38""",0
"r"" target=""#b5"">[6]</ref>, <ref type=""bibr"" target=""#b6"">[7]</ref>, <ref type=""bibr"" target=""#b7"">[8]</ref>, <ref type=""bibr"" target=""#b8"">[9]</ref>, <ref type=""bibr"" target=""#b9"">[10]</ref>, <ref type=""bibr"" target=""#b10"">[11]</ref>, <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" targe nalysis shows for temporal prefetching. Figure <ref type=""figure"" target=""#fig_0"">1</ref> shows the opportunity of temporal prefetching and what STMS <ref type=""bibr"" target=""#b9"">[10]</ref> and ISB <ref type=""bibr"" target=""#b12"">[13]</ref>, two state-of-the-art temporal data prefetchers, offer for the missed address in the history and prefetch the addresses that follow the missed address in the  history <ref type=""bibr"" target=""#b5"">[6]</ref>, <ref type=""bibr"" target=""#b9"">[10]</ref>. As it is evident from Figure <ref type=""figure"" target=""#fig_0"">1</ref>, the coverage of such a prefetcher i raction of useful lookups. These results clearly show that temporal prefetchers that rely on just one miss address to look up the history (e.g., STMS <ref type=""bibr"" target=""#b9"">[10]</ref>), frequently prefetch incorrectly.</p><p>Another interesting data point, as we increase the number of address t policy. As both HT and IT require multi-megabyte storage for STMS to have a reasonable coverage, both tables are placed off chip in the main memory <ref type=""bibr"" target=""#b9"">[10]</ref>. Consequently, every access to these tables (read or update) should be sent to the main memory, which is slow is slow, and brings/updates a cache block worth of data.</p><p>Figure <ref type=""figure"" target=""#fig_5"">6</ref> shows the timing of events with STMS <ref type=""bibr"" target=""#b9"">[10]</ref>. Upon a cache miss, a request is sent to the main memory to bring an entry of the IT that points to the last the IT. It has been shown that this implementation offers the level of performance similar to that of the non-practical always-update implementation <ref type=""bibr"" target=""#b9"">[10]</ref>.</p><p>As both metadata tables are off-chip, the on-chip storage requirement of the prefetcher is negligible. ggering events and replay the sequence of future data misses. As the size of these two tables is very large (several megabytes), just like prior work <ref type=""bibr"" target=""#b9"">[10]</ref>, both tables are stored in the main memory.</p><p>Domino prefetcher allocates a contiguous portion of the phy the super-entry associated with the missed address. The delay of the search is tolerable because it is considerably smaller than the off-chip latency <ref type=""bibr"" target=""#b9"">[10]</ref>. In case a match is not found, nothing will be done, and otherwise, a prefetch will be sent for the address f tter performance as compared to its practical implementation <ref type=""bibr"" target=""#b12"">[13]</ref>.</p><p>Sampled Temporal Memory Streaming. STMS <ref type=""bibr"" target=""#b9"">[10]</ref> records miss sequences in a global per-core HT and locates streams through an IT. It benefits from a stream-e =""#b9"">[10]</ref> records miss sequences in a global per-core HT and locates streams through an IT. It benefits from a stream-end detection heuristic <ref type=""bibr"" target=""#b9"">[10]</ref>, <ref type=""bibr"" target=""#b39"">[40]</ref> to reduce useless prefetches. We implement STMS with infinite-size ion</head><p>We compare Domino prefetcher against ISB <ref type=""bibr"" target=""#b12"">[13]</ref>, VLDP <ref type=""bibr"" target=""#b33"">[34]</ref>, STMS <ref type=""bibr"" target=""#b9"">[10]</ref>, and Digram <ref type=""bibr"" target=""#b20"">[21]</ref>. As a point of reference, we also include the temporal",1
"match for eliminating future misses. Many pieces of prior work <ref type=""bibr"" target=""#b19"">[20]</ref>, <ref type=""bibr"" target=""#b20"">[21]</ref>, <ref type=""bibr"" target=""#b21"">[22]</ref>, <ref type=""bibr"" target=""#b22"">[23]</ref> demonstrated the effectiveness of temporal prefetching in reducin ref type=""bibr"" target=""#b32"">[33]</ref>, <ref type=""bibr"" target=""#b33"">[34]</ref> prefetchers are usually incapable of prefetching dependent misses <ref type=""bibr"" target=""#b21"">[22]</ref> due to the lack of stride/spatial access patterns among dependent misses, temporal prefetchers can capture s ns. Like prior studies of measuring repetitiveness in data misses <ref type=""bibr"" target=""#b6"">[7]</ref>, <ref type=""bibr"" target=""#b17"">[18]</ref>, <ref type=""bibr"" target=""#b21"">[22]</ref>, we use the Sequitur hierarchical data compression algorithm <ref type=""bibr"" target=""#b34"">[35]</ref> to id entry OPT, and three infinite-size DPTs. As this prefetcher works based on spatial correlation, it is orthogonal to our work and can be used together <ref type=""bibr"" target=""#b21"">[22]</ref>.</p><p>Irregular Stream Buffer. ISB <ref type=""bibr"" target=""#b12"">[13]</ref> combines the use of PC localiz ike prior studies of measuring repetitiveness of access sequences <ref type=""bibr"" target=""#b6"">[7]</ref>, <ref type=""bibr"" target=""#b17"">[18]</ref>, <ref type=""bibr"" target=""#b21"">[22]</ref>, we use the Sequitur hierarchical data compression algorithm <ref type=""bibr"" target=""#b34"">[35]</ref> to id hing while Domino captures temporal correlation of data accesses. As each technique targets different subset of misses, they can be used orthogonally <ref type=""bibr"" target=""#b21"">[22]</ref>. VLDP uses patterns that fall into  a single page and is able to prefetch unobserved misses but is incapable",0
"target=""#b7"">[8]</ref>, <ref type=""bibr"" target=""#b8"">[9]</ref>, <ref type=""bibr"" target=""#b9"">[10]</ref>, <ref type=""bibr"" target=""#b10"">[11]</ref>, <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" target=""#b12"">[13]</ref>, <ref type=""bibr"" target=""#b13"">[14]</ref>, <ref type=""bibr"" targ org/ns/1.0""><head>VI. RELATED WORK</head><p>Data prefetching is an active research area in computer architecture. Thread-based prefetching techniques <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" target=""#b44"">[45]</ref>, <ref type=""bibr"" target=""#b45"">[46]</ref>, <ref type=""bibr"" targ",0
"y because, unlike LLCs, L1 caches cannot significantly exploit the spatial correlation of data accesses due to the low residency of data in the cache <ref type=""bibr"" target=""#b40"">[41]</ref>, <ref type=""bibr"" target=""#b41"">[42]</ref>. Sharing the metadata of different pages in a unified history is performance enhancement. SAT Solver produces its dataset on-the-fly during the execution, and thus, does not have a static and wellstructured dataset <ref type=""bibr"" target=""#b40"">[41]</ref>. Consequently, its memory accesses are hard-to-predict and all techniques manifest low coverage and high ove",0
"stalls, and a fixed instructionper-cycle (IPC) of 1.0.</p><p>We estimate the performance of various designs using Flexus full-system timing simulator <ref type=""bibr"" target=""#b37"">[38]</ref>. Flexus timing simulator extends the Virtutech Simics functional simulator with timing models of cores, cach",0
"such higher-order embeddings based on network motifs. The term motif is used generally and may refer to graphlets or orbits (graphlet automorphisms) <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b5"">6]</ref>. The HONE framework expresses a general family of embedding methods base",1
"network motifs. The term motif is used generally and may refer to graphlets or orbits (graphlet automorphisms) <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b5"">6]</ref>. The HONE framework expresses a general family of embedding methods based on a set of motif-based matrices and f steps K which is selected automatically via a grid search over K ? {1, 2, 3, 4} using 10% of the labeled data. We use all 2-4 node connected orbits <ref type=""bibr"" target=""#b5"">[6]</ref> and set D ? = 16 for the local motif embeddings. All methods use logistic regression (LR) with an L2 penalty.",1
"of networks and embedding methods.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""1"">INTRODUCTION</head><p>Roles represent node (or edge <ref type=""bibr"" target=""#b1"">[2]</ref>) connectivity patterns such as hubs, star-centers, star-edge nodes, near-cliques or nodes that act as bridges",0
"iques or nodes that act as bridges to different regions of the graph. Intuitively, two nodes belong to the same role if they are structurally similar <ref type=""bibr"" target=""#b7"">[8]</ref>. Many network representation learning methods (including random-walk based methods such as node2vec [4]) seek ity locally based on neighborhood properties and/or proximity (e.g., near one another in the graph). However, such methods are insufficient for roles <ref type=""bibr"" target=""#b7"">[8]</ref> as they fail to capture the higher-order connectivity patterns of a node. For instance, instead of representin",0
"embeddings Z = z 1 ? ? ? z N T</formula><p>where z i ? R D . For node2vec, we perform a grid search over p, q ? {0.25, 0.5, 1, 2, 4} as mentioned in <ref type=""bibr"" target=""#b3"">[4]</ref>. All other hyperparameters for node2vec <ref type=""bibr"" target=""#b3"">[4]</ref>, DeepWalk <ref type=""bibr"" tar erform a grid search over p, q ? {0.25, 0.5, 1, 2, 4} as mentioned in <ref type=""bibr"" target=""#b3"">[4]</ref>. All other hyperparameters for node2vec <ref type=""bibr"" target=""#b3"">[4]</ref>, DeepWalk <ref type=""bibr"" target=""#b4"">[5]</ref>, and LINE <ref type=""bibr"" target=""#b8"">[9]</ref> correspond =""#b3"">[4]</ref>, DeepWalk <ref type=""bibr"" target=""#b4"">[5]</ref>, and LINE <ref type=""bibr"" target=""#b8"">[9]</ref> correspond to those mentioned in <ref type=""bibr"" target=""#b3"">[4]</ref>. In contrast, the HONE variants have only one hyperparameter, namely, the number of steps K which is selected",0
"[4]</ref>. All other hyperparameters for node2vec <ref type=""bibr"" target=""#b3"">[4]</ref>, DeepWalk <ref type=""bibr"" target=""#b4"">[5]</ref>, and LINE <ref type=""bibr"" target=""#b8"">[9]</ref> correspond to those mentioned in <ref type=""bibr"" target=""#b3"">[4]</ref>. In contrast, the HONE variants have",0
"lected using 10-fold cross-validation on 10% of the labeled data. Experiments are repeated for 10 random seed initializations. Data was obtained from <ref type=""bibr"" target=""#b6"">[7]</ref>.</p><p>We evaluate the HONE variants for link prediction. Given a partially observed graph G with a fraction o",0
"4} as mentioned in <ref type=""bibr"" target=""#b3"">[4]</ref>. All other hyperparameters for node2vec <ref type=""bibr"" target=""#b3"">[4]</ref>, DeepWalk <ref type=""bibr"" target=""#b4"">[5]</ref>, and LINE <ref type=""bibr"" target=""#b8"">[9]</ref> correspond to those mentioned in <ref type=""bibr"" target=""#b",0
"mness (i.e., exploration) into the decision to find new items. State-of-art reinforcement learning methods usually apply the simple ϵ-greedy strategy <ref type=""bibr"" target=""#b29"">[31]</ref> or Upper Confidence Bound (UCB) <ref type=""bibr"" target=""#b21"">[23,</ref><ref type=""bibr"" target=""#b41"">43]< commendation. First, in order to better model the dynamic nature of news characteristics and user preference, we propose to use Deep Q-Learning (DQN) <ref type=""bibr"" target=""#b29"">[31]</ref> framework. This framework can consider current reward and future reward simultaneously. Some recent attempts ich can both improve recommendation diversity and avoid the harm to recommendation accuracy induced by classical exploration strategies like ϵ-greedy <ref type=""bibr"" target=""#b29"">[31]</ref> and Upper Confidence Bound <ref type=""bibr"" target=""#b21"">[23]</ref>.</p><p>Our method is significantly diff t G will use the user feedback B and user activeness stored in the memory to update the network Q.</p><p>Here, we use the experience replay technique <ref type=""bibr"" target=""#b29"">[31]</ref> to update the network. Specifically, agent G maintains a memory with recent historical click and user active ><p>Considering the previous mentioned dynamic feature of news recommendation and the need to estimate future reward, we apply a Deep Q-Network (DQN) <ref type=""bibr"" target=""#b29"">[31]</ref> to model the probability that one user may click on one specific piece of news. Under the setting of reinfor p://www.tei-c.org/ns/1.0""><head n=""4.5"">Explore</head><p>The most straightforward strategies to do exploration in reinforcement learning are ϵ-greedy <ref type=""bibr"" target=""#b29"">[31]</ref> and UCB <ref type=""bibr"" target=""#b21"">[23]</ref>. ϵ-greedy will randomly recommend new items with a probabi",1
"41"">43]</ref> usually only consider the click / no click labels or ratings as users' feedback. However, how soon one user will return to this service <ref type=""bibr"" target=""#b46"">[48]</ref> will also indicate how satisfied this user is with the recommendation. Nevertheless, there has been little w we consider user return as another form of user feedback information, by maintaining an activeness score for each user. Different from existing work <ref type=""bibr"" target=""#b46"">[48]</ref> that can only consider the most recent return interval, we consider multiple historical return interval info recent return interval, we consider multiple historical return interval information to better measure the user feedback. In addition, different from <ref type=""bibr"" target=""#b46"">[48]</ref>, our model can estimate user activeness at any time (not just when user returns). This property enables the rsized and can not be updated incrementally), <ref type=""bibr"" target=""#b43"">[45]</ref> (similar with W&amp;D when textual features are removed), and <ref type=""bibr"" target=""#b46"">[48]</ref> (user return is not applicable to experience replay update).</p><p>• LR. Logistic Regression is widely used",0
"rience replay update used in DQN. Third, we propose to apply a Dueling Bandit Gradient Descent (DBGD) method <ref type=""bibr"" target=""#b14"">[16,</ref><ref type=""bibr"" target=""#b15"">17,</ref><ref type=""bibr"" target=""#b47"">49]</ref> for exploration, by choosing random item candidates in the neighborho eriod. Therefore, rather than doing random exploration, we apply a Dueling Bandit Gradient Descent algorithm <ref type=""bibr"" target=""#b14"">[16,</ref><ref type=""bibr"" target=""#b15"">17,</ref><ref type=""bibr"" target=""#b47"">49]</ref> to do the exploration. Intuitively, as shown in Figure <ref type=""fig",0
"ection 6.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2"">RELATED WORK 2.1 News recommendation algorithms</head><p>Recommender systems <ref type=""bibr"" target=""#b1"">[3,</ref><ref type=""bibr"" target=""#b2"">4]</ref> have been investigated extensively because of its direct connection to p",0
"target=""#b22"">24,</ref><ref type=""bibr"" target=""#b23"">25]</ref>. Recently, as an extension and integration of previous methods, deep learning models <ref type=""bibr"" target=""#b6"">[8,</ref><ref type=""bibr"" target=""#b43"">45,</ref><ref type=""bibr"" target=""#b50"">52]</ref> have become the new state-of-a </ref> are further proposed to improve the user profile modeling. Recently, as an extension and integration of previous methods, deep learning models <ref type=""bibr"" target=""#b6"">[8,</ref><ref type=""bibr"" target=""#b43"">45,</ref><ref type=""bibr"" target=""#b50"">52]</ref> have shown much superior perfo =""bibr"" target=""#b10"">[12,</ref><ref type=""bibr"" target=""#b22"">24,</ref><ref type=""bibr"" target=""#b23"">25]</ref>. Recently, many deep learning models <ref type=""bibr"" target=""#b6"">[8,</ref><ref type=""bibr"" target=""#b43"">45,</ref><ref type=""bibr"" target=""#b50"">52]</ref> are further proposed in order akes all the four categories of features as input, use the combination of features and their interactions to do the click prediction.</p><p>• W&amp;D <ref type=""bibr"" target=""#b6"">[8]</ref>. Wide &amp; Deep is a widely used state-of-art deep learning model combining the memorization (through a logis",0
"mplex tasks such as sampling in a principled way. The field that gathers all these questions under a common umbrella is graph signal processing (GSP) <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b2"">[3]</ref>.</p><p>While the precise definition of a graph signal will be given l al processing</head><p>We now briefly review some of the prior work that is more directly connected and in the spirit of signal processing on graphs, <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b2"">[3]</ref>. We organize the discussion along two main lines; some parts of the e #b1"">[2]</ref>, <ref type=""bibr"" target=""#b2"">[3]</ref>. We organize the discussion along two main lines; some parts of the exposition follow closely <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b44"">[45]</ref>.</p><p>1) From algebraic signal processing to graph signal processi ed, graph adjacency matrices as shifts that generate the graph signal model for signals indexed by nodes of an arbitrary directed or undirected graph <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b50"">[51]</ref>. This choice is satisfying in the sense that, when the signal model "">[80]</ref>, <ref type=""bibr"" target=""#b80"">[81]</ref> studies time signals. Graph signal processing (GSP)<ref type=""foot"" target=""#foot_0"">1</ref>  <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b2"">[3]</ref>, <ref type=""bibr"" target=""#b44"">[45]</ref> extends DSP to signal samp − 1 s 0 s 1 s N−2 s N−1</formula><p>This graph interpretation of DSP can be extended to develop a linear time shift invariant Graph Signal Processing <ref type=""bibr"" target=""#b1"">[2]</ref>. Consider now a graph signal s ∈ C N , where the entries of the signal s are indexed by the N nodes of an arbi lacian defined as L = D−A, and the symmetric normalized Laplacian L = D −1/2 LD −1/2 .</formula><p>The adjacency matrix A can be adopted as the shift <ref type=""bibr"" target=""#b1"">[2]</ref> for this general graph. Other choices have been proposed, including the Laplacians <ref type=""bibr"" target=""#b s. A filter represented by H will be shift invariant if it commutes with the shift,</p><formula xml:id=""formula_18"">AH = HA.</formula><p>As proven in <ref type=""bibr"" target=""#b1"">[2]</ref>, if the characteristic polynomial p A (z) and the minimum polynomial <ref type=""foot"" target=""#foot_3"">4</ref> he graph Fourier filtering Theorem that reduces graph filtering to two graph Fourier transforms and a pointwise multiplication in the spectral domain <ref type=""bibr"" target=""#b1"">[2]</ref>. With a notion of frequency we can now consider the GSP equivalents to classical concepts of low-, high-, and rtible and eigenvectors do exist (as discussed in Section II-C). If these conditions do not hold, the Jordan canonical form is used to obtain the GFT <ref type=""bibr"" target=""#b1"">[2]</ref>, but this is well known to be a numerically unstable procedure. As an alternative, some authors have proposed",1
"at generate the graph signal model for signals indexed by nodes of an arbitrary directed or undirected graph <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b50"">[51]</ref>. This choice is satisfying in the sense that, when the signal model is the classical time signal model, the G (v k ) = v k − A norm v k 1 ,</formula><p>where • 1 is norm 1, and A norm = 1 λmax A. Other norms could be used to define the total variation, see <ref type=""bibr"" target=""#b50"">[51]</ref> <ref type=""bibr"" target=""#b2"">[3]</ref>. Using this, graph frequency λ m is larger than graph frequency λ if n criterion it has been shown experimentally and justified theoretically that the frequency bases obtained from the shift operator tend to be ordered <ref type=""bibr"" target=""#b50"">[51]</ref>.</p><p>Up to this point, we have focused primarily on frequency representations derived from the adjacency m w-pass) graph signal. Such a smooth graph signal model makes it possible to detect outliers or abnormal values by highpass filtering and thresholding <ref type=""bibr"" target=""#b50"">[51]</ref>, <ref type=""bibr"" target=""#b154"">[155]</ref>, or to build effective signal reconstruction methods from spars an be used in the process of estimating labels <ref type=""bibr"" target=""#b4"">[5]</ref>, optimizing the prediction of unknown labels in classification <ref type=""bibr"" target=""#b50"">[51]</ref> or semisupervised learning problems <ref type=""bibr"" target=""#b194"">[195]</ref>. Furthermore, as labeled sam",1
"18"">[119]</ref>, and subsequently several papers proposed solutions for different aspects of the problem <ref type=""bibr"" target=""#b119"">[120]</ref>, <ref type=""bibr"" target=""#b120"">[121]</ref>, <ref type=""bibr"" target=""#b121"">[122]</ref>. In particular, sampling results have been generalized to dir br"" target=""#b120"">[121]</ref>, <ref type=""bibr"" target=""#b121"">[122]</ref>. In particular, sampling results have been generalized to directed graphs <ref type=""bibr"" target=""#b120"">[121]</ref>, <ref type=""bibr"" target=""#b121"">[122]</ref> and to other classes of signals such as piecewise smooth sign em, several methods approach the problem of sampling set selection from an experiment design perspective <ref type=""bibr"" target=""#b123"">[124]</ref>, <ref type=""bibr"" target=""#b120"">[121]</ref>, <ref type=""bibr"" target=""#b121"">[122]</ref> setting as a goal to identify a set of vertices that minimize in sampling set identification, especially for large-scale graphs. Some techniques require computing and storing the first K basis vectors of the GFT <ref type=""bibr"" target=""#b120"">[121]</ref>. For larger graph sizes, where this may not be practical, the approach in <ref type=""bibr"" target=""#b121""> ty but, as a random sampling technique, it may not always lead to performance comparable to those of more complex greedy optimization methods such as <ref type=""bibr"" target=""#b120"">[121]</ref>, <ref type=""bibr"" target=""#b121"">[122]</ref>.</p><p>Given the samples of a graph signal, the next objectiv",1
"/ref>) or localization operators that have both a spectral interpretation and vertex domain localization <ref type=""bibr"" target=""#b129"">[130]</ref>, <ref type=""bibr"" target=""#b130"">[131]</ref>. Notions of stationarity can help develop probabilistic graph signal processing methods leading to graph-b y can help develop probabilistic graph signal processing methods leading to graph-based Wiener filtering <ref type=""bibr"" target=""#b131"">[132]</ref>, <ref type=""bibr"" target=""#b130"">[131]</ref>.</p><p>A study of vertex/spectral localization and uncertainty principles was first developed by <ref type",1
"of the adjacency matrix shift <ref type=""bibr"" target=""#b51"">[52]</ref> 2) From graph Laplacian spectral clustering to Laplacianbased GSP: References <ref type=""bibr"" target=""#b53"">[54]</ref>, <ref type=""bibr"" target=""#b54"">[55]</ref>, <ref type=""bibr"" target=""#b55"">[56]</ref>, <ref type=""bibr"" targ",0
"al reconstruction filters have been proposed in order to reconstruct an estimated signal on the whole graph based on the observed vertex measurements <ref type=""bibr"" target=""#b125"">[126]</ref>, <ref type=""bibr"" target=""#b126"">[127]</ref>.</p><p>While theoretical aspects of graph signal sampling are",0
"103"">[104]</ref>, <ref type=""bibr"" target=""#b84"">[85]</ref> can be applied to circulant graphs, for which the GFT corresponds to the DFT. Recent work <ref type=""bibr"" target=""#b104"">[105]</ref>, <ref type=""bibr"" target=""#b105"">[106]</ref> has shown that similar filterbank designs can be developed fo work is focusing on i) providing better tools to characterize Mblock cyclic graphs, including for example the definition of polyphase representations <ref type=""bibr"" target=""#b104"">[105]</ref>, <ref type=""bibr"" target=""#b105"">[106]</ref>, <ref type=""bibr"" target=""#b107"">[108]</ref>, <ref type=""bibr",0
"Section III-A).</p><p>A. The role of shifts in digital signal processing Discrete signal processing (DSP) <ref type=""bibr"" target=""#b76"">[77]</ref>, <ref type=""bibr"" target=""#b77"">[78]</ref>, <ref type=""bibr"" target=""#b78"">[79]</ref>, <ref type=""bibr"" target=""#b79"">[80]</ref>, <ref type=""bibr"" targ",0
"br"" target=""#b148"">[149]</ref>. This allows inferring system features and behaviors that are hidden in the measured datasets (e.g., ozone datasets in <ref type=""bibr"" target=""#b164"">[165]</ref>).</p><p>Finally, several of the graph signal processing operators presented in this paper are amenable to",0
"struction methods from sparse set of sensor readings, as in <ref type=""bibr"" target=""#b155"">[156]</ref>, <ref type=""bibr"" target=""#b156"">[157]</ref>, <ref type=""bibr"" target=""#b157"">[158]</ref>, which can potentially lead to significant savings in energy resources, bandwidth and latency in sensor ne",0
"plorations of graph-based processing focused on sensor networks <ref type=""bibr"" target=""#b63"">[64]</ref>, <ref type=""bibr"" target=""#b64"">[65]</ref>, <ref type=""bibr"" target=""#b152"">[153]</ref>, <ref type=""bibr"" target=""#b153"">[154]</ref>.</p><p>A first approach to define a graph associated to a sen",0
"]</ref>, <ref type=""bibr"" target=""#b130"">[131]</ref>.</p><p>A study of vertex/spectral localization and uncertainty principles was first developed by <ref type=""bibr"" target=""#b132"">[133]</ref>, where it was shown that in general it is not possible to achieve arbitrarily good localization in both sp",0
"n alternative definitions of frequency (see Section III-A).</p><p>A. The role of shifts in digital signal processing Discrete signal processing (DSP) <ref type=""bibr"" target=""#b76"">[77]</ref>, <ref type=""bibr"" target=""#b77"">[78]</ref>, <ref type=""bibr"" target=""#b78"">[79]</ref>, <ref type=""bibr"" targ",0
"he pre-specified class. To address this problem, several methods approach the problem of sampling set selection from an experiment design perspective <ref type=""bibr"" target=""#b123"">[124]</ref>, <ref type=""bibr"" target=""#b120"">[121]</ref>, <ref type=""bibr"" target=""#b121"">[122]</ref> setting as a goa",0
"ere sensors are irregularly placed, different authors develop regression algorithms <ref type=""bibr"" target=""#b61"">[62]</ref>, wavelet decompositions <ref type=""bibr"" target=""#b62"">[63]</ref>, <ref type=""bibr"" target=""#b63"">[64]</ref>, <ref type=""bibr"" target=""#b64"">[65]</ref>, <ref type=""bibr"" targ",0
"nd the underlying graph (whose nodes label the variables) captures statistical dependence and conditional independence among the data. Acyclic graphs <ref type=""bibr"" target=""#b30"">[31]</ref>, <ref type=""bibr"" target=""#b31"">[32]</ref> represent Bayesian networks, and undirected graphs represent Mark",0
"as introduced in <ref type=""bibr"" target=""#b118"">[119]</ref>, and subsequently several papers proposed solutions for different aspects of the problem <ref type=""bibr"" target=""#b119"">[120]</ref>, <ref type=""bibr"" target=""#b120"">[121]</ref>, <ref type=""bibr"" target=""#b121"">[122]</ref>. In particular,",0
"b37"">[38]</ref>, <ref type=""bibr"" target=""#b38"">[39]</ref>, <ref type=""bibr"" target=""#b39"">[40]</ref>, <ref type=""bibr"" target=""#b40"">[41]</ref>, see <ref type=""bibr"" target=""#b41"">[42]</ref> for illustrative applications in several domains. Recent work on learning graph from data <ref type=""bibr"" t",0
"genvectors that are often crucial in the design of clustering algorithms and other machine learning tasks. One of the initial works in this direction <ref type=""bibr"" target=""#b199"">[200]</ref> proposes to use power methods (that can be shown to be related to graph filter operators) to speed up the",0
"ant.) Note also that critical sampling with polynomial analysis and synthesis filters on undirected graphs can only be achieved in the bipartite case <ref type=""bibr"" target=""#b106"">[107]</ref>  <ref type=""foot"" target=""#foot_6"">7</ref> . Ongoing work is focusing on i) providing better tools to char ed for any graph, but this requires a synthesis operation corresponding to an N × N matrix multiplication, which may not be practical for large graphs<ref type=""bibr"" target=""#b106"">[107]</ref>. As an example, the approach in<ref type=""bibr"" target=""#b84"">[85]</ref> guarantees invertibility but reco",0
"textual, audio and visual features as described below.</p><p>Visual Feature Extraction For extracting visual features from the videos, we use 3D-CNN <ref type=""bibr"" target=""#b15"">[16]</ref>. 3D-CNN has achieved state-of-the-art results in object classification on tridimensional data <ref type=""bib we use 3D-CNN <ref type=""bibr"" target=""#b15"">[16]</ref>. 3D-CNN has achieved state-of-the-art results in object classification on tridimensional data <ref type=""bibr"" target=""#b15"">[16]</ref>. 3D-CNN not only extracts features from each image frame, but also extracts spatiotemporal features <ref typ",1
"ut video, v.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>Textual Features Extraction</head><p>We use Convolutional Neural Networks (CNN) <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b18"">19]</ref> to extract features from the transcript of a video, v. First, we use",1
"/www.tei-c.org/ns/1.0""><head>Textual Features Extraction</head><p>We use Convolutional Neural Networks (CNN) <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b18"">19]</ref> to extract features from the transcript of a video, v. First, we use pretrained Word2Vec <ref type=""bibr"" tar",1
"put video, v. Finally, we obtain a feature vector, t f , of dimension 300 for an input text (transcript), t.</p><p>Audio Feature Extraction openSMILE <ref type=""bibr"" target=""#b21"">[22]</ref> is an open-source toolkit used to extract high dimensional features from an audio file. In this work, we use",1
"target=""#b10"">[11]</ref> defined micro-expressions as short involuntary expressions, which could potentially indicate deceptive behavior. Caso et al. <ref type=""bibr"" target=""#b11"">[12]</ref> identified particular hand gesture to be important to identify the act of deception. Cohen et al. <ref type=",0
"others may have severe ramifications in the society. Reports suggest that the ability of humans to detect deception without special aids is only 54% <ref type=""bibr"" target=""#b0"">[1]</ref>. A study by DePaulo et al. <ref type=""bibr"" target=""#b1"">[2]</ref> found that deception without any particular",0
"et=""#b17"">[18,</ref><ref type=""bibr"" target=""#b18"">19]</ref> to extract features from the transcript of a video, v. First, we use pretrained Word2Vec <ref type=""bibr"" target=""#b19"">[20]</ref> model to extract the vector representations for every word in the transcript. These vectors are concatenated",0
"al data <ref type=""bibr"" target=""#b15"">[16]</ref>. 3D-CNN not only extracts features from each image frame, but also extracts spatiotemporal features <ref type=""bibr"" target=""#b16"">[17]</ref> from the whole video which helps in identifying the facial expressions such as smile, fear, or stress.</p><p",0
"pe=""bibr"" target=""#b5"">[6]</ref>, of the statement by the subject under consideration. Use of more complex features such as psycholinguistic features <ref type=""bibr"" target=""#b6"">[7]</ref> based on the Linguistic Inquiry and Word Count (LIWC) lexicon, have also been explored by <ref type=""bibr"" tar",0
"nce judgment made by humans are often biased. Facial expressions and hand gestures were found to be very helpful in detecting deceptive nature. Ekman <ref type=""bibr"" target=""#b10"">[11]</ref> defined micro-expressions as short involuntary expressions, which could potentially indicate deceptive behav",0
"e is removed from the audio and Z-standardization is used to perform voice normalization. To remove the background noise, we use SoX (Sound eXchange) <ref type=""bibr"" target=""#b22"">[23]</ref> audio processing tool. The noiseless input audio is then fed to the openSMILE tool to extract high-dimension",0
"st that the ability of humans to detect deception without special aids is only 54% <ref type=""bibr"" target=""#b0"">[1]</ref>. A study by DePaulo et al. <ref type=""bibr"" target=""#b1"">[2]</ref> found that deception without any particular motivation or intention exhibited almost no detectable cues of dec",0
"is employed for max-pooling over these feature maps. Subsequently, a full-connected layer with 300 neurons is used with rectified linear unit (ReLU) <ref type=""bibr"" target=""#b20"">[21]</ref> as the activation function. The activations of this full-connected layer is used as the textual feature repr perceptron (MLP) (cite) with hidden layer of size 1024 followed by a linear output layer. We use the rectified linear unit (ReLU) activation function <ref type=""bibr"" target=""#b20"">[21]</ref> for non-linearity at the hidden layer. A dropout <ref type=""bibr"" target=""#b23"">[24]</ref> of keep probabili",0
"re helpful in detecting deceptive behavior. Yancheva and Rudzicz studied the relation between the syntactic complexity of text and deceptive behavior <ref type=""bibr"" target=""#b7"">[8]</ref>. 1 In non-verbal deception detection, physiological measures were the main source of signals for detecting dec",0
"e use the rectified linear unit (ReLU) activation function <ref type=""bibr"" target=""#b20"">[21]</ref> for non-linearity at the hidden layer. A dropout <ref type=""bibr"" target=""#b23"">[24]</ref> of keep probability, p = 0.5, is applied to the hidden layer for regularization.</p><p>Figure <ref type=""fig",0
"tecting deception. They also evaluated the performance of humans in deception detection and compared it with their machine learning models. Wu et al. <ref type=""bibr"" target=""#b14"">[15]</ref> have developed methods that leverage multi-modal features for detecting detection. Their method heavily emph bleeding of personalities between train and test set, we perform a 10-fold cross validation with subjects instead of videos as suggested by Wu et al. <ref type=""bibr"" target=""#b14"">[15]</ref>. This ensures that videos of the same subjects are not in both training and test set.</p></div> <div xmlns="" he same subjects are not in both training and test set.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.2"">Baselines</head><p>Wu et al. <ref type=""bibr"" target=""#b14"">[15]</ref> have made use of various classifiers such as Logistic Regression (LR), Linear SVM (L-SVM), Kernel SVM (K-SVM",0
"rt rate, respiration rate, skin temperature of the subject under investigation. But these tests are not reliable and often misleading as indicated by <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b9"">10]</ref> since judgment made by humans are often biased. Facial expressions and",0
"nd Non-verbal. In verbal deception detection, the features are based on the linguistic characteristics, such as n-grams and sentence count statistics <ref type=""bibr"" target=""#b5"">[6]</ref>, of the statement by the subject under consideration. Use of more complex features such as psycholinguistic fe linguistic features <ref type=""bibr"" target=""#b6"">[7]</ref> based on the Linguistic Inquiry and Word Count (LIWC) lexicon, have also been explored by <ref type=""bibr"" target=""#b5"">[6]</ref> and shown that they are helpful in detecting deceptive behavior. Yancheva and Rudzicz studied the relation bet",0
"sely affects the victim and leads to a variety of emotional responses such as lowered self-esteem, increased suicidal thoughts, anger, and depression <ref type=""bibr"" target=""#b3"">[4]</ref>. Teenagers fall prey to these attacks due to their inability to comprehend the chicanery and pretentious behav",0
"dentity and/or pretend to be someone else on the social media. Cyberbullying is increasingly becoming a common problem amongst the teenagers nowadays <ref type=""bibr"" target=""#b2"">[3]</ref>. These include spreading rumors about a person, threats, and sexual harassment. Cyberbullying adversely affect",0
"periments.</p><p>ESPnet fully utilizes benefits of two major end-to-end ASR implementations based on both connectionist temporal classification (CTC) <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b11"">12]</ref> and attention-based encoder-d",1
"n many cases.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.2.2."">Attention</head><p>ESPnet uses a location-aware attention mechanism <ref type=""bibr"" target=""#b34"">[35]</ref>, as a default attention. A dot-product attention <ref type=""bibr"" target=""#b35"">[36]</ref> is also supported ports. Since these reports are based on different conditions (e.g., <ref type=""bibr"" target=""#b32"">[33]</ref> does not use any language models, while <ref type=""bibr"" target=""#b34"">[35]</ref> and <ref type=""bibr"" target=""#b10"">[11]</ref> use a word-based language model through FST), we cannot direct",1
"ition in an end-to-end manner. ESPnet adopts widely-used dynamic neural network toolkits, Chainer <ref type=""bibr"" target=""#b7"">[8]</ref> and PyTorch <ref type=""bibr"" target=""#b8"">[9]</ref>, as a main deep learning engine. ESPnet also follows the style of Kaldi ASR toolkit <ref type=""bibr"" target=""#",0
"rget=""#b5"">[6]</ref> network <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b15"">16]</ref>. Attention-based methods use an attention mechanism to perform alignment between acoustic frames and recogniz s below 5%, and this degradation probably comes from the lack of the amount of training data. Actually, <ref type=""bibr"" target=""#b11"">[12]</ref> and <ref type=""bibr"" target=""#b15"">[16]</ref> report comparable or superior performance to the state-of-the-art hybrid HMM/DNN systems in very large Engli",0
"s including Google voice search, Amazon Alexa, and Apple Siri and open source activities including Kaldi <ref type=""bibr"" target=""#b0"">[1]</ref>, HTK <ref type=""bibr"" target=""#b1"">[2]</ref>, Sphinx <ref type=""bibr"" target=""#b2"">[3]</ref>, Julius <ref type=""bibr"" target=""#b3"">[4]</ref>, RASR <ref typ",0
"target=""#b21"">[22]</ref>, HKUST Mandarin CTS <ref type=""bibr"" target=""#b22"">[23]</ref>, VoxForge <ref type=""bibr"" target=""#b23"">[24]</ref>, CHiME-4/5 <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b25"">26]</ref>, etc. Thus, ESPnet provides publicly available state-of-the-art endt ion, the ESPnet recipes also include noise robust/far-field speech recognition tasks including AMI <ref type=""bibr"" target=""#b21"">[22]</ref>, CHiME-4 <ref type=""bibr"" target=""#b24"">[25]</ref>, and CHiME-5 tasks <ref type=""bibr"" target=""#b25"">[26]</ref>. Especially ES-Pnet is an official end-to-end A",0
"f>, Corpus of Spontaneous Japanese (CSJ) <ref type=""bibr"" target=""#b20"">[21]</ref>, AMI <ref type=""bibr"" target=""#b21"">[22]</ref>, HKUST Mandarin CTS <ref type=""bibr"" target=""#b22"">[23]</ref>, VoxForge <ref type=""bibr"" target=""#b23"">[24]</ref>, CHiME-4/5 <ref type=""bibr"" target=""#b24"">[25,</ref><ref",0
""">[19]</ref>, TED-LIUM <ref type=""bibr"" target=""#b19"">[20]</ref>, Corpus of Spontaneous Japanese (CSJ) <ref type=""bibr"" target=""#b20"">[21]</ref>, AMI <ref type=""bibr"" target=""#b21"">[22]</ref>, HKUST Mandarin CTS <ref type=""bibr"" target=""#b22"">[23]</ref>, VoxForge <ref type=""bibr"" target=""#b23"">[24]< <ref type=""bibr"" target=""#b41"">[42]</ref>. In addition, the ESPnet recipes also include noise robust/far-field speech recognition tasks including AMI <ref type=""bibr"" target=""#b21"">[22]</ref>, CHiME-4 <ref type=""bibr"" target=""#b24"">[25]</ref>, and CHiME-5 tasks <ref type=""bibr"" target=""#b25"">[26]</r",0
"while CTC uses Markov assumptions to efficiently solve sequential problems by dynamic programming. ESPnet adopts hybrid CTC/attention end-to-end ASR <ref type=""bibr"" target=""#b16"">[17]</ref>, which effectively utilizes the advantages of both architectures in training and decoding. During training, above basic architecture, ESPnet supports a number of end-to-end ASR techniques including a fusion of recurrent neural network language model (RNNLM) <ref type=""bibr"" target=""#b16"">[17]</ref>, fast CTC computation by using the warp CTC library <ref type=""bibr"" target=""#b11"">[12]</ref>, many variatio f>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.3."">Hybrid CTC/attention</head><p>ESPnet adopts hybrid CTC/attention end-to-end ASR <ref type=""bibr"" target=""#b16"">[17]</ref>, which effectively utilizes the advantages of both architectures in training and decoding.</p></div> <div xm el></formula><p>This hybrid CTC/attention architecture (multiobjective learning during training and joint decoding during recognition) is proposed in <ref type=""bibr"" target=""#b16"">[17]</ref>, and a unique function compared with the other end-to-end ASR systems.</p></div> <div xmlns=""http://www.tei-",0
"s, for example SRILM <ref type=""bibr"" target=""#b5"">[6]</ref> network <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b15"">16]</ref>. Attention-based methods use an attention mechanism to perform alignm ."">Encoder</head><p>The default encoder network is represented by bidirectional long short-term memory (BLSTM) with subsampling (called pyramid BLSTM <ref type=""bibr"" target=""#b14"">[15]</ref>) given T -length speech feature sequence o1:T to extract high-level feature sequence h 1:T as</p><formula xm",0
"havior history and then apply the state-of-art graph embedding methods <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b16"">17]</ref> to learn the embedding of each item, dubbed Base Graph Embedding (BGE). In this way, we can generate the cand sets are greater than 99%.</p><p>Comparing Methods. Experiments are conducted to compare four methods: BGE, LINE, GES, and EGES. LINE was proposed in <ref type=""bibr"" target=""#b16"">[17]</ref>, which captures the first-order and second-order proximity in graph embedding. We use the implementation pro",1
"of graph embedding. With the representation in hand, various prediction models can be used to recommend. In <ref type=""bibr"" target=""#b26"">[27,</ref><ref type=""bibr"" target=""#b28"">29]</ref>, embeddings of users and items are learned under the supervision of meta-path and meta-graphs, respectively, etworks. Yu et al. <ref type=""bibr"" target=""#b26"">[27]</ref> proposed a linear model to aggregate the embeddings for recommendation while Zhao et al. <ref type=""bibr"" target=""#b28"">[29]</ref> proposed to apply factorization machine to the embeddings for recommendation. In <ref type=""bibr"" target=""#b",0
"/p><p>Dataset. We use two datasets for the link prediction task. The first is Amazon Electronics<ref type=""foot"" target=""#foot_0"">2</ref> provided by <ref type=""bibr"" target=""#b11"">[12]</ref>, denoted as Amazon. The second is extracted from Mobile Taobao App, denoted as Taobao. Both of these two dat",0
"focusing on designing new embedding algorithms. These methods could be categorized into three broad categories: 1) Factorization methods such as LINE <ref type=""bibr"" target=""#b0"">[1]</ref> try to approximately factorize the adjacency matrix and preserve both first order and second proximities; 2) D",0
"10"">11,</ref><ref type=""bibr"" target=""#b15"">16]</ref>, content-based methods <ref type=""bibr"" target=""#b1"">[2]</ref>, and deep learning based methods <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b21"">22]</ref>, the problems facing these metho",0
"tandard regression models. We take inspiration from recent works in image and audio generation that discretize the space, namely PixelRNN and Wavenet <ref type=""bibr"" target=""#b37"">(Oord et al., 2016a;</ref><ref type=""bibr"">b)</ref>. Discretization makes prefetching more analogous to neural language",1
"ere is a second order benefit if it is within a page, usually 4096 bytes, but even predicting at the page level would leave 2 52 possible targets. In <ref type=""bibr"" target=""#b38"">(Oord et al., 2016b)</ref>, they predict 16-bit integer values from an acoustic signal. To avoid having to apply a soft",1
"ning in microarchitecture include applying reinforcement learning for optimizing the long-term performance of memory controller scheduling algorithms <ref type=""bibr"" target=""#b15"">(Ipek et al., 2008)</ref>, tuning performance knobs <ref type=""bibr"" target=""#b2"">(Blanton et al., 2015)</ref>, and usi",0
"rged as a powerful technique to address sequence prediction problems, such as those found in natural language processing (NLP) and text understanding <ref type=""bibr"" target=""#b1"">(Bengio et al., 2003;</ref><ref type=""bibr"" target=""#b31"">Mikolov et al., 2010;</ref><ref type=""bibr"">2013)</ref>. Simpl",0
">Very recent work has also explored the usage of machine learning to replace conventional database index structures such as b-trees and bloom filters <ref type=""bibr"" target=""#b24"">(Kraska et al., 2017)</ref>.</p><p>Although the nature of this problem differs from cache prefetching, there are many s",0
"ibr"" target=""#b35"">Murdoch et al., 2018)</ref>.</p><p>Recent work has also identified timing based attacks as a vulnerability for speculative systems <ref type=""bibr"" target=""#b22"">(Kocher et al., 2018;</ref><ref type=""bibr"" target=""#b26"">Lipp et al., 2018)</ref>, security implications and adversari",0
"SR network equipped with SFT can generate more realistic and visually pleasing textures in comparison to state-of-the-art SRGAN [27]  and EnhanceNet <ref type=""bibr"" target=""#b37"">[38]</ref>.</p></div> 			</abstract> 		</profileDesc> 	</teiHeader> 	<text xml:lang=""en""> 		<body> <div xmlns=""http://w optimize a superresolution model in a feature space instead of pixel space. Ledig et al. <ref type=""bibr"" target=""#b26"">[27]</ref> and Sajjadi et al. <ref type=""bibr"" target=""#b37"">[38]</ref> further propose adversarial loss to encourage the network to favor solutions building x4 plant x4</p></div> ce. Ledig et al. <ref type=""bibr"" target=""#b26"">[27]</ref> introduce an adversarial loss, generating images with more natural details. Sajjadi et al. <ref type=""bibr"" target=""#b37"">[38]</ref> develop a similar approach and further explore the local texture matching loss, partly reducing visually unp RGAN[27] SFT-GAN (ours) GT</head><p>Figure <ref type=""figure"">5</ref>. GAN-based methods (SRGAN <ref type=""bibr"" target=""#b26"">[27]</ref>, EnhanceNet <ref type=""bibr"" target=""#b37"">[38]</ref> and ours) clearly outperform PSNR-oriented approaches in term of perceptual quality. Our proposed SFT-GAN is ef>, MemNet <ref type=""bibr"" target=""#b43"">[44]</ref>, and GAN-based methods, such as SRGAN <ref type=""bibr"" target=""#b26"">[27]</ref> and En-hanceNet <ref type=""bibr"" target=""#b37"">[38]</ref>. More results are provided in the supplementary material. For SRGAN, we re-implemented their method and fine Each pair consists of an image of the proposed SFT-GAN and the counterpart generated by SRGAN <ref type=""bibr"" target=""#b26"">[27]</ref> or EnhanceNet <ref type=""bibr"" target=""#b37"">[38]</ref>. The users were asked to pick the image with more natural and realistic textures. This session involved 96 r ng confusion within the users. In the second session, our method is ranked higher than SRGAN <ref type=""bibr"" target=""#b26"">[27]</ref> and EnhanceNet <ref type=""bibr"" target=""#b37"">[38]</ref>, especially in building, animal, and grass categories. Comparable performance is found on sky and plant cate e only present its . First row: the results of user studies, comparing our method with SRGAN <ref type=""bibr"" target=""#b26"">[27]</ref> and EnhanceNet <ref type=""bibr"" target=""#b37"">[38]</ref>. Second row: our methods produce visual results that are ranked higher in all categories in comparison with bel><figDesc>Figure 7. First row: the results of user studies, comparing our method with SRGAN<ref type=""bibr"" target=""#b26"">[27]</ref> and EnhanceNet<ref type=""bibr"" target=""#b37"">[38]</ref>. Second row: our methods produce visual results that are ranked higher in all categories in comparison with al loss <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b2"">3]</ref> and adversarial loss <ref type=""bibr"" target=""#b26"">[27,</ref><ref type=""bibr"" target=""#b37"">38]</ref> are introduced to solve the regression-to-the-mean problem that is usually caused by conventional MSE-oriente org/ns/1.0""><head n=""3.2."">Architecture</head><p>Our framework is based on adversarial learning, inspired by <ref type=""bibr"" target=""#b26"">[27,</ref><ref type=""bibr"" target=""#b37"">38]</ref>. Specifically, it consists of one generator G θ and one discriminator D η , parametrized by θ and η respectiv div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.3."">Loss Function</head><p>We draw inspiration from <ref type=""bibr"" target=""#b26"">[27,</ref><ref type=""bibr"" target=""#b37"">38]</ref> and apply perceptual loss and adversarial loss in our model. The perceptual loss measures the distance in a f of SFT-GAN in generating realistic and visually pleasing textures, outperforming previous GAN-based methods <ref type=""bibr"" target=""#b26"">[27,</ref><ref type=""bibr"" target=""#b37"">38]</ref>.</p><p>Our work currently focuses on SR of outdoor scenes. Despite robust to out-of-category images, it does",1
"ol perceptual factors in neural style transfer. <ref type=""bibr"" target=""#b36"">[37]</ref> uses semantic segmentation for video deblurring. Zhu et al. <ref type=""bibr"" target=""#b51"">[52]</ref> propose an approach to generate new clothing on a wearer. It first generates human segmentation maps and the network. This is equivalent to adding SFT conditional bias at the input layer.</p><p>2) Compositional mapping -This method is identical to Zhu et al. <ref type=""bibr"" target=""#b51"">[52]</ref>. It decomposes an LR image based on the predicted semantic classes and processes each region separately usin",1
"the inherent class can be better achieved by selecting the correct class-dedicated model. This phenomenon is previously documented by Timofte et al. <ref type=""bibr"" target=""#b46"">[47]</ref>. They train specialized models for each semantic category on exemplar-based methods <ref type=""bibr"" target= pe=""bibr"" target=""#b41"">[42]</ref> propose context-constrained super-resolution by learning from texturally similar training segments. Timofte et al. <ref type=""bibr"" target=""#b46"">[47]</ref> investigate semantic priors by training specialized models separately for each semantic category on exemplar",0
"br"" target=""#b40"">41]</ref>, statistics <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b0"">1]</ref> and internal patch recurrence <ref type=""bibr"" target=""#b15"">[16]</ref> are employed to improve performance. Dong et al. <ref type=""bibr"" target=""#b9"">[10]</ref> train domain speci",0
"image with rich semantic regions can be achieved with just a single forward pass through transforming the intermediate features of a single network. <ref type=""bibr"" target=""#b1"">(2)</ref> SFT layers can be easily introduced to existing SR network structures. The layers can be trained end-to-end to y facilitates the generation of images with more realistic textures.</p><p>We did not conduct our main evaluation on standard benchmarks such as Set5 <ref type=""bibr"" target=""#b1"">[2]</ref>, Set14 <ref type=""bibr"" target=""#b49"">[50]</ref> and BSD100 <ref type=""bibr"" target=""#b32"">[33]</ref> since th",0
"they are not pre-processed. Segmentation probability maps were generated by the segmentation network (Sec. 3.1).</p><p>For optimization, we used Adam <ref type=""bibr"" target=""#b24"">[25]</ref> with β 1 = 0.9. The learning rate was set to 1 × 10 −4 and then decayed by a factor of 2 every 100k iteratio",0
"e=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b42"">43,</ref><ref type=""bibr"" target=""#b43"">44]</ref> constrain the solution space Laplacian pyramid structure <ref type=""bibr"" target=""#b25"">[26]</ref>, residual blocks <ref type=""bibr"" target=""#b26"">[27]</ref>, recursive learning <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b42"">43]</ref>, and densely connected network <ref type=""bibr"" target=""#b43"">[44]</",0
"deep convolutional neural networks <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" target=""#b22"" images in an end-to-end manner. Later on, the field has witnessed a variety of network architectures, such as a deeper network with residual learning <ref type=""bibr"" target=""#b21"">[22]</ref>, Laplacian pyramid structure <ref type=""bibr"" target=""#b25"">[26]</ref>, residual blocks <ref type=""bibr"" tar </ref> shows the qualitative results of different models including PSNR-oriented methods, such as SRCNN <ref type=""bibr"" target=""#b6"">[7]</ref>, VDSR <ref type=""bibr"" target=""#b21"">[22]</ref>, LapSRN <ref type=""bibr"" target=""#b25"">[26]</ref>, DRRN <ref type=""bibr"" target=""#b42"">[43]</ref>, MemNet <r",0
"e-wise affine transformation in BN. Some variants of CN have proven highly effective in image style transfer <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b14"">15]</ref>, visual question answering <ref type=""bibr"" target=""#b5"">[6]</ref> an",0
"variants of CN have proven highly effective in image style transfer <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b14"">15]</ref>, visual question answering <ref type=""bibr"" target=""#b5"">[6]</ref> and visual reasoning <ref type=""bibr"" targ",0
"el can be eliminated while maintaining the high prediction accuracy. Similar attention mechanisms have been proposed for natural image classification <ref type=""bibr"" target=""#b10"">[11]</ref> and captioning <ref type=""bibr"" target=""#b0"">[1]</ref> to perform adaptive feature pooling, where model pred ular, in the context of image segmentation. The contributions of this work can be summarised as follows: • We take the attention approach proposed in <ref type=""bibr"" target=""#b10"">[11]</ref> a step further by proposing grid-based gating that allows attention coefficients to be more specific to loca and g are linearly mapped to a R Fint dimensional intermediate space. In image captioning <ref type=""bibr"" target=""#b0"">[1]</ref> and classification <ref type=""bibr"" target=""#b10"">[11]</ref> tasks, the  softmax activation function is used to normalise the attention coefficients (σ 2 ); however, seq his reason, we choose a sigmoid activation function. This results experimentally in better training convergence for the AG parameters. In contrast to <ref type=""bibr"" target=""#b10"">[11]</ref> we propose a grid-attention technique. In this case, gating signal is not a global single vector for all ima The corresponding linear transformations decouple the feature-maps and map them to lower dimensional space for the gating operation. As suggested in <ref type=""bibr"" target=""#b10"">[11]</ref>, low-level feature-maps, i.e. the first skip connections, are not used in the gating function since they do bibr"" target=""#b0"">[1]</ref>, machine translation <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b29"">30]</ref>, and classification <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b30"">31,</ref><ref type=""bibr"" target=""#b31"">32]</ref> tasks. Initial work has expl entence translation <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b28"">29]</ref> and more recently applied to image classification <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b31"">32]</ref>. In <ref type=""bibr"" target=""#b9"">[10]</ref>, channel-wise attention d to highlight important feature dimensions, which was the top-performer in the ILSVRC 2017 image classification challenge. Self-attention techniques <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b32"">33]</ref> have been proposed to remove the dependency on external gating infor ating information. For instance, non-local self attention is used in <ref type=""bibr"" target=""#b32"">[33]</ref> to capture long range dependencies. In <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b31"">32]</ref> self-attention is used to perform class-specific pooling, which resu",1
"abdominal CT used statistical shape models <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b27"">28]</ref> or multi-atlas techniques <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b33"">34]</ref>. In particular, atlas approaches benefit from implicit shape constra the TCIA dataset <ref type=""bibr"" target=""#b24"">[25]</ref>, Dice similarity coefficients (DSC) for atlas-based frameworks ranges from 69.6% to 73.9% <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b33"">34]</ref>. In <ref type=""bibr"" target=""#b38"">[39]</ref> a classification based type=""bibr"" target=""#b35"">[36]</ref> and external organ localisation models in image segmentation frameworks <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b26"">27]</ref>.</p><p>• An extension to the",1
"automated medical image analysis tasks including cardiac MR segmentation <ref type=""bibr"" target=""#b2"">[3]</ref> and cancerous lung nodule detection <ref type=""bibr"" target=""#b16"">[17]</ref>. High representation power, fast inference, and filter sharing properties have made CNNs the de facto standa [23]</ref>, abdominal CT <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b26"">27]</ref> segmentation, and lung CT nodule detection <ref type=""bibr"" target=""#b16"">[17]</ref>. However, this approach leads to excessive and redundant use of computational resources and model parameters ral networks (CNNs) outperform traditional approaches in medical image analysis on public benchmark datasets <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b16"">17]</ref> while being an order of magnitude faster than, e.g., graph-cut and multi-atlas segmentation techniques <ref t",0
"ral image analysis, knowledge graphs, and language processing (NLP) for image captioning <ref type=""bibr"" target=""#b0"">[1]</ref>, machine translation <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b29"">30]</ref>, and classification <ref type=""bibr"" target=""#b10"">[11,</ref><ref type es standard back-propagation without need for Monte Carlo sampling. For instance, additive soft attention is used in sentence-to-sentence translation <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b28"">29]</ref> and more recently applied to image classification <ref type=""bibr"" tar ature responses as suggested in <ref type=""bibr"" target=""#b31"">[32]</ref>, which uses AGs for natural image classification. We use additive attention <ref type=""bibr"" target=""#b1"">[2]</ref> to obtain the gating coefficient. Although this is computationally more expensive, it has experimentally shown",0
"ref type=""bibr"" target=""#b13"">[14]</ref>, cardiac CT <ref type=""bibr"" target=""#b22"">[23]</ref>, abdominal CT <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b26"">27]</ref> segmentation, and lung CT nodule detection <ref type=""bibr"" target=""#b16"">[17]</ref>. However, this approach mage segmentation frameworks <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b26"">27]</ref>.</p><p>• An extension to the standard U-Net model is proposed to improve model sensitivity to foreground pixe type=""bibr"" target=""#b2"">[3]</ref>, brain tumours <ref type=""bibr"" target=""#b11"">[12]</ref> and abdominal CT <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b26"">27]</ref> image segmentation tasks.</p><p>Convolutional layers progressively extract higher dimensional image represent y. In order to improve the accuracy, current segmentation frameworks <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b26"">27]</ref> rely on additional preceding object localisation models to simplify the task into separate localisation and s proposed to remove the dependency of atlas to image registration. Recently, cascaded multi-stage CNN models <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" target=""#b37"">38]</ref> have been proposed to address the problem. Here, an initial coarse-le d spleen boundaries were semi-automatically delineated by three trained researchers and manually verified by a clinician. The same dataset is used in <ref type=""bibr"" target=""#b26"">[27]</ref> to benchmark the U-Net model in pancreas segmentation. (II) The second dataset<ref type=""foot"" target=""#foot rks are evaluated on the same public benchmark (CT -82) using different number of training and testing images. Similarly, the FCN approach proposed in<ref type=""bibr"" target=""#b26"">[27]</ref> is benchmarked on CT -150 although it is trained on an external dataset (Ext). performance compared to conca",0
"w tissue contrast and large variability in organ shape and size. We evaluate our implementation on two commonly used benchmarks: TCIA Pancreas CT -82 <ref type=""bibr"" target=""#b24"">[25]</ref> and multi-class abdominal CT -150. The results show that AGs consistenly improve prediction accuracy across roaches benefit from implicit shape constraints enforced by propagation of manual annotations. However, in public benchmarks such as the TCIA dataset <ref type=""bibr"" target=""#b24"">[25]</ref>, Dice similarity coefficients (DSC) for atlas-based frameworks ranges from 69.6% to 73.9% <ref type=""bibr"" t t_2"">1</ref> (CT -82) consists of 82 contrast enhanced 3D CT scans with pancreas manual annotations performed slice-by-slice. This dataset (NIH-TCIA) <ref type=""bibr"" target=""#b24"">[25]</ref> is publicly available and commonly used to benchmark CT pancreas segmentation frameworks. The images from bo 1.0"" type=""table"" xml:id=""tab_2""><head>Table 3 :</head><label>3</label><figDesc>Pancreas segmentation results obtained on the TCIA Pancreas-CT Dataset<ref type=""bibr"" target=""#b24"">[25]</ref>. The dataset contains in total 82 scans which are split into training (61) and testing<ref type=""bibr"" targe",0
"e made CNNs the de facto standard for image segmentation. Fully convolutional networks (FCNs) <ref type=""bibr"" target=""#b17"">[18]</ref> and the U-Net <ref type=""bibr"" target=""#b23"">[24]</ref> are two commonly used architectures. Despite their good representational power, these architectures rely on uctural information in medical images well. In particular, fully convolutional networks (FCN) <ref type=""bibr"" target=""#b17"">[18]</ref> such as U-Net <ref type=""bibr"" target=""#b23"">[24]</ref>, DeepMedic <ref type=""bibr"" target=""#b12"">[13]</ref> and holistically nested networks <ref type=""bibr"" targe",0
"power, fast inference, and filter sharing properties have made CNNs the de facto standard for image segmentation. Fully convolutional networks (FCNs) <ref type=""bibr"" target=""#b17"">[18]</ref> and the U-Net <ref type=""bibr"" target=""#b23"">[24]</ref> are two commonly used architectures. Despite their g , and (III) image convolution operations exploit the structural information in medical images well. In particular, fully convolutional networks (FCN) <ref type=""bibr"" target=""#b17"">[18]</ref> such as U-Net <ref type=""bibr"" target=""#b23"">[24]</ref>, DeepMedic <ref type=""bibr"" target=""#b12"">[13]</ref>",0
"2,</ref><ref type=""bibr"" target=""#b29"">30]</ref>, and classification <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b30"">31,</ref><ref type=""bibr"" target=""#b31"">32]</ref> tasks. Initial work has explored attention-maps by interpreting gradient of output class scores with respect et=""#b1"">[2,</ref><ref type=""bibr"" target=""#b28"">29]</ref> and more recently applied to image classification <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b31"">32]</ref>. In <ref type=""bibr"" target=""#b9"">[10]</ref>, channel-wise attention is used to highlight important feature d l self attention is used in <ref type=""bibr"" target=""#b32"">[33]</ref> to capture long range dependencies. In <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b31"">32]</ref> self-attention is used to perform class-specific pooling, which results in more accurate and robust image cla for each pixel i to determine focus regions. The gating vector contains contextual information to prune lower-level feature responses as suggested in <ref type=""bibr"" target=""#b31"">[32]</ref>, which uses AGs for natural image classification. We use additive attention <ref type=""bibr"" target=""#b1"">[2",0
"ular, fully convolutional networks (FCN) <ref type=""bibr"" target=""#b17"">[18]</ref> such as U-Net <ref type=""bibr"" target=""#b23"">[24]</ref>, DeepMedic <ref type=""bibr"" target=""#b12"">[13]</ref> and holistically nested networks <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b34"">35]",0
"ad><p>YOLOv3 predicts boxes at 3 different scales. Our system extracts features from those scales using a similar concept to feature pyramid networks <ref type=""bibr"" target=""#b7"">[8]</ref>. From our base feature extractor we add several convolutional layers. The last of these predicts a 3-d tensor",1
"s/1.0""><head n=""2.1."">Bounding Box Prediction</head><p>Following YOLO9000 our system predicts bounding boxes using dimension clusters as anchor boxes <ref type=""bibr"" target=""#b14"">[15]</ref>. The network predicts 4 coordinates for each bounding box, t x , t y , t w , t h . If the cell is offset fro he center coordinates of the box relative to the location of filter application using a sigmoid function. This figure blatantly self-plagiarized from <ref type=""bibr"" target=""#b14"">[15]</ref>.</p><p>is not the best but does overlap a ground truth object by more than some threshold we ignore the pred the center coordinates of the box relative to the location of filter application using a sigmoid function. This figure blatantly self-plagiarized from<ref type=""bibr"" target=""#b14"">[15]</ref>.</figDesc></figure> <figure xmlns=""http://www.tei-c.org/ns/1.0"" xml:id=""fig_2""><head>Figure 4 .</head><label",1
"a whole lot of research this year. Spent a lot of time on Twitter. Played around with GANs a little. I had a little momentum left over from last year <ref type=""bibr"" target=""#b11"">[12]</ref>  <ref type=""bibr"" target=""#b0"">[1]</ref>; I managed to make some improvements to YOLO. But, honestly, nothin",0
"lot of time on Twitter. Played around with GANs a little. I had a little momentum left over from last year <ref type=""bibr"" target=""#b11"">[12]</ref>  <ref type=""bibr"" target=""#b0"">[1]</ref>; I managed to make some improvements to YOLO. But, honestly, nothing like super interesting, just a bunch of s ell>35.4</cell><cell>41.9</cell></row></table></figure> 		</body> 		<back>  			<div type=""acknowledgement""> <div xmlns=""http://www.tei-c.org/ns/1.0""> <ref type=""bibr"" target=""#b0"">1</ref> <p>The author is funded by the Office of Naval Research and Google.</p></div> 			</div>  			<div type=""reference",0
"arget=""#b14"">[15]</ref>.</p><p>is not the best but does overlap a ground truth object by more than some threshold we ignore the prediction, following <ref type=""bibr"" target=""#b17"">[17]</ref>. We use the threshold of .5. Unlike <ref type=""bibr"" target=""#b17"">[17]</ref> our system only assigns one bo object by more than some threshold we ignore the prediction, following <ref type=""bibr"" target=""#b17"">[17]</ref>. We use the threshold of .5. Unlike <ref type=""bibr"" target=""#b17"">[17]</ref> our system only assigns one bounding box prior for each ground truth object. If a bounding box prior is not",0
"of justification. For PASCAL VOC, the IOU threshold was ""set deliberately low to account for inaccuracies in bounding boxes in the ground truth data"" <ref type=""bibr"" target=""#b1"">[2]</ref>. Does COCO have better labelling than VOC? This is definitely possible since COCO has segmentation masks maybe",0
"modeling tasks. The sequence modeling chapter in the canonical textbook on deep learning is titled ""Sequence Modeling: Recurrent and Recursive Nets"" <ref type=""bibr"" target=""#b22"">(Goodfellow et al., 2016)</ref>, capturing the common association of sequence modeling and recurrent architectures. A w actively explored <ref type=""bibr"" target=""#b18"">(El Hihi &amp; Bengio, 1995;</ref><ref type=""bibr"" target=""#b60"">Schuster &amp; Paliwal, 1997;</ref><ref type=""bibr"" target=""#b22"">Gers et al., 2002;</ref><ref type=""bibr"" target=""#b40"">Koutnik et al., 2014;</ref><ref type=""bibr"" target=""#b43"">Le et",0
"g tasks. This is particularly notable because the tasks include diverse benchmarks that have commonly been used to evaluate recurrent network designs <ref type=""bibr"" target=""#b11"">(Chung et al., 2014;</ref><ref type=""bibr"" target=""#b57"">Pascanu et al., 2014;</ref><ref type=""bibr"" target=""#b35"">Joze and is much larger than JSB Chorales. JSB Chorales and Nottingham have been used in numerous empirical investigations of recurrent sequence modeling <ref type=""bibr"" target=""#b11"">(Chung et al., 2014;</ref><ref type=""bibr"" target=""#b57"">Pascanu et al., 2014;</ref><ref type=""bibr"" target=""#b35"">Joze s of different recurrent architectures. These studies have been motivated in part by the many degrees of freedom in the design of such architectures. <ref type=""bibr"" target=""#b11"">Chung et al. (2014)</ref> compared different types of recurrent units (LSTM vs. GRU) on the task of polyphonic music mo sed to benchmark the performance of different RNN sequence modeling architectures <ref type=""bibr"" target=""#b29"">(Hermans &amp; Schrauwen, 2013;</ref><ref type=""bibr"" target=""#b11"">Chung et al., 2014;</ref><ref type=""bibr"" target=""#b57"">Pascanu et al., 2014;</ref><ref type=""bibr"" target=""#b43"">Le et",0
"</ref>. More recently, convolutional networks were applied to sentence classification <ref type=""bibr"" target=""#b36"">(Kalchbrenner et al., 2014;</ref><ref type=""bibr"" target=""#b38"">Kim, 2014)</ref> and document classification <ref type=""bibr"" target=""#b75"">(Zhang et al., 2015;</ref><ref type=""bibr""",0
"ref>, convolution <ref type=""bibr"" target=""#b3"">(Gehring et al., 2017;</ref><ref type=""bibr"" target=""#b4"">Kalchbrenner et al., 2016)</ref>, attention <ref type=""bibr"" target=""#b10"">(Vaswani et al., 2017)</ref>, or a combination of recurrence and attention <ref type=""bibr"" target=""#b1"">(Bahdanau et a www.tei-c.org/ns/1.0""><head n=""2"">Background</head></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2.1"">Transformer</head><p>The Transformer <ref type=""bibr"" target=""#b10"">(Vaswani et al., 2017)</ref> employs an encoder-decoder structure, consisting of stacked encoder and decoder layers. En he Transformer computes self-attention efficiently for all sequences, heads, and positions in a batch using parallel matrix multiplication operations <ref type=""bibr"" target=""#b10"">(Vaswani et al., 2017)</ref>. Without relative position representations, each e ij can be computed using bh parallel mu org/ns/1.0""><head n=""4.2"">Machine Translation</head><p>We compared our model using only relative position representations to the baseline Transformer <ref type=""bibr"" target=""#b10"">(Vaswani et al., 2017)</ref> with sinusoidal position encodings. We generated baseline results to isolate the impact of ation to the model. These position encodings can be a deterministic function of position <ref type=""bibr"" target=""#b7"">(Sukhbaatar et al., 2015;</ref><ref type=""bibr"" target=""#b10"">Vaswani et al., 2017)</ref> or learned representations. Convolutional neural networks inherently capture relative posit ion experiments, the result was a modest 7% decrease in steps per second, but we were able to maintain the same model and batch sizes on P100 GPUs as <ref type=""bibr"" target=""#b10"">Vaswani et al. (2017)</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4"">Experiments</head></div> <d r"" target=""#b5"">(Kingma and Ba, 2014)</ref> with β 1 = 0.9, β 2 = 0.98, and = 10 −9 . We used the same warmup and decay strategy for learning rate as <ref type=""bibr"" target=""#b10"">Vaswani et al. (2017)</ref>, with 4,000 warmup steps. During training, we employed label smoothing of value ls = 0.1 <r",1
"mlns=""http://www.tei-c.org/ns/1.0""><head n=""1"">Introduction</head><p>Recent approaches to sequence to sequence learning typically leverage recurrence <ref type=""bibr"" target=""#b8"">(Sutskever et al., 2014)</ref>, convolution <ref type=""bibr"" target=""#b3"">(Gehring et al., 2017;</ref><ref type=""bibr"" t",0
"Kalchbrenner et al., 2016)</ref>, attention <ref type=""bibr"" target=""#b10"">(Vaswani et al., 2017)</ref>, or a combination of recurrence and attention <ref type=""bibr"" target=""#b1"">(Bahdanau et al., 2014;</ref><ref type=""bibr"" target=""#b2"">Cho et al., 2014;</ref><ref type=""bibr"" target=""#b5"">Luong et",0
"attention, followed by a position-wise feed-forward layer. It uses residual connections around each of the sublayers, followed by layer normalization <ref type=""bibr"" target=""#b0"">(Ba et al., 2016)</ref>. The decoder uses masking in its selfattention to prevent a given output position from incorpora",0
""" target=""#b10"">(Vaswani et al., 2017)</ref>, or a combination of recurrence and attention <ref type=""bibr"" target=""#b1"">(Bahdanau et al., 2014;</ref><ref type=""bibr"" target=""#b2"">Cho et al., 2014;</ref><ref type=""bibr"" target=""#b5"">Luong et al., 2015;</ref><ref type=""bibr"" target=""#b12"">Wu et al.,",0
"with our onestep feedback setting. Finally, there has been much work in the social sciences on analyzing the effect of affirmative action (see e.g., <ref type=""bibr"" target=""#b12"">Keith et al., 1985;</ref><ref type=""bibr"" target=""#b11"">Kalev et al., 2006)</ref>.</p></div> <div xmlns=""http://www.tei",0
"is complementary to this type of domain specific approach. Demographic parity and related formulations have been considered in numerous papers (e.g. <ref type=""bibr"" target=""#b1"">Calders et al., 2009;</ref><ref type=""bibr"" target=""#b18"">Zafar et al., 2017)</ref>. <ref type=""bibr"" target=""#b8"">Hardt",0
"r"" target=""#b8"">Hardt et al. (2016)</ref> introduced the equality of opportunity constraint and demonstrate limitations of a broad class of criteria. <ref type=""bibr"" target=""#b13"">Kleinberg et al. (2017)</ref> and Chouldechova (2016) point out the tension between ""calibration by group"" and equal tr",0
"onsidered in numerous papers (e.g. <ref type=""bibr"" target=""#b1"">Calders et al., 2009;</ref><ref type=""bibr"" target=""#b18"">Zafar et al., 2017)</ref>. <ref type=""bibr"" target=""#b8"">Hardt et al. (2016)</ref> introduced the equality of opportunity constraint and demonstrate limitations of a broad class",0
"een much work in the social sciences on analyzing the effect of affirmative action (see e.g., <ref type=""bibr"" target=""#b12"">Keith et al., 1985;</ref><ref type=""bibr"" target=""#b11"">Kalev et al., 2006)</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2."">Problem Setting</head><p>We",0
"head n=""1"">INTRODUCTION</head><p>Many crucial machine learning tasks involve graph structured datasets, such as classifying posts in a social network <ref type=""bibr"" target=""#b10"">(Hamilton et al., 2017a)</ref>, predicting interfaces between proteins <ref type=""bibr"" target=""#b8"">(Fout et al., 2017 r-sharing operator on a set of neighboring nodes to aggregate a local set of lower-level features. We refer to such an operator as a graph aggregator <ref type=""bibr"" target=""#b10"">(Hamilton et al., 2017a)</ref> and the set of local nodes as the receptive field of the aggregator. Then, by stacking m ibr"" target=""#b6"">(Defferrard et al., 2016;</ref><ref type=""bibr"" target=""#b15"">Kipf and Welling, 2017</ref>) can be interpreted as graph aggregators <ref type=""bibr"" target=""#b10"">(Hamilton et al., 2017a)</ref>.</p><p>Graph aggregators are the basic building blocks of graph convolutional neural net effectiveness of our new aggregator by applying it to the inductive node classification problem. We also improve the sampling strategy introduced in <ref type=""bibr"" target=""#b10"">(Hamilton et al., 2017a)</ref> to reduce the memory cost and increase the run-time efficiency, in order to train our mo GGRU), which is directly applicable for spatiotemporal forecasting problem. Extensive experiments on two node classification datasets, PPI and Reddit <ref type=""bibr"" target=""#b10"">(Hamilton et al., 2017a)</ref>, and one traffic speed forecasting dataset, METR-LA <ref type=""bibr"" target=""#b18"">(Li e challenging because the memory complexity is proportional to the total number of nodes, which could be hundreds of thousands of nodes in large graphs <ref type=""bibr"" target=""#b10"">(Hamilton et al., 2017a)</ref>. To reduce memory usage and computational cost, <ref type=""bibr"" target=""#b10"">(Hamilton s of thousands of nodes in large graphs <ref type=""bibr"" target=""#b10"">(Hamilton et al., 2017a)</ref>. To reduce memory usage and computational cost, <ref type=""bibr"" target=""#b10"">(Hamilton et al., 2017a)</ref> proposed the GraphSAGE framework that uses a sampling algorithm to select a small subset , the validation and testing nodes are not observable and the goal is to predict the labels of the unseen testing nodes. Our approach follows that of <ref type=""bibr"" target=""#b10"">(Hamilton et al., 2017a)</ref>, where a mini-batch of nodes are sampled on each iteration during training and multiple state-of-the-art models, five aggregator-based models in our framework and a two-layer fully connected neural network on the PPI and Reddit datasets <ref type=""bibr"" target=""#b10"">(Hamilton et al., 2017a)</ref>. The five baseline aggregators include the multihead attention aggregator, two pooling b (100, 40) and (200, 80)</ref>.</p><p>To illustrate the effectiveness of incorporating graph structures, we also evaluate a two-layer fully-connected  <ref type=""bibr"" target=""#b10"">(Hamilton et al., 2017a)</ref> (61.2)<ref type=""foot"" target=""#foot_0"">1</ref> 95.4 GAT <ref type=""bibr"" target=""#b27""> s, we use the validation set to select the optimal hyperparameters for training. The training, validation, and testing splits are the same as that in <ref type=""bibr"" target=""#b10"">(Hamilton et al., 2017a)</ref>. The micro-averaged F1 score is used to evaluate the prediction accuracy for both datase ""5.4"">MAIN RESULTS</head><p>We compare our model with the previous state-of-the-art methods on inductive node classification. This includes GraphSAGE <ref type=""bibr"" target=""#b10"">(Hamilton et al., 2017a)</ref>, GAT <ref type=""bibr"" target=""#b27"">(Veličković et al., 2018)</ref>, and FastGCN <ref ty the better the prediction results.</p><p>Also, we can see steady improvement with larger sampling sizes, which is consistent with the observation in <ref type=""bibr"" target=""#b10"">(Hamilton et al., 2017a)</ref>.</p><p>Effect of output dimensions in the PPI dataset We changed the output dimension to #b0"">Atwood and Towsley, 2016;</ref><ref type=""bibr"" target=""#b15"">Kipf and Welling, 2017;</ref><ref type=""bibr"" target=""#b8"">Fout et al., 2017;</ref><ref type=""bibr"" target=""#b10"">Hamilton et al., 2017a;</ref><ref type=""bibr"" target=""#b27"">Veličković et al., 2018;</ref><ref type=""bibr"" target=""#b18 regators. Most existing graph aggregators are based on either pooling over neighborhoods <ref type=""bibr"" target=""#b15"">(Kipf and Welling, 2017;</ref><ref type=""bibr"" target=""#b10"">Hamilton et al., 2017a)</ref> or computing a weighted sum of the neighboring features <ref type=""bibr"" target=""#b21"">(M node features of neighborhoods <ref type=""bibr"" target=""#b7"">(Duvenaud et al., 2015;</ref><ref type=""bibr"" target=""#b15"">Kipf and Welling, 2017;</ref><ref type=""bibr"" target=""#b10"">Hamilton et al., 2017a)</ref>, while others integrated edge features as well <ref type=""bibr"" target=""#b0"">(Atwood and",1
"""#b15"">Kipf and Welling, 2017;</ref><ref type=""bibr"" target=""#b8"">Fout et al., 2017;</ref><ref type=""bibr"" target=""#b10"">Hamilton et al., 2017a;</ref><ref type=""bibr"" target=""#b27"">Veličković et al., 2018;</ref><ref type=""bibr"" target=""#b18"">Li et al., 2018)</ref>, which generalizes the stan- * Thes ><ref type=""bibr"" target=""#b26"">Vaswani et al., 2017)</ref>. It has later been adopted as a graph aggregator to solve the node classification problem <ref type=""bibr"" target=""#b27"">(Veličković et al., 2018)</ref>. A single attention head sums the elements that are similar to the query vector in one e a two-layer fully-connected  <ref type=""bibr"" target=""#b10"">(Hamilton et al., 2017a)</ref> (61.2)<ref type=""foot"" target=""#foot_0"">1</ref> 95.4 GAT <ref type=""bibr"" target=""#b27"">(Veličković et al., 2018)</ref> 97.3 ± 0.2 -Fast GCN <ref type=""bibr"">(Chen et</ref>  We train all the aggregator-based state-of-the-art methods on inductive node classification. This includes GraphSAGE <ref type=""bibr"" target=""#b10"">(Hamilton et al., 2017a)</ref>, GAT <ref type=""bibr"" target=""#b27"">(Veličković et al., 2018)</ref>, and FastGCN <ref type=""bibr"" target=""#b3"">(Chen et al., 2018)</ref>. The GraphSAGE mod ly-connected layer parameterized by θ o to get the final output y i , which has dimension d o . The difference between our aggregator and that in GAT <ref type=""bibr"" target=""#b27"">(Veličković et al., 2018</ref>) is that we have adopted the key-value attention mechanism and the dot product attention",1
"hanism is widely adopted in deep learning literature and many variants have been proposed <ref type=""bibr"" target=""#b4"">(Chorowski et al., 2014;</ref><ref type=""bibr"" target=""#b29"">Xu et al., 2015b;</ref><ref type=""bibr"">Seo et al., 2017;</ref><ref type=""bibr"" target=""#b26"">Vaswani et al., 2017)</re",0
"ined end-to-end to extract the local and global features across the graph. Note that we use the spatial definition instead of the spectral definition <ref type=""bibr"" target=""#b12"">(Hammond et al., 2011;</ref><ref type=""bibr"" target=""#b2"">Bruna et al., 2014)</ref> of graph convolution because the fu",0
"/p><p>Recent research, however, has pivoted to solving these problems by graph convolution <ref type=""bibr"" target=""#b7"">(Duvenaud et al., 2015;</ref><ref type=""bibr"" target=""#b0"">Atwood and Towsley, 2016;</ref><ref type=""bibr"" target=""#b15"">Kipf and Welling, 2017;</ref><ref type=""bibr"" target=""#b8"" ph. DCRNN replaces the fully-connected layers in GRU <ref type=""bibr"" target=""#b5"">(Chung et al., 2014)</ref> with the diffusion convolution operator <ref type=""bibr"" target=""#b0"">(Atwood and Towsley, 2016)</ref>. Furthermore, DCRNN takes the direction of graph edges into account. The difference bet arget=""#b15"">Kipf and Welling, 2017;</ref><ref type=""bibr"" target=""#b10"">Hamilton et al., 2017a)</ref>, while others integrated edge features as well <ref type=""bibr"" target=""#b0"">(Atwood and Towsley, 2016;</ref><ref type=""bibr"" target=""#b8"">Fout et al., 2017;</ref><ref type=""bibr"" target=""#b22"">Sch",0
"ture traffic speeds in a sensor network given historic traffic speeds and the underlying road graph. DCRNN replaces the fully-connected layers in GRU <ref type=""bibr"" target=""#b5"">(Chung et al., 2014)</ref> with the diffusion convolution operator <ref type=""bibr"" target=""#b0"">(Atwood and Towsley, 20",0
"wo authors contributed equally. dard definition of convolution over a regular grid topology <ref type=""bibr"" target=""#b9"">(Gehring et al., 2017;</ref><ref type=""bibr"" target=""#b16"">Krizhevsky et al., 2012)</ref> to 'convolution' over graph structures. The basic idea behind 'graph convolution' is to",0
"n 4.3.</p><p>Neural attention mechanism Neural attention mechanism is widely adopted in deep learning literature and many variants have been proposed <ref type=""bibr"" target=""#b4"">(Chorowski et al., 2014;</ref><ref type=""bibr"" target=""#b29"">Xu et al., 2015b;</ref><ref type=""bibr"">Seo et al., 2017;</",0
"rget=""#b24"">(Seo et al., 2016)</ref> proposed Graph Convolutional Recurrent Neural Network (GCRNN), which replaced the fully-connected layers in LSTM <ref type=""bibr"" target=""#b13"">(Hochreiter and Schmidhuber, 1997)</ref> with the ChebNet operator <ref type=""bibr"" target=""#b6"">(Defferrard et al., 20",0
"icle describes our experiments in neural machine translation using the recent Ten-sor2Tensor framework and the Transformer sequence-to-sequence model <ref type=""bibr"" target=""#b22"">(Vaswani et al., 2017)</ref>. We examine some of the critical parameters that affect the final translation quality, mem <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""6."">Conclusion</head><p>We presented a broad range of basic experiments with the Transformer model <ref type=""bibr"" target=""#b22"">(Vaswani et al., 2017)</ref> for English-to-Czech neural machine translation. While we limit our exploration to the mor t al. (2015)</ref>, sometimes the exact number of training steps is given but no indication on ""how much converged"" the model was at that point, e.g. <ref type=""bibr"" target=""#b22"">Vaswani et al. (2017)</ref>. Most probably, the training was run until no further improvements were clearly apparent on lso the learning rate or learning rate schedule (or faking the global_step stored in the checkpoint) to make sure the learning rate is not too small. <ref type=""bibr"" target=""#b22"">Vaswani et al. (2017)</ref> suggest to average the last 20 checkpoints saved in 10-minute intervals (using utils/avg_ch",1
"ults in practice <ref type=""bibr"" target=""#b8"">(Goyal et al., 2017;</ref><ref type=""bibr"">Smith et al., 2017)</ref> and also theoretical explanations <ref type=""bibr"" target=""#b6"">(Bottou et al., 2016;</ref><ref type=""bibr"">Smith and Le, 2017;</ref><ref type=""bibr"" target=""#b11"">Jastrzebski et al., ing rate and batch size breaks down if the learning rate gets too large or the batch size gets too small"". A similar observation was reported e.g. by <ref type=""bibr"" target=""#b6"">Bottou et al. (2016)</ref>. Thus our initial hypothesis was that 0.20 (or 0.25) is the maximal learning rate suitable fo",0
"al., 2017)</ref> and also theoretical explanations <ref type=""bibr"" target=""#b6"">(Bottou et al., 2016;</ref><ref type=""bibr"">Smith and Le, 2017;</ref><ref type=""bibr"" target=""#b11"">Jastrzebski et al., 2017)</ref>. <ref type=""bibr"">Smith and Le (2017)</ref> interpret SGD (and its variants) as a stoch ver, none of these experiments led to any improvements over the default learning rate -all had about the same BLEU curve after few hours of training. <ref type=""bibr"" target=""#b11"">Jastrzebski et al. (2017)</ref> shows that ""the invariance under simultaneous rescaling of learning rate and batch size",0
"anslation do not specify any stopping criteria whatsoever. Sometimes, they mention only an approximate number of days the model was trained for, e.g. <ref type=""bibr"" target=""#b0"">Bahdanau et al. (2015)</ref>, sometimes the exact number of training steps is given but no indication on ""how much conve",0
"ch leads to ""flat minima"" that generalize well), we need to scale the learning rate linearly when increasing the effective batch size.</p><p>However, <ref type=""bibr"" target=""#b9"">Hoffer et al. (2017)</ref> suggest to use √ k scaling instead of the linear scaling and provide both theoretical and emp he same when scaling to k times bigger effective batch, we actually increase the actual learning rate √ k times, in accordance with the suggestion of <ref type=""bibr"" target=""#b9"">Hoffer et al. (2017)</ref>.<ref type=""foot"" target=""#foot_25"">33</ref> This holds only for the linear_warmup_rsqrt_decay lti-GPU training on a single GPU, simply by doing the update once after N batches (and summing the gradients). This is similar to the ghost batches of<ref type=""bibr"" target=""#b9"">Hoffer et al. (2017)</ref>, but using ghost batch size higher than the actual batch size. We leave this for future work. ase.</note> 			<note xmlns=""http://www.tei-c.org/ns/1.0"" place=""foot"" n=""33"" xml:id=""foot_25"">In addition to suggesting the √ k learning-rate scaling,<ref type=""bibr"" target=""#b9"">Hoffer et al. (2017)</ref> show that to fully close the ""generalization gap"", we need to train longer because the absolu",0
"n tasks tackled with deep learning, neural networks have been achieved dramatic improvement in SR. Dong et al. <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b3"">4]</ref> first exploit a three-layer convolutional neural network, named SRCNN, to jointly optimize the feature extracti with state-of-the-arts</head><p>We compare the proposed method with other SR methods, including bicubic, SRCNN <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b3"">4]</ref>, VDSR <ref type=""bibr"" target=""#b11"">[12]</ref>, DRC-N <ref type=""bibr"" target=""#b12"">[13]</ref>, LapSRN <ref t",1
"""bibr"" target=""#b12"">[13]</ref>. In this way, the feature maps of each layer are sent to the sequential layer without distinction. However, Hu et al. <ref type=""bibr"" target=""#b8"">[9]</ref> experimentally demonstrate that adaptively recalibrating channel-wise features responses can improve the repre s to another module. By this way, the feature maps of the second module naturally become the local long-path features. Different from the approach in <ref type=""bibr"" target=""#b8"">[9]</ref>, we divide feature maps into two parts. One part represents reserved shortpath features and another expresses",1
"s try to train a deep network to gain better reconstruction performance. <ref type=""bibr"">Kim et al.</ref> propose a 20-layer CNN model known as VDSR <ref type=""bibr"" target=""#b11"">[12]</ref>, which adopts residual learning and adaptive gradient clipping to ease training difficulty. To control the m ch as mobile and embedded vision applications. Moreover, the traditional convolutional networks usually adopt cascaded network topologies, e.g., VDSR <ref type=""bibr"" target=""#b11"">[12]</ref> and DRC-N <ref type=""bibr"" target=""#b12"">[13]</ref>. In this way, the feature maps of each layer are sent to ber of convolutional layer.</p><p>• Due to the concise structure of the proposed IDN, it is much faster than several CNN-based SR methods, e.g., VDSR <ref type=""bibr"" target=""#b11"">[12]</ref>, DRCN <ref type=""bibr"" target=""#b12"">[13]</ref>, LapSRN <ref type=""bibr"" target=""#b14"">[15]</ref>, DRRN <ref bibr"" target=""#b4"">[5]</ref> adopt deconvolution to accelerate SRCNN in combination with smaller filter sizes and more convolution layers. Kim et al. <ref type=""bibr"" target=""#b11"">[12]</ref> propose a very deep CNN model with global residual architecture to achieve superior performance, which utili er, assembles the output of the final DBlock to generate the residual image. The bias term of this transposed convolution can auto-Dataset Scale VDSR <ref type=""bibr"" target=""#b11"">[12]</ref> DRCN <ref type=""bibr"" target=""#b12"">[13]</ref> LapSRN <ref type=""bibr"" target=""#b14"">[15]</ref> DRRN <ref ty e proposed method with other SR methods, including bicubic, SRCNN <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b3"">4]</ref>, VDSR <ref type=""bibr"" target=""#b11"">[12]</ref>, DRC-N <ref type=""bibr"" target=""#b12"">[13]</ref>, LapSRN <ref type=""bibr"" target=""#b14"">[15]</ref>, DRRN <re i-c.org/ns/1.0""><head n=""4.1."">Datasets</head></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.1.1"">Training datasets</head><p>By following <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b22",1
"d s of enhancement unit in each block are set to 64, 16 and 4 respectively. To reduce the parameters of network, we use the grouped convolution layer <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b23"">24]</ref> in the second and fourth layers in each enhancement unit with 4 groups",0
"gradient clipping to ease training difficulty. To control the model parameters, the authors construct a deeply-recursive convolutional network (DRCN) <ref type=""bibr"" target=""#b12"">[13]</ref> by adopting recursive layer. To mitigate training difficulty, <ref type=""bibr"">Mao et al.</ref> propose a ve ver, the traditional convolutional networks usually adopt cascaded network topologies, e.g., VDSR <ref type=""bibr"" target=""#b11"">[12]</ref> and DRC-N <ref type=""bibr"" target=""#b12"">[13]</ref>. In this way, the feature maps of each layer are sent to the sequential layer without distinction. However, oncise structure of the proposed IDN, it is much faster than several CNN-based SR methods, e.g., VDSR <ref type=""bibr"" target=""#b11"">[12]</ref>, DRCN <ref type=""bibr"" target=""#b12"">[13]</ref>, LapSRN <ref type=""bibr"" target=""#b14"">[15]</ref>, DRRN <ref type=""bibr"" target=""#b21"">[22]</ref> and MemNet architecture to achieve superior performance, which utilizes contextual information over large image regions. Another network designed by Kim et al. <ref type=""bibr"" target=""#b12"">[13]</ref>, which has recursive convolution with skip connection to avoid introducing additional parameters when the de generate the residual image. The bias term of this transposed convolution can auto-Dataset Scale VDSR <ref type=""bibr"" target=""#b11"">[12]</ref> DRCN <ref type=""bibr"" target=""#b12"">[13]</ref> LapSRN <ref type=""bibr"" target=""#b14"">[15]</ref> DRRN <ref type=""bibr"" target=""#b21"">[22]</ref> MemNet <ref g bicubic, SRCNN <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b3"">4]</ref>, VDSR <ref type=""bibr"" target=""#b11"">[12]</ref>, DRC-N <ref type=""bibr"" target=""#b12"">[13]</ref>, LapSRN <ref type=""bibr"" target=""#b14"">[15]</ref>, DRRN <ref type=""bibr"" target=""#b21"">[22]</ref> and MemNet",0
"""http://www.tei-c.org/ns/1.0""><head n=""4.1.2"">Testing datasets</head><p>The proposed method is evaluated on four widely used benchmark datasets: Set5 <ref type=""bibr"" target=""#b0"">[1]</ref>, Set14 <ref type=""bibr"" target=""#b26"">[27]</ref>, BSD100 <ref type=""bibr"" target=""#b17"">[18]</ref>, Urban100 <",0
"""#b22"">23]</ref>, we use 91 images from Yang et al. <ref type=""bibr"" target=""#b25"">[26]</ref> and 200 images from Berkeley Segmentation Dataset (BSD) <ref type=""bibr"" target=""#b17"">[18]</ref> as the training data. As in <ref type=""bibr"" target=""#b21"">[22]</ref>, to make full use of the training data aluated on four widely used benchmark datasets: Set5 <ref type=""bibr"" target=""#b0"">[1]</ref>, Set14 <ref type=""bibr"" target=""#b26"">[27]</ref>, BSD100 <ref type=""bibr"" target=""#b17"">[18]</ref>, Urban100 <ref type=""bibr"" target=""#b9"">[10]</ref>. Among these datasets, Set5, Set14 and BSD100 consist of",0
"opting recursive layer. To mitigate training difficulty, <ref type=""bibr"">Mao et al.</ref> propose a very deep residual encoder-decoder network (RED) <ref type=""bibr"" target=""#b16"">[17]</ref>, which consists of a series of convolutional and subsequent transposed convolution layers to learn end-to-en >[13]</ref>, which has recursive convolution with skip connection to avoid introducing additional parameters when the depth is increasing. Mao et al. <ref type=""bibr"" target=""#b16"">[17]</ref> tackle the general image restoration problem with encoderdecoder networks and symmetric skip connections. La",0
"SPCN), which extracts feature maps in the LR space and replaces the bicubic upsampling operation with an efficient sub-pixel convolution. Dong et al. <ref type=""bibr"" target=""#b4"">[5]</ref> adopt deconvolution to accelerate SRCNN in combination with smaller filter sizes and more convolution layers.",0
"by using the method proposed in <ref type=""bibr"" target=""#b7"">[8]</ref> and the biases are set to zero. The proposed network is optimized using Adam <ref type=""bibr"" target=""#b13"">[14]</ref>. We set the parameters of mini-batch size and weight decay to 64 and 1e − 4 respectively. In order to get be",0
"pts 17 × 17 filters for all scaling factors and the negative scope of LReLU is set as 0.05. We initialize the weights by using the method proposed in <ref type=""bibr"" target=""#b7"">[8]</ref> and the biases are set to zero. The proposed network is optimized using Adam <ref type=""bibr"" target=""#b13"">[1",0
"ages. The corresponding HR training images are divided into ml sub × ml sub size sub-images. As the proposed model is trained using the Caffe package <ref type=""bibr"" target=""#b10"">[11]</ref>, its transposed convolution filters will generate the output with size (ml sub − m + 1) 2 instead of (ml sub",0
"mage restoration as defined below:</p><formula xml:id=""formula_3"">l M SE = 1 N N i=1 I i − Îi 2 2 .<label>(4)</label></formula><p>However, Lim et al. <ref type=""bibr"" target=""#b15"">[16]</ref> experimentally demonstrate that training with MSE loss is not a good choice. The second loss function is mea",0
"ets. This type of approaches usually focuses on how to learn a compact dictionary or manifold space to relate LR/HR patches, such as nearest neighbor <ref type=""bibr"" target=""#b6"">[7]</ref>, manifold embedding <ref type=""bibr"" target=""#b1"">[2]</ref>, random forest <ref type=""bibr"" target=""#b19"">[20]",0
", the most important choice for dynamic scheduling is load balancing; its use is a complex trade-off between its cost and its benefits. Work stealing <ref type=""bibr"" target=""#b3"">[4]</ref> can be considered the most widely used load balancing technique in task-based runtime systems. The main concep",1
"llsupported parallel libraries based on task parallelism have emerged, such as Intel Cilk Plus <ref type=""bibr"" target=""#b22"">[23]</ref> or Intel TBB <ref type=""bibr"" target=""#b27"">[28]</ref>. There are also runtimes specifically designed to improve shared memory performance of existing language ext",0
"party libraries or implementation-specific details of the API. For instance, some APIs offer arbitrary task graphs via manual task reference counting <ref type=""bibr"" target=""#b11"">[12]</ref>. This does not qualify as support in our classification. Also note that all APIs marked as featuring task ca",0
"ion, or energy consumption.</p><p>When targeting performance observation, performance monitoring software is either generating data to be used online <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b14"">15",0
"llel program. While some examples of global address space environments with task-based parallelism are specifically designed languages such as Chapel <ref type=""bibr"" target=""#b6"">[7]</ref> and X10 <ref type=""bibr"" target=""#b7"">[8]</ref>, it is also possible to implement these concepts as a library.",0
"he relation BornIn at all. To address the issue of noisy labeling, previous studies adopt multi-instance learning to consider the noises of instances <ref type=""bibr"" target=""#b13"">(Riedel, Yao, and McCallum 2010;</ref><ref type=""bibr"" target=""#b5"">Hoffmann et al. 2011;</ref><ref type=""bibr"" target= s suffer from the noisy labeling issue. To alleviate this issue, many studies formulated relation classification as a multi-instance learning problem <ref type=""bibr"" target=""#b13"">(Riedel, Yao, and McCallum 2010;</ref><ref type=""bibr"" target=""#b5"">Hoffmann et al. 2011;</ref><ref type=""bibr"" target= ramework which first selects correct sentences in the framework of reinforcement learning <ref type=""bibr"" target=""#b16"">(Sutton and Barto 1998;</ref><ref type=""bibr"" target=""#b13"">Narasimhan, Yala, and Barzilay 2016)</ref> and then predicts relations from each sentence in the cleansed data.</p></di ely used dataset<ref type=""foot"" target=""#foot_1"">4</ref> generated by the sentences in NYT<ref type=""foot"" target=""#foot_2"">5</ref> and developed by <ref type=""bibr"" target=""#b13"">(Riedel, Yao, and McCallum 2010)</ref>. There are 522,611 sentences, 281,270 entity pairs, and 18,252 relational facts",1
"r"" target=""#b15"">Surdeanu et al. 2012;</ref><ref type=""bibr"" target=""#b23"">Zeng et al. 2015;</ref><ref type=""bibr"" target=""#b9"">Lin et al. 2016;</ref><ref type=""bibr"" target=""#b6"">Ji et al. 2017)</ref>. In these studies, the training and test process is proceeded at the bag level, where a bag contai get=""#b15"">Surdeanu et al. 2012;</ref><ref type=""bibr"" target=""#b23"">Zeng et al. 2015)</ref>. In <ref type=""bibr"" target=""#b9"">(Lin et al. 2016;</ref><ref type=""bibr"" target=""#b6"">Ji et al. 2017;</ref><ref type=""bibr"">Tianyu Liu and Sui 2017)</ref>, a sentence-level attention mechanism over multiple",0
"os, Xiang, and Zhou 2015)</ref>, but all require high-quality annotated data.</p><p>In order to obtain large-scale training data, distant supervision <ref type=""bibr"" target=""#b11"">(Mintz et al. 2009</ref>) was proposed by assuming that if two entities have a relation in a given knowledge base, all ith CNN(Selected) (CNN+ATT(Selected)) on the relation classification task.</p><p>The results are compared under the held-out evaluation configuration <ref type=""bibr"" target=""#b11"">(Mintz et al. 2009</ref>) which provides an approximate measure of relation classification without expensive  human ann",0
"We found such a pre-training strategy is quite crucial for our method, which is also widely recommended by many other reinforcement learning studies <ref type=""bibr"" target=""#b0"">(Bahdanau et al. 2016)</ref>. Algorithm 2 presents the details of the joint training process. The relation classifier pr",0
"rget=""#b5"">Hoffmann et al. 2011;</ref><ref type=""bibr"" target=""#b15"">Surdeanu et al. 2012;</ref><ref type=""bibr"" target=""#b23"">Zeng et al. 2015;</ref><ref type=""bibr"" target=""#b9"">Lin et al. 2016;</ref><ref type=""bibr"" target=""#b6"">Ji et al. 2017)</ref>. In these studies, the training and test proce ""#b5"">Hoffmann et al. 2011;</ref><ref type=""bibr"" target=""#b15"">Surdeanu et al. 2012;</ref><ref type=""bibr"" target=""#b23"">Zeng et al. 2015)</ref>. In <ref type=""bibr"" target=""#b9"">(Lin et al. 2016;</ref><ref type=""bibr"" target=""#b6"">Ji et al. 2017;</ref><ref type=""bibr"">Tianyu Liu and Sui 2017)</ref works.</p><p>With the help of the instance selector, our method directly filters out noisy sentences. Unlike reducing the weights of noisy sentences <ref type=""bibr"" target=""#b9"">(Lin et al. 2016)</ref> or retaining one sentence in a bag <ref type=""bibr"" target=""#b22"">(Zeng et al. 2014</ref>), our ng layer. The convolution operation is performed on 3 consecutive words, and the number of feature maps d s is set to 230, the same as the setting of <ref type=""bibr"" target=""#b9"">(Lin et al. 2016)</ref>. Hence, the convolution parameters are</p><formula xml:id=""formula_10"">W f ∈ R d s ×(3d) and b f",0
"ce we cannot obtain it until the final representation is built.</p><p>Objective Function We optimize the parameters of PNet using REINFORCE algorithm <ref type=""bibr"" target=""#b24"">(Williams 1992</ref>) and policy gradient methods <ref type=""bibr"" target=""#b21"">(Sutton et al. 2000)</ref>, aiming to",1
"is able to retain important words for the task. For instance, as we may know, sentiment and negation words are important for sentiment classification <ref type=""bibr"" target=""#b28"">(Zhu et al. 2014;</ref><ref type=""bibr"" target=""#b18"">Qian et al. 2017)</ref>. As can be seen in Table <ref type=""table",0
"ref>. We also compared with Par-HLSTM which has the same structured representation model except that the phrase structure is given by Stanford parser <ref type=""bibr"" target=""#b11"">(Klein and Manning 2003)</ref> instead of RL. The results in Table <ref type=""table"" target=""#tab_6"">8</ref> show that",0
"><head>Introduction</head><p>Representation learning is a fundamental problem in AI, and particularly important for natural language processing (NLP) <ref type=""bibr"" target=""#b0"">(Bengio, Courville, and Vincent 2013;</ref><ref type=""bibr"" target=""#b12"">Le and Mikolov 2014)</ref>. As one of the most",0
"rget=""#b13"">Lei, Barzilay, and Jaakkola 2015)</ref> and recurrent neural network <ref type=""bibr"" target=""#b5"">(Hochreiter and Schmidhuber 1997;</ref><ref type=""bibr"" target=""#b4"">Chung et al. 2014)</ref> consider word order but do not use any structure. Structured representation models such as tree",0
""" target=""#b25"">Zhao et al. (2017)</ref> reduces bias by using corpus level constraints, but is only practical for models with specialized structure. <ref type=""bibr"" target=""#b13"">Kusner et al. (2017)</ref> propose the method based on causal inference to achieve the model fairness where they do the",1
"> 			<note xmlns=""http://www.tei-c.org/ns/1.0"" place=""foot"" n=""5"" xml:id=""foot_4"">To exclude mentions such as ""his mother"", we use Collins head finder<ref type=""bibr"" target=""#b5"">(Collins, 2003)</ref> to identify the head word of each mention, and only consider the mentions whose head word is gende",0
"lected since they do not influence the overall accuracy as much. For binary classification <ref type=""bibr"" target=""#b10"">Kamishima et al. (2012</ref><ref type=""bibr"" target=""#b11"">Kamishima et al. ( , 2011) )</ref> add a regularization term to their objective that penalizes biased predictions. Vari",0
"on pre-trained word embeddings, previous feature rich and rule-based approaches rely on corpus based gender statistics mined from external resources <ref type=""bibr"" target=""#b0"">(Bergsma and Lin, 2006)</ref>. Such lists were generated from large unlabeled corpora using heuristic data mining method",0
"ical conditions. * indicates the difference between pro/anti stereotypical conditions is significant (p &lt; .05) under an approximate randomized test<ref type=""bibr"" target=""#b8"">(Graham et al., 2014)</ref>. Our methods eliminate the difference between pro-stereotypical and anti-stereotypical condi",0
"ds solving the cocktail party problem. The inventions of deep clustering <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b2"">3]</ref>, deep attractor networks <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b4"">5]</ref> and perm h enhancement and separation <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" target=""#b2"">3]</ref>. However, this usually only leads to small improvements, even though the magnitude estimates from DNNs are reas omising approaches, the current stateof-the-art approach for supervised speech separation is via T-F masking <ref type=""bibr"" target=""#b34"">[35,</ref><ref type=""bibr"" target=""#b2"">3]</ref>. The proposed approach is expected to produce even better separation if the phase can be reconstructed.</p></di Network</head><p>To elicit a good phase via phase reconstruction, it is necessary to first obtain a good enough magnitude estimate. Our recent study <ref type=""bibr"" target=""#b2"">[3]</ref> proposed a novel multi-task learning approach combining the regularization capability of deep clustering with pproximating the affinity matrix from the embeddings:</p><formula xml:id=""formula_0"">LDC,classic = V V T − Y Y T 2 F (1)</formula><p>Our recent study <ref type=""bibr"" target=""#b2"">[3]</ref> suggests that an alternative loss function, which whitens the embedding in a k-means objective, leads to bette ivation functions that can work with γ &gt; 1 will be discussed in Section 3.4. Following <ref type=""bibr"" target=""#b21"">[22]</ref>, our recent study <ref type=""bibr"" target=""#b2"">[3]</ref> proposed a chimera++ network combining the two approaches via multi-task learning, as illustrated in the botto r study. The Griffin-Lim algorithm <ref type=""bibr"" target=""#b13"">[14]</ref> only performs iterative reconstruction for each source independently. In <ref type=""bibr"" target=""#b2"">[3]</ref>, we therefore proposed to utilize the MISI algorithm <ref type=""bibr"" target=""#b14"">[15]</ref> (see Algorithm e mixture signal. Note that the estimated magnitudes remain fixed during iterations, while the phase of each source are iteratively reconstructed. In <ref type=""bibr"" target=""#b2"">[3]</ref>, the phase reconstruction was only added as a post-processing, and it was not part of the objective function d es the performance to 11.5 dB, while 11.3 dB is obtained when applying five iterations of Griffin-Lim on each source independently, as is reported in <ref type=""bibr"" target=""#b2"">[3]</ref>. Performing end-to-end optimization using LWA improves the results to 11.6 dB from 11.2 dB, without requiring me-domain audio separation network (Tas-Net), operates directly in the time domain. Our result is 1.1 dB better than the previous state-of-the-art by <ref type=""bibr"" target=""#b2"">[3]</ref> in terms of both SI-SDR and SDR.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""6."">Concluding Re",1
"s a post-processing step on the magnitudes produced by deep learning based speech enhancement and separation <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" target=""#b2"">3]</ref>. However, this usually only lea plex spectrum domain, when using the noisy phases, as in <ref type=""bibr"" target=""#b20"">[21]</ref>, proposed in the same conference. A follow-up work <ref type=""bibr"" target=""#b18"">[19]</ref> of <ref type=""bibr"" target=""#b22"">[23]</ref> supplies clean phase during training. However, this makes their",1
"es produced by deep learning based speech enhancement and separation <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" target=""#b2"">3]</ref>. However, this usually only leads to small improvements, even though th",1
"n-Lim algorithm <ref type=""bibr"" target=""#b13"">[14]</ref>, multiple input spectrogram inverse (MISI) <ref type=""bibr"" target=""#b14"">[15]</ref>, ISSIR <ref type=""bibr"" target=""#b15"">[16]</ref>, and consistent Wiener filtering <ref type=""bibr"" target=""#b16"">[17]</ref>, which can recover the clean phas",0
"have witnessed exciting advances towards solving the cocktail party problem. The inventions of deep clustering <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b2"">3]</ref>, deep attractor networks <ref type=""bibr"" target=""#b3"">[4,</ref><ref type ks <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b4"">5]</ref> and permutation free training <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b6"">7]</ref> have dramatically improved the perf hout phase reconstruction unless specified.</p><p>We report the performance using scale-invariant SDR (SI-SDR) <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b38"">39]</ref>, as well as the SDR metric comput ning scheme was proposed for mask-inference networks first in <ref type=""bibr"" target=""#b0"">[1]</ref>, and was later found to be working very well in <ref type=""bibr"" target=""#b1"">[2]</ref> and <ref type=""bibr"" target=""#b5"">[6]</ref>. The idea is to train a maskinference network to minimize the mini",0
"puted on a GPU and through which backpropagation can be performed.</p><p>A recent study by Williamson et al. <ref type=""bibr"" target=""#b26"">[27,</ref><ref type=""bibr"" target=""#b27"">28]</ref> proposed a complex ratio masking approach for phase reconstruction and speech enhancement, where a feed-forwa",0
"se-sensitive mask <ref type=""bibr"" target=""#b24"">[25]</ref>. Closest to the above WA objective, an adaptive front-end framework was recently proposed <ref type=""bibr"" target=""#b25"">[26]</ref> in which the STFT and its inverse are subsumed by the network, along with the noisy phase, so that training",0
"o conventional magnitude spectrum approximation <ref type=""bibr"" target=""#b23"">[24]</ref>, which does not perform as well as the phase-sensitive mask <ref type=""bibr"" target=""#b24"">[25]</ref>. Closest to the above WA objective, an adaptive front-end framework was recently proposed <ref type=""bibr"" t type=""bibr"" target=""#b20"">[21]</ref>, which contains patterns clearly predictable from energy-based features <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b24"">25]</ref>. However, recent studies along this line suggest that the patterns in the imaginary component are too random",0
"pectrogram inverse (MISI) <ref type=""bibr"" target=""#b14"">[15]</ref>, ISSIR <ref type=""bibr"" target=""#b15"">[16]</ref>, and consistent Wiener filtering <ref type=""bibr"" target=""#b16"">[17]</ref>, which can recover the clean phase to some extent starting from the mixture phase and a good estimated magni",0
"target=""#b29"">[30,</ref><ref type=""bibr"" target=""#b30"">31]</ref>, WaveNet <ref type=""bibr"" target=""#b31"">[32]</ref>, generative adversarial networks <ref type=""bibr"" target=""#b32"">[33]</ref>, or encoder-decoder architectures <ref type=""bibr"" target=""#b33"">[34]</ref>. Although they are promising app",0
"gely because phase is difficult to estimate. It is well-known that this incurs a phase inconsistency problem <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b12"">13]</ref>, especially for speech processing, where there is typically at least",0
"phase inconsistency problem altogether by operating in the time domain, using convolutional neural networks <ref type=""bibr"" target=""#b29"">[30,</ref><ref type=""bibr"" target=""#b30"">31]</ref>, WaveNet <ref type=""bibr"" target=""#b31"">[32]</ref>, generative adversarial networks <ref type=""bibr"" target=""",0
"ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b38"">39]</ref>, as well as the SDR metric computed using the bss eval sources software <ref type=""bibr"" target=""#b39"">[40]</ref> because it is used by other groups. We believe SI-SDR is a more proper measure for singlechannel instantaneo",0
"eaker-independent multispeaker speech separation, demonstrating overwhelming advantages over previous methods including graphical modeling approaches <ref type=""bibr"" target=""#b7"">[8]</ref>, spectral clustering approaches <ref type=""bibr"" target=""#b8"">[9]</ref>, and CASA methods <ref type=""bibr"" tar",0
"e phase for time-domain re-synthesis, largely because phase is difficult to estimate. It is well-known that this incurs a phase inconsistency problem <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b12"">13]</ref>, especially for speech proce",0
"e=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b91"">94,</ref><ref type=""bibr"" target=""#b55"">58,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b60"">63]</ref>. Here we make an attempt to actually find this structure. We acknowledge that this is related to a breadth of e=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b91"">94,</ref><ref type=""bibr"" target=""#b55"">58,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b60"">63]</ref>. Unlike multi-task learning, we explicitly model the relations among tasks and extract a meta-structure. The",1
"ence and Dirichlet processes <ref type=""bibr"" target=""#b50"">[52,</ref><ref type=""bibr"" target=""#b85"">88,</ref><ref type=""bibr"" target=""#b84"">87,</ref><ref type=""bibr"" target=""#b83"">86,</ref><ref type=""bibr"" target=""#b33"">35,</ref><ref type=""bibr"" target=""#b35"">37]</ref>, few-shot learning <ref type=",0
"86,</ref><ref type=""bibr"" target=""#b33"">35,</ref><ref type=""bibr"" target=""#b35"">37]</ref>, few-shot learning <ref type=""bibr"" target=""#b75"">[78,</ref><ref type=""bibr"" target=""#b21"">23,</ref><ref type=""bibr"" target=""#b20"">22,</ref><ref type=""bibr"" target=""#b64"">67,</ref><ref type=""bibr"" target=""#b80""",0
"building-wide Source Task Encoder Target Task Output meshes similar to <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b95"">98,</ref><ref type=""bibr"" target=""#b11"">12]</ref> enabling us to programmatically compute the ground truth for many tasks without human labeling. For the tasks",0
"ages are registered on and aligned with building-wide Source Task Encoder Target Task Output meshes similar to <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b95"">98,</ref><ref type=""bibr"" target=""#b11"">12]</ref> enabling us to programmatically compute the ground truth for many tas",0
"rds into subword units <ref type=""bibr"" target=""#b19"">(Schuster and Nakajima, 2012;</ref><ref type=""bibr"" target=""#b4"">Chitnis and DeNero, 2015;</ref><ref type=""bibr"" target=""#b21"">Sennrich et al., 2016;</ref><ref type=""bibr"" target=""#b31"">Wu et al., 2016)</ref>.</p><p>Byte-Pair-Encoding  <ref type= u et al., 2016)</ref>.</p><p>Byte-Pair-Encoding  <ref type=""table"">1</ref>: Multiple subword sequences encoding the same sentence ""Hello World"" (BPE) <ref type=""bibr"" target=""#b21"">(Sennrich et al., 2016)</ref> is a de facto standard subword segmentation algorithm applied to many NMT systems and ach pace with the Enhanced Suffix Array algorithm <ref type=""bibr"" target=""#b16"">(Nong et al., 2009)</ref>, where T is the size of the corpus. Similar to <ref type=""bibr"" target=""#b21"">(Sennrich et al., 2016)</ref>, we do not consider subwords that cross word boundaries.</p><p>As the final vocabulary V resampling <ref type=""bibr"" target=""#b11"">(Koehn, 2004)</ref>. The same mark is used in Table <ref type=""table"">4</ref> and<ref type=""table"">6</ref>. <ref type=""bibr"" target=""#b21"">(Sennrich et al., 2016)</ref> and our unigram model with or without subword regularization. The BLEU scores of word, ch oding and n-best decoding respectively.</p><p>3 Subword segmentations with language model 3.1 Byte-Pair-Encoding (BPE)</p><p>Byte-Pair-Encoding (BPE) <ref type=""bibr"" target=""#b21"">(Sennrich et al., 2016;</ref><ref type=""bibr"" target=""#b19"">Schuster and Nakajima, 2012</ref>) is a subword segmentatio",1
"ied to many NMT systems and achieving top translation quality in several shared tasks <ref type=""bibr"" target=""#b5"">(Denkowski and Neubig, 2017;</ref><ref type=""bibr"" target=""#b15"">Nakazawa et al., 2017)</ref>. BPE segmentation gives a good balance between the vocabulary size and the decoding effici",0
"er> 	<text xml:lang=""en""> 		<body> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""1"">Introduction</head><p>Neural Machine Translation (NMT) models <ref type=""bibr"" target=""#b1"">(Bahdanau et al., 2014;</ref><ref type=""bibr"" target=""#b13"">Luong et al., 2015;</ref><ref type=""bibr"" target=""#b31"">Wu e",0
"s/1.0""><head n=""4"">Related Work</head><p>Regularization by noise is a well studied technique in deep neural networks. A well-known example is dropout <ref type=""bibr"" target=""#b23"">(Srivastava et al., 2014)</ref>, which randomly turns off a subset of hidden units during training. Dropout is analyzed",0
"xt and question by standard attentions <ref type=""bibr"" target=""#b43"">(Xiong et al., 2016;</ref><ref type=""bibr"" target=""#b31"">Seo et al., 2016;</ref><ref type=""bibr"" target=""#b2"">Bahdanau et al., 2015)</ref>. The resulting representation is encoded again with our recurrency-free encoder before fina <ref type=""figure"">2</ref> with French as a pivotal language.</p><p>In this work, we consider attention-based neural machine translation (NMT) models <ref type=""bibr"" target=""#b2"">Bahdanau et al. (2015)</ref>; <ref type=""bibr"" target=""#b24"">Luong et al. (2015)</ref>, which have demonstrated excellen",1
"dels of our data augmentation pipeline. Specifically, we utilize the publicly available codebase<ref type=""foot"" target=""#foot_4"">3</ref> provided by <ref type=""bibr"" target=""#b25"">Luong et al. (2017)</ref>, which replicates the Google's NMT (GNMT) systems <ref type=""bibr"" target=""#b42"">Wu et al. (2 lish-German<ref type=""foot"" target=""#foot_6"">5</ref> (4.5M sentence pairs). All data have been tokenized and split into subword units as described in <ref type=""bibr"" target=""#b25"">Luong et al. (2017)</ref>. All models share the same hyperparameters<ref type=""foot"" target=""#foot_7"">6</ref> and are t",1
"component to cope with long term interactions. A successful combination of these two ingredients is the Bidirectional Attention Flow (BiDAF) model by <ref type=""bibr"" target=""#b31"">Seo et al. (2016)</ref>, which achieve strong results on the SQuAD dataset <ref type=""bibr"" target=""#b30"">(Rajpurkar et tion [x w ; x c ] ∈ R p1+p2 , where x w and x c are the word embedding and the convolution output of character embedding of x respectively. Following <ref type=""bibr"" target=""#b31"">Seo et al. (2016)</ref>, we also adopt a two-layer highway network <ref type=""bibr"" target=""#b34"">(Srivastava et al., 2 max function, and the</p><formula xml:id=""formula_2"">query-to-context attention is B = S • S T • C T .</formula><p>4. Model Encoder Layer. Similar to <ref type=""bibr"" target=""#b31"">Seo et al. (2016)</ref>, the input of this layer at each position is [c, a, c a, c b], where a and b are respectively a utput layer. This layer is task-specific. Each example in SQuAD is labeled with a span in the context containing the answer. We adopt the strategy of <ref type=""bibr"" target=""#b31"">Seo et al. (2016)</ref> to predict the probability of each position in the context being the start or end of an answer context. Then we learn the interactions between context and question by standard attentions <ref type=""bibr"" target=""#b43"">(Xiong et al., 2016;</ref><ref type=""bibr"" target=""#b31"">Seo et al., 2016;</ref><ref type=""bibr"" target=""#b2"">Bahdanau et al., 2015)</ref>. The resulting representation is enco x faster in training and 4x to 9x faster in inference. As a simple comparison, our model can achieve the same accuracy (77.0 F1 score) as BiDAF model <ref type=""bibr"" target=""#b31"">(Seo et al., 2016)</ref> within 3 hours training that otherwise should have taken 15 hours. The speed-up gain also allo petitive models. For instance, if we allow our model to train for 18 hours, it achieves an F1 score of 82.7 on the dev set, which is much better than <ref type=""bibr"" target=""#b31"">(Seo et al., 2016)</ref>, and is on par with best published results.</p><p>As our model is fast, we can train it with m attention is computed as</p><formula xml:id=""formula_0"">A = S • Q T ∈ R n×d .</formula><p>The similarity function used here is the trilinear function <ref type=""bibr"" target=""#b31"">(Seo et al., 2016)</ref>:</p><formula xml:id=""formula_1"">f (q, c) = W 0 [q, c, q c],</formula><p>where is the element-w iplication and W 0 is a trainable variable.</p><p>Most high performing models additionally use some form of query-to-context attention, such as BiDaF <ref type=""bibr"" target=""#b31"">(Seo et al., 2016)</ref> and DCN <ref type=""bibr"" target=""#b43"">(Xiong et al., 2016)</ref>. Empirically, we find that, iong et al., 2016)</ref> 66.2 / 75.9 66.2 / 75.9 FastQA <ref type=""bibr"" target=""#b40"">(Weissenborn et al., 2017)</ref> 68.4 / 77.1 68.4 / 77.1 BiDAF <ref type=""bibr"" target=""#b31"">(Seo et al., 2016)</ref> 68.0 / 77.3 68.0 / 77.3 SEDT <ref type=""bibr"" target=""#b22"">(Liu et al., 2017a)</ref> 68.1 / 7 =""http://www.tei-c.org/ns/1.0"" type=""table"" xml:id=""tab_4""><head>Table 4 :</head><label>4</label><figDesc>Speed comparison between our model and BiDAF<ref type=""bibr"" target=""#b31"">(Seo et al., 2016)</ref> on SQuAD dataset.</figDesc><table><row><cell>4.1.3 ABALATION STUDY AND ANALYSIS</cell></row></ .0 SEDT <ref type=""bibr"" target=""#b22"">(Liu et al., 2017a)</ref> 33.9 44.8 DCR <ref type=""bibr"" target=""#b45"">(Yu et al., 2016)</ref> 37.8 45.1 BiDAF <ref type=""bibr"" target=""#b31"">(Seo et al., 2016)</ref> 34.3 45.7 jNet <ref type=""bibr"" target=""#b46"">(Zhang et al., 2017)</ref> 37.9 47.0 Ruminating s we do not want to probe the unseen test set by frequent submissions. According to the observations from our experiments and previous works, such as <ref type=""bibr"" target=""#b31"">(Seo et al., 2016;</ref><ref type=""bibr"" target=""#b43"">Xiong et al., 2016;</ref><ref type=""bibr"" target=""#b38"">Wang et",1
"ional encoding is added to the input at the beginning of each encoder layer consisting of sin and cos functions at varying wavelengths, as defined in <ref type=""bibr"" target=""#b35"">(Vaswani et al., 2017a)</ref>. Each sub-layer after the positional encoding (one of convolution, self-attention, or fee rs is d = 128 and the number of conv layers within a block is 4. For the self-attention-layer, we adopt the multi-head attention mechanism defined in <ref type=""bibr"" target=""#b35"">(Vaswani et al., 2017a)</ref> which, for each position in the input, called the query, computes a weighted sum of all p (one of convolution, self-attention, or feed-forward-net) inside the encoder structure is wrapped inside a residual block.</p><p>used extensively in <ref type=""bibr"" target=""#b35"">Vaswani et al. (2017a)</ref>, the combination of convolutions and self-attention is novel, and is significantly better",1
"convolution or full attention architectures <ref type=""bibr"" target=""#b19"">(Kim, 2014;</ref><ref type=""bibr"" target=""#b9"">Gehring et al., 2017;</ref><ref type=""bibr"" target=""#b36"">Vaswani et al., 2017b;</ref><ref type=""bibr"" target=""#b32"">Shen et al., 2017a)</ref>. Those models have been shown to b",1
"blocks of encoders that separately encodes the query and context. Then we learn the interactions between context and question by standard attentions <ref type=""bibr"" target=""#b43"">(Xiong et al., 2016;</ref><ref type=""bibr"" target=""#b31"">Seo et al., 2016;</ref><ref type=""bibr"" target=""#b2"">Bahdanau orming models additionally use some form of query-to-context attention, such as BiDaF <ref type=""bibr"" target=""#b31"">(Seo et al., 2016)</ref> and DCN <ref type=""bibr"" target=""#b43"">(Xiong et al., 2016)</ref>. Empirically, we find that, the DCN attention can provide a little benefit over simply apply f> 64.7 / 73.7 64.7 / 73.7 Multi-Perspective Matching <ref type=""bibr"">(Wang et al., 2016)</ref> 65.5 / 75.1 70.4 / 78.8 Dynamic Coattention Networks <ref type=""bibr"" target=""#b43"">(Xiong et al., 2016)</ref> 66.2 / 75.9 66.2 / 75.9 FastQA <ref type=""bibr"" target=""#b40"">(Weissenborn et al., 2017)</re ckle these challenges, including <ref type=""bibr"">BiDAF (Seo et al., 2016)</ref>, r-net <ref type=""bibr"" target=""#b38"">(Wang et al., 2017)</ref>, DCN <ref type=""bibr"" target=""#b43"">(Xiong et al., 2016)</ref>, ReasoNet <ref type=""bibr"" target=""#b33"">(Shen et al., 2017b)</ref>, Document Reader <ref ty nt submissions. According to the observations from our experiments and previous works, such as <ref type=""bibr"" target=""#b31"">(Seo et al., 2016;</ref><ref type=""bibr"" target=""#b43"">Xiong et al., 2016;</ref><ref type=""bibr"" target=""#b38"">Wang et al., 2017;</ref><ref type=""bibr"" target=""#b3"">Chen et a",1
"type=""bibr"" target=""#b33"">(Shen et al., 2017b)</ref>, Document Reader <ref type=""bibr"" target=""#b3"">(Chen et al., 2017)</ref>, Interactive AoA Reader <ref type=""bibr"" target=""#b7"">(Cui et al., 2017)</ref> and Reinforced Mnemonic Reader <ref type=""bibr"" target=""#b14"">(Hu et al., 2017)</ref>.</p><p>Re",0
"ments. The use of convolutions also allows us to take advantage of common regularization methods in ConvNets such as stochastic depth (layer dropout) <ref type=""bibr"" target=""#b15"">(Huang et al., 2016)</ref>, which gives an additional gain of 0.2 F1 in our experiments.</p><p>In detail, our model con rates are 0.1 and 0.05 respectively, and the dropout rate between every two layers is 0.1. We also adopt the stochastic depth method (layer dropout) <ref type=""bibr"" target=""#b15"">(Huang et al., 2016)</ref> within each embedding or model encoder layer, where sublayer l has survival probability p l",0
"norm(x)) + x, meaning there is a full identity path from the input to output of each block, where layernorm indicates layer-normalization proposed in <ref type=""bibr"" target=""#b1"">(Ba et al., 2016)</ref>. The total number of encoder blocks is 1. Note that the input of this layer is a vector of dimen",0
"sk Sennrich et al. ( <ref type=""formula"">2016</ref>) or instrinsic paraphrase evaluations <ref type=""bibr"" target=""#b41"">Wieting et al. (2017)</ref>; <ref type=""bibr"" target=""#b26"">Mallinson et al. (2017)</ref>. Our approach is a novel application of backtranslation to enrich training data for down-",0
""">(Weissenborn et al., 2017)</ref> 68.4 / 77.1 68.4 / 77.1 BiDAF <ref type=""bibr"" target=""#b31"">(Seo et al., 2016)</ref> 68.0 / 77.3 68.0 / 77.3 SEDT <ref type=""bibr"" target=""#b22"">(Liu et al., 2017a)</ref> 68.1 / 77.5 68.5 / 78.0 RaSoR <ref type=""bibr"" target=""#b21"">(Lee et al., 2016)</ref> 70.8 / type=""bibr"" target=""#b30"">(Rajpurkar et al., 2016)</ref> 23.2 30.4 Match <ref type=""bibr"" target=""#b37"">(Wang &amp; Jiang, 2016)</ref> 27.3 39.0 SEDT <ref type=""bibr"" target=""#b22"">(Liu et al., 2017a)</ref> 33.9 44.8 DCR <ref type=""bibr"" target=""#b45"">(Yu et al., 2016)</ref> 37.8 45.1 BiDAF <ref typ",0
"QANet trained with the augmented data achieves 84.6 F1 score on the test set, which is significantly better than the best published result of 81.8 by <ref type=""bibr"" target=""#b14"">Hu et al. (2017)</ref>. <ref type=""foot"" target=""#foot_3"">2</ref> We also conduct ablation test to justify the usefulne verified subset that all the contexts inside can answer the associated questions. As the text could be long, we adopt the data processing similar to <ref type=""bibr"" target=""#b14"">Hu et al. (2017)</ref>; <ref type=""bibr"" target=""#b17"">Joshi et al. (2017)</ref>. In particular, for training and valid target=""#b3"">(Chen et al., 2017)</ref>, Interactive AoA Reader <ref type=""bibr"" target=""#b7"">(Cui et al., 2017)</ref> and Reinforced Mnemonic Reader <ref type=""bibr"" target=""#b14"">(Hu et al., 2017)</ref>.</p><p>Recurrent Neural Networks (RNNs) have featured predominatnly in Natural Language Process 49.5 MPCM <ref type=""bibr"">(Wang et al., 2016)</ref> 40.3 50.0 ReasoNet <ref type=""bibr"" target=""#b33"">(Shen et al., 2017b)</ref> 39.4 50.3 Mnemonic <ref type=""bibr"" target=""#b14"">(Hu et al., 2017)</ref> 46.6 56.0 QANet 45.2 55.7</p><p>Table <ref type=""table"">6</ref>: The F1 scores on the adversari del on the subset consisting of answers from Wikipedia.</p><p>According to the previous work <ref type=""bibr"" target=""#b17"">(Joshi et al., 2017;</ref><ref type=""bibr"" target=""#b14"">Hu et al., 2017;</ref><ref type=""bibr"" target=""#b27"">Pan et al., 2017)</ref>, the same model would have similar perform",0
"3 Single Model EM / F1 EM / F1 LR Baseline <ref type=""bibr"" target=""#b30"">(Rajpurkar et al., 2016)</ref> 40.4 / 51.0 40.4 / 51.0 Dynamic Chunk Reader <ref type=""bibr"" target=""#b45"">(Yu et al., 2016)</ref> 62.5 / 71.0 62.5 / 71.0 Match-LSTM with Ans-Ptr <ref type=""bibr"" target=""#b37"">(Wang &amp; Jian h <ref type=""bibr"" target=""#b37"">(Wang &amp; Jiang, 2016)</ref> 27.3 39.0 SEDT <ref type=""bibr"" target=""#b22"">(Liu et al., 2017a)</ref> 33.9 44.8 DCR <ref type=""bibr"" target=""#b45"">(Yu et al., 2016)</ref> 37.8 45.1 BiDAF <ref type=""bibr"" target=""#b31"">(Seo et al., 2016)</ref> 34.3 45.7 jNet <ref typ",0
"xperiments and previous works, such as <ref type=""bibr"" target=""#b31"">(Seo et al., 2016;</ref><ref type=""bibr"" target=""#b43"">Xiong et al., 2016;</ref><ref type=""bibr"" target=""#b38"">Wang et al., 2017;</ref><ref type=""bibr"" target=""#b3"">Chen et al., 2017)</ref>, the validation score is well correlated r of end-to-end neural network models have been proposed to tackle these challenges, including <ref type=""bibr"">BiDAF (Seo et al., 2016)</ref>, r-net <ref type=""bibr"" target=""#b38"">(Wang et al., 2017)</ref>, DCN <ref type=""bibr"" target=""#b43"">(Xiong et al., 2016)</ref>, ReasoNet <ref type=""bibr"" tar ref type=""bibr"" target=""#b48"">Zhou et al. (2017)</ref> improved the diversity of the SQuAD data by generating more questions. However, as reported by <ref type=""bibr"" target=""#b38"">Wang et al. (2017)</ref>, their method did not help improve the performance. The data augmentation technique proposed i",0
"ntial moving average is applied on all trainable variables with a decay rate 0.9999.</p><p>Finally, we implement our model in Python using Tensorflow <ref type=""bibr"" target=""#b0"">(Abadi et al., 2016)</ref> and carry out our experiments on an NVIDIA p100 GPU.<ref type=""foot"" target=""#foot_12"">11</re",0
", it is often used to improve either the same translation task Sennrich et al. ( <ref type=""formula"">2016</ref>) or instrinsic paraphrase evaluations <ref type=""bibr"" target=""#b41"">Wieting et al. (2017)</ref>; <ref type=""bibr"" target=""#b26"">Mallinson et al. (2017)</ref>. Our approach is a novel appl",0
"l., 2017)</ref> 70.0 / 79.0 70.7 / 79.4 Ruminating Reader <ref type=""bibr"" target=""#b10"">(Gong &amp; Bowman, 2017)</ref> 70.6 / 79.5 70.6 / 79.5 jNet <ref type=""bibr"" target=""#b46"">(Zhang et al., 2017)</ref> 70 LSTMs as is used in most existing models. Specifically, each (embedding and model) encode 4.8 DCR <ref type=""bibr"" target=""#b45"">(Yu et al., 2016)</ref> 37.8 45.1 BiDAF <ref type=""bibr"" target=""#b31"">(Seo et al., 2016)</ref> 34.3 45.7 jNet <ref type=""bibr"" target=""#b46"">(Zhang et al., 2017)</ref> 37.9 47.0 Ruminating <ref type=""bibr"" target=""#b10"">(Gong &amp; Bowman, 2017)</ref> 37.4 47.",0
"pplication of backtranslation to enrich training data for down-stream tasks, in this case, the question answering (QA) task. It is worth to note that <ref type=""bibr"" target=""#b8"">(Dong et al., 2017)</ref> use paraphrasing techniques to improve QA; however, they only paraphrase questions and did not",0
".</p><p>In this work, we consider attention-based neural machine translation (NMT) models <ref type=""bibr"" target=""#b2"">Bahdanau et al. (2015)</ref>; <ref type=""bibr"" target=""#b24"">Luong et al. (2015)</ref>, which have demonstrated excellent translation quality <ref type=""bibr"" target=""#b42"">Wu et a",0
"word embedding and character embedding. The word embedding is fixed during training and initialized from the p 1 = 300 dimensional pre-trained GloVe <ref type=""bibr"" target=""#b28"">(Pennington et al., 2014)</ref> word vectors, which are fixed during training. All the out-of-vocabulary words are mapp cally pad the short sentences with special symbol &lt;PAD&gt;. The maximum answer length is set to 30. We use the pretrained 300-D word vectors GLoVe <ref type=""bibr"" target=""#b28"">(Pennington et al., 2014)</ref>, and all the out-of-vocabulary words are replace with &lt;UNK&gt;, whose embedding is u",0
"o of dimension d = 128.</p><p>3. Context-Query Attention Layer. This module is standard in almost every previous reading comprehension models such as <ref type=""bibr"" target=""#b40"">Weissenborn et al. (2017)</ref> and <ref type=""bibr"" target=""#b3"">Chen et al. (2017)</ref>. We use C and Q to denote th nditional on the search beams. Nevertheless, their model is still based on the RNNs and the accuracy is not competitive, with an EM 68.4 and F1 76.2. <ref type=""bibr"" target=""#b40"">Weissenborn et al. (2017)</ref> also tried to build a fast Q&amp;A model by deleting the context-query attention module )</ref> 65.5 / 75.1 70.4 / 78.8 Dynamic Coattention Networks <ref type=""bibr"" target=""#b43"">(Xiong et al., 2016)</ref> 66.2 / 75.9 66.2 / 75.9 FastQA <ref type=""bibr"" target=""#b40"">(Weissenborn et al., 2017)</ref> 68.4 / 77.1 68.4 / 77.1 BiDAF <ref type=""bibr"" target=""#b31"">(Seo et al., 2016)</ref> 22"">(Liu et al., 2017a)</ref> 68.1 / 77.5 68.5 / 78.0 RaSoR <ref type=""bibr"" target=""#b21"">(Lee et al., 2016)</ref> 70.8 / 78.7 69.6 / 77.7 FastQAExt <ref type=""bibr"" target=""#b40"">(Weissenborn et al., 2017)</ref> 70.8 / 78.9 70.8 / 78.9 ReasoNet <ref type=""bibr"" target=""#b33"">(Shen et al., 2017b)</",0
"r"" target=""#b19"">(Kim, 2014;</ref><ref type=""bibr"" target=""#b9"">Gehring et al., 2017;</ref><ref type=""bibr"" target=""#b36"">Vaswani et al., 2017b;</ref><ref type=""bibr"" target=""#b32"">Shen et al., 2017a)</ref>. Those models have been shown to be not only faster than the RNN architectures, but also effe",0
"t=""#b31"">(Seo et al., 2016)</ref> 68.0 / 77.3 68.0 / 77.3 SEDT <ref type=""bibr"" target=""#b22"">(Liu et al., 2017a)</ref> 68.1 / 77.5 68.5 / 78.0 RaSoR <ref type=""bibr"" target=""#b21"">(Lee et al., 2016)</ref> 70.8 / 78.7 69.6 / 77.7 FastQAExt <ref type=""bibr"" target=""#b40"">(Weissenborn et al., 2017)</r e=""bibr"" target=""#b46"">(Zhang et al., 2017)</ref> 37.9 47.0 Ruminating <ref type=""bibr"" target=""#b10"">(Gong &amp; Bowman, 2017)</ref> 37.4 47.7 RaSOR <ref type=""bibr"" target=""#b21"">(Lee et al., 2016)</ref> 39.5 49.5 MPCM <ref type=""bibr"">(Wang et al., 2016)</ref> 40.3 50.0 ReasoNet <ref type=""bibr""",0
"works note that adversarial examples for substitute networks do not always transfer to the target model, especially when conducting targeted attacks <ref type=""bibr"" target=""#b6"">(Chen et al., 2017;</ref><ref type=""bibr"" target=""#b19"">Narodytska &amp; Kasiviswanathan, 2017)</ref>. These works inste e additional restrictions on the black-box setting:</p><p>Black-box setting. In this paper, we use the definition of black-box access as query access <ref type=""bibr"" target=""#b6"">(Chen et al., 2017;</ref><ref type=""bibr"" target=""#b15"">Liu et al., 2017;</ref><ref type=""bibr"" target=""#b11"">Hayes &amp does not require a substitute network and is 2-3 orders of magnitude more query-efficient than previous methods based on gradient estimation such as <ref type=""bibr"" target=""#b6"">Chen et al. (2017)</ref>. We show that our approach reliably produces targeted adversarial examples in the blackbox sett with an estimate of the gradient, which is approximated by querying the classifier rather than computed by autodifferentiation. This idea is used in <ref type=""bibr"" target=""#b6"">Chen et al. (2017)</ref>, where the gradient is estimated via pixel-by-pixel finite differences, and then the CW attack standard black-box setting).</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.1.2."">BLACK-BOX ATTACKS WITH GRADIENT</head><p>ESTIMATION <ref type=""bibr"" target=""#b6"">Chen et al. (2017)</ref> explore black-box gradient estimation methods as an alternative to substitute networks, where w do not provide a direct comparison due to the incompability of the 2 and ∞ metric as well as the fixed-budget nature of the optimization algorithm in <ref type=""bibr"" target=""#b6"">Chen et al. (2017)</ref>, our method takes far fewer queries to generate imperceptible adversarial examples. <ref type=""",1
"<p>Black-box setting. In this paper, we use the definition of black-box access as query access <ref type=""bibr"" target=""#b6"">(Chen et al., 2017;</ref><ref type=""bibr"" target=""#b15"">Liu et al., 2017;</ref><ref type=""bibr"" target=""#b11"">Hayes &amp; Danezis, 2017)</ref>. In this model, the adversary ca f><ref type=""bibr"">2017)</ref> trained the Cloud Prediction API with small datasets like MNIST and successfully demonstrated an untargeted attack. As <ref type=""bibr"" target=""#b15"">Liu et al. (2017)</ref> demonstrated, it is more difficult to transfer targeted adversarial examples with or without th amples with or without their target labels, particularly when attacking models trained on large datasets like ImageNet. Using ensemble-based methods, <ref type=""bibr"" target=""#b15"">Liu et al. (2017)</ref> overcame these limitations to attack the Clarifai API. Their threat model specifies that the ad",1
"acks on real-world systems.</p><p>We propose the variant of NES described in <ref type=""bibr"" target=""#b25"">Salimans et al. (2017)</ref> (inspired by <ref type=""bibr"" target=""#b29"">Wierstra et al. (2014)</ref>) as a method for generating adversarial examples in the query-limited setting. We use NES this section, we detail our algorithm for efficiently estimating the gradient from queries, based on the Natural Evolutionary Strategies approach of <ref type=""bibr"" target=""#b29"">Wierstra et al. (2014)</ref>, and then state how the estimated gradient is used to generate adversarial examples.</p></ ent estimation in far fewer queries than typical finite-difference methods. For a loss function F (•) and a current set of parameters x, we have from <ref type=""bibr"" target=""#b29"">Wierstra et al. (2014)</ref>:</p><formula xml:id=""formula_0"">E π(θ|x) [F (θ)] = F (θ)π(θ|x) dθ ∇ x E π(θ|x) [F (θ)] = ∇ dθ = F (θ) π(θ|x) π(θ|x) ∇ x π(θ|x) dθ = π(θ|x)F (θ)∇ x log (π(θ|x)) dθ = E π(θ|x) [F (θ)∇ x log (π(θ|x))]</formula><p>In a manner similar to that in <ref type=""bibr"" target=""#b29"">Wierstra et al. (2014)</ref>, we choose a search distribution of random Gaussian noise around the current image x; that ach for conducting an attack in each of the proposed threat models. We begin with a description of our application of Natural Evolutionary Strategies <ref type=""bibr"" target=""#b29"">(Wierstra et al., 2014)</ref> to enable query-efficient generation of black-box adversarial examples. We then show the es.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2.1.1."">NATURAL EVOLUTIONARY STRATEGIES</head><p>To estimate the gradient, we use NES <ref type=""bibr"" target=""#b29"">(Wierstra et al., 2014)</ref>, a method for derivative-free optimization based on the idea of a search distribution π(θ",0
"ps; their goal is to show that CNNs do not generalize to inverted images, rather than to demonstrate a novel attack or to consider a new threat model <ref type=""bibr"" target=""#b12"">(Hosseini et al., 2017)</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""5."">Conclusion</head><p>Our",0
"l., 2013)</ref>. These adversarial examples can potentially be exploited in the real world <ref type=""bibr"" target=""#b14"">(Kurakin et al., 2016;</ref><ref type=""bibr"" target=""#b0"">Athalye et al., 2017;</ref><ref type=""bibr"" target=""#b26"">Sharif et al., 2017;</ref><ref type=""bibr"" target=""#b8"">Evtimo",0
"get=""#fig_4"">5</ref> 5 .  <ref type=""bibr"" target=""#b9"">(Goodfellow et al., 2015;</ref><ref type=""bibr"" target=""#b4"">Carlini &amp; Wagner, 2017;</ref><ref type=""bibr"" target=""#b18"">Moosavi-Dezfooli et al., 2016;</ref><ref type=""bibr"" target=""#b17"">Moosavi-Dezfooli et al., 2017;</ref><ref type=""bibr""",0
"et al., 2015;</ref><ref type=""bibr"" target=""#b4"">Carlini &amp; Wagner, 2017;</ref><ref type=""bibr"" target=""#b18"">Moosavi-Dezfooli et al., 2016;</ref><ref type=""bibr"" target=""#b17"">Moosavi-Dezfooli et al., 2017;</ref><ref type=""bibr"" target=""#b11"">Hayes &amp; Danezis, 2017)</ref>, where an attacker",0
"nition systems <ref type=""bibr"" target=""#b5"">(Carlini et al., 2016)</ref>, malware detectors <ref type=""bibr"" target=""#b13"">(Hu &amp; Tan, 2017;</ref><ref type=""bibr"" target=""#b30"">Xu et al., 2016)</ref>, and face recognition systems <ref type=""bibr"" target=""#b26"">(Sharif et al., 2017)</ref>. Curren ersarial attacks</head><p>Several notable works in adversarial examples use similar techniques but with different adversarial goals or threat models. <ref type=""bibr"" target=""#b30"">Xu et al. (2016)</ref> explore black-box adversarial examples to fool PDF malware classifiers. To generate an adversari",0
"=""#b14"">(Kurakin et al., 2016;</ref><ref type=""bibr"" target=""#b0"">Athalye et al., 2017;</ref><ref type=""bibr"" target=""#b26"">Sharif et al., 2017;</ref><ref type=""bibr"" target=""#b8"">Evtimov et al., 2017)</ref>. For many commercial or proprietary systems, adversarial examples must be considered under a",0
"ly be exploited in the real world <ref type=""bibr"" target=""#b14"">(Kurakin et al., 2016;</ref><ref type=""bibr"" target=""#b0"">Athalye et al., 2017;</ref><ref type=""bibr"" target=""#b26"">Sharif et al., 2017;</ref><ref type=""bibr"" target=""#b8"">Evtimov et al., 2017)</ref>. For many commercial or proprietary etectors <ref type=""bibr"" target=""#b13"">(Hu &amp; Tan, 2017;</ref><ref type=""bibr"" target=""#b30"">Xu et al., 2016)</ref>, and face recognition systems <ref type=""bibr"" target=""#b26"">(Sharif et al., 2017)</ref>. Current black-box attacks use either substitute networks or gradient estimation techniques",0
"gradient estimate.</p><p>The special case of NES that we have described here can be seen as a finite-differences estimate on a random Gaussian basis. <ref type=""bibr"" target=""#b10"">Gorban et al. (2016)</ref> shows that for an n-dimensional space and N randomly sampled Gaussian vectors v 1 . . . v N",0
"yright 2018 by the author(s). a substitute network to emulate the original network and then attacks the substitute with first-order white-box methods <ref type=""bibr"" target=""#b22"">(Papernot et al., 2016a;</ref><ref type=""bibr"">2017)</ref>. Recent works note that adversarial examples for substitute ient estimation techniques.  <ref type=""bibr"" target=""#b27"">(Szegedy et al., 2013;</ref><ref type=""bibr"" target=""#b9"">Goodfellow et al., 2015)</ref>. <ref type=""bibr"" target=""#b22"">Papernot et al. (2016a;</ref><ref type=""bibr"">2017)</ref> have successfully used this method to attack commercial class s not the same as the Google Cloud Prediction API<ref type=""foot"" target=""#foot_5"">7</ref> (now the Google Cloud Machine Learning Engine) attacked in <ref type=""bibr"" target=""#b22"">Papernot et al. (2016a;</ref><ref type=""bibr"">2017)</ref>. Both systems are black-box, but the Prediction API is intend versary does not have access to the internals of the model architecture and has no knowledge of how the model was trained or what datasets were used. <ref type=""bibr"" target=""#b22"">Papernot et al. (2016a;</ref><ref type=""bibr"">2017)</ref> trained the Cloud Prediction API with small datasets like MNI",0
"sed to evaluate the proposed algorithm. Lately, we presented an intelligent video-based system for automated detection of suicide by hanging attempts <ref type=""bibr"" target=""#b1"">(Bouachir and Noumeir, 2016)</ref>. Unlike in <ref type=""bibr"" target=""#b9"">(Lee et al., 2014)</ref>, we performed our e",1
"posed in the aim of improving the existing video-surveillance systems to automatically detect suicidal behaviors and trigger an alarm. In this sense, <ref type=""bibr"" target=""#b9"">(Lee et al., 2014)</ref> presented a method for automatically analyzing depth images captured by an Asus Xtion Pro camer video-based system for automated detection of suicide by hanging attempts <ref type=""bibr"" target=""#b1"">(Bouachir and Noumeir, 2016)</ref>. Unlike in <ref type=""bibr"" target=""#b9"">(Lee et al., 2014)</ref>, we performed our experiments on a large dataset captured by an RGB-D camera, where 21 persons",1
"deling is often done by exploiting the spatial configuration of human body structure. This representation is derived from the principle, published in <ref type=""bibr"" target=""#b8"">(Johansson, 1973)</ref>, explaining how humans observe actions. This work demonstrates that humans are able to identify",0
"the Correctional Investigator of Canada, 2014). In addition, about one-third of prison deaths were caused by suicide in the USA between 2000 and 2010 <ref type=""bibr"" target=""#b10"">(Noonan and Ginder, 2013)</ref>. Generally, hanging is the most common method of inmate suicide, where inmates use clot",0
"instance, the space-time interest descriptors (Yang and Tian, 2014) were proposed to generalize the concept of interest points and local descriptors <ref type=""bibr"" target=""#b0"">(Bay et al., 2006)</ref>. Despite their effectiveness to overcome some global representation limitations, including nois",0
"st by splitting the dataset into testing data and training data, and rebalance the latter using the SMOTE: Synthetic Minority Over-sampling Technique <ref type=""bibr"" target=""#b2"">(Chawla et al., 2002)</ref> For each experiment, we applied the algorithm, depicted in Fig. <ref type=""figure"">3</ref>, etween the number of cases belonging to each class. To overcome this problem, we propose to use the SMOTE: Synthetic Minority Over-sampling Technique <ref type=""bibr"" target=""#b2"">(Chawla et al., 2002)</ref> in order to rebalance the training data set by oversampling the positive class (suicidal beh",0
"hanging attempts. For instance, a special protective clothing (Safety smocks and blankets) has been designed to be worn by actively suicidal inmates <ref type=""bibr"" target=""#b7"">(Hayes, 2013)</ref>. A top door alarm <ref type=""bibr"" target=""#b4"">(Cook, 2011)</ref>, which triggers an alarm if the d",0
"ty smocks and blankets) has been designed to be worn by actively suicidal inmates <ref type=""bibr"" target=""#b7"">(Hayes, 2013)</ref>. A top door alarm <ref type=""bibr"" target=""#b4"">(Cook, 2011)</ref>, which triggers an alarm if the door is used as a ligature point, allowing for a life-saving proactiv",0
"idual body parts. Numerous type of features have been used such as silhouette <ref type=""bibr"" target=""#b6"">(Gouiaa and Meunier, 2015)</ref>, contour <ref type=""bibr"" target=""#b3"">(Cheema et al., 2011)</ref> or optical flow <ref type=""bibr"" target=""#b5"">(Fathi and Mori, 2008)</ref>. Most proposed me",0
"whole human body without caring about identifying and labeling the individual body parts. Numerous type of features have been used such as silhouette <ref type=""bibr"" target=""#b6"">(Gouiaa and Meunier, 2015)</ref>, contour <ref type=""bibr"" target=""#b3"">(Cheema et al., 2011)</ref> or optical flow <ref",0
"espite their outstanding performance on several machine learning tasks, deep neural networks have been shown to be susceptible to adversarial attacks <ref type=""bibr"" target=""#b20"">(Szegedy et al., 2014;</ref><ref type=""bibr"" target=""#b4"">Goodfellow et al., 2015)</ref>. These attacks come in the for to a legitimate input sample. In the context of classification, these perturbations cause the legitimate sample to be misclassified at inference time <ref type=""bibr"" target=""#b20"">(Szegedy et al., 2014;</ref><ref type=""bibr"" target=""#b4"">Goodfellow et al., 2015;</ref><ref type=""bibr"" target=""#b16""> e the classifier more robust against attacks, e.g., adversarial training which augments the training data of the classifier with adversarial examples <ref type=""bibr"" target=""#b20"">(Szegedy et al., 2014;</ref><ref type=""bibr"" target=""#b4"">Goodfellow et al., 2015)</ref>, (2) modifying the training pr n the substitute network. It was found that such examples designed to fool the substitute often end up being misclassified by the targeted classifier <ref type=""bibr"" target=""#b20"">(Szegedy et al., 2014;</ref><ref type=""bibr"" target=""#b19"">Papernot et al., 2017)</ref>. In other words, black-box atta .1"">ADVERSARIAL TRAINING</head><p>A popular approach to defend against adversarial noise is to augment the training dataset with adversarial examples <ref type=""bibr"" target=""#b20"">(Szegedy et al., 2014;</ref><ref type=""bibr"" target=""#b4"">Goodfellow et al., 2015;</ref><ref type=""bibr"" target=""#b14"">",1
"| 2 −→ 0 (7)</formula><p>where G t is the generator of a GAN or WGAN<ref type=""foot"" target=""#foot_1"">1</ref> after t steps of its training algorithm <ref type=""bibr"" target=""#b8"">(Kabkab et al., 2018)</ref>.</p><p>This serves to show that, under ideal conditions, the addition of the GAN reconstruct",0
""">(Szegedy et al., 2014;</ref><ref type=""bibr"" target=""#b4"">Goodfellow et al., 2015;</ref><ref type=""bibr"" target=""#b16"">Papernot et al., 2016b;</ref><ref type=""bibr"" target=""#b11"">Liu et al., 2017)</ref>. Such perturbations are often small in magnitude and do not affect human recognition but can dr",0
"r network (encoder) for the MagNet baseline is provided in Table <ref type=""table"" target=""#tab_8"">7</ref>. Our implementation is based on TensorFlow <ref type=""bibr"" target=""#b0"">(Abadi et al., 2015)</ref> and builds on open-source software: CleverHans by <ref type=""bibr"" target=""#b15"">Papernot et",0
"isclassified at inference time <ref type=""bibr"" target=""#b20"">(Szegedy et al., 2014;</ref><ref type=""bibr"" target=""#b4"">Goodfellow et al., 2015;</ref><ref type=""bibr"" target=""#b16"">Papernot et al., 2016b;</ref><ref type=""bibr"" target=""#b11"">Liu et al., 2017)</ref>. Such perturbations are often small ck models exist, such as the Iterative FGSM <ref type=""bibr"" target=""#b9"">(Kurakin et al., 2017)</ref>, the Jacobian-based Saliency Map Attack (JSMA) <ref type=""bibr"" target=""#b16"">(Papernot et al., 2016b), and</ref><ref type=""bibr"">Deepfool (Moosavi-Dezfooli et al., 2016)</ref>, we focus on these t",0
"is often very costly or even infeasible in certain cases. In recent years, unsupervised learning has received increasing attention from the community <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b1"">2]</ref>.</p><p>Our novel approach to unsupervised learning stems from a few obse ferent: It learns the feature and thus the induced metric in an unsupervised fashion, without any human annotations.</p><p>Exemplar CNN. Exemplar CNN <ref type=""bibr"" target=""#b4"">[5]</ref> appears similar to our work. The fundamental difference is that it adopts a parametric paradigm during both tr s of these methods are reported with AlexNet architecture <ref type=""bibr"" target=""#b17"">[18]</ref> in their original papers, except for exemplar CNN <ref type=""bibr"" target=""#b4"">[5]</ref>, whose results are reported with ResNet-101 <ref type=""bibr"" target=""#b2"">[3]</ref>. As the network architectu",1
"br"" target=""#b27"">[28]</ref>, filling in missing parts of an image <ref type=""bibr"" target=""#b30"">[31]</ref>, recovering colors from grayscale images <ref type=""bibr"" target=""#b46"">[47]</ref>, or even solving a jigsaw puzzle <ref type=""bibr"" target=""#b26"">[27]</ref>. For videos, self-supervision str lized network (as a lower bound) and various unsupervised learning methods, including self-supervised learning <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b46"">47,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" target=""#b47"">48]</ref>, adversarial learning <ref ty",0
"in certain cases. In recent years, unsupervised learning has received increasing attention from the community <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b1"">2]</ref>.</p><p>Our novel approach to unsupervised learning stems from a few observations on the results of supervised l e=""bibr"" target=""#b28"">[29]</ref> to stabilize the learning process.</p><p>To evaluate the effectiveness of unsupervised learning, past works such as <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b30"">31]</ref> have relied on a linear classifier, e.g. Support Vector Machine (SVM), mpare our method with a randomly initialized network (as a lower bound) and various unsupervised learning methods, including self-supervised learning <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b46"">47,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" target=""#b47""> either an omitted aspect or component of an instance given the rest. To learn a representation of images, the tasks could be: predicting the context <ref type=""bibr"" target=""#b1"">[2]</ref>, counting the objects <ref type=""bibr"" target=""#b27"">[28]</ref>, filling in missing parts of an image <ref typ ers above and freezing all batch normalization layers. We follow the standard pipeline for finetuning and do not use the rescaling method proposed in <ref type=""bibr"" target=""#b1"">[2]</ref>. We use the standard trainval set in VOC 2007 for training and testing.</p><p>We compare three settings: 1) di >We evaluate the classification effectiveness based on the learned feature representation. A common practice <ref type=""bibr"" target=""#b47"">[48,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b30"">31]</ref> is to train an SVM on the learned feature over the training set, and to",0
"many more classes becomes infeasible. We tackle this challenge by approximating the full softmax distribution with noise-contrastive estimation (NCE) <ref type=""bibr"" target=""#b8"">[9]</ref>, and by resorting to a proximal regularization method <ref type=""bibr"" target=""#b28"">[29]</ref> to stabilize t . Popular techniques to reduce computation include hierarchical softmax <ref type=""bibr"" target=""#b25"">[26]</ref>, noise-contrastive estimation (NCE) <ref type=""bibr"" target=""#b8"">[9]</ref>, and negative sampling <ref type=""bibr"" target=""#b23"">[24]</ref>. We use NCE <ref type=""bibr"" target=""#b8"">[9] ise-contrastive estimation (NCE) <ref type=""bibr"" target=""#b8"">[9]</ref>, and negative sampling <ref type=""bibr"" target=""#b23"">[24]</ref>. We use NCE <ref type=""bibr"" target=""#b8"">[9]</ref> to approximate the full softmax.</p><p>We adapt NCE to our problem, in order to tackle the difficulty of compu",0
"neration methods have dealt with this problem by generating the entire sequence at once <ref type=""bibr"" target=""#b24"">[25]</ref> or in small batches <ref type=""bibr"" target=""#b19"">[20]</ref>. However, this introduces a lag in the generation process, prohibiting their use in real-time applications a n the generator and discriminator networks is able to capture temporal dependencies but requires fixed length videos. This limitation was overcome in <ref type=""bibr"" target=""#b19"">[20]</ref> but constraints need to be imposed in the latent space to generate consistent videos.</p><p>The MoCoGAN syst f type=""bibr"" target=""#b23"">24]</ref>.</p><p>Straight-forward adaptations of GANs for videos are proposed in <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b19"">20]</ref>, replacing the 2D convolutional layers with 3D convolutional layers. Using 3D convolutions in the generator a",1
"seamless transition between frames, challenging. Some video generation methods have dealt with this problem by generating the entire sequence at once <ref type=""bibr"" target=""#b24"">[25]</ref> or in small batches <ref type=""bibr"" target=""#b19"">[20]</ref>. However, this introduces a lag in the generat t limited to these applications and can be extended to handle videos <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b23"">24]</ref>.</p><p>Straight-forward adaptations of GANs for videos are proposed i ef type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b23"">24]</ref>.</p><p>Straight-forward adaptations of GANs for videos are proposed in <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b19"">20]</ref>, replacing the 2D convolutional layers with 3D convolutional layers.",1
"ble overhead when transferring to new speakers.</p><p>Subject independent approaches have been proposed that transform audio features to video frames <ref type=""bibr"" target=""#b4"">[5]</ref> but there is still no method to directly transform raw audio to video. Furthermore, many methods restrict the ents, and typically neglect the importance of generating natural facial expressions. Some methods generate frames based solely on present information <ref type=""bibr"" target=""#b4"">[5]</ref>, without taking into account the facial dynamics. This makes generating natural sequences, which are character conceptually broken into sub-networks responsible for capturing articulation dynamics and estimating the 3D points of the mesh. Finally, Chung et al. <ref type=""bibr"" target=""#b4"">[5]</ref> proposed a CNN applied on Mel-frequency cepstral coefficients (MFCCs) that generates subject independent video where ""bin"" is followed by the word ""blue"".   The works that are closest to ours are those proposed in <ref type=""bibr"" target=""#b21"">[22]</ref> and <ref type=""bibr"" target=""#b4"">[5]</ref>. The former method is subject dependent and requires a large amount of data for a specific person to generate ilable implementation so we compare our model to a static method that produces video frames using a sliding window of audio samples like that used in <ref type=""bibr"" target=""#b4"">[5]</ref>. This is a GAN-based method that uses a combination of an L 1 loss and an adversarial loss on individual frame nts are not only a result of using a temporal generator but also due to the use of the conditional Sequence Discriminator. Unlike previous approaches <ref type=""bibr"" target=""#b4"">[5]</ref> that prohibit the generation of facial expressions, the adversarial loss on the entire sequence encourages spo",0
"mapping audio features (e.g. MFCCs) to visual features (e.g. landmarks, visemes) and using computer graphics (CG) methods to generate realistic faces <ref type=""bibr"" target=""#b11"">[12]</ref>. Some methods avoid the use of CG by selecting frames from a person-specific database and combining them to s but are subject dependent and require retraining or re-targeting steps to adapt to new faces.</p><p>Convolutional neural networks (CNN) are used in <ref type=""bibr"" target=""#b11"">[12]</ref> to transform audio features to 3D meshes of a specific person. This system is conceptually broken into sub-n",0
") <ref type=""bibr"" target=""#b23"">[24]</ref>, which measures the average Euclidean distance of the still image representation, obtained using OpenFace <ref type=""bibr"" target=""#b0"">[1]</ref>, from the representation of the generated frames. The accuracy of the spoken message is measured using the wor",0
"trics penalize videos for any spontaneous expression. The frame sharpness is evaluated using the cumulative probability blur detection (CPBD) measure <ref type=""bibr"" target=""#b16"">[17]</ref>, which determines blur based on the presence of edges in the image and the frequency domain blurriness measu",0
"harper, more detailed images compared to L 1 and L 2 losses. However, GANs are not limited to these applications and can be extended to handle videos <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b23",0
"on to ensure that the identity of the speaker is maintained throughout the sequence. Evaluation is performed in a subject independent way on the GRID <ref type=""bibr"" target=""#b5"">[6]</ref> and TCD TIMIT <ref type=""bibr"" target=""#b10"">[11]</ref> datasets, where our model achieves truly natural resul",0
"ng a tuning procedure on the validation set. arg min</p><formula xml:id=""formula_2"">G max D L adv + λL L 1<label>(3)</label></formula><p>We used Adam <ref type=""bibr"" target=""#b12"">[13]</ref> for all the networks with a learning rate of 0.0002 for the Generator and 0.001 Frame Discriminator which de",0
"raw audio ensures that the method is subject independent. Similar deep architectures based on recurrent neural networks (RNNs) have been proposed in <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b21"">22]</ref>, producing realistic results but are subject dependent and require ret",0
"to boost accuracy without affecting the inference graph, including auxiliary training <ref type=""bibr"" target=""#b18"">[19]</ref>, multi-task learning <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b2"">3]</ref>, and knowledge distillation <ref type=""bibr"" target=""#b9"">[10]</ref>. Au lti-task learning is an approach to learn multiple related tasks simultaneously so that knowledge obtained from each task can be reused by the others <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b20"">21]</ref>. However, it is not useful for a n the same low layers, which is shown in Figure <ref type=""figure"" target=""#fig_1"">1 (c</ref>). This structure is very similar to multi-task learning <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b2"">3]</ref>, in which different supervised tasks share the same input, as well as so",1
"n developed by adding additional networks in the training graph to boost accuracy without affecting the inference graph, including auxiliary training <ref type=""bibr"" target=""#b18"">[19]</ref>, multi-task learning <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b2"">3]</ref>, and know Auxiliary training is introduced to improve the convergence of deep networks by adding auxiliary classifiers connected to certain intermediate layers <ref type=""bibr"" target=""#b18"">[19]</ref>. However, auxiliary classifiers require specific new designs for their network structures in addition to the ing subsections.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.1"">Generation of training graph</head><p>Similar to auxiliary training <ref type=""bibr"" target=""#b18"">[19]</ref>, we add several new classifier heads into the original network graph during training time. At inference time",1
"rget=""#b18"">[19]</ref>, multi-task learning <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b2"">3]</ref>, and knowledge distillation <ref type=""bibr"" target=""#b9"">[10]</ref>. Auxiliary training is introduced to improve the convergence of deep networks by adding auxiliary classifiers by transferring knowledge from another high-capacity model, so that the smaller one obtains better performance than that trained by using labels only <ref type=""bibr"" target=""#b9"">[10]</ref>. However, distillation is not an end-to-end solution due to having two separate training phases, which consum i (z (h) ; T ))</formula><p>which can be regarded as a distance measure between an average prediction from population and the prediction of each head <ref type=""bibr"" target=""#b9"">[10]</ref>. Minimizing this objective aims at transferring the information from the soft label to the logits and regular iveness on classification accuracy will be validated in Section 4.1.</p><p>Balance between hard and soft loss objectives. We follow the suggestion in <ref type=""bibr"" target=""#b9"">[10]</ref> that the backpropagation flow from each soft objective should be multiplied by T 2 since the magnitudes of th",0
"m ImageNet classification models. For instance, a more accurate classifier typically leads to a better object detection model based on the classifier <ref type=""bibr"" target=""#b11"">[12]</ref>.  Model weight distribution and mechanisms of ILR sharing. We have plotted the statistical distribution of e",0
"rained with collaborative learning is evaluated using the first classifier head without head selection. All experiments are conducted with Tensorflow <ref type=""bibr"" target=""#b0"">[1]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.1"">CIFAR Datasets</head><p>The two CIFAR datase",0
"eep one head with its dependent nodes and discard the rest. Therefore, the inference graph is identical to the original graph g.</p><p>It is shown in <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b4"">5]</ref> that the training memory size is roughly proportional to the number of",0
"ad needs to recalculate a new prediction after updating its parameters. In terms of convergence, recent work <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b15"">16]</ref> reveals that simultaneous SGD has faster convergence and achieves better performance than the alternative one",0
"s/1.0""><head n=""4.2"">ImageNet Dataset</head><p>The ILSVRC 2012 classification dataset consists of 1.2 million for training, and 50,000 for validation <ref type=""bibr"" target=""#b5"">[6]</ref>. We evaluate how collaborative learning helps improve the performance of ResNet-50 network.</p><p>As following",0
"ectively. We conduct empirical studies on the CIFAR-10 dataset with ResNet-32, ResNet-110 <ref type=""bibr"" target=""#b8"">[9]</ref>, and DenseNet-40-12 <ref type=""bibr"" target=""#b10"">[11]</ref>. ResNets and DenseNets for CIFAR are all designed to have three building blocks, residual or dense blocks. F",0
"e inference graph, including auxiliary training <ref type=""bibr"" target=""#b18"">[19]</ref>, multi-task learning <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b2"">3]</ref>, and knowledge distillation <ref type=""bibr"" target=""#b9"">[10]</ref>. Auxiliary training is introduced to impro n Figure <ref type=""figure"" target=""#fig_1"">1 (c</ref>). This structure is very similar to multi-task learning <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b2"">3]</ref>, in which different supervised tasks share the same input, as well as some ILR. However, collaborative learning rn multiple related tasks simultaneously so that knowledge obtained from each task can be reused by the others <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b20"">21]</ref>. However, it is not useful for a single task use case. Knowledge distil",0
"gt;</formula><p>Objective Function S e &lt; l a t e x i t s h a 1 _ b a s e 6 4 = "" t A  We use primitives from an existing code generation framework <ref type=""bibr"" target=""#b8"">[9]</ref> to form S e . Our search space includes multi-level tiling on each loop axis, loop ordering, shared memory cac",1
"information (e.g., memory access count and data reuse ratio) and generic annotations (e.g., vectorization, unrolling, thread binding). We use XGBoost <ref type=""bibr"" target=""#b6"">[7]</ref>, which has proven to be a strong feature-based model in past problems. Our second model is a TreeGRU <ref type",0
"workload evaluation. We evaluated real world end-to-end DL inference workloads, including ResNet <ref type=""bibr"" target=""#b13"">[14]</ref>, MobileNet <ref type=""bibr"" target=""#b15"">[16]</ref>, LSTM Language Model <ref type=""bibr"" target=""#b43"">[44]</ref>, Deep Q Network (DQN) <ref type=""bibr"" target",0
"models. Scalable learning systems <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b1"">2]</ref> rely on manually optimized, high-performance tensor operation libraries, such as cuDNN, that are optimized for",0
"""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" target=""#b34"">35,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b24"">25]</ref> but with several key differentiating characteristics:</p><p>Relatively Low Experiment Cost. Traditionally, hy hat were 2× faster than those found with random searches. This result is particularly interesting compared to prior results in hyper-parameter tuning <ref type=""bibr"" target=""#b24"">[25]</ref>, where model-based approaches were shown to work only as well as random searching. Our statistical models be",0
"W (D i , D j ) is large). Thus, we try to leverage the structure of G to improve the global embedding.We use an unsupervised autoencoder architecture <ref type=""bibr"" target=""#b22"">[14,</ref><ref type=""bibr"" target=""#b34"">26]</ref> to learn from the local linkage graph.</p><p>Graph Auto-encoder. Let embeddings. Z incorporates information from both global and local context. In our implementation, we use a variational version of graph auto-encoder <ref type=""bibr"" target=""#b22"">[14]</ref> by assuming Z is generated from a latent Gaussian distribution, hence, Equation ( <ref type=""formula"" target",1
"<label>(3)</label></formula><p>Instead of projecting to a single point, triplet loss enables documents with the same identity to reside on a manifold <ref type=""bibr"" target=""#b28"">[20]</ref>, and at the same time maintain a distance from other documents. The architecture of global metric learning w",1
"on Criterion which cannot handle a complex mixture of data (hence tend to over merge the data when the number of clusters is large) in high dimension <ref type=""bibr"" target=""#b15"">[7]</ref>. Second, it involves an iterative trial of potential splits which is not e cient enough on large datasets. To",0
"arly better performance (+7-35% in terms of F1-score) than several state-of-the-art methods including GHOST [5], Zhang et al. [33], and Louppe et al. <ref type=""bibr"" target=""#b25"">[17]</ref>.</p><p>Finally, the algorithm has been deployed in AMiner to deal with the disambiguation problem at the bil -the-art methods including GHOST <ref type=""bibr"" target=""#b41"">[33]</ref>, Zhang et al. <ref type=""bibr"" target=""#b41"">[33]</ref>, and Louppe et al. <ref type=""bibr"" target=""#b25"">[17]</ref> (+7-35% in terms of F1-score). The automatically estimated number of persons by our proposed model is close . Han et al. <ref type=""bibr"" target=""#b16"">[8]</ref> present supervised disambiguation methods based on SVM and Naïve Bayes. Moreover, Louppe et al. <ref type=""bibr"" target=""#b25"">[17]</ref> use a classi er to learn pairwise similarity and perform semi-supervised hierarchical clustering to generate two nodes is measured based on the number of valid paths. The nal clustering result is generated by a nity propagation algorithm.</p><p>Louppe et al. <ref type=""bibr"" target=""#b25"">[17]</ref>: This method rst trains a pairwise distance function base on a set of carefully designed similarity features",0
"ne matching constraint, where sim(C i , C j ) is de ned as the Jaccard coe cient. An optimal one-to-one mapping is found using Kuhn-Munkres algorithm <ref type=""bibr"" target=""#b23"">[15]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.4"">Human in the Loop</head><p>The automatical",0
"construction error between the predicted Ã and the original adjacency matrix A.</p><p>We instantiate 1 as a two-layer graph convolution network (GCN) <ref type=""bibr"" target=""#b21"">[13]</ref> due to its e ectiveness for modeling networked data:</p><formula xml:id=""formula_11"">1 (Y, A) = AReLU( AYW 0",0
"r name disambiguation problem can be divided into two categories: feature-based and linkage-based.</p><p>Feature-based methods. Feature-based methods <ref type=""bibr"" target=""#b16"">[8,</ref><ref type=""bibr"" target=""#b18"">10,</ref><ref type=""bibr"" target=""#b30"">22,</ref><ref type=""bibr"" target=""#b40"" =""bibr"" target=""#b40"">[32]</ref> propose a two-stage clustering method to learn better feature representation via the rst clustering step. Han et al. <ref type=""bibr"" target=""#b16"">[8]</ref> present supervised disambiguation methods based on SVM and Naïve Bayes. Moreover, Louppe et al. <ref type=""bi",0
"edge across di erent species <ref type=""bibr"" target=""#b38"">[30]</ref>, constructing canonicalized knowledge base based on facts extracted from texts <ref type=""bibr"" target=""#b14"">[6]</ref>, and identifying users across multiple online social networks <ref type=""bibr"" target=""#b42"">[34]</ref>. Howe",0
"neric graph data, we instead propose to transform graphs into grid-like data to enable the use of CNNs directly. This idea was previously explored in <ref type=""bibr"" target=""#b19"">[20]</ref>. However, the transformation in <ref type=""bibr"" target=""#b19"">[20]</ref> is implemented in the preprocessin ta to enable the use of CNNs directly. This idea was previously explored in <ref type=""bibr"" target=""#b19"">[20]</ref>. However, the transformation in <ref type=""bibr"" target=""#b19"">[20]</ref> is implemented in the preprocessing process while our method includes the transformation in the networks. Ad",1
"atically decide what kind of features to extract by learning the weights in these trainable filters, thereby avoiding hand-crafted feature extraction <ref type=""bibr"" target=""#b28"">[29]</ref>.</p><p>In many real-world applications, the data can be naturally represented as graphs, such as social, cit",0
"lutional networks (LGCNs) for graph node classification. We build LGCNs based on the architecture of densely connected convolutional networks (DCNNs) <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b12"">13]</ref>, which achieved state-of-the-art performance in the ImageNet classif",0
"Consequently, the weights are not learned. Graph attention networks (GATs) <ref type=""bibr"" target=""#b27"">[28]</ref> employed the attention mechanism <ref type=""bibr"" target=""#b0"">[1]</ref> to obtain different and trainable weights for adjacent nodes by measuring the correlation between their featur e=""bibr"" target=""#b27"">[28]</ref> tried to enable learnable weights when aggregating neighboring feature vectors by employing the attention mechanism <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b26"">27]</ref>. Like GCNs, each node still has a local filter with a receptive field",0
"training, the model only use the training graphs without access to validation and testing graphs. We use the proteinprotein interaction (PPI) dataset <ref type=""bibr"" target=""#b30"">[31]</ref>, which contains 20 graphs for training, 2 graphs for validation, and 2 graphs for testing. Since the graphs le"" xml:id=""tab_2""><head>Table 1 :</head><label>1</label><figDesc>Summary of datasets used in our experiments<ref type=""bibr"" target=""#b29"">[30,</ref><ref type=""bibr"" target=""#b30"">31]</ref>. The Cora, Citeseer, and Pubmed datasets are used for transductive learning experiments, while the PPI datase",0
"edicted rank for this document, we apply Weston et al <ref type=""bibr"" target=""#b35"">[36]</ref>'s sequential sampling, and do it in a parallel manner <ref type=""bibr"" target=""#b15"">[16]</ref>. As described in Algorithm 1, for the r -th document π r , if we want to know its rank in a list of N docume",1
"1, K],<label>(6)</label></formula><p>where r is the rank range from 1 to K. As pointed out above, this scheme pre-determines the weight. Rendle et al <ref type=""bibr"" target=""#b28"">[29]</ref> proposed an empirical weight for sampling a single position from a top-K ranking, following a geometric dist",1
"instead of its rank without computing relevance scores for all documents. To get the student predicted rank for this document, we apply Weston et al <ref type=""bibr"" target=""#b35"">[36]</ref>'s sequential sampling, and do it in a parallel manner <ref type=""bibr"" target=""#b15"">[16]</ref>. As describe",1
"veness and good representation power <ref type=""bibr"" target=""#b20"">[21]</ref>. Recently, with the great impact of neural networks on computer vision <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b21"">22]</ref> and natural language processing <ref type=""bibr"" target=""#b18"">[19,<",0
"etter performance. Several works in information retrieval followed this direction, using weak-labeled or unlabeled data to construct test collections <ref type=""bibr"" target=""#b1"">[2]</ref>, to provide extra features <ref type=""bibr"" target=""#b10"">[11]</ref> and labels <ref type=""bibr"" target=""#b9"">",0
"ut replacement ŷd ← M S (q, d; θ S ) if ŷd &gt; ŷπ r then n ← n + 1 end if end for rπ r ← n×(| Ō |−1)</formula><p>ϵ + 1 return rπ r from noise labels <ref type=""bibr"" target=""#b25"">[26]</ref>, we use a weighted sum over the loss on documents from π 1..K with weight w r on each position r from 1 to K",0
"ast models for candidate generation and applying time-consuming models to the candidates for online inferences <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b23"">24]</ref>. These methods either lose much of effectiveness, due to the introduced model constraints, or cannot be easil",0
"phase when responding to user requests due to the larger model size.</p><p>Balancing effectiveness and efficiency has been a line of recent research <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b33"">34,</ref><ref type=""bibr"" target=""#b38"">39,</ref><ref type=""bibr"" target=""#b40 ance score for a given (q,d) pair. Other works focus on database-related methods, such as pruning and indexing to speed-up retrieval of related items <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b33"">34]</ref>, using fast models for candidate generation and applying time-consum",0
"through convex optimization. The objectives of latent factor models and neural networks are usually non-convex <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b17"">18]</ref>, which means that their training processes are more challenging and need more attentions. The goal of ranking",0
"for better inference efficiency while retaining the model effectiveness <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b14"">15]</ref>. The idea of KD is shown in Figure <ref type=""figure"">1a</ref>  from the training set, and a smaller student student model. The student model trained with KD has an effectiveness comparable to that of the teacher model <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b19"">20]</ref> and can make more efficient online inference due to its small model s veral related research areas. Knowledge Distillation Knowledge distillation has been used in image recognition <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b30"">31]</ref> and neural machine translation <ref type=""bibr"" target=""#b19"">[20]</r having more training data, side information, human-defined rules etc., the model can be trained with more guidance and has less variance in gradients <ref type=""bibr"" target=""#b14"">[15]</ref>. Figure <ref type=""figure"" target=""#fig_1"">2b</ref> shows that, when more training instances are sampled fro",0
"eving the manual feature engineering work required by other approaches. Several successful ranking models with neural networks have been investigated <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" target=""#b34"">35,</ref><ref type=""bibr"" target=""#b36 first two are more widely adopted, we don't discuss list-wise loss in this work. The point-wise loss is widely used when relevance labels are binary <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b32"">33]</ref>. One typical point-wise loss is taking the negative logarithmic of t",0
"works have been investigated <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" target=""#b34"">35,</ref><ref type=""bibr"" target=""#b36"">37]</ref>. However, the size of such models (in terms of the number of model parameters) increases by an order of magni ce. Evaluation Metrics. As in <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b36"">37]</ref>, three different evaluation metrics used are Precision@n (Prec@n), nDCG@n, and Mean Average Precision (MAP).",0
"""bibr"" target=""#b33"">34]</ref>, using fast models for candidate generation and applying time-consuming models to the candidates for online inferences <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b23"">24]</ref>. These methods either lose much of effectiveness, due to the introduce",0
"ledge transfer, is a modelindependent strategy for generating a compact model for better inference efficiency while retaining the model effectiveness <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b14"">15]</ref>. The idea of KD is shown in Figu onal supervision to the training of the student model. The student model trained with KD has an effectiveness comparable to that of the teacher model <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b19"">20]</ref> and can make more efficient on rst, we train a larger teacher model M T by minimizing a ranking-based loss with the groundtruth ranking from the training data set, as showed in Eqn <ref type=""bibr"" target=""#b0"">(1)</ref>. With much more parameters in this model, it captures more patterns from data and thus has a strong performanc >Here ŷ is the student model's predicted scores <ref type=""foot"" target=""#foot_1"">1</ref> . L R (, ) stands for the ranking-based objective as in Eqn <ref type=""bibr"" target=""#b0"">(1)</ref>. The distillation loss, denoted by L D (, ), uses teacher model's top-K ranking on unlabeled documents to guid odeLs (Fossil) <ref type=""bibr"" target=""#b12"">[13]</ref> models sequential patterns and user Table <ref type=""table"">2</ref>: Performance comparison. <ref type=""bibr"" target=""#b0"">(1)</ref> The performance of the models with ranking distillation, Fossil-RD and Caser-RD, always has statistically sign",0
"this direction, using weak-labeled or unlabeled data to construct test collections <ref type=""bibr"" target=""#b1"">[2]</ref>, to provide extra features <ref type=""bibr"" target=""#b10"">[11]</ref> and labels <ref type=""bibr"" target=""#b9"">[10]</ref> for ranking model training. The basic idea of ranking di",0
"><p>Researchers have reported multi-task learning models can improve model predictions on all tasks by utilizing regularization and transfer learning <ref type=""bibr"" target=""#b7"">[8]</ref>. However, in practice, multi-task learning models do not always outperform the corresponding single-task model e need of adding many new parameters per task.</p><p>The backbone of MMoE is built upon the most commonly used Shared-Bottom multi-task DNN structure <ref type=""bibr"" target=""#b7"">[8]</ref>. The Shared-Bottom model structure is shown in Figure <ref type=""figure"" target=""#fig_0"">1</ref> (a), where se introduce the shared-bottom multi-task model in Figure <ref type=""figure"" target=""#fig_0"">1</ref> (a), which is a framework proposed by Rich Caruana <ref type=""bibr"" target=""#b7"">[8]</ref> and widely adopted in many multi-task learning applications <ref type=""bibr"" target=""#b17"">[18,</ref><ref type del parameters are extensively shared among all tasks.</p><p>Prior works <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b7"">8]</ref> investigated task di erences in multi-task learning by assuming particular data generation processes for each t e ectiveness of multitask models <ref type=""bibr"" target=""#b3"">[4]</ref><ref type=""bibr"" target=""#b4"">[5]</ref><ref type=""bibr"" target=""#b5"">[6]</ref><ref type=""bibr"" target=""#b7"">8]</ref>.</p><p>Instead of sharing hidden layers and same model parameters across tasks, some recent approaches add di e i erences across di erent tasks. Doing so can result in both improved e ciency and model quality for each task <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b29"">30]</ref>. One of the widely used multi-task learning models is proposed by Carua f type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b29"">30]</ref>. One of the widely used multi-task learning models is proposed by Caruana <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b8"">9]</ref>, which has a shared-bottom model structure, where the bottom hidden laye",1
"multi-task learning approach based on a novel Multi-gate Mixture-of-Experts (MMoE) structure, which is inspired by the Mixture-of-Experts (MoE) model <ref type=""bibr"" target=""#b20"">[21]</ref> and the recent MoE layer <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b30"">31]</ref>. <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4"">MODELING APPROACHES 4.1 Mixture-of-Experts</head><p>The Original Mixture-of-Experts (MoE) Model <ref type=""bibr"" target=""#b20"">[21]</ref> can be formulated as:</p><formula xml:id=""formula_5"">= n i=1 (x ) i f i (x ),<label>(5)</label></formula><p>",1
"-Experts (MMoE) structure, which is inspired by the Mixture-of-Experts (MoE) model <ref type=""bibr"" target=""#b20"">[21]</ref> and the recent MoE layer <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b30"">31]</ref>. MMoE explicitly models the task relationships and learns task-speci een proven to be able to improve model performance <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b19"">20]</ref>.</p><p>Eigen et al <ref type=""bibr"" target=""#b15"">[16]</ref> and Shazeer et al <ref type=""bibr"" target=""#b30"">[31]</ref> turn the mixture-of-experts model into basic bui ted sum of the outputs of all experts.</p><p>MoE Layer : While MoE was rst developed as an ensemble method of multiple individual models, Eigen et al <ref type=""bibr"" target=""#b15"">[16]</ref> and Shazeer et al <ref type=""bibr"" target=""#b30"">[31]</ref> turn it into basic building blocks (MoE layer) a s to a successive layer. The whole model is then trained in an end-to-end way.</p><p>The main goal of the MoE layer structure proposed by Eigen et al <ref type=""bibr"" target=""#b15"">[16]</ref> and Shazeer et al <ref type=""bibr"" target=""#b30"">[31]</ref> is to achieve conditional computation <ref type=",0
"riments in section 3. (1) For all models, the performance on the data with higher correlation is better than that on the data with lower correlation. <ref type=""bibr"" target=""#b1"">(2)</ref> The gap between performances on data with di erent correlations of the MMoE model is much smaller than that of he census-income data.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""6.3.1"">Dataset Description.</head><p>The UCI census-income dataset <ref type=""bibr"" target=""#b1"">[2]</ref> is extracted from the 1994 census database. It contains 299,285 instances of demographic information of Americ",0
"red by the Mixture-of-Experts (MoE) model <ref type=""bibr"" target=""#b20"">[21]</ref> and the recent MoE layer <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b30"">31]</ref>. MMoE explicitly models the task relationships and learns task-speci c functionalities to leverage shared rep =""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b19"">20]</ref>.</p><p>Eigen et al <ref type=""bibr"" target=""#b15"">[16]</ref> and Shazeer et al <ref type=""bibr"" target=""#b30"">[31]</ref> turn the mixture-of-experts model into basic building blocks (MoE layer) and stack them in a DNN. The MoE la le MoE was rst developed as an ensemble method of multiple individual models, Eigen et al <ref type=""bibr"" target=""#b15"">[16]</ref> and Shazeer et al <ref type=""bibr"" target=""#b30"">[31]</ref> turn it into basic building blocks (MoE layer) and stack them in a DNN. The MoE layer has the same structure an end-to-end way.</p><p>The main goal of the MoE layer structure proposed by Eigen et al <ref type=""bibr"" target=""#b15"">[16]</ref> and Shazeer et al <ref type=""bibr"" target=""#b30"">[31]</ref> is to achieve conditional computation <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b11""> ll the tasks. Moreover, this model has the potential to achieve even better computational eciency by making the gating network as a sparse top-k gate <ref type=""bibr"" target=""#b30"">[31]</ref>. We hope this work inspire other researchers to further investigate multi-task modeling using these approach",0
"his model is not only more powerful in modeling but also lowers computation cost by introducing sparsity into the gating networks. Similarly, PathNet <ref type=""bibr"" target=""#b16"">[17]</ref>, which is designed for arti cial general intelligence to handle di erent tasks, is a huge neural network wit",0
"ns. This relates to recent discoveries that modulation and gating mechanisms can improve the trainability in training non-convex deep neural networks <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b18"">19]</ref>.</p><p>We further evaluate the performance of MMoE on a benchmark dat their trainability, i.e., how robust the model is within a range of hyper-parameter settings and model initializations.</p><p>Recently, Collins et al <ref type=""bibr"" target=""#b9"">[10]</ref> nd that some gated RNN models (like LSTM and GRU) we thought to perform better than the vanilla RNN are simpl ww.tei-c.org/ns/1.0""><head n=""6.2"">Hyper-Parameter Tuning</head><p>We adopt a hyper-parameter tuner, which is used in recent deep learning frameworks <ref type=""bibr"" target=""#b9"">[10]</ref>, to search the best hyperparameters for all the models in the experiments with real datasets. The tuning algo",0
"tion and manipulated di erent types of task relatedness so as to evaluate the e ectiveness of multitask models <ref type=""bibr"" target=""#b3"">[4]</ref><ref type=""bibr"" target=""#b4"">[5]</ref><ref type=""bibr"" target=""#b5"">[6]</ref><ref type=""bibr"" target=""#b7"">8]</ref>.</p><p>Instead of sharing hidden",0
"ttp://www.tei-c.org/ns/1.0""><head n=""2.3"">Multi-task Learning Applications</head><p>Thanks to the development of distributed machine learning systems <ref type=""bibr"" target=""#b12"">[13]</ref>, many large-scale real-world applications have adopted DNN-based multi-task learning algorithms and observed",0
"gen et al <ref type=""bibr"" target=""#b15"">[16]</ref> and Shazeer et al <ref type=""bibr"" target=""#b30"">[31]</ref> is to achieve conditional computation <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b11"">12]</ref>, where only parts of a network are active on a per-example basis. For",0
"e=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b28"" ce in a real-world environment.</p><p>In terms of algorithm design, our work is most closely related to Hamilton et al. (2017a)'s GraphSAGE algorithm <ref type=""bibr"" target=""#b17"">[18]</ref> and the closely related follow-up work of <ref type=""bibr"" target=""#b7"">Chen et al. (2018)</ref>  <ref type= n are two-fold. First, selecting a fixed number of nodes to aggregate from allows us to control the memory footprint of the algorithm during training <ref type=""bibr"" target=""#b17"">[18]</ref>. Second, it allows Algorithm 1 to take into account the importance of neighbors when aggregating the vector n as a symmetric aggregation function (i.e., γ = mean); • mean-pooling-xent is the same as mean-pooling but uses the cross-entropy loss introduced in <ref type=""bibr"" target=""#b17"">[18]</ref>. • mean-pooling-hard is the same as mean-pooling, except that it incorporates hard negative samples as detai ooling in the convolution step.</p><p>The max-pooling and cross-entropy settings are extensions of the best-performing GCN model from Hamilton et al. <ref type=""bibr"" target=""#b17"">[18]</ref>-other variants (e.g., based on Kipf et al. <ref type=""bibr"" target=""#b20"">[21]</ref>) performed significantl",1
"=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b28"">29,</ref><ref type=""bibr"" target=""#b29"">30]</ref>.</p><p>Most prominent among these recent advancements is the success ""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b28"">29,</ref><ref type=""bibr"" target=""#b30"">31]</ref>, leading to new state-of-the-art results on benchmarks such as node c onvolutional Networks (GCNs) <ref type=""bibr"" target=""#b18"">[19,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b28"">29]</ref>. The core idea behind GCNs is to learn how to iteratively aggregate feature information from local graph neig oot"" n=""4"" xml:id=""foot_4"">Note that the recent GCN-based recommender systems of Monti et al.<ref type=""bibr"" target=""#b23"">[24]</ref> and Berg et al.<ref type=""bibr"" target=""#b28"">[29]</ref> are not directly comparable because they cannot scale to the Pinterest size data.</note> 			<note xmlns=""htt",0
"tract> 		</profileDesc> 	</teiHeader> 	<text xml:lang=""en""> 		<body> <div xmlns=""http://www.tei-c.org/ns/1.0""> <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b11"">12]</ref><p>. The representations learned using deep models can be used to complement, or even replace, traditional rec ks have already proven state-of-the-art performance of deep learning approaches for generating such embeddings <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b23"">24]</ref>. We also conduct ablation studies and consider several variants of Pi",0
"to be 1024.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>Computation resources. Training of PinSage is implemented in</head><p>TensorFlow <ref type=""bibr"" target=""#b0"">[1]</ref> and run on a single machine with 32 cores and 16 Tesla K80 GPUs. To ensure fast fetching of item's visual and",0
"""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b28"">29,</ref><ref type=""bibr"" target=""#b30"">31]</ref>, leading to new state-of-the-art results on benchmarks such as node classification, link prediction, as well ds to applications ranging from recommender systems <ref type=""bibr"" target=""#b23"">[24]</ref> to drug design <ref type=""bibr"" target=""#b19"">[20,</ref><ref type=""bibr"" target=""#b30"">31]</ref>. <ref type=""bibr"">Hamilton et al. (2017b)</ref>  <ref type=""bibr"" target=""#b18"">[19]</ref> and <ref type=""bib",0
"ify the underlying mechanisms and dynamics of social influence. Indeed, extensive work has been done on social influence prediction in the literature <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b31"">32,</ref><ref type=""bibr"" target=""#b41"">42,</ref><ref type=""bibr"" target=""#b42 social influence by carefully designing differential equations extended from the classic 'Susceptible-Infected' (SI) model; Most recently, Li et al. <ref type=""bibr"" target=""#b25"">[26]</ref> proposed an end-toend predictor for inferring cascade size by incorporating recurrent neural network (RNN) a arget=""#b42"">[43]</ref>. Recently, there have been efforts to detect those global patterns automatically using deep learning, e.g., the DeepCas model <ref type=""bibr"" target=""#b25"">[26]</ref> which formulate cascade prediction as a sequence problem and solve it with Recurrent Neural Network. Another",1
"5"">[46]</ref>. The above observations inspire a lot of user-level influence prediction models, most of which <ref type=""bibr"" target=""#b26"">[27,</ref><ref type=""bibr"" target=""#b52"">53,</ref><ref type=""bibr"" target=""#b53"">54]</ref> consider complicated hand-crafted features, which require extensive k ced only by their near neighbors within the network, while external sources are assumed to be not present.</p><p>Problem 1. Social Influence Locality <ref type=""bibr"" target=""#b52"">[53]</ref> Social influence locality models the probability of v's action status conditioned on her r -ego network G r bibr"" target=""#b52"">[53,</ref><ref type=""bibr"" target=""#b53"">54]</ref> Weibo 6 is the most popular Chinese microblogging service. The dataset is from <ref type=""bibr"" target=""#b52"">[53]</ref> and can be downloaded here. 7 The complete dataset contains the directed following networks and tweets (post k is defined to be the Twitter friendship network, and the social action is defined to be whether a user retweets ""Higgs"" related tweets.</p><p>Weibo <ref type=""bibr"" target=""#b52"">[53,</ref><ref type=""bibr"" target=""#b53"">54]</ref> Weibo 6 is the most popular Chinese microblogging service. The datas n Weibo -a user forwards (retweets) a post (tweet).</p><p>Data Preparation We process the above four datasets following the practice in existing work <ref type=""bibr"" target=""#b52"">[53,</ref><ref type=""bibr"" target=""#b53"">54]</ref>. More concretely, for a user v who was influenced to perform a socia",1
"er which maps a user u to her D-dimensional representation x u ∈ R D , as shown in Figure <ref type=""figure"">2(b)</ref>.</p><p>Instance Normalization <ref type=""bibr"" target=""#b46"">[47]</ref> Instance normalization is a recently proposed technique in image style transfer <ref type=""bibr"" target=""#b4 p><p>Instance Normalization <ref type=""bibr"" target=""#b46"">[47]</ref> Instance normalization is a recently proposed technique in image style transfer <ref type=""bibr"" target=""#b46"">[47]</ref>. We adopt this technique in our social influence prediction task. As shown in Figure <ref type=""figure"">2</r",0
"on д in Eq. 5). All the parameters are initialized with Glorot initialization <ref type=""bibr"" target=""#b17"">[18]</ref> and trained using the Adagrad <ref type=""bibr"" target=""#b15"">[16]</ref> optimizer with learning rate 0.1 (0.05 for Digg dataset), weight decay 5e −4 (1e −3 for Digg dataset), and d",0
"above is prevalent in real-world applications whereas its complexity and non-linearity have frequently been observed, such as the ""S-shaped"" curve in <ref type=""bibr"" target=""#b1"">[2]</ref> v Figure <ref type=""figure"">1</ref>: A motivating example of social influence locality prediction. The goal is =""bibr"" target=""#b35"">[36]</ref>, 64-dim).</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>Ego</head><p>The number/ratio of active neighbors <ref type=""bibr"" target=""#b1"">[2]</ref>. Density of subgnetwork induced by active neighbors <ref type=""bibr"" target=""#b45"">[46]</ref>. #Connected comp rk.</p><p>A natural choice of the sampling method is to perform random walk with restart (RWR) <ref type=""bibr"" target=""#b44"">[45]</ref>. Inspired by <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b45"">46]</ref> which suggest that people are more likely to be influenced by active n he homophily assumption may not be true. By averaging over neighbors, GCN may mix predictive signals with noise. On the other hand, as pointed out by <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b45"">46]</ref>, active neighbors are more important than inactive neighbors, which al r"" target=""#b18"">[19,</ref><ref type=""bibr"" target=""#b38"">39]</ref>, topic-level influence <ref type=""bibr"" target=""#b41"">[42]</ref>, group formation <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b37"">38]</ref> and structural diversity <ref type=""bibr"" target=""#b13"">[14,</ref><ref",0
"spire a lot of user-level influence prediction models, most of which <ref type=""bibr"" target=""#b26"">[27,</ref><ref type=""bibr"" target=""#b52"">53,</ref><ref type=""bibr"" target=""#b53"">54]</ref> consider complicated hand-crafted features, which require extensive knowledge of specific domains and are usu network, and the social action is defined to be whether a user retweets ""Higgs"" related tweets.</p><p>Weibo <ref type=""bibr"" target=""#b52"">[53,</ref><ref type=""bibr"" target=""#b53"">54]</ref> Weibo 6 is the most popular Chinese microblogging service. The dataset is from <ref type=""bibr"" target=""#b52"" t (tweet).</p><p>Data Preparation We process the above four datasets following the practice in existing work <ref type=""bibr"" target=""#b52"">[53,</ref><ref type=""bibr"" target=""#b53"">54]</ref>. More concretely, for a user v who was influenced to perform a social action a at some timestamp t, we genera Academic Graph (OAG), Digg, Twitter, and Weibo. We compare DeepInf with several conventional methods such as linear models with hand-crafted features <ref type=""bibr"" target=""#b53"">[54]</ref> as well as the state-ofthe-art graph classification model <ref type=""bibr"" target=""#b33"">[34]</ref>. Experim chieved datasets are facing data imbalance problems in two respects. The first comes from the number of active neighbors. As observed by Zhang et al. <ref type=""bibr"" target=""#b53"">[54]</ref>, structural features become significantly correlated with social influence locality when the ego user has a",0
"predictive signals in social influence automatically. By architecting network embedding <ref type=""bibr"" target=""#b36"">[37]</ref>, graph convolution <ref type=""bibr"" target=""#b24"">[25]</ref>, and graph attention mechanism <ref type=""bibr"" target=""#b48"">[49]</ref> into a unified framework, we expect Also, the input layer covers all other customized vertex features such as structural features, content features, and demographic features.</p><p>GCN <ref type=""bibr"" target=""#b24"">[25]</ref> Based Network Encoding Graph Convolutional Network (GCN) is a semi-supervised learning algorithm for graphst epInf-GCN, as shown in Table <ref type=""table"" target=""#tab_6"">5</ref>. Previously, we have seen the success of GCN in may label classification tasks <ref type=""bibr"" target=""#b24"">[25]</ref>. However, in this application, DeepInf-GCN achieves the worst performance over all the methods. We attribute ecently, there have been several attempts to incorporate semi-supervised information into graph representation learning. Typical examples include GCN <ref type=""bibr"" target=""#b24"">[25]</ref>, GraphSAGE <ref type=""bibr"" target=""#b20"">[21]</ref>, and the state-of-the-art model GAT <ref type=""bibr"" ta",0
"esearcher cites a paper from the above conferences. We are interested in how one's citation behaviors are influenced by her collaborators.</p><p>Digg <ref type=""bibr"" target=""#b22"">[23]</ref> Digg is a news aggregator which allows people to vote web content, a.k.a, story, up or down. The dataset con",0
"losely related to a large body of literature on social influence analysis <ref type=""bibr"" target=""#b41"">[42]</ref> and graph representation learning <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b36"">37]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>Social Inf",0
"d-crafted egonetwork features. The features we used are listed in Table <ref type=""table"" target=""#tab_3"">2</ref>.</p><p>Support Vector Machine (SVM) <ref type=""bibr"" target=""#b16"">[17]</ref> We also use support vector machine (SVM) with linear kernel as the classification model. The model use the s",0
"models, it is important to have the ability to learn both long-term interests and short-term interests of such implicit feedbacks. As Jannach et al. <ref type=""bibr"" target=""#b6"">[7]</ref> noted that both the users' short-term and long-term interests are of great importance for recommendation, but",1
"e-of-the-art in SRS research field <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b9"">10]</ref>. Hidasi et al. <ref type=""bibr"" target=""#b4"">[5]</ref> use deep recurrent neural networks with a gated recurre item embeddings in a current session, which gives each item a fixed weight based on the relative distance with response to the target item. Li et al. <ref type=""bibr"" target=""#b9"">[10]</ref> propose an RNN based encoder-decoder model (NARM), which takes the last hidden state from the RNN as the sequ ing from the CIKM Cup 2016 2 , for which only the transaction data is used in this study.</p><p>Following <ref type=""bibr"" target=""#b4"">[5]</ref> and <ref type=""bibr"" target=""#b9"">[10]</ref>, we filter out sessions of length 1 and items that appear less than 5 times in both of the datasets. The test",1
"where recommendation systems can only suggest a few items at once, the relevant item should be amongst the first few items in the recommendation list <ref type=""bibr"" target=""#b11"">[12]</ref>. We therefore evaluate the recommendation quality in terms of P@5, MRR@5, P@10 and MRR@10 in trying to simul",0
"just been clicked by a user and recorded in a session, it is highly likely that the user's next intended action is in response to the current action. <ref type=""bibr"" target=""#b0"">(1)</ref> If the current action is to browse the product description before making a purchase decision, then the user is paper, we propose a short-term attention/memory priority model for session-based recommendations. Two important findings can be made from the study: <ref type=""bibr"" target=""#b0"">(1)</ref> The next move of a user is mostly affected by the last-click of a session prefix, and our model can effectivel t way. Also the hidden state in the last time step contains information about the sequence with a strong focus on the parts nearest to the next click <ref type=""bibr"" target=""#b0"">[1]</ref>, thus some general interest features of items with a long distance may be forgotten. To solve this problem, a",0
"pplication of RNNs in session-based recommendation tasks has led to significant progress in the past few years <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b16"">17]</ref>. Although RNN models have been proven useful in capturing users' general interests from a sequence of actions apability provided by the RNNs, their model can take into account the users' historical behavior when making predictions of the next move. Tan et al. <ref type=""bibr"" target=""#b16"">[17]</ref>propose a data augmentation technique to improve the performance of the RNNs for session-based recommendation f 31,637,239 clicks on 37,483 items in Yoochoose dataset, and 202,633 sessions of 982,961 clicks on 43,097 items in Diginetica dataset.</p><p>Same as <ref type=""bibr"" target=""#b16"">[17]</ref>, we use a sequence splitting preprocess that for an input session S = {s 1 , s 2 , . . . , s n }, we generat raining set is quite large and training on the recent fractions yields better results than training on the entire fractions as per the experiments of <ref type=""bibr"" target=""#b16"">[17]</ref>, we use the recent fractions 1/64 and 1/4 of training sequences. The statistics of the three datasets are sh",0
"fective in modeling sequential data recently <ref type=""bibr"" target=""#b8"">[9]</ref>. Inspired by recent advances in natural language processing area <ref type=""bibr"" target=""#b15"">[16]</ref>, some deep learning based solutions have been developed and some of which represent the state-of-the-art in",0
"global information conveyed by the session context.</p><p>Deep neural networks have proven to be very effective in modeling sequential data recently <ref type=""bibr"" target=""#b8"">[9]</ref>. Inspired by recent advances in natural language processing area <ref type=""bibr"" target=""#b15"">[16]</ref>, so",0
"ortance for recommendation, but traditional RNN architectures are not designed to distinguish and exploit these two types of interests simultaneously <ref type=""bibr"" target=""#b10"">[11]</ref>.</p><p>In this study, we consider to solve this problem by introducing a recent action priority mechanism in",0
"ref type=""bibr"" target=""#b16"">17]</ref>. Although RNN models have been proven useful in capturing users' general interests from a sequence of actions <ref type=""bibr"" target=""#b19"">[20]</ref>, learning to predict from sessions is still a challenging problem to tackle largely due to the uncertainty i ibr"" target=""#b16"">[17]</ref>propose a data augmentation technique to improve the performance of the RNNs for session-based recommendation. Yu et al. <ref type=""bibr"" target=""#b19"">[20]</ref>propose a dynamic recurrent model, which applies RNN to learn dynamic representation for each basket for user e session. A constraint is included to avoid coincidental high similarities between rarely visited items as in <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b19"">20]</ref>. </p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.3"">Evaluation</head><p>We use the following m",0
"e product description before making a purchase decision, then the user is very likely to visit another digital camera brand catalog in the next move. <ref type=""bibr"" target=""#b1"">(2)</ref> If the current action is to add a camera into the shopping cart, then the user's browsing interest is likely b affected by the last-click of a session prefix, and our model can effectively utilize such information through the temporal interests representation. <ref type=""bibr"" target=""#b1"">(2)</ref> The proposed attention mechanism can effectively capture long-term and short-term interests of a session, empi rget=""#b15"">[16]</ref>, some deep learning based solutions have been developed and some of which represent the state-of-the-art in SRS research field <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b9"">10]</",0
"separately. <ref type=""bibr"" target=""#b15"">(Lai et al., 2015)</ref> firstly model sentences by RNN, and then use CNN to get the final representation. <ref type=""bibr"" target=""#b21"">Shi et al. (2016)</ref> replace convolution filters with deep LSTM, which is similar to what is proposed in this paper. more general and can make use of different kinds of recurrent units. We find that using GRU as recurrent units outperforms LSTM which is utilized by <ref type=""bibr"" target=""#b21"">Shi et al. (2016)</ref>.</p><formula xml:id=""formula_0"">w 1 w 2 w 3 w 4 h 1 h 2 h 3 h 4 RNN (a) RNN p w 1 w 2 w 3 w 4 p",1
"the advantages of CNN and RNN. <ref type=""bibr"" target=""#b28"">(Xiao and Cho, 2016)</ref> extract local and global features by CNN and RNN separately. <ref type=""bibr"" target=""#b15"">(Lai et al., 2015)</ref> firstly model sentences by RNN, and then use CNN to get the final representation. <ref type=""b",0
"our models.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>Implementation Details</head><p>We tokenize all the corpus with NLTK's tokenizer <ref type=""bibr"" target=""#b1"">(Bird and Loper, 2004)</ref>. We limit the vocabulary size of each dataset as shown in Table <ref type=""table"">3</ref>.",0
"eep neural networks have shown great success in many NLP tasks such as machine translation <ref type=""bibr"" target=""#b0"">(Bahdanau et al., 2015;</ref><ref type=""bibr"" target=""#b25"">Tu et al., 2016</ref><ref type=""bibr"">), reading comprehension (Hermann et al., 2015)</ref>, sentiment classification <",0
"n is a fundamental and traditional task in natural language processing (NLP), which can be applied in various applications such as sentiment analysis <ref type=""bibr"" target=""#b23"">(Tang et al., 2015)</ref>, question classification <ref type=""bibr"" target=""#b35"">(Zhang and Lee, 2003)</ref> and topic ><ref type=""bibr"" target=""#b25"">Tu et al., 2016</ref><ref type=""bibr"">), reading comprehension (Hermann et al., 2015)</ref>, sentiment classification <ref type=""bibr"" target=""#b23"">(Tang et al., 2015)</ref>, etc. Nowadays, nearly most of deep neural networks models are based on CNN or RNN. Below, we the optimal window size is 40 or even larger. The result is intuitive, because sentiment analysis such as Yelp often involves long-term dependencies <ref type=""bibr"" target=""#b23"">(Tang et al., 2015)</ref>, while topic classification such as AG and DBPedia relys more on the key phrases.</p><p>From Recurrent Neural Networks RNN is suitable for handling sequence input like natural language. Thus, many RNN variants are used in text classification. <ref type=""bibr"" target=""#b23"">Tang et al. (2015)</ref> utilize LSTM to model the relation of sentences. Similarly, <ref type=""bibr"" target=""#b30"">Yan",0
"former system <ref type=""bibr"" target=""#b41"">(Rozovskaya et al., 2014)</ref> uses a classifier-based approach, which is improved by the latter system <ref type=""bibr"" target=""#b44"">(Rozovskaya and Roth, 2016)</ref> through combining with an SMT-based approach.</p><p>• NUS14, NUS16 and NUS17: GEC sys ethods. Recently, many novel approaches <ref type=""bibr"" target=""#b50"">(Susanto et al., 2014;</ref><ref type=""bibr"">Chollampatt et al., 2016b,a;</ref><ref type=""bibr"" target=""#b44"">Rozovskaya and Roth, 2016;</ref><ref type=""bibr"" target=""#b26"">Junczys-Dowmunt and Grundkiewicz, 2016;</ref><ref type=""",1
"</div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.1"">Back-boost learning</head><p>Back-boost learning borrows the idea from back translation <ref type=""bibr"" target=""#b49"">(Sennrich et al., 2016)</ref> in NMT, referring to training a backward model (we call it error generation model, as opp",1
"candidates generated, the more helpful for training an error correction model. Inspired by <ref type=""bibr"" target=""#b22"">He et al. (2016)</ref> and <ref type=""bibr"" target=""#b61"">Zhang et al. (2018)</ref>, we propose a dual-boost learning strategy, combining both back-and selfboost's perspectives",1
"br"">Madnani et al., 2011;</ref><ref type=""bibr"" target=""#b12"">Dahlmeier and Ng, 2012c;</ref><ref type=""bibr"" target=""#b33"">Napoles et al., 2015;</ref><ref type=""bibr"" target=""#b46"">Sakaguchi et al., 2016;</ref><ref type=""bibr"" target=""#b34"">Napoles et al., 2016;</ref><ref type=""bibr"" target=""#b3"">Br",0
"ns/1.0""><head n=""6"">Related work</head><p>Most of advanced GEC systems are classifierbased <ref type=""bibr"" target=""#b4"">(Chodorow et al., 2007;</ref><ref type=""bibr"" target=""#b15"">De Felice and Pulman, 2008;</ref><ref type=""bibr"" target=""#b21"">Han et al., 2010;</ref><ref type=""bibr"" target=""#b28"">L",0
"ader> 	<text xml:lang=""en""> 		<body> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""1"">Introduction</head><p>Sequence-to-sequence (seq2seq) models <ref type=""bibr"" target=""#b1"">(Cho et al., 2014;</ref><ref type=""bibr"" target=""#b51"">Sutskever et al., 2014)</ref> for grammatical error correction (G l GEC approach uses a Recurrent Neural Network (RNN) based encoder-decoder seq2seq model <ref type=""bibr"" target=""#b51"">(Sutskever et al., 2014;</ref><ref type=""bibr"" target=""#b1"">Cho et al., 2014)</ref> with attention mechanism <ref type=""bibr"" target=""#b1"">(Bahdanau et al., 2014)</ref> to edit a r seq model <ref type=""bibr"" target=""#b51"">(Sutskever et al., 2014;</ref><ref type=""bibr"" target=""#b1"">Cho et al., 2014)</ref> with attention mechanism <ref type=""bibr"" target=""#b1"">(Bahdanau et al., 2014)</ref> to edit a raw sentence into the grammatically correct sentence it should be, as Figure <re",0
"b15"">De Felice and Pulman, 2008;</ref><ref type=""bibr"" target=""#b21"">Han et al., 2010;</ref><ref type=""bibr"" target=""#b28"">Leacock et al., 2010;</ref><ref type=""bibr"" target=""#b53"">Tetreault et al., 2010a;</ref><ref type=""bibr"" target=""#b14"">Dale and Kilgarriff, 2011)</ref> 7 The state-of-the-art re",0
"or correction (GEC) have drawn growing attention <ref type=""bibr"">(Yuan and Briscoe, 2016;</ref><ref type=""bibr"" target=""#b56"">Xie et al., 2016;</ref><ref type=""bibr"" target=""#b24"">Ji et al., 2017;</ref><ref type=""bibr"" target=""#b48"">Schmaltz et al., 2017;</ref><ref type=""bibr"" target=""#b47"">Sakaguc een proposed for GEC. Among them, seq2seq models <ref type=""bibr"">(Yuan and Briscoe, 2016;</ref><ref type=""bibr"" target=""#b56"">Xie et al., 2016;</ref><ref type=""bibr"" target=""#b24"">Ji et al., 2017;</ref><ref type=""bibr"" target=""#b47"">Sakaguchi et al., 2017;</ref><ref type=""bibr"" target=""#b48"">Schmal /1.0""><head n=""5"">Experiments</head></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""5.1"">Dataset and evaluation</head><p>As previous studies <ref type=""bibr"" target=""#b24"">(Ji et al., 2017)</ref>, we use the public Lang-8 Corpus <ref type=""bibr"" target=""#b31"">(Mizumoto et al., 2011;</ref><r /ref>. It uses a rule-based method to synthesize errors for data augmentation.</p><p>• Nested-seq2seq: a nested attention neural hybrid seq2seq model <ref type=""bibr"" target=""#b24"">(Ji et al., 2017)</ref>.</p><p>• Adapt-seq2seq: a seq2seq model adapted to incorporate edit operations <ref type=""bibr""",0
"correction model. Intuitively, the more diverse disfluency candidates generated, the more helpful for training an error correction model. Inspired by <ref type=""bibr"" target=""#b22"">He et al. (2016)</ref> and <ref type=""bibr"" target=""#b61"">Zhang et al. (2018)</ref>, we propose a dual-boost learning s",0
"ystems are classifierbased <ref type=""bibr"" target=""#b4"">(Chodorow et al., 2007;</ref><ref type=""bibr"" target=""#b15"">De Felice and Pulman, 2008;</ref><ref type=""bibr"" target=""#b21"">Han et al., 2010;</ref><ref type=""bibr"" target=""#b28"">Leacock et al., 2010;</ref><ref type=""bibr"" target=""#b53"">Tetreau",0
"(Brockett et al., 2006;</ref><ref type=""bibr"">Dahlmeier and</ref><ref type=""bibr"">Ng, 2011, 2012a;</ref><ref type=""bibr"">Yoshimoto et al., 2013;</ref><ref type=""bibr"" target=""#b60"">Yuan and Felice, 2013;</ref><ref type=""bibr"">Behera and Bhattacharyya, 2013)</ref>. For example, top-performing systems",0
"=""#b21"">Han et al., 2010;</ref><ref type=""bibr"" target=""#b28"">Leacock et al., 2010;</ref><ref type=""bibr"" target=""#b53"">Tetreault et al., 2010a;</ref><ref type=""bibr"" target=""#b14"">Dale and Kilgarriff, 2011)</ref> 7 The state-of-the-art result on CoNLL-2014 dataset has been recently advanced by Chol",0
"through combining with an SMT-based approach.</p><p>• NUS14, NUS16 and NUS17: GEC systems <ref type=""bibr"" target=""#b50"">(Susanto et al., 2014;</ref><ref type=""bibr"" target=""#b5"">Chollampatt et al., 2016a;</ref><ref type=""bibr"" target=""#b6"">Chollampatt and Ng, 2017</ref>) that combine SMT with othe",0
"b51"">Sutskever et al., 2014)</ref> for grammatical error correction (GEC) have drawn growing attention <ref type=""bibr"">(Yuan and Briscoe, 2016;</ref><ref type=""bibr"" target=""#b56"">Xie et al., 2016;</ref><ref type=""bibr"" target=""#b24"">Ji et al., 2017;</ref><ref type=""bibr"" target=""#b48"">Schmaltz et target=""#b57"">Yannakoudakis et al., 2017)</ref> have been proposed for GEC. Among them, seq2seq models <ref type=""bibr"">(Yuan and Briscoe, 2016;</ref><ref type=""bibr"" target=""#b56"">Xie et al., 2016;</ref><ref type=""bibr"" target=""#b24"">Ji et al., 2017;</ref><ref type=""bibr"" target=""#b47"">Sakaguchi et ref><ref type=""bibr"">Roth, 2010, 2011;</ref><ref type=""bibr"" target=""#b45"">Rozovskaya et al., 2012;</ref><ref type=""bibr"">Felice and Yuan, 2014;</ref><ref type=""bibr"" target=""#b56"">Xie et al., 2016;</ref><ref type=""bibr"" target=""#b39"">Rei et al., 2017)</ref>. Moreover, we propose fluency boost infer highest attention weight.</p><p>We resolve spelling errors with a public spell checker<ref type=""foot"" target=""#foot_2"">4</ref> as preprocessing, as <ref type=""bibr"" target=""#b56"">Xie et al. (2016)</ref> and <ref type=""bibr"" target=""#b47"">Sakaguchi et al. (2017)</ref> do.</p></div> <div xmlns=""http b6"">Chollampatt and Ng, 2017</ref>) that combine SMT with other techniques (e.g., classifiers).</p><p>• Char-seq2seq: a character-level seq2seq model <ref type=""bibr"" target=""#b56"">(Xie et al., 2016)</ref>. It uses a rule-based method to synthesize errors for data augmentation.</p><p>• Nested-seq2se",0
"ss. The utility of a document is estimated based on the MDP state, which consists of the query, the preceding documents, and the remaining candidates <ref type=""bibr"" target=""#b32"">[33]</ref>.</p><p>The greedy sequential document selection simpli es the ranking process and can accelerate the online and AlphaGo Zero <ref type=""bibr"" target=""#b27"">[28]</ref> for the Game of Go, in this paper we propose to enhance the MDP model for diverse ranking <ref type=""bibr"" target=""#b32"">[33]</ref> with the Monte Carlo tree search (MCTS), for alleviating the suboptimal ranking problem. The new ranking mod pic information explicitly with the attention mechanism <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b14"">15]</ref>. Xia et al. <ref type=""bibr"" target=""#b32"">[33]</ref> proposed to model the dynamics of the document utility with MDP and learning the model parameters with polic coded as a four hidden decision making states. In <ref type=""bibr"" target=""#b37"">[38]</ref>, the log-based document re-ranking is modeled as a POMDP. <ref type=""bibr"" target=""#b32"">[33]</ref> and <ref type=""bibr"" target=""#b29"">[30]</ref> propose to model the process of constructing a document rankin ns/1.0""><head n=""5.1"">Experimental settings</head><p>In our experiments, for e ective training of the model parameters and following the practices in <ref type=""bibr"" target=""#b32"">[33]</ref>, we combined four TREC datasets and constructed a new dataset with 200 queries and in total about 45,000 lab type=""bibr"" target=""#b31"">[32]</ref>: a learning approach which automatically learns novelty features based on neural tensor networks.</p><p>MDP-DIV <ref type=""bibr"" target=""#b32"">[33]</ref>: a state-of-the-art learning approach which uses an MDP for modeling the diverse ranking process. Following target=""#b32"">[33]</ref>: a state-of-the-art learning approach which uses an MDP for modeling the diverse ranking process. Following the practice in <ref type=""bibr"" target=""#b32"">[33]</ref>, we con gured the reward function in MDP-DIV as α-DCG and the discounting parameter γ = 1.</p><p>M 2 Div, an nd the baselines of MDP-DIV and NTN-DIV need preliminary representations of the queries and the documents as their inputs. Following the practices in <ref type=""bibr"" target=""#b32"">[33]</ref>, in the experiments we used the query vector and document vector generated by the doc2vec <ref type=""bibr"" t",1
"s. <ref type=""bibr"" target=""#b36"">[37]</ref> formalizes the interactively optimizing of information retrieval systems as a dueling bandit problem and <ref type=""bibr"" target=""#b15"">[16]</ref> proposes cascading bandits to identify K most attractive document for users. See also <ref type=""bibr"" targe",0
"2"">[3]</ref> uses the sum of the query-document relevance and the maximal document distance (referred to as marginal relevance) as the utility. xQuAD <ref type=""bibr"" target=""#b24"">[25]</ref> de nes the utility so as to explicitly account for the relationship between documents retrieved for the orig",0
"for the original query and the possible subqueries. In recent years, machine learning based methods have been proposed for conducting diverse ranking <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b30"">31,</ref><ref type=""bibr"" target=""#b31"">32,</ref><ref type=""bibr"" target=""#b33 the basis of training queries. Some researchers de ne the utility as a linear combination of the handcrafted relevance features and novelty features <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b30"">31,</ref><ref type=""bibr"" target=""#b33"">34,</ref><ref type=""bibr"" target=""#b38 ters with policy gradient. Other learning approaches please refer to <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b33"">34,</ref><ref type=""bibr"" target=""#b34"">35,</ref><ref type=""bibr"" target=""#b36"" levance ranking, respectively. Muti-armed bandit, another type of reinforcement learning model, is also widely applied to rank learning. For example, <ref type=""bibr"" target=""#b22"">[23]</ref> proposes two online learning bandit algorithms to learn a diverse ranking of documents based on users clicki",0
"hborhood aggregation (or ""message passing"" scheme), and those have been very promising <ref type=""bibr"" target=""#b17"">(Kipf &amp; Welling, 2017;</ref><ref type=""bibr"" target=""#b6"">Hamilton et al., 2017;</ref><ref type=""bibr"" target=""#b4"">Gilmer et al., 2017;</ref><ref type=""bibr"" target=""#b32"">Velič res in the neighborhood <ref type=""bibr"" target=""#b28"">(Shervashidze et al., 2011;</ref><ref type=""bibr"" target=""#b17"">Kipf &amp; Welling, 2017;</ref><ref type=""bibr"" target=""#b6"">Hamilton et al., 2017)</ref>.</p><p>Yet, such aggregation schemes sometimes lead to surprises. For example, it has been /p><formula xml:id=""formula_3"">h (l) v = ReLU W l • u∈ N (v) (deg(v)deg(u)) −1/2 h (l−1) u (2)</formula><p>where deg(v) is the degree of node v in G. <ref type=""bibr"" target=""#b6"">Hamilton et al. (2017)</ref> derived a variant of GCN that also works in inductive settings (previously unseen nodes), b ic model. The COMBINE step is key to this paradigm and can be viewed as a form of a ""skip connection"" between different layers.For COMBINE, GraphSAGE <ref type=""bibr"" target=""#b6"">(Hamilton et al., 2017)</ref> uses concatenation after a feature transform.</p><p>Column Networks <ref type=""bibr"" targe =""bibr"" target=""#b12"">(Hoshen, 2017)</ref> learn to select the important neighbors via an attention mechanism. The max-pooling operation in GraphSAGE <ref type=""bibr"" target=""#b6"">(Hamilton et al., 2017)</ref> implicitly selects the important nodes. This line of work is orthogonal to ours, because i different subjects. The dataset contains bag-of-words features for each document (node) and citation links (edges) between documents. (II) On Reddit <ref type=""bibr"" target=""#b6"">(Hamilton et al., 2017)</ref>, the task is to predict the community to which different Reddit posts belong. Reddit is an nected if some user commented on both posts. The dataset contains word vectors as node features. (III) For protein-protein interaction networks (PPI) <ref type=""bibr"" target=""#b6"">(Hamilton et al., 2017)</ref> We compare against three baselines: Graph Convolutional Networks (GCN) <ref type=""bibr"" ta f> We compare against three baselines: Graph Convolutional Networks (GCN) <ref type=""bibr"" target=""#b17"">(Kipf &amp; Welling, 2017)</ref>, Graph-SAGE <ref type=""bibr"" target=""#b6"">(Hamilton et al., 2017)</ref> and Graph Attention Networks (GAT) <ref type=""bibr"" target=""#b32"">(Veličković et al., 2018 ion or LSTM-attention as the last aggregation layer gives 6 JK-Net variants. We follow exactly the same setting of GraphSAGE as in the original paper <ref type=""bibr"" target=""#b6"">(Hamilton et al., 2017)</ref>, where the model consists of 2 hidden layers, each with 128 hidden units and is trained wi in the Reddit dataset were explicitly chosen from the well-behaved middle-sized communities to avoid the noisy cores and tree-like small communities <ref type=""bibr"" target=""#b6"">(Hamilton et al., 2017)</ref>. As a result, this graph is more regular than the original Reddit data, and hence not exhi",1
"#b6"">Hamilton et al., 2017;</ref><ref type=""bibr"" target=""#b4"">Gilmer et al., 2017;</ref><ref type=""bibr"" target=""#b32"">Veličković et al., 2018;</ref><ref type=""bibr"" target=""#b15"">Kearnes et al., 2016)</ref>. These models learn to iteratively aggregate the hidden features of every node in the graph",0
"Work</head><p>Spectral graph convolutional neural networks apply convolution on graphs by using the graph Laplacian eigenvectors as the Fourier atoms <ref type=""bibr"" target=""#b0"">(Bruna et al., 2014;</ref><ref type=""bibr"" target=""#b29"">Shuman et al., 2013;</ref><ref type=""bibr"" target=""#b3"">Defferr",0
"rning, Stockholm, Sweden, PMLR 80, 2018.</ref> Copyright 2018 by the author(s). 2017; <ref type=""bibr"" target=""#b5"">Grover &amp; Leskovec, 2016;</ref><ref type=""bibr"" target=""#b30"">Tang et al., 2015)</ref>.</p><p>Recent works focus on deep learning approaches to node representation. Many of these ap",0
"ose have been very promising <ref type=""bibr"" target=""#b17"">(Kipf &amp; Welling, 2017;</ref><ref type=""bibr"" target=""#b6"">Hamilton et al., 2017;</ref><ref type=""bibr"" target=""#b4"">Gilmer et al., 2017;</ref><ref type=""bibr"" target=""#b32"">Veličković et al., 2018;</ref><ref type=""bibr"" target=""#b15"">Ke target=""#b7"">(Hammond et al., 2011;</ref><ref type=""bibr"" target=""#b3"">Defferrard et al., 2016)</ref>, are a specific instantiation of this framework <ref type=""bibr"" target=""#b4"">(Gilmer et al., 2017)</ref>, of the form</p><formula xml:id=""formula_3"">h (l) v = ReLU W l • u∈ N (v) (deg(v)deg(u)) −1/",0
"tions of expansion more than the others.</p><p>Graph Attention Networks (GAT) <ref type=""bibr"" target=""#b32"">(Veličković et al., 2018)</ref> and VAIN <ref type=""bibr"" target=""#b12"">(Hoshen, 2017)</ref> learn to select the important neighbors via an attention mechanism. The max-pooling operation in G",0
"raph Convolutional Networks (GCN) <ref type=""bibr"" target=""#b17"">(Kipf &amp; Welling, 2017)</ref>, initially motivated by spectral graph convolutions <ref type=""bibr"" target=""#b7"">(Hammond et al., 2011;</ref><ref type=""bibr"" target=""#b3"">Defferrard et al., 2016)</ref>, are a specific instantiation o",0
"ormal packet transmission.</p><p>Device-free Tracking. Various hardware <ref type=""bibr"" target=""#b0"">[2]</ref><ref type=""bibr"" target=""#b1"">[3]</ref><ref type=""bibr"" target=""#b2"">[4]</ref><ref type=""bibr"" target=""#b9"">11]</ref> are manufactured to capture extremely weak human reflections. WiTrack <",1
"only 20 MHz, resulting in small phase change by ToF and erroneous ToF estimation. In revision, we plan to combine multiple Wi-Fi NICs on one receiver <ref type=""bibr"" target=""#b7"">[9]</ref>, and splice multiple channels <ref type=""bibr"" target=""#b35"">[37]</ref> for fine-grained AoA and ToF estimatio ve research in the last decade. Generally, it requires objects to carry devices that transmits RF signals, and calculates signal parameters, e.g. AoA <ref type=""bibr"" target=""#b7"">[9,</ref><ref type=""bibr"" target=""#b10"">12,</ref><ref type=""bibr"" target=""#b21"">23,</ref><ref type=""bibr"" target=""#b22"">",0
"sing research interests recently <ref type=""bibr"" target=""#b1"">[3,</ref><ref type=""bibr"" target=""#b3"">5,</ref><ref type=""bibr"" target=""#b14"">16,</ref><ref type=""bibr"" target=""#b15"">17]</ref>.</p><p>Techniques based on visible light <ref type=""bibr"" target=""#b42"">[44]</ref> and depth imaging [1] have ref type=""bibr"" target=""#b9"">[11]</ref> Widar <ref type=""bibr"" target=""#b19"">[21]</ref> Dy. Music <ref type=""bibr"" target=""#b14"">[16]</ref> IndoTrack <ref type=""bibr"" target=""#b15"">[17]</ref> LiFS <ref type=""bibr"" target=""#b30"">[32]</ref> Widar2  <ref type=""bibr"" target=""#b14"">[16,</ref><ref type=""b 0 NICs for CSI collection. We evaluate Widar2.0 in three different indoor scenarios and compare it with two state-of-the-art methods namely IndoTrack <ref type=""bibr"" target=""#b15"">[17]</ref> and DynamicMusic <ref type=""bibr"" target=""#b14"">[16]</ref>. Experimental results show that, using a single W by subtracting a constant value ? from CSI amplitudes of all sensors, and adding a constant value ? to CSI amplitudes of the reference sensor, as in <ref type=""bibr"" target=""#b15"">[17]</ref>. When m m 0 , we have</p><formula xml:id=""formula_15"">|P n (m)P * l (m 0 )| = (|? n | -?)|? l | &lt;&lt; |? gate multiplication of CSIs for phase calibration is first proposed in WiDance <ref type=""bibr"" target=""#b20"">[22]</ref>, and then used in In-doTrack <ref type=""bibr"" target=""#b15"">[17]</ref> for tracking. However, these works only focus on estimate of DFS, while Widar2.0 addresses all signal parame ion 5.3.</p><p>Comparative study. We compare Widar2.0 with the state-ofthe-arts, DynamicMusic <ref type=""bibr"" target=""#b14"">[16]</ref> and IndoTrack <ref type=""bibr"" target=""#b15"">[17]</ref>. Specifically, Dy-namicMusic uses JADE to estimate AoAs of signals reflected by the target at receivers, and rs from accumulative error. DynamicMusic <ref type=""bibr"" target=""#b14"">[16]</ref> estimates AoA of reflection from CSI by MUSIC algorithm. IndoTrack <ref type=""bibr"" target=""#b15"">[17]</ref> further incorporates AoA with DFS for successive tracking. Differently, Widar2.0 employs a maximum likelihoo timation of multiple parameters including AoA, ToF, DFS, and attenuation. Most of previous works exploit only signal power and DFS for motion sensing <ref type=""bibr"" target=""#b15"">[17,</ref><ref type=""bibr"" target=""#b19"">21,</ref><ref type=""bibr"" target=""#b30"">32]</ref> since they are the easiest t",0
"ail analytics, etc. As such, device-free passive localization without any device attached to the user attracts increasing research interests recently <ref type=""bibr"" target=""#b1"">[3,</ref><ref type=""bibr"" target=""#b3"">5,</ref><ref type=""bibr"" target=""#b14"">16,</ref><ref type=""bibr"" target=""#b15"">17 multipath environments indoors, previous works either rely on specialized hardware or software-defined radios <ref type=""bibr"" target=""#b0"">[2,</ref><ref type=""bibr"" target=""#b1"">3,</ref><ref type=""bibr"" target=""#b11"">13]</ref>, rendering them not ubiquitously applicable, or</p></div> <div xmlns=""h omparison of state-of-the-art works for passive WiFi tracking</head></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>Properties</head><p>WiTrack <ref type=""bibr"" target=""#b1"">[3]</ref> WiDeo <ref type=""bibr"" target=""#b9"">[11]</ref> Widar <ref type=""bibr"" target=""#b19"">[21]</ref> Dy. Music <ref ates ToF of reflection path with only normal packet transmission.</p><p>Device-free Tracking. Various hardware <ref type=""bibr"" target=""#b0"">[2]</ref><ref type=""bibr"" target=""#b1"">[3]</ref><ref type=""bibr"" target=""#b2"">[4]</ref><ref type=""bibr"" target=""#b9"">11]</ref> are manufactured to capture extr ><ref type=""bibr"" target=""#b9"">11]</ref> are manufactured to capture extremely weak human reflections. WiTrack <ref type=""bibr"" target=""#b0"">[2,</ref><ref type=""bibr"" target=""#b1"">3]</ref> develops FMCW radar to accurately estimate ToFs of reflections in frequency domain. WiDeo <ref type=""bibr"" targ",0
"hardware are difficult to be generalized, researches are shifted to ubiquitous COTS RF devices, such as RFID <ref type=""bibr"" target=""#b24"">[26,</ref><ref type=""bibr"" target=""#b25"">27,</ref><ref type=""bibr"" target=""#b31"">33,</ref><ref type=""bibr"" target=""#b40"">42]</ref>, millimetre wave <ref type=""b",0
"f type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b4"">5]</ref>. Conventional studies concentrate on the area of multilingual acoustic modeling by the contextdependent deep ne f type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b4"">5]</ref> for a long time, these researches are commonly limited to making acoustic model (AM) multilingual, which requir tilingual ASR. These shared hidden layers and language dependent softmax layers of SHL-MDNN are optimized jointly by multilingual datasets. SHL-MLSTM <ref type=""bibr"" target=""#b4"">[5]</ref> further explores long short-term memory (LSTM) <ref type=""bibr"" target=""#b6"">[7]</ref> with residual learning quence <ref type=""bibr"" target=""#b12"">[13]</ref> under the condition of language information being known during training. A comparison with SHL-MLSTM <ref type=""bibr"" target=""#b4"">[5]</ref> with residual learning is investigated on CALL-HOME datasets with 6 languages. Experimental results reveal tha ub-words is 8062.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.4."">Results</head><p>The baseline systems come from our previous work <ref type=""bibr"" target=""#b4"">[5]</ref> and all results are summarized in Table <ref type=""table"" target=""#tab_3"">5</ref>. First, we train six monolin",1
"ed pronunciation lexicon. Recent researches on sequence-to-sequence attention-based models try to remove this dependency on the pronunciation lexicon <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b9"">10]</ref>. <ref type=""bibr"">Chiu et al.</re",0
"icon. The ASR Transformer is chosen to be the basic architecture of sequence-to-sequence attention-based model <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b11"">12]</ref>. To alleviate the problem of few training data on low-resource languages, a well-trained ASR Transformer from sformer model architecture</head><p>The ASR Transformer architecture used in this work is the same as our work <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b11"">12]</ref> which is shown in Figure <ref type=""figure"" target=""#fig_0"">1</ref>. It stacks multihead attention (MHA) <ref",0
"ody> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""1."">Introduction</head><p>Multilingual speech recognition has been investigated for many years <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b3"">4,</r on 5.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2."">Related work</head><p>Although multilingual speech recognition has been studied <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b3"">4,</r ture transformation through multiple layers of nonlinearity, which can be used to extract universal feature transformation from multilingual datasets <ref type=""bibr"" target=""#b0"">[1]</ref>. Among the CD-DNN-HMM based approaches, the architecture of SHL-MDNN <ref type=""bibr"" target=""#b0"">[1]</ref>, ransformation from multilingual datasets <ref type=""bibr"" target=""#b0"">[1]</ref>. Among the CD-DNN-HMM based approaches, the architecture of SHL-MDNN <ref type=""bibr"" target=""#b0"">[1]</ref>, in which the hidden layers are shared across multiple languages while the softmax layers are language depende",0
"ref><ref type=""bibr"" target=""#b11"">12]</ref> which is shown in Figure <ref type=""figure"" target=""#fig_0"">1</ref>. It stacks multihead attention (MHA) <ref type=""bibr"" target=""#b16"">[17]</ref> and position-wise, fully connected layers for both the encode and decoder. The encoder is composed of a stac the decoder, the self-attention sub-layers in the decoder mask out all values corresponding to illegal connections. In addition, positional encodings <ref type=""bibr"" target=""#b16"">[17]</ref> are added to the input at the bottoms of these encoder and decoder stacks, which inject some information abo about the relative or absolute position of the tokens in the sequence.</p><p>The difference between the neural machine translation (NMT) Transformer <ref type=""bibr"" target=""#b16"">[17]</ref> and the ASR Transformer is the input of the encoder. We add a linear transformation with a layer normalizati = 0.1 is employed <ref type=""bibr"" target=""#b23"">[24]</ref>. After trained, the last 20 checkpoints are averaged to make the performance more stable <ref type=""bibr"" target=""#b16"">[17]</ref>.</p><p>At the beginning we train the ASR Transformer on English data with a random initialization, but the r .0""><head n=""4.2."">Model and training details</head><p>We perform our experiments on the big model (D1024-H16) <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b16"">17]</ref> of the ASR Transformer. Table <ref type=""table"" target=""#tab_1"">3</ref> lists our experimental parameters. Th",0
". A wide variety of graph neural network (GNN) models have been proposed in recent years, including methods inspired by convolutional neural networks <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b15"">16",1
"n developing graph neural networks (GNNs)general deep learning architectures that can operate over graph structured data, such as social network data <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b35"">36]</ref> or graph-based representatio convolutions.</p><p>Graph classification with graph neural networks. GNNs have been applied to a wide variety of tasks, including node classification <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b20"">21]</ref>, link prediction <ref type=""bibr"" target=""#b30"">[31]</ref>, graph cl dividual node embeddings by passing, transforming, and aggregating node feature information across the graph <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b15"">16]</ref>. The generated node embeddings can then be used as input to any differentiable prediction layer, e.g., for no atures on the graph, H (0) = F .</p><p>There are many possible implementations of the propagation function M <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b15"">16]</ref>. For example, one popular variant of GNNs-Kipf's et al. <ref type=""bibr"" target=""#b20"">[21]</ref> Graph Convo arget=""#b15"">16]</ref>. The generated node embeddings can then be used as input to any differentiable prediction layer, e.g., for node classification <ref type=""bibr"" target=""#b15"">[16]</ref> or link prediction <ref type=""bibr"" target=""#b31"">[32]</ref>, and the whole model can be trained in an end-t performance compared to the standard GCN approach as introduced in <ref type=""bibr"" target=""#b20"">[21]</ref>. We use the ""mean"" variant of GRAPHSAGE <ref type=""bibr"" target=""#b15"">[16]</ref> and apply a DIFFPOOL layer after every two GRAPHSAGE layers in our architecture. A total of 2 DIFFPOOL layer rt kernel-based approaches.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>GNN-based methods.</head><p>? GRAPHSAGE with global mean-pooling <ref type=""bibr"" target=""#b15"">[16]</ref>. Other GNN variants such as those proposed in <ref type=""bibr"" target=""#b20"">[21]</ref> are omitted as empir do observe that disconnected nodes are pooled together.</p><p>In practice we rely on the identifiability assumption similar to Theorem 1 in GraphSAGE <ref type=""bibr"" target=""#b15"">[16]</ref>, where nodes are identifiable via their features. This holds in many real datasets <ref type=""foot"" target="" by convolutional neural networks <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b28""",0
"on the original author's guidelines.</p><p>Kernel-based algorithms. We use the GRAPHLET <ref type=""bibr"" target=""#b33"">[34]</ref>, the SHORTEST-PATH <ref type=""bibr"" target=""#b1"">[2]</ref>, WEISFEILER-LEHMAN kernel (WL) <ref type=""bibr"" target=""#b32"">[33]</ref>, and WEISFEILER-LEHMAN OPTIMAL ASSIGN",0
"etwork that operates over sets <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b24"">25]</ref>. This global pooling approach ignores any hierarchical structure that might be present in the graph, and it p f><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b28"">29,</ref><ref type=""bibr"" target=""#b35"">36]</ref>, recurrent neural networks <ref type=""bibr"" target=""#b24"">[25]</ref>, recursive neural networks <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b29"">30]</ref> a e embeddings in a final layer <ref type=""bibr"" target=""#b10"">[11]</ref>, introducing a ""virtual node"" that is connected to all the nodes in the graph <ref type=""bibr"" target=""#b24"">[25]</ref>, or aggregating the node embeddings using a deep learning architecture that operates over sets <ref type=""bi",0
"GNN models that can learn to operate on hierarchical representations of a graph. We develop a graph analogue of the spatial pooling operation in CNNs <ref type=""bibr"" target=""#b22"">[23]</ref>, which allows for deep CNN architectures to iteratively operate on coarser and coarser representations of an",0
"es to infer indirect interactions between drugs using network inference algorithms from the biological network, involving various biomedical entities <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b10"">11]</ref>. Zhang et al. <ref type=""bibr""",1
"etwork (AFN). The classification system is used to categorize drug substances according to their therapeutic, pharmacological and chemical properties <ref type=""bibr"" target=""#b45"">[46]</ref>. It is based on the assumption that drugs with similar therapeutic or pharmacological properties share a fun",0
"er. A large number of similarity measures have been used to compute the similarities between the drugs pairs <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b15"">16]</ref>. Vilar et al. <ref type=""bibr"" target=""#b16"">[17]</ref> analyzed the molecular structural similarity of diffe or DDI prediction task that use multiple data sources <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b18"">19]</ref>. Zhang et al. <ref type=""bibr"" target=""#b15"">[16]</ref> proposed joint matrix factorization approach for drug repurposing by integrating multiple information source",0
"is reasonably high. By considering other 2D molecular fingerprints and the 3D molecular structure of the drugs, such as the one used by Vilar et al. <ref type=""bibr"" target=""#b51"">[52]</ref>, the classifier's performance and prediction results can be improved. </p></div> <div xmlns=""http://www.tei-",0
"The massive proliferation of personal computing devices is opening up new human-centered designs that blur the boundaries between humans and machines <ref type=""bibr"" target=""#b0"">[1]</ref>. Now, the frontier for research on data management is related to the so-called edge computation and communicat ing is expected to support not only the ever-growing number of users and devices but also a diverse set of new applications and services. The work in <ref type=""bibr"" target=""#b0"">[1]</ref> introduced a system that can pervasively operate in any networking environment and allows for the development",1
"ich are closer to the user, can play an important part in supporting latency-and privacysensitive applications <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b8"">9]</ref>. Therefore, the security challenges relate to the protection of device data, such that an unauthorized person c",0
"mputing services.</p><p>Applying cognitive computing in various applications for smart cities has been widely researched; for example, the authors in <ref type=""bibr"" target=""#b12"">[13]</ref> studied the role of intelligence algorithms such as machine learning and data analytics within the framework",0
"time and have more control over the data with respect to cloud-based services, and to consume fewer resources and less energy to reduce the workload <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b2"">3]</ref>.</p><p>The edge computing paradigm has multiple advantages. First, the e 1 , x 2 , . . . , x N }, the goal of the service migration is to determine S 1 , S 2 , . . . , S N to maximize the system reward defined by Equation <ref type=""bibr"" target=""#b1"">(2)</ref>.</p><p>Q-learning is one of the most popular Reinforcement Learning <ref type=""bibr"" target=""#b22"">[23]</ref>",0
"ions with edge computing since the existing edge nodes have limited processing capability. The works in <ref type=""bibr"" target=""#b16"">[17]</ref> and <ref type=""bibr"" target=""#b17"">[18]</ref> proposed a novel deep reinforcement learning approach to solve the resource allocation problems in terms of",0
"skip pathway of UNet++.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.2"">Deep Supervision</head><p>We propose to use deep supervision <ref type=""bibr"" target=""#b5"">[6]</ref> in UNet++, enabling the model to operate in two modes: (1) accurate mode wherein the outputs from all segmenta",1
"f>, Li et al. <ref type=""bibr"" target=""#b6"">[7]</ref> proposed H-denseunet for liver and liver tumor segmentation. In the same spirit, Drozdzalet al. <ref type=""bibr"" target=""#b1"">[2]</ref> systematically investigated the importance of skip connections, and introduced short skip connections within t",0
"nts, can degrade segmentation performance.</p><p>The other two recent related works are GridNet <ref type=""bibr"" target=""#b2"">[3]</ref> and Mask-RCNN <ref type=""bibr"" target=""#b3"">[4]</ref>. GridNet is an encoder-decoder architecture wherein the feature maps are wired in a grid fashion, generalizing",0
"25]</ref> leads to better accuracy for deep super-resolution networks. Previous works including EDSR <ref type=""bibr"" target=""#b18"">[19]</ref>, BTSRN <ref type=""bibr"" target=""#b6"">[7]</ref> and RDN <ref type=""bibr"" target=""#b41"">[42]</ref> found that batch normalization <ref type=""bibr"" target=""#b11 f type=""bibr"" target=""#b29"">30,</ref><ref type=""bibr"" target=""#b30"">31]</ref>. It is also experimentally proved in single image super-resolution task <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b18""> that batch normalization <ref type=""bibr"" target=""#b11"">[12]</ref> hinders the accuracy of image super-resolution. Thus, in recent image SR networks <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b41"">42]</ref>, batch normalization is abando ems. 1) For image super-resolution, commonly only small image patches (e.g. 48 × 48) and small mini-batch size (e.g. 16) are used to speedup training <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b18""> networks overfit on training datasets. Instead, many kinds of regularizers, for examples, weight decaying and dropout, are not adopted in SR networks <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b18""> image as training output-input pairs. Training data is augmented with random horizontal flips and rotations following common data augmentation methods<ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b18"">19]</ref>. During training, the input images are also subtracted with the mean R",1
"al neural networks (CNNs) have been successfully applied to the task of single image super-resolution (SISR) <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" target=""#b41"">42]</ref>. SISR aims at recovery of a h le image super-resolution task <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" target=""#b41"" br"" target=""#b11"">[12]</ref> hinders the accuracy of image super-resolution. Thus, in recent image SR networks <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b41"">42]</ref>, batch normalization is abandoned.</p></div> <div xmlns=""http://www.t ) are used to speedup training <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" target=""#b41"" are not adopted in SR networks <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" target=""#b41"" th later proposed deep SR networks (e.g., VDSR <ref type=""bibr"" target=""#b13"">[14]</ref>, SRResNet <ref type=""bibr"" target=""#b16"">[17]</ref> and EDSR <ref type=""bibr"" target=""#b18"">[19]</ref>). The increasing of depth brings benefits to representation power <ref type=""bibr"" target=""#b1"">[2,</ref><re onvolution. We demonstrate different residual building blocks for image super-resolution networks. Compared with vanilla residual blocks used in EDSR <ref type=""bibr"" target=""#b18"">[19]</ref>, we introduce WDSR-A which has a slim identity mapping pathway with wider (2× to 4×) channels before activat ht normalization <ref type=""bibr"" target=""#b24"">[25]</ref> leads to better accuracy for deep super-resolution networks. Previous works including EDSR <ref type=""bibr"" target=""#b18"">[19]</ref>, BTSRN <ref type=""bibr"" target=""#b6"">[7]</ref> and RDN <ref type=""bibr"" target=""#b41"">[42]</ref> found that strong regularization side-effects, is not suitable for training SR networks. However, with the increasing depth of neural networks for SR (e.g. MDSR <ref type=""bibr"" target=""#b18"">[19]</ref> has depth around 180), the networks without batch normalization become difficult to train. To this end, we i ation layers As image super-resolution networks going deeper and deeper (from 3-layer SRCNN <ref type=""bibr"" target=""#b3"">[4]</ref> to 160-layer MDSR <ref type=""bibr"" target=""#b18"">[19]</ref>), training becomes more difficult. Batch normalization layers are one of the cures for this problem in many features before activation, as shown in Figure <ref type=""figure"">1</ref>. Two-layer residual blocks are specifically studied following baseline EDSR <ref type=""bibr"" target=""#b18"">[19]</ref>. Assume the width of identity mapping pathway (Fig. <ref type=""figure"">2</ref>) is w 1 and width before acti org/ns/1.0""><head n=""3.4"">Network Structure</head><p>Figure <ref type=""figure"">2</ref>: Demonstration of our simplified SR network compared with EDSR <ref type=""bibr"" target=""#b18"">[19]</ref>.</p><p>In this part, we overview the WDSR network architectures. We made two major modifications based on ED f type=""bibr"" target=""#b18"">[19]</ref>.</p><p>In this part, we overview the WDSR network architectures. We made two major modifications based on EDSR <ref type=""bibr"" target=""#b18"">[19]</ref> super-resolution network.</p><p>Global residual pathway Firstly we find that the global residual pathway is Results</head><p>We train our models on DIV2K dataset <ref type=""bibr"" target=""#b34"">[35]</ref>  In this part, we show results of baseline model EDSR <ref type=""bibr"" target=""#b18"">[19]</ref> and our proposed WDSR-A and WDSR-B for the task of image bicubic x2 super-resolution on DIV2K dataset. To en each model with its number of residual blocks. The results suggest that our proposed WDSR-A and WDSR-B have better accuracy and efficiency than EDSR <ref type=""bibr"" target=""#b18"">[19]</ref>. WDSR-B with wider activation also has better or similar performance compared with WDSR-A, which supports ou ation. In our experiments we have not found any accuracy drop with our simpler form.</p><p>Upsampling layer Different from previous state-of-the-arts <ref type=""bibr"" target=""#b18"">[19,</ref><ref type=""bibr"" target=""#b41"">42]</ref> where one or more convolutional layers are inserted after upsampling raining data is augmented with random horizontal flips and rotations following common data augmentation methods<ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b18"">19]</ref>. During training, the input images are also subtracted with the mean RGB values of the DIV2K training images.",1
"e evaluated on 100 validation images (testing images are not publicly available) of DIV2K dataset. We mainly measure PSNR on RGB space. ADAM optimizer<ref type=""bibr"" target=""#b15"">[16]</ref> is used with β 1 = 0.9, β 2 = 0.999 and = 10 −8 . The batch size is set to 16. The learning rate is initiali",0
"=""bibr"" target=""#b16"">[17]</ref> and EDSR <ref type=""bibr"" target=""#b18"">[19]</ref>). The increasing of depth brings benefits to representation power <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b27"">28",0
"e. Instead of adding various shortcut connections, we conjecture that the non-linear ReLUs impede information flow from shallow layers to deeper ones <ref type=""bibr"" target=""#b25"">[26]</ref>. Based on residual SR network, we demonstrate that without additional parameters Figure <ref type=""figure"">1",0
"tion (LR) counterpart (typically a bicubic downsampled version of HR). It has many applications in security, surveillance, satellite, medical imaging <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b33"">34]</ref> and can serve as a built-in module for other image restoration or re",0
"d n=""2.1"">Super-Resolution Networks</head><p>Deep learning-based methods for single image super-resolution significantly outperform conventional ones <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b38"">39]</ref> in terms of peak signal-to-noise ratio (PSNR) and structural similar",0
"3 to 5). They are inferior in accuracy compared with later proposed deep SR networks (e.g., VDSR <ref type=""bibr"" target=""#b13"">[14]</ref>, SRResNet <ref type=""bibr"" target=""#b16"">[17]</ref> and EDSR <ref type=""bibr"" target=""#b18"">[19]</ref>). The increasing of depth brings benefits to representati ry deep VGG-like <ref type=""bibr"" target=""#b29"">[30]</ref> network with global residual connection (i.e. identity skip connection) for SISR. SRResNet <ref type=""bibr"" target=""#b16"">[17]</ref> proposed a ResNetlike <ref type=""bibr"" target=""#b8"">[9]</ref> network. Densely connected networks <ref type= lem in many tasks <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b30"">31]</ref>. It is also introduced in SISR networks in SRResNet <ref type=""bibr"" target=""#b16"">[17]</ref>. However, empirically it is found that batch normalization <ref type=""bibr"" target=""#b11"">[12]</ref> hinders It is also experimentally proved in single image super-resolution task <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" target=""#b35"" × 48) and small mini-batch size (e.g. 16) are used to speedup training <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" target=""#b35"" examples, weight decaying and dropout, are not adopted in SR networks <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" target=""#b35""",0
"epth of convolutional neural networks introduces over-parameterization and difficulty of training. To address these issues, recursive neural networks <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b31"">32]</ref> are proposed by re-using weights repeatedly.</p><p>Skip connections",0
"1"">Introduction</head><p>Deep convolutional neural networks (CNNs) have been successfully applied to the task of single image super-resolution (SISR) <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" target=""#b41 w convolutional neural networks (with its depth from 3 to 5). They are inferior in accuracy compared with later proposed deep SR networks (e.g., VDSR <ref type=""bibr"" target=""#b13"">[14]</ref>, SRResNet <ref type=""bibr"" target=""#b16"">[17]</ref> and EDSR <ref type=""bibr"" target=""#b18"">[19]</ref>). The low-level features are also important for image super-resolution task <ref type=""bibr"" target=""#b41"">[42]</ref>. To address this contradictory, VDSR <ref type=""bibr"" target=""#b13"">[14]</ref> proposed a very deep VGG-like <ref type=""bibr"" target=""#b29"">[30]</ref> network with global residual connect ef type=""bibr"" target=""#b30"">31]</ref>. It is also experimentally proved in single image super-resolution task <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b32"" monly only small image patches (e.g. 48 × 48) and small mini-batch size (e.g. 16) are used to speedup training <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b32"" Instead, many kinds of regularizers, for examples, weight decaying and dropout, are not adopted in SR networks <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b32""",0
"f> is one of the most valuable data formats due to the massive multi-level pathological information on nidus and its surrounding tissues.</p><p>WSISA <ref type=""bibr"" target=""#b21"">[21]</ref> was the first trial of moving survival prediction onto whole slide pathological images. To have a efficient network will be trained jointly with the prediction network. Different from previous DL-based survival models that basically act as feature extractor <ref type=""bibr"" target=""#b21"">[21]</ref>, DeepGraphSurv directly generates predicted risks. We integrated regression of survival risk with graph feat I features only.</p><p>Besides classical models, we compared DeepGraphSurv with the state-of-theart deep learning based survival models on WSI. WSISA <ref type=""bibr"" target=""#b21"">[21]</ref> worked on clustered patches from WSIs, however, they simply neglected the topological relationship of the in ><ref type=""bibr"" target=""#b21"">21]</ref> who learn little from topology. The previous GCN <ref type=""bibr"" target=""#b4"">[5]</ref> outperformed WSISA <ref type=""bibr"" target=""#b21"">[21]</ref> on most of datasets because it can aggregate node features as graph representation of WSI according to graph arget=""#b21"">[21]</ref> on most of datasets because it can aggregate node features as graph representation of WSI according to graph structure, while <ref type=""bibr"" target=""#b21"">[21]</ref> cannot. However, <ref type=""bibr"" target=""#b4"">[5]</ref> still worked on unsupervised graphs obtained with n s makes more sense in recognition of survival patterns. This may explain the lift by DeepGraphSurv compared to <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b21"">21]</ref> who learn little from topology. The previous GCN <ref type=""bibr"" target=""#b4"">[5]</ref> outperformed WSISA <",1
"cancer subtypes from TCGA data: glioblastoma multiforme (GBM) and lung squamous cell carcinoma (LUSC). Besides, NLST (National Lung Screening Trials <ref type=""bibr"" target=""#b9"">[10]</ref>) employed 53,454 heavy smokers of age 55 to 74 with at least 30-year smoking history as high risk group for l",0
"<ref type=""bibr"" target=""#b13"">[14]</ref>, have become ideal data sources for training DL-based survival models. Among them, whole slide image (WSI) <ref type=""bibr"" target=""#b11"">[12]</ref> is one of the most valuable data formats due to the massive multi-level pathological information on nidus an",0
"x and D is the degree matrix of G. The graph on WSI is irregular with ?(G) ?(G). A spectral convolutional filter built based on spectral graph theory <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b20"">20]</ref> is more applicable to irregular",0
"f type=""bibr"" target=""#b18"">[18]</ref>, BoostCI <ref type=""bibr"" target=""#b17"">[17]</ref> and Multi-Task Learning model for Survival Analysis (MTLSA) <ref type=""bibr"" target=""#b16"">[16]</ref>. However, their effectiveness largely depends on the quality of hand-crafted features. Moreover, they were e",0
"ighlighted by the diversity of datasets and shared tasks evaluation campaigns (STECs) which have been proposed for SemEval workshops as stipulated by <ref type=""bibr"" target=""#b1"">Agirre et al., (2012)</ref>. Hybrid methods focus on identifying and quantifying the presence of semantic relations betw",0
", depth of the two words <ref type=""bibr"" target=""#b35"">(Wu &amp; Palmer, 1994)</ref> and measures based on use of information content (IC) of a word <ref type=""bibr"" target=""#b24"">(Rus et al., 2013)</ref>.</p><p>The applied semantic similarity measures in the proposed method, is based on informatio d combinations are very useful in different applications such in summarization, they can be used to assess how well a sentence summarizes a paragraph <ref type=""bibr"" target=""#b24"">(Rus et al., 2013)</ref>. Semantic similarity methods such as Xsimilarity by <ref type=""bibr"" target=""#b20"">Petrakis et e=""bibr"" target=""#b20"">Petrakis et al., (2006)</ref> and WordNet similarity library handle word-to-word similarity only as opposed to SEMILAR toolkit <ref type=""bibr"" target=""#b24"">(Rus et al., 2013)</ref> which can handle both same size text and mixed size i.e. from word-to-word similarity up to wo ncies which are very useful in computing semantic similarities between the reviews. The task consisted four steps as described by the SEMILAR toolkit <ref type=""bibr"" target=""#b24"">(Rus et al., 2013)</ref>; Tokenization -Stanford standard tokenizer was used, Extraction of word's base form -Porter st ""#b24"">(Rus et al., 2013)</ref>.</p><p>The applied semantic similarity measures in the proposed method, is based on information content as defined by <ref type=""bibr"" target=""#b24"">Rus et al.(2013)</ref>. Information Content (IC) is computed based on frequency counts of concepts as found in a corpus",0
"cell phones and accessories etc. It widely employs recommender systems to recommend products to users based on the user""s purchase and rating history <ref type=""bibr"" target=""#b22"">(Raghavan et al., 2012)</ref>. The data consisted of user IDs, product IDs, associated ratings and the review texts fro",0
"weakly supervised learning for TTS <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b6"">7]</ref>. Related to our work, for example, <ref type=""bibr"" target=""#b6"">[7]</ref> uses pre-trained word vectors in a L <ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b6"">7]</ref>. Related to our work, for example, <ref type=""bibr"" target=""#b6"">[7]</ref> uses pre-trained word vectors in a LSTM-based acoustic model in parametric TTS <ref type=""bibr"" target=""#b6"">[ ated to our work, for example, <ref type=""bibr"" target=""#b6"">[7]</ref> uses pre-trained word vectors in a LSTM-based acoustic model in parametric TTS <ref type=""bibr"" target=""#b6"">[7]</ref>. These studies consider learning methods within the traditional TTS paradigm, however. This work, by contrast,",1
"blem.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2."">PROPOSED APPROACH</head><p>We use a baseline Tacotron architecture specified in <ref type=""bibr"" target=""#b7"">[8]</ref>, where we use a GMM attention <ref type=""bibr"" target=""#b8"">[9]</ref>, LSTM-based decoder with zoneout regular",1
"PROPOSED APPROACH</head><p>We use a baseline Tacotron architecture specified in <ref type=""bibr"" target=""#b7"">[8]</ref>, where we use a GMM attention <ref type=""bibr"" target=""#b8"">[9]</ref>, LSTM-based decoder with zoneout regularization <ref type=""bibr"" target=""#b9"">[10]</ref> and phoneme inputs de",1
"y contain millions to billions of words. From these large text corpora, one can train real-valued word vectors that contain the meanings of the words <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b12"">13]</ref> or language models that model grammatical and semantic context <ref",0
"h (TTS) have shown great promise. We are now able to produce natural prosody with high audio fidelity using a much simplified voice building pipeline <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b2"">3]</ref>. However, such models typically re ed data are plentiful and relatively easy to collect. Specifically, we propose a simple yet effective semi-supervised framework for training Tacotron <ref type=""bibr"" target=""#b0"">[1]</ref>, a recently proposed end-to-end TTS model. We propose to transfer the textual and acoustic representations lea",0
"with high audio fidelity using a much simplified voice building pipeline <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b2"">3]</ref>. However, such models typically require a sizable dataset consisting of high-quality &lt;text, audio&gt; traini",0
"From these large text corpora, one can train real-valued word vectors that contain the meanings of the words <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b12"">13]</ref> or language models that model grammatical and semantic context <ref type=""bibr"" target=""#b13"">[14]</ref>. The",0
"e=""bibr"" target=""#b7"">[8]</ref>, where we use a GMM attention <ref type=""bibr"" target=""#b8"">[9]</ref>, LSTM-based decoder with zoneout regularization <ref type=""bibr"" target=""#b9"">[10]</ref> and phoneme inputs derived from normalized text. We use Griffin-Lim <ref type=""bibr"" target=""#b10"">[11]</ref>",0
"><p>There exists previous work studying the application of unsupervised and weakly supervised learning for TTS <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b6"">7]</ref>. Related to our work, for example,",0
"ased decoder with zoneout regularization <ref type=""bibr"" target=""#b9"">[10]</ref> and phoneme inputs derived from normalized text. We use Griffin-Lim <ref type=""bibr"" target=""#b10"">[11]</ref> as the inversion algorithm to convert the predicted spectrograms to waveforms, as our main focus is to enabl",0
"e now able to produce natural prosody with high audio fidelity using a much simplified voice building pipeline <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b2"">3]</ref>. However, such models typically require a sizable dataset consisting of h",0
"ews 200B corpus from TensorFlow Hub as the word embedding module. The module maps each word to a 128-dimensional vector. We also tried word2vec (W2V) <ref type=""bibr"" target=""#b16"">[17]</ref> trained on the same corpus as the word embedding module.</p><p>For decoder pre-training, we used VCTK <ref t",0
"both objective and subjective tests.</p><p>There exists previous work studying the application of unsupervised and weakly supervised learning for TTS <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b6"">7]</r",0
"ad of simply conditioning with word vectors, a likely more effective method is to initialize the entire encoder with a pre-trained bidirectional NNLM <ref type=""bibr"" target=""#b18"">[19]</ref>. For decoder pre-training, the model mismatch during pre-training and fine-tuning can be further studied. An",0
"g the application of unsupervised and weakly supervised learning for TTS <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b6"">7]</ref>. Related to our work, for example, <ref type=""bibr"" target=""#b6"">[7]</ref",0
"#b8"">(Collobert et al., 2011;</ref><ref type=""bibr"" target=""#b19"">Huang et al., 2015;</ref><ref type=""bibr"" target=""#b7"">Chiu and Nichols, 2016;</ref><ref type=""bibr"" target=""#b22"">Lample et al., 2016;</ref><ref type=""bibr"" target=""#b46"">Yang et al., 2016;</ref><ref type=""bibr"" target=""#b27"">Ma and "">NER Model Architecture</head><p>We describe the model we use to perform NER. We will first describe the basic hierarchical neural CRF tagging model <ref type=""bibr"" target=""#b22"">(Lample et al., 2016;</ref><ref type=""bibr"" target=""#b27"">Ma and Hovy, 2016;</ref><ref type=""bibr"" target=""#b46"">Yang e chain CRF layer that models the dependency between labels and performs inference.</p><p>In this paper, we closely follow the architecture proposed by <ref type=""bibr"" target=""#b22"">Lample et al. (2016)</ref>, and use bidirectional LSTMs for both the character level and word level neural networks. Sp",1
"t=""#b19"">Huang et al., 2015;</ref><ref type=""bibr"" target=""#b7"">Chiu and Nichols, 2016;</ref><ref type=""bibr"" target=""#b22"">Lample et al., 2016;</ref><ref type=""bibr"" target=""#b46"">Yang et al., 2016;</ref><ref type=""bibr"" target=""#b27"">Ma and Hovy, 2016;</ref><ref type=""bibr"" target=""#b35"">Peters et ierarchical neural CRF tagging model <ref type=""bibr"" target=""#b22"">(Lample et al., 2016;</ref><ref type=""bibr"" target=""#b27"">Ma and Hovy, 2016;</ref><ref type=""bibr"" target=""#b46"">Yang et al., 2016)</ref>, and introduce the self-attention mechanism that we propose to deal with divergence of word or",0
"ned by using a small dictionary to project two sets of embeddings into a consistent space <ref type=""bibr"" target=""#b30"">(Mikolov et al., 2013a;</ref><ref type=""bibr"" target=""#b15"">Faruqui and Dyer, 2014;</ref><ref type=""bibr"" target=""#b2"">Artetxe et al., 2016;</ref><ref type=""bibr"" target=""#b38"">Sm",0
"pproaches in the past have leveraged the shared embedding space for cross-lingual applications <ref type=""bibr"" target=""#b17"">(Guo et al., 2015;</ref><ref type=""bibr"" target=""#b1"">Ammar et al., 2016b;</ref><ref type=""bibr"" target=""#b50"">Zhang et al., 2016;</ref><ref type=""bibr"" target=""#b14"">Fang an",0
", 2011;</ref><ref type=""bibr"" target=""#b39"">Täckström et al., 2013;</ref><ref type=""bibr"" target=""#b13"">Fang and Cohn, 2016)</ref>, mention detection <ref type=""bibr"" target=""#b52"">(Zitouni and Florian, 2008)</ref> and parsing <ref type=""bibr"" target=""#b20"">(Hwa et al., 2005;</ref><ref type=""bibr"" t",0
"=""#b17"">(Min et al., 2013)</ref>. To automatically complete KGs, extensive research efforts <ref type=""bibr"" target=""#b21"">(Nickel et al., 2011;</ref><ref type=""bibr"" target=""#b2"">Bordes et al., 2013</ref>  et <ref type=""bibr"">al., 2014;</ref><ref type=""bibr"" target=""#b31"">Trouillon et al., 2016;</r s. RESCAL <ref type=""bibr"" target=""#b21"">(Nickel et al., 2011)</ref> is one of the earlier work that models the relationship using tensor operations. <ref type=""bibr"" target=""#b2"">Bordes et al. (2013)</ref> proposed to model relationships in the 1-D vector space. Following this line of research, mor nhance the representation of each entity with its local connections in knowledge graph.</p><p>Although the entity embeddings from KG embedding models <ref type=""bibr"" target=""#b2"">(Bordes et al., 2013;</ref><ref type=""bibr"" target=""#b38"">Yang et al., 2014)</ref> already have relational information e d><p>In our experiments, we consider the following embedding-based methods: RESCAL <ref type=""bibr"" target=""#b21"">(Nickel et al., 2011)</ref>, TransE <ref type=""bibr"" target=""#b2"">(Bordes et al., 2013)</ref>, Dist-Mult <ref type=""bibr"" target=""#b38"">(Yang et al., 2014)</ref> and ComplEx <ref type=""b",1
"t=""#b38"">(Yang et al., 2014)</ref> and ComplEx <ref type=""bibr"" target=""#b31"">(Trouillon et al., 2016)</ref>. For TransE, we use the code released by <ref type=""bibr"" target=""#b15"">Lin et al. (2015b)</ref>. For the other models, we have tried the code released by <ref type=""bibr"" target=""#b31"">Troui",0
"-scale knowledge graphs <ref type=""bibr"" target=""#b29"">(Suchanek et al., 2007;</ref><ref type=""bibr"" target=""#b34"">Vrandečić and Krötzsch, 2014;</ref><ref type=""bibr"" target=""#b1"">Bollacker et al., 2008;</ref><ref type=""bibr"" target=""#b0"">Auer et al., 2007;</ref><ref type=""bibr"" target=""#b3"">Carlson",0
"ually assume enough training instances for all relations and entities and do not pay attention to those sparse symbols. More recently, several models <ref type=""bibr"" target=""#b25"">(Shi and Weninger, 2017;</ref><ref type=""bibr"" target=""#b35"">Xie et al., 2016)</ref> have been proposed to handle unsee",0
"et al., 2016)</ref>, which make predictions by comparing the input example with a small labeled support set;</p><p>(2) meta-learner based approaches <ref type=""bibr"" target=""#b23"">(Ravi and Larochelle, 2017;</ref><ref type=""bibr"" target=""#b19"">Munkhdalai and Yu, 2017;</ref><ref type=""bibr"" target="" he parameter updates or directly predicting the model parameters) given the gradients on few-shot examples. One example is the LSTMbased meta-learner <ref type=""bibr"" target=""#b23"">(Ravi and Larochelle, 2017)</ref>, which learns the step size for each dimension of the stochastic gradients. Besides t predict new facts with oneshot examples. Following the standard one-shot learning settings <ref type=""bibr"" target=""#b33"">(Vinyals et al., 2016;</ref><ref type=""bibr"" target=""#b23"">Ravi and Larochelle, 2017)</ref>, we assume access to a set of training tasks. In our problem, each training task corre",0
"ef><ref type=""bibr"" target=""#b50"">50,</ref><ref type=""bibr"" target=""#b53"">53]</ref> in the network to infer important features. More advanced methods <ref type=""bibr"" target=""#b33"">[34,</ref><ref type=""bibr"" target=""#b45"">45]</ref> produce explanations under a ""blackbox"" setting where no knowledge o ssumes the local detection boundary is linear, nor does it assume the features are independent. These are two key assumptions made by existing models <ref type=""bibr"" target=""#b33"">[34,</ref><ref type=""bibr"" target=""#b45"">45]</ref> which are often violated in security applications, causing a poor ex type=""bibr"" target=""#b59"">59]</ref> attempted to address this problem through approximation. However, this sacrifices the fidelity of the explanation <ref type=""bibr"" target=""#b33"">[34]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2.3"">Blackbox Explanation Methods</head><p>Blac y f (x). Linear regression is selfexplanatory, and thus LIME can pinpoint important features based on the regression coefficients. A recent work SHAP <ref type=""bibr"" target=""#b33"">[34]</ref> tries to extend LIME by adding weights to the artificially generated data samples. Other works propose to us k follows, i.e., ? k ? N(0, ? 2 k ).</p></note> 			<note xmlns=""http://www.tei-c.org/ns/1.0"" place=""foot"" n=""4"" xml:id=""foot_2""><p>We have tested SHAP<ref type=""bibr"" target=""#b33"">[34]</ref>, which is an extension of LIME. We find that SHAP is very slow and its performance is worse than LIME for ou",1
"ch are even treated as ""golden rules"". Certain ""golden rules"" are derived from the specifications of the Application Binary Interface (ABI) standards <ref type=""bibr"" target=""#b21"">[22]</ref>. For example, the ABI requires a function to store the old frame pointer (ebp) at the start if this function",0
"ta. Recent research has explored ways to mitigate misclassifications introduced by contaminated training data <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b46"">46,</ref><ref type=""bibr"" target=""#b65"">65]</ref>. A representative method is """,0
"mpletely different from feature selection methods such as Principal Component Analysis (PCA) <ref type=""bibr"" target=""#b25"">[26]</ref>, Sparse Coding <ref type=""bibr"" target=""#b38"">[39]</ref> or Chi-square Statistics <ref type=""bibr"" target=""#b49"">[49]</ref>. Explanation methods aim to identify the",0
"Mikolov et al., 2013)</ref>, see also <ref type=""bibr"" target=""#b6"">(Levy and Goldberg, 2014</ref>)), and the Noise Contrastive Estimation methods of <ref type=""bibr"" target=""#b9"">(Mnih and Teh, 2012;</ref><ref type=""bibr"" target=""#b4"">Jozefowicz et al., 2016)</ref> for estimation of language models fine K 1 to be the number of negative examples sampled per training example):</p><p>• For any K 1, a binary classification variant of NCE, as used by <ref type=""bibr"" target=""#b9"">(Mnih and Teh, 2012;</ref><ref type=""bibr"" target=""#b8"">Mikolov et al., 2013)</ref>, gives consistent parameter estimate achieve Fisher efficiency (the same asymptotic mean square error as the MLE) as K ! 1.</p><p>• We discuss application of our results to approaches of <ref type=""bibr"" target=""#b9"">(Mnih and Teh, 2012;</ref><ref type=""bibr"" target=""#b8"">Mikolov et al., 2013;</ref><ref type=""bibr"" target=""#b6"">Levy an oduce a new parameter c</p><p>x for each possible history x. This is the most straightforward extension of NCE to the conditional case; it is used by <ref type=""bibr"" target=""#b9"">(Mnih and Teh, 2012)</ref>. It has the clear drawback however of introducing a large number of additional parameters to n=""3.2"">The Algorithms in Previous Work</head><p>To motivate the importance of the two algorithms, we now discuss their application in previous work. <ref type=""bibr"" target=""#b9"">Mnih and Teh (2012)</ref> consider language modeling, where x = w 1 w 2 . . . w n 1 is a history consisting of the previ hod to conditional estimation, through introduction of the parameters c</p><p>x corresponding to normalization terms for each history. Interestingly, <ref type=""bibr"" target=""#b9"">Mnih and Teh (2012)</ref> acknowledge the difficulties in maintaining a separate parameter c</p><p>x for each history, a",1
"bution, and a objective function is derived based on binary classification or ranking. Prominent examples are the binary objective used in word2vec ( <ref type=""bibr"" target=""#b8"">(Mikolov et al., 2013)</ref>, see also <ref type=""bibr"" target=""#b6"">(Levy and Goldberg, 2014</ref>)), and the Noise Con P y p N (y) exp v 0 y • v</p><p>x ⌘ H(✓) which does not vary with x.</p><p>Levy and Goldberg (2014) make a connection between the NCE-based method of <ref type=""bibr"" target=""#b8"">(Mikolov et al., 2013)</ref>, and factorization of a matrix of pointwise mutual information (PMI) values of (x, y) pairs er training example):</p><p>• For any K 1, a binary classification variant of NCE, as used by <ref type=""bibr"" target=""#b9"">(Mnih and Teh, 2012;</ref><ref type=""bibr"" target=""#b8"">Mikolov et al., 2013)</ref>, gives consistent parameter estimates under the assumption that Z(x; ✓) is constant with res re error as the MLE) as K ! 1.</p><p>• We discuss application of our results to approaches of <ref type=""bibr"" target=""#b9"">(Mnih and Teh, 2012;</ref><ref type=""bibr"" target=""#b8"">Mikolov et al., 2013;</ref><ref type=""bibr"" target=""#b6"">Levy and Goldberg, 2014;</ref><ref type=""bibr"" target=""#b4"">Joz separate parameter c</p><p>x for each history, and set c x = 0 for all x, noting that empirically this works well, but without giving justification. <ref type=""bibr"" target=""#b8"">Mikolov et al. (2013)</ref> consider an NCE-based method using the binary objective function for estimation of word embe",0
"t is hard to overstate the importance of models of this form in NLP. In log-linear models, including both the original work on maximum-entropy models <ref type=""bibr"" target=""#b1"">(Berger et al., 1996)</ref>, and later work on conditional random fields <ref type=""bibr"" target=""#b5"">(Lafferty et al.,",0
"ef type=""bibr"" target=""#b4"">Jozefowicz et al. (2016)</ref> partially motivate the ranking-based variant throught the importance sampling viewpoint of <ref type=""bibr"" target=""#b0"">Bengio and Senécal (2008)</ref>. However there are two critical differences: 1) the algorithm of <ref type=""bibr"" target sampling viewpoint of <ref type=""bibr"" target=""#b0"">Bengio and Senécal (2008)</ref>. However there are two critical differences: 1) the algorithm of <ref type=""bibr"" target=""#b0"">Bengio and Senécal (2008)</ref> does not lead to the same objective L n R in the ranking-based variant of NCE; instead i",0
"ribution of the MLE will converge to a multivariate normal distribution, and the mean square error of the MLE will achieve the Cramer-Rao lower bound <ref type=""bibr"" target=""#b2"">(Ferguson, 1996)</ref>.</p><p>We have shown the consistency of the NCE estimators in Theorem 4.2 and Theorem 4.4. In thi mula xml:id=""formula_45"">MSE 1 ( b ✓ MLE ) = Tr(I 1 ✓ ⇤ )/d.</formula><p>where Tr(•) denotes the trace of a matrix. According to classical MLE theory <ref type=""bibr"" target=""#b2"">(Ferguson, 1996)</ref>, under certain regularity conditions, this is the best achievable mean square error. So the next",0
"R) models without handcrafting features <ref type=""bibr"" target=""#b13"">(Liu et al., 2018;</ref><ref type=""bibr"" target=""#b14"">Ma and Hovy, 2016;</ref><ref type=""bibr"" target=""#b10"">Lample et al., 2016)</ref>. However, most existing methods require large amounts of manually annotated sentences for tr el y j .</p><p>We follow previous works <ref type=""bibr"" target=""#b13"">(Liu et al., 2018;</ref><ref type=""bibr"" target=""#b14"">Ma and Hovy, 2016;</ref><ref type=""bibr"" target=""#b10"">Lample et al., 2016)</ref> to define the score of the predicted sequence, the score of the predicted sequence (y 1 , y d models (e.g., neural sequence models) <ref type=""bibr"" target=""#b13"">(Liu et al., 2018;</ref><ref type=""bibr"" target=""#b14"">Ma and Hovy, 2016;</ref><ref type=""bibr"" target=""#b10"">Lample et al., 2016;</ref><ref type=""bibr"" target=""#b3"">Finkel et al., 2005)</ref>. This is particularly challenging in F) with the IOB or IOBES tagging scheme <ref type=""bibr"" target=""#b13"">(Liu et al., 2018;</ref><ref type=""bibr"" target=""#b14"">Ma and Hovy, 2016;</ref><ref type=""bibr"" target=""#b10"">Lample et al., 2016;</ref><ref type=""bibr"" target=""#b18"">Ratinov and Roth, 2009;</ref><ref type=""bibr"" target=""#b3"">Fin >Finkel et al., 2005)</ref>. However, such design cannot deal with multi-label tokens. Therefore, we customize the conventional CRF layer in LSTM-CRF <ref type=""bibr"" target=""#b10"">(Lample et al., 2016)</ref> into a Fuzzy CRF layer, which allows each token to have multiple labels without sacrificing y types are {Chemical, Disease}. The non-entity tokens, such as ""Thus"" and ""by"", are labeled as O.</p><p>Fuzzy-LSTM-CRF. We revise the LSTM-CRF model <ref type=""bibr"" target=""#b10"">(Lample et al., 2016)</ref> to the Fuzzy-LSTM-CRF model to support the modified IOBES labels.</p><p>Given a word sequen er competitive performance. On the BC5CDR and NCBI-Disease datasets, LM-LSTM-CRF <ref type=""bibr"" target=""#b13"">(Liu et al., 2018)</ref> and LSTM-CRF <ref type=""bibr"" target=""#b10"">(Lample et al., 2016)</ref> achieve the state-of-the-art F 1 scores without external resources, respectively <ref type= target=""#b11"">Leaman and Gonzalez, 2008)</ref>. Recent advances in neural models have freed do-main experts from handcrafting features for NER tasks. <ref type=""bibr"" target=""#b10"">(Lample et al., 2016;</ref><ref type=""bibr"" target=""#b14"">Ma and Hovy, 2016;</ref><ref type=""bibr"" target=""#b13"">Liu et",1
"ng phrase mining <ref type=""bibr"" target=""#b22"">(Shang et al., 2018)</ref>, entity recognition <ref type=""bibr"" target=""#b19"">(Ren et al., 2015;</ref><ref type=""bibr"" target=""#b4"">Fries et al., 2017;</ref><ref type=""bibr"" target=""#b7"">He, 2017)</ref>, aspect term extraction <ref type=""bibr"" target="" e domain-specific dictionary. There are attempts on the distantly supervised NER task recently <ref type=""bibr"" target=""#b19"">(Ren et al., 2015;</ref><ref type=""bibr"" target=""#b4"">Fries et al., 2017;</ref><ref type=""bibr"" target=""#b7"">He, 2017;</ref><ref type=""bibr"" target=""#b5"">Giannakopoulos et al span detection problem by heuristic matching rules, such as POS tag-based regular expressions <ref type=""bibr"" target=""#b19"">(Ren et al., 2015;</ref><ref type=""bibr"" target=""#b4"">Fries et al., 2017)</ref> and exact string matching <ref type=""bibr"" target=""#b5"">(Giannakopoulos et al., 2017;</ref><re self.</p><p>SwellShark, in the biomedical domain, is arguably the best distantly supervised model, especially on the BC5CDR and NCBI-Disease datasets <ref type=""bibr"" target=""#b4"">(Fries et al., 2017)</ref>. It needs no human annotated data, however, it requires extra expert effort for entity span d l., 2017;</ref><ref type=""bibr"" target=""#b7"">He, 2017;</ref><ref type=""bibr"" target=""#b5"">Giannakopoulos et al., 2017)</ref>. For example, SwellShark <ref type=""bibr"" target=""#b4"">(Fries et al., 2017)</ref>, specifically designed for biomedical NER, leverages a generative model to unify and model no",0
"the total number of matched tokens <ref type=""bibr"" target=""#b2"">(Etzioni et al., 2005;</ref><ref type=""bibr"" target=""#b6"">Hanisch et al., 2005;</ref><ref type=""bibr"" target=""#b12"">Lin et al., 2012;</ref><ref type=""bibr"" target=""#b7"">He, 2017)</ref>.</p><p>Based on the result of dictionary matching,",0
"attracted many attentions to alleviate human efforts. Originally, it was proposed to leverage knowledge bases to supervise relation extraction tasks <ref type=""bibr"" target=""#b0"">(Craven et al., 1999;</ref><ref type=""bibr"" target=""#b15"">Mintz et al., 2009)</ref>. AutoPhrase has demonstrated powers",0
"et al., 2018)</ref>, entity recognition <ref type=""bibr"" target=""#b19"">(Ren et al., 2015;</ref><ref type=""bibr"" target=""#b4"">Fries et al., 2017;</ref><ref type=""bibr"" target=""#b7"">He, 2017)</ref>, aspect term extraction <ref type=""bibr"" target=""#b5"">(Giannakopoulos et al., 2017)</ref>, and relation ef><ref type=""bibr"" target=""#b4"">Fries et al., 2017)</ref> and exact string matching <ref type=""bibr"" target=""#b5"">(Giannakopoulos et al., 2017;</ref><ref type=""bibr"" target=""#b7"">He, 2017)</ref>. In these models, every unmatched token will be tagged as nonentity. However, as most existing dictionar rget=""#b2"">(Etzioni et al., 2005;</ref><ref type=""bibr"" target=""#b6"">Hanisch et al., 2005;</ref><ref type=""bibr"" target=""#b12"">Lin et al., 2012;</ref><ref type=""bibr"" target=""#b7"">He, 2017)</ref>.</p><p>Based on the result of dictionary matching, each token falls into one of three categories: 1) it distantly supervised NER task recently <ref type=""bibr"" target=""#b19"">(Ren et al., 2015;</ref><ref type=""bibr"" target=""#b4"">Fries et al., 2017;</ref><ref type=""bibr"" target=""#b7"">He, 2017;</ref><ref type=""bibr"" target=""#b5"">Giannakopoulos et al., 2017)</ref>. For example, SwellShark <ref type=""bibr expressions, which require extensive expert effort to cover many special cases. Other methods <ref type=""bibr"" target=""#b19"">(Ren et al., 2015;</ref><ref type=""bibr"" target=""#b7"">He, 2017</ref>) also utilize similar approaches to extract entity span candidates before entity typing. Distant-LSTM-CRF",0
"g of unstructured text on a large scale, such as question answering <ref type=""bibr"" target=""#b22"">(Yu et al., 2017)</ref>, knowledge base population <ref type=""bibr"" target=""#b25"">(Zhang et al., 2017)</ref>, and biomedical knowledge discovery <ref type=""bibr"" target=""#b14"">(Quirk and Poon, 2017)</r fore use this variant in our experiments. Earlier, our group compared (1) and ( <ref type=""formula"" target=""#formula_1"">2</ref>) with sequence models <ref type=""bibr"" target=""#b25"">(Zhang et al., 2017)</ref>, and we report these results; for (3) we report results with our own implementation.</p><p>N aware attention mechanism over LSTM outputs (PA-LSTM), and showed that it outperforms several CNN and dependency-based models by a substantial margin <ref type=""bibr"" target=""#b25"">(Zhang et al., 2017)</ref>. We compare with this strong baseline, and use its open implementation in further analysis.< www.tei-c.org/ns/1.0""><head n=""5.2"">Experimental Setup</head><p>We conduct experiments on two relation extraction datasets: (1) TACRED: Introduced in <ref type=""bibr"" target=""#b25"">(Zhang et al., 2017)</ref>, TACRED contains over 106k mention pairs drawn from the yearly TAC KBP<ref type=""foot"" targe ve when applied to the subtree rooted at the LCA of the two entities.</p><p>More recently, <ref type=""bibr"" target=""#b0"">Adel et al. (2016)</ref> and <ref type=""bibr"" target=""#b25"">Zhang et al. (2017)</ref> have shown that relatively simple neural models (CNN and augmented LSTM, respectively) can ac conventional. ( <ref type=""formula"" target=""#formula_1"">2</ref> For fair comparisons on the TACRED dataset, we follow the evaluation protocol used in <ref type=""bibr"" target=""#b25"">(Zhang et al., 2017</ref>) by selecting the model with the median dev F 1 from 5 independent runs and reporting its tes",1
"the model is restricted to only considering the dependency path.</p><p>In this work, we propose a novel extension of the graph convolutional network <ref type=""bibr"" target=""#b4"">(Kipf and Welling, 2017;</ref><ref type=""bibr"" target=""#b9"">Marcheggiani and Titov, 2017)</ref> that is tailored for rel",0
"nformation is to perform bottom-up or top-down computation along the parse tree or the subtree below the lowest common ancestor (LCA) of the entities <ref type=""bibr"" target=""#b11"">(Miwa and Bansal, 2016)</ref>. Another popular approach, inspired by <ref type=""bibr"" target=""#b1"">Bunescu and Mooney ( n the subtree rooted at the lowest common ancestor (LCA) of the two entities. Previous studies <ref type=""bibr"" target=""#b21"">(Xu et al., 2015b;</ref><ref type=""bibr"" target=""#b11"">Miwa and Bansal, 2016)</ref> have shown that removing tokens outside this scope helps relation extraction by eliminatin al. (2015)</ref> first applied a recursive network over the subtrees rooted at the words on the dependency path and then applied a CNN over the path. <ref type=""bibr"" target=""#b11"">Miwa and Bansal (2016)</ref> applied a Tree-LSTM <ref type=""bibr"" target=""#b17"">(Tai et al., 2015)</ref>, a generalized path-based counterpart (K = 0). This confirms our hypothesis in Section 3 that incorporating off-path information is crucial to relation extraction. <ref type=""bibr"" target=""#b11"">Miwa and Bansal (2016)</ref> reported that a Tree-LSTM achieves similar performance when the dependency path and the LC",0
"lation extraction setting. They found it to be most effective when applied to the subtree rooted at the LCA of the two entities.</p><p>More recently, <ref type=""bibr"" target=""#b0"">Adel et al. (2016)</ref> and <ref type=""bibr"" target=""#b25"">Zhang et al. (2017)</ref> have shown that relatively simple",0
"u et al. (2015b)</ref> generalized the idea of dependency path kernels by applying a LSTM network over the shortest dependency path between entities. <ref type=""bibr"" target=""#b7"">Liu et al. (2015)</ref> first applied a recursive network over the subtrees rooted at the words on the dependency path a",0
"Android phone.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2"">Related Work</head><p>CNN Compression and Acceleration. Extensive works <ref type=""bibr"" target=""#b20"">[20,</ref><ref type=""bibr"" target=""#b19"">19,</ref><ref type=""bibr"" target=""#b34"">34,</ref><ref type=""bibr"" target=""#b11 e-tuning a pruned model usually takes a very long time. We observe a correlation between the pre-fine-tune accuracy and the post fine-tuning accuracy <ref type=""bibr"" target=""#b20"">[20,</ref><ref type=""bibr"" target=""#b22"">22]</ref>. As shown in Table <ref type=""table"" target=""#tab_1"">2</ref>, polici urn action t fully connected layer is set to 0.98. For channel pruning, we use max response selection (pruning the weights according to the magnitude <ref type=""bibr"" target=""#b20"">[20]</ref>), and preserve Batch Normalization <ref type=""bibr"" target=""#b25"">[25]</ref> layers during pruning instead o #b17"">17,</ref><ref type=""bibr"" target=""#b39"">39]</ref>. However, it requires iterative prune &amp; fine-tune procedure to achieve decent performance <ref type=""bibr"" target=""#b20"">[20]</ref>, and single-shot pruning without retraining will greatly hurt the prediction accuracy Crests: our RL agent a",1
"arsity ratio for each layer. Neural Architecture Search and AutoML. Many works on searching models with reinforcement learning and genetic algorithms <ref type=""bibr"" target=""#b46"">[46,</ref><ref type=""bibr"" target=""#b42"">42,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b37"">",0
"xperiments suggest that AMC offers better performance than hand-crafted heuristic policies. For ResNet-50, we push the expert-tuned compression ratio <ref type=""bibr"" target=""#b15"">[16]</ref> from 3.4× to 5× with no loss of accuracy. Furthermore, we reduce the FLOPs of MobileNet-V1 <ref type=""bibr"" ssion rate (say 4×), which cannot provide useful supervision for reinforcement learning agent.</p><p>To tackle the problem, we follow the settings in <ref type=""bibr"" target=""#b15"">[16]</ref> to conduct 4-iteration pruning &amp; fine-tuning experiments, where the overall density of the full model is 4</ref>. We can find that the density distribution of AMC is quite different from human expert's result shown in Table <ref type=""table"">3</ref>.8 of <ref type=""bibr"" target=""#b15"">[16]</ref>, suggesting that AMC can fully explore the design space and allocate sparsity in a better way.</p><p>Compari",0
"tecture Search <ref type=""bibr"" target=""#b57"">[57]</ref>, NT: Network Transformation <ref type=""bibr"" target=""#b5"">[6]</ref>, N2N: Network to Network <ref type=""bibr"" target=""#b1"">[2]</ref>, and AMC: AutoML for Model Compression. AMC distinguishes from other works by getting reward without fine-tuni target=""#b5"">[6]</ref> proposed to speed up the exploration via network transformation <ref type=""bibr"" target=""#b7"">[8]</ref>. Inspired by them, N2N <ref type=""bibr"" target=""#b1"">[2]</ref> integrated reinforcement learning into channel selection. In Table <ref type=""table"">1</ref>, we demonstrate s",0
"we greatly reduce the inference time of MobileNet-V1 under the same accuracy. We also compare our learning based policy with a heuristic-based policy <ref type=""bibr"" target=""#b52"">[52]</ref>, and AMC better trades off accuracy and latency. Furthermore, since AMC uses the validation accuracy before f>, and AMC better trades off accuracy and latency. Furthermore, since AMC uses the validation accuracy before fine-tuning as the reward signal while <ref type=""bibr"" target=""#b52"">[52]</ref> needs local fine-tuning after each step, AMC is more sampling-efficient, requiring fewer GPU hours for polic",0
"ype=""bibr"" target=""#b4"">[5]</ref><ref type=""bibr"" target=""#b5"">[6]</ref><ref type=""bibr"" target=""#b6"">[7]</ref><ref type=""bibr"" target=""#b7"">[8]</ref><ref type=""bibr"" target=""#b8"">[9]</ref> were proposed. EDSR <ref type=""bibr"" target=""#b8"">[9]</ref> was the champion of the NTIRE2017 SR Challenge. It 5"">[6]</ref><ref type=""bibr"" target=""#b6"">[7]</ref><ref type=""bibr"" target=""#b7"">[8]</ref><ref type=""bibr"" target=""#b8"">[9]</ref> were proposed. EDSR <ref type=""bibr"" target=""#b8"">[9]</ref> was the champion of the NTIRE2017 SR Challenge. It based on SRResNet <ref type=""bibr"" target=""#b7"">[8]</ref> w resources, time, and tricks. In this work, we have reconstructed some classic SR models, such as SRCNN <ref type=""bibr"" target=""#b0"">[1]</ref>, EDSR <ref type=""bibr"" target=""#b8"">[9]</ref> and SRResNet <ref type=""bibr"" target=""#b7"">[8]</ref>. During the reconstruction experiments, we find most exis N <ref type=""bibr"" target=""#b6"">[7]</ref>, LapSRN <ref type=""bibr"" target=""#b5"">[6]</ref>, SRResNet <ref type=""bibr"" target=""#b7"">[8]</ref>, and EDSR <ref type=""bibr"" target=""#b8"">[9]</ref>. Unfortunately, these models become more and more deeper and extremely difficult to train. </p></div> <div xml VDSR <ref type=""bibr"" target=""#b3"">[4]</ref>, DR-CN <ref type=""bibr"" target=""#b4"">[5]</ref>, LapSRN <ref type=""bibr"" target=""#b5"">[6]</ref> and EDSR <ref type=""bibr"" target=""#b8"">[9]</ref>. For fair, we retrain most of these models (except for EDSR <ref type=""bibr"" target=""#b8"">[9]</ref>, the resul <ref type=""bibr"" target=""#b5"">[6]</ref> and EDSR <ref type=""bibr"" target=""#b8"">[9]</ref>. For fair, we retrain most of these models (except for EDSR <ref type=""bibr"" target=""#b8"">[9]</ref>, the results of EDSR provided by their original papers).</p><p>Taking the equality of comparison into account, ur model outperforms by a large margin on different upscaling factors and test-datasets. It can be seen that our results are slightly lower than EDSR <ref type=""bibr"" target=""#b8"">[9]</ref>. But it is worth noting that EDSR <ref type=""bibr"" target=""#b8"">[9]</ref> use  RGB channels for training, mean test-datasets. It can be seen that our results are slightly lower than EDSR <ref type=""bibr"" target=""#b8"">[9]</ref>. But it is worth noting that EDSR <ref type=""bibr"" target=""#b8"">[9]</ref> use  RGB channels for training, meanwhile, the data augment methods are different.</p><p>To better illustrate #b8"">[9]</ref> use  RGB channels for training, meanwhile, the data augment methods are different.</p><p>To better illustrate the difference with EDSR <ref type=""bibr"" target=""#b8"">[9]</ref>, we show a comparison of model specifications in Table <ref type=""table"" target=""#tab_3"">3</ref>. EDSR <ref ty h EDSR <ref type=""bibr"" target=""#b8"">[9]</ref>, we show a comparison of model specifications in Table <ref type=""table"" target=""#tab_3"">3</ref>. EDSR <ref type=""bibr"" target=""#b8"">[9]</ref> is an outstanding model gained amazing results. However, it is a deep and wide network which contains large qu In other words, training this model will cost more memory, space and datasets. In contrast, the specifications of our model is much smaller than EDSR <ref type=""bibr"" target=""#b8"">[9]</ref>, which makes it easier to reproduce and promote.</p><p>In Fig. <ref type=""figure"">6</ref> and Fig. <ref type="" . For example, multi-scale (the scale here represents the upscaling factor) mixed training method is used in <ref type=""bibr"" target=""#b3"">[4]</ref>, <ref type=""bibr"" target=""#b8"">[9]</ref>, and geometric selfensemble method is proposed in <ref type=""bibr"" target=""#b8"">[9]</ref>. We believe that the method is used in <ref type=""bibr"" target=""#b3"">[4]</ref>, <ref type=""bibr"" target=""#b8"">[9]</ref>, and geometric selfensemble method is proposed in <ref type=""bibr"" target=""#b8"">[9]</ref>. We believe that these training tricks can also improve our model performance. However, we are more inclined t",1
"nal Networks (ESPCN <ref type=""bibr"" target=""#b1"">[2]</ref>). All of the models mentioned above are shallow networks (less than 5 layers). Kim et al. <ref type=""bibr"" target=""#b11"">[12]</ref> first introduced the residual architecture for training much deeper network (20 layers) and achieved great p work. However, these different scale features simply concatenate together, which leads to the underutilization of local features. In 2016, Kim et al. <ref type=""bibr"" target=""#b11"">[12]</ref> proposed a residual learning framework (Fig. <ref type=""figure"" target=""#fig_0"">1</ref>.(a)) to ease the tra of the image. To validate the effectiveness of our module, we design a set of comparative experiments to compare the performance with residual block <ref type=""bibr"" target=""#b11"">[12]</ref>, dense block <ref type=""bibr"" target=""#b23"">[24]</ref>   </p></div> <div xmlns=""http://www.tei-c.org/ns/1.0"" :id=""fig_5""><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Quantitative comparison of three different feature extraction blocks (residual block<ref type=""bibr"" target=""#b11"">[12]</ref>, dense block<ref type=""bibr"" target=""#b23"">[24]</ref>, and MSRB(our)) on SISR. The green line represents our",1
"ype=""bibr"" target=""#b3"">[4]</ref><ref type=""bibr"" target=""#b4"">[5]</ref><ref type=""bibr"" target=""#b5"">[6]</ref><ref type=""bibr"" target=""#b6"">[7]</ref><ref type=""bibr"" target=""#b7"">[8]</ref><ref type=""bibr"" target=""#b8"">[9]</ref> were proposed. EDSR <ref type=""bibr"" target=""#b8"">[9]</ref> was the cha rget=""#b8"">[9]</ref> were proposed. EDSR <ref type=""bibr"" target=""#b8"">[9]</ref> was the champion of the NTIRE2017 SR Challenge. It based on SRResNet <ref type=""bibr"" target=""#b7"">[8]</ref> while enhanced the network by removing the normalization layers as well as using deeper and wider network stru econstructed some classic SR models, such as SRCNN <ref type=""bibr"" target=""#b0"">[1]</ref>, EDSR <ref type=""bibr"" target=""#b8"">[9]</ref> and SRResNet <ref type=""bibr"" target=""#b7"">[8]</ref>. During the reconstruction experiments, we find most existing SR models have the following problems:</p><p>(a) DRCN <ref type=""bibr"" target=""#b4"">[5]</ref>, DRNN <ref type=""bibr"" target=""#b6"">[7]</ref>, LapSRN <ref type=""bibr"" target=""#b5"">[6]</ref>, SRResNet <ref type=""bibr"" target=""#b7"">[8]</ref>, and EDSR <ref type=""bibr"" target=""#b8"">[9]</ref>. Unfortunately, these models become more and more deeper and",0
".0""><head n=""2.2"">Feature Extraction Block</head><p>Nowadays, many feature extraction blocks have been proposed. The main idea of the inception block <ref type=""bibr"" target=""#b12"">[13]</ref> (Fig. <ref type=""figure"" target=""#fig_0"">1</ref>.(c)) is to find out how an optimal local sparse structure w",0
"have indicated that they can provide remarkable performance in the SISR problem. In 2014, Dong et al. proposed a model for SISR problem termed SRCNN <ref type=""bibr"" target=""#b0"">[1]</ref>, which was the first successful model adopting CNNs to SR problem. SRCNN was an efficient network that could l hich means training these models consumes more resources, time, and tricks. In this work, we have reconstructed some classic SR models, such as SRCNN <ref type=""bibr"" target=""#b0"">[1]</ref>, EDSR <ref type=""bibr"" target=""#b8"">[9]</ref> and SRResNet <ref type=""bibr"" target=""#b7"">[8]</ref>. During the build an end-to-end CNNs model to learn mapping functions from LR to HR images by using large training datasets. Since Dong et al. proposed the SRCNN <ref type=""bibr"" target=""#b0"">[1]</ref> model, various CNNs architectures have been used on SISR problem. Previous work often used pre-processed LR im ate-of-the-art SR methods, including Bicubic, A+ <ref type=""bibr"" target=""#b22"">[23]</ref>, SelfExSR <ref type=""bibr"" target=""#b19"">[20]</ref>, SRCNN <ref type=""bibr"" target=""#b0"">[1]</ref>, ESPCN <ref type=""bibr"" target=""#b1"">[2]</ref>, FSRCNN <ref type=""bibr"" target=""#b2"">[3]</ref>, VDSR <ref type",0
"for image restoration challenge. During testing, we choose five widely used benchmark datasets: Set5 <ref type=""bibr"" target=""#b16"">[17]</ref>, Set14 <ref type=""bibr"" target=""#b17"">[18]</ref>, BSDS100 <ref type=""bibr"" target=""#b18"">[19]</ref>, Ur-ban100 <ref type=""bibr"" target=""#b19"">[20]</ref> and",0
"raining dataset, a new highquality image dataset for image restoration challenge. During testing, we choose five widely used benchmark datasets: Set5 <ref type=""bibr"" target=""#b16"">[17]</ref>, Set14 <ref type=""bibr"" target=""#b17"">[18]</ref>, BSDS100 <ref type=""bibr"" target=""#b18"">[19]</ref>, Ur-ban1",0
"series of CNNs-based SISR models <ref type=""bibr"" target=""#b1"">[2]</ref><ref type=""bibr"" target=""#b2"">[3]</ref><ref type=""bibr"" target=""#b3"">[4]</ref><ref type=""bibr"" target=""#b4"">[5]</ref><ref type=""bibr"" target=""#b5"">[6]</ref><ref type=""bibr"" target=""#b6"">[7]</ref><ref type=""bibr"" target=""#b7"">[8] chitecture for training much deeper network (20 layers) and achieved great performance. After that, many SR models have been proposed, including DRCN <ref type=""bibr"" target=""#b4"">[5]</ref>, DRNN <ref type=""bibr"" target=""#b6"">[7]</ref>, LapSRN <ref type=""bibr"" target=""#b5"">[6]</ref>, SRResNet <ref t >, ESPCN <ref type=""bibr"" target=""#b1"">[2]</ref>, FSRCNN <ref type=""bibr"" target=""#b2"">[3]</ref>, VDSR <ref type=""bibr"" target=""#b3"">[4]</ref>, DR-CN <ref type=""bibr"" target=""#b4"">[5]</ref>, LapSRN <ref type=""bibr"" target=""#b5"">[6]</ref> and EDSR <ref type=""bibr"" target=""#b8"">[9]</ref>. For fair, we",0
"rg/ns/1.0""><head n=""4.1"">Datasets</head><p>The most widely used training dataset in previous studies includes 291 images, of which 91 images are from <ref type=""bibr"" target=""#b13"">[14]</ref> and the other 200 images are from <ref type=""bibr"" target=""#b14"">[15]</ref>. And some methods take ImageNet",0
"ore efficient network to learn the mapping between LR and HR images so that a series of CNNs-based SISR models <ref type=""bibr"" target=""#b1"">[2]</ref><ref type=""bibr"" target=""#b2"">[3]</ref><ref type=""bibr"" target=""#b3"">[4]</ref><ref type=""bibr"" target=""#b4"">[5]</ref><ref type=""bibr"" target=""#b5"">[6] omplexity and produce visible artifacts. To avoid this, new methods are proposed, such as Fast Super-Resolution Convolutional Neural Networks (FSRCNN <ref type=""bibr"" target=""#b2"">[3]</ref>) and Efficient Sub-pixel Convolutional Networks (ESPCN <ref type=""bibr"" target=""#b1"">[2]</ref>). All of the mo fExSR <ref type=""bibr"" target=""#b19"">[20]</ref>, SRCNN <ref type=""bibr"" target=""#b0"">[1]</ref>, ESPCN <ref type=""bibr"" target=""#b1"">[2]</ref>, FSRCNN <ref type=""bibr"" target=""#b2"">[3]</ref>, VDSR <ref type=""bibr"" target=""#b3"">[4]</ref>, DR-CN <ref type=""bibr"" target=""#b4"">[5]</ref>, LapSRN <ref type",0
"tioning the generated image on known ground truth information which may be head pose, expression, or landmarks <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b29"">30,</ref><ref type=""bibr"" target=""#b41""",1
"ained for identity to measure the similarity of the images in feature space by comparing appropriate layers of the network (i.e. a content loss as in <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b12"">13]</ref>). The precise layers are chosen based on whether we are considering g",0
"lities, the generation quality is not as high as approaches specifically designed for transforming faces (e.g. <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b39"">40]</ref>). This opens an interesting avenue of research: how can the approach",0
"32"">[33]</ref> or 3D scans <ref type=""bibr"" target=""#b3"">[4]</ref>, or by learning 3DMM parameters directly from RGB data without ground truth labels <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b38"">39]</ref>. Please refer to Zollhöfer et. al. <ref type=""bibr"" target=""#b45"">[46]",0
"network used for these losses is the 11-layer VGG network (configuration A) <ref type=""bibr"" target=""#b36"">[37]</ref> trained on the VGG-Face Dataset <ref type=""bibr"" target=""#b25"">[26]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4"">Controlling the image generation with other",0
"based on which we can update the matching scores of its neighbor pairs <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" target=""#b27"">28]</ref>. For example, SiGMa <ref type=""bibr"" target=""#b8"">[9]</ref> proposed an unsupervised method to iteratively pr oss two networks is very large. Thus, we need to generate candidate user pairs from all the pairs. Following <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b27"">28]</ref>, we only keep the user pairs if their names are similar to each other (Cf. Section 5 for the details).</p><fo oposed an unsupervised method to iteratively propagate the matching scores from a confidential seed set of user pairs to their neighbor pairs. COSNET <ref type=""bibr"" target=""#b27"">[28]</ref> proposed a supervised method to infer the marginal probabilities that two users can be matched based on the ervised model-COSNET to iteratively infer the labels of the unlabeled pairs and update the model based on the inferred labels and the user attributes <ref type=""bibr"" target=""#b27"">[28]</ref>. However, error propagations may be introduced in above methods. This paper aims at investigating a supervis on 4). To improve the computational efficiency, before resorting to the model, we can easily select the most useful neighbor pairs by heuristic rules <ref type=""bibr"" target=""#b27"">[28]</ref>. This paper simply selects the neighbor pairs if their names are similar, i.e., the Jaro-Winkler similarity ommon names) and iteratively augments the seed set by propagating the matching scores (predicted by SVM) through the two input networks.</p><p>COSNET <ref type=""bibr"" target=""#b27"">[28]</ref>: is a factor graph model that incorporates the attributes of two users as local factors (the same as SVM), a",1
"""bibr"">Zhong et al.</ref> proposed an unsupervised co-training algorithm to incorporate attributes and relationships with the neighbor pairs together <ref type=""bibr"" target=""#b28"">[29]</ref>. Zhang et al. further formulated the propagation process based on attributes and network topologies from an",0
"rget=""#b29"">30]</ref>. However, most of the neighbor pairs are unlabeled, preventing us from calculating the above metrics. Some methods such as IONE <ref type=""bibr"" target=""#b11"">[12]</ref> and PALE <ref type=""bibr"" target=""#b14"">[15]</ref> learned user embeddings by encoding the structure informa",0
"o extended the output to sequences <ref type=""bibr"" target=""#b9"">[10]</ref>.</p><p>Several works studied how to generalize convolutions to graph data <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b15"">16,<",0
"ural networks. Shallow models such as DeepWalk <ref type=""bibr"" target=""#b17"">[18]</ref>, LINE <ref type=""bibr"" target=""#b20"">[21]</ref> and node2vec <ref type=""bibr"" target=""#b4"">[5]</ref> learn node embeddings via the prediction of the first-order or highorder neighbors. Graph neural networks (GNN",0
"Torch, rely on a computational graph intermediate representation to implement optimizations, e.g., auto differentiation and dynamic memory management <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b8"">9]</ref>. Graph-level optimizations, howeve w.tei-c.org/ns/1.0""><head n=""3"">Optimizing Computational Graphs</head><p>Computational graphs are a common way to represent programs in DL frameworks <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b8"">9]</r Q COMPUTE STORE CMD Q controller</formula></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""7"">Related Work</head><p>Deep learning frameworks <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b8"">9]</r er of hardware devices.</p><p>High-level computation graph DSLs are a typical way to represent and perform high-level optimizations. Tensorflow's XLA <ref type=""bibr"" target=""#b2"">[3]</ref> and the recently introduced DLVM <ref type=""bibr"" target=""#b44"">[45]</ref> fall into this category. The repres",1
"parallelism and therefore hide memory access latency. Results are shown in Figure <ref type=""figure"" target=""#fig_12"">10</ref> as a roofline diagram <ref type=""bibr"" target=""#b46"">[47]</ref>; roofline performance diagrams provide insight into how well a given system uses computation and memory reso",0
"ed CPU, and (4) a DL accelerator implemented on a low-power FPGA SoC. The benchmarks are based on real world DL inference workloads, including ResNet <ref type=""bibr"" target=""#b15"">[16]</ref>, MobileNet <ref type=""bibr"" target=""#b18"">[19]</ref>, the LSTM Language Model <ref type=""bibr"" target=""#b47""",0
"celerators. We can then apply different sequences of program transformations to form a rich space of valid programs for a given operator declaration. <ref type=""bibr"" target=""#b1"">(2)</ref> We introduce an automated program optimization framework to find optimized tensor operators. The optimizer is target=""#b30"">[31]</ref>. We compare our approach to existing DL frameworks, including MxNet <ref type=""bibr"" target=""#b8"">[9]</ref> and Tensor-Flow <ref type=""bibr"" target=""#b1"">[2]</ref>, that rely on highly engineered, vendor-specific libraries. TVM performs end-to-end automatic optimization and",0
"h polyhedral optimizations to optimize CUDA kernels. OpenTuner <ref type=""bibr"" target=""#b4"">[5]</ref> and existing hyper parameter-tuning algorithms <ref type=""bibr"" target=""#b25"">[26]</ref> apply domainagnostic search. A predefined cost model is used to automatically schedule image processing pipe",0
"namical system parameterized by variables associated with the supply and demand for cloud resources. Thus, unlike prior works on bidding optimization <ref type=""bibr"" target=""#b9"">[10]</ref>, our model not only explicitly accounts for the interplay between the on-demand and spot markets under a reso <ref type=""bibr"" target=""#b8"">[9]</ref>, do not hold on 2017 spot data (see Figure <ref type=""figure"" target=""#fig_4"">3</ref>).</p><p>The authors of <ref type=""bibr"" target=""#b9"">[10]</ref> used a profit-maximization model to understand spot price distributions; however, they considered only the as trategies for resources in the cloud spot market by explicitly considering job deadlines <ref type=""bibr"" target=""#b15"">[16]</ref>, cost minimization <ref type=""bibr"" target=""#b9"">[10]</ref>, <ref type=""bibr"" target=""#b16"">[17]</ref>, and task dependency <ref type=""bibr"" target=""#b17"">[18]</ref>, <r into account both the N (d) t currently running on-demand instances and a set B t ⊂ R + of bids from B t = |B t | spot instance requests. Many works <ref type=""bibr"" target=""#b9"">[10]</ref>, <ref type=""bibr"" target=""#b22"">[23]</ref>, <ref type=""bibr"" target=""#b23"">[24]</ref> assume that the CP's ob itself, satisfies our assumption).</p><p>Our use of this weak assumption has basis in both previous analyses of spot markets and other auctions (e.g. <ref type=""bibr"" target=""#b9"">[10]</ref> assumes bids are drawn from U[π, π]) as well in the simple bidding strategy of bidding the on-demand price si t . To simplify terminology we first normalize these variables by the total number of instances, i.e. define n t = N b t instead of B t ), we follow <ref type=""bibr"" target=""#b9"">[10]</ref> and assume that all bids b ∈ B t are drawn independently from the uniform distribution on [π, π]. We can then ad of optimizing over other instances in the same family. A better strategy would be to consider the collective behavior of the spot prices over time <ref type=""bibr"" target=""#b9"">[10]</ref>, which we do in this section by accounting for their temporal dynamics.</p><p>In this section, we propose bid get=""#formula_13"">6</ref>) is non-smooth and possibly ill-posed (it tends to −∞ as b t → 0), and the fact that we have constraints on the state space <ref type=""bibr"" target=""#b9"">(10)</ref>. Therefore, we need to resort to algorithms that support more modeling flexibility. We make use of Sequential",1
"nd and spot markets.</p><p>A few works have argued that the cost benefits brought by spot offerings can be realized with intuitive bidding strategies <ref type=""bibr"" target=""#b14"">[15]</ref>. However, choosing between spot instances and bid levels affects both the cost and interruptibility of users well in the simple bidding strategy of bidding the on-demand price since the user only pays the spot price anyway, as is commonly advocated (e.g. in <ref type=""bibr"" target=""#b14"">[15]</ref>) and used in practice. Under the given model, the WSD assumption yields the following proposition, which, in",1
"challenging task. Although one can extend the Kalman filter to nonlinear dynamical systems via linearization, i.e., using the Extended Kalman Filter <ref type=""bibr"" target=""#b28"">[29]</ref> or the Unscented Kalman Filter <ref type=""bibr"" target=""#b29"">[30]</ref>, they are likely to perform poorly",0
".org/ns/1.0""><head>II. RELATED WORK</head><p>Since the advent of Amazon EC2 spot pricing in 2009, many works <ref type=""bibr"" target=""#b8"">[9]</ref>, <ref type=""bibr"" target=""#b11"">[12]</ref>- <ref type=""bibr"" target=""#b13"">[14]</ref> have characterized spot prices from a statistical perspective. Th",0
"increasingly utilize cloud infrastructure to run their applications, cloud providers (CPs) have begun to offer them a variety of cloud-based services <ref type=""bibr"" target=""#b0"">[1]</ref>- <ref type=""bibr"" target=""#b2"">[3]</ref>. Infrastructureas-a-service (IaaS), as its name suggests, provides us er, longer-lived jobs that can take advantage of the volume discount. Examples of such pricing include Google Cloud Platform's sustained-use discount <ref type=""bibr"" target=""#b0"">[1]</ref> and Amazon EC2's reserved instances <ref type=""bibr"" target=""#b1"">[2]</ref>. However, while CPs can help to st",0
"rticle paths) over time to approximate the hidden state distributions; each sample is weighted by the probability that it generates the observed data <ref type=""bibr"" target=""#b30"">[31]</ref>.</p><p>While SMC has been most widely and successfully used in robotics, the low dimensionality of our model",0
"target=""#b15"">[16]</ref>, cost minimization <ref type=""bibr"" target=""#b9"">[10]</ref>, <ref type=""bibr"" target=""#b16"">[17]</ref>, and task dependency <ref type=""bibr"" target=""#b17"">[18]</ref>, <ref type=""bibr"" target=""#b18"">[19]</ref>. Some works have attempted to bid across different geographical r",0
"market by explicitly considering job deadlines <ref type=""bibr"" target=""#b15"">[16]</ref>, cost minimization <ref type=""bibr"" target=""#b9"">[10]</ref>, <ref type=""bibr"" target=""#b16"">[17]</ref>, and task dependency <ref type=""bibr"" target=""#b17"">[18]</ref>, <ref type=""bibr"" target=""#b18"">[19]</ref>. S",0
"ied to spot price data (see Figure <ref type=""figure"" target=""#fig_4"">3</ref>), the , where the latter is chosen via the Akaike Information Criterion <ref type=""bibr"" target=""#b32"">[33]</ref>. Both models were fit on m3.medium data from the US-East-1 region (zone a). The distributions are clearly sk ribution (left) and auto-correlation (right) of an AR(1) (top) and an AR(17) (bottom), where the latter is chosen via the Akaike Information Criterion<ref type=""bibr"" target=""#b32"">[33]</ref>. Both models were fit on m3.medium data from the US-East-1 region (zone a). The distributions are clearly sk",0
"_34"">E π (i) τ P (i) τ ≈ t+ T C i t=τ +1 max π (i) , E π (i) τ π (i) t<label>(15)</label></formula><p>This approximation can then be substituted into <ref type=""bibr"" target=""#b12"">(13)</ref> in order to easily find the lowest-cost instance type to pick.</p><p>Linear Auto-Regression: Linear auto-reg",0
"efers to learning style surveys and association rules <ref type=""bibr"" target=""#b50"">[51]</ref>. The related study is presented in our previous paper <ref type=""bibr"" target=""#b49"">[50]</ref>.</p><formula xml:id=""formula_15"">Hmatch 2 = 1/(norm(|P U − D li | + |AT − D li | + |CL − D li | + |CL − C li e xmlns=""http://www.tei-c.org/ns/1.0"" xml:id=""fig_6""><head></head><label></label><figDesc>) T U describes learning styles. Referring to other research<ref type=""bibr"" target=""#b49"">[50]</ref>, we design the elements of learning styles as: T U = {CL, M P, F P, P U, AT, F E, AC, DC, HP }. Competency (",1
"tudent knowledge based on five parameters: prior, learn rate, forget, guess, and slip. Some other factors are introduced to improve the basic BKT. In <ref type=""bibr"" target=""#b45"">[46]</ref>, learners' affective states are combined with learners' knowledge ststes to optimize the knowledge tracing a",0
"ers to the fact that the recommendation approaches have a low ability to capture and perceive the changes in learners' preferences in an adaptive way <ref type=""bibr"" target=""#b15"">[16]</ref>. The main reasons are: (1) the learner and LO models are often limited when it comes to comprehensively extr",0
"the most efficient or effective paths through a plethora of learning resources to achieve a certain competence <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref>. However, over specification and excessive searching in e-learning recommender systems result in information ov omes to comprehensively extracting the useful information of both learners and learning resources. As a result, the adaptive models are also limited; <ref type=""bibr"" target=""#b1"">(2)</ref> the implicit or predictive demands are seldom considered in the existing adaptive models.</p><p>• The CB recom",0
"g to the simulation experiments. In which, the problem scale, entropy and clustering validation are the main criteria that determine these parameters <ref type=""bibr"" target=""#b52"">[53]</ref>.</p><p>In the Algorithm 1, the self-organization behaviors of LOs are mainly determined by their similarity",0
"assing can capture structural information that is not visible at the node-level.</p><p>Graph kernels based on the k-WL have been proposed in the past <ref type=""bibr"" target=""#b27"">(Morris, Kersting, and Mutzel 2017)</ref>. However, a key advantage of implementing higher-order message passing in GNN st notably, the Weisfeiler-Lehman subtree kernel <ref type=""bibr"" target=""#b35"">(Shervashidze et al. 2011)</ref> as well as its higher-order variants <ref type=""bibr"" target=""#b27"">(Morris, Kersting, and Mutzel 2017)</ref>. Graphlet and Weisfeiler-Leman kernels have been successfully employed within feiler-Lehman Optimal Assignment kernel (W L -O A) <ref type=""bibr"" target=""#b22"">(Kriege, Giscard, and Wilson 2016)</ref>, and the global-local k-WL <ref type=""bibr"" target=""#b27"">(Morris, Kersting, and Mutzel 2017)</ref> with k in {2, 3} as kernel baselines. For each kernel, we computed the normal //www.tei-c.org/ns/1.0"" place=""foot"" n=""2"" xml:id=""foot_0"">Note that the definition of the local neighborhood is different from the the one defined in<ref type=""bibr"" target=""#b27"">(Morris, Kersting, and Mutzel 2017)</ref> which is a superset of our definition. Our computations therefore involve spa f subsets of size k times the maximum degree). Note that we can scale our method to larger datasets by using sampling strategies introduced in, e.g., <ref type=""bibr"" target=""#b27"">(Morris, Kersting, and Mutzel 2017;</ref><ref type=""bibr"" target=""#b14"">Hamilton, Ying, and Leskovec 2017a)</ref>. We c",1
"Hilbert space-have been the dominant approach for supervised learning on graphs. Important early work in this area includes random-walk based kernels <ref type=""bibr"" target=""#b9"">(Gärtner, Flach, and Wrobel 2003;</ref><ref type=""bibr"" target=""#b17"">Kashima, Tsuda, and Inokuchi 2003)</ref>) and kern",0
"aches based on graph kernels <ref type=""bibr"" target=""#b36"">(Vishwanathan et al. 2010)</ref> or, alternatively, using graph neural network algorithms <ref type=""bibr"" target=""#b15"">(Hamilton, Ying, and Leskovec 2017b)</ref>.</p><p>Kernel approaches typically fix a set of features in advancee.g., ind nteractions in molecules <ref type=""bibr"" target=""#b34"">(Schütt et al. 2017)</ref>. A survey of recent advancements in GNN techniques can be found in <ref type=""bibr"" target=""#b15"">(Hamilton, Ying, and Leskovec 2017b)</ref>.</p><p>Up to this point (and despite their empirical success) there has been f neighbors, around each node and then passes this aggregated information on to the next layer.</p><p>A basic GNN model can be implemented as follows <ref type=""bibr"" target=""#b15"">(Hamilton, Ying, and Leskovec 2017b)</ref>. In each layer t &gt; 0, we compute a new feature</p><formula xml:id=""formul",0
""">(Cai, Fürer, and Immerman 1992)</ref>. Nonetheless, it is a powerful heuristic, which can successfully test isomorphism for a broad class of graphs <ref type=""bibr"" target=""#b1"">(Babai and Kucera 1979)</ref>.</p><p>The k-dimensional Weisfeiler-Leman algorithm (k-WL), for k ≥ 2, is a generalization",0
". Notable instances of this model include Neural Fingerprints <ref type=""bibr"" target=""#b7"">(Duvenaud et al. 2015)</ref>, Gated Graph Neural Networks <ref type=""bibr"" target=""#b24"">(Li et al. 2016)</ref>, GraphSAGE <ref type=""bibr"" target=""#b14"">(Hamilton, Ying, and Leskovec 2017a)</ref>, SplineCNN",0
"ks such as classification and retrieval tasks <ref type=""bibr"" target=""#b5"">(Gao et al. 2012</ref>). However, traditional hypergraph learning methods <ref type=""bibr"" target=""#b23"">(Zhou, Huang, and Schölkopf 2007)</ref> suffer from their high computation complexity and storage cost, which limits th vision tasks, the hypergraph structure has been employed to model high-order correlation among data.</p><p>Hypergraph learning is first introduced in <ref type=""bibr"" target=""#b23"">(Zhou, Huang, and Schölkopf 2007)</ref>, as a propagation process on hypergraph structure. The transductive inference o rgraph, where the node labels should be smooth on the hypergraph structure. The task can be formulated as a regularization framework as introduced by <ref type=""bibr"" target=""#b23"">(Zhou, Huang, and Schölkopf 2007)</ref>:</p><formula xml:id=""formula_2"">arg min f {R emp (f ) + Ω(f )},<label>(2)</labe",1
"ructure can only be represented in the form of graph, extending neural networks to graph structure has attracted great attention from researchers. In <ref type=""bibr"" target=""#b7"">(Gori, Monfardini, and Scarselli 2005)</ref> and <ref type=""bibr"" target=""#b16"">(Scarselli et al. 2009</ref>), the neura",0
"data. In <ref type=""bibr"" target=""#b6"">(Gao et al. 2013)</ref>, a l 2 regularize on the weights is introduced to learn optimal hyperedge weights. In <ref type=""bibr"" target=""#b11"">(Hwang et al. 2008</ref>), the correlation among hyperedges is further explored by a assumption that highly correlated",0
"rm an efficient layer-wise propagation model.</p><p>For spatial approaches, the convolution operation is defined in groups of spatial close nodes. In <ref type=""bibr"" target=""#b0"">(Atwood and Towsley 2016)</ref>, the powers of a transition matrix is employed to define the neighborhood of nodes. <ref",0
"to graph structure has attracted great attention from researchers. In <ref type=""bibr"" target=""#b7"">(Gori, Monfardini, and Scarselli 2005)</ref> and <ref type=""bibr"" target=""#b16"">(Scarselli et al. 2009</ref>), the neural network on graph is first introduced to apply recurrent neural networks to de",0
"urge of interest in Graph Neural Network (GNN) approaches for representation learning of graphs <ref type=""bibr"" target=""#b23"">(Li et al., 2016;</ref><ref type=""bibr"" target=""#b13"">Hamilton et al., 2017a;</ref><ref type=""bibr"" target=""#b21"">Kipf &amp; Welling, 2017;</ref><ref type=""bibr"" target=""#b3 3"">Battaglia et al., 2016;</ref><ref type=""bibr"" target=""#b6"">Defferrard et al., 2016;</ref><ref type=""bibr"" target=""#b8"">Duvenaud et al., 2015;</ref><ref type=""bibr"" target=""#b13"">Hamilton et al., 2017a;</ref><ref type=""bibr"" target=""#b19"">Kearnes et al., 2016;</ref><ref type=""bibr"" target=""#b21"">K on schemes that we do not cover, e.g., weighted average via attention <ref type=""bibr"" target=""#b34"">(Velickovic et al., 2018)</ref> and LSTM pooling <ref type=""bibr"" target=""#b13"">(Hamilton et al., 2017a;</ref><ref type=""bibr"" target=""#b24"">Murphy et al., 2018)</ref>. We emphasize that our theoreti",1
"function ϕ.</p><p>We can use multi-layer perceptrons (MLPs) to model and learn f and ϕ in Corollary 6, thanks to the universal approximation theorem <ref type=""bibr"" target=""#b16"">(Hornik et al., 1989;</ref><ref type=""bibr"" target=""#b15"">Hornik, 1991)</ref>. In practice, we model f (k+1) • ϕ (k) wi",0
"l-time algorithm is known for it yet <ref type=""bibr"" target=""#b10"">(Garey, 1979;</ref><ref type=""bibr"" target=""#b11"">Garey &amp; Johnson, 2002;</ref><ref type=""bibr"" target=""#b1"">Babai, 2016)</ref>. Apart from some corner cases <ref type=""bibr"" target=""#b4"">(Cai et al., 1992)</ref>, the Weisfeiler-",0
", and answer verifier. Since these three sub-tasks are highly related, we regard the MRC with unanswerable questions as a multi-task learning problem <ref type=""bibr"" target=""#b1"">(Caruana 1997</ref>) by sharing some meta-knowledge.</p><p>We propose the U-Net to incorporate these three sub-tasks int ined separately.</p><p>Multi-task models Different from existing work, we regard the MRC with unanswerable questions as a multi-task learning problem <ref type=""bibr"" target=""#b1"">(Caruana 1997</ref>) by sharing some metaknowledge. Intuitively, answer prediction and answer verification are related t",1
"C Currently, end-to-end neural network models have achieved great successes for machine reading comprehension <ref type=""bibr"">(Seo et al. 2016;</ref><ref type=""bibr"" target=""#b6"">Kumar et al. 2015;</ref><ref type=""bibr"" target=""#b11"">Sukhbaatar et al. 2015;</ref><ref type=""bibr"" target=""#b2"">Cui et",0
"al. 2015;</ref><ref type=""bibr"" target=""#b8"">Rajpurkar et al. 2016)</ref>, the end-to-end neural methods have achieved promising results on MRC task <ref type=""bibr"" target=""#b10"">(Seo et al. 2016;</ref><ref type=""bibr"" target=""#b4"">Huang et al. 2017;</ref><ref type=""bibr"" target=""#b2"">Chen et al. #b2"">Cui et al. 2016;</ref><ref type=""bibr"" target=""#b14"">Xiong, Zhong, and Socher 2016;</ref><ref type=""bibr"" target=""#b2"">Dhingra et al. 2016;</ref><ref type=""bibr"" target=""#b10"">Shen et al. 2016;</ref><ref type=""bibr"">Hu et al. 2017;</ref><ref type=""bibr"" target=""#b13"">Wang, Yan, and Wu 2018)</re =""table"" xml:id=""tab_1""><head>Table 2 :</head><label>2</label><figDesc>Evaluation results on the SQuAD 2.0 (extracted on Sep 9, 2018). means the BiDAF<ref type=""bibr"" target=""#b10"">(Seo et al. 2016)</ref> with No Answer.</figDesc><table><row><cell></cell><cell>Model</cell><cell></cell><cell></cell><",0
"s/1.0""><head>(B) Multi-Level Attention</head><p>To fully fuse the semantic representation of the question and passage, we use the attention mechanism <ref type=""bibr"" target=""#b0"">(Bahdanau, Cho, and Bengio 2014)</ref> to capture their interactions on different levels.</p><p>We expected that we coul",0
"f><ref type=""bibr"" target=""#b2"">Dhingra et al. 2016;</ref><ref type=""bibr"" target=""#b10"">Shen et al. 2016;</ref><ref type=""bibr"">Hu et al. 2017;</ref><ref type=""bibr"" target=""#b13"">Wang, Yan, and Wu 2018)</ref>. Most of these models consist of three components: encoder, interaction, and pointer. The",0
"n ads or items (a.k.a., click-through rate prediction) is a critical problem for many applications such as online advertising and recommender systems <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b14"">15]</ref>. The performance of the predict ttributes as input features. The problem is very challenging for several reasons. First, the input features are extremely sparse and high-dimensional <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b20""> 9%. With such sparse and high-dimensional input features, the machine learning models are easily overfitted. Second, as shown in extensive literature <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b31""> l high-order combinatorial features heavily relies on domain experts. Moreover, it is almost impossible to hand-craft all the meaningful combinations <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b25"">26]</ref>. One may ask that we can enumerate all the possible high-order feature me, it is only effective for modeling low-order feature interactions and impractical to capture high-order feature interactions. Recently, many works <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b37""> lightly higher AUC or lower Logloss at 0.001-level is regarded significant for CTR prediction task, which has also been pointed out in existing works <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b37"">38]</ref>.</p></div> <div xmlns=""http:// ed-forward neural networks are capable of modeling implicit feature interactions and have been widely integrated into existing CTR prediction methods <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b18"">19]</ref>. To investigate whether integr ous systems have been developed by different companies <ref type=""bibr"">[8-10, 15, 21, 29, 43]</ref>. For example, Google developed the Wide&amp;Deep <ref type=""bibr"" target=""#b7"">[8]</ref> learning system for recommender systems, which combines the advantages of both the linear shallow models and d ""bibr"" target=""#b24"">[25]</ref>, FNN <ref type=""bibr"" target=""#b40"">[41]</ref>, DeepCrossing <ref type=""bibr"" target=""#b31"">[32]</ref>, Wide&amp;Deep <ref type=""bibr"" target=""#b7"">[8]</ref> and DeepFM <ref type=""bibr"" target=""#b10"">[11]</ref> utilized feed-forward neural networks to model high-order r feed-forward neural network by joint training. We name the joint model AutoInt+ and compare it with the following algorithms:</p><p>? Wide&amp;Deep <ref type=""bibr"" target=""#b7"">[8]</ref>. Wide&amp;Deep integrates the outputs of logistic regression and feed-forward neural networks. ? DeepFM <ref t erature, it is crucial to utilize the higher-order combinatorial features to yield good prediction performance <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b25"">2",1
"vily relies on domain experts. Moreover, it is almost impossible to hand-craft all the meaningful combinations <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b25"">26]</ref>. One may ask that we can enumerate all the possible high-order features and let machine learning models selec e and high-dimensional input features and meanwhile modeling different orders of feature combinations.</p><p>For example, Factorization Machines (FM) <ref type=""bibr"" target=""#b25"">[26]</ref>, which combine polynomial regression models with factorization techniques, are developed to model feature in eature interactions is a fundamental problem and therefore extensively studied in the literature. A well-known example is Factorization Machines (FM) <ref type=""bibr"" target=""#b25"">[26]</ref>, which were proposed to mainly capture the first-and second-order feature interactions and have been proved ture comes from a distinct field, p is the number of involved feature fields, and ?(?) is a non-additive combination function, such as multiplication <ref type=""bibr"" target=""#b25"">[26]</ref> and outer product <ref type=""bibr"" target=""#b18"">[19,</ref><ref type=""bibr"" target=""#b37"">38]</ref>. For exa re interactions. We associate the model classes with model names accordingly.</p><p>LR (A). LR only models the linear combination of raw features. FM <ref type=""bibr"" target=""#b25"">[26]</ref> (B). FM uses factorization techniques to model secondorder feature interactions.</p><p>AFM <ref type=""bibr"" pe=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b31"">32]</ref>. Specifically, we define the high-order combinatorial features as fol",1
"=""#b27"">28]</ref>. Afterwards, different variants of factorization machines have been proposed. For example, Field-aware Factorization Machines (FFM) <ref type=""bibr"" target=""#b15"">[16]</ref> modeled fine-grained interactions between features from different fields. GBFM <ref type=""bibr"" target=""#b6"" he exact number of clicks, we treat this problem as a binary classification problem (1 for clicks&gt;0, 0 for without click), which is similar to FFM <ref type=""bibr"" target=""#b15"">[16]</ref>. MovieLens-1M <ref type=""foot"" target=""#foot_6"">6</ref> This dataset contains users' ratings on movies. Duri",0
"n=""2.3"">Attention and Residual Networks</head><p>Our proposed model makes use of the latest techniques in the literature of deep learning: attention <ref type=""bibr"" target=""#b1"">[2]</ref> and residual networks <ref type=""bibr"" target=""#b11"">[12]</ref>. Attention is first proposed in the context of "">[2]</ref> and residual networks <ref type=""bibr"" target=""#b11"">[12]</ref>. Attention is first proposed in the context of neural machine translation <ref type=""bibr"" target=""#b1"">[2]</ref> and has been proved effective in a variety of tasks such as question answering <ref type=""bibr"" target=""#b34"">",0
">[13]</ref> stacked deep neural networks on top of the output of the second-order feature interactions to model higher-order features. Similarly, PNN <ref type=""bibr"" target=""#b24"">[25]</ref>, FNN <ref type=""bibr"" target=""#b40"">[41]</ref>, DeepCrossing <ref type=""bibr"" target=""#b31"">[32]</ref>, Wide",0
"ield good prediction performance <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b31"">32]</ref>. Specifically, we define the r"" target=""#b18"">[19]</ref><ref type=""bibr"" target=""#b19"">[20]</ref><ref type=""bibr"" target=""#b20"">[21]</ref><ref type=""bibr"" target=""#b21"">[22]</ref><ref type=""bibr"" target=""#b22"">[23]</ref><ref type=""bibr"" target=""#b23"">[24]</ref>, MovieGenre=Action&amp;Triller&gt; (i.e., red dotted rectangle). Th",0
"tend this latest technique to model the correlations between different feature fields.</p><p>Specifically, we adopt the key-value attention mechanism <ref type=""bibr"" target=""#b21"">[22]</ref> to determine which feature combinations are meaningful. Taking the feature m as an example, next we explain r"" target=""#b17"">[18]</ref><ref type=""bibr"" target=""#b18"">[19]</ref><ref type=""bibr"" target=""#b19"">[20]</ref><ref type=""bibr"" target=""#b20"">[21]</ref><ref type=""bibr"" target=""#b21"">[22]</ref><ref type=""bibr"" target=""#b22"">[23]</ref><ref type=""bibr"" target=""#b23"">[24]</ref>, MovieGenre=Action&amp;Tri",0
"neural layers. Once all network structures are fixed, we also apply grid search to baseline methods for optimal hype-parameters. Finally, we use Adam <ref type=""bibr"" target=""#b16"">[17]</ref> to optimize all deep neural network-based models.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>L",0
"or many applications such as online advertising and recommender systems <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b14"">15]</ref>. The performance of the prediction has a direct impact on the final revenue of the business providers. Due to",0
"ta are lying on irregular or non-euclidean domains, such as graphs which encode the pairwise relationships. This includes examples of social networks <ref type=""bibr"" target=""#b2"">[3]</ref>, protein interfaces <ref type=""bibr"" target=""#b3"">[4]</ref>, and 3D meshes <ref type=""bibr"" target=""#b4"">[5]</ our method on four popular benchmarks for node classification, including Cora, Citeseer, Pubmed <ref type=""bibr"" target=""#b10"">[11]</ref> and Reddit <ref type=""bibr"" target=""#b2"">[3]</ref>. Intensive experiments verify the effectiveness of our method regarding the classification accuracy and conver aph by attending over its neighbors following a self-attention strategy.</p><p>More recently, two kinds of sampling-based methods including GraphSAGE <ref type=""bibr"" target=""#b2"">[3]</ref> and FastGCN <ref type=""bibr"" target=""#b20"">[21]</ref> were developed for fast representation learning on graph p://www.tei-c.org/ns/1.0""><head n=""6"">Discussions and Extensions</head><p>Relation to other sampling methods. We contrast our approach with GraphSAGE <ref type=""bibr"" target=""#b2"">[3]</ref> and FastGC-N <ref type=""bibr"" target=""#b20"">[21]</ref> regarding the following aspects:</p><p>1. The proposed s in the citation network datasets-Cora, Citeseer and Pubmed <ref type=""bibr"" target=""#b10"">[11]</ref>  community different posts belong to in Reddit <ref type=""bibr"" target=""#b2"">[3]</ref>. These graphs are varying in sizes from small to large. Particularly, the number of nodes in Cora and Citeseer ork datasets (i.e., Cora, Citeseer and Pubmed) are set to be 16. For the Reddit dataset, the hidden dimensions are selected to be 256 as suggested by <ref type=""bibr"" target=""#b2"">[3]</ref>. The numbers of the sampling nodes for all layers excluding the top one are set to 128 for Cora and Citeseer, </p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""7.1"">Alation Studies on the Adaptive Sampling</head><p>Baselines. The codes of GraphSAGE <ref type=""bibr"" target=""#b2"">[3]</ref> and FastGCNN <ref type=""bibr"" target=""#b20"">[21]</ref> provided by the authors are implemented inconsistently;",1
"-attention strategy.</p><p>More recently, two kinds of sampling-based methods including GraphSAGE <ref type=""bibr"" target=""#b2"">[3]</ref> and FastGCN <ref type=""bibr"" target=""#b20"">[21]</ref> were developed for fast representation learning on graphs. To be specific, GraphSAGE computes node represent Extensions</head><p>Relation to other sampling methods. We contrast our approach with GraphSAGE <ref type=""bibr"" target=""#b2"">[3]</ref> and FastGC-N <ref type=""bibr"" target=""#b20"">[21]</ref> regarding the following aspects:</p><p>1. The proposed layer-wise sampling method is novel. GraphSAGE random contain more than 10<ref type=""foot"" target=""#foot_3"">4</ref> and 10 5 vertices, respectively. Following the supervised learning scenario in FastGCN <ref type=""bibr"" target=""#b20"">[21]</ref>, we use all labels of the training examples for training. More details of the benchmark datasets and more ex head n=""7.1"">Alation Studies on the Adaptive Sampling</head><p>Baselines. The codes of GraphSAGE <ref type=""bibr"" target=""#b2"">[3]</ref> and FastGCNN <ref type=""bibr"" target=""#b20"">[21]</ref> provided by the authors are implemented inconsistently; here we re-implement them based on our framework to mber of the sampling neighborhoods for each node are set to 5. For FastGCN, we adopt the Independent-Identical-Distribution (IID) sampler proposed by <ref type=""bibr"" target=""#b20"">[21]</ref> in Eq. ( <ref type=""formula"" target=""#formula_6"">5</ref>), where the number of the sampling nodes for each l For GraphSAGE, we report the results by the mean aggregator with the default parameters. For FastGCN, we directly make use of the provided results by <ref type=""bibr"" target=""#b20"">[21]</ref>. For the baselines and our approach, we run the experiments with random seeds over 20 trials and record the",1
"aphs. Whereas learning the graph embedding is already an important topic <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b7"">8]</ref>, this paper mainly focus on learning the representations for graph vertices by aggregating their features/attri",0
"c.org/ns/1.0""><head n=""2"">Related Work</head><p>While graph structures are central tools for various learning tasks (e.g. semi-supervised learning in <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b8"">9]</ref>), how to design efficient graph convolution networks has become a popu",0
"ially Convolutional Neural Networks (CNNs), has revolutionized various machine learning tasks with grid-like input data, such as image classification <ref type=""bibr"" target=""#b0"">[1]</ref> and machine translation <ref type=""bibr"" target=""#b1"">[2]</ref>. By making use of local connection and weight model training.</p><p>While the designs are similar, our motivation of applying the skip connection is different to the residual function in ResNets <ref type=""bibr"" target=""#b0"">[1]</ref>. The purpose of employing the skip connection in <ref type=""bibr"" target=""#b0"">[1]</ref> is to gain accuracy b connection is different to the residual function in ResNets <ref type=""bibr"" target=""#b0"">[1]</ref>. The purpose of employing the skip connection in <ref type=""bibr"" target=""#b0"">[1]</ref> is to gain accuracy by increasing the network depth. Here, we apply it to preserve the second-order proximity.",0
"expansion and re-parameterization trick. Nonspectral approaches define convolution on graph by using the spatial connections directly. For instance, <ref type=""bibr"" target=""#b16"">[17]</ref> learns a weight matrix for each node degree, the work by <ref type=""bibr"" target=""#b17"">[18]</ref> defines m",0
"operation in Fourier domain. Later, <ref type=""bibr"" target=""#b14"">[15]</ref> enables localized filtering by applying efficient spectral filters, and <ref type=""bibr"" target=""#b15"">[16]</ref> employs Chebyshev expansion of the graph Laplacian to avoid the eigendecomposition. Recently, GCN is propose",0
"the literature to develop neural networks to handle arbitrarily structured graphs. Whereas learning the graph embedding is already an important topic <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b7"">8]</ref>, this paper mainly focus on learni ning the network parameters and the sampler.</p><p>Moreover, we explore how to enable efficient message passing across distant nodes. Current methods <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b9"">10]</ref> resort to random walks to generate neighborhoods of various steps, and to better utilize information across distant nodes, we can sample the multi-hop neighborhoods for the GCN update in a similar way as the random walk <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b9"">10]</ref>. However, the random walk requires extra sampling to obtain distant nod",0
"convolution networks has become a popular research topic. Graph convolutional approaches are often categorized into spectral and non-spectral classes <ref type=""bibr"" target=""#b12"">[13]</ref>. The spectral approach first proposed by <ref type=""bibr"" target=""#b13"">[14]</ref> defines the convolution o cent line of research is to generalize convolutions by making use of the patch operation <ref type=""bibr"" target=""#b19"">[20]</ref> and self-attention <ref type=""bibr"" target=""#b12"">[13]</ref>. As opposed to GCNs, these methods implicitly assign different importance weights to nodes of a same neighbo rget=""#b19"">[20]</ref> presents mixture model CNNs to build CNN architectures on graphs using the patch operation, while the graph attention networks <ref type=""bibr"" target=""#b12"">[13]</ref> compute the hidden representations of each node on graph by attending over its neighbors following a self-at function. The resulting variance is explicitly reduced by fine-tuning the network and sampler.</p><p>Taking the attention into account. The GAT model <ref type=""bibr"" target=""#b12"">[13]</ref> applies the idea of self-attention to graph representation learning. Concisely, it replaces the re-normaliza",0
"the representations for graph vertices by aggregating their features/attributes. The closest work to this vein is the Graph Convolution Network (GCN) <ref type=""bibr"" target=""#b8"">[9]</ref> that applies connections between vertices as convolution filters to perform neighborhood aggregation. As demon bibr"" target=""#b8"">[9]</ref> that applies connections between vertices as convolution filters to perform neighborhood aggregation. As demonstrated in <ref type=""bibr"" target=""#b8"">[9]</ref>, GCNs have achieved the state-of-the-art performance on node classification.</p><p>An obvious challenge for ap type=""bibr"" target=""#b15"">[16]</ref> employs Chebyshev expansion of the graph Laplacian to avoid the eigendecomposition. Recently, GCN is proposed in <ref type=""bibr"" target=""#b8"">[9]</ref> to simplify previous methods with first-order expansion and re-parameterization trick. Nonspectral approaches so have a feature matrix X ∈ R N ×D with x i denoting the D-dimensional feature for node v i .</p><p>GCN. The GCN model developed by Kipf and Welling <ref type=""bibr"" target=""#b8"">[9]</ref> is one of the most successful convolutional networks for graph representation learning. If we define h (l) (v stochastic mini-batch size) are chosen to be 256 for all datasets. We train all models using early stopping with a window size of 30, as suggested by <ref type=""bibr"" target=""#b8"">[9]</ref>, and report the results corresponding to the best validation accuracies. Further details on the network archit ead><p>While graph structures are central tools for various learning tasks (e.g. semi-supervised learning in <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b8"">9]</ref>), how to design efficient graph convolution networks has become a popular research topic. Graph convolutional a",0
"substantial amount of work has also been dedicated to changing the connectivity structure of the internal convolutional blocks such as in ShuffleNet <ref type=""bibr"" target=""#b19"">[20]</ref> or introducing sparsity <ref type=""bibr"" target=""#b20"">[21]</ref> and others <ref type=""bibr"" target=""#b21""> f between computation and accuracy via a width multiplier parameter, and has been incorporated into efficient model designs of other networks as well <ref type=""bibr"" target=""#b19"">[20]</ref>. Following that intuition, the width multiplier approach allows one to reduce the dimensionality of the acti plimentary to the one described in <ref type=""bibr"" target=""#b22"">[23]</ref> and related work. In this vein our approach is similar to those taken by <ref type=""bibr"" target=""#b19"">[20,</ref><ref type=""bibr"" target=""#b21"">22]</ref> and allows to further improve the performance, while providing a gli a key building block for many efficient neural network architectures <ref type=""bibr"" target=""#b26"">[27,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b19"">20]</ref> and we use them in the present work as well. The basic idea is to replace a full convolutional operator with",1
"</ref> and allows to further improve the performance, while providing a glimpse on its internal operation. Our network design is based on MobileNetV1 <ref type=""bibr"" target=""#b26"">[27]</ref>. It retains its simplicity and does not require any special operators while significantly improves its accur se separable convolutions) so the computational cost is 8 to 9 times smaller than that of standard convolutions at only a small reduction in accuracy <ref type=""bibr"" target=""#b26"">[27]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.2."">Linear Bottlenecks</head><p>Consider a de y reducing the dimensionality of a layer thus reducing the dimensionality of the operating space. This has been successfully exploited by MobileNetV1 <ref type=""bibr"" target=""#b26"">[27]</ref> to effectively trade off between computation and accuracy via a width multiplier parameter, and has been inc dual bottleneck layers described in the Table 2. We use ReLU6 as the non-linearity because of its robustness when used with low-precision computation <ref type=""bibr"" target=""#b26"">[27]</ref>. We always use kernel size 3 × 3 as is standard for modern networks, and utilize dropout and batch normaliza ensor and produces a tensor with 128 channels, the intermediate expansion layer is then 64 • 6 = 384 channels.</p><p>Trade-off hyper parameters As in <ref type=""bibr"" target=""#b26"">[27]</ref> we tailor our architecture to different performance points, by using the input image resolution and width mu anges from 7 multiply adds to 585M MAdds, while the model size vary between 1.7M and 6.9M parameters.</p><p>One minor implementation difference, with <ref type=""bibr"" target=""#b26"">[27]</ref> is that for multipliers less than one, we apply width multiplier to all layers except the very last convolut y and momentum set to 0.9.</p><p>We use batch normalization after every layer, and the standard weight decay is set to 0.00004. Following MobileNetV1 <ref type=""bibr"" target=""#b26"">[27]</ref> setup we use initial learning rate of 0.045, and learning rate decay rate of 0.98 per epoch. We use 16 GPU a "">Depthwise Separable Convolutions</head><p>Depthwise Separable Convolutions are a key building block for many efficient neural network architectures <ref type=""bibr"" target=""#b26"">[27,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b19"">20]</ref> and we use them in the prese",1
"<p>Depthwise Separable Convolutions are a key building block for many efficient neural network architectures <ref type=""bibr"" target=""#b26"">[27,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b19"">20]</ref> and we use them in the present work as well. The basic idea is to rep",1
"ic improvements over early designs such as AlexNet <ref type=""bibr"" target=""#b4"">[5]</ref>, VGGNet <ref type=""bibr"" target=""#b5"">[6]</ref>, GoogLeNet <ref type=""bibr"" target=""#b6"">[7]</ref>. , and ResNet <ref type=""bibr"" target=""#b7"">[8]</ref>. Recently there has been lots of progress in algorithmic",0
"ng <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"">15,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b16"">17]</ref> and connectivity learning <ref type=""bibr"" target=""#b17"">[18,</ref><r",0
"rmation, which provides a convenient framework for further analysis. We measure our performance on ImageNet [1] classification, COCO object detection <ref type=""bibr"" target=""#b1"">[2]</ref>, VOC image segmentation <ref type=""bibr"" target=""#b2"">[3]</ref>. We evaluate the trade-offs between accuracy, "">[33]</ref> for object detection with a modified version of the Single Shot Detector (SSD) <ref type=""bibr"" target=""#b33"">[34]</ref> on COCO dataset <ref type=""bibr"" target=""#b1"">[2]</ref>. We also compare to YOLOv2 <ref type=""bibr"" target=""#b34"">[35]</ref> and original SSD (with VGG-16 <ref type=""",0
">[22]</ref>.</p><p>Recently, <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b25"">26]</ref>, opened up a new direction of bringing optimization methods including genetic algorithms and reinforcement le",0
"f type=""bibr"" target=""#b20"">[21]</ref> and others <ref type=""bibr"" target=""#b21"">[22]</ref>.</p><p>Recently, <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b25"">26]</ref>, opened up a new direction of",0
"9"">[20]</ref> or introducing sparsity <ref type=""bibr"" target=""#b20"">[21]</ref> and others <ref type=""bibr"" target=""#b21"">[22]</ref>.</p><p>Recently, <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b25 ral networks operate and use that to guide the simplest possible network design. Our approach should be seen as complimentary to the one described in <ref type=""bibr"" target=""#b22"">[23]</ref> and related work. In this vein our approach is similar to those taken by <ref type=""bibr"" target=""#b19"">[20,",0
"ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"">15,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b16"">17]</ref> and connectivity learning <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b18"">19]</ref>.",0
"explicitly control the resolution of computed feature maps, and builds five parallel heads including (a) Atrous Spatial Pyramid Pooling module (ASPP) <ref type=""bibr"" target=""#b42"">[43]</ref> containing three 3 × 3 convolutions with different atrous rates, (b) 1 × 1 convolution head, and (c) Image-l",0
"arget=""#b10"">11]</ref> as well as various methods of network pruning <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"">15,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b16"">17]</ref> and",0
"network) as baselines. We do not compare performance with other architectures such as Faster-RCNN <ref type=""bibr"" target=""#b35"">[36]</ref> and RFCN <ref type=""bibr"" target=""#b36"">[37]</ref> since our focus is on mobile/real-time models.</p><p>SSDLite: In this paper, we introduce a mobile friendly",0
"gorithmic architecture exploration included hyperparameter optimization <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b10"">11]</ref> as well as various methods of network pruning <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" targe",0
"roduction</head><p>Deep neural networks (DNNs) are challenged by their vulnerability to adversarial examples <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b4"">5]</ref>, which are crafted by adding small, human-imperceptible noises to legitimate examples, but make a model output enerated adversarial images. We also show the predicted labels and probabilities of these images given by the Inception v3. more varied training data <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b9"">10]</ref>.</p><p>With the knowledge of the structure and parameters of a given mo "">17,</ref><ref type=""bibr"" target=""#b10"">11]</ref>, adversarial training is the most extensively investigated way to increase the robustness of DNNs <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b23"">24]</ref>. By injecting adversarial examp on-based methods such as box-constrained L-BFGS <ref type=""bibr"" target=""#b22"">[23]</ref>, one-step gradient-based methods such as fast gradient sign <ref type=""bibr"" target=""#b4"">[5]</ref> and iterative variants of gradient-based methods <ref type=""bibr"" target=""#b8"">[9]</ref>. In general, a more s ted adversarial examples. Beyond iterative gradient-based methods that iteratively perturb the input with the gradients to maximize the loss function <ref type=""bibr"" target=""#b4"">[5]</ref>, momentum-based methods accumulate a velocity vector in the gradient direction of the loss function across ite d methods alleviate the trade-off between the white-box attacks and the transferability, and act as a stronger attack algorithm than one-step methods <ref type=""bibr"" target=""#b4"">[5]</ref> and vanilla iterative methods <ref type=""bibr"" target=""#b8"">[9]</ref>.</p><p>To further improve the transferab attacks here, and the targeted version can be simply derived.</p><p>One-step gradient-based approaches, such as the fast gradient sign method (FGSM) <ref type=""bibr"" target=""#b4"">[5]</ref>, find an adversarial example x * by maximizing the loss function J(x * , y), where J is often the cross-entrop",1
"been broadly adopted in researches and competitions for enhancing the performance and improving the robustness <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b1"">2]</ref>. The idea of ensemble can also be applied to adversarial attacks, due to",0
"arget=""#b8"">[9]</ref>. In general, a more severe issue of adversarial examples is their good transferability <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b13"">14]</ref>, i.e., the adversarial examples crafted for one model remain adversar ng an ensemble of models, because if an adversarial example fools multiple models, it is more likely to remain adversarial for other black-box models <ref type=""bibr"" target=""#b11"">[12]</ref>. We show that the adversarial examples generated by the momentum iterative methods for multiple models, can ion of linearity of the decision boundary around the data point. However in practice, the linear assumption may not hold when the distortion is large <ref type=""bibr"" target=""#b11"">[12]</ref>, which makes the adversarial example generated by FGSM ""underfits"" the model, limiting its attack ability. I multiple models, it may capture an intrinsic direction that always fools these models and is more likely to transfer to other models at the same time <ref type=""bibr"" target=""#b11"">[12]</ref>, thus enabling powerful black-box attacks.</p><p>We propose to attack multiple models whose logit activation s whose logits are averaged in Algorithm 2.</p><p>For comparison, we also introduce two alternative ensemble schemes, one of which is already studied <ref type=""bibr"" target=""#b11"">[12]</ref>. Specifically, K models can be averaged in predictions <ref type=""bibr"" target=""#b11"">[12]</ref> as p(x) = K ve ensemble schemes, one of which is already studied <ref type=""bibr"" target=""#b11"">[12]</ref>. Specifically, K models can be averaged in predictions <ref type=""bibr"" target=""#b11"">[12]</ref> as p(x) = K k=1 w k p k (x), where p k (x) is the predicted probability of the k-th model given input x. K m ne similarity in MI-FGSM.</p><p>Recall that the transferability comes from the fact that models learn similar decision boundaries around a data point <ref type=""bibr"" target=""#b11"">[12]</ref>. Although the decision boundaries are similar, they are unlikely the same due to the highly non-linear struc y exist some exceptional decision regions around a data point for a model (holes as shown in Fig. <ref type=""figure"" target=""#fig_5"">4&amp;5</ref> in <ref type=""bibr"" target=""#b11"">[12]</ref>), which are hard to transfer to other models. These regions correspond to poor local maxima in the optimizat is large. The phenomenon largely attributes to the inappropriate assumption of the linearity of the decision boundary when the perturbation is large <ref type=""bibr"" target=""#b11"">[12]</ref>. For the black-box attacks, although the success rates of these three methods grow linearly with the size of",0
"xamples subject to the misclassification of adversarial examples. Boxconstrained L-BFGS can be used to solve such a problem. A more sophisticated way <ref type=""bibr"" target=""#b0"">[1]</ref> is solving:</p><formula xml:id=""formula_4"">arg min x * λ • x * − x p − J(x * , y).<label>(4)</label></formula>",0
"pe=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b10"">11]</ref>, adversarial training is the most extensively investigated way to inc",0
"matic coverage that can be derived directly from our suite of open-source benchmark circuits. This kind of coverage has been used in related academic <ref type=""bibr"" target=""#b12"">[13]</ref> and industrial <ref type=""bibr"" target=""#b4"">[5]</ref> work.</p><p>Most automatic coverage definitions focus s in the future.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""6"">RELATED WORK</head><p>The prior work most similar to rfuzz is MicroGP <ref type=""bibr"" target=""#b12"">[13]</ref> which focuses on maximizing statement coverage in the HDL description of various processor implementations.",1
"whether the simulated executions explore all interesting behaviors of the DUT. To this end, various notions of functional coverage have been defined <ref type=""bibr"" target=""#b11"">[12]</ref>. Concrete functional coverage points or groups that are derived from the DUT specification need to be define #b4"">[5]</ref> work.</p><p>Most automatic coverage definitions focus on the description of the circuit expressed in a common HDL like Verilog or VHDL <ref type=""bibr"" target=""#b11"">[12]</ref>. However, our system works with any RTL circuit regardless of the hardware description or generation languag",0
"mlns=""http://www.tei-c.org/ns/1.0""><head n=""3.5"">Coverage Definition</head><p>We require precise definitions of our coverage metrics for two reasons: <ref type=""bibr"" target=""#b0"">(1)</ref> To define an end-to-end metric that can be used to measure how well our implementation of mutational fuzz test supply the core with instructions to execute. (4) RISC-V Rocket Core: In order to test the scalability of our approach, we use the RISC-V Rocket Chip <ref type=""bibr"" target=""#b0"">[1]</ref> as our final benchmark. This 64-bit in order core is supported by industry and is able to boot the Linux opera",0
". The first part of our tool is an instrumentation and harness generation component, which works on arbitrary RTL circuits described in the FIRRTL IR <ref type=""bibr"" target=""#b5"">[6]</ref>. It automatically generates a test harness for software or FPGA-accelerated simulation. The second part of our inputs and analyse the resulting coverage.</p><p>Our tool is language-agnostic since it can work on arbitrary RTL designs expressed in the FIRRTL IR <ref type=""bibr"" target=""#b5"">[6]</ref>. Once a target design is translated into FIRRTL IR from its source HDL, we can apply compiler passes for the t",0
"es as feedback to the fuzz engine in order to guide the search of the input space. While prior industrial work <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b14"">15]</ref> uses functional coverage models manually specified by verification en ded by such a platform.</p><p>There are various other approaches to the CDG problem that do not rely on a modified genetic algorithm. Tarsiran et al. <ref type=""bibr"" target=""#b13"">[14]</ref> analyse the circuit in order to improve the biases for an existing random input generator. Our work on the o",0
"high-level continuous representation h = (h1, . . . , h T ′ ) (T ′ ≤ T ), interleaved with subsampling layers to reduce the computational complexity <ref type=""bibr"" target=""#b25"">[26]</ref>. The decoder network generates a probability distribution PS2S of the corresponding U -length transcription",1
"ype=""table"" target=""#tab_3"">4</ref>. We used stochastic gradient descent (SGD) for RNNLM optimization. All networks are implemented by ESPnet toolkit <ref type=""bibr"" target=""#b36"">[37]</ref> with pytorch backend <ref type=""bibr"" target=""#b37"">[38]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/n",1
""">[6]</ref>, and we can further use a unified architecture with a shared vocabulary among multiple languages <ref type=""bibr"" target=""#b14"">[15]</ref><ref type=""bibr"" target=""#b15"">[16]</ref><ref type=""bibr"" target=""#b16"">[17]</ref>. Since it would take much time to train such systems from scratch f",0
"peech recognition (ASR). Recently, end-to-end ASR systems based on the sequence-to-sequence (S2S) architecture <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref> are filling up the gap of performance from the conventional HMM-based hybrid systems and showing promising resu te at the u-th timestep, and cu is a context vector summarizing notable parts from the encoder states h. We adopt the location-based scoring function <ref type=""bibr"" target=""#b1"">[2]</ref>. To encourage monotonic alignments, the auxiliary Connectionist Temporal Classification (CTC) <ref type=""bibr""",0
"data augmentation of speech data based on text-tospeech (TTS) synthesis is investigated in the S2S framework <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b24"">25]</ref>. Since we are interested in the usage of linguistic context during adaptation, we leave this direction to the",0
"t explore this direction. Apart from the external LM, the MTL approach with LM objective are investigated in <ref type=""bibr"" target=""#b18"">[19,</ref><ref type=""bibr"" target=""#b22"">23]</ref>. Although the MTL approach does not require any additional parameters, it gets minor gains compared to LM fus",0
"her usage of the external LM is to initialize the lower layer in the decoder network with the pre-trained LM <ref type=""bibr"" target=""#b18"">[19,</ref><ref type=""bibr"" target=""#b21"">22]</ref>. However, we transfer almost all parameters in a multilingual S2S model (both encoder and decoder networks),",0
"l bottleneck features (BNF) <ref type=""bibr"" target=""#b9"">[10]</ref><ref type=""bibr"" target=""#b10"">[11]</ref><ref type=""bibr"" target=""#b11"">[12]</ref><ref type=""bibr"" target=""#b12"">[13]</ref> and language feature vectors (LFV) <ref type=""bibr"" target=""#b13"">[14]</ref> (cross-lingual adaptation). To",0
"as the validation set. In addition, we used Librispeech corpus <ref type=""bibr"" target=""#b29"">[30]</ref> and the Corpus of Spontaneous Japanese (CSJ) <ref type=""bibr"" target=""#b30"">[31]</ref> as additional high resources.</p><p>We used Kaldi toolkit <ref type=""bibr"" target=""#b31"">[32]</ref> for feat",0
"he multilingual model such as multilingual bottleneck features (BNF) <ref type=""bibr"" target=""#b9"">[10]</ref><ref type=""bibr"" target=""#b10"">[11]</ref><ref type=""bibr"" target=""#b11"">[12]</ref><ref type=""bibr"" target=""#b12"">[13]</ref> and language feature vectors (LFV) <ref type=""bibr"" target=""#b13"">[",0
"#b10"">[11]</ref> used graphemes in sequenceto-sequence (seq2seq) models. Sub-word units were used in seq2seq <ref type=""bibr"" target=""#b11"">[12]</ref><ref type=""bibr"" target=""#b12"">[13]</ref><ref type=""bibr"" target=""#b13"">[14]</ref> and RNNT <ref type=""bibr"" target=""#b14"">[15]</ref> models, and word",1
"need to change the output softmax layer. This language independence makes it more preferable for modeling multiple languages and also code-switching <ref type=""bibr"" target=""#b28"">[29]</ref> speech within a single model.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3."">MULTILINGUAL B",0
"""#b9"">[10]</ref><ref type=""bibr"" target=""#b10"">[11]</ref> used graphemes in sequenceto-sequence (seq2seq) models. Sub-word units were used in seq2seq <ref type=""bibr"" target=""#b11"">[12]</ref><ref type=""bibr"" target=""#b12"">[13]</ref><ref type=""bibr"" target=""#b13"">[14]</ref> and RNNT <ref type=""bibr"" n=""2.2."">Output Unit</head><p>End-to-end speech recognition models have typically used characters <ref type=""bibr"" target=""#b8"">[9]</ref>, sub-words <ref type=""bibr"" target=""#b11"">[12]</ref>, word-pieces <ref type=""bibr"" target=""#b14"">[15]</ref> or words <ref type=""bibr"" target=""#b15"">[16]</ref> as",0
"d's languages in Automatic Speech Recognition (ASR) and Text-to-Speech (TTS) systems have been attracting much interest in both academia and industry <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref>. Conventional phonetically-based speech processing systems require pronu",0
"ed, and are representative of Google's voice search and dictation traffic. These utterances are further artificially corrupted using a room simulator <ref type=""bibr"" target=""#b30"">[31]</ref>, adding varying degrees of noise and reverberation such that the overall SNR is between 0dB and 30dB, with a",0
"very sophisticated pronunciation dictionaries of high quality for most languages, which can potentially improve the performance of end-to-end systems <ref type=""bibr"" target=""#b6"">[7]</ref>.</p><p>Sub-word representations have recently seen their success in ASR <ref type=""bibr"" target=""#b7"">[8]</ref l alignment to generate a probabilistic lexicon and proposed a worddependent silence model to improve ASR accuracy; for use in end-to-end ASR models, <ref type=""bibr"" target=""#b6"">[7]</ref> investigated the value of a lexicon in end-to-end ASR. Sub-word methods have a long history of application in",1
"the performance of end-to-end systems <ref type=""bibr"" target=""#b6"">[7]</ref>.</p><p>Sub-word representations have recently seen their success in ASR <ref type=""bibr"" target=""#b7"">[8]</ref>. Using sub-word features has a number of benefits for ASR, in that it can speed up both training and inference ts.</p><p>In Table <ref type=""table"" target=""#tab_2"">3</ref>, we report the WER results on the LibriSpeech dataset, using the parameters described in <ref type=""bibr"" target=""#b7"">[8]</ref>. We have seen that PASM significantly improves the character-based baseline; BPEs do not help in this case, po",1
"by utilizing a pronunciation dictionary and an aligner. We call this method pronunciation-assisted sub-word modeling (PASM), which adopts fast align <ref type=""bibr"" target=""#b8"">[9]</ref> to align a pronunciation lexicon arXiv:1811.04284v2 [cs.CL] 21 Feb 2019 file and use the result to figure out",1
"anism has proven to be very successful in a number of tasks, including automatic speech recognition (ASR) <ref type=""bibr"" target=""#b1"">[2]</ref> [3] <ref type=""bibr"" target=""#b3"">[4]</ref> and neural machine translation <ref type=""bibr"" target=""#b4"">[5]</ref> <ref type=""bibr"" target=""#b5"">[6]</ref> sing bi-directional LSTMs with projection layers as the encoder, location-based attention, and LSTM decoder, with a CTC-weight of 0.5 during training <ref type=""bibr"" target=""#b3"">[4]</ref>. To fully see the effect of sub-word methods, we do not perform language model rescoring but report the 1st pa",0
"ms, the attention-based encoder-decoder mechanism has proven to be very successful in a number of tasks, including automatic speech recognition (ASR) <ref type=""bibr"" target=""#b1"">[2]</ref> [3] <ref type=""bibr"" target=""#b3"">[4]</ref> and neural machine translation <ref type=""bibr"" target=""#b4"">[5]</",0
"trained, an end-to-end system is a single neural-network which implicitly models all three. Although modular training of those components is possible <ref type=""bibr"" target=""#b0"">[1]</ref>, an end-to-end model is usually jointly optimized during training. Among the different network typologies for",0
"r for detecting unseen words. <ref type=""bibr"" target=""#b11"">[12]</ref> used sub-words units in building text-independent speech recognition systems. <ref type=""bibr"" target=""#b12"">[13]</ref> improved upon sub-word methods in WFST-based speech recognition.</p><p>Apart from the application in ASR, th",0
"ation in a number of language related tasks. <ref type=""bibr"" target=""#b10"">[11]</ref> used sub-words units in particular for detecting unseen words. <ref type=""bibr"" target=""#b11"">[12]</ref> used sub-words units in building text-independent speech recognition systems. <ref type=""bibr"" target=""#b12""",0
"ef> investigated the value of a lexicon in end-to-end ASR. Sub-word methods have a long history of application in a number of language related tasks. <ref type=""bibr"" target=""#b10"">[11]</ref> used sub-words units in particular for detecting unseen words. <ref type=""bibr"" target=""#b11"">[12]</ref> use",0
"ord representations is largely driven by neural machine translation. <ref type=""bibr"" target=""#b4"">[5]</ref> proposed to use byte-pair encoding (BPE) <ref type=""bibr"" target=""#b13"">[14]</ref> to build a sub-word dictionary by greedily keep the most frequent co-occurring character sequences. Concurre",0
"automatic speech recognition (ASR) <ref type=""bibr"" target=""#b1"">[2]</ref> [3] <ref type=""bibr"" target=""#b3"">[4]</ref> and neural machine translation <ref type=""bibr"" target=""#b4"">[5]</ref> <ref type=""bibr"" target=""#b5"">[6]</ref>.</p><p>Due to lack of a pronunciation dictionary, most end-toend syste .</p><p>Apart from the application in ASR, the most recent tide of adopting sub-word representations is largely driven by neural machine translation. <ref type=""bibr"" target=""#b4"">[5]</ref> proposed to use byte-pair encoding (BPE) <ref type=""bibr"" target=""#b13"">[14]</ref> to build a sub-word diction coring but report the 1st pass numbers directly. We also compare our systems with BPE baselines. The BPE procedure follows the algorithm described in <ref type=""bibr"" target=""#b4"">[5]</ref>. All the PASM segmentation schemes are trained using the lexicon included in its default recipe, and we use N",0
"www.tei-c.org/ns/1.0""><head n=""2."">RELATED WORK</head><p>The use of a pronunciation dictionary is the standard approach in hybrid speech recognition. <ref type=""bibr"" target=""#b9"">[10]</ref> use the phone-level alignment to generate a probabilistic lexicon and proposed a worddependent silence model",0
"ype=""bibr"" target=""#b13"">[14]</ref> to build a sub-word dictionary by greedily keep the most frequent co-occurring character sequences. Concurrently, <ref type=""bibr"" target=""#b14"">[15]</ref> borrow the practice in voice search <ref type=""bibr"" target=""#b15"">[16]</ref> to segment words into wordpiec",0
"keep the most frequent co-occurring character sequences. Concurrently, <ref type=""bibr"" target=""#b14"">[15]</ref> borrow the practice in voice search <ref type=""bibr"" target=""#b15"">[16]</ref> to segment words into wordpiece which maximizes the language model probability. <ref type=""bibr"" target=""#b5",0
"ce of such end-to-end systems. Recently explored techniques to mitigate this issue include multi-task learning <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b9"">10]</ref> and pre-trained components <ref type=""bibr"" target=""#b10"">[11]</ref> in order to utilize weakly supervised dat "" target=""#b8"">9]</ref>.</p><p>In order to utilize both fully supervised data and also weakly supervised data, <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b9"">10]</ref> use multi-task learning to train the ST model jointly with the ASR and/or the MT model. By doing so, both of t irectly on the 1M ST set. We then adopt pretraining and multi-task learning as proposed in previous literature <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b14"">15]</ref> in order to improve its perfor",1
"gate this issue include multi-task learning <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b9"">10]</ref> and pre-trained components <ref type=""bibr"" target=""#b10"">[11]</ref> in order to utilize weakly supervised data, i.e. speech-to-transcript or text-to-translation pairs, in contr el jointly with the ASR and/or the MT model. By doing so, both of them achieved better performance with the end-to-end model than the cascaded model. <ref type=""bibr"" target=""#b10"">[11]</ref> conducts experiments on a larger 236 hour English-to-French dataset and pre-trains the encoder and decoder p pretraining and multi-task learning as proposed in previous literature <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b14"">15]</ref> in order to improve its performance. We pre-train the encoder on the",1
""">[27]</ref> from <ref type=""bibr"" target=""#b25"">[26]</ref>, except that we use a Griffin-Lim <ref type=""bibr"" target=""#b27"">[28]</ref> vocoder as in <ref type=""bibr"" target=""#b28"">[29]</ref> which has significantly lower cost, but results in reduced audio quality <ref type=""foot"" target=""#foot_0"">1",0
"ATED WORK</head><p>Early work on speech translation typically used a cascade of an ASR model and an MT model <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b13"">14]</ref>, giving the MT model access to the predicted probabilities and uncert",0
"INTRODUCTION</head><p>Recent advances in deep learning and more specifically in sequenceto-sequence modeling have led to dramatic improvements in ASR <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref> and MT <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#",0
"omain eval set again indicates worse generalization.</p><p>Incorporating recent advances in TTS to introduce more natural prosody and style variation <ref type=""bibr"" target=""#b30"">[31,</ref><ref type=""bibr"" target=""#b31"">32,</ref><ref type=""bibr"" target=""#b32"">33]</ref> to the synthetic speech migh",0
"builds a cycle chain between TTS and ASR models, in which the output from one model is used to help the training of the other. Instead of using TTS, <ref type=""bibr"" target=""#b17"">[18]</ref>  The MT synthetic data in this work helps the system in a manner similar to knowledge distillation <ref type",0
"multi-speaker TTS system can further improve the performance of an end-to-end ST model. Synthetic data has also been used to improve ASR performance. <ref type=""bibr"" target=""#b16"">[17]</ref> builds a cycle chain between TTS and ASR models, in which the output from one model is used to help the trai",0
"dvances in TTS to introduce more natural prosody and style variation <ref type=""bibr"" target=""#b30"">[31,</ref><ref type=""bibr"" target=""#b31"">32,</ref><ref type=""bibr"" target=""#b32"">33]</ref> to the synthetic speech might further improve performance when training on synthetic speech. We leave such in",0
"translation typically used a cascade of an ASR model and an MT model <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b13"">14]</ref>, giving the MT model access to the predicted probabilities and uncertainties from the ASR. Recent work has fo",0
"(NMT) has become a benchmark task for any architecture used for NLP <ref type=""bibr"" target=""#b32"">[33,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b33"">34,</ref><ref type=""bibr"" target=""#b34"">35,</ref><ref type=""bibr"" target=""#b35"">36]</ref>. For this reason, we continue the feed-forward layers and the number of attention heads (as well as # attention channels) in multi-head attention layers similar to Shazeer et al. <ref type=""bibr"" target=""#b33"">[34]</ref>. Please refer to the supplementary material for a detailed description of our dataset, baselines, training c ecks. Single Program Multiple Data (SPMD) and pipeline parallelism have been proposed as solutions to counter these challenges.</p><p>Mesh-Tensorflow <ref type=""bibr"" target=""#b33"">[34]</ref> follows the SPMD paradigm, which extends the Single Instruction Multiple Data (SIMD) approach used for data",1
"arning tasks.  <ref type=""bibr"" target=""#b4"">[5]</ref> and model size for representative state-of-the-art image classification models in recent years <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b8"">9,</r",0
"GPipe Library</head><p>We now describe the interface and the main design features of GPipe. This open-source library is implemented under the Lingvo <ref type=""bibr"" target=""#b15"">[16]</ref> framework. The core design features of GPipe are generally applicable and can be implemented for other frame e discuss some of our empirical findings based on these large-scale experiments.  <ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"">8192,</ref><ref type=""bibr"" target=""#b15"">16)</ref>. While the quality of these two models on high-resource languages (left of Figure <ref type=""figure"" target="" g/ns/1.0"" type=""table"" xml:id=""tab_3""><head></head><label></label><figDesc>1.3B-parameter deep model, T<ref type=""bibr"" target=""#b23"">(24,</ref> 8192,<ref type=""bibr"" target=""#b15"">16)</ref>, a 1.3B-parameter wide model, T<ref type=""bibr"" target=""#b11"">(12,</ref> 16384,<ref type=""bibr"" target=""#b31""",0
"g (Figure <ref type=""figure"" target=""#fig_0"">1b</ref>) where simple shallow models of sentence representations <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref> are outperformed by their deeper and larger counterparts <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr",0
"asets. For example, CIFAR-10 error rate is reduced to 1% and CIFAR-100 error rate to 8.7%. These results corroborate the findings by Kornblith et al. <ref type=""bibr"" target=""#b24"">[25]</ref>, i.e., better ImageNet models transfer better.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""5",0
"of human-generated textual claims against textual evidence to be retrieved from Wikipedia.</p><p>We constructed a purpose-built dataset for this task <ref type=""bibr"" target=""#b15"">(Thorne et al., 2018)</ref> that contains 185,445 human-generated claims, manually verified against the introductory se f using human annotators to identify correct evidence when constructing the dataset was the trade-off between annotation velocity and evidence recall <ref type=""bibr"" target=""#b15"">(Thorne et al., 2018)</ref>. Evidence selected by annotators was often incomplete. As part of the FEVER shared task, an d n=""2.1"">Data</head><p>Training and development data was released through the FEVER website. 1 We used the reserved portion of the data presented in <ref type=""bibr"" target=""#b15"">Thorne et al. (2018)</ref>   </p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2.2"">Scoring Metric</head><p> al. (2018)</ref>   </p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2.2"">Scoring Metric</head><p>We used the scoring metric described in <ref type=""bibr"" target=""#b15"">Thorne et al. (2018)</ref> to evaluate the submissions. The FEVER shared task requires submission of evidence to justif rticipated in the task (presented in Table <ref type=""table"" target=""#tab_2"">2</ref>). 19 of these teams scored higher than the baseline presented in <ref type=""bibr"" target=""#b15"">Thorne et al. (2018)</ref>. All participating teams were invited to submit a description of their systems. We received on, and to highlight which sentences (if any), either individually or as a group, can be used as evidence. We retained the annotation guidelines from <ref type=""bibr"" target=""#b15"">Thorne et al. (2018)</ref> (see Sections A.7.1, A.7.3 and A.8 from that paper for more details).</p><p>At the time of w d VERification shared task attracted submissions from 86 submissions from 23 teams. 19 of these teams exceeded the score of the baseline presented in <ref type=""bibr"" target=""#b15"">Thorne et al. (2018)</ref>. For the teams which provided a system description, we highlighted the approaches, identifyi",1
", 2015)</ref> or cosine similarity over smooth inverse frequency weightings <ref type=""bibr"" target=""#b0"">(Arora et al., 2017)</ref>, ELMo embeddings <ref type=""bibr"" target=""#b11"">(Peters et al., 2018)</ref> and TF-IDF <ref type=""bibr"" target=""#b13"">(Salton et al., 1983)</ref>. UCL Machine Reading ntence similarity score from the sentence selection module. Both Ohio State and UNC-NLP report alternative token encodings: UNC-NLP report using ELMo <ref type=""bibr"" target=""#b11"">(Peters et al., 2018)</ref> and WordNet <ref type=""bibr"" target=""#b9"">(Miller, 1995)</ref> and Ohio State report using omponent of DrQA to get the top 5 sentences and then further extracted the top 3 sentences using cosine similarity between vectors obtained from Elmo <ref type=""bibr"" target=""#b11"">(Peters et al., 2018)</ref> sentence embeddings of the claim and the evidence.</p><p>For RTE we used the same model as",0
"y using Word Mover's Distance <ref type=""bibr"" target=""#b8"">(Kusner et al., 2015)</ref> or cosine similarity over smooth inverse frequency weightings <ref type=""bibr"" target=""#b0"">(Arora et al., 2017)</ref>, ELMo embeddings <ref type=""bibr"" target=""#b11"">(Peters et al., 2018)</ref> and TF-IDF <ref t",0
"dy. UNC-NLP, Athene UKP TU Darmstadt and Columbia NLP modeled the task as supervised binary classification, using architectures such as Enhanced LSTM <ref type=""bibr"" target=""#b4"">(Chen et al., 2016)</ref>, Decomposable Attention <ref type=""bibr"" target=""#b10"">(Parikh et al., 2016)</ref> or similar claim, sentence representations and training schemes. While many different approaches were used for sentence pair classification, e.g. Enhanced LSTM <ref type=""bibr"" target=""#b4"">(Chen et al., 2016)</ref>, Decomposable Attention <ref type=""bibr"" target=""#b10"">(Parikh et al., 2016)</ref>, Transforme",0
"e=""bibr"" target=""#b4"">(Chen et al., 2016)</ref>, Decomposable Attention <ref type=""bibr"" target=""#b10"">(Parikh et al., 2016)</ref>, Transformer Model <ref type=""bibr"" target=""#b12"">(Radford and Salimans, 2018)</ref>, Random Forests <ref type=""bibr"" target=""#b14"">(Svetnik et al., 2003)</ref> and ense xtraction and verification challenge that uses a high precision entailment classifier based on transformer networks pretrained with language modeling <ref type=""bibr"" target=""#b12"">(Radford and Salimans, 2018)</ref>, to classify a broad set of potential evidence. The precision of the entailment clas",0
"nsE <ref type=""bibr"" target=""#b1"">(Bordes et al. 2013) and</ref><ref type=""bibr"">TransH (Wang et al. 2014</ref>)), and bilinear models, e.g. DistMult <ref type=""bibr"" target=""#b22"">(Yang et al. 2015)</ref> and ComplEx <ref type=""bibr"" target=""#b20"">(Trouillon et al. 2016)</ref>, have achieved promis is first adopted by RESCAL <ref type=""bibr"" target=""#b17"">(Nickel, Tresp, and Kriegel 2011)</ref>, a collective matrix factorization model. DistMult <ref type=""bibr"" target=""#b22"">(Yang et al. 2015)</ref> constrains W r as a diagonal matrix which reduces the computing cost and also enhances the per e inner product. This function captures the relatedness between embeddings h and t under the condition of relation r and is first adopted by DistMult <ref type=""bibr"" target=""#b22"">(Yang et al. 2015)</ref>. We employ this triple modeling technique for three reasons: (i) This technique has represente ered in our comparison, which include (i) deterministic KG embedding models TransE <ref type=""bibr"" target=""#b1"">(Bordes et al. 2013)</ref>, DistMult <ref type=""bibr"" target=""#b22"">(Yang et al. 2015)</ref> and ComplEx <ref type=""bibr"" target=""#b20"">(Trouillon et al. 2016)</ref>, (ii) an uncertain gr and ComplEx <ref type=""bibr"" target=""#b20"">(Trouillon et al. 2016)</ref>, have achieved promising performance in many tasks, such as link prediction <ref type=""bibr"" target=""#b22"">(Yang et al. 2015;</ref><ref type=""bibr"" target=""#b20"">Trouillon et al. 2016)</ref>, relation extraction (Weston, <ref dataset for deterministic KG embeddings <ref type=""bibr"" target=""#b1"">(Bordes et al. 2013;</ref><ref type=""bibr"" target=""#b20"">Wang et al. 2014;</ref><ref type=""bibr"" target=""#b22"">Yang et al. 2015)</ref>, while NL27k is a larger dataset. PPI5k is a denser graph with fewer entities but more relation",1
"the embedding space. Hence, they have been the crucial feature models that benefit numerous knowledge-driven tasks (Bordes, Weston, and Usunier 2014; <ref type=""bibr"" target=""#b8"">He et al. 2017;</ref><ref type=""bibr"" target=""#b5"">Das et al. 2018)</ref>. Recently, extensive efforts have been devoted",0
"any relation r. <ref type=""bibr"">Variants of TransE, such as TransH (Wang et al. 2014</ref><ref type=""bibr"">), TransR (Lin et al. 2015)</ref>, TransD <ref type=""bibr"" target=""#b11"">(Ji et al. 2015), and</ref><ref type=""bibr"">TransA, (Jia et al. 2016)</ref> differentiate the translations of entity em",0
"nge-Loss Markov Random Field (HL-MRF), PSL is widely used in probabilistic reasoning tasks, such as social-trust prediction and preference prediction <ref type=""bibr"" target=""#b0"">(Bach et al. 2013;</ref><ref type=""bibr"">2017)</ref>. In this paper, we adopt PSL to enhance the embedding model perform is to minimize the distance to rule satisfaction for each triple l. In particular, we choose to use the square of the distance as the following loss <ref type=""bibr"" target=""#b0"">(Bach et al. 2013)</ref>:</p><formula xml:id=""formula_12"">J − = l∈L − γ∈Γ l |ψγ(f (l))| 2 (14)</formula><p>where ψ γ (f",0
"tions as the second-order correlations between entities, using the scoring function f (h, r, t) = h T W r t. This function is first adopted by RESCAL <ref type=""bibr"" target=""#b17"">(Nickel, Tresp, and Kriegel 2011)</ref>, a collective matrix factorization model. DistMult <ref type=""bibr"" target=""#b2",0
"rrectly interpret negative links, we add the same amount of negative links as existing relation facts into the test sets.</p><p>We use Adam optimizer <ref type=""bibr"" target=""#b16"">(Kingma and Ba 2014)</ref> for training, for which we set the exponential decay rates β 1 = 0.9 and β 2 = 0.99. We repo nk all the entities in the vocabulary as tail candidates and evaluate the ranking performance using the normalized Discounted Cumulative G ain (nDCG) <ref type=""bibr"" target=""#b16"">(Li, Liu, and Zhai 2009)</ref>. We define the gain in retrieving a relevant tail t 0 as the ground truth confidence sco",0
"ial feature models that benefit numerous knowledge-driven tasks (Bordes, Weston, and Usunier 2014; <ref type=""bibr"" target=""#b8"">He et al. 2017;</ref><ref type=""bibr"" target=""#b5"">Das et al. 2018)</ref>. Recently, extensive efforts have been devoted into embedding deterministic KGs. Translational mo",0
"or architecture of the new model, or (b) a better-tuned training procedure and / or hyperparameter configuration that unfairly benefits the new model <ref type=""bibr"" target=""#b7"">[Lipton and Steinhardt, 2018]</ref>.</p><p>In this paper we address these issues and perform a thorough experimental eva ription is provided in Appendix A). Such diverse experimental setups makes it hard to empirically identify the driver behind the improved performance <ref type=""bibr"" target=""#b7"">[Lipton and Steinhardt, 2018]</ref>. Thus, in our experiments we use a standardized training and hyperparameter tuning p",1
"proposed models have all been tested exclusively on the same train/validation/test splits of the same three datasets (CORA, CiteSeer and PubMed) from <ref type=""bibr"" target=""#b15"">Yang et al. [2016]</ref>. Such experimental setup favors the model that overfits the most and defeats the main purpose w.tei-c.org/ns/1.0""><head n=""2"">Models</head><p>We consider the problem of semi-supervised transductive node classification in a graph, as defined in <ref type=""bibr"" target=""#b15"">Yang et al. [2016]</ref>. In this paper we compare the four following popular graph neural network architectures. Graph igDesc>(a)  Relative accuracy scores and ranks averaged over all datasets. See text for the definition. (b) Model accuracy on the Planetoid split from<ref type=""bibr"" target=""#b15"">Yang et al. [2016]</ref> and another split on the same datasets. Different splits lead to a completely different rankin validation/test splits on the performance, we execute the following simple experiment. We run the 4 models on the datasets and respective splits from <ref type=""bibr"" target=""#b15"">[Yang et al., 2016]</ref>. As shown in Table <ref type=""table"" target=""#tab_2"">2b</ref>, GAT achieves the best scores f",1
"yer).</p><p>For a more balanced comparison, however, we use the same training procedure for all the models. That is, we used the same optimizer (Adam <ref type=""bibr"" target=""#b4"">[Kingma and Ba, 2015]</ref> with default parameters), same initialization (weights initialized according to <ref type=""b",0
"ribute-based models that do not consider the graph structure. Label Propagation (LabelProp) and Normalized Laplacian Label Propagation (LabelProp NL) <ref type=""bibr"" target=""#b1"">[Chapelle et al., 2009]</ref>, on the other hand, only consider the graph structure and ignore the node attributes.</p><",0
"mata et al., 2012]</ref>, CiteSeer and CORA from <ref type=""bibr"" target=""#b13"">Sen et al. [2008]</ref>, as well as the extended version of CORA from <ref type=""bibr"" target=""#b0"">Bojchevski and Günnemann [2018]</ref>, denoted as CORA-Full. We also introduce four new datasets for the node classifica",0
"mental evaluation of four prominent GNN architectures on the transductive semi-supervised node classification task. We implement the four models -GCN <ref type=""bibr"" target=""#b5"">[Kipf and Welling, 2017]</ref>, MoNet <ref type=""bibr"" target=""#b11"">[Monti et al., 2017]</ref>, GraphSage <ref type=""bi "">Yang et al. [2016]</ref>. In this paper we compare the four following popular graph neural network architectures. Graph Convolutional Network (GCN) <ref type=""bibr"" target=""#b5"">[Kipf and Welling, 2017]</ref> is one of the earlier models that works by performing a linear approximation to spectral",0
"p://www.tei-c.org/ns/1.0""><head n=""3"">Evaluation</head><p>Datasets For our experiments, we used the four well-known citation network datasets: PubMed <ref type=""bibr"" target=""#b12"">[Namata et al., 2012]</ref>, CiteSeer and CORA from <ref type=""bibr"" target=""#b13"">Sen et al. [2008]</ref>, as well as",0
"ctive semi-supervised node classification task. We implement the four models -GCN <ref type=""bibr"" target=""#b5"">[Kipf and Welling, 2017]</ref>, MoNet <ref type=""bibr"" target=""#b11"">[Monti et al., 2017]</ref>, GraphSage <ref type=""bibr"" target=""#b3"">[Hamilton et al., 2017]</ref> and GAT <ref type=""bi 017]</ref> is one of the earlier models that works by performing a linear approximation to spectral graph convolutions. Mixture Model Network (MoNet) <ref type=""bibr"" target=""#b11"">[Monti et al., 2017]</ref> generalizes the GCN architecture and allows to learn adaptive convolution filters. The autho",0
"models -GCN <ref type=""bibr"" target=""#b5"">[Kipf and Welling, 2017]</ref>, MoNet <ref type=""bibr"" target=""#b11"">[Monti et al., 2017]</ref>, GraphSage <ref type=""bibr"" target=""#b3"">[Hamilton et al., 2017]</ref> and GAT <ref type=""bibr"" target=""#b14"">[Velickovic et al., 2018]</ref> -within the same fr 2018]</ref> propose an attention mechanism that allows to weigh nodes in the neighborhood differently during the aggregation step. Lastly, GraphSAGE <ref type=""bibr"" target=""#b3"">[Hamilton et al., 2017]</ref> focuses on inductive node classification, but can also be applied for transductive setting",0
"rted in other fields. Simpler models often outperform more sophisticated ones if hyperparameter tuning is performed equally carefully for all methods <ref type=""bibr"" target=""#b10"">[Melis et al., 2018</ref><ref type=""bibr"" target=""#b8"">, Lucic et al., 2017]</ref>. In future work, we plan to further",0
"fact, it changes over time slightly and slowly <ref type=""bibr"" target=""#b57"">[58,</ref><ref type=""bibr"" target=""#b60"">61]</ref>. Recently, Ai et al. <ref type=""bibr"" target=""#b0"">[1]</ref> have presented a personalized product search method, which falls into the second category. They modeled the lo Short-Term Preference Modeling for Personalized Product Search 19:5 for products is also critical to persuade users to purchase. Recently, Ai et al. <ref type=""bibr"" target=""#b0"">[1]</ref> proposed a personalized product search model and took into consideration the users' preference. They highlight bag-of-words representations, such as Query Likelihood Model <ref type=""bibr"" target=""#b68"">[69]</ref> and Extended Query Likelihood with User Models <ref type=""bibr"" target=""#b0"">[1]</ref> and (2) representation learning approaches based on latent space modeling, such as Latent Semantic Entity <ref aches based on latent space modeling, such as Latent Semantic Entity <ref type=""bibr"" target=""#b58"">[59]</ref> and Hierarchical Embedding Model (HEM) <ref type=""bibr"" target=""#b0"">[1]</ref>. It is worth noting that the recently proposed HEM is the state-of-the-art method for personalized product sea step size of 4,000. Extended Query Likelihood with User Models (UQL). This model is first introduced to the personalized product search by Ai et al. <ref type=""bibr"" target=""#b0"">[1]</ref>. Specifically, let U be the set of the most frequent words <ref type=""foot"" target=""#foot_9"">11</ref> of revie ponentially</p><formula xml:id=""formula_23"">{2 i | 2 ? i ? 4}.</formula><p>Hierarchical Embedding Model (HEM). This model (HEM) proposed in Reference <ref type=""bibr"" target=""#b0"">[1]</ref> is the state-of-the-art approach for the personalized product search. It extends LSE <ref type=""bibr"" target="" set of terms that describe the category of the product as the query in retrieval. Based on this observation and following the strategy of References <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b58"">59]</ref>, for each product a user purchased, we extracted the corresponding sea",1
"d to several state-of-the-art methods. It is worth mentioning that our model is applicable to many other scenarios, such as personalized movie search <ref type=""bibr"" target=""#b44"">[45]</ref> and academic article search <ref type=""bibr"" target=""#b56"">[57]</ref>.</p><p>In summary, our main contributi",1
"tion <ref type=""bibr"" target=""#b1"">[2]</ref> and information retrieval <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b33"">34,</ref><ref type=""bibr"" target=""#b46"">47,</ref><ref type=""bibr"" target=""#b63"">64]</ref>. A survey on the attention mechanism on those domain is out of the sc",1
"f type=""bibr"" target=""#b58"">[59]</ref> by mapping the user into the same latent space with the query and product. Concurrently, there is another work <ref type=""bibr"" target=""#b22"">[23]</ref> aiming to combine the visual preference and textual preference for product search. Nevertheless, they both i",0
"ly. p and q are the product and query representations after transformation, respectively. ?(?) is the activation function. In our implementation, ELU <ref type=""bibr"" target=""#b12"">[13]</ref> is adopted. <ref type=""foot"" target=""#foot_5"">7</ref>Given the current query q n , considering that the rece",0
"evertheless, the proposed model failed to explicitly model the relevance generation process and capture the important IR characteristics. Pang et al. <ref type=""bibr"" target=""#b41"">[42]</ref> extended the method in Reference <ref type=""bibr"" target=""#b21"">[22]</ref> to better capture the intrinsic r",0
"0]</ref> by using representation learning techniques (e.g., word2vec <ref type=""bibr"" target=""#b31"">[32,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" target=""#b38"">39]</ref>). Gysel et al. <ref type=""bibr"" target=""#b58"">[59]</ref> introduced a latent semantic entity model to learn t",0
"hased products.</p><p>Attentive Long Short-Term Preference Modeling for Personalized Product Search 19:3 search model. Methods in the second category <ref type=""bibr"" target=""#b35"">[36,</ref><ref type=""bibr"" target=""#b55"">56]</ref> model the long-term user preference based on user's complete browsin web and the diversity of user interests, personalization has been well recognized as an important way to improve the search experience in web search <ref type=""bibr"" target=""#b35"">[36]</ref>. Instead of giving a complete review on the personalized web search, we mainly recap the related studies fal Richardson et al. <ref type=""bibr"" target=""#b48"">[49]</ref> analyzed the long-term query logs to learn more about the user behavior. Matthijs et al. <ref type=""bibr"" target=""#b35"">[36]</ref> adopted NLP techniques, such as web page structure parsing, term extraction and part of speech tagging, to e",0
", the short-term one changes more frequently and drastically.</p><p>Traditional approaches to product search <ref type=""bibr"" target=""#b16"">[17]</ref><ref type=""bibr"" target=""#b17"">[18]</ref><ref type=""bibr"" target=""#b18"">[19]</ref><ref type=""bibr"" target=""#b58"">59]</ref> often employ simple matchin ormation) can be hard for the search engine system. Some efforts have been dedicated to solving this problem <ref type=""bibr"" target=""#b16"">[17]</ref><ref type=""bibr"" target=""#b17"">[18]</ref><ref type=""bibr"" target=""#b18"">[19]</ref>. For instance, to fill the big gap between the keyword queries and type=""bibr"" target=""#b18"">[19]</ref>. For instance, to fill the big gap between the keyword queries and the structured product entities, Duan et al. <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b18"">19]</ref> proposed a probabilistic retrieval model to optimize the ranking of",0
"long-term profile-based web search. Approaches in the first category <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b28"">29,</ref><ref type=""bibr"" target=""#b54"">55]</ref> capture the short-term user preference from search sessions. Nevertheless, different from the web search wher roblems, namely, how to determine the duration of a session and how to learn the user preference in a session. To solve these problems, Sriram et al. <ref type=""bibr"" target=""#b54"">[55]</ref> used temporal closeness and probabilistic similarities between queries to determine the duration of a sessio",0
"prediction and purchase prediction tasks. In view of clicks providing a strong signal of a user's interest in an item, the methods in the first line <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b42"">43,</ref><ref type=""bibr"" target=""#b43",0
"type=""bibr"" target=""#b26"">[27]</ref>, Match-SRNN <ref type=""bibr"" target=""#b59"">[60]</ref>, and MatchPyramid <ref type=""bibr"" target=""#b40"">[41,</ref><ref type=""bibr"" target=""#b50"">51]</ref> also share the similar idea. Guo et al. <ref type=""bibr"" target=""#b21"">[22]</ref> pointed out that the aforem",0
"bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b42"">43,</ref><ref type=""bibr"" target=""#b43"">44,</ref><ref type=""bibr"" target=""#b67"">68]</ref> focus on the click prediction of e-commerce users. For example, Yu et al. <ref type=""bibr"" target=""#b67"">[68] type=""bibr"" target=""#b43"">44,</ref><ref type=""bibr"" target=""#b67"">68]</ref> focus on the click prediction of e-commerce users. For example, Yu et al. <ref type=""bibr"" target=""#b67"">[68]</ref> proposed a Latent Dirichlet Allocation (LDA) based method for diversified product search. Approaches in <ref",0
"ref> and information retrieval <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b33"">34,</ref><ref type=""bibr"" target=""#b46"">47,</ref><ref type=""bibr"" target=""#b63"">64]</ref>. A survey on the attention mechanism on those domain is out of the scope of this article. Here we only focus ive filtering approach to modeling user preference on both the item-level and component-level is seamlessly applied on implicit feedback. Xiao et al. <ref type=""bibr"" target=""#b63"">[64]</ref> improved Factorization Machine (FM) by discriminating the importance of different feature interactions.</p><",0
"br"" target=""#b26"">[27]</ref>, ARC-2 <ref type=""bibr"" target=""#b26"">[27]</ref>, Match-SRNN <ref type=""bibr"" target=""#b59"">[60]</ref>, and MatchPyramid <ref type=""bibr"" target=""#b40"">[41,</ref><ref type=""bibr"" target=""#b50"">51]</ref> also share the similar idea. Guo et al. <ref type=""bibr"" target=""#b2",0
"d the path-level network transformation to effectively search the tree-structured architecture space. Motivated by these AutoML frameworks, He et al. <ref type=""bibr"" target=""#b10"">[11]</ref> leveraged the reinforcement learning to automatically prune the convolution channels. Our framework further",1
"arget=""#b14"">[15]</ref> made use of 8-bit integers for both weights and activations. We refer the reader to the survey paper by Krishnamoorthi et al. <ref type=""bibr"" target=""#b16"">[17]</ref> for a more detailed overview. These conventional quantization methods either simply assign the same number o",0
", many researchers have proposed to directly design efficient models <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b1"">2]</ref> or to quantize the weights and activations to low precision <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""b",0
"ardware is not optimal for the other. This suggests we need a specialized quantization solution for different hardware architectures. (HW1: BitFusion <ref type=""bibr"" target=""#b25"">[26]</ref>, HW2: BISMO <ref type=""bibr"" target=""#b26"">[27]</ref> edge accelerator, HW3: BISMO cloud accelerator, batch dware design: BISMO <ref type=""bibr"" target=""#b26"">[27]</ref> proposed the bit-serial multiplier to support multiplications of 1 to 8 bits; BitFusion <ref type=""bibr"" target=""#b25"">[26]</ref> supports multiplications of 2, 4, 8 and 16 bits in a spatial manner. However, a very missing part is how to t one time and accumulates their partial products by shifting over time.</p><p>Spatial Architecture. BitFusion architecture proposed by Hardik et al. <ref type=""bibr"" target=""#b25"">[26]</ref> is a state-of-the-art spatial ASIC design for neural network accelerator. It employs a 2D systolic array of ware architectures, we further compare our framework with PACT <ref type=""bibr"" target=""#b2"">[3]</ref> under the latency constraints on the BitFusion <ref type=""bibr"" target=""#b25"">[26]</ref> architecture (Table <ref type=""table"" target=""#tab_6"">4</ref>). Our framework performs much better than the",0
"and fully-connected layers. Courbariaux et al. <ref type=""bibr"" target=""#b4"">[5]</ref> binarized the network weights into {-1, +1}; Rastegari et al. <ref type=""bibr"" target=""#b23"">[24]</ref> and Zhou et al. <ref type=""bibr"" target=""#b32"">[33]</ref> binarized each convolution filter</p><formula xml:",0
"il search, recommendation), its use for learning-to-rank (LTR) is challenging due to its biased nature. To address this bias problem, Joachims et al. <ref type=""bibr"" target=""#b19"">[19]</ref> proposed a counterfactual inference approach, providing an unbiased LTR framework via Empirical Risk Minimiz ""bibr"" target=""#b22"">[22]</ref> and trust bias <ref type=""bibr"" target=""#b18"">[18]</ref>.</p><p>To handle biases in a principled way, Joachims et al. <ref type=""bibr"" target=""#b19"">[19]</ref> introduced an unbiased learning-to-rank framework, which is a consistent learning approach despite biased fe del, as it is arguably the simplest model for correcting the examination bias in learning to rank from implicit feedback. As shown by Joachims et al. <ref type=""bibr"" target=""#b19"">[19]</ref>, the parameters of the PBM can serve as propensity estimates, enabling the use of IPS weighting for unbiased el(q, d) ∈ {0, 1} is the binary relevance of document d for query q.</p><p>While Pr(E = 1|k) can be used as an estimate of the examination propensity <ref type=""bibr"" target=""#b19"">[19]</ref>, it is a rather simplistic model since it assumes that examination does not vary across queries. However, it ., p k max ] with p k for each position k ∈ [1, k max ]. In this case, randomly swapping results at positions k and k ′ before presenting the ranking <ref type=""bibr"" target=""#b19"">[19]</ref> makes the expected relevance of results at the two positions equal. An illustrative example is given in Figu p><p>This means that the ratio of the observed click-through rates is a consistent estimator of the relative propensities p k and p k ′ under the PBM <ref type=""bibr"" target=""#b19"">[19]</ref>. Note that knowing the relative propensities with respect to a single ""anchor"" position (e.g.</p><formula xm ormula xml:id=""formula_7"">p k p 1</formula><p>) is sufficient, since the counterfactual ERM learning objective is invariant to multiplicative scaling <ref type=""bibr"" target=""#b19"">[19]</ref>.</p><p>While this ratio estimator is a sensible approach for the PBM, it is not directly applicable to the C and rank k ∈ {1, 2, ..., 21}. These explicit swap interventions were then used to get a gold-standard estimate of the propensities via the methods in <ref type=""bibr"" target=""#b19"">[19]</ref>. To avoid any confounding due to changes in the query distribution, data for all conditions was collected in e numbers of queries and clicks are given in Table <ref type=""table"" target=""#tab_0"">1</ref>. We then use the gold-standard propensity estimator from <ref type=""bibr"" target=""#b19"">[19]</ref> to learn two PBM models from the swap intervention data, one for complex and one for simple queries.</p><p>F irs achieves much improved error bars. This is to be expected, given that AllPairs makes more efficient use of the data than the ratio-estimates from <ref type=""bibr"" target=""#b19"">[19]</ref>.</p><p>Can AllPairs learn CPBM models with many context features? it is infeasible to introduce additional f whether the CPBM model improves learning performance compared to using the propensities from the PBM.</p><p>We trained a Clipped Propensity SVM-Rank <ref type=""bibr"" target=""#b19"">[19]</ref> for each of the following three propensity models: PBM estimated via All-Pairs, CPBM estimated via AllPairs, tic data generation. All hyperparameters were picked via cross-validation. For rank r &gt; 21, we impute the propensity p r (x) = p 21 (x). Following <ref type=""bibr"" target=""#b19"">[19]</ref>, we measure test-set ranking performance via the average sum of the ranks of the relevant results across the mation for unbiased LTR.</p><p>There are two key limitations of existing propensity estimation methods for LTR <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b19"">19,</ref><ref type=""bibr"" target=""#b27"">27]</ref>. First, existing methods are restricted to the Position-Based Model ( e used to compute context-dependent propensities for LTR algorithms like <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b19"">19]</ref>. We evaluate the fidelity of the CPBM model and the effectiveness of the estimator in real-world experiments or LTR are based on the Position-Based Model (PBM) <ref type=""bibr"" target=""#b23"">[23]</ref>. The most effective methods use randomized interventions <ref type=""bibr"" target=""#b19"">[19,</ref><ref type=""bibr"" target=""#b26"">26]</ref>, which unfortunately degrade the user's search experience. To avoid for relevance at each position. To start, let us first review how explicit interventions have been used for estimating p k := Pr(E = 1|k) in the PBM <ref type=""bibr"" target=""#b19"">[19,</ref><ref type=""bibr"" target=""#b26"">26]</ref>. The PBM requires estimating a single vector p = [p 1 , p 2 , ..., p",1
"terventions, Wang et al. <ref type=""bibr"" target=""#b27"">[27]</ref> proposed a regression-based Expectation-Maximization (EM) algorithm, and Ai et al. <ref type=""bibr"" target=""#b3"">[4]</ref> proposed a learning algorithm that learns propensity models together with the ranking model. Unfortunately, bo",0
"total)</p><p>Other reasonable features can also be taken into consideration, like query performance predictors <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b11"">11]</ref>.</p><p>Table <ref type=""table"" target=""#tab_1"">2</ref> shows the test-set performance. The baseline is a PBM",0
"ere deployed on the system in the past. The resulting deep network can then be used to compute context-dependent propensities for LTR algorithms like <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b19"">19]</ref>. We evaluate the fidelity of the <ref type=""bibr"" target=""#b15"">[15,</ref><ref type=""bibr"" target=""#b24"">24]</ref>. IPS has been commonly adopted for unbiased evaluation and learning <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b12"">12,</ref><ref type=""bibr"" target=""#b21"">21",0
"nsumer-level applications. Commodity RGB-D camera based methods <ref type=""bibr"" target=""#b55"">[56]</ref>, <ref type=""bibr"" target=""#b32"">[33]</ref>, <ref type=""bibr"" target=""#b5"">[6]</ref>, <ref type=""bibr"" target=""#b51"">[52]</ref> have demonstrated real-time reconstruction and animation Yudong Guo",1
"arget=""#b2"">[3]</ref> presents an algorithm for fully automatically fitting a 3D Morphable Model to a single image using landmarks and edge features. <ref type=""bibr"" target=""#b45"">[46]</ref> introduces a framework to fit a parametric face model with Bayesian inference. <ref type=""bibr"" target=""#b12 ompare our method with <ref type=""bibr"" target=""#b41"">[42]</ref>, <ref type=""bibr"" target=""#b2"">[3]</ref>, <ref type=""bibr"" target=""#b23"">[24]</ref>, <ref type=""bibr"" target=""#b45"">[46]</ref>, <ref type=""bibr"" target=""#b13"">[14]</ref> on single-image based face reconstruction. We thank the authors o the forehead regions). The reconstruction results of the methods <ref type=""bibr"" target=""#b2"">[3]</ref>, <ref type=""bibr"" target=""#b23"">[24]</ref>, <ref type=""bibr"" target=""#b45"">[46]</ref>, <ref type=""bibr"" target=""#b13"">[14]</ref> are generated using the source codes provided by the authors 2345 es, and the results of <ref type=""bibr"" target=""#b41"">[42]</ref>, <ref type=""bibr"" target=""#b2"">[3]</ref>, <ref type=""bibr"" target=""#b23"">[24]</ref>, <ref type=""bibr"" target=""#b45"">[46]</ref>, <ref type=""bibr"" target=""#b13"">[14]</ref> and ours. It can be seen that our results are more convincing in",0
"<ref type=""bibr"" target=""#b52"">[53]</ref> and face replacement <ref type=""bibr"" target=""#b29"">[30]</ref>, <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" target=""#b15"">[16]</ref>. Traditional facial performance capture methods usually require complex hardware and significant user interv",0
"ough such a model is able to capture the global structure of a 3D face from a single image <ref type=""bibr"" target=""#b3"">[4]</ref> or multiple images <ref type=""bibr"" target=""#b1"">[2]</ref>, the facial details like wrinkles and folds are not possible to be captured. In addition, the reconstructed fa",0
"tion <ref type=""bibr"" target=""#b4"">[5]</ref>, <ref type=""bibr"" target=""#b53"">[54]</ref> and face animation <ref type=""bibr"" target=""#b22"">[23]</ref>, <ref type=""bibr"" target=""#b52"">[53]</ref>. Video-based dense face reconstruction and tracking or facial performance capturing has a long history <ref y <ref type=""bibr"" target=""#b56"">[57]</ref> also with many applications such as facial expression transfer <ref type=""bibr"" target=""#b51"">[52]</ref>, <ref type=""bibr"" target=""#b52"">[53]</ref> and face replacement <ref type=""bibr"" target=""#b29"">[30]</ref>, <ref type=""bibr"" target=""#b11"">[12]</ref>, < ave been proposed for RGB video based facial performance captureing <ref type=""bibr"" target=""#b7"">[8]</ref>, <ref type=""bibr"" target=""#b6"">[7]</ref>, <ref type=""bibr"" target=""#b52"">[53]</ref>, <ref type=""bibr"" target=""#b17"">[18]</ref>, <ref type=""bibr"" target=""#b44"">[45]</ref>, <ref type=""bibr"" targ ef> can reconstruct personalized face rig of high-quality, but their optimization-based method is time-consuming and needs about 3 minutes per frame. <ref type=""bibr"" target=""#b52"">[53]</ref> achieves real-time face reconstruction and facial reenactment through data-parallel optimization strategy, b <ref type=""bibr"" target=""#b59"">[60]</ref>, <ref type=""bibr"" target=""#b33"">[34]</ref>, <ref type=""bibr"" target=""#b26"">[27]</ref>, and face reenactment <ref type=""bibr"" target=""#b52"">[53]</ref>. Although such a model is able to capture the global structure of a 3D face from a single image <ref type=""b ideos. Recently, a variety of methods have been proposed to do 3D face reconstruction with monocular RGB video. Most of them use a 3D Morphable Model <ref type=""bibr"" target=""#b52"">[53]</ref>, <ref type=""bibr"" target=""#b17"">[18]</ref>, <ref type=""bibr"" target=""#b21"">[22]</ref> or a multi-linear face el to the detected landmarks. Although they are able to reconstruct and track 3D face in real-time, they do not estimate facial appearance. Recently, <ref type=""bibr"" target=""#b52"">[53]</ref> presented an approach for real-time face tracking and facial reenactment, but the method is not able to reco </ref> proposes to use an analysis-by-synthesis energy function as the loss function during network training <ref type=""bibr"" target=""#b3"">[4]</ref>, <ref type=""bibr"" target=""#b52"">[53]</ref>. <ref type=""bibr"" target=""#b23"">[24]</ref> proposes to directly regress volumes with CNN for a single face i roll}, ? t = {s, t} and ? l = {? alb , r}. The fitting process is based on the analysisby-synthesis strategy <ref type=""bibr"" target=""#b3"">[4]</ref>, <ref type=""bibr"" target=""#b52"">[53]</ref>, and we seek a solution that by minimizes the difference between the input face image and the rendered image",0
"and/or bioactive compounds.</p><p>In terms of the statistics, there are tens of millions of compounds available in compound and bioactivity databases <ref type=""bibr"" target=""#b0"">[1]</ref><ref type=""bibr"" target=""#b1"">[2]</ref><ref type=""bibr"" target=""#b2"">[3]</ref><ref type=""bibr"" target=""#b3"">[4] al drug discovery (specifically VS) in the past decade. In this sense, the prominent bioactivity and compound data resources can be listed as PubChem <ref type=""bibr"" target=""#b0"">[1]</ref>, ChEMBL <ref type=""bibr"" target=""#b1"">[2]</ref>, DrugBank <ref type=""bibr"" target=""#b4"">[5]</ref>, STITCH <ref annotations resources (e.g. UniProtKB/Swiss-Prot) and nearly 2700 of human proteins are known to be targeted by either approved or experimental drugs <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b5"">6]</ref>. The 3D structure information of proteins and compounds provide importan",1
"as other applications such as finding beneficial drug pairs <ref type=""bibr"" target=""#b10"">[11]</ref> and the prediction of ATC codes for known drugs <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b12"">13]</ref>. In addition, the computational approaches mainly employed in VS can pecific interaction. Examples of feature-based VS methods are given below.</p><p>A supervised machine learning methodology was proposed by Liu et al. <ref type=""bibr"" target=""#b11"">[12]</ref> using a combination of both similarity and feature-based approaches to predict drug-ATC code associations. D",0
"ers; however, after a certain point, adding more features to the system starts to decrease the performance, which is known as curse of dimensionality <ref type=""bibr"" target=""#b225"">[226]</ref>. Therefore, feature-based methods may require the application of feature selection techniques to reduce di",0
"ther with the known DTIs to generate predictions for target families of enzymes, ion channels, GPCRs and nuclear receptors. In another study, Go ?nen <ref type=""bibr"" target=""#b211"">[212]</ref>  In a review study by Chen et al., the available resources for DTI prediction were presented, including da tudies, especially between 2006 and 2013 <ref type=""bibr"" target=""#b66"">[67,</ref><ref type=""bibr"">90,</ref><ref type=""bibr"" target=""#b183"">184,</ref><ref type=""bibr"" target=""#b211"">212,</ref><ref type=""bibr"" target=""#b213"">[214]</ref><ref type=""bibr"" target=""#b214"">[215]</ref><ref type=""bibr"" targe",0
"g/ns/1.0""><head>VI. RELATED WORK</head><p>The issue queue was extensively studied around 2000. A comprehensive survey was carried out by Abella et al <ref type=""bibr"" target=""#b23"">[28]</ref>.</p><p>Butler et al. investigated the effect of several select policies of the IQ, including random selectio",1
"ed, dependent instructions cannot be issued back-to-back.</p><p>There are two types of wakeup logic circuits: content addressable memory (CAM) or RAM <ref type=""bibr"" target=""#b3"">[5]</ref>, <ref type=""bibr"" target=""#b5"">[7]</ref>. In the CAM type, the wakeup logic is a one-dimensional array, where . If both data dependences are resolved, an issue request is output.</p><p>In contrast, the RAM type has two matrices for each of two source operands <ref type=""bibr"" target=""#b3"">[5]</ref>. Each row and column of the matrix is associated with an instruction in the IQ, and each element represents th",0
"t publish their IQ organization, the random queue alone or with an age matrix are used in modern processors <ref type=""bibr"" target=""#b9"">[11]</ref>- <ref type=""bibr"" target=""#b11"">[13]</ref>. In this paper, we assume a random queue without an age matrix as the base organization, and compare the per and Skylake <ref type=""bibr"" target=""#b12"">[14]</ref>- <ref type=""bibr"" target=""#b14"">[16]</ref>, and is also used as the main IQ of IBM POWER7 and 8 <ref type=""bibr"" target=""#b11"">[13]</ref>, <ref type=""bibr"" target=""#b15"">[17]</ref>. In contrast, AMD Zen uses an IQ distributed among integer functi he random queue because of random instruction ordering. To mitigate the IPC degradation, several processors <ref type=""bibr"" target=""#b9"">[11]</ref>- <ref type=""bibr"" target=""#b11"">[13]</ref> add a circuit called the age matrix to the IQ.</p><p>The age matrix receives the issue requests in parallel high complexity and large area.</p><p>In several modern processors, a random queue with age matrix is used <ref type=""bibr"" target=""#b9"">[11]</ref>- <ref type=""bibr"" target=""#b11"">[13]</ref>. Because the position-based select logic cannot implement the age-aware policy in the random queue, the age abled or disabled.</figDesc></figure> <figure xmlns=""http://www.tei-c.org/ns/1.0"" xml:id=""fig_12""><head>Fig. 13 .</head><label>13</label><figDesc>Fig.<ref type=""bibr"" target=""#b11"">13</ref>. Comparison of performance with processor with a large branch predictor with 8.4KB cost increase.</figDesc></f",0
"anch prediction is categorized as this approach. Branch predictors have been studied for several decades, but are extensively studied even at present <ref type=""bibr"" target=""#b0"">[2]</ref>, because the performance degradation due to misprediction is still serious, even with a modern branch predicto",0
"is sent to the select logic, which grants some requests by considering resource constraints. As a circuit of the select logic, a tree arbiter circuit <ref type=""bibr"" target=""#b6"">[8]</ref> and prefix-sum circuit <ref type=""bibr"" target=""#b4"">[6]</ref>, <ref type=""bibr"" target=""#b7"">[9]</ref> are pu f type=""bibr"" target=""#b7"">[9]</ref> are published. When using the tree arbiter circuit, the circuit must be stacked by the number of the issue width <ref type=""bibr"" target=""#b6"">[8]</ref>, <ref type=""bibr"" target=""#b7"">[9]</ref>. This stacking considerably lengthens the delay of the select logic < <ref type=""bibr"" target=""#b6"">[8]</ref>, <ref type=""bibr"" target=""#b7"">[9]</ref>. This stacking considerably lengthens the delay of the select logic <ref type=""bibr"" target=""#b6"">[8]</ref>. In contrast, the prefix-sum circuits does not need to be stacked for multiple issues (i.e., a single circuit to make the select logic simple, where the closer to the head the instruction is, the higher the priority is <ref type=""bibr"" target=""#b4"">[6]</ref>, <ref type=""bibr"" target=""#b6"">[8]</ref>, <ref type=""bibr"" target=""#b7"">[9]</ref>. Otherwise, the delay of the IQ is lengthened (see Section III-C1). I ngthened (see Section III-C1). Increasing the delay is not acceptable, because the critical path of the IQ is one of the critical paths of processors <ref type=""bibr"" target=""#b6"">[8]</ref>, and thus increasing the delay of the IQ can lengthen the clock cycle time.</p><p>Taxonomy in terms of instruc",0
"<ref type=""bibr"" target=""#b29"">[30]</ref>  <ref type=""bibr"" target=""#b30"">[31]</ref>. A hybrid approach has been introduced to increase its viability <ref type=""bibr"" target=""#b31"">[32]</ref>.</p><p>The concept of STRAIGHT <ref type=""bibr"" target=""#b6"">[7]</ref> reduces the required hardware and imp",1
"major drawbacks.</p><p>The limitation and optimization of the superscalar pipeline depth are comprehensively explored around the turn of the century <ref type=""bibr"" target=""#b15"">[16]</ref> [17] <ref type=""bibr"" target=""#b17"">[18]</ref>, which enforces researchers to explore novel ILP architecture",0
"wer-utilized cores, reflecting the recent dilemma that even though the number of transistors can be increased, they cannot be switched simultaneously <ref type=""bibr"" target=""#b1"">[2]</ref>. In this scenario, the CPU is expected to effectively execute the programs that are not parallelized or cannot e, "" <ref type=""bibr"" target=""#b0"">[1]</ref>"" of instruction  I 2 means that the instruction uses the result value of the previous instruction, and "" <ref type=""bibr"" target=""#b1"">[2]</ref>"" means that the other operand is the result value of the second previous instruction. Therefore, this code cal e BB1 take the counter variable as its source operand. There are two paths to this operand: from the BB0 and the BB2. The operand of SLTi is fixed as <ref type=""bibr"" target=""#b1"">[2]</ref> by adding RMOVs.</p><p>3) Distance Bounding: The STRAIGHT architecture has the maximum distance of source regi",0
"us multicore processors, the technology for ""OoO performance with in-order power"" is desired for the total chip performance improvement. Kumar et al. <ref type=""bibr"" target=""#b24"">[25]</ref> showed the potential of heterogeneous multicore architectures to improve the efficiency of a single-thread e",0
"d to supply many instruction streams to the core(s) by speculatively separating a singlethreaded program into multiple threads. Slipstream processors <ref type=""bibr"" target=""#b22"">[23]</ref> and Runahead execution <ref type=""bibr"" target=""#b23"">[24]</ref> are similar but more drastic technologies t",0
"18"">[18]</ref>, compiler optimization <ref type=""bibr"" target=""#b19"">[19]</ref>- <ref type=""bibr"" target=""#b21"">[21]</ref>, architecture optimization <ref type=""bibr"" target=""#b22"">[22]</ref>, and many more. A number of programable performance measurement tools have therefore been developed, includi e how large-scale online services use resources in data centers and then they provide several insights for server architecture design in data centers <ref type=""bibr"" target=""#b22"">[22]</ref>. Chen et al. leveraged hardware-event sampling to generate edge profiles to perform feedback-directed optimi",1
"ith the Google Wide profiler (GWP) to provide more meaningful results. In addition, it can be integrated with cluster management tools such as Quasar <ref type=""bibr"" target=""#b46"">[46]</ref> and other cloud computing researches such as <ref type=""bibr"" target=""#b47"">[47]</ref>- <ref type=""bibr"" tar",0
"orkload characterization <ref type=""bibr"" target=""#b7"">[7]</ref>- <ref type=""bibr"" target=""#b14"">[14]</ref>, performance optimization of applications <ref type=""bibr"" target=""#b15"">[15]</ref>- <ref type=""bibr"" target=""#b18"">[18]</ref>, compiler optimization <ref type=""bibr"" target=""#b19"">[19]</ref>- //www.tei-c.org/ns/1.0""><head>B. Motivation</head><p>1) Measurement Errors: while some prior works such as <ref type=""bibr"" target=""#b14"">[14]</ref>, <ref type=""bibr"" target=""#b15"">[15]</ref> employ OCOE to measure performance, MLPX is mandatory when a large number of events need to be sampled, e.g. nd Gumbel (general extreme value -GEV) distributions. We find the GEV distribution best fits the long tail distributions, which has been confirmed by <ref type=""bibr"" target=""#b15"">[15]</ref>.</p><p>After knowing the value distributions of the events, we design the criterion to replace the outliers. CPI (Cycle Per Instruction) collected from hardware counters, Zhang et al. proposed a CPU performance isolation strategy for shared compute clusters <ref type=""bibr"" target=""#b15"">[15]</ref>. Tam et al. firstly carefully analyzed the L2 cache miss rate behavior of commodity systems and subsequently",0
"I. INTRODUCTION</head><p>Modern processors typically provide 4-8 hardware counters to measure hundreds of crucial events such as cache and TLB misses <ref type=""bibr"" target=""#b1"">[1]</ref>- <ref type=""bibr"" target=""#b4"">[4]</ref>. These events can generally reveal root causes and key insights about easure specific events such as clock cycles and retired instructions) and eight programmable counters per core (four per SMT thread if it is enabled) <ref type=""bibr"" target=""#b1"">[1]</ref>.</p><p>The events that can be measured by hardware counters are predefined by processor vendors. The number of",0
"target=""#b28"">[28]</ref> and then proposed to leverage statistics techniques such as clustering to analyze the performance data of parallel machines <ref type=""bibr"" target=""#b52"">[52]</ref>. Dong et al. proposed to use statistical techniques such as PCA (Principle Component Analysis) to extract im",0
"dicting the result of the instruction <ref type=""bibr"" target=""#b13"">[14]</ref>- <ref type=""bibr"" target=""#b16"">[17]</ref>, issuing prefetch requests <ref type=""bibr"" target=""#b17"">[18]</ref>, etc. However, the impact of these optimizations has not been studied to date for mobile workloads, and that possibly one or more non-critical instructions coming into the dependence chain between them. For instance, we show that one such recent optimization <ref type=""bibr"" target=""#b17"">[18]</ref>, which prioritizes critical loads, does very well for SPEC workloads (as in prior works), but provides a mea #b25"">[26]</ref>- <ref type=""bibr"" target=""#b27"">[28]</ref>, caches <ref type=""bibr"" target=""#b7"">[8]</ref>, <ref type=""bibr"" target=""#b8"">[9]</ref>, <ref type=""bibr"" target=""#b17"">[18]</ref>, memory requests <ref type=""bibr"" target=""#b10"">[11]</ref>, predicting instruction results <ref type=""bibr"" s <ref type=""bibr"" target=""#b13"">[14]</ref>, <ref type=""bibr"" target=""#b28"">[29]</ref>- <ref type=""bibr"" target=""#b30"">[31]</ref>, issuing prefetches <ref type=""bibr"" target=""#b17"">[18]</ref>, etc. Until now, these optimizations have been primarily proposed and evaluated for server/desktop workloads lstudied and well-proven criticality optimizations in prioritizing two important resources -one for memory which issues prefetches for critical loads <ref type=""bibr"" target=""#b17"">[18]</ref> and another for ALU resources in instruction scheduling <ref type=""bibr"" target=""#b3"">[4]</ref>, <ref type="" t=""#b31"">[32]</ref>, <ref type=""bibr"" target=""#b32"">[33]</ref>. These proposals identify high-fanout loads to mark them as critical to issue prefetch <ref type=""bibr"" target=""#b17"">[18]</ref> and prioritize the critical instructions for ALU resource allocations <ref type=""bibr"" target=""#b31"">[32]</r erver workloads. The highfanout based optimization has also been shown to outperform the latency based ways of identifying and exploiting criticality <ref type=""bibr"" target=""#b17"">[18]</ref>, <ref type=""bibr"" target=""#b33"">[34]</ref>. We next evaluate the usefulness of both these criticality optimi nt for SPEC.int (15% from prefetching, 9% from prioritizing) and SPEC.float (34% from prefetching, 25% from prioritizing), re-affirming prior results <ref type=""bibr"" target=""#b17"">[18]</ref>, <ref type=""bibr"" target=""#b32"">[33]</ref>. Interestingly, the gains from these two optimizations are a rela ty based optimizations <ref type=""bibr"" target=""#b8"">[9]</ref>, <ref type=""bibr"" target=""#b10"">[11]</ref>, <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" target=""#b17"">[18]</ref> have targeted one critical instruction at a time, rather than groups or chains.</p></div> <div xmlns=""http:/ "" target=""#b58"">[59]</ref> Memory2-way 32KB i-cache, 64KB d-cache, 2 cycle hit latency; 8-way 2MB L2 with System CLPT prefetcher (1024?7bits entries) <ref type=""bibr"" target=""#b17"">[18]</ref>   Data Processing call), and the 3-bit argument with it to denote that the next l + 1 instructions would be cal load optimizations <ref type=""bibr"" target=""#b8"">[9]</ref>, <ref type=""bibr"" target=""#b10"">[11]</ref>, <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" target=""#b17"">[18]</ref>, <ref type=""bibr"" target=""#b78"">[79]</ref> or even backend optimizations for critical instructions such as < 81"">[82]</ref>. However, such techniques require fairly extensive hardware to identify these chains, and optimizing for them, e.g. techniques such as <ref type=""bibr"" target=""#b17"">[18]</ref>, <ref type=""bibr"" target=""#b75"">[76]</ref>, <ref type=""bibr"" target=""#b76"">[77]</ref> require 16KB SRAM, and",1
"ical loads <ref type=""bibr"" target=""#b17"">[18]</ref> and another for ALU resources in instruction scheduling <ref type=""bibr"" target=""#b3"">[4]</ref>, <ref type=""bibr"" target=""#b31"">[32]</ref>, <ref type=""bibr"" target=""#b32"">[33]</ref>. These proposals identify high-fanout loads to mark them as criti rk them as critical to issue prefetch <ref type=""bibr"" target=""#b17"">[18]</ref> and prioritize the critical instructions for ALU resource allocations <ref type=""bibr"" target=""#b31"">[32]</ref>, <ref type=""bibr"" target=""#b32"">[33]</ref>. These techniques have shown significant benefits for server work the prioritization hardware for the back-end resources proposed in <ref type=""bibr"" target=""#b32"">[33]</ref>, using the tracking hardware proposed in <ref type=""bibr"" target=""#b31"">[32]</ref>, which requires 1.5KB SRAM for maintaining the tokens. ? AllHW: This consists of hardwares for both front an",1
"t=""#b15"">[16]</ref>, <ref type=""bibr"" target=""#b24"">[25]</ref>, <ref type=""bibr"" target=""#b30"">[31]</ref>, <ref type=""bibr"" target=""#b74"">[75]</ref>- <ref type=""bibr"" target=""#b76"">[77]</ref>, which prioritize the backend resources for those instructions, we additionally consider the following confi t=""#b23"">[24]</ref>, <ref type=""bibr"" target=""#b24"">[25]</ref>, <ref type=""bibr"" target=""#b30"">[31]</ref>, <ref type=""bibr"" target=""#b74"">[75]</ref>- <ref type=""bibr"" target=""#b76"">[77]</ref>. While one can potentially employ these optimizations for mobile apps, as we showed (in Fig. <ref type=""figu these chains, and optimizing for them, e.g. techniques such as <ref type=""bibr"" target=""#b17"">[18]</ref>, <ref type=""bibr"" target=""#b75"">[76]</ref>, <ref type=""bibr"" target=""#b76"">[77]</ref> require 16KB SRAM, and <ref type=""bibr"" target=""#b78"">[79]</ref> incurs 22% additional power, making them le",0
"memory request queues <ref type=""bibr"" target=""#b10"">[11]</ref>- <ref type=""bibr"" target=""#b12"">[13]</ref>, predicting the result of the instruction <ref type=""bibr"" target=""#b13"">[14]</ref>- <ref type=""bibr"" target=""#b16"">[17]</ref>, issuing prefetch requests <ref type=""bibr"" target=""#b17"">[18]</r ""#b8"">[9]</ref>, <ref type=""bibr"" target=""#b17"">[18]</ref>, memory requests <ref type=""bibr"" target=""#b10"">[11]</ref>, predicting instruction results <ref type=""bibr"" target=""#b13"">[14]</ref>, <ref type=""bibr"" target=""#b28"">[29]</ref>- <ref type=""bibr"" target=""#b30"">[31]</ref>, issuing prefetches <r ch is somewhat addressed by prior criticality optimizations such as <ref type=""bibr"" target=""#b4"">[5]</ref>, <ref type=""bibr"" target=""#b5"">[6]</ref>, <ref type=""bibr"" target=""#b13"">[14]</ref>, <ref type=""bibr"" target=""#b15"">[16]</ref>, <ref type=""bibr"" target=""#b24"">[25]</ref>, <ref type=""bibr"" targ ef> or even backend optimizations for critical instructions such as <ref type=""bibr"" target=""#b4"">[5]</ref>, <ref type=""bibr"" target=""#b5"">[6]</ref>, <ref type=""bibr"" target=""#b13"">[14]</ref>, <ref type=""bibr"" target=""#b15"">[16]</ref>, <ref type=""bibr"" target=""#b23"">[24]</ref>, <ref type=""bibr"" targ",0
"ally integrate with them to significantly boost the improvements. While similar in spirit to some of the prior work on instruction stream compression <ref type=""bibr"" target=""#b100"">[101]</ref>- <ref type=""bibr"" target=""#b102"">[103]</ref>, we quantitatively showed the need to identify critical chain",0
"izations for Mobile Platforms: There has been significant recent interest to optimize mobile CPU execution <ref type=""bibr"" target=""#b82"">[83]</ref>- <ref type=""bibr"" target=""#b88"">[89]</ref>. Some of these optimizations target specific domains (e.g. web-browsers <ref type=""bibr"" target=""#b89"">[90]<",0
"rI contributes to 3% of the overall execution in SPEC, Android apps execute from a much larger code base with a diverse set of libraries (&gt;7k APIs <ref type=""bibr"" target=""#b34"">[35]</ref>- <ref type=""bibr"" target=""#b36"">[37]</ref>) with more frequent function calls, which causes i-cache stalls f tions selectively before doing the compression. Software Profiling for Mobile Platforms: A number of software profiling frameworks have been proposed <ref type=""bibr"" target=""#b34"">[35]</ref>, <ref type=""bibr"" target=""#b103"">[104]</ref>- <ref type=""bibr"" target=""#b106"">[107]</ref> -studying library ype=""bibr"" target=""#b34"">[35]</ref>, <ref type=""bibr"" target=""#b103"">[104]</ref>- <ref type=""bibr"" target=""#b106"">[107]</ref> -studying library usage <ref type=""bibr"" target=""#b34"">[35]</ref>, <ref type=""bibr"" target=""#b105"">[106]</ref>, app-market level changes to the source/advertisement models, <",0
"employed. Over the years, numerous criticality based optimizations have been proposed and studied for high-end workloads -prioritizing CPU resources <ref type=""bibr"" target=""#b3"">[4]</ref>- <ref type=""bibr"" target=""#b6"">[7]</ref>, caches <ref type=""bibr"" target=""#b7"">[8]</ref>- <ref type=""bibr"" tar ould give all instructions the resources that they need. However, when resources are constrained, priority has to be given to ""critical instructions"" <ref type=""bibr"" target=""#b3"">[4]</ref>, <ref type=""bibr"" target=""#b8"">[9]</ref>, <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" target="" >[24]</ref>- <ref type=""bibr"" target=""#b25"">[26]</ref>. In this work, we use a simple definition of criticality, similar to those in some prior works <ref type=""bibr"" target=""#b3"">[4]</ref>, <ref type=""bibr"" target=""#b25"">[26]</ref> -an instruction is critical if its execution time becomes visible ( r memory which issues prefetches for critical loads <ref type=""bibr"" target=""#b17"">[18]</ref> and another for ALU resources in instruction scheduling <ref type=""bibr"" target=""#b3"">[4]</ref>, <ref type=""bibr"" target=""#b31"">[32]</ref>, <ref type=""bibr"" target=""#b32"">[33]</ref>. These proposals identif .</p><p>2) How to find them?: There are two broad strategies for identifying CritICs: (a) using hardware predictor tables as used in many prior works <ref type=""bibr"" target=""#b3"">[4]</ref>, <ref type=""bibr"" target=""#b7"">[8]</ref>, <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" target="" be an important criterion in selectively optimizing the instruction stream. Prior work has revolved around both (i) identifying critical instructions <ref type=""bibr"" target=""#b3"">[4]</ref>, <ref type=""bibr"" target=""#b8"">[9]</ref>, <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" target=""",0
"p>Different optimizations can be employed upon fetching a critical instruction -prioritizing CPU resources <ref type=""bibr"" target=""#b25"">[26]</ref>- <ref type=""bibr"" target=""#b27"">[28]</ref>, caches <ref type=""bibr"" target=""#b7"">[8]</ref>, <ref type=""bibr"" target=""#b8"">[9]</ref>, <ref type=""bibr"" t",0
"get=""#b10"">[11]</ref>- <ref type=""bibr"" target=""#b12"">[13]</ref>, predicting the result of the instruction <ref type=""bibr"" target=""#b13"">[14]</ref>- <ref type=""bibr"" target=""#b16"">[17]</ref>, issuing prefetch requests <ref type=""bibr"" target=""#b17"">[18]</ref>, etc. However, the impact of these opti",0
"with user click feedback, and model such implicit feedback as a composition of user result examination and relevance judgment. Examination hypothesis <ref type=""bibr"" target=""#b7"">[8]</ref>, which is a fundamental assumption in click modeling, postulates that a user clicks on a system's returned res /ref>, various factors affect users' click decisions; and among them result examination plays a central role <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b7"">8]</ref>. Unfortunately, most applications of bandit algorithms simply treat user clicks as explicit feedback for model C at does not truly reflect users' evaluation of the selected arm a t . Based on the examination hypothesis <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b7"">8]</ref>, when C at = 1, the chosen a t must be relevant to the user's information need at time t; but when C at = 0, a",1
"</teiHeader> 	<text xml:lang=""en""> 		<body> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""1"">Introduction</head><p>Contextual bandit algorithms <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" target=""#b18"">19]</ref> provide modern information ser",0
"(⇠ C , ⇠ E , q). A typical criterion is to choose the values such that the likelihood on the observations is maximized. Similar to the choice made by <ref type=""bibr"" target=""#b11"">[12]</ref>, we choose the closed form update formulas of those variational parameters as,</p><formula xml:id=""formula_1",0
"ity of the corresponding optimization problem. To make things even worse, the two popular bandit learning paradigms, upper confidence bound principle <ref type=""bibr"" target=""#b0"">[1]</ref> and Thompson sampling <ref type=""bibr"" target=""#b2"">[3]</ref>, both demand an accurate estimation of bandit pa",0
"iv xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2"">Related Works</head><p>As having been extensively studied in click modeling of user search results <ref type=""bibr"" target=""#b6"">[7]</ref>, various factors affect users' click decisions; and among them result examination plays a central role <ref ty",0
"dancy are challenging to apply due to power constraints. As a result, researchers have explored software-based techniques to tolerate hardware faults <ref type=""bibr"" target=""#b2"">[3]</ref>. Softwarebased techniques do not require any modification in the hardware of the microprocessor. In fact, some",1
"arguments passed to function calls, can incur SDCs. Although the time cost was reduced, it brought a large number of false positives.</p><p>The work <ref type=""bibr"" target=""#b15"">[16]</ref> proposes a configurable protection technique for SDC-causing errors that allows users to trade-off performan ance overhead when protected.</p><p>In recent years, machine learning based methods are introduced to identify the SDC-causing instructions. The work <ref type=""bibr"" target=""#b15"">[16]</ref> proposes a machine learning algorithm based model, namely SDCAuto, to predict the SDC proneness of a program <p>In this Section we will describe the proposed approach in detail. We first define some terms used in this paper, some of which are drawn from work <ref type=""bibr"" target=""#b15"">[16]</ref>.</p><p>Dynamic Dependency Graph: A Dynamic Dependency Graph (DDG) is a directed acyclic graph (V, E) that ca tracted according to the above analysis and also based on prior work <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b15"">[16]</ref><ref type=""bibr"" target=""#b16"">[17]</ref><ref type=""bibr"" target=""#b17"">[18]</ref>. In total, 62 features are ror detectors at these instructions. Our detectors are based on duplicating the backward slices of the instructions to protect, similar to prior work <ref type=""bibr"" target=""#b15"">[16]</ref>.</p><p>We insert a check immediately after the instructions to be protected, which compares the original val ound. As mentioned before, SDC coverage, SDC detection efficiency and SDC impact are imperative parameters for evaluating our approach. In literature <ref type=""bibr"" target=""#b15"">[16]</ref>, the SDC detection efficiency (DE) is defined as the ratio between SDC coverage and performance overhead. Ho",1
"s, recent works have proposed the selective hardening based on software <ref type=""bibr"" target=""#b4"">[5]</ref><ref type=""bibr"" target=""#b5"">[6]</ref><ref type=""bibr"" target=""#b6"">[7]</ref>.</p><p>Studies have shown that SDCs are caused by errors in a relatively small proportion of programs' data va truction related features. If SDC-masked instructions exist in the successor instruction of an instruction I, the SDC proneness of I will be derated. <ref type=""bibr"" target=""#b6"">(7)</ref> Code structure related features. SDC causing code tends to be on the hot paths of the application. BBs with a",0
"ed. Relyzer <ref type=""bibr"" target=""#b10"">[11]</ref> ran fault injections for the selected dynamic instruction sequences called ""pilots"". SymPLIFIED <ref type=""bibr"" target=""#b11"">[12]</ref> identified SDC-causing instructions by symbolic execution, which covers all SDCs in real executions. However g techniques that prune faults that need detailed study by either predicting their outcomes or showing them equivalent to other faults. SmartInjector <ref type=""bibr"" target=""#b11"">[12]</ref> firstly lists all possible faults in an application, and then exploits the fault pruning techniques to remov onsideration in determining the SDC vulnerability of instruction. Features are extracted according to the above analysis and also based on prior work <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b15"">[16]</ref><ref type=""bibr"" target=""#b1",0
"afts. Soft errors may be explicit bit flips in latches or memories, or glitches in combinational logics that can propagate and be captured in latches <ref type=""bibr"" target=""#b0"">[1]</ref>. SEU could result in silent data corruption (SDC), which means wrong outcomes of a program without any crash d",0
"br"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b15"">[16]</ref><ref type=""bibr"" target=""#b16"">[17]</ref><ref type=""bibr"" target=""#b17"">[18]</ref>. In total, 62 features are extracted. We categorize these features of instructions into nine categories show",0
"nd also based on prior work <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b15"">[16]</ref><ref type=""bibr"" target=""#b16"">[17]</ref><ref type=""bibr"" target=""#b17"">[18]</ref>. In total, 62 features are extracted. We categorize these features",0
"and to offer more exibility to designers, recent works have proposed the selective hardening based on software <ref type=""bibr"" target=""#b4"">[5]</ref><ref type=""bibr"" target=""#b5"">[6]</ref><ref type=""bibr"" target=""#b6"">[7]</ref>.</p><p>Studies have shown that SDCs are caused by errors in a relativel",0
"structions by symbolic execution, which covers all SDCs in real executions. However, it was even more time-consuming than fault injection. Shoestring <ref type=""bibr"" target=""#b12"">[13]</ref> assumed that instructions, which impact global memory or produce arguments passed to function calls, can inc ly time-consuming and not unacceptable for large applications.</p><p>Another SDC identifying method is statistical vulnerability analysis. Shoestring <ref type=""bibr"" target=""#b12"">[13]</ref> uses a static analysis approach to identify the instructions which are likely to result in SDCs, and employs rability of instruction. Features are extracted according to the above analysis and also based on prior work <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b15"">[16]</ref><ref type=""bibr"" target=""#b16"">[17]</ref><ref type=""bibr"" target=""#b1",0
"Studies have shown that SDCs are caused by errors in a relatively small proportion of programs' data variables <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b8"">9]</ref>, and by selectively protecting these SDC-prone variables, one can achieve high coverage against SDCs. However, wn to be accurate for measuring SDCs in programs <ref type=""bibr"" target=""#b18"">[19]</ref>. LLFI works at the LLVM compiler's intermediate code level <ref type=""bibr"" target=""#b8"">[9]</ref>, and allows fault injections to be performed at specific program points, and into specific data types. It also",0
"sis to avoid the injections that result in mask. Since SFI was applied by Relyzer and CriticalFault, the weaknesses of SFI cannot be avoided. Relyzer <ref type=""bibr"" target=""#b10"">[11]</ref> ran fault injections for the selected dynamic instruction sequences called ""pilots"". SymPLIFIED <ref type=""b map out relevant faults for stress testing. However, the remaining faults are still too many to be simulated for accurate SDC rate analysis. Relyzer <ref type=""bibr"" target=""#b10"">[11]</ref> systematically analyzes all fault injection sites in an application for transient faults, and employs fault",0
"njection experiment is conducted using LLFI, a program level fault injection tool, which has been shown to be accurate for measuring SDCs in programs <ref type=""bibr"" target=""#b18"">[19]</ref>. LLFI works at the LLVM compiler's intermediate code level <ref type=""bibr"" target=""#b8"">[9]</ref>, and allo",0
"uctions by symbolic execution, which covers all SDCs in real executions. However, it is even more time-consuming than fault injection.</p><p>The work <ref type=""bibr"" target=""#b14"">[15]</ref> proposes a software-based method to identify and harden the most vulnerable blocks of a program. Using the g hardware-error is said to be derated if it is inherently masked in the system. Different instructions of a program have different error-derating rate <ref type=""bibr"" target=""#b14"">[15]</ref>. In other words, different instructions of a program have different SDC vulnerability.</p><p>Fault propagati",0
"e=""bibr"" target=""#b6"">[7]</ref>.</p><p>Studies have shown that SDCs are caused by errors in a relatively small proportion of programs' data variables <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b8"">9]</ref>, and by selectively protecting these SDC-prone variables, one can achiev",0
"/p><p>Standard deep neural network metric learning methods train image embeddings through the local relationships between images in the form of pairs <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref> or triplets <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" targ",1
"/ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b22"">23]</ref>, fine-grained retrieval <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b26"">27]</ref>, clustering <ref type=""bibr"" each class. The effect of class balanced sampling is studied through ablation experiment in Section 4.4.2 and is a common approach to retrieval tasks <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b26"">27]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4"">Expe too easy result in triplets that have near zero loss leading to slow convergence.</p><p>Recent methods such as <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b26"">27]</ref> focus on addressing this samp c learning design the loss function to consider the relationships of all the samples within the training batch <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b26""",0
"f type=""bibr"" target=""#b26"">27]</ref>, clustering <ref type=""bibr"" target=""#b30"">[31]</ref>, and visual search <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b31"">32,</ref><ref type=""bibr"" target=""#b32"">33]</ref>. The goal of metric learning s however are preferred for scalability to reduce storage and distance computation costs especially in large scale applications such as visual search <ref type=""bibr"" target=""#b10"">[11]</ref>. We observe however that as we increase dimensionality of our embeddings, the optimizer does not fully utili",0
"trieval performance. The ensembled embeddings are trained via boosting <ref type=""bibr"" target=""#b14"">[15]</ref>, attending diverse spatial locations <ref type=""bibr"" target=""#b25"">[26]</ref>, or partitioning the training classes <ref type=""bibr"" target=""#b28"">[29]</ref>. However, such ensembled emb ods that ensemble models <ref type=""bibr"" target=""#b28"">[29,</ref><ref type=""bibr"" target=""#b29"">30]</ref>, modify the feature extractor architecture <ref type=""bibr"" target=""#b25"">[26]</ref>, or propose complex activation paths between the featurizer and final embedding <ref type=""bibr"" target=""#b1",0
"suggests that with a balanced mix of difficulties during training, the image retrieval performance can be further improved. Hierarchical Triplet Loss <ref type=""bibr"" target=""#b23"">[24]</ref> proposed that by merging similar classes dynamically during training into a hierarchical tree, more informat",0
"-to-speech (TTS) models which generate speech directly from characters have made rapid progress in recent years, and achieved very high voice quality <ref type=""bibr"" target=""#b0"">[1]</ref><ref type=""bibr"" target=""#b1"">[2]</ref><ref type=""bibr"" target=""#b2"">[3]</ref>. While the single style TTS, usu ns. The latent state plays a pretty similar role as the latent variable does in VAE. Therefore, in this paper we intend to introduce VAE to Tacotron2 <ref type=""bibr"" target=""#b0"">[1]</ref>, a state-of-the-art end-to-end speech synthesis model, to learn the latent representation of speaker state in o make sure the dimension equal to text encoder state before add operation. The attention module and decoder have the same architecture as Tacotron 2 <ref type=""bibr"" target=""#b0"">[1]</ref>. Then, WaveNet <ref type=""bibr"" target=""#b18"">[19]</ref> vocoder is utilized to reconstruct waveform.</p><p>Th arget=""#b2"">[3]</ref>. While the single style TTS, usually neutral speaking style, is approaching the extreme quality close to human expert recording <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b2"">3]</ref>, the interests in expressive speech synthesis also keep rising. Recently",1
"bibr"" target=""#b8"">[9]</ref>, image generation <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b10"">11]</ref> and speech generation <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b12"">13]</ref> tasks. VAE has many merits, such as learning disentangled factors, s or data augmentation. Comprehensive evaluation shows the good performance of this method.</p><p>We have become aware of recent work by Akuzawa et al. <ref type=""bibr"" target=""#b11"">[12]</ref> which combines an autoregressive speech synthesis model with VAE for expressive speech synthesis. The propos",1
"t variables z to reconstruct x. The decoder may produce the expected reconstruction if the output of decoder is averaged over many samples of x and z <ref type=""bibr"" target=""#b13"">[14]</ref>. In the rest of the paper, −E q φ (z|x) [log p θ (x|z)] is referred to as reconstruction loss and KL[q φ (z|",0
"speech directly from characters have made rapid progress in recent years, and achieved very high voice quality <ref type=""bibr"" target=""#b0"">[1]</ref><ref type=""bibr"" target=""#b1"">[2]</ref><ref type=""bibr"" target=""#b2"">[3]</ref>. While the single style TTS, usually neutral speaking style, is approac et <ref type=""bibr"" target=""#b18"">[19]</ref> vocoder is utilized to reconstruct waveform.</p><p>The total loss of proposed model is shown in equation <ref type=""bibr"" target=""#b1"">(2)</ref>.</p><formula xml:id=""formula_1"">Loss = KL[q φ (z|x)||p θ (z)] − E q φ (z|x) [log p θ (x|z, t)] + l stop<label>",0
"s topic, such as transferring prosody and speaking style within or cross speakers based on end-toend TTS model <ref type=""bibr"" target=""#b3"">[4]</ref><ref type=""bibr"" target=""#b4"">[5]</ref><ref type=""bibr"" target=""#b5"">[6]</ref>.</p><p>Deep generative models, such as Variational Autoencoder (VAE) <r",0
"k. The encoder which deals with character inputs consists of three 1-D convolutional layers with 5 width and 512 channels followed by a bidirectional <ref type=""bibr"" target=""#b14"">[15]</ref> LSTM <ref type=""bibr"" target=""#b15"">[16]</ref> layer using zoneout <ref type=""bibr"" target=""#b16"">[17]</ref>",0
"r"">IEEE ICASSP 2019</ref> Work done during internship at Microsoft STC Asia text generation <ref type=""bibr"" target=""#b8"">[9]</ref>, image generation <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b10"">11]</ref> and speech generation <ref type=""bibr"" target=""#b11"">[12,</ref><ref t head><p>A disentangled representation means that a latent variable completely controls a concept alone and is invariant to changes from other factors <ref type=""bibr"" target=""#b9"">[10]</ref>. In experiments, we found that several dimensions of z could independently control different style attributes",0
"nd speaking style within or cross speakers based on end-toend TTS model <ref type=""bibr"" target=""#b3"">[4]</ref><ref type=""bibr"" target=""#b4"">[5]</ref><ref type=""bibr"" target=""#b5"">[6]</ref>.</p><p>Deep generative models, such as Variational Autoencoder (VAE) <ref type=""bibr"" target=""#b6"">[7]</ref> a",0
"ation <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b10"">11]</ref> and speech generation <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b12"">13]</ref> tasks. VAE has many merits, such as learning disentangled factors, smoothly interpolating or continuously sam",0
"ng internship at Microsoft STC Asia text generation <ref type=""bibr"" target=""#b8"">[9]</ref>, image generation <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b10"">11]</ref> and speech generation <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b12"">13]</ref> tasks",0
"e=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" target=""#b36"">37,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b81""",1
"""bibr"" target=""#b73"">75,</ref><ref type=""bibr"" target=""#b54"">55,</ref><ref type=""bibr"" target=""#b87"">89,</ref><ref type=""bibr"" target=""#b84"">86,</ref><ref type=""bibr"" target=""#b83"">85,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b76"">78,</ref><ref type=""bibr"" target=""#b51"">5",0
"""bibr"" target=""#b38"">39,</ref><ref type=""bibr"" target=""#b41"">42,</ref><ref type=""bibr"" target=""#b88"">90,</ref><ref type=""bibr"" target=""#b90"">92,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b65"">66,</ref><ref type=""bibr"" target=""#b62"">63,</ref><ref type=""bibr"" target=""#b60""",0
"</ref>, Pokec <ref type=""bibr"" target=""#b67"">[68]</ref>, and Twitter <ref type=""bibr"" target=""#b37"">[38]</ref> are common social networks. Europe-osm <ref type=""bibr"" target=""#b11"">[12]</ref> and RoadCA-net [70] are two large roadmap graphs, and UK-2002 [70] is a web graph. Furthermore, we use Graph",0
"events and corresponding likelihoods. In BP, vertex possibility is the metadata. k-Core (KC), which is widely used in graph visualization application <ref type=""bibr"" target=""#b41"">[42,</ref><ref type=""bibr"" target=""#b52"">53]</ref>, iteratively deletes the vertices whose degree is less than k until bibr"" target=""#b48"">[49,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b44"">45,</ref><ref type=""bibr"" target=""#b38"">39,</ref><ref type=""bibr"" target=""#b41"">42,</ref><ref type=""bibr"" target=""#b88"">90,</ref><ref type=""bibr"" target=""#b90"">92,</ref><ref type=""bibr"" target=""#b21""",0
"ressions and body motions (see Figure <ref type=""figure"" target=""#fig_0"">1</ref>). For the training data scarcity problem, we introduce meta learning <ref type=""bibr"" target=""#b38"">[39,</ref><ref type=""bibr"" target=""#b51"">52,</ref><ref type=""bibr"" target=""#b24"">25]</ref> and adversarial learning <re =""bibr"" target=""#b23"">24]</ref> has recently been used as a data augmentation strategy to deal with the lack of training data. However, meta-learning <ref type=""bibr"" target=""#b38"">[39,</ref><ref type=""bibr"" target=""#b51"">52,</ref><ref type=""bibr"" target=""#b24"">25]</ref> was originally proposed for re modeled with deep neural networks with a large number of model parameters. To deal with the data scarcity problem, we propose to use meta learning <ref type=""bibr"" target=""#b38"">[39,</ref><ref type=""bibr"" target=""#b51"">52,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b41",1
"=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b34"">35,</ref><ref type=""bibr"" target=""#b39"">40,</ref><ref type=""bibr"" target=""#b46"">47,</ref><ref type=""bibr"" target=""#b58"">59]</ref>. For example, it is often imp aper. For the multimodal fusion problem, we propose a novel face-focused cross-stream network (FFCSN). Different from the popular two-stream networks <ref type=""bibr"" target=""#b39"">[40,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b46"">47,</ref><ref type=""bibr"" target=""#b34""> h a two-stream architecture was originally proposed for action recognition in videos and has been popular for many human-centric video analysis tasks <ref type=""bibr"" target=""#b39"">[40,</ref><ref type=""bibr"" target=""#b5"">6]</ref>. Various improvements such as temporal segment network (TSN) <ref type",1
"=""http://www.tei-c.org/ns/1.0""><head n=""1."">Introduction</head><p>With the recent rapid development of human-centric AI, human-centric video analysis <ref type=""bibr"" target=""#b47"">[48,</ref><ref type=""bibr"" target=""#b48"">49,</ref><ref type=""bibr"" target=""#b53"">54,</ref><ref type=""bibr"" target=""#b29",0
"os. Its reliable detection needs to resort to multiple modalities including the visual, verbal, and acoustic <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b20"" . Earlier works on video-based ADD are limited by the datasets which contain only staged deceptive behaviors <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b20""",0
"he same setting: loss of Siamese network <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b42"">43]</ref>, and loss of triplet network <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b14"">15]</ref>. The conventional non-pairwise loss is also included as the baseline. enerated videos and the sparsity of video frames that express the emotion content. Our FFCSN model is clearly effective in overcoming this challenge. <ref type=""bibr"" target=""#b1"">(2)</ref> The improvements obtained by our FFCSN model are really impressive, given that only visual features are extrac",0
"rapid development of human-centric AI, human-centric video analysis <ref type=""bibr"" target=""#b47"">[48,</ref><ref type=""bibr"" target=""#b48"">49,</ref><ref type=""bibr"" target=""#b53"">54,</ref><ref type=""bibr"" target=""#b29"">30,</ref><ref type=""bibr"" target=""#b31"">32,</ref><ref type=""bibr"" target=""#b60"" problems such as deception detection <ref type=""bibr"" target=""#b35"">[36,</ref><ref type=""bibr"" target=""#b36"">37]</ref>, emotion recognition in videos <ref type=""bibr"" target=""#b53"">[54,</ref><ref type=""bibr"" target=""#b17"">18]</ref>, personality computing <ref type=""bibr"" target=""#b48"">[49,</ref><ref t is often important to recognize the deceptive behaviors <ref type=""bibr"" target=""#b35"">[36,</ref><ref type=""bibr"" target=""#b36"">37]</ref>, emotions <ref type=""bibr"" target=""#b53"">[54,</ref><ref type=""bibr"" target=""#b17"">18]</ref>, or personality traits <ref type=""bibr"" target=""#b48"">[49,</ref><ref ate that our FFCSN model can be easily extended to other human-centric video analysis problems such as emotion recognition from user-generated videos <ref type=""bibr"" target=""#b53"">[54,</ref><ref type=""bibr"" target=""#b17"">18]</ref>. Extensive experiments are carried out on benchmark datasets and the een proposed in recent works <ref type=""bibr"" target=""#b33"">[34,</ref><ref type=""bibr"" target=""#b52"">53,</ref><ref type=""bibr"" target=""#b56"">57,</ref><ref type=""bibr"" target=""#b53"">54]</ref>. In this paper, we show that our FFCSN model can be easily extended to emotion recognition from user-generate bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b33"">34,</ref><ref type=""bibr"" target=""#b52"">53,</ref><ref type=""bibr"" target=""#b56"">57,</ref><ref type=""bibr"" target=""#b53"">54]</ref>. The comparative results are presented in Table <ref type=""table"" target=""#tab_2"">3</ref>. We have the follow",0
". For the training data scarcity problem, we introduce meta learning <ref type=""bibr"" target=""#b38"">[39,</ref><ref type=""bibr"" target=""#b51"">52,</ref><ref type=""bibr"" target=""#b24"">25]</ref> and adversarial learning <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b22"">23,</ref><re ategy to deal with the lack of training data. However, meta-learning <ref type=""bibr"" target=""#b38"">[39,</ref><ref type=""bibr"" target=""#b51"">52,</ref><ref type=""bibr"" target=""#b24"">25]</ref> was originally proposed for transfer learning. Here, we re-purpose it for learning with scarce data and uniqu deal with the data scarcity problem, we propose to use meta learning <ref type=""bibr"" target=""#b38"">[39,</ref><ref type=""bibr"" target=""#b51"">52,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b41"">42]</ref> to train our FFCSN (see Figure <ref type=""figure"">2</ref>). To best u",0
"ef><ref type=""bibr"" target=""#b51"">52,</ref><ref type=""bibr"" target=""#b24"">25]</ref> and adversarial learning <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b23"">24]</ref> into the training process of our FFCSN. Meta learning, based on the p explicitly, based on a meta learning and adversarial learning based training strategy. Adversarial learning <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b23"">24]</ref> has recently been used as a data augmentation strategy to deal with t",0
"rt to multiple modalities including the visual, verbal, and acoustic <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b20"">21]</ref>. Among them, the visual modal imited by the datasets which contain only staged deceptive behaviors <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b20"">21]</ref>. Their usefulness for detecti",0
"ination of features extracted from different modalities is used for deception detection. Thanks to this benchmark dataset, more advancing ADD methods <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b0"">1,</ref><ref type=""bibr"" target=""#b49"">50]</ref> have been developed to leverag ly combine it with adversarial learning to cope with the extreme challenge of data scarcity in ADD. We show in experiments that our model outperforms <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b0"">1,</ref><ref type=""bibr"" target=""#b49"">50]</ref> by big margin, thanks to the p <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.1.3"">Comparative Results</head><p>We further make comparison to the state-of-the-art alternatives <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b49"">50,</ref><ref type=""bibr"" target=""#b19""> mer has been widely used in previous research on deception detection <ref type=""bibr"" target=""#b35"">[36,</ref><ref type=""bibr"" target=""#b36"">37,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b8"">9]</ref>, while the latter is mainly used in recent works <ref type=""bibr"" targe",0
"14"">[15]</ref> propose to use high-level features to guide the pixel denoiser; in contrast, our denoising is applied directly on features. Guo et al. <ref type=""bibr"" target=""#b7"">[8]</ref> transform the images via non-differentiable image preprocessing, like image quilting <ref type=""bibr"" target="" s because the attacker can approximate the gradients of their non-differentiable computations <ref type=""bibr"" target=""#b0"">[1]</ref>. In contrast to <ref type=""bibr"" target=""#b7"">[8]</ref>, our feature denoising models are differentiable, but are still able to improve adversarial robustness against features induced by an adversarial image gradually increases as the image is propagated through the network <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b7"">8]</ref>, and non-existing activations in the feature maps are hallucinated. In other words, the transformations perform",1
"on features. Guo et al. <ref type=""bibr"" target=""#b7"">[8]</ref> transform the images via non-differentiable image preprocessing, like image quilting <ref type=""bibr"" target=""#b3"">[4]</ref>, total variance minimization <ref type=""bibr"" target=""#b16"">[17]</ref>, and quantization. While these defenses ng blocks make it more difficult to fool networks.</p><p>Variants of denoising operations. Next, we evaluate variants of denoising operations in Sec. <ref type=""bibr"" target=""#b3"">4</ref>. In these ablations, we add blocks of different kinds to baseline ResNet-152.</p><p>We consider the following de",0
"/1.0"" place=""foot"" n=""4"" xml:id=""foot_2"">We have also evaluated other attackers, including FGSM<ref type=""bibr"" target=""#b5"">[6]</ref>, iterative FGSM<ref type=""bibr"" target=""#b11"">[12]</ref>, and its momentum variant<ref type=""bibr"" target=""#b2"">[3]</ref>. Similar to<ref type=""bibr"" target=""#b9"">[1",0
"settings, they can be circumvented in white-box settings because the attacker can approximate the gradients of their non-differentiable computations <ref type=""bibr"" target=""#b0"">[1]</ref>. In contrast to <ref type=""bibr"" target=""#b7"">[8]</ref>, our feature denoising models are differentiable, but kers can iteratively back-propagate through the denoising blocks and create adversarial perturbations that are tailored to the denoisers. Recent work <ref type=""bibr"" target=""#b0"">[1]</ref> reports that pixel denoising methods can be circumvented by attackers in the white-box settings. By contrast, e ImageNet classification dataset <ref type=""bibr"" target=""#b17"">[18]</ref> that has ∼1.28 million images in 1000 classes. Following common protocols <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b9"">10]</ref> for adversarial images on ImageNet, we consider targeted attacks when e ification accuracy on the 50k ImageNet validation images that are adversarially perturbed by the attacker (regardless of its targets), also following <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b9"">10]</ref>.</p><p>In this paper, adversarial perturbation is considered under L ∞",0
"ized SGD on 128 GPUs. Each mini-batch contains 32 images per GPU (i.e., the total mini-batch size is 128×32 = 4096). We follow the training recipe of <ref type=""bibr"" target=""#b6"">[7]</ref> 3 to train models with such large minibatches. On ImageNet, our models are trained for a total of 110 epochs;",0
"ype=""figure"" target=""#fig_0"">1</ref>, we store them for the chest database creation or expansion, performing NER training and modeling using NeuroNER <ref type=""bibr"" target=""#b9"">[12]</ref> and then generating the current or the new model, and hence determining whether the new chest x_ray tags exis",1
"nd apply for obtaining or embedding the specific codes by matching the structured output generated consisting of findings and modifiers automatically <ref type=""bibr"" target=""#b1"">[2]</ref>  <ref type=""bibr"" target=""#b2"">[3]</ref>. In particularly, based on the structured electronic medical records,",0
"generate or learn by implementing the methods such as the deep recursive neural network (deep RNN) constructed by stacking multiple recursive layers <ref type=""bibr"" target=""#b0"">[1]</ref> from the unstructured medical records, and apply for obtaining or embedding the specific codes by matching the",0
"alth care provider and transforming them into the attribute-value structures by the simplified mammographic ontology preset or selected, Taira et al. <ref type=""bibr"" target=""#b5"">[6]</ref> develop a natural language processor and a graphical user interface (GUI) within a system that the radiologist",0
"pecific codes by matching the structured output generated consisting of findings and modifiers automatically <ref type=""bibr"" target=""#b1"">[2]</ref>  <ref type=""bibr"" target=""#b2"">[3]</ref>. In particularly, based on the structured electronic medical records, the relationship between some medical su",0
"r work has investigated methods to reduce latency and speed up the training process of BLSTM. This includes context-sensitive-chunk BLSTM (CSC-BLSTM) <ref type=""bibr"" target=""#b24"">[25]</ref> and latency-controlled BLSTM (LC-BLSTM) <ref type=""bibr"" target=""#b25"">[26]</ref>. Figure <ref type=""figure""",1
"r"" target=""#b42"">[43]</ref><ref type=""bibr"" target=""#b43"">[44]</ref><ref type=""bibr"" target=""#b44"">[45]</ref><ref type=""bibr"" target=""#b45"">[46]</ref><ref type=""bibr"" target=""#b46"">[47]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.3"">BLSTM and LW-BLSTM</head><p>The results of",0
"ges. Of all these architectures, recurrent neural networks have shown strong comparative performance. speech applications, such as voice search tasks <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b16"">17]</ref>, with good performance.</p><p>Although LSTM models have achieved exc conventional LSTM.</p><p>Recently, linear recurrent projection layers have been proposed for reducing the number of parameters at no loss of accuracy <ref type=""bibr"" target=""#b15"">[16]</ref>. Following this work, we use the term LSTM to denote such a deep LSTM-projected architecture and use this ap e tasks. In addition, training requires several complex mechanisms such as nonlinear clipping operations on cell activations and peephole connections <ref type=""bibr"" target=""#b15"">[16]</ref> which may make it difficult to tune the parameters. To address these problems, we will adopt GRUs, another t e used, the performance of the DNN is better than that of the LSTM, which is contrary to what has been found for traditional speech recognition tasks <ref type=""bibr"" target=""#b15"">[16]</ref>. When using MBN features, the merits of advanced acoustic models become smaller. Despite this, our new model",0
"etworks (DNN) combined with hidden Markov models (HMM) have become the dominant approach for acoustic modeling <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref>, replacing the traditional Gaussian mixture modelhidden Markov models (GMM-HMMs) approach. Utilizing increased",0
"[53].</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.6"">Visualization for different models</head><p>The final experiment, inspired by <ref type=""bibr"" target=""#b52"">[54]</ref>, investigates the evolution of different models when performing recognition. Following this approach, we dra",0
"x. Many graph processing architectures/accelerators based on ReRAMs have been proposed (e.g., RPBFS <ref type=""bibr"" target=""#b17"">[18]</ref>, GraphR <ref type=""bibr"" target=""#b0"">[1]</ref>, and HyVE <ref type=""bibr"" target=""#b18"">[19]</ref>), achieving great speedup and energy efficiency improvemen ars can process multiple edges in just one operation, it still suffers from two main problems. (1) Heavy writing overheads. Previous work like GraphR <ref type=""bibr"" target=""#b0"">[1]</ref> converted the edge list into the adjacency matrix by sequentially writing each edge into the ReRAM crossbar. W We also show that only single-bit cells are required for unweighted graph algorithms, leading to 3.77x lower EDP.</p><p>We take the design of GraphR <ref type=""bibr"" target=""#b0"">[1]</ref> as the baseline and compare it with GraphSAR in Figure <ref type=""figure"" target=""#fig_0"">1</ref>. Experimenta ormula xml:id=""formula_1"">value(v i ) = Update(v i , v j , e i, j ) 5:</formula><p>end for 6: end while 7: return V To ensure the scalability, GraphR <ref type=""bibr"" target=""#b0"">[1]</ref> and HyVE <ref type=""bibr"" target=""#b18"">[19]</ref> divide a graph into subgraphs using interval-block partitio block list and the edge list, GraphSAR can process subgraphs in the memory where these subgraphs are stored. Thus, GraphSAR is different from GraphR <ref type=""bibr"" target=""#b0"">[1]</ref> and HyVE <ref type=""bibr"" target=""#b18"">[19]</ref>, by both computing and storing edge data in ReRAM crossbar. in the edge list to perform graph algorithms based on the hybrid-centric model. The order of blocks and edges in lists is the same as that in GraphR <ref type=""bibr"" target=""#b0"">[1]</ref>, where data are stored in the columnoriented order, leading to less usage of registers and write cost.</p><p>T the model of ReRAM cell in NVSim.</p><p>The parameter of ReRAM cell model is from the same source <ref type=""bibr"" target=""#b19"">[20]</ref> in GraphR <ref type=""bibr"" target=""#b0"">[1]</ref> (read/write energy consumption: 1.08pJ/7.4pJ, <ref type=""foot"" target=""#foot_1"">3</ref> read-/write latency: 2 writing to ReRAMs leads to heavy overheads, especially when writing data to ReRAMs and only performing computation once over these data (e.g, GraphR <ref type=""bibr"" target=""#b0"">[1]</ref>). Compared with GraphR, GraphSAR actually exploits the processing-in-memory feature of ReRAMs by directly proc titioning Model</head><p>The Gather-Apply-Scatter (GAS) model is used to describe different graph algorithms. In GAS, processing one vertex includes: <ref type=""bibr"" target=""#b0"">(1)</ref> Gathering values from incoming neighbors; (2) Applying gathered values to get a new value; (3) Scattering the nalog to Digital Converter (ADC). ADC is used to con-</head><p>vert analog data to digital data. We share ADC among bitlines, based on Previous works <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b24"">25]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>? Shift and",1
"g-in-memory operations <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b15"">16]</ref>, especially for matrixvector multiplications <ref type=""bibr"" target=""#b16"">[17]</ref>. On the other hand, graph algorithms can be naturally represented by updating destination vertices using sou",0
"rdware specialized accelerators <ref type=""bibr"" target=""#b7"">[8]</ref><ref type=""bibr"" target=""#b8"">[9]</ref><ref type=""bibr"" target=""#b9"">[10]</ref><ref type=""bibr"" target=""#b10"">[11]</ref>, and processing-inmemory designs <ref type=""bibr"" target=""#b11"">[12]</ref><ref type=""bibr"" target=""#b12"">[13",0
"div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>Table 1: Graph datasets used in evaluation</head><p>Datasets #Vertices #Edges Type wiki-Vote (WV) <ref type=""bibr"" target=""#b25"">[26]</ref> 7.12 k 0.10 m social network cit-HepPh (HP) <ref type=""bibr"" target=""#b25"">[26]</ref> 34.5 k 0.42 m citation aluation</head><p>Datasets #Vertices #Edges Type wiki-Vote (WV) <ref type=""bibr"" target=""#b25"">[26]</ref> 7.12 k 0.10 m social network cit-HepPh (HP) <ref type=""bibr"" target=""#b25"">[26]</ref> 34.5 k 0.42 m citation graph web-Google (GG) <ref type=""bibr"" target=""#b25"">[26]</ref> 0.88 m 5.11 m web gra =""#b25"">[26]</ref> 7.12 k 0.10 m social network cit-HepPh (HP) <ref type=""bibr"" target=""#b25"">[26]</ref> 34.5 k 0.42 m citation graph web-Google (GG) <ref type=""bibr"" target=""#b25"">[26]</ref> 0.88 m 5.11 m web graph com-Youtube (YT) <ref type=""bibr"" target=""#b25"">[26]</ref> 1.13 m 2.99 m community s get=""#b25"">[26]</ref> 34.5 k 0.42 m citation graph web-Google (GG) <ref type=""bibr"" target=""#b25"">[26]</ref> 0.88 m 5.11 m web graph com-Youtube (YT) <ref type=""bibr"" target=""#b25"">[26]</ref> 1.13 m 2.99 m community soc-Pokec (PK) <ref type=""bibr"" target=""#b25"">[26]</ref> 1.63 m 30.6 m social networ r"" target=""#b25"">[26]</ref> 0.88 m 5.11 m web graph com-Youtube (YT) <ref type=""bibr"" target=""#b25"">[26]</ref> 1.13 m 2.99 m community soc-Pokec (PK) <ref type=""bibr"" target=""#b25"">[26]</ref> 1.63 m 30.6 m social network roadNet-CA (CA) <ref type=""bibr"" target=""#b25"">[26]</ref> 1.97 m 2.77 m road ne arget=""#b25"">[26]</ref> 1.13 m 2.99 m community soc-Pokec (PK) <ref type=""bibr"" target=""#b25"">[26]</ref> 1.63 m 30.6 m social network roadNet-CA (CA) <ref type=""bibr"" target=""#b25"">[26]</ref> 1.97 m 2.77 m road network wiki-Talk (WT) <ref type=""bibr"" target=""#b25"">[26]</ref> 2.39 m 5.02 m communicat et=""#b25"">[26]</ref> 1.63 m 30.6 m social network roadNet-CA (CA) <ref type=""bibr"" target=""#b25"">[26]</ref> 1.97 m 2.77 m road network wiki-Talk (WT) <ref type=""bibr"" target=""#b25"">[26]</ref> 2.39 m 5.02 m communication twitter-2010 (TW) <ref type=""bibr"" target=""#b26"">[27]</ref> 41.7 m 1.47 b social "">[26]</ref> 2.39 m 5.02 m communication twitter-2010 (TW) <ref type=""bibr"" target=""#b26"">[27]</ref> 41.7 m 1.47 b social network com-Friendster (FS) <ref type=""bibr"" target=""#b25"">[26]</ref> 65.6 m 1.81 b community yahoo-web (YH) <ref type=""bibr"" target=""#b27"">[28]</ref> 1.41 b 6.64 b web graph 5.1",0
"n perform matrix-vector multiplication based computation on these mats with peripheral circuits. The size of each mat can be 512 ? 512 or 1024 ? 1024 <ref type=""bibr"" target=""#b23"">[24]</ref>, while we can perform multiplication on matrices of smaller sizes (e.g., 4 ? 4 or 8 ? 8) by selecting wordli",0
"gh probabilities represent the most relevant recommendations. While effective, these RNN-based models, such as <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b14"">15]</ref>, depend on a hidden state of the entire past that cannot fully utilize parallel computation within a sequence lmost dominated the area of sequential recommendations. For example, a Gated Recurrent Unit (GRURec) architecture with a ranking loss was proposed by <ref type=""bibr"" target=""#b14"">[15]</ref> for session-based recommendation. In the follow-up papers, various RNN variants have been designed to extend idual blocks.</p><p>4.1.2 Evaluation Protocols. We reported the evaluated results by three popular top-N metrics, namely MRR@N (Mean Reciprocal Rank) <ref type=""bibr"" target=""#b14"">[15]</ref>, HR@N (Hit Ratio) <ref type=""bibr"" target=""#b12"">[13]</ref> and NDCG@N <ref type=""bibr"" target=""#b33"">[34]</ n we detail our experiments, report results for several data sets, and compare our model (called NextItNet) with the wellknown RNN-based model GRURec <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b27"">28]</ref> and the state-of-the-art CNN-based model Caser. Note that (1) since",1
"w-up papers, various RNN variants have been designed to extend the typical one for different application scenarios, such as by adding personalization <ref type=""bibr"" target=""#b24"">[25]</ref>, content <ref type=""bibr"" target=""#b8"">[9]</ref> and contextual features <ref type=""bibr"" target=""#b26"">[27] 0:13 . It is worth noting that in previous sequential recommendation literatures, such as Caser, GRURec and <ref type=""bibr"" target=""#b19"">[20,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b29"">30]</ref>, they only model a single con adding, splitting or shifting the input sequence), such as shown in Eq. ( <ref type=""formula"">3</ref>) ( see <ref type=""bibr"" target=""#b19"">[20,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b28"">29,</ref><ref type=""bibr"" target=""#b29"">30]</ref>).</p><formula xml:id=""formula ender <ref type=""bibr"" target=""#b29"">[30]</ref> and other RNN variants <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b26"">27]</ref>; <ref type=""bibr"" target=""#b1"">(2)</ref> the GRURec baseline could be",1
"ver every element in a sequence. Inspired by the successful use of CNNs in image tasks, a newly proposed sequential recommender, referred to as Caser <ref type=""bibr"" target=""#b28"">[29]</ref>, abandoned RNN structures, proposing instead a convolutional sequence embedding model, and demonstrated that sequence), such as shown in Eq. ( <ref type=""formula"">3</ref>) ( see <ref type=""bibr"" target=""#b19"">[20,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b28"">29,</ref><ref type=""bibr"" target=""#b29"">30]</ref>).</p><formula xml:id=""formula_2"">Caser /GRU Rec sub − session − 1 : {",1
"the proposed network structure. It is worth noting that although the dilated convolution was invented for dense prediction in image generation tasks <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b30"">31]</ref>, and has been applied in other",0
"rly work in sequential recommendations mostly rely on the markov chain <ref type=""bibr"" target=""#b4"">[5]</ref> and feature-based matrix factorization <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b31"">[32]</ref><ref type=""bibr"" target=""#b32"">[33]</ref><ref type=""bibr"" target=""#b",0
"n techniques for the next item recommendation task. We show that the typical network architecture used in Caser has several obvious drawbacks -e.g.,: <ref type=""bibr"" target=""#b0"">(1)</ref> the max pooling scheme that is safely used in computer vision may discard important position and recurrent sig et=""#b20"">[21]</ref>), convolutional layers and a skip connection in a specific order. In this work we adopt the state-of-the-art layer normalization <ref type=""bibr"" target=""#b0"">[1]</ref> before each activation layer, as it is well suited to sequence processing and online learning in contrast with",0
"bibr"" target=""#b4"">[5]</ref> and feature-based matrix factorization <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b31"">[32]</ref><ref type=""bibr"" target=""#b32"">[33]</ref><ref type=""bibr"" target=""#b33"">[34]</ref> approaches. Compared with neural network models, the markov chain b",0
"nd has been applied in other fields (e.g., acoustic <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b25"">26]</ref> and translation <ref type=""bibr"" target=""#b17"">[18]</ref> tasks), it is yet unexplored in recommender systems with huge sparse data. Furthermore, to ease the optimiza g the residual mapping F (E) is much easier than the original, unreferenced mapping H (E).</p><p>Inspired by <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b17"">18]</ref>, we introduce two residual modules in Fig. <ref type=""figure"" target=""#fig_2"">3 (a)  and (b)</ref>.</p><p>In",0
"nvolutional layer is replaced by a fully-connected (FC) layer with weight matrix E д ∈ R 2k ×n . For example, we can apply either the sampled softmax <ref type=""bibr"" target=""#b16"">[17]</ref> or kernel based sampling <ref type=""bibr"" target=""#b1"">[2]</ref>. The recommendation accuracy by these negat",0
"<div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2.3"">Related Work</head><p>Early work in sequential recommendations mostly rely on the markov chain <ref type=""bibr"" target=""#b4"">[5]</ref> and feature-based matrix factorization <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b31""",0
"the evaluated results by three popular top-N metrics, namely MRR@N (Mean Reciprocal Rank) <ref type=""bibr"" target=""#b14"">[15]</ref>, HR@N (Hit Ratio) <ref type=""bibr"" target=""#b12"">[13]</ref> and NDCG@N <ref type=""bibr"" target=""#b33"">[34]</ref> (Normalized Discounted Cumulative Gain). N is set to 5",0
"d to extend the typical one for different application scenarios, such as by adding personalization <ref type=""bibr"" target=""#b24"">[25]</ref>, content <ref type=""bibr"" target=""#b8"">[9]</ref> and contextual features <ref type=""bibr"" target=""#b26"">[27]</ref>, attention mechanism <ref type=""bibr"" target nt-or context-based sequential recommendation models, such as the 3D CNN recommender <ref type=""bibr"" target=""#b29"">[30]</ref> and other RNN variants <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b26"">",0
"er than the desired item) over the raw item sequence. Second, instead of using inefficient huge filters, we stack the 1D dilated convolutional layers <ref type=""bibr"" target=""#b30"">[31]</ref> on top of each other to increase the receptive fields when modeling long-range dependencies. The pooling lay onvolution was invented for dense prediction in image generation tasks <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b30"">31]</ref>, and has been applied in other fields (e.g., acoustic <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bib",0
"such as by adding personalization <ref type=""bibr"" target=""#b24"">[25]</ref>, content <ref type=""bibr"" target=""#b8"">[9]</ref> and contextual features <ref type=""bibr"" target=""#b26"">[27]</ref>, attention mechanism <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b19"">20]</ref> and dif ]</ref> and other RNN variants <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b26"">27]</ref>; <ref type=""bibr"" target=""#b1"">(2)</ref> the GRURec baseline could be regarded as the state-of-the-art Improv",0
"""bibr"" target=""#b9"">[10]</ref>, optimizing the residual mapping F (E) is much easier than the original, unreferenced mapping H (E).</p><p>Inspired by <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b17"">18]</ref>, we introduce two residual modules in Fig. <ref type=""figure"" target",0
"isting model-specific defense methods.</p><p>Closely related to our approach are the Defense-GAN <ref type=""bibr"" target=""#b36"">[37]</ref> and MagNet <ref type=""bibr"" target=""#b24"">[25]</ref>, which first estimate the manifold of clean data to detect adversarial examples and then apply a mapping fun",1
"re or parameters. This can easily complement other existing model-specific defense methods.</p><p>Closely related to our approach are the Defense-GAN <ref type=""bibr"" target=""#b36"">[37]</ref> and MagNet <ref type=""bibr"" target=""#b24"">[25]</ref>, which first estimate the manifold of clean data to det",1
"e Enhanced Deep Super-Resolution (EDSR) <ref type=""bibr"" target=""#b46"">[47]</ref> network (trained on the DIVerse 2K resolution image (DIV2K) dataset <ref type=""bibr"" target=""#b47"">[48]</ref>), which uses a hierarchy of such residual blocks. While our proposed approach achieves competitive performan",0
"s, e.g an adversarial example generated for an Inception v-3 model is able to fool other CNN architectures <ref type=""bibr"" target=""#b12"">[13]</ref>, <ref type=""bibr"" target=""#b15"">[16]</ref>. Owing to the critical nature of security-sensitive CNN applications, significant research has been carried",0
"over the past several years has lead to their extensive deployment in a wide range of computer vision tasks <ref type=""bibr"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b1"">[2]</ref>, including image classification <ref type=""bibr"" target=""#b2"">[3]</ref>- <ref type=""bibr"" target=""#b4"">[5]</re",0
"e Input Iterative FGSM (MDI 2 FGSM) <ref type=""bibr"" target=""#b35"">[36]</ref>. We use publicly available implementations of these methods: Cleverhans <ref type=""bibr"" target=""#b53"">[54]</ref>, Foolbox <ref type=""bibr"" target=""#b54"">[55]</ref> and codes 2 3 provided by <ref type=""bibr"" target=""#b32"">",0
"target=""#b30"">[31]</ref>, foveation-based methods, which crop the image background <ref type=""bibr"" target=""#b31"">[32]</ref>, random pixel deflection <ref type=""bibr"" target=""#b25"">[26]</ref> and random image padding &amp; re-sizing <ref type=""bibr"" target=""#b17"">[18]</ref>. Compared with differenti f type=""bibr"" target=""#b35"">[36]</ref> and comparison with other recently proposed model-agnostic defenses <ref type=""bibr"" target=""#b17"">[18]</ref>, <ref type=""bibr"" target=""#b25"">[26]</ref>, <ref type=""bibr"" target=""#b37"">[38]</ref>, <ref type=""bibr"" target=""#b38"">[39]</ref> (see Section IV).</p>< pe=""bibr"" target=""#b12"">[13]</ref> and FGSM <ref type=""bibr"" target=""#b13"">[14]</ref>. Another closely related work to ours is that of Prakash et al. <ref type=""bibr"" target=""#b25"">[26]</ref>, which deflects attention by carefully corrupting less critical image pixels. This introduces new artifacts denoising in the wavelet domain yields superior performance than other techniques such as bilateral, an-isotropic, TVM and Wiener-Hunt deconvolution <ref type=""bibr"" target=""#b25"">[26]</ref>. Another closely related work is that of Xie et al. <ref type=""bibr"" target=""#b17"">[18]</ref>, which perform ields better results than various other techniques including bilateral, anisotropic, Total Variance Minimization (TVM) and Wiener-Hunt de-convolution <ref type=""bibr"" target=""#b25"">[26]</ref>. The main principle behind wavelet shrinkage is that Discrete Wavelet Transform (DWT) of real world signals ype=""bibr"" target=""#b17"">[18]</ref>, Image quilting + total variance minimization <ref type=""bibr"" target=""#b38"">[39]</ref> and Pixel Deflection (PD) <ref type=""bibr"" target=""#b25"">[26]</ref>. We use publicly available implementations<ref type=""foot"" target=""#foot_3"">4</ref> <ref type=""foot"" target=",0
"l nature of security-sensitive CNN applications, significant research has been carried out to devise defense mechanisms against these vulnerabilities <ref type=""bibr"" target=""#b16"">[17]</ref>- <ref type=""bibr"" target=""#b27"">[28]</ref>. We can broadly categorize these defenses along two directions: t the first being model-specific mechanisms, which aim to regularize a specific model's parameters through adversarial training or parameter smoothing <ref type=""bibr"" target=""#b16"">[17]</ref>, <ref type=""bibr"" target=""#b18"">[19]</ref>, <ref type=""bibr"" target=""#b19"">[20]</ref>, <ref type=""bibr"" targ hich results in regularizing the network by softening the decision boundaries, thereby encompassing nearby adversarial images. Defensive distillation <ref type=""bibr"" target=""#b16"">[17]</ref> improves the model robustness in an essentially similar fashion by retraining a given model using soft label",0
"presentational and temporal capabilities. While RNNs have been previously applied to make recommendations based on collaborative filtering principles <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b22"">23]</ref>, they have not been re-purpo",1
"While RNNs have been previously applied to make recommendations based on collaborative filtering principles <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b22"">23]</ref>, they have not been re-purposed to make more targeted personalized go",1
"t information in predicting performance stems from capturing the interaction effect that can occur when a student takes two difficult courses at once <ref type=""bibr"" target=""#b5"">[6]</ref>. Other feature engineering approaches to course grade prediction found that better discrimination between grad l influence their grades in the current semester, but also the course co-enrollment composition in the current semester will impact their performance <ref type=""bibr"" target=""#b5"">[6]</ref>. The reasons lie in interaction effects among courses enrolled in together, such as (1) 1 RNNs were evaluated; navailable in semester t. (4) Filter out courses the student has already taken. <ref type=""bibr"" target=""#b4"">(5)</ref> Filter out the target course. <ref type=""bibr"" target=""#b5"">(6)</ref> Only consider the courses with predicted grades higher than the threshold (A in Figure <ref type=""figure"" targ",0
"ential research in MOOCs has focused on models of predicting course outcomes based on within-course activities <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b19"">20]</ref>. It should be noted that even",0
"evement in higher education. Work showing the correlation between course outcomes and timely access to course materials among late enrolling students <ref type=""bibr"" target=""#b0"">[1]</ref> serves as a reminder of this.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3"">GOAL-BASED RECOMM",0
"""#b26"">27]</ref>. Early-warning implementations like this can experience unintended consequences, however, such as leading to greater course drop-out <ref type=""bibr"" target=""#b18"">[19]</ref>. One system attempted a more pre-emptive approach, showing grade distributions of courses to students and co",0
"mation from past observations that will be useful for future predictions.</p><p>We use a popular variant of RNNs called Long Short-Term Memory (LSTM) <ref type=""bibr"" target=""#b17"">[18]</ref>, which helps RNNs learn temporal dependencies with the addition of several gates which can retain and forget",0
"k of elective course selection prediction but were substantially outperformed when feature sets of student-course ratings could be taken into account <ref type=""bibr"" target=""#b9"">[10]</ref>. Classical collaborative filtering was used to predict course grades, which served as weights on an inferred",0
"models of predicting course outcomes based on within-course activities <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b19"">20]</ref>. It should be noted that even given a positive validation and eventua",0
"ibr"" target=""#b24"">[25]</ref>. Their inferences have been used to produce pre-requisite graphs in the same math tutoring contexts as well as in MOOCs <ref type=""bibr"" target=""#b8"">[9]</ref>. They have also been used as models of behavior prediction, to suggest the next resource a learner is likely t",0
"#b42"">[43]</ref>), neighbor aggregation based methods (GCN <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b21"">22]</ref>, GraphSAGE <ref type=""bibr"" target=""#b12"">[13]</ref>), etc.</p><p>Graph-level embedding. The most intuitive way to generate one embedding per graph is to aggrega",1
"esent graphs can be achieved by viewing a graph as a hierarchical data structure and applying graph coarsening <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b46"">47]</ref>. Besides, <ref type=""bibr"" target=""#b18"">[19]</ref> aggregates sets of nodes via histograms, and <ref type=""b ks, most existing works deal with the classification of a single graph <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b28"">29,</ref><ref type=""bibr"" target=""#b46"">47]</ref>. In this work, we consider graph similarity computation for the first time.</p></div> <div xmlns=""http://www.",0
"ased on one embedding per sentence can be further enhanced through using fine-grained word-level information <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b16"">17]</ref>. This leads to our second strategy.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.2"">Strategy",0
"imilarity computation directly. Instead of calculating the exact similarity metric, these methods find approximate values in a fast and heuristic way <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b27"">28,< airwise GED computation algorithms. A flurry of approximate algorithms has been proposed to reduce the time complexity with the sacrifice in accuracy <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b27"">28,<",0
"most intuitive way to generate one embedding per graph is to aggregate the node-level embeddings, either by a simple average or some weighted average <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b49"">50]</ref>, named the ""sum-based"" approaches <ref type=""bibr"" target=""#b13"">[14]<",0
"a pool of unlabeled data points, in order to maximize the accuracy. Although it is not possible to obtain a universally good active learning strategy <ref type=""bibr"" target=""#b4"">(Dasgupta, 2004)</ref>, there exist many heuristics <ref type=""bibr"" target=""#b36"">(Settles, 2010)</ref> which have been h approaches in Section 5.</p><p>On the theoretical side, it is shown that greedy active learning is not possible in algorithm and data agnostic case <ref type=""bibr"" target=""#b4"">(Dasgupta, 2005)</ref>. However, there are data dependent results showing that it is indeed possible to obtain a query s",1
"hods which can learn a data distribution as a result of a two-player non-cooperative game <ref type=""bibr"" target=""#b35"">(Salimans et al., 2016;</ref><ref type=""bibr"" target=""#b15"">Goodfellow et al., 2014;</ref><ref type=""bibr"" target=""#b32"">Radford et al., 2015)</ref>. These methods are further ext",0
"methods <ref type=""bibr"" target=""#b29"">(MacKay, 1992)</ref>, ensemble approaches <ref type=""bibr"" target=""#b30"">(McCallumzy &amp; Nigamy, 1998;</ref><ref type=""bibr"" target=""#b9"">Freund et al., 1997)</ref> and uncertainty based methods <ref type=""bibr"" target=""#b39"">(Tong &amp; Koller, 2001;</ref><",0
"lay a central role in the mechanistic understanding of cellular function under normal and disease conditions <ref type=""bibr"" target=""#b24"">[27]</ref><ref type=""bibr"" target=""#b25"">[28]</ref><ref type=""bibr"" target=""#b26"">[29]</ref><ref type=""bibr"" target=""#b27"">[30]</ref><ref type=""bibr"" target=""#b",1
"ltiple shared interaction partners are likely to share even more partners (SI. C) <ref type=""bibr"" target=""#b14"">[15]</ref><ref type=""bibr"">[16]</ref><ref type=""bibr"" target=""#b15"">[17]</ref>. To test the L3 principle, we measured the number of = 3 paths between all protein pairs, finding a direct c",0
"ituents, understanding cellular mechanisms requires us to catalogue all physical interactions between proteins <ref type=""bibr"" target=""#b0"">[1]</ref><ref type=""bibr"" target=""#b1"">[2]</ref><ref type=""bibr"" target=""#b2"">[3]</ref><ref type=""bibr"" target=""#b3"">[4]</ref>. Despite spectacular advances in",0
"on: Protein-protein interactions (PPIs) play a central role in the mechanistic understanding of cellular function under normal and disease conditions <ref type=""bibr"" target=""#b24"">[27]</ref><ref type=""bibr"" target=""#b25"">[28]</ref><ref type=""bibr"" target=""#b26"">[29]</ref><ref type=""bibr"" target=""#b",0
"0"">[23]</ref>. FAM161A localizes at the ciliary basal body and the connecting cilium of human photoreceptors <ref type=""bibr"" target=""#b21"">[24,</ref><ref type=""bibr"" target=""#b22"">25]</ref>, binds to microtubules <ref type=""bibr"" target=""#b22"">[25]</ref> and has recently been found to interact with he connecting cilium of human photoreceptors <ref type=""bibr"" target=""#b21"">[24,</ref><ref type=""bibr"" target=""#b22"">25]</ref>, binds to microtubules <ref type=""bibr"" target=""#b22"">[25]</ref> and has recently been found to interact with proteins of the Golgi apparatus (GA) <ref type=""bibr"" target=""#",0
"istics is assumed. On the other hand, measure theory is not assumed; for a much more thorough treatment with all the measure theoretical details, see <ref type=""bibr"" target=""#b0"">Daley and Vere-Jones (2003)</ref> and <ref type=""bibr"" target=""#b1"">Daley and Vere-Jones (2008)</ref>.</p></div> <div xm er words, the conditional intensity function specifies the mean number of events in a region conditional on the past. Here we use the notation * from <ref type=""bibr"" target=""#b0"">Daley and Vere-Jones (2003)</ref> to remind ourselves that this density is conditional on the past right up to but not i",1
"for a much more thorough treatment with all the measure theoretical details, see <ref type=""bibr"" target=""#b0"">Daley and Vere-Jones (2003)</ref> and <ref type=""bibr"" target=""#b1"">Daley and Vere-Jones (2008)</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2"">Evolutionary point pro",1
"tion. Here it would be more appropriate to base the model checking on other aspects of the model (such as the summary statistics given for example in <ref type=""bibr"" target=""#b9"">Møller and Waagepetersen (2004)</ref>), which may not be caught so well by the conditional intensity function.</p></div>",0
"eases the intensity by αe βκ i . Note that large earthquakes increase the intensity more than small earthquakes. For more on the ETAS model, see e.g. <ref type=""bibr"" target=""#b11"">Ogata (1988</ref><ref type=""bibr"" target=""#b12"">Ogata ( , 1998))</ref>.</p><p>We sometimes make simplifying independenc ty function known as residual analysis.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""5.1"">Residual analysis</head><p>Residual analysis <ref type=""bibr"" target=""#b11"">(Ogata, 1988)</ref> is a type of model checking for point processes specified by a conditional intensity function. It i",0
"s).</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.2"">Ogata's modified thinning algorithm</head><p>Ogata's modified thinning algorithm <ref type=""bibr"" target=""#b10"">(Ogata, 1981)</ref> is a thinning algorithm based on simulating homogeneous Poisson processes with too high intensities",0
"ding on some parameter β (which may be a single value or a vector, depending on the choice of distribution). For more on the Hawkes process, see e.g. <ref type=""bibr"" target=""#b3"">Hawkes (1971b</ref><ref type=""bibr"" target=""#b4"">Hawkes ( ,a, 1972))</ref>; <ref type=""bibr"" target=""#b5"">Hawkes and Oak",0
"). For more on the Hawkes process, see e.g. <ref type=""bibr"" target=""#b3"">Hawkes (1971b</ref><ref type=""bibr"" target=""#b4"">Hawkes ( ,a, 1972))</ref>; <ref type=""bibr"" target=""#b5"">Hawkes and Oakes (1974)</ref>.</p><p>qq q q q q q qq q q q q qq q qq q q q q qq q q q q q q q q q q q q q q q q q q q q",0
"alue or a vector, depending on the choice of distribution). For more on the Hawkes process, see e.g. <ref type=""bibr"" target=""#b3"">Hawkes (1971b</ref><ref type=""bibr"" target=""#b4"">Hawkes ( ,a, 1972))</ref>; <ref type=""bibr"" target=""#b5"">Hawkes and Oakes (1974)</ref>.</p><p>qq q q q q q qq q q q q qq",0
"pr.ia.ac.cn.</p><p>Ye Bai is currently a PhD student at University of Chinese Academy of Sciences (UCAS), Beijing, China. e-mail: baiye2016@ia.ac.cn. <ref type=""bibr"" target=""#b13"">[14]</ref>, <ref type=""bibr"">[15]</ref>, <ref type=""bibr"" target=""#b14"">[16]</ref>, <ref type=""bibr"" target=""#b15"">[17]",1
". This approach benefits from multi-task learning <ref type=""bibr"" target=""#b16"">[18]</ref>. The source model is trained jointly on several languages <ref type=""bibr"" target=""#b17"">[19]</ref>, <ref type=""bibr"" target=""#b18"">[20]</ref>. In addition, language identification based multilingual training",1
"arning <ref type=""bibr"" target=""#b16"">[18]</ref>. The source model is trained jointly on several languages <ref type=""bibr"" target=""#b17"">[19]</ref>, <ref type=""bibr"" target=""#b18"">[20]</ref>. In addition, language identification based multilingual training is proposed to extract multilingual bottle",1
"hallenging to rapidly build an ASR system for a novel language with significantly less labeled training data <ref type=""bibr"" target=""#b4"">[5]</ref>, <ref type=""bibr"" target=""#b5"">[6]</ref>, <ref type=""bibr"" target=""#b6"">[7]</ref>. This was also the goal of IARPA Babel program. Without any question, br"" target=""#b29"">[31]</ref> combine this method and semi-supervised learning to transfer cross-lingual knowledge to a target model. More recently in <ref type=""bibr"" target=""#b5"">[6]</ref>, Karafiat et al. use bi-directional long-short term memory (BLSTM) based source model to transfer shared param ve shown improved performance with ivector features in addition to the acoustic features in the ASR modeling <ref type=""bibr"" target=""#b2"">[3]</ref>, <ref type=""bibr"" target=""#b5"">[6]</ref>. The i-vector approach is also successfully applied to language recognition <ref type=""bibr"" target=""#b41"">[43",0
"br"">[15]</ref>, <ref type=""bibr"" target=""#b14"">[16]</ref>, <ref type=""bibr"" target=""#b15"">[17]</ref>. This approach benefits from multi-task learning <ref type=""bibr"" target=""#b16"">[18]</ref>. The source model is trained jointly on several languages <ref type=""bibr"" target=""#b17"">[19]</ref>, <ref ty p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>B. Multilingual Training</head><p>Multilingual training is an instance of multi-task learning <ref type=""bibr"" target=""#b16"">[18]</ref>. The source model is trained simultaneously on the training data of multiple languages. Each language has it",0
") based acoustic models have obtained significant improvement for automatic speech recognition (ASR) systems <ref type=""bibr"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b2"">[3]</ref>, <ref type=""bibr"" target=""#b3"">[4]</ref>. However it is still challen",0
"usly, shared feature representations for low-resource languages have been studied by Thomas <ref type=""bibr"" target=""#b26"">[28]</ref>. Scanzio et al. <ref type=""bibr"" target=""#b27"">[29]</ref> present a front-end consisting of an artificial neural network architecture trained with multilingual data. d.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>A. Shared Hidden Layer Model</head><p>The SHL-Model is widely used for multilingual tasks <ref type=""bibr"" target=""#b27"">[29]</ref>, <ref type=""bibr"" target=""#b28"">[30]</ref>, <ref type=""bibr"" target=""#b29"">[31]</ref>. A lot of variants of",0
"bibr"" target=""#b24"">[26]</ref> and transferring model parameters <ref type=""bibr"" target=""#b9"">[10]</ref>, <ref type=""bibr"" target=""#b10"">[11]</ref>, <ref type=""bibr"" target=""#b25"">[27]</ref>. This paper focuses on the latter.</p><p>The basic idea of transferring model parameters is that the source =""bibr"" target=""#b29"">[31]</ref>. A lot of variants of SHL-Models are proposed to train multilingual models <ref type=""bibr"" target=""#b9"">[10]</ref>, <ref type=""bibr"" target=""#b25"">[27]</ref>, <ref type=""bibr"" target=""#b21"">[23]</ref>, <ref type=""bibr"" target=""#b30"">[32]</ref>. The SHL-Model is comp osed to transfer shared layers to the target model. One kind of the methods is widely used in previous work <ref type=""bibr"" target=""#b9"">[10]</ref>, <ref type=""bibr"" target=""#b25"">[27]</ref>, <ref type=""bibr"" target=""#b28"">[30]</ref>. This method is to initialize all the hidden layers of the target",0
"t=""#b15"">[17]</ref>, <ref type=""bibr"" target=""#b21"">[23]</ref>, <ref type=""bibr"" target=""#b22"">[24]</ref>, <ref type=""bibr"" target=""#b23"">[25]</ref>, <ref type=""bibr"" target=""#b24"">[26]</ref> and transferring model parameters <ref type=""bibr"" target=""#b9"">[10]</ref>, <ref type=""bibr"" target=""#b10"">[",0
"tion, language identification based multilingual training is proposed to extract multilingual bottleneck features for low resource speech recognition <ref type=""bibr"" target=""#b19"">[21]</ref>, <ref type=""bibr"" target=""#b20"">[22]</ref>. The knowledge from the source model can be transferred to the ta",0
"China. e-mail: baiye2016@ia.ac.cn. <ref type=""bibr"" target=""#b13"">[14]</ref>, <ref type=""bibr"">[15]</ref>, <ref type=""bibr"" target=""#b14"">[16]</ref>, <ref type=""bibr"" target=""#b15"">[17]</ref>. This approach benefits from multi-task learning <ref type=""bibr"" target=""#b16"">[18]</ref>. The source model using knowledge from the source model. The transfer learning methods can be roughly classified into two categories: transferring bottleneck features <ref type=""bibr"" target=""#b15"">[17]</ref>, <ref type=""bibr"" target=""#b21"">[23]</ref>, <ref type=""bibr"" target=""#b22"">[24]</ref>, <ref type=""bibr"" targ",0
"in this paper.</p><p>All the BLSTM models use a single frame as the input, with no frame stacking. The BLSTM acoustic models are based on the work in <ref type=""bibr"" target=""#b42"">[44]</ref>, where each BLSTM layer consists of peephole connections and a recurrent projection layer. Each BLSTM layer",0
"inese Academy of Sciences (UCAS), Beijing, China. e-mail: baiye2016@ia.ac.cn. <ref type=""bibr"" target=""#b13"">[14]</ref>, <ref type=""bibr"">[15]</ref>, <ref type=""bibr"" target=""#b14"">[16]</ref>, <ref type=""bibr"" target=""#b15"">[17]</ref>. This approach benefits from multi-task learning <ref type=""bibr""",0
"hang et al. <ref type=""bibr"" target=""#b35"">[37]</ref> use adversarial strategy to obtain bilingual lexicon without cross-lingual knowledge. Shinohara <ref type=""bibr"" target=""#b36"">[38]</ref> utilizes adversarial training to perform environment adaptation for robust speech recognition. Saon et al. <",0
"e language invariant low-level components across various languages <ref type=""bibr"" target=""#b8"">[9]</ref>, <ref type=""bibr"" target=""#b9"">[10]</ref>, <ref type=""bibr"" target=""#b10"">[11]</ref>. An acoustic model trained using other source languages is referred to as a source model. An acoustic model bibr"" target=""#b23"">[25]</ref>, <ref type=""bibr"" target=""#b24"">[26]</ref> and transferring model parameters <ref type=""bibr"" target=""#b9"">[10]</ref>, <ref type=""bibr"" target=""#b10"">[11]</ref>, <ref type=""bibr"" target=""#b25"">[27]</ref>. This paper focuses on the latter.</p><p>The basic idea of transf",0
"onsistent hashing, which approximates true d-FCFS only with high numbers of client connections spreading requests out evenly over queues.</p><p>ZygOS <ref type=""bibr"" target=""#b44"">[46]</ref> improved on d-FCFS by implementing low-overhead task stealing: threads that complete short requests steal wo those with fixed or low-dispersion service time distributions.</p><p>We compare Shinjuku with IX <ref type=""bibr"" target=""#b14"">[16]</ref> and ZygOS <ref type=""bibr"" target=""#b44"">[46]</ref>, two state-of-the-art dataplane operating systems. Using synthetic workloads, we show that Shinjuku matches IX dataplane being a canonical example <ref type=""bibr"" target=""#b14"">[16]</ref>. Zy-gOS improves on IX by using work stealing to approximate c-FCFS <ref type=""bibr"" target=""#b44"">[46]</ref>. Linux applications built with libevent <ref type=""bibr"" target=""#b45"">[47]</ref> or libuv [5] also implemen for low-latency applications. Shinjuku is a significant departure from the common pattern in IX <ref type=""bibr"" target=""#b14"">[16]</ref> and Zy-gOS <ref type=""bibr"" target=""#b44"">[46]</ref>, which rely heavily on RSS to distribute incoming requests to workers that process them without interruption div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4"">Evaluation</head><p>We compare Shinjuku to IX <ref type=""bibr"" target=""#b14"">[16]</ref> and ZygOS <ref type=""bibr"" target=""#b44"">[46]</ref>, two recent systems that use d-FCFS and approximate c-FCFS respectively to improve tail latency. All three s 0%) even for homogeneous workloads which exacerbates its stealing overheads. A similar performance drop was also observed in the original ZygOS paper <ref type=""bibr"" target=""#b44"">[46]</ref>. Figure <ref type=""figure"">5c</ref> uses a Bimodal(99.5 -0.5, 0.5 -500) service time distribution where 99.5 ef type=""bibr"" target=""#b43"">[45]</ref>, MICA <ref type=""bibr"" target=""#b37"">[39]</ref>, Chronos <ref type=""bibr"" target=""#b30"">[32]</ref>, and ZygOS <ref type=""bibr"" target=""#b44"">[46]</ref> fall in this category. Shinjuku improves on these systems by introducing preemptive scheduling that allows s he idea of safe, low-overhead interrupts was introduced in ELI for fast delivery of interrupts to VMs <ref type=""bibr"" target=""#b8"">[10]</ref>. ZygOS <ref type=""bibr"" target=""#b44"">[46]</ref> uses inter-processor interrupts for work stealing but does not implement preemptive scheduling. Shinjuku use",1
"en several efforts to implement efficient, user-space thread libraries <ref type=""bibr"" target=""#b6"">[8,</ref><ref type=""bibr"" target=""#b54"">56,</ref><ref type=""bibr"" target=""#b46"">48,</ref><ref type=""bibr"" target=""#b0"">1]</ref>. They all focus on cooperative scheduling. Shinjuku shows that preempti",0
"items depending on the popularity of search terms <ref type=""bibr"" target=""#b11"">[13]</ref>; microservices and functionas-a-service (FaaS) frameworks <ref type=""bibr"" target=""#b15"">[17]</ref>; and in-memory stores or databases, such as RocksDB <ref type=""bibr"" target=""#b24"">[26]</ref>, Redis <ref ty",0
"=""5"">Related Work</head><p>Optimized network stacks: There is significant work in optimizing network stacks, including polling based processing (DPDK <ref type=""bibr"" target=""#b1"">[3]</ref>), multi-core scalability (mTCP <ref type=""bibr"" target=""#b29"">[31]</ref>), modularity and specialization (Sand",0
"d in-memory stores or databases, such as RocksDB <ref type=""bibr"" target=""#b24"">[26]</ref>, Redis <ref type=""bibr"" target=""#b33"">[35]</ref>, and Silo <ref type=""bibr"" target=""#b52"">[54]</ref>, that support both simple get/put requests and complex range or SQL queries, and use slower nonvolatile memo mix simple get/put requests with complex range or relational queries <ref type=""bibr"" target=""#b33"">[35,</ref><ref type=""bibr"" target=""#b24"">26,</ref><ref type=""bibr"" target=""#b52"">54]</ref>. Plot (c) shows such a distribution in which 99.5% of requests take 0.5?sec and 0.5% take 500?sec. Compared t",0
"e access pattern of these PHP applications, contrary to prior works deploying a hash table that supports only GET requests in a memcached environment <ref type=""bibr"" target=""#b53"">[55]</ref>. Furthermore, supporting such a hash table in the PHP environment presents a new set of challenges in order prior works with other server-side applications (serversside Javascript applications <ref type=""bibr"" target=""#b72"">[73]</ref> or memcached workloads <ref type=""bibr"" target=""#b53"">[55]</ref>). Note that we simulate an aggressive memory system with prefetchers at every cache level. Although there ar ons are at most 24 bytes in length. As a result, we store the keys in the hash table itself, unlike the hash table designed for memcached deployments <ref type=""bibr"" target=""#b53"">[55]</ref>. Storing the keys directly in the hash table eases the traversal of the hash table in hardware.</p><p>We nex t host those. Specialization alternatives. A hash table that supports only GET operation has been deployed in hardware before for memcached workloads <ref type=""bibr"" target=""#b53"">[55]</ref>. Furthermore, <ref type=""bibr"" target=""#b28"">[30]</ref> deployed the entire memcached algorithm (supporting cteristics of these PHP applications.</p><p>First, in contrast to the most large-scale memcached deployments <ref type=""bibr"" target=""#b21"">[23,</ref><ref type=""bibr"" target=""#b53"">55]</ref> where the GET requests vastly outnumber the SET and other request types, these PHP applications observe relat bibr"" target=""#b22"">[24,</ref><ref type=""bibr"" target=""#b34"">36,</ref><ref type=""bibr"" target=""#b45"">47,</ref><ref type=""bibr"" target=""#b51"">53,</ref><ref type=""bibr"" target=""#b53"">55,</ref><ref type=""bibr"" target=""#b54"">56,</ref><ref type=""bibr"" target=""#b68"">69]</ref> developed in C++-like compile",1
"ured textual data (such as social media updates, web documents, blog posts, news articles, and system logs) into appropriate HTML format. Prior works <ref type=""bibr"" target=""#b66"">[68]</ref> have realized the potential of hardware specialization for string matching, but do not support all the neces rough numerous different string operations. These tasks include string finding, matching, replacing, trimming, comparing, etc. Previous work, such as <ref type=""bibr"" target=""#b66"">[68]</ref>, propose methods for string matching in hardware. However, the hardware proposed processes a single characte",1
"r design leverages several inherent characteristics of these PHP applications.</p><p>First, in contrast to the most large-scale memcached deployments <ref type=""bibr"" target=""#b21"">[23,</ref><ref type=""bibr"" target=""#b53"">55]</ref> where the GET requests vastly outnumber the SET and other request ty",0
"evelopment, PHP is the most commonly used <ref type=""bibr"" target=""#b69"">[70,</ref><ref type=""bibr"" target=""#b71"">72]</ref>, representing about 82.3% <ref type=""bibr"" target=""#b11"">[13]</ref> of all web applications. In particular, server-side PHP web applications have created an ecosystem of their pplications. However, PHP is most commonly used <ref type=""bibr"" target=""#b69"">[70,</ref><ref type=""bibr"" target=""#b71"">72]</ref>, representing 82.3% <ref type=""bibr"" target=""#b11"">[13]</ref> of all web applications. In this context, our work is the first to present a comprehensive analysis of the m has gone up significantly. In the client side, Javascript is used predominantly whereas PHP is the language of choice for server-side web development <ref type=""bibr"" target=""#b11"">[13]</ref>. <ref type=""bibr"" target=""#b20"">[22,</ref><ref type=""bibr"" target=""#b57"">59]</ref> propose microarchitectura",0
"""bibr"" target=""#b6"">[8,</ref><ref type=""bibr"" target=""#b55"">57]</ref> or recent hardware regexp accelerators <ref type=""bibr"" target=""#b32"">[34,</ref><ref type=""bibr"" target=""#b37"">39,</ref><ref type=""bibr"" target=""#b56"">58,</ref><ref type=""bibr"" target=""#b64"">66]</ref> are overly generic in nature applications.</p><p>There is a large body of research in accelerating regular expression (regexp) processing <ref type=""bibr"" target=""#b24"">[26,</ref><ref type=""bibr"" target=""#b37"">39,</ref><ref type=""bibr"" target=""#b55"">57,</ref><ref type=""bibr"" target=""#b56"">58,</ref><ref type=""bibr"" target=""#b59"" gexp-intrinsic characteristics in real-world PHP applications. On the other hand, the high hardware cost associated with parallel regexp accelerators <ref type=""bibr"" target=""#b37"">[39,</ref><ref type=""bibr"" target=""#b56"">58,</ref><ref type=""bibr"" target=""#b64"">66]</ref> may deter their commercial d f type=""bibr"" target=""#b56"">58,</ref><ref type=""bibr"" target=""#b59"">61,</ref><ref type=""bibr"" target=""#b64"">66]</ref>. In recent years, several works <ref type=""bibr"" target=""#b37"">[39,</ref><ref type=""bibr"" target=""#b56"">58,</ref><ref type=""bibr"" target=""#b64"">66]</ref> attempt to parallelize proce",0
"years, numerous research efforts have been devoted to optimizing warehouse-scale(WSC) and big data workloads <ref type=""bibr"" target=""#b22"">[24,</ref><ref type=""bibr"" target=""#b34"">36,</ref><ref type=""bibr"" target=""#b45"">47,</ref><ref type=""bibr"" target=""#b51"">53,</ref><ref type=""bibr"" target=""#b53""",0
"n to multi-character inputs leads to exponential growth in the state space <ref type=""bibr"" target=""#b42"">[44]</ref>. Recent software-based solutions <ref type=""bibr"" target=""#b24"">[26,</ref><ref type=""bibr"" target=""#b55"">57,</ref><ref type=""bibr"" target=""#b59"">61]</ref> use SIMD parallelism to miti ipal data structures observed in the PHP applications.</p><p>There is a large body of research in accelerating regular expression (regexp) processing <ref type=""bibr"" target=""#b24"">[26,</ref><ref type=""bibr"" target=""#b37"">39,</ref><ref type=""bibr"" target=""#b55"">57,</ref><ref type=""bibr"" target=""#b56",0
"ons' overall distribution reveals that many leaf functions suffer from either the abstraction overheads of scripting languages (such as type checking <ref type=""bibr"" target=""#b20"">[22]</ref>, hash table accesses for user-defined types <ref type=""bibr"" target=""#b29"">[31,</ref><ref type=""bibr"" target specialized code for accessing variables of primitive or user-defined types now requires run-time type checks. We adopted a technique from prior work <ref type=""bibr"" target=""#b20"">[22]</ref> to mitigate this overhead in hardware. With this technique, the cache subsystem performs the required type c ref type=""bibr"" target=""#b23"">[25]</ref>. These observations guide us to apply several hardware and software optimization techniques from prior works <ref type=""bibr"" target=""#b20"">[22,</ref><ref type=""bibr"" target=""#b29"">31,</ref><ref type=""bibr"" target=""#b30"">32,</ref><ref type=""bibr"" target=""#b38 Furthermore PHP, like all garbage collected languages, suffers from the overhead of reference counting.</p><p>While there are many research proposals <ref type=""bibr"" target=""#b20"">[22,</ref><ref type=""bibr"" target=""#b29"">31,</ref><ref type=""bibr"" target=""#b30"">32,</ref><ref type=""bibr"" target=""#b38 de, Javascript is used predominantly whereas PHP is the language of choice for server-side web development <ref type=""bibr"" target=""#b11"">[13]</ref>. <ref type=""bibr"" target=""#b20"">[22,</ref><ref type=""bibr"" target=""#b57"">59]</ref> propose microarchitectural changes to avoid runtime checks in jitted",0
"=""#b10"">(Daumé III and Campbell, 2007;</ref><ref type=""bibr"" target=""#b9"">Daumé III, 2009;</ref><ref type=""bibr"" target=""#b5"">Coke et al., 2016;</ref><ref type=""bibr"" target=""#b22"">Littell et al., 2017)</ref>.</p><p>In this study, we examine whether we can tackle the problem of inferring linguistic prediction both on their own and in composite with feature vectors from previous work based on the genetic and geographic distance between languages <ref type=""bibr"" target=""#b22"">(Littell et al., 2017)</ref>. Results show that the extracted representations do in fact allow us to learn about the ty g/ns/1.0""><head n=""2"">Dataset and Experimental Setup</head><p>Typology Database: To perform our analysis, we use the URIEL language typology database <ref type=""bibr"" target=""#b22"">(Littell et al., 2017)</ref>, which is a collection of binary features extracted from multiple typological, phylogeneti et al., 2016)</ref>. As an alternative that does not necessarily require pre-existing knowledge of the typological features in the language at hand, <ref type=""bibr"" target=""#b22"">Littell et al. (2017)</ref> have proposed a method for inferring typological features directly from the language's k ne",1
"HOIBLE <ref type=""bibr"" target=""#b25"">(Moran et al., 2014)</ref>, Ethnologue <ref type=""bibr"" target=""#b20"">(Lewis et al., 2015)</ref>, and Glottolog <ref type=""bibr"" target=""#b15"">(Hammarström et al., 2015)</ref>. These features are divided into separate classes regarding syntax (e.g. whether a lan",0
", and geographical databases such as WALS (World Atlas of Language Structures) <ref type=""bibr"" target=""#b6"">(Collins and Kayne, 2011)</ref>, PHOIBLE <ref type=""bibr"" target=""#b25"">(Moran et al., 2014)</ref>, Ethnologue <ref type=""bibr"" target=""#b20"">(Lewis et al., 2015)</ref>, and Glottolog <ref ty",0
"eson, 2013b)</ref>, while initial velar nasals are common in Southeast Asia (Anderson, 2013), and lateral consonants are uncommon in the Amazon Basin <ref type=""bibr"" target=""#b23"">(Maddieson, 2013a)</ref>. Since these are also regions with a particular and sometimes distinct syntactic character, we",0
"ttered. Furthermore, the clusters overlap because of high inter-class similarities. Most recently, an additional center loss was introduced into CNNs <ref type=""bibr"" target=""#b43"">[44]</ref> to reduce the intra-class variations of the learned features for face recognition. As shown in Fig. <ref typ not considered. In addition, the contrastive loss suffers from drastic data expansion when constructing image pairs from the training set. Wen et al. <ref type=""bibr"" target=""#b43"">[44]</ref> introduced a center loss for face recognition, which targets directly on one of the learning objectives, i.e s=""http://www.tei-c.org/ns/1.0""><head>A. A Brief Review of Center Loss</head><p>As illustrated in Fig. <ref type=""figure"">1</ref>(b), the center loss <ref type=""bibr"" target=""#b43"">[44]</ref> explicitly reduces the intra-class variations by pushing samples towards their corresponding class centers i using Stochastic Gradient Descent (SGD) as part of the CNN training.</p><p>1) Forward propagation: The center loss denoted as L C is defined in Eq. 1 <ref type=""bibr"" target=""#b43"">[44]</ref> as the summation of squared distances between samples and their corresponding centers in the feature space:<",1
"he recent EmotiW2015 <ref type=""bibr"" target=""#b15"">[16]</ref>, <ref type=""bibr"" target=""#b47"">[48]</ref>, <ref type=""bibr"" target=""#b29"">[30]</ref>, <ref type=""bibr"" target=""#b46"">[47]</ref>, <ref type=""bibr"" target=""#b38"">[39]</ref>, <ref type=""bibr"" target=""#b53"">[54]</ref> and EmotiW2016 challen",0
"by deep convolutional neural networks (CNNs) have achieved promising results on facial expression recognition especially in more challenging settings <ref type=""bibr"" target=""#b15"">[16]</ref>, <ref type=""bibr"" target=""#b47"">[48]</ref>, <ref type=""bibr"" target=""#b29"">[30]</ref>, <ref type=""bibr"" targ /ref>. Among them, the deep CNNs have achieved promising recognition performance under real-world conditions as demonstrated in the recent EmotiW2015 <ref type=""bibr"" target=""#b15"">[16]</ref>, <ref type=""bibr"" target=""#b47"">[48]</ref>, <ref type=""bibr"" target=""#b29"">[30]</ref>, <ref type=""bibr"" targ uses a single CNN with a shallow architecture, is ranked at the third place for the testing set among all the methods compared. Note that, Kim et al. <ref type=""bibr"" target=""#b15"">[16]</ref> and Yu et al. <ref type=""bibr"" target=""#b47"">[48]</ref>, who are ranked the 1 st and 2 nd , utilized an ense ombination of different network structures. Thus, an ensemble of IL-CNNs has been constructed and achieves comparable performance as the best methods <ref type=""bibr"" target=""#b15"">[16]</ref>, <ref type=""bibr"" target=""#b47"">[48]</ref> on the SFEW dataset. </p></div> <div xmlns=""http://www.tei-c.org/",0
"t=""#b3"">[4]</ref>, Scale Invariant Feature Transform (SIFT) features <ref type=""bibr"" target=""#b48"">[49]</ref>, histogram of Oriented Gradients (HOG) <ref type=""bibr"" target=""#b1"">[2]</ref>, histograms of Local Binary Patterns (LBP) <ref type=""bibr"" target=""#b40"">[41]</ref>, <ref type=""bibr"" target=",0
"efiting from the advance in feature learning, features can be learned either unsupervised by sparse coding <ref type=""bibr"" target=""#b22"">[23]</ref>, <ref type=""bibr"" target=""#b52"">[53]</ref>, <ref type=""bibr"" target=""#b31"">[32]</ref>, <ref type=""bibr"" target=""#b26"">[27]</ref> or supervised by deep",0
"next item that the user is probably interested in based merely on implicit feedbacks, i.e., user clicks, in the current session.</p><p>Hidasi et al. <ref type=""bibr"" target=""#b11"">[12]</ref> apply recurrent neural networks (RNN) with Gated Recurrent Units (GRU) for session-based recommendation. The type=""bibr"" target=""#b5"">[6]</ref>. Recurrent Neural Networks (RNN) have been devised to model variable-length sequence data. Recently, Hidasi et al. <ref type=""bibr"" target=""#b11"">[12]</ref> apply RNN to sessionbased recommendation and achieve significant improvements over traditional methods. The ows the graphical model of the global encoder in NARM. We use a RNN with Gated Recurrent Units (GRU) rather than a standard RNN because Hidasi et al. <ref type=""bibr"" target=""#b11"">[12]</ref> demonstrate that GRU can outperform the Long Short-Term Memory (LSTM) <ref type=""bibr"" target=""#b13"">[14]</r andidate items is used to compute a similarity score S i , To learn the parameters of the model, we do not utilize the proposed training procedure in <ref type=""bibr"" target=""#b11"">[12]</ref>, where the model is trained in a session-parallel, sequence-to-sequence manner. Instead, in order to fit the IGINETICA, the only difference is that we use the sessions of subsequent week for testing. Because we did not train NARM in a session-parallel manner <ref type=""bibr"" target=""#b11"">[12]</ref>, a </p><formula xml:id=""formula_15"">],V (x 2 ), ([x 1 , x 2 ], V (x 3 ), ..., ([x 1 , x 2 , ..., x n−1 ],V ( d recommendation, we do not consider the user latent representations when computing recommendation scores. • GRU-Rec: We denote the model proposed in <ref type=""bibr"" target=""#b11"">[12]</ref> as GRU-Rec, which utilizes session-parallel mini-batch training process and also employs ranking-based loss",1
"encoder-decoder mechanism. Deep neural networks have also been used in cross-domain recommendations whereby items are mapped to a joint latent space <ref type=""bibr"" target=""#b5"">[6]</ref>. Recurrent Neural Networks (RNN) have been devised to model variable-length sequence data. Recently, Hidasi et",0
"Methods</head><p>Deep learning has recently been applied very successfully in areas such as image recognition <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b16"">17]</ref>, speech recognition <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""b",0
"y successfully in areas such as image recognition <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b16"">17]</ref>, speech recognition <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b12"">13]</ref> and neural language processing <",0
"v> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.2"">Overview</head><p>In this paper, we propose an improved neural encoder-decoder architecture <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b34"">35]</ref> to address the session-based recommendation problem, named Neural At",0
"ble devices will then be ubiquitously present. Apart from communication, their shared access to the wireless channel enables environmental perception <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b2"">[3]</ref>, <ref type=""bibr"" target=""#b3"">[4]</ref>. However, unlike classical R ing-state and walking speed recognition has been exploited by various modalities, including RF-fluctuation, such as WiFi, FM-radio or software radios <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b3"">[4]</ref>, <ref type=""bibr"" target=""#b6"">[7]</ref>. Walking speed is also used",1
"xploiting Fresnel zones <ref type=""bibr"" target=""#b7"">[8]</ref> as well as emotion recognition from phase and time-domain signal strength fluctuation <ref type=""bibr"" target=""#b8"">[9]</ref>, <ref type=""bibr"" target=""#b9"">[10]</ref>, <ref type=""bibr"" target=""#b10"">[11]</ref>. Recognition of environme",0
"6"">[7]</ref>. Walking speed is also used in a number of relevant applications for the IoT, such as fitness tracking, attention monitoring, and health <ref type=""bibr"" target=""#b12"">[13]</ref>, <ref type=""bibr"" target=""#b13"">[14]</ref>. For instance, walking speed has been proposed as a reliable moda",0
"://www.tei-c.org/ns/1.0""><head>I. INTRODUCTION</head><p>The upcoming 5G cellular communication standard is expected to become the backbone of the IoT <ref type=""bibr"" target=""#b0"">[1]</ref>. 5G capable devices will then be ubiquitously present. Apart from communication, their shared access to the wi",0
"/div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>Approach</head><p>Accuracy FM-radio <ref type=""bibr"" target=""#b13"">[14]</ref> 0.64 RSSI (Active) <ref type=""bibr"" target=""#b17"">[18]</ref> 0.72 RSSI (Passive) <ref type=""bibr"" target=""#b16"">[17]</ref> 0.78 Acceleration <ref type=""bibr"" target=""#b1 man walking and running. Individuals have two distinct gaits (walking and running) and the natural transition point from walking to running is 2.2m/s <ref type=""bibr"" target=""#b17"">[18]</ref>.</p><p>We used a Metronome to help participants maintaining walking speed during experiments. Participants p",0
"cking multiple graph convolution layers, GCNs can learn node representations by utilizing information from distant neighbors. GCNs and their variants <ref type=""bibr"" target=""#b4"">(Hamilton et al., 2017a;</ref><ref type=""bibr"" target=""#b14"">Veličković et al., 2018)</ref> have been applied to semi-su al., 2018)</ref> have been applied to semi-supervised node classification <ref type=""bibr"">(Kipf &amp; Welling, 2017)</ref>, inductive node embedding <ref type=""bibr"" target=""#b4"">(Hamilton et al., 2017a)</ref>, link prediction <ref type=""bibr"" target=""#b6"">(Kipf &amp; Welling, 2016;</ref><ref type= xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.3."">Implementation Details</head><p>Training with the CV estimator is similar as with the NS estimator <ref type=""bibr"" target=""#b4"">(Hamilton et al., 2017a)</ref>. Particularly, each iteration of the algorithm involves the following steps:</p><p>Stocha 1 for the multi-label PPI dataset, and accuracy for all the other multi-class datasets. The model is GCN for the former 4 datasets and GraphSAGE-mean <ref type=""bibr"" target=""#b4"">(Hamilton et al., 2017a)</ref> for the latter 2 datasets, see Appendix E for the details on the architectures. We repeat accuracy result for IS+PP can match the result reported by <ref type=""bibr"" target=""#b2"">Chen et al. (2018)</ref>, while their NS baseline, GraphSAGE <ref type=""bibr"" target=""#b4"">(Hamilton et al., 2017a)</ref>, does not implement the preprocessing technique in Sec. 5.3.</p></div> <div xmlns=""http:/ er, batch algorithms cannot handle largescale datasets because of their slow convergence and the requirement to fit the entire dataset in GPU memory. <ref type=""bibr"" target=""#b4"">Hamilton et al. (2017a)</ref> make an initial attempt to develop stochastic training algorithms for GCNs via a scheme of f features (Sec. 5).</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2.3."">Neighbor Sampling</head><p>To reduce the receptive field size, <ref type=""bibr"" target=""#b4"">Hamilton et al. (2017a)</ref> propose a neighbor sampling (NS) algorithm. NS randomly chooses D (l) neighbors for each n P (l) , i.e., E P (l) = P , where</p><formula xml:id=""formula_8"">P (l) uv = n(u) D (l) P uv if v ∈ n(l) (u)</formula><p>, and P (l) uv = 0 otherwise. <ref type=""bibr"" target=""#b4"">Hamilton et al. (2017a)</ref> propose to perform an approximate forward propagation as Eq. ( <ref type=""formula"" target= . Because of the biased gradient, the sample size D (l) needs to be large for NS, to keep comparable predictive performance with the exact algorithm. <ref type=""bibr"" target=""#b4"">Hamilton et al. (2017a)</ref> choose D (1) = 10 and D (2) = 25, and the receptive field size D (1) × D (2) = 250 is much hms empirically on six datasets, including Citeseer, Cora, PubMed and NELL from <ref type=""bibr"">Kipf &amp; Welling (2017)</ref> and Reddit, PPI from <ref type=""bibr"" target=""#b4"">Hamilton et al. (2017a)</ref>, with the same train / validation / test splits, as summarized in Table <ref type=""table"" 017)</ref>. However, the algorithm is neither limited to the task nor the model. Our algorithm is applicable to other models including GraphSAGE-mean <ref type=""bibr"" target=""#b4"">(Hamilton et al., 2017a</ref>) and graph attention networks (GAT) <ref type=""bibr"" target=""#b14"">(Veličković et al., 201 ted layer instead of a graph convolution one. Since most GCNs only have two graph convolution layers <ref type=""bibr"">(Kipf &amp; Welling, 2017;</ref><ref type=""bibr"" target=""#b4"">Hamilton et al., 2017a)</ref>, this gives a significant reduction of the receptive field size and speeds up the computat",1
"embedding <ref type=""bibr"" target=""#b4"">(Hamilton et al., 2017a)</ref>, link prediction <ref type=""bibr"" target=""#b6"">(Kipf &amp; Welling, 2016;</ref><ref type=""bibr"" target=""#b1"">Berg et al., 2017)</ref> and knowledge graphs <ref type=""bibr"" target=""#b11"">(Schlichtkrull et al., 2017)</ref>, outperf s (GAT) <ref type=""bibr"" target=""#b14"">(Veličković et al., 2018)</ref>, and other tasks <ref type=""bibr"" target=""#b6"">(Kipf &amp; Welling, 2016;</ref><ref type=""bibr"" target=""#b1"">Berg et al., 2017;</ref><ref type=""bibr"" target=""#b11"">Schlichtkrull et al., 2017;</ref><ref type=""bibr"" target=""#b5"">Ha",0
"and update the parameters by SGD; 5. Update the historical activations.</p><p>Step 3 and 4 are handled automatically by frameworks such as TensorFlow <ref type=""bibr"" target=""#b0"">(Abadi et al., 2016)</ref>. The computational graph at Step 2 is defined by the receptive field r (l) and the propagatio",0
"ad n=""5."">Handling Dropout of Features</head><p>In this section, we consider introducing a third source of randomness, the random dropout of features <ref type=""bibr"" target=""#b12"">(Srivastava et al., 2014)</ref>, which is adopted in various GCN models as a regularization <ref type=""bibr"">(Kipf &amp riance from neighbor sampling (VNS) and variance from dropout (ND) of different estimators.</p><p>Our method is based on the weight scaling procedure <ref type=""bibr"" target=""#b12"">(Srivastava et al., 2014)</ref> to approximately compute the mean µ</p><formula xml:id=""formula_37"">(l) v := E M h (l)",0
"e=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b18"">20,</ref><ref type=""bibr"" target=""#b19"">21,</ref><ref type=""bibr"" target=""#b22"">24,</ref><ref type=""bibr"" target=""#b40"">42]</ref>.</p><p>Single image SR is an ill-posed inverse problem where the aim emented as a deep neural network <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b35"">37,</ref><ref type=""bibr"" target=""#b22"">24,</ref><ref type=""bibr"" target=""#b19"">21,</ref><ref type=""bibr"" target=""#b20"">22,</ref><ref type=""bibr"" target=""#b40"" eatures, then construct the SR image at the last step. (c) Progressive upsampling uses a Laplacian pyramid network which gradually predicts SR images <ref type=""bibr"" target=""#b22"">[24]</ref>. (d) Iterative up and downsampling approach is proposed by our DBPN which exploit the mutually connected up- he opportunities to propose lighter networks that can preserve HR components better.</p><p>(c) Progressive upsampling was recently proposed in LapSRN <ref type=""bibr"" target=""#b22"">[24]</ref>. It progressively reconstructs the multiple SR images with different scales in one feed-forward network. For DSR <ref type=""bibr"" target=""#b19"">[21]</ref>, DRCN <ref type=""bibr"" target=""#b20"">[22]</ref>, DRRN <ref type=""bibr"" target=""#b40"">[42]</ref>, LapSRN <ref type=""bibr"" target=""#b22"">[24]</ref>) on Set5 dataset for 4× enlargement.</p><p>the-art methods (VDSR, DRCN, LapSRN, and DRRN). At the best perfo <ref type=""bibr"" target=""#b19"">[21]</ref>,    DRCN <ref type=""bibr"" target=""#b20"">[22]</ref>, DRRN <ref type=""bibr"" target=""#b40"">[42]</ref>, LapSRN <ref type=""bibr"" target=""#b22"">[24]</ref>, and EDSR <ref type=""bibr"" target=""#b28"">[30]</ref>. We carry out extensive experiments using 5 datasets: Se features, then construct the SR image at the last step. (c) Progressive upsampling uses a Laplacian pyramid network which gradually predicts SR images<ref type=""bibr"" target=""#b22"">[24]</ref>. (d) Iterative up and downsampling approach is proposed by our DBPN which exploit the mutually connected up-",1
"=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b32"">34,</ref><ref type=""bibr"" target=""#b44"">46,</ref><ref type=""bibr"" target=""#b26"">28,</ref><ref type=""bibr"" target=""#b46"">48,</ref><ref type=""bibr"" target=""#b36"">38,</ref><ref type=""bibr"" target=""#b29"">31]</ref>.</p><p>In the context of huma",0
"e=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b37"">39,</ref><ref type=""bibr"" target=""#b24"">26,</ref><ref type=""bibr"" target=""#b31"">33,</ref><ref type=""bibr"" target=""#b15"">17]</ref> has recently been propagating to the field of super-resolution (SR) <ref type=""bibr"" target=""#b17"">[19,</ref>",0
"f type=""bibr"" target=""#b22"">[24]</ref>, and EDSR <ref type=""bibr"" target=""#b28"">[30]</ref>. We carry out extensive experiments using 5 datasets: Set5 <ref type=""bibr"" target=""#b1"">[2]</ref>, Set14 <ref type=""bibr"" target=""#b47"">[49]</ref>, BSDS100 <ref type=""bibr"" target=""#b0"">[1]</ref>, Urban100 [1",0
"=""http://www.tei-c.org/ns/1.0""><head n=""2.3."">Adversarial training</head><p>Adversarial training, such as with Generative Adversarial Networks (GANs) <ref type=""bibr"" target=""#b9"">[10]</ref> has been applied to various image reconstruction problems <ref type=""bibr"" target=""#b25"">[27,</ref><ref type=",0
"</ref>.</p><p>To our knowledge, none of the existing works has paid special attention to embed bipartite networks. While a recent work by Dong et al. <ref type=""bibr"" target=""#b13"">[14]</ref> proposed metapath2vec++ for embedding heterogeneous networks which can also be applied to bipartite networks us, these homogeneous network embedding methods might be suboptimal for learning vertex representations for a bipartite network.</p><p>Metapath2vec++ <ref type=""bibr"" target=""#b13"">[14]</ref>, HNE <ref type=""bibr"" target=""#b26"">[27]</ref> and EOE <ref type=""bibr"" target=""#b27"">[28]</ref> are represe rtex importance can be preserved to some extent. • We assign a probability to stop a random walk in each step. In contrast to DeepWalk and other work <ref type=""bibr"" target=""#b13"">[14]</ref> that apply a fixed length on the random walk, we allow the generated vertex sequences have a variable length ks to generate the corpus of vertex sequences. The hyper-parameters p and q are set to 0.5 which has empirically shown good results. • Metapath2vec++ <ref type=""bibr"" target=""#b13"">[14]</ref>: This is the state-of-the-art method for embedding heterogeneous networks.The meta-path scheme chosen in our",1
"There are some follow-up works exploiting both 1st-order and 2nd-order proximities between vertices to embed homogeneous networks. Specifically, LINE <ref type=""bibr"" target=""#b19"">[20]</ref> learns two separated embeddings for 1st-order and 2nd-order relations; SDNE <ref type=""bibr"" target=""#b20"">[ es of two different types, providing an explicit signal on constructing the bipartite network. Similar to the modeling of 1st-order proximity in LINE <ref type=""bibr"" target=""#b19"">[20]</ref>, we model explicit relations by considering the local proximity between two connected vertices. The joint pr Walk performs uniform random walks to get a corpus of vertex sequences. Then the word2vec is applied on the corpus to learn vertex embeddings. • LINE <ref type=""bibr"" target=""#b19"">[20]</ref>: This approach optimizes both the 1st-order and 2nd-order proximities in a homogeneous network. We use the L g space. The effectiveness and prevalence of word2vec inspire many works <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b19"">20]</ref> to use inner product to model the interaction between two entities. We follow this setting, and use sigmoid f abel>(6)</label></formula><p>Following existing neural embedding methods <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b19"">20]</ref>, we parameterize the conditional probability P(u c |u i ) and P(v c |v j ) using the inner product kernel wit",1
"re vertices are of the same type <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b7"">[8]</ref><ref type=""bibr"" target=""#b8"">[9]</ref><ref type=""bibr"" target=""#b9"">[10]</ref>. Following the pioneering work of DeepWalk <ref type=""bibr"" target=""#b7"">[8]</ref>, these methods typically a s capturing high-order proximities, there are several proposals to incorporate side information into vertex embedding learning, such as vertex labels <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b22"">23]</ref>, community information <ref type=""bibr"" target=""#b23"">[24]</ref>, tex",0
"rization (MF)-based and neural network-based methods.</p><p>MF-based methods are either linear <ref type=""bibr"" target=""#b15"">[16]</ref> or nonlinear <ref type=""bibr"" target=""#b16"">[17]</ref> in learning vertex embeddings. The former employs the linear transformations to embed network vertices into inear transformations, e.g., kernel PCA, spectral embedding , marginal fisher analysis (MFA), and manifold learning approaches include LLE and ISOMAP <ref type=""bibr"" target=""#b16"">[17]</ref>. Generally speaking, MF-based methods have two main drawbacks: (1) they are usually computationally expensiv",0
"bipartite network, where the edges can indicate users' click behaviors that provide valuable relevance signal <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref>; in another application of recommender systems, users and items form a bipartite network, where the edges can e",0
"might be suboptimal for learning vertex representations for a bipartite network.</p><p>Metapath2vec++ <ref type=""bibr"" target=""#b13"">[14]</ref>, HNE <ref type=""bibr"" target=""#b26"">[27]</ref> and EOE <ref type=""bibr"" target=""#b27"">[28]</ref> are representative vertex embedding methods for heterogene",0
"br"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b22"">23]</ref>, community information <ref type=""bibr"" target=""#b23"">[24]</ref>, textual content <ref type=""bibr"" target=""#b24"">[25]</ref>, user profiles <ref type=""bibr"" target=""#b8"">[9]</ref>, location information <ref type=""bibr"" target=""#b25"">",0
"tex embedding learning, such as vertex labels <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b22"">23]</ref>, community information <ref type=""bibr"" target=""#b23"">[24]</ref>, textual content <ref type=""bibr"" target=""#b24"">[25]</ref>, user profiles <ref type=""bibr"" target=""#b8"">[9]<",0
"re usually computationally expensive due to the eigen-decomposition operations on data matrices, making them difficult to handle large-scale networks <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b18"">19]</ref>;</p><p>(2) their performance are rather sensitive to the predefined",0
", which can be categorized into two types: matrix factorization (MF)-based and neural network-based methods.</p><p>MF-based methods are either linear <ref type=""bibr"" target=""#b15"">[16]</ref> or nonlinear <ref type=""bibr"" target=""#b16"">[17]</ref> in learning vertex embeddings. The former employs the s to embed network vertices into a low dimensional embedding space, such as singular value decomposition (SVD) and multiple dimensional scaling (MDS) <ref type=""bibr"" target=""#b15"">[16]</ref>. However, the latter maps network vertices into a low dimensional latent space by utilizing the nonlinear tr",0
"n features have been widely used <ref type=""bibr"" target=""#b6"">(Collobert et al., 2011;</ref><ref type=""bibr"" target=""#b29"">Passos et al., 2014;</ref><ref type=""bibr"" target=""#b15"">Huang et al., 2015;</ref><ref type=""bibr"" target=""#b26"">Luo et al., 2015)</ref>. <ref type=""bibr"" target=""#b37"">Rei (20 for segmentation-free Chinese NER.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3"">Model</head><p>We follow the best English NER model <ref type=""bibr"" target=""#b15"">(Huang et al., 2015;</ref><ref type=""bibr"" target=""#b27"">Ma and Hovy, 2016;</ref><ref type=""bibr"" target=""#b17"">Lample",1
"ere entity boundary and category labels are jointly predicted. The current stateof-the-art for English NER has been achieved by using LSTM-CRF models <ref type=""bibr"" target=""#b17"">(Lample et al., 2016;</ref><ref type=""bibr"" target=""#b27"">Ma and Hovy, 2016;</ref><ref type=""bibr"" target=""#b5"">Chiu an >We follow the best English NER model <ref type=""bibr"" target=""#b15"">(Huang et al., 2015;</ref><ref type=""bibr"" target=""#b27"">Ma and Hovy, 2016;</ref><ref type=""bibr"" target=""#b17"">Lample et al., 2016)</ref>, using LSTM-CRF as the main network structure. Formally, denote an input sentence as s = c 1 its representation:</p><p>Integrating character representations Both character CNN <ref type=""bibr"" target=""#b27"">(Ma and Hovy, 2016)</ref> and LSTM <ref type=""bibr"" target=""#b17"">(Lample et al., 2016)</ref> have been used for representing the character sequence within a word. We experiment with bo",1
"ted. The current stateof-the-art for English NER has been achieved by using LSTM-CRF models <ref type=""bibr"" target=""#b17"">(Lample et al., 2016;</ref><ref type=""bibr"" target=""#b27"">Ma and Hovy, 2016;</ref><ref type=""bibr"" target=""#b5"">Chiu and Nichols, 2016;</ref><ref type=""bibr"" target=""#b21"">Liu e http://www.tei-c.org/ns/1.0""><head n=""3"">Model</head><p>We follow the best English NER model <ref type=""bibr"" target=""#b15"">(Huang et al., 2015;</ref><ref type=""bibr"" target=""#b27"">Ma and Hovy, 2016;</ref><ref type=""bibr"" target=""#b17"">Lample et al., 2016)</ref>, using LSTM-CRF as the main network s y, for each word w i , − → h w i and ← − h w i are concatenated as its representation:</p><p>Integrating character representations Both character CNN <ref type=""bibr"" target=""#b27"">(Ma and Hovy, 2016)</ref> and LSTM <ref type=""bibr"" target=""#b17"">(Lample et al., 2016)</ref> have been used for repres",1
"table. seg(c j ) denotes the segmentation label on the character c j given by a word segmentor. We use the BMES scheme for repre-senting segmentation <ref type=""bibr"" target=""#b48"">(Xue, 2003)</ref>.</p><formula xml:id=""formula_6"">h w i = [ − → h w i ; ← − h w i ]<label>(5)</label></formula><p>Simil",0
"e severe in the open domain since crossdomain word segmentation remains an unsolved problem <ref type=""bibr"" target=""#b22"">(Liu and Zhang, 2012;</ref><ref type=""bibr"" target=""#b16"">Jiang et al., 2013;</ref><ref type=""bibr"" target=""#b23"">Liu et al., 2014;</ref><ref type=""bibr"" target=""#b35"">Qiu and Z",0
"segmentor gives 95.93% accuracy on 5-fold cross-validated training set. The best statistical models on the dataset leverage rich handcrafted features <ref type=""bibr"" target=""#b1"">(Chen et al., 2006a;</ref><ref type=""bibr"" target=""#b52"">Zhang et al., 2006;</ref><ref type=""bibr"" target=""#b54"">Zhou et",0
"et leverage rich handcrafted features <ref type=""bibr"" target=""#b1"">(Chen et al., 2006a;</ref><ref type=""bibr"" target=""#b52"">Zhang et al., 2006;</ref><ref type=""bibr"" target=""#b54"">Zhou et al., 2013)</ref> and character embedding features <ref type=""bibr"" target=""#b25"">(Lu et al., 2016)</ref>. <ref",0
"=""#b6"">(Collobert et al., 2011;</ref><ref type=""bibr"" target=""#b29"">Passos et al., 2014;</ref><ref type=""bibr"" target=""#b15"">Huang et al., 2015;</ref><ref type=""bibr"" target=""#b26"">Luo et al., 2015)</ref>. <ref type=""bibr"" target=""#b37"">Rei (2017)</ref> uses a word-level language modeling objective",0
"It has been shown that character-based methods outperform word-based methods for Chinese NER <ref type=""bibr"" target=""#b13"">(He and Wang, 2008;</ref><ref type=""bibr"" target=""#b24"">Liu et al., 2010;</ref><ref type=""bibr"" target=""#b20"">Li et al., 2014)</ref>.</p><p>One drawback of character-based NER haracter-based methods for the task, showing that the latter is empirically a superior choice <ref type=""bibr"" target=""#b13"">(He and Wang, 2008;</ref><ref type=""bibr"" target=""#b24"">Liu et al., 2010;</ref><ref type=""bibr"" target=""#b20"">Li et al., 2014)</ref>. We find that with proper representation s",0
"ntation remains an unsolved problem <ref type=""bibr"" target=""#b22"">(Liu and Zhang, 2012;</ref><ref type=""bibr"" target=""#b16"">Jiang et al., 2013;</ref><ref type=""bibr"" target=""#b23"">Liu et al., 2014;</ref><ref type=""bibr"" target=""#b35"">Qiu and Zhang, 2015;</ref><ref type=""bibr"" target=""#b4"">Chen et a",0
""" target=""#b23"">Liu et al., 2014;</ref><ref type=""bibr"" target=""#b35"">Qiu and Zhang, 2015;</ref><ref type=""bibr"" target=""#b4"">Chen et al., 2017;</ref><ref type=""bibr"" target=""#b14"">Huang et al., 2017)</ref>. It has been shown that character-based methods outperform word-based methods for Chinese NER",0
"ogonal to and can also be used for our lattice LSTM model.</p><p>Lattice structured RNNs can be viewed as a natural extension of tree-structured RNNs <ref type=""bibr"" target=""#b42"">(Tai et al., 2015)</ref> to DAGs. They have been used to model motion dynamics <ref type=""bibr"" target=""#b41"">(Sun et a",0
"ref type=""bibr"" target=""#b14"">Huang et al., 2017)</ref>. It has been shown that character-based methods outperform word-based methods for Chinese NER <ref type=""bibr"" target=""#b13"">(He and Wang, 2008;</ref><ref type=""bibr"" target=""#b24"">Liu et al., 2010;</ref><ref type=""bibr"" target=""#b20"">Li et al. licit discussions comparing statistical word-based and character-based methods for the task, showing that the latter is empirically a superior choice <ref type=""bibr"" target=""#b13"">(He and Wang, 2008;</ref><ref type=""bibr"" target=""#b24"">Liu et al., 2010;</ref><ref type=""bibr"" target=""#b20"">Li et al.",0
"cker can only issue classification queries to the targeted network, which corresponds to a more restrictive black box threat model.</p><p>Recent work <ref type=""bibr"" target=""#b4"">(Chen et al., 2017;</ref><ref type=""bibr"" target=""#b1"">Bhagoji et al., 2017;</ref><ref type=""bibr"" target=""#b10"">Ilyas e target=""#b1"">Bhagoji et al., 2017;</ref><ref type=""bibr"" target=""#b10"">Ilyas et al., 2017)</ref> provides a number of attacks for this threat model. <ref type=""bibr"" target=""#b4"">Chen et al. (2017)</ref> show how to use a basic primitive of zeroth order optimization, the finite difference method, t timator:</p><formula xml:id=""formula_4"">x l = Π Bp(x, ) (x l−1 + η s l ) with s l = Π ∂Bp(0,1) ∇ x L(x l−1 , y)<label>(4)</label></formula><p>Indeed, <ref type=""bibr"" target=""#b4"">Chen et al. (2017)</ref> were the first to use finite differences methods in this basic form to power PGD-based adversar lassifier on the ImageNet dataset has dimensionality d=268,203 and thus this method would require 268,204 queries. (It is worth noting, however, that <ref type=""bibr"" target=""#b4"">Chen et al. (2017)</ref> developed additional methods to, at least partially, reduce this query complexity.)</p></div> <",1
"a wide range of settings, from situations in which inputs are fed directly to classifiers <ref type=""bibr"" target=""#b23"">(Szegedy et al., 2013;</ref><ref type=""bibr"" target=""#b3"">Carlini et al., 2016)</ref> to highly variable real-world environments <ref type=""bibr"" target=""#b12"">(Kurakin et al., 2",0
"""bibr"" target=""#b3"">Carlini et al., 2016)</ref> to highly variable real-world environments <ref type=""bibr"" target=""#b12"">(Kurakin et al., 2016;</ref><ref type=""bibr"" target=""#b0"">Athalye et al., 2017)</ref>. Researchers have developed a host of methods to construct such attacks <ref type=""bibr"" tar",0
"et al., 2016;</ref><ref type=""bibr"" target=""#b0"">Athalye et al., 2017)</ref>. Researchers have developed a host of methods to construct such attacks <ref type=""bibr"" target=""#b7"">(Goodfellow et al., 2014;</ref><ref type=""bibr"">Moosavi-Dezfooli et al., 2015;</ref><ref type=""bibr"" target=""#b2"">Carlin la_0"">x = arg max x : x −x p ≤ p L(x , y)</formula><p>First order methods tend to be very successful at solving the problem despite its non-convexity <ref type=""bibr"" target=""#b7"">(Goodfellow et al., 2014;</ref><ref type=""bibr"" target=""#b2"">Carlini &amp; Wagner, 2017;</ref><ref type=""bibr"" target=""# We examine this question first in the simplest possible setting: one in which we only take a single PGD step (i.e., the case of k = 1). Previous work <ref type=""bibr"" target=""#b7"">(Goodfellow et al., 2014)</ref>   <ref type=""figure"">1</ref>, suggest that it is feasible to generate adversarial exampl",0
"t rich responses that are based on a model of syntax and topics <ref type=""bibr"" target=""#b12"">(Griffiths et al., 2005)</ref> and semantic similarity <ref type=""bibr"" target=""#b0"">(Arora et al., 2016)</ref>. We evaluate our approach against a variety of competitive baselines, using both automatic me rages generated responses to be semantically similar to the user's input; semantic similarity is measured using fixed-dimensional sentence embeddings <ref type=""bibr"" target=""#b0"">(Arora et al., 2016)</ref>.</p><p>After introducing distributional constraints into the decoding objective, we empirical ically function words) in the overall sentence embedding. Next the first principal component of all the sentence embeddings in the corpus is removed. <ref type=""bibr"" target=""#b0"">(Arora et al., 2016)</ref> points that the first principal component has high cosine similarity with common function wor ence embedding methods that could be used, however we want this encoding to be relatively efficient as it will be used many times during beam search. <ref type=""bibr"" target=""#b0"">Arora et. al. (2016)</ref> recently proposed a simple sentence embedding method, which was shown to have competitive per",1
"ver the generated responses. We propose two constraints that help generate more content rich responses that are based on a model of syntax and topics <ref type=""bibr"" target=""#b12"">(Griffiths et al., 2005)</ref> and semantic similarity <ref type=""bibr"" target=""#b0"">(Arora et al., 2016)</ref>. We eva esponse to match that found in the user's input. To estimate these distributions, we leverage the unsupervised model of topics and syntax proposed by <ref type=""bibr"" target=""#b12"">Griffiths and Steyvers (2005)</ref>. The second constraint encourages generated responses to be semantically similar to",1
"mple: ""I don't know"" or ""What are you talking about ?"". This is a pervasive problem that has been independently reported  by multiple research groups <ref type=""bibr"" target=""#b17"">(Li et al., 2016a;</ref><ref type=""bibr"">Serban et al., 2016;</ref><ref type=""bibr"" target=""#b20"">Li et al., 2016c)</re man judgments, that our approach generates more content-rich responses when compared with two competitive baselines: Maximum Mutual Information (MMI) <ref type=""bibr"" target=""#b17"">(Li et al., 2016a)</ref>, in addition to an approach that conditions on topic models as additional context in neural co sing a separately trained source given tar-get model, P (X|Y ). Combining both directions in this way has the effect of maximizing mutual information <ref type=""bibr"" target=""#b17"">(Li et al., 2016a)</ref>. TA-Seq2Seq: Another relevant baseline is the TA-Seq2Seq model of <ref type=""bibr"" target=""#b4 ref> which has been used as a basis for a broad range of recent work on neural conversation <ref type=""bibr"" target=""#b13"">(Kannan et al., 2016;</ref><ref type=""bibr"" target=""#b17"">Li et al., 2016a;</ref><ref type=""bibr"">Serban et al., 2016;</ref><ref type=""bibr"" target=""#b31"">Shao et al., 2017)</re trained and evaluated neural conversation models using this corpus. In our experiments we used a preprocessed version of this dataset distributed by <ref type=""bibr"" target=""#b17"">Li et. al. (2016a)</ref>. <ref type=""foot"" target=""#foot_4"">6</ref> The dataset contains large number of two turn dialo g learning we use the same hyperparameters for all models; these are displayed in Table <ref type=""table"">1</ref>, and are based on those reported by <ref type=""bibr"" target=""#b17"">Li et. al. (2016a)</ref>. <ref type=""foot"" target=""#foot_5"">7</ref> We compare our approach with the following baseline ref type=""foot"" target=""#foot_5"">7</ref> We compare our approach with the following baselines: MMI: We re-implemented the MMI-bidi method proposed by <ref type=""bibr"" target=""#b17"">Li et. al. (2016a)</ref>. MMI is a particularly appropriate baseline for comparison, as it encourages responses that ha e presented in Table <ref type=""table"">3</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""7.1"">Automatic Metrics</head><p>Following <ref type=""bibr"" target=""#b17"">Li et. al. (2016a)</ref>, we report distinct-1 and distinct-2, which measure the diversity of re-  <ref type=""table"">3< or data-driven response generation <ref type=""bibr"" target=""#b30"">(Shang et al., 2015;</ref><ref type=""bibr"" target=""#b32"">Sordoni et al., 2015;</ref><ref type=""bibr"" target=""#b17"">Li et al., 2016a)</ref>. Our approach, which incorporates distributional constraints into the decoding objective, is re",0
"aints into the decoding objective, is related to prior work on posterior regularization <ref type=""bibr"" target=""#b24"">(Mann and McCallum, 2008;</ref><ref type=""bibr"" target=""#b10"">Ganchev et al., 2010;</ref><ref type=""bibr"" target=""#b45"">Zhu et al., 2014)</ref>. Posterior regularization introduces",0
"al. (2018)</ref> used crowd-workers to gather a corpus of 100K information-seeking QA dialogues that are answerable using text spans from Wikipedia. <ref type=""bibr"" target=""#b26"">Niu and Bansal (2018)</ref> designed a number of weakly-supervised models that generate polite, neutral or rude respons",0
"</ref>. Our approach, which incorporates distributional constraints into the decoding objective, is related to prior work on posterior regularization <ref type=""bibr"" target=""#b24"">(Mann and McCallum, 2008;</ref><ref type=""bibr"" target=""#b10"">Ganchev et al., 2010;</ref><ref type=""bibr"" target=""#b45""",0
"ation of automatic metrics and human judgments. Some recent work has explored the possibility of adversarial evaluation of neural conversation models <ref type=""bibr"" target=""#b23"">(Lowe et al., 2017;</ref><ref type=""bibr"" target=""#b21"">Li et al., 2017)</ref>.</p></div> <div xmlns=""http://www.tei-c.",0
"function. Similar ideas have been explored in the context of neural generation models. <ref type=""bibr"" target=""#b36"">(Vijayakumar et al., 2016;</ref><ref type=""bibr"" target=""#b18"">Li and Jurafsky, 2016;</ref><ref type=""bibr"" target=""#b19"">Li et al., 2016b)</ref> Following previous work we evaluated",0
"ype=""bibr"" target=""#b21"">(Li et al., 2017)</ref> that rewards generated conversations that are indistinguishable from real conversations in the data. <ref type=""bibr"" target=""#b16"">Lewis et. al. (2017)</ref> applied reinforcement learning with dialogue rollouts to generate replies that maximize expe",0
"echniques such as fully convolutional networks (FCNs) <ref type=""bibr"" target=""#b12"">[13]</ref>- <ref type=""bibr"" target=""#b16"">[17]</ref>, and U-Net <ref type=""bibr"" target=""#b17"">[18]</ref> algorithms have been used.</p><p>Although it is possible for deep learning algorithms to learn contextual fe d the decoder part consists of a deconvolution layer for upsampling.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>B. U-Net</head><p>U-Net <ref type=""bibr"" target=""#b17"">[18]</ref> is a modified FCN for yielding more precise segmentation. U-Net has two different architectures compared to tract long-range information, it can lose local information such as boundaries of objects. To overcome this problem, skip connection methods are used <ref type=""bibr"" target=""#b17"">[18]</ref>, <ref type=""bibr"" target=""#b24"">[25]</ref>. The features extracted from PPL are upsampled and concatenated w mIOU to 79.52, which is 0.50 higher than mIOU of U-Net. It is important to note that the compared U-Net is not the original architecture proposed in <ref type=""bibr"" target=""#b17"">[18]</ref> but a highly calibrated model for the enhanced capability of object segmentation from satellite images <ref",1
"TATION SYSTEMS</head><p>There are several algorithms that are applied to object segmentation based on CNNs <ref type=""bibr"" target=""#b21"">[22]</ref>- <ref type=""bibr"" target=""#b23"">[24]</ref>. Because the performance of deep learning algorithms depends on their structures, it should be optimized to",1
"of the CNN model from classification to dense prediction by reinterpretation of fully connected layers of the classifier as a fully convolution layer <ref type=""bibr"" target=""#b24"">[25]</ref>. The FCN consists of an encoder of input images and a decoder that upsamples encoded images by their origina ocal information such as boundaries of objects. To overcome this problem, skip connection methods are used <ref type=""bibr"" target=""#b17"">[18]</ref>, <ref type=""bibr"" target=""#b24"">[25]</ref>. The features extracted from PPL are upsampled and concatenated with the output of the 14th rectified linear",1
"and a decoder that upsamples encoded images by their original image size. The encoder part of FCN consists of visual geometry group network (VGGNet) <ref type=""bibr"" target=""#b25"">[26]</ref> that is a famous CNN classification model and the decoder part consists of a deconvolution layer for upsampl",1
"hidden layers. We here used the crossentropy loss between prediction results and the ground truth for training the proposed model. The Adam optimizer <ref type=""bibr"" target=""#b30"">[31]</ref> with the learning rate α = 10 −4 , the exponential decay rates for the moment of estimates β 1 = 0.9 and β 2",0
"convolutional neural networks (CNNs) have attracted much attention to segment objects in satellite images <ref type=""bibr"" target=""#b10"">[11]</ref>, <ref type=""bibr"" target=""#b11"">[12]</ref>. To segment roads and buildings from satellite images, several semantic segmentation models based on deep le",0
"ad>I. INTRODUCTION</head><p>A ERIAL images can provide valuable information on areas that are difficult for people to access or access nonintrusively <ref type=""bibr"" target=""#b0"">[1]</ref>. The information obtained using aerial images is used in a variety of industries including land inventory, veg",0
"s, which are modeled on the analysis and interpretation of a human operator, to segment objects from aerial images is widely used in segmenting roads <ref type=""bibr"" target=""#b5"">[6]</ref>, and buildings <ref type=""bibr"" target=""#b6"">[7]</ref>. Also, traditional machine learning techniques such as",0
"semantic segmentation models based on deep learning techniques such as fully convolutional networks (FCNs) <ref type=""bibr"" target=""#b12"">[13]</ref>- <ref type=""bibr"" target=""#b16"">[17]</ref>, and U-Net <ref type=""bibr"" target=""#b17"">[18]</ref> algorithms have been used.</p><p>Although it is possibl",0
"ks to segment objects such as roads and buildings have been carried out using aerial images in rural areas <ref type=""bibr"" target=""#b13"">[14]</ref>- <ref type=""bibr"" target=""#b15"">[16]</ref>. In addition, they have used many data from the OpenStreetMap (OSM) <ref type=""bibr"" target=""#b28"">[29]</ref",0
"by the abovementioned method.</p><p>There are two main types of research for detecting objects from aerial images: semiautomatic and fully automatic <ref type=""bibr"" target=""#b4"">[5]</ref>. However, because the semiautomatic methods require prior knowledge of the extraction process, such as identif",0
"d buildings from satellite images, several semantic segmentation models based on deep learning techniques such as fully convolutional networks (FCNs) <ref type=""bibr"" target=""#b12"">[13]</ref>- <ref type=""bibr"" target=""#b16"">[17]</ref>, and U-Net <ref type=""bibr"" target=""#b17"">[18]</ref> algorithms h on requires IEEE permission.</p><p>See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p><p>research <ref type=""bibr"" target=""#b12"">[13]</ref>, <ref type=""bibr"" target=""#b13"">[14]</ref>. To address these problems, we make a large scale of the data set urb the training for CNN <ref type=""bibr"" target=""#b29"">[30]</ref>. To overcome the aforementioned disadvantages, a fine-tuning scheme is proposed in <ref type=""bibr"" target=""#b12"">[13]</ref>, showing that the fine-tuned network outperforms the untuned one. After they trained CNN by using a large sc oblems have been investigated in binary classification, mainly road classification, and building segmentation due to the lack of multiclass data sets <ref type=""bibr"" target=""#b12"">[13]</ref>, <ref type=""bibr"" target=""#b13"">[14]</ref>. Moreover, classifying several classes at once can solve the prob",0
"ne learning techniques such as support vector machine and artificial neural networks have been studied in <ref type=""bibr"" target=""#b7"">[8]</ref> and <ref type=""bibr"" target=""#b8"">[9]</ref>. The automatic methods are objective without human intervention, but their results have generally been discour",0
"f type=""bibr"" target=""#b13"">[14]</ref>- <ref type=""bibr"" target=""#b15"">[16]</ref>. In addition, they have used many data from the OpenStreetMap (OSM) <ref type=""bibr"" target=""#b28"">[29]</ref>. However, the OSM has several defects (e.g., buildings match in a different direction or some regions are no",0
"addition, deep learning techniques such as convolutional neural networks (CNNs) have attracted much attention to segment objects in satellite images <ref type=""bibr"" target=""#b10"">[11]</ref>, <ref type=""bibr"" target=""#b11"">[12]</ref>. To segment roads and buildings from satellite images, several se",0
"<head>II. DEEP LEARNING-BASED OBJECT SEGMENTATION SYSTEMS</head><p>There are several algorithms that are applied to object segmentation based on CNNs <ref type=""bibr"" target=""#b21"">[22]</ref>- <ref type=""bibr"" target=""#b23"">[24]</ref>. Because the performance of deep learning algorithms depends on t",0
"rmation obtained using aerial images is used in a variety of industries including land inventory, vegetation monitoring, and environmental assessment <ref type=""bibr"" target=""#b1"">[2]</ref>. In particular, extraction of manufactured features such as buildings, roads, railway lines, etc., or natural",0
"get=""#b6"">[7]</ref>. Also, traditional machine learning techniques such as support vector machine and artificial neural networks have been studied in <ref type=""bibr"" target=""#b7"">[8]</ref> and <ref type=""bibr"" target=""#b8"">[9]</ref>. The automatic methods are objective without human intervention, b",0
"layer. Thus, U-Net shows an outstanding performance not only for processing biomedical images but also for segmenting objects from satellite imagery <ref type=""bibr"" target=""#b26"">[27]</ref>. In this paper, we also exploit the U-Net architecture for multiclass object segmentation.</p></div> <div xm d in <ref type=""bibr"" target=""#b17"">[18]</ref> but a highly calibrated model for the enhanced capability of object segmentation from satellite images <ref type=""bibr"" target=""#b26"">[27]</ref>. Therefore, even a small gain of mIOU with UNetPPL is remarkable. In particular, UNetPPL outperforms other m",0
"igitize aerial images. It is still the main method to generate geospatial data but takes much labor and time to extract or identify features manually <ref type=""bibr"" target=""#b3"">[4]</ref>. With the development of the optical sensor technology, it is possible to obtain higher resolution images, and",0
"management of LLC in order to make the most out of it.</p><p>This paper presents the results of our study of the nonuniform cache architecture (NUCA) <ref type=""bibr"" target=""#b36"">[35]</ref> characteristics of LLC in Intel processors where the LLC is divided into multiple slices interconnected via ype=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b23"">23,</ref><ref type=""bibr"" target=""#b36"">35,</ref><ref type=""bibr"" target=""#b66"">65,</ref><ref type=""bibr"" target=""#b78"">77]</ref>. Some other works focus on so",1
"#b77"">76]</ref> or exploited <ref type=""bibr"" target=""#b17"">[17,</ref><ref type=""bibr"" target=""#b64"">63,</ref><ref type=""bibr"" target=""#b65"">64,</ref><ref type=""bibr"" target=""#b83"">[81]</ref><ref type=""bibr"" target=""#b84"">[82]</ref><ref type=""bibr"" target=""#b85"">[83]</ref> hardware-based cache parti",0
"mploy user-space packet processing frameworks to eliminate the costly traditional kernel-based network stack <ref type=""bibr"" target=""#b15"">[15,</ref><ref type=""bibr"" target=""#b21"">21,</ref><ref type=""bibr"" target=""#b53"">52,</ref><ref type=""bibr"" target=""#b57"">56]</ref>. In addition to user-space I/",0
"ory/cache access and these access times are different from the nominal LLC access times stated by Intel (e.g., 34 cycles for the Haswell architecture <ref type=""bibr"" target=""#b11"">[12]</ref>). However, this extra overhead shows the actual impact of access time on real-world applications, as using p",0
". Many of these works (e.g., <ref type=""bibr"" target=""#b24"">[24,</ref><ref type=""bibr"" target=""#b37"">36,</ref><ref type=""bibr"" target=""#b39"">38,</ref><ref type=""bibr"" target=""#b41"">40,</ref><ref type=""bibr"" target=""#b67"">66]</ref>) proposed software techniques for cache-aware memory allocation. Thes",0
"which can help the model to learn target tasks better <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b10"">11]</ref>. According to <ref type=""bibr"" target=""#b11"">[12]</ref>, the accuracy rate of the cross-language speech recognition model based on sequence was 6% higher than that",1
"s, had an insufficient effect of speech recognition. It achieved topic and keyword identification that the cross-language transfer learning method in <ref type=""bibr"" target=""#b13"">[14]</ref> used to learn the characteristics of low-resourced languages from rich-resourced languages achieves, but it",1
"ype=""bibr"" target=""#b2"">[3]</ref><ref type=""bibr"" target=""#b3"">[4]</ref><ref type=""bibr"" target=""#b4"">[5]</ref><ref type=""bibr"" target=""#b5"">[6]</ref><ref type=""bibr"" target=""#b6"">[7]</ref>. This method directly models between the phoneme sequence or context-dependent phone (CD-phone) sequence and t",0
"n and modelling. Moreover, it has more advantages in the processing of complex signals. Deep learning was first applied to speech recognition in 2009 <ref type=""bibr"" target=""#b17"">[18]</ref>. It provides a 20% improvement over the traditional Gaussian mixture model-hidden Markov model (GMM-HMM) spe",0
"o the character sequence W = (w 1 , w 2 , ? ? ? , w N ) and calculates the probability P(W|X) , where T is the time and N is the number of characters <ref type=""bibr"" target=""#b1"">[2]</ref>. In the timing classification task, the commonly used method is to train by frame using the hidden Markov mode",0
"t the signal. Compared with traditional speech features, the CNN can obtain more robust features using local filtering and maximum pooling techniques <ref type=""bibr"" target=""#b26"">[27,</ref><ref type=""bibr"" target=""#b27"">28]</ref>. The CNN was originally designed to solve problems such as computer",0
">[21]</ref><ref type=""bibr"" target=""#b21"">[22]</ref><ref type=""bibr"" target=""#b22"">[23]</ref>. In References <ref type=""bibr"" target=""#b23"">[24]</ref><ref type=""bibr"" target=""#b24"">[25]</ref><ref type=""bibr"" target=""#b25"">[26]</ref> proposed a bottleneck (BN) deep neural network with a narrow interm",0
"rch topics <ref type=""bibr"" target=""#b20"">[21]</ref><ref type=""bibr"" target=""#b21"">[22]</ref><ref type=""bibr"" target=""#b22"">[23]</ref>. In References <ref type=""bibr"" target=""#b23"">[24]</ref><ref type=""bibr"" target=""#b24"">[25]</ref><ref type=""bibr"" target=""#b25"">[26]</ref> proposed a bottleneck (BN)",0
"o-end model in the CTC framework or the recently proposed low frame rate and chain model, which are based on coarse-grained modelling unit technology <ref type=""bibr"" target=""#b18"">[19,</ref><ref type=""bibr"" target=""#b19"">20]</ref>, has enabled progress to be made in recognition performance and has",0
"tures in a supervised or unsupervised method, thereby improving the accuracy of classification or prediction <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b16"">17]</ref>. Compared with the traditional shallow model, the deep learning model is more efficient in expression and mod",0
"GPUs with a hardware mechanism that memoizes virtual to physical translations directly in memory. Similar in design to the recently proposed PoM-TLB <ref type=""bibr"" target=""#b48"">(Ryoo et al. 2017)</ref>, we propose a TLB in GPU DRAM (DRAM-TLB) that serves as the next larger TLB in the processor T -TLB architected using stacked memory is referred to as a Stacked-TLB while a DRAM-TLB architected using system memory is referred to as a SysMem-TLB <ref type=""bibr"" target=""#b48"">(Ryoo et al. 2017)</ref>. DRAM-TLBs are physically placed in a large contiguous segment of memory and are entirely hard idth and latency for the multiple accesses, we leverage recent work that uses a simple predictor to learn the page size for any given virtual address <ref type=""bibr"" target=""#b48"">(Ryoo et al. 2017)</ref>. In this work, the authors propose a 512-entry table of 2-bit saturating counters indexed by t no software overhead (e.g., expensive traps). DRAM-TLBs also resemble recent CPU-centric work that extends TLB reach of CPUs (referred to as PoM-TLB) <ref type=""bibr"" target=""#b48"">(Ryoo et al. 2017)</ref>. Our work explores the DRAM-TLB design space on GPUs when architected in CPU memory space as p",1
"tures. Recent proposals extend the processor cache hierarchy by architecting stacked memory as a DRAM cache <ref type=""bibr"">(Chou et al. 2015b;</ref><ref type=""bibr"" target=""#b46"">Qureshi and Loh 2012;</ref><ref type=""bibr"" target=""#b27"">Jevdjic et al. 2014;</ref><ref type=""bibr"" target=""#b35"">Loh",0
"bibr"">Chou et al. 2015a</ref>). NUMA aware placement on the other hand focuses on data placement near computing resources to minimize overall latency <ref type=""bibr"" target=""#b15"">(Dashti et al. 2013;</ref><ref type=""bibr"" target=""#b55"">Verghese et al. 1996;</ref><ref type=""bibr"" target=""#b8"">Bolos",0
"m where GPU LLT misses are handled by the CPU IOMMU using Address Translation Services (ATS) <ref type=""bibr"" target=""#b56"">(Vesely et al. 2016;</ref><ref type=""bibr"" target=""#b1"">ATS 2009)</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.1"">System Configuration</head><p>We use a",0
"performance of virtual memory translation using large pages to help TLB coverage. However, large pages can create unintended OS performance overheads <ref type=""bibr"" target=""#b53"">(Talluri and Hill 1994;</ref><ref type=""bibr"" target=""#b54"">Talluri et al. 1992</ref>) due to memory imbalance <ref typ als using larger page sizes. Note that while large page sizes improve TLB coverage, unrestricted use of them can create various performance overheads <ref type=""bibr"" target=""#b53"">(Talluri and Hill 1994;</ref><ref type=""bibr"" target=""#b54"">Talluri et al. 1992;</ref><ref type=""bibr"" target=""#b18"">Ga can significantly improve TLB coverage. However, recent work has shown that unrestricted use of large pages creates unintended performance overheads <ref type=""bibr"" target=""#b53"">(Talluri and Hill 1994;</ref><ref type=""bibr"" target=""#b54"">Talluri et al. 1992;</ref><ref type=""bibr"" target=""#b18"">Ga",0
"p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>Alternate Page Table Implementation.</head><p>To reduce memory accesses, Inverted Page Tables <ref type=""bibr"" target=""#b22"">(Jacob and Mudge 1998;</ref><ref type=""bibr"">Rashid et al. 1987</ref>) by indexing the page table using a hash of the v on the frequency of hash collisions. Inverted page tables also do not support mapping of two different virtual addresses to the same physical address <ref type=""bibr"" target=""#b22"">(Jacob and Mudge 1998)</ref>. DRAM-TLBs provide benefits of inverted page tables (i.e., address translation in a single",0
"an 8MB cache with 32B cache lines). Similarly, the unified LLT contains 512-1024 TLB entries <ref type=""bibr"" target=""#b52"">(Sodani et al. 2016;</ref><ref type=""bibr"" target=""#b21"">Intel 2009)</ref>. The LLT and LLC are cache structures with a tag array and a data array. The LLT tag array stores vir",0
"d n=""1"">INTRODUCTION</head><p>Heterogeneous systems composed of latency optimized cores (e.g., CPUs) and throughput optimized cores (e.g., GPUs, MIC) <ref type=""bibr"" target=""#b16"">(Duran and Klemm 2012)</ref> are becoming the defacto organization of future high-performance computing platforms. Such",0
"a load memory address, followed by a data cache access, to generate a speculative value that does not necessarily exhibit value locality (e.g., DLVP <ref type=""bibr"" target=""#b2"">[3]</ref>). While value predictors can generate speculative results for all instruction types, recent work has shown tha ative results for all instruction types, recent work has shown that load-only predictors are most efficient with a modest hardware budget (e.g., 8KB) <ref type=""bibr"" target=""#b2"">[3]</ref>, <ref type=""bibr"" target=""#b3"">[4]</ref>.</p><p>In this study, we investigated techniques to increase the effe ware Context Value Prediction (CVP) <ref type=""bibr"" target=""#b6"">[7]</ref>, <ref type=""bibr"" target=""#b7"">[8]</ref> Context Address Prediction (CAP) <ref type=""bibr"" target=""#b2"">[3]</ref> one another. We found that no individual predictor is strictly better than another since they all target loads tion type can be predicted, though in this paper we focus only on predicting load values since that is most effective with limited hardware resources <ref type=""bibr"" target=""#b2"">[3]</ref>, <ref type=""bibr"" target=""#b3"">[4]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>A. Value Pr target=""#foot_1"">2</ref> VPE provides the mechanism needed to communicate the predicted values from the value-predicted producers to their consumers <ref type=""bibr"" target=""#b2"">[3]</ref>. Consumers of the load can use the prediction by reading the stored value out of the VPE rather than waiting o ediction approaches and the recent advances towards practical implementations of value prediction, we encourage the readers to visit prior art papers <ref type=""bibr"" target=""#b2"">[3]</ref>, <ref type=""bibr"" target=""#b6"">[7]</ref>, <ref type=""bibr"" target=""#b7"">[8]</ref>.</p><p>Regarding memory cons results; this is caused by different assumptions about the baseline ISA, microarchitecture, and storage constraints (Sheikh reports similar findings <ref type=""bibr"" target=""#b2"">[3]</ref>, <ref type=""bibr"" target=""#b26"">[27]</ref>).</p><p>In all of the studied predictors, we use forward probabilis "" target=""#b6"">[7]</ref>, <ref type=""bibr"" target=""#b7"">[8]</ref>, and subsequent work confirmed the same is true for load instructions in particular <ref type=""bibr"" target=""#b2"">[3]</ref>, <ref type=""bibr"" target=""#b3"">[4]</ref>.</p><p>Our implementation of CVP is similar to VTAGE value predictor but uses the data cache as a value store rather than directly generating values from We use the state-of-the-art DLVP predictor as a reference design <ref type=""bibr"" target=""#b2"">[3]</ref>. The predictor consists of one tagged table indexed by a hash of PC and load path history. An entry contains a",1
"[13]</ref>, browser benchmark <ref type=""bibr"" target=""#b13"">[14]</ref>, and various Javascript benchmarks <ref type=""bibr"" target=""#b14"">[15]</ref>, <ref type=""bibr"" target=""#b15"">[16]</ref>, <ref type=""bibr"" target=""#b16"">[17]</ref>, <ref type=""bibr"" target=""#b17"">[18]</ref>, <ref type=""bibr"" targ",0
"/> 				</application> 			</appInfo> 		</encodingDesc> 		<profileDesc> 			<abstract> <div xmlns=""http://www.tei-c.org/ns/1.0""><p>Value prediction [1], <ref type=""bibr"" target=""#b1"">[2]</ref> has the potential to break through the performance limitations imposed by true data dependencies. Aggressive v",0
"Value prediction can violate this rule. To avoid violating ARM's memory consistency model, we employ a technique similar to the work of Martin et al. <ref type=""bibr"" target=""#b25"">[26]</ref>. Also, address/value prediction is not used with memory ordering instructions, atomic and exclusive memory a",0
"ncy, ARM's relaxed consistency model allows for reordering most memory operations with one exception: dependent loads are not allowed to be reordered <ref type=""bibr"" target=""#b23"">[24]</ref>, <ref type=""bibr"" target=""#b24"">[25]</ref>. Value prediction can violate this rule. To avoid violating ARM's",0
"is phenomenon has received particular attention in the context of deep neural networks, and there is now a quickly growing body of work on this topic <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" target=""#b17""> perceptual similarity between images. For instance, the ∞ -ball around x has recently been studied as a natural notion for adversarial perturbations <ref type=""bibr"" target=""#b10"">[11]</ref>. While we focus on robustness against ∞ -bounded attacks in this paper, we remark that more comprehensive no problem (2.1) gives answers to both these questions. On the attack side, prior work has proposed methods such as the Fast Gradient Sign Method (FGSM) <ref type=""bibr"" target=""#b10"">[11]</ref> and multiple variations of it <ref type=""bibr"" target=""#b17"">[18]</ref>. FGSM is an attack for an ∞ -bounded r"" target=""#b9"">10]</ref> (see <ref type=""bibr"" target=""#b2"">[3]</ref> for an overview of earlier work).</p><p>Adversarial training was introduced in <ref type=""bibr"" target=""#b10"">[11]</ref>, however the adversary utilized was quite weak-it relied on linearizing the loss around the data points. As lity</head><p>A lot of recent literature on adversarial training discusses the phenomenon of transferability <ref type=""bibr"" target=""#b27"">[28,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b28"">29]</ref>-adversarial examples transfer between independently trained networks.",1
"e=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b28"">29]</ref>. Computer vision presents a particularly striking challenge: very small changes to the input image can fool 1 on adversarial training discusses the phenomenon of transferability <ref type=""bibr"" target=""#b27"">[28,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b28"">29]</ref>-adversarial examples transfer between independently trained networks. This raises concerns for practical appl a xml:id=""formula_2"">x t+1 = Π x+S x t + α sgn(∇ x L(θ, x, y)) .</formula><p>Other methods like FGSM with random perturbation have also been proposed <ref type=""bibr"" target=""#b28"">[29]</ref>. Clearly, all of these approaches can be viewed as specific attempts to solve the inner maximization problem FGSM. While training against FGSM adversaries has shown some successes, recent work also highlights important shortcomings of this one-step approach <ref type=""bibr"" target=""#b28"">[29]</ref>-slightly more sophisticated adversaries can still find points of high loss.</p><p>To understand the inner pr the distribution of maxima suggests that the recently developed subspace view of adversarial examples is not fully capturing the richness of attacks <ref type=""bibr"" target=""#b28"">[29]</ref>. In particular, we observe adversarial perturbations with negative inner product with the gradient of the ex on natural examples, denoted A nat .</p><p>• Black-box attacks from a different convolution architecture, denoted B, described in Tramer et al. 2017 <ref type=""bibr"" target=""#b28"">[29]</ref>.</p><p>MNIST. We run 40 iterations of projected gradient descent as our adversary, with a step size of 0.01 y that is allowed to perturb each pixel by more than 0.5 can construct a uniformly gray image, thus fooling any classifier.</p><p>A more recent paper <ref type=""bibr"" target=""#b28"">[29]</ref> also explores the transferability phenomenon. This exploration focuses mostly on the region around natural e e allowed, this region does not give a complete picture of the adversarial landscape. This is confirmed by our experiments, as well as pointed out by <ref type=""bibr"" target=""#b28"">[29]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""7"">Conclusion</head><p>Our findings provide evi d for the attack are: the network itself (A) (white-box attack), an indepentenly initialized and trained copy of the network (A'), architecture B from<ref type=""bibr"" target=""#b28"">[29]</ref> (B).</note></figure> <figure xmlns=""http://www.tei-c.org/ns/1.0"" type=""table"" xml:id=""tab_2""><head>Table 2 : d, we list the accuracy of the resulting network on the full CIFAR10 evaluation set of 10,000 examples. The FGSM random method is the one suggested by<ref type=""bibr"" target=""#b28"">[29]</ref>, whereby we first do a small random perturbation of the natural example, and the apply FGSM to that.</figDes",0
"constructed with ε = 0.3. The results are in Figure <ref type=""figure"" target=""#fig_5"">4</ref>.</p><p>For the CIFAR10 dataset, we used a ResNet model <ref type=""bibr"" target=""#b12"">[13]</ref>. We performed data augmentation using random crops and flips, as well as per image standarization. To increa",0
"papers here. Before we compare our contributions, we remark that robust optimization has been studied outside deep learning for multiple decades (see <ref type=""bibr"" target=""#b0"">[1]</ref> for an overview of this field). We also want to note that the study of adversarial ML predates the widespread",0
"-layer pipelining) results in substantial challenges in resource utilization and on-chip buffer requirements <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b39"">40]</ref>.</p><p>To overcome these challenges, we present Tangram, a scalable tiled accelerator with novel dataflow opt nd leveraged ReRAM crossbars for in-situ analog dot-product operations <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" target=""#b39"">40]</ref>.</p><p>A notable technique to further improve NN efficiency is to exploit sparsity in NN workloads in order t rated ofmaps; in FC layers, the errors are multiplied with the output vectors. They can be formulated as CONV and FC layers with different dimensions <ref type=""bibr"" target=""#b39"">[40,</ref><ref type=""bibr"" target=""#b43"">44]</ref>. Because the output of each layer is needed for backward propagation d similarly to NN training. The error backward propagation of CONV and FC layers can be formulated as new CONV or FC layers with different dimensions <ref type=""bibr"" target=""#b39"">[40,</ref><ref type=""bibr"" target=""#b43"">44]</ref>. The BSD optimization in Section 3.1 can be applied to better parall <ref type=""bibr"" target=""#b13"">[14]</ref>.</p><p>The data forwarding needed for inter-layer pipelining is already available in tiled NN architectures <ref type=""bibr"" target=""#b39"">[40,</ref><ref type=""bibr"" target=""#b43"">44]</ref>. In Tangram, ALLO forwards the intermediate data in a more fine-grai cture with multiple NN engines <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b39"">40,</ref><ref type=""bibr"" target=""#b43"">44]</ref>. As shown in Figure <ref type=""figure"">4</ref>, engines communicate t =""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" target=""#b37"">38,</ref><ref type=""bibr"" target=""#b39"">40,</ref><ref type=""bibr"" target=""#b43"">44]</ref>. Such inter-layer pipelining is effective in increasing the hardware ficient hardware resources, so that the entire NN (all layers) can be mapped onto a single or multiple chips <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b39"">40,</ref><ref type=""bibr"" target=""#b43"">44]</ref>. This approach does not scale to large NNs with hundreds of layers.</ otation in order to optimally migrate data between tiles.</p><p>Inter-layer pipelining: ISAAC <ref type=""bibr"" target=""#b35"">[36]</ref> and PipeLayer <ref type=""bibr"" target=""#b39"">[40]</ref> used inter-layer pipelining in ReRAM-based accelerators, but did not consider dataflow optimizations. The fu",1
"/ref>, or statically compressed the NN structures into sparse formats <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b46"">47,</ref><ref type=""bibr"" target=""#b48"">49]</ref>. While Tangram focuses on dense NNs, its insights should be useful fo",0
"the local buffer and do not need to be accessed remotely. Hence the buffers operate like an optimal NUCA cache <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b15"">16]</ref>. Data rotation happens between neighbor engines, which minimizes the number of NoC hops.</p></div> <div xmlns modeled in our baseline system. Our BSD proposal shares similar insights with non-uniform cache access (NUCA) <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b15"">16]</ref>. It leverages application-specific knowledge of NNs to statically schedule computation skew and data rotation",0
"ap loop.</p><p>Alternatively, we can use multiple engines to process multiple layers in a pipelined manner by spatially mapping the NN DAG structures <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" target=""#b37""> rates data rotation and computation skew according to the loop transformation model in Figure <ref type=""figure"" target=""#fig_4"">7</ref> and Equation <ref type=""bibr"" target=""#b1"">(2)</ref>. We also need to synchronize the data rotation among the engines to avoid stalls or data overwrites. We levera der dataflow optimizations. The fused-layer CNN accelerator sacrificed programmability to fuse the operations between layers in a fine-grained manner <ref type=""bibr"" target=""#b1"">[2]</ref>. Li et al. implemented an inter-layer pipeline with an optimization for FC layers similar to ALLO <ref type=""b",0
"alues <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b32"">33]</ref>, or statically compressed the NN structures into sparse formats <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b46"">47,</ref><ref type=""bibr"" target=""#b48""",0
"e instructions becomes a key problem.</p><p>Recently, a lot of works <ref type=""bibr"" target=""#b9"">[10]</ref><ref type=""bibr"" target=""#b10"">[11]</ref><ref type=""bibr"" target=""#b11"">[12]</ref><ref type=""bibr"" target=""#b12"">[13]</ref><ref type=""bibr"" target=""#b13"">[14]</ref><ref type=""bibr"" target=""#b ults. Although the quantity of fault injections is reduced, the statistical fault injection (SFI) experiments are still time-consuming. SmartInjector <ref type=""bibr"" target=""#b11"">[12]</ref> proposes an intelligent fault injection framework to identify the SDC-prone instructions. It firstly decreas to be equivalent faults and fall into one group. Only one representative fault is selected to implement fault injection for each group. SmartInjector <ref type=""bibr"" target=""#b11"">[12]</ref> proposes an intelligent fault injection framework to identify the SDC-prone instructions. It firstly decreas",1
"r"" target=""#b11"">[12]</ref><ref type=""bibr"" target=""#b12"">[13]</ref><ref type=""bibr"" target=""#b13"">[14]</ref><ref type=""bibr"" target=""#b14"">[15]</ref><ref type=""bibr"" target=""#b15"">[16]</ref> try to improve the static injection framework. CriticalFault <ref type=""bibr"" target=""#b9"">[10]</ref> applie "" target=""#b13"">[14]</ref> identifies SDC-prone instructions using symbolic execution, which enumerates all potential hardware errors.</p><p>The work <ref type=""bibr"" target=""#b15"">[16]</ref> presents a selective protection technique that allows users to selectively protect these SDC-prone data. The rget=""#b15"">[16]</ref> presents a selective protection technique that allows users to selectively protect these SDC-prone data. The main idea of work <ref type=""bibr"" target=""#b15"">[16]</ref> is predicting the SDC proneness of a program's data firstly, then selectively protects the most SDC-prone in e. In step 2, the most vulnerable blocks of the input program are selected by GA.</p><p>Step 3 strengthens the identified vulnerable blocks. The work <ref type=""bibr"" target=""#b15"">[16]</ref> introduces a prediction model named SDCAuto to predict the SDC proneness of a program's instructions. SDCAut of instructions selective protection against SDC-causing errors. Fig. <ref type=""figure"" target=""#fig_1"">2</ref> illustrates the diagram of the work <ref type=""bibr"" target=""#b15"">[16]</ref>. The work <ref type=""bibr"" target=""#b15"">[16]</ref> first compiles the source code into LLVM IR, and extract using errors. Fig. <ref type=""figure"" target=""#fig_1"">2</ref> illustrates the diagram of the work <ref type=""bibr"" target=""#b15"">[16]</ref>. The work <ref type=""bibr"" target=""#b15"">[16]</ref> first compiles the source code into LLVM IR, and extracts instruction features based on LLVM IR file. Then, Faults in the instruction opcode are also not considered, since it always causes illegal opcode exception rather than SDC.</p><p>Finally, as in work <ref type=""bibr"" target=""#b15"">[16]</ref>, we assume that at most one fault occurs during a program's execution.</p></div> <div xmlns=""http://www.tei- e according the results calculated. We also compare our results with the work <ref type=""bibr"" target=""#b14"">[15]</ref> and SDCAuto presented in work <ref type=""bibr"" target=""#b15"">[16]</ref>. We use our approach to maximize SDC coverage under the user-specified performance overhead.</p><p>By comput /></figure> <figure xmlns=""http://www.tei-c.org/ns/1.0"" xml:id=""fig_1""><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Block diagram of the work<ref type=""bibr"" target=""#b15"">[16]</ref> </figDesc><graphic url=""image-2.png"" coords=""4,99.47,451.07,380.15,79.20"" type=""bitmap"" /></figure> <figure",1
"<p>Recently, a lot of works <ref type=""bibr"" target=""#b9"">[10]</ref><ref type=""bibr"" target=""#b10"">[11]</ref><ref type=""bibr"" target=""#b11"">[12]</ref><ref type=""bibr"" target=""#b12"">[13]</ref><ref type=""bibr"" target=""#b13"">[14]</ref><ref type=""bibr"" target=""#b14"">[15]</ref><ref type=""bibr"" target=""#b ting the outcomes of faults, and then reduces the time for a single fault simulation by predicting the fault outcome prediction technique. Shoestring <ref type=""bibr"" target=""#b12"">[13]</ref> leverages compiler to analyze and identify the statistically vulnerable instructions. The time cost of Shoes utcome prediction technique.</p><p>Instructions vulnerability analysis is another important solution to identify SDC-causing instructions. Shoestring <ref type=""bibr"" target=""#b12"">[13]</ref> uses a static compiler analysis technology to identify SDC-causing instructions, and protect them by inserti",0
"ssor design trends towards smaller transistor size, lower core voltage and higher frequency, the threat of soft errors becomes more and more serious. <ref type=""bibr"" target=""#b0"">[1]</ref>. Soft errors could lead to silent data corruption (SDC) which are difficult to be detected. When SDCs occur, t",0
"igh-quality training samples, we create training data set with the help of faults injection experiments. We use the famous fault injection tool PINFI <ref type=""bibr"" target=""#b18"">[19]</ref> to implement fault injection experiment. PINFI is built with Intel Pin <ref type=""bibr"" target=""#b19"">[20]</",0
"become to be the top issue of such techniques. To reduce performance overheads, recent works tend to protect these SDC-prone instructions selectively <ref type=""bibr"" target=""#b4"">[5]</ref><ref type=""bibr"" target=""#b5"">[6]</ref><ref type=""bibr"" target=""#b6"">[7]</ref>.</p><p>It was recently reported",0
"br"" target=""#b9"">[10]</ref><ref type=""bibr"" target=""#b10"">[11]</ref><ref type=""bibr"" target=""#b11"">[12]</ref><ref type=""bibr"" target=""#b12"">[13]</ref><ref type=""bibr"" target=""#b13"">[14]</ref><ref type=""bibr"" target=""#b14"">[15]</ref><ref type=""bibr"" target=""#b15"">[16]</ref> try to improve the static wever, the compiler analysis technology is static and lack of dynamic analysis of program instructions, resulting in lower error coverage. SymPLIFIED <ref type=""bibr"" target=""#b13"">[14]</ref> identifies SDC-prone instructions using symbolic execution, which enumerates all potential hardware errors.< e instructions which are possible to lead to SDC errors are left unprotected. Although Shoestring is time-saving, the SDC coverage is low. SymPLIFIED <ref type=""bibr"" target=""#b13"">[14]</ref> identifies SDC-prone instructions using symbolic execution, which enumerates all potential hardware errors.",0
"ibr"" target=""#b1"">[2]</ref>. For example, it is estimated that up to 82% Americans take one or more drugs, and 29% take more than four drugs together <ref type=""bibr"" target=""#b2"">[3]</ref>. Consider all drugs have a small chance of side effects, taking different medications combined inevitably incr l. proposed to discover drugdrug interactions and DDAAs from MedHelp.org <ref type=""bibr"" target=""#b22"">[23]</ref>, a popular online health community <ref type=""bibr"" target=""#b2"">[3]</ref>. They first built a heterogeneous healthcare network comprising entities (e.g., drugs, ADRs) extracted from th nel for people to share experiences and seek for help online. A recent survey shows that 72% of Internet users went online to seek health information <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b20"">21]</ref>. Several attempts have been made to detect DDAAs from social media. Fo",1
"r"" target=""#b11"">[12]</ref><ref type=""bibr"" target=""#b12"">[13]</ref><ref type=""bibr"" target=""#b13"">[14]</ref><ref type=""bibr"" target=""#b14"">[15]</ref><ref type=""bibr"" target=""#b15"">[16]</ref><ref type=""bibr"" target=""#b16"">[17]</ref>. However, post-marketing ADR identification of combined medication",0
"ere two or more drugs are taken together or concomitantly <ref type=""bibr"" target=""#b0"">[1]</ref>. It is very common in therapy and clinical practice <ref type=""bibr"" target=""#b1"">[2]</ref>. For example, it is estimated that up to 82% Americans take one or more drugs, and 29% take more than four dru pplied association rule mining to detect ADRs of multi-drugs from 162,744 FAERS (Food and Drug Administration Adverse Event Reporting System) reports <ref type=""bibr"" target=""#b1"">[2]</ref>. They successfully obtained 1,167 multi-drug ADRs associations, demonstrating the feasibility to detect ADRs o",0
"/1.0""><head>Methods</head></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>Data resources</head><p>In this work, we focus on drugs from DrugBank <ref type=""bibr"" target=""#b25"">[26]</ref>, a comprehensive drug database. The drug chemical structures, drug target proteins, and drug substituents ar",0
"ing an appropriate drug representation to establish the associations between drugs and ADRs. Second, pharmacologic databases, like Twosides databases <ref type=""bibr"" target=""#b23"">[24]</ref>, define a set of positive samples, namely known drug-drug-ADR associations included in the database. However",0
"hod for a special type of graphs (i.e., knowledge graph).</p><p>Our method also connects to PinSage <ref type=""bibr"" target=""#b20"">[21]</ref> and GAT <ref type=""bibr"" target=""#b14"">[15]</ref>. But note that both PinSage and GAT are designed for homogeneous graphs. The major difference between our wo дд concat = σ W • concat(v, v u S(v) ) + b .</formula><p>(5) 2 The knowledge graph G is treated undirected. 3 Technically, S(v) • Neighbor aggregator <ref type=""bibr"" target=""#b14"">[15]</ref> directly takes the neighborhood representation of entity v as the output representation:</p><formula xml:id=",1
".</p><p>Our work can be seen as a non-spectral method for a special type of graphs (i.e., knowledge graph).</p><p>Our method also connects to PinSage <ref type=""bibr"" target=""#b20"">[21]</ref> and GAT <ref type=""bibr"" target=""#b14"">[15]</ref>. But note that both PinSage and GAT are designed for homog",1
"acting locally connected regions from graphs <ref type=""bibr"" target=""#b12"">[13]</ref>, or sampling a fixed-size set of neighbors as the support size <ref type=""bibr"" target=""#b6"">[7]</ref>.</p><p>Our work can be seen as a non-spectral method for a special type of graphs (i.e., knowledge graph).</p> 4)</label></formula><p>where W and b are transformation weight and bias, respectively, and σ is the nonlinear function such ReLU. • Concat aggregator <ref type=""bibr"" target=""#b6"">[7]</ref> concatenates the two representation vectors first before applying nonlinear transformation:</p><formula xml:id",0
". However, commonly-used KGE methods focus on modeling rigorous semantic relatedness (e.g., TransE <ref type=""bibr"" target=""#b0"">[1]</ref> and TransR <ref type=""bibr"" target=""#b11"">[12]</ref> assume head + relation = tail), which are more suitable for in-graph applications such as KG completion and",0
"ns. Compared with KG-free methods, incorporating KG into recommendation benefits the results in three ways <ref type=""bibr"" target=""#b17"">[18]</ref>: <ref type=""bibr"" target=""#b0"">(1)</ref> The rich semantic relatedness among items in a KG can help explore their latent connections and improve the pr own to consistently outperform state-of-the-art baselines in movie, book, and music recommendation.</p><p>We point out three avenues for future work. <ref type=""bibr"" target=""#b0"">(1)</ref> In this work we uniformly sample from the neighbors of an entity to construct its receptive field. Exploring a "">19,</ref><ref type=""bibr"" target=""#b22"">23]</ref>. However, commonly-used KGE methods focus on modeling rigorous semantic relatedness (e.g., TransE <ref type=""bibr"" target=""#b0"">[1]</ref> and TransR <ref type=""bibr"" target=""#b11"">[12]</ref> assume head + relation = tail), which are more suitable f d model using inner product to model user-item interactions.   • LibFM + TransE extends LibFM by attaching an entity representation learned by TransE <ref type=""bibr"" target=""#b0"">[1]</ref> to each user-item pair.</p><p>• PER <ref type=""bibr"" target=""#b21"">[22]</ref> treats the KG as heterogeneous i",0
"on <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b16"">17]</ref>.</p><p>A few recent studies <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b21"">[22]</ref><ref type=""bibr"" target=""#b22 attributes) and edges correspond to relations. Compared with KG-free methods, incorporating KG into recommendation benefits the results in three ways <ref type=""bibr"" target=""#b17"">[18]</ref>: <ref type=""bibr"" target=""#b0"">(1)</ref> The rich semantic relatedness among items in a KG can help explore paths/graphs. However, PER and FMG rely heavily on manually designed meta-paths or meta-graphs, which are hardly to be optimal in reality. RippleNet <ref type=""bibr"" target=""#b17"">[18]</ref> is a memory-network-like model that propagates users' potential preferences in the KG and explores their hie and visual knowledge in a unified framework for recommendation. We implement CKE as CF plus a structural knowledge module in this paper. • RippleNet <ref type=""bibr"" target=""#b17"">[18]</ref> is a memory-network-like approach that propagates users' preferences on the KG for recommendation.</p></div> and link prediction rather than recommendation. A more natural and intuitive way is to design a graph algorithm directly to exploit the KG structure <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b23"">24]</ref>. For example, PER <ref type=",0
"butes of users and items are used to compensate for the sparsity and improve the performance of recommendation <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b16"">17]</ref>.</p><p>A few recent studies <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b17"">18,</ref><r",0
"he neighborhoods with varying size and maintain the weight sharing property of CNN, researchers propose learning a weight matrix for each node degree <ref type=""bibr"" target=""#b5"">[6]</ref>, extracting locally connected regions from graphs <ref type=""bibr"" target=""#b12"">[13]</ref>, or sampling a fix",0
"chers propose learning a weight matrix for each node degree <ref type=""bibr"" target=""#b5"">[6]</ref>, extracting locally connected regions from graphs <ref type=""bibr"" target=""#b12"">[13]</ref>, or sampling a fixed-size set of neighbors as the support size <ref type=""bibr"" target=""#b6"">[7]</ref>.</p><",0
"improve the performance of recommendation <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b16"">17]</ref>.</p><p>A few recent studies <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b21""> e graph embedding (KGE) methods <ref type=""bibr"" target=""#b19"">[20]</ref>, which map entities and relations to low-dimensional representation vectors <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b22"">23]</ref>. However, commonly-used KGE me",0
"the profile reviser aims to remove the noisy courses with few contributions in a prediction. Inspired by the theory of hierarchical abstract machines <ref type=""bibr"" target=""#b15"">(Parr and Russell 1998)</ref>, we cast the task of profile reviser as a hierarchical Markov Decision Process (MDP). Gen",1
"urther extended to deep neural network models <ref type=""bibr"" target=""#b5"">(He et al. 2017;</ref><ref type=""bibr"" target=""#b5"">Guo et al. 2017;</ref><ref type=""bibr"" target=""#b28"">Zhang, Du, and Wang 2016)</ref>. The user-to-item CF suffers from the sparsity of users' profiles. On the contrary, the",0
"preferences in different ways. For example, when ignoring the order of the historical courses, we can adopt the factored item similarity model (FISM) <ref type=""bibr"" target=""#b11"">(Kabbur, Ning, and Karypis 2013)</ref> to represent each course as an embedding vector and average Figure <ref type=""fi led approach that can easily incorporate any heuristic features. But for fair comparison, we only use the embeddings of users and courses.</p><p>FISM <ref type=""bibr"" target=""#b11"">(Kabbur, Ning, and Karypis 2013)</ref>: is an itemto-item collaborative filtering algorithm which conducts recommendati",0
"rther proposed to distinguish the effects of different items. Several researches are conducted on MOOCs platforms, such as learning behavior analysis <ref type=""bibr"" target=""#b0"">(Anderson et al. 2014;</ref><ref type=""bibr"" target=""#b17"">Qiu et al. 2016;</ref><ref type=""bibr"" target=""#b16"">Qi et al",0
"al reward G(a l t , s l t ) which is used only inside the low-level task to speed up its local learning and does not propagate to the high-level task <ref type=""bibr"" target=""#b4"">(Ghavamzadeh and Mahadevan 2003)</ref>. Specifically, we first calculate the average cosine similarity between each hist",0
"discuss a justification of decoupled weight decay in the framework of Bayesian filtering for a unified theory of adaptive gradient algorithms due to <ref type=""bibr"" target=""#b0"">Aitchison (2018)</ref>. After we posted a preliminary version of our current paper on arXiv, Aitchison noted that his th st of the filtering distribution: updates are larger for parameters we are more uncertain about and smaller for parameters we are more certain about. <ref type=""bibr"" target=""#b0"">Aitchison (2018)</ref> goes on to show that popular adaptive gradient methods, such as Adam and RMSprop, as well as Kron gularization, because it is weight decay, rather than L 2 regularization that emerges through the straightforward application of Bayesian filtering."" <ref type=""bibr"" target=""#b0"">(Aitchison, 2018)</ref>. While full credit for this theory goes to Aitchison, we summarize it here to shed some light on",1
"<text xml:lang=""en""> 		<body> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""1"">INTRODUCTION</head><p>Adaptive gradient methods, such as AdaGrad <ref type=""bibr"" target=""#b4"">(Duchi et al., 2011)</ref>, <ref type=""bibr"">RMSProp (Tieleman &amp; Hinton, 2012)</ref>, Adam <ref type=""bibr"" target="" >. While we focused our experimental analysis on Adam, we believe that similar results also hold for other adaptive gradient methods, such as AdaGrad <ref type=""bibr"" target=""#b4"">(Duchi et al., 2011) and</ref><ref type=""bibr"">AMSGrad (Reddi et al., 2018)</ref>.  <ref type=""bibr"" target=""#b11"">&amp;",0
"hod, we refer the interested reader to <ref type=""bibr"" target=""#b5"">Gastaldi (2017)</ref>. We also perform experiments on the Im-ageNet32x32 dataset <ref type=""bibr"" target=""#b1"">(Chrabaszcz et al., 2017)</ref>, a downsampled version of the original ImageNet dataset with 1.2 million 32×32 pixels im root normalization we proposed in Eq. ( <ref type=""formula"">15</ref>) and double-checked that this is not a coincidence on the ImageNet32x32 dataset <ref type=""bibr"" target=""#b1"">(Chrabaszcz et al., 2017)</ref>, a downsampled version of the original ImageNet dataset with 1.2 million 32×32 pixels im",0
"er test set accuracy on ResNet (72.04% versus 61.34%). <ref type=""bibr"" target=""#b14"">Radford et al. (2018)</ref> employed AdamW to train Transformer <ref type=""bibr"" target=""#b17"">(Vaswani et al., 2017)</ref> architectures to obtain new state-of-the-art results on a wide range of benchmarks for nat",0
"batch iteration t and is thus not constrained to integer values. Adjusting (e.g., decreasing) η (i) min and η (i) max at every i-th restart (see also <ref type=""bibr"" target=""#b16"">Smith (2016)</ref>) could potentially improve performance, but we do not consider that option here because it would inv",0
"have become a default method of choice for training feed-forward and recurrent neural networks <ref type=""bibr"" target=""#b21"">(Xu et al., 2015;</ref><ref type=""bibr"" target=""#b13"">Radford et al., 2015)</ref>. Nevertheless, state-of-the-art results for popular image classification datasets, such as",0
"uency parsing. Different hypotheses about the origins of this worse generalization have been investigated, such as the presence of sharp local minima <ref type=""bibr"" target=""#b8"">(Keskar et al., 2016;</ref><ref type=""bibr"" target=""#b3"">Dinh et al., 2017)</ref> and inherent problems of adaptive grad",0
"graph spectral domain. Particularly, by applying Chebyshev polynomials to the graph Laplacian, spatially localized filtering is obtained. Kipf et al. <ref type=""bibr"" target=""#b18"">[18]</ref> approximate the polynomials using a re-normalized first-order adjacency matrix to obtain comparable results "" target=""#b28"">[27]</ref>:</p><formula xml:id=""formula_4"">E ijp = Êijp N j=1 Êijp<label>(5)</label></formula><p>or symmetric normalization as in GCN <ref type=""bibr"" target=""#b18"">[18]</ref>:</p><formula xml:id=""formula_5"">E ijp = Êijp N i=1 Êijp N j=1 Êijp<label>(6)</label></formula><p>Further, th al case of the graph attention operation, we derive our EGNN(C) layer from the formula of EGNN(A) layer. Indeed, the essential difference between GCN <ref type=""bibr"" target=""#b18"">[18]</ref> and GAT <ref type=""bibr"" target=""#b28"">[27]</ref> is whether we use the attention coefficients (i.e., matrix parse"" splitting. Another splitting has 60%, 20% and 20% sized subsets, which is called ""dense"" splitting.</p><p>Following the experiment settings of <ref type=""bibr"" target=""#b18"">[18]</ref>[27], we use two layers of EGNN in all of our experiments for fair comparison. Throughout the experiments in des in a minority class are given larger weights and are penalized more in the loss than a majority class.</p><p>The baseline methods we used are GCN <ref type=""bibr"" target=""#b18"">[18]</ref> and GAT <ref type=""bibr"" target=""#b28"">[27]</ref>. To investigate the effectivenesses of each components, we",1
"e=""bibr"" target=""#b11"">[11]</ref>[18] combine graph node features and graph topological structural information to make predictions. Velickovic et al. <ref type=""bibr"" target=""#b28"">[27]</ref> adopt attention mechanism into graph learning, and propose a graph attention network (GAT). Unlike GCNs, whi ence; further, the weights for aggregation are functions of the contents of a neighborhood or sequence. Therefore, they are adaptive to the contents. <ref type=""bibr"" target=""#b28"">[27]</ref> adapts an attention mechanism to graph learning and proposes a graph attention network (GAT), achieving curr m exploding or shrinking to zero during diffusion, thus can help stabilize the process, compared with the previously used row normalization as in GAT <ref type=""bibr"" target=""#b28"">[27]</ref>:</p><formula xml:id=""formula_4"">E ijp = Êijp N j=1 Êijp<label>(5)</label></formula><p>or symmetric normaliza /www.tei-c.org/ns/1.0""><head n=""3.3."">EGNN(A): Attention based EGNN layer</head><p>We describe the attention based EGNN layer. The original GAT model <ref type=""bibr"" target=""#b28"">[27]</ref> is only able to handle one dimensional binary edge features, i.e., the attention mechanism is defined on the e specific entry α l ijp is a function of</p><formula xml:id=""formula_9"">X l−1 i• , X l−1 j•</formula><p>and E ijp . In existing attention mechanisms <ref type=""bibr"" target=""#b28"">[27]</ref>, the attention coefficient depends on X i• and X j• only. Our mechanism allows the attention operation to be e our EGNN(C) layer from the formula of EGNN(A) layer. Indeed, the essential difference between GCN <ref type=""bibr"" target=""#b18"">[18]</ref> and GAT <ref type=""bibr"" target=""#b28"">[27]</ref> is whether we use the attention coefficients (i.e., matrix α) or the adjacency matrix to aggregate node feat yed to describe the network nodes (i.e., papers). The three citation network datasets are also used in <ref type=""bibr"" target=""#b33"">[32]</ref> [18] <ref type=""bibr"" target=""#b28"">[27]</ref>. However, they all use a pre-processed version which discards the edge directions. Since our EGNN models req nd are penalized more in the loss than a majority class.</p><p>The baseline methods we used are GCN <ref type=""bibr"" target=""#b18"">[18]</ref> and GAT <ref type=""bibr"" target=""#b28"">[27]</ref>. To investigate the effectivenesses of each components, we perform ablation study of EGNN(A) and EGNN(C).</p",1
"et=""#b32"">[31]</ref>: Random Forest and Weave. Random Forest is a traditional learning model which is widely applied to various problems. Weave model <ref type=""bibr"" target=""#b15"">[15]</ref> is similar to graph convolution but specifically designed for molecular analysis.</p><p>All the three datase",0
"proaches are based on matrix-factorization, e.g., Laplacian Eigenmap (LE) <ref type=""bibr"" target=""#b4"">[4]</ref>, Graph Factorization (GF) algorithm <ref type=""bibr"" target=""#b1"">[2]</ref>, GraRep <ref type=""bibr"" target=""#b7"">[7]</ref>, and HOPE <ref type=""bibr"" target=""#b21"">[21]</ref>. Another c employing a flexible, stochastic measure of node similarity based on random walks, e.g., DeepWalk <ref type=""bibr"" target=""#b22"">[22]</ref>, node2vec <ref type=""bibr"" target=""#b1"">[2]</ref>, LINE <ref type=""bibr"" target=""#b27"">[26]</ref>, and HARP <ref type=""bibr"" target=""#b9"">[9]</ref>. There are s",0
"djacency matrix, e.g., Deep Neural Graph Representations (DNGR) <ref type=""bibr"" target=""#b8"">[8]</ref> and Structural Deep Network Embeddings (SDNE) <ref type=""bibr"" target=""#b31"">[30]</ref>. Although autoencoder-based approaches are able to capture more complex patterns than matrix factorization b",0
"ctors of the Laplacian.</p><p>Attention mechanisms have been widely employed in many sequence-based tasks <ref type=""bibr"" target=""#b3"">[3]</ref>[33] <ref type=""bibr"" target=""#b16"">[16]</ref>. Compared with convolution operators, attention mechanisms enjoy two benefits: Firstly, they are able to agg",0
"<ref type=""bibr"" target=""#b22"">[22]</ref>, node2vec <ref type=""bibr"" target=""#b1"">[2]</ref>, LINE <ref type=""bibr"" target=""#b27"">[26]</ref>, and HARP <ref type=""bibr"" target=""#b9"">[9]</ref>. There are several limitations in matrix factorization-based and random walkbased graph learning approaches. F",0
"ex non-Euclidean structure of graph data. To address this challenge, traditional machine learning approaches extract graph statistics (e.g., degrees) <ref type=""bibr"" target=""#b5"">[5]</ref>, kernel functions <ref type=""bibr"" target=""#b29"">[28]</ref> <ref type=""bibr"" target=""#b25"">[24]</ref> or other",0
", the edge features in EGNN are adapted at each layer before being fed to next layer.</p><p>in image recognition, and recurrent neural networks (RNN) <ref type=""bibr"" target=""#b12"">[12]</ref> and Long Short Term Memory (LSTM) <ref type=""bibr"" target=""#b14"">[14]</ref> in natural language processing.",0
", Laplacian Eigenmap (LE) <ref type=""bibr"" target=""#b4"">[4]</ref>, Graph Factorization (GF) algorithm <ref type=""bibr"" target=""#b1"">[2]</ref>, GraRep <ref type=""bibr"" target=""#b7"">[7]</ref>, and HOPE <ref type=""bibr"" target=""#b21"">[21]</ref>. Another class of approaches focus on employing a flexible",0
"r building extraction using both high-resolution aerial images and LiDAR data. The developed network is based on a modified residual learning network <ref type=""bibr"" target=""#b13"">(He et al., 2016)</ref> that extracts robust low/mid/high-level features from remotely sensed data. A new gated feature re more robust than traditional hand-crafted features in the input images <ref type=""bibr"" target=""#b50"">(Zhang et al., 2016)</ref>. Previous studies <ref type=""bibr"" target=""#b13"">(He et al., 2016)</ref> have found that increasing the depth of neural networks do not necessarily improve the performa are displayed in different colors in Fig. <ref type=""figure"" target=""#fig_2"">1</ref>).</p><p>A more detailed description of ResNet-50 can be found in <ref type=""bibr"" target=""#b13"">(He et al., 2016)</ref> and here ResNet-50 is modified as follows to improve the model performance. First, a convolutio mprove the performance of CNN. Instead, the accuracy may degrade after a saturation. This phenomenon is often referred to as the degradation problem. <ref type=""bibr"" target=""#b13"">He et al. (2016)</ref> recently proposed a Residual Network (ResNet) that is considerably deeper than previous networks",1
"etween multi-source data is still a key factor affecting the accuracy of building extraction, and it also brings challenges for many existing studies <ref type=""bibr"" target=""#b46"">(Yang and Chen, 2015)</ref>. In the DataPlus dataset, we found that the ground truth annotations are more consistent wi",0
"tation and building vectorization and advanced applications like three-dimensional city modeling, urban expansion analysis, and environment surveying <ref type=""bibr"" target=""#b16"">(Huang and Zhang, 2011)</ref>. Developing automatic and robust algorithms of building extraction is therefore a researc",0
"f>, DeconvNet <ref type=""bibr"" target=""#b31"">(Noh et al., 2015)</ref>, CNN-FPL <ref type=""bibr"" target=""#b40"">(Volpi and Tuia, 2017)</ref>, V-FuseNet <ref type=""bibr"" target=""#b1"">(Audebert et al., 2017)</ref>, and Res-U-Net <ref type=""bibr"" target=""#b44"">(Xu et al., 2018)</ref>, were used for compa",0
"include image-based <ref type=""bibr"" target=""#b9"">(Ghanea et al., 2016;</ref><ref type=""bibr"" target=""#b15"">Huang and Zhang, 2012)</ref>, LiDAR-based <ref type=""bibr"" target=""#b6"">(Du et al., 2017;</ref><ref type=""bibr"" target=""#b29"">Mongus et al., 2014;</ref><ref type=""bibr"" target=""#b30"">Niemeyer",0
"Remarkable works include Forrest, et al., <ref type=""bibr"" target=""#b0"">[1]</ref>, Giffin, et al., <ref type=""bibr"" target=""#b22"">[23]</ref>, Spivey <ref type=""bibr"" target=""#b23"">[24]</ref>, Bond and McKinley <ref type=""bibr"" target=""#b24"">[25]</ref>. Forrest firstly proposed distinguishing progra",1
"ata flow in the program, subverting machine-code execution. For example, <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b10"">11,</",0
"type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b11"">12]</ref>. In general, the methods of software behavior were two patterns. Both of them are supported by reverse techno",0
"ng of software trajectory must match needs of practicability. The research on system call is indispensable. Remarkable works include Forrest, et al., <ref type=""bibr"" target=""#b0"">[1]</ref>, Giffin, et al., <ref type=""bibr"" target=""#b22"">[23]</ref>, Spivey <ref type=""bibr"" target=""#b23"">[24]</ref>,",0
"technology and hardware technology <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b12"">13]</ref> .</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3."">Anal",0
"target=""#b41"">(Seo et al., 2017)</ref> while some other approaches utilized LSTM <ref type=""bibr"" target=""#b0"">(Almahairi et al., 2015)</ref> or RNN <ref type=""bibr"" target=""#b1"">(Bansal et al., 2016)</ref>. To the best of authors' knowledge, the manner in which our approach learns and represents p",0
"(Figure 1,</ref><ref type=""bibr"">left)</ref>.</p><p>Existing literature on training with noisy labels focuses primarily on loss correction approaches <ref type=""bibr"" target=""#b22"">(Reed et al., 2015;</ref><ref type=""bibr"" target=""#b9"">Hendrycks et al., 2018;</ref><ref type=""bibr"" target=""#b11"">Jian at it is possible to do unsupervised sample relabeling using the network predictions to predict hard or soft labels.</p><p>Loss correction approaches <ref type=""bibr"" target=""#b22"">(Reed et al., 2015;</ref><ref type=""bibr"" target=""#b11"">Jiang et al., 2018b;</ref><ref type=""bibr"">Patrini et al., 2017 target=""#b9"">Hendrycks et al., 2018;</ref><ref type=""bibr"" target=""#b11"">Jiang et al., 2018b)</ref>. A well-known approach is the bootstrapping loss <ref type=""bibr"" target=""#b22"">(Reed et al., 2015)</ref>, which introduces a perceptual consistency term in the learning objective that assigns a weig ef> modify either the loss directly, or the probabilities used to compute it, to compensate for the incorrect guidance provided by the noisy samples. <ref type=""bibr"" target=""#b22"">(Reed et al., 2015)</ref> extend the loss with a perceptual term that introduces a certain reliance on the model predic sample to model clean and noisy samples. We use this unsupervised model to implement a loss correction approach that benefits both from bootstrapping <ref type=""bibr"" target=""#b22"">(Reed et al., 2015)</ref> and mixup data augmentation <ref type=""bibr"" target=""#b34"">(Zhang et al., 2018)</ref> to deal as it encourages fitting label noise <ref type=""bibr"" target=""#b33"">(Zhang et al., 2017)</ref>.</p><p>The static hard bootstrapping loss proposed in <ref type=""bibr"" target=""#b22"">(Reed et al., 2015)</ref> provides a mechanism to deal with label noise by adding a perceptual term to the standard cro = − N i=1 ((1 − w i ) y i + w i z i ) T log (h i ) ,<label>(10)</label></formula><p>where w i weights the model prediction z i in the loss function. <ref type=""bibr"" target=""#b22"">(Reed et al., 2015)</ref> use w i = 0.2, ∀i. We refer to this approach as static hard bootstrapping. <ref type=""bibr"" t he loss function. <ref type=""bibr"" target=""#b22"">(Reed et al., 2015)</ref> use w i = 0.2, ∀i. We refer to this approach as static hard bootstrapping. <ref type=""bibr"" target=""#b22"">(Reed et al., 2015)</ref> also proposed a static soft bootstrapping loss (w i = 0.05, ∀i) that uses the predicted softm ue label is not selected when performing random labeling. We also run our proposed approach under these conditions in Subsection 4.5 for comparison.  <ref type=""bibr"" target=""#b22"">(Reed et al., 2015)</ref>. The overall results demonstrate that applying persample weights (DY) benefits training by al xup and dynamic loss correction</head><p>The proposed dynamic hard bootstrapping exhibits better performance than the state-of-the-art static version <ref type=""bibr"" target=""#b22"">(Reed et al., 2015)</ref>. It is, however, not better than the performance of mixup data augmentation, which exhibits e vels of label noise using a common architecture and the 300 epochs training scheme (see Subsection 4.1) . We introduce bootstrapping in epoch 105 for <ref type=""bibr"" target=""#b22"">(Reed et al., 2015)</ref> for the proposed methods, estimate the T matrix of <ref type=""bibr"">(Patrini et al., 2017)</r",1
"blem by introducing a similarity learning strategy that pulls representations of noisy samples away from clean ones. Finally, mixup data augmentation <ref type=""bibr"" target=""#b34"">(Zhang et al., 2018)</ref> has recently demonstrated outstanding robustness against label noise without explicitly mode preventing overfitting to label noise.</p><p>3. Pushing the state-of-the-art one step forward by combining our approach with mixup data augmentation <ref type=""bibr"" target=""#b34"">(Zhang et al., 2018)</ref>.</p><p>4. Guiding mixup data augmentation to achieve convergence even under extreme label no a loss correction approach that benefits both from bootstrapping <ref type=""bibr"" target=""#b22"">(Reed et al., 2015)</ref> and mixup data augmentation <ref type=""bibr"" target=""#b34"">(Zhang et al., 2018)</ref> to deal with the closed-set label noise scenario.</p></div> <div xmlns=""http://www.tei-c.org erior results.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.3."">Joint label correction and mixup data augmentation</head><p>Recently <ref type=""bibr"" target=""#b34"">(Zhang et al., 2018)</ref> proposed a data augmentation technique named mixup that exhibits strong robustness to label rini et al., 2017)</ref> in epoch 75 (as done in <ref type=""bibr"" target=""#b9"">(Hendrycks et al., 2018)</ref>), and use the configuration reported in <ref type=""bibr"" target=""#b34"">(Zhang et al., 2018)</ref> for mixup. We outperform the related work in the presence of label noise, obtaining remarkab pproach</head><p>Table <ref type=""table"" target=""#tab_8"">6</ref> shows the results of the proposed approaches M-DYR-H and MD-DYR-SH compared to mixup <ref type=""bibr"" target=""#b34"">(Zhang et al., 2018)</ref> on TinyImageNet to demonstrate that our approach is useful far from CIFAR data. The proposed >(Zhang et al., 2018)</ref> on TinyImageNet to demonstrate that our approach is useful far from CIFAR data. The proposed approach clearly outperforms <ref type=""bibr"" target=""#b34"">(Zhang et al., 2018)</ref> for different levels of label noise, obtaining consistent results with the CIFAR experiments pe=""bibr"" target=""#b22"">(Reed et al., 2015;</ref><ref type=""bibr"" target=""#b11"">Jiang et al., 2018b;</ref><ref type=""bibr"">Patrini et al., 2017;</ref><ref type=""bibr"" target=""#b34"">Zhang et al., 2018)</ref> modify either the loss directly, or the probabilities used to compute it, to compensate for t",1
"o deal with many computer vision tasks <ref type=""bibr"" target=""#b4"">(DeTone et al., 2016;</ref><ref type=""bibr"" target=""#b17"">Ono et al., 2018;</ref><ref type=""bibr"" target=""#b1"">Beluch et al., 2018;</ref><ref type=""bibr"" target=""#b21"">Redmon et al., 2016;</ref><ref type=""bibr"" target=""#b35"">Zhao e",0
"label, these approaches rely on the assumption that a small set of clean samples is always available, which limits their applicability. Tanaka et al. <ref type=""bibr"" target=""#b25"">(Tanaka et al., 2018)</ref> have, however, recently demonstrated that it is possible to do unsupervised sample relabeli rediction in the bootstrapping loss, encouraging the network to predict the same class to minimize the loss. We apply the regularization term used in <ref type=""bibr"" target=""#b25"">(Tanaka et al., 2018)</ref>, which seeks preventing the assignment of all samples to a single class, to overcome this i taset. Note that we assume a uniform distribution for the prior probabilities (i.e. p c = 1/C), while approximating h c using mini-batches as done in <ref type=""bibr"" target=""#b25"">(Tanaka et al., 2018)</ref>. We add the term ηR to * (Eq. ( <ref type=""formula"" target=""#formula_18"">13</ref>)) with η el noise with label flips concentrated in classes sharing similar visual patterns with the true class. We followed a similar network and procedure as <ref type=""bibr"" target=""#b25"">(Tanaka et al., 2018)</ref> with ImageNet pre-trained weights and ResNet-50, obtaining over 71% test accuracy, which fa al., 2018)</ref> with ImageNet pre-trained weights and ResNet-50, obtaining over 71% test accuracy, which falls short of the state-of-the-art (72.23% <ref type=""bibr"" target=""#b25"">(Tanaka et al., 2018)</ref>). We found that finetuning a pre-trained network for one epoch, as done in <ref type=""bibr"" the-art (72.23% <ref type=""bibr"" target=""#b25"">(Tanaka et al., 2018)</ref>). We found that finetuning a pre-trained network for one epoch, as done in <ref type=""bibr"" target=""#b25"">(Tanaka et al., 2018)</ref>, easily fits label noise limiting our unsupervised label noise model. We believe this occur f type=""bibr"" target=""#b32"">(Xiao et al., 2015)</ref>  We follow <ref type=""bibr"" target=""#b33"">(Zhang et al., 2017;</ref><ref type=""bibr"">2018;</ref><ref type=""bibr"" target=""#b25"">Tanaka et al., 2018)</ref> criterion for label noise addition, which consists of randomly selecting labels for a percen",0
";</ref><ref type=""bibr"" target=""#b12"">Krishna et al., 2017)</ref>. Their widespread use is attributable to their capability to model complex patterns <ref type=""bibr"" target=""#b23"">(Ren et al., 2018)</ref> when vast amounts of labeled data are available. Obtaining such volumes of data, however, is n unsupervised estimation on data complexity through its distribution in a feature space that benefits from training with both clean and noisy samples. <ref type=""bibr"" target=""#b23"">(Ren et al., 2018)</ref> weights each sample in the loss based on the gradient directions in training compared to those different architectures employed and the use of sets of clean data during training in <ref type=""bibr"" target=""#b11"">(Jiang et al., 2018b)</ref> and <ref type=""bibr"" target=""#b23"">(Ren et al., 2018)</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.6."">Generalization of the propo es. Still other approaches use curriculum learning to formulate a robust learning procedure <ref type=""bibr"" target=""#b11"">(Jiang et al., 2018b;</ref><ref type=""bibr"" target=""#b23"">Ren et al., 2018)</ref>. Curriculum learning <ref type=""bibr"" target=""#b2"">(Bengio et al., 2009)</ref> is based on the ons that achieve a high classification accuracy. Contrary to most successful recent approaches that assume the existence of a known set of clean data <ref type=""bibr"" target=""#b23"">(Ren et al., 2018;</ref><ref type=""bibr"" target=""#b9"">Hendrycks et al., 2018)</ref>, we propose an unsupervised model o erion adopted by several other authors <ref type=""bibr"" target=""#b11"">(Jiang et al., 2018b;</ref><ref type=""bibr"" target=""#b14"">Ma et al., 2018;</ref><ref type=""bibr"" target=""#b23"">Ren et al., 2018;</ref><ref type=""bibr"">Wang et al., 2018b)</ref>. We also run our proposed approach under this setup t",0
"ting literature on training with noisy labels focuses primarily on loss correction approaches <ref type=""bibr"" target=""#b22"">(Reed et al., 2015;</ref><ref type=""bibr"" target=""#b9"">Hendrycks et al., 2018;</ref><ref type=""bibr"" target=""#b11"">Jiang et al., 2018b)</ref>. A well-known approach is the boo on to compensate for the erroneous guiding of noisy samples. Other approaches modify class probabilities <ref type=""bibr"">(Patrini et al., 2017;</ref><ref type=""bibr"" target=""#b9"">Hendrycks et al., 2018)</ref> by estimating the noise associated with each class, thus computing a loss that guides the ry to most successful recent approaches that assume the existence of a known set of clean data <ref type=""bibr"" target=""#b23"">(Ren et al., 2018;</ref><ref type=""bibr"" target=""#b9"">Hendrycks et al., 2018)</ref>, we propose an unsupervised model of label noise based exclusively on the loss on each sam with uniform random probability. Non-uniform or class-conditional label noise, on the other hand, has different flipping probabilities for each class <ref type=""bibr"" target=""#b9"">(Hendrycks et al., 2018)</ref>. Previous research <ref type=""bibr"">(Patrini et al., 2017)</ref> suggests that uniform la ward method that, instead of operating directly on the loss, goes back to the predicted probabilities to correct them by multiplying by the T matrix. <ref type=""bibr"" target=""#b9"">(Hendrycks et al., 2018)</ref> corrects the predicted probabilities using a corruption matrix computed using a model tra 2"">(Reed et al., 2015)</ref> for the proposed methods, estimate the T matrix of <ref type=""bibr"">(Patrini et al., 2017)</ref> in epoch 75 (as done in <ref type=""bibr"" target=""#b9"">(Hendrycks et al., 2018)</ref>), and use the configuration reported in <ref type=""bibr"" target=""#b34"">(Zhang et al., 201",0
"arget=""#b1"">Beluch et al., 2018;</ref><ref type=""bibr"" target=""#b21"">Redmon et al., 2016;</ref><ref type=""bibr"" target=""#b35"">Zhao et al., 2017;</ref><ref type=""bibr"" target=""#b12"">Krishna et al., 2017)</ref>. Their widespread use is attributable to their capability to model complex patterns <ref ty",0
"are the interquartile ranges.</p><p>noise is a common adverse scenario that requires attention to ensure useful visual representations can be learnt <ref type=""bibr"" target=""#b11"">(Jiang et al., 2018b;</ref><ref type=""bibr"" target=""#b29"">Wang et al., 2018a;</ref><ref type=""bibr"" target=""#b31"">Wu et that guides the training process towards the correct classes. Still other approaches use curriculum learning to formulate a robust learning procedure <ref type=""bibr"" target=""#b11"">(Jiang et al., 2018b;</ref><ref type=""bibr"" target=""#b23"">Ren et al., 2018)</ref>. Curriculum learning <ref type=""bibr"" training data using all possible labels (i.e. the true label could be randomly maintained). Note that there is another popular label noise criterion <ref type=""bibr"" target=""#b11"">(Jiang et al., 2018b;</ref><ref type=""bibr"">Wang et al., 2018b)</ref> in which the true label is not selected when perf entage of incorrect labels instead of random ones (i.e. the criterion followed in previous experiments), a criterion adopted by several other authors <ref type=""bibr"" target=""#b11"">(Jiang et al., 2018b;</ref><ref type=""bibr"" target=""#b14"">Ma et al., 2018;</ref><ref type=""bibr"" target=""#b23"">Ren et a rily on loss correction approaches <ref type=""bibr"" target=""#b22"">(Reed et al., 2015;</ref><ref type=""bibr"" target=""#b9"">Hendrycks et al., 2018;</ref><ref type=""bibr"" target=""#b11"">Jiang et al., 2018b)</ref>. A well-known approach is the bootstrapping loss <ref type=""bibr"" target=""#b22"">(Reed et al. sing the network predictions to predict hard or soft labels.</p><p>Loss correction approaches <ref type=""bibr"" target=""#b22"">(Reed et al., 2015;</ref><ref type=""bibr"" target=""#b11"">Jiang et al., 2018b;</ref><ref type=""bibr"">Patrini et al., 2017;</ref><ref type=""bibr"" target=""#b34"">Zhang et al., 2018 lean set of samples and their prediction on the corrupted data. Other approaches focus on re-weighting the contribution of noisy samples on the loss. <ref type=""bibr"" target=""#b11"">(Jiang et al., 2018b)</ref> proposes an alternating minimization framework in which a mentor network learns a curriculu The proposed method outperforms all related work in CIFAR-10 and CIFAR-100 with MD-DYR-SH, while the results for M-DYR-H are slightly below those of <ref type=""bibr"" target=""#b11"">(Jiang et al., 2018b)</ref> for low label noise levels in CIFAR-100. Nevertheless, these results should be interpreted eless, these results should be interpreted with care due to the different architectures employed and the use of sets of clean data during training in <ref type=""bibr"" target=""#b11"">(Jiang et al., 2018b)</ref> and <ref type=""bibr"" target=""#b23"">(Ren et al., 2018)</ref>.</p></div> <div xmlns=""http://w",0
"rent image manipulation tasks <ref type=""bibr"" target=""#b60"">[61,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b61"">62,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b52"">53,</ref><ref type=""bibr"" target=""#b55"">56,</ref><ref type=""bibr"" target=""#b41"">4 >53]</ref>. Examples include interactive image editing <ref type=""bibr"" target=""#b60"">[61,</ref><ref type=""bibr"" target=""#b9"">10]</ref>, sketch2image <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b42"">43]</ref>, and other image-to-image translation tasks <ref type=""bibr"" target=""#",1
"<ref type=""bibr"" target=""#b62"">63]</ref>. The role of the convonlutional layers is to generate the missing details in (x n+1 ) ↑ r (residual learning <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b56"">57]</ref>). Namely, G n performs the operation</p><formula xml:id=""formula_2"">",0
"ref type=""bibr"" target=""#b46"">[47]</ref>). However, capturing the distribution of highly diverse datasets with multiple object classes (e.g. ImageNet <ref type=""bibr"" target=""#b11"">[12]</ref>), is still considered a major challenge and often requires conditioning the generation on another input sign",0
"=""bibr"" target=""#b5"">[6]</ref> or training the model for a specific task (e.g. super-resolution <ref type=""bibr"" target=""#b29"">[30]</ref>, inpainting <ref type=""bibr"" target=""#b40"">[41]</ref>, retargeting <ref type=""bibr"" target=""#b44"">[45]</ref>).</p><p>Here, we take the use of GANs into a new real",0
", while texture and high frequency information matching the original image are realistically generated. Our method outperforms style transfer methods <ref type=""bibr"" target=""#b37"">[38,</ref><ref type=""bibr"" target=""#b16"">17]</ref> in terms of visual quality (Fig. <ref type=""figure"" target=""#fig_6""> enerating realistic texture and fine details that match the training image. Well-known style transfer methods<ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b37"">38]</ref> fail in this task.</figDesc><graphic url=""image-151.png"" coords=""7,50.54,328.55,96.92,54.66"" type=""bitmap"" />",0
"provide protein function prediction tools with broad coverage of the protein universe, as found in the 17929 families of the recent Pfam 32.0 release <ref type=""bibr"" target=""#b10"">[11]</ref>. Recent work that applies deep learning is either restricted to regimes that are not practical or representa of labelled protein sequence data; while the Pfam database is carefully curated, at least 25% of sequences have no experimentally validation function <ref type=""bibr"" target=""#b10"">[11]</ref>, and additional experimental functional characterization of protein sequences would be highly valuable. Our fication and compare deep models to current state of the art models including profile HMMs we use the highly curated Protein families (Pfam) database <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b22"">23]</ref>. The 17929 families of the Pfam 32.0 release are labelled using HMMs erse; 77.2% of the ⇠137 million sequences in UniprotKB have at least one Pfam family annotation, including 74.5% of proteins from reference proteomes <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b16"">17]</ref>. Many domains have functional annotations, although at least 22% of",1
"he intermediate layers of a deep neural network trained with supervision can capture high-level structure of the data through learned representations <ref type=""bibr"" target=""#b9"">[10]</ref>. These can be leveraged for exploratory data analysis or supervised learning on new tasks, in particular thos",0
"wissProt classes with more than 150 sequences to train a deep model <ref type=""bibr"" target=""#b14"">[15]</ref><ref type=""bibr"" target=""#b15"">[16]</ref><ref type=""bibr"" target=""#b16"">[17]</ref>. DEEPre uses sequence and Pfam annotations to predict enzyme commission (EC) classes <ref type=""bibr"" target in UniprotKB have at least one Pfam family annotation, including 74.5% of proteins from reference proteomes <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b16"">17]</ref>. Many domains have functional annotations, although at least 22% of Pfam domains have ""unknown function"". The",0
"ficient tools that annotate open reading frames with function will play a central role in exploiting this data <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref>. Identifying proteins that catalyze novel reactions, bind specific microbial targets or work together to build",0
"f opportunities, such as annotation of domains of unknown function and supervised learning on small datasets <ref type=""bibr"" target=""#b19"">[20,</ref><ref type=""bibr"" target=""#b25"">26]</ref>  We compute an average embedding for each of the 12361 large training families, and embed a single training s",0
"ifferent structures including text <ref type=""bibr"" target=""#b0"">[1]</ref>, images <ref type=""bibr"" target=""#b4"">[5]</ref>, and even acoustic signals <ref type=""bibr"" target=""#b5"">[6]</ref> with impressive results. The Transformer model using self-attention achieved the state-of-the-art in mainstrea ype=""bibr"" target=""#b7"">[8]</ref>. Self-attention and the Transformer have been exploratorily applied to ASR, but so far with unsatisfactory results. <ref type=""bibr"" target=""#b5"">[6]</ref> found that self-attention in the encoder (acoustic model) was not effective, but combined with an LSTM brought nal sub-layers coupled with feed-forward neural networks.</p><p>To adapt the encoder to long speech utterances, we follow the reshaping practice from <ref type=""bibr"" target=""#b5"">[6]</ref> by grouping consecutive frames into one step. Subsequently we combine the input features with sinusoidal posit get=""#b6"">[7]</ref>. While directly adding acoustic features to the positional encoding is harmful, potentially leading to divergence during training <ref type=""bibr"" target=""#b5"">[6]</ref>, we resolved that problem by simply projecting the concatenated features to a higher dimension before adding ( /1.0""><head n=""5."">Related Work</head><p>The idea of using self-attention as the main component of ASR models has been investigated in various forms. <ref type=""bibr"" target=""#b5"">[6]</ref> combines self-attention with LSTMs, while <ref type=""bibr"" target=""#b29"">[30]</ref> uses self-attention as an =""bibr"" target=""#b19"">[20]</ref> with adaptive learning rate over the training progress: lr = init lr * d −0.5 * min(step −0.5 , step * warmup −1.5 ) <ref type=""bibr"" target=""#b5"">(6)</ref> in which the init lr is set to 2, and we warm up the learning rate for 8000 steps. Dropout <ref type=""bibr"" ta",1
"tention in the encoder (acoustic model) was not effective, but combined with an LSTM brought marginal improvement and greater interpretability, while <ref type=""bibr"" target=""#b8"">[9]</ref> did not find any notable improvement using the Transformer in which the encoder combines self-attention with c as an alternative in CTC models. A variation of the Transformer has been applied to ASR with additional TDNN layers to downsample the acoustic signal <ref type=""bibr"" target=""#b8"">[9]</ref>. Though self-attention has provided various benefits such as training speed or model interpretability, previou",1
"p configurations, we propose a variation of stochastic depth for the Transformer inspired by the Stochastic Residual Network for image classification <ref type=""bibr"" target=""#b9"">[10]</ref>.</p><p>We discovered that its ability to regularize is the key contribution to obtain the state-of-the-art re paths through shortcut connections <ref type=""bibr"" target=""#b15"">[16]</ref>, and thus there are redundant layers. Motivated by the previous work of <ref type=""bibr"" target=""#b9"">[10]</ref>, we propose to apply stochastic residual layers into our Transformers. The method resembles Dropout <ref type to drop or to keep the whole layer (including the sub-layers inside). This way we have one hyper-parameter p for each layer.</p><p>• As suggested by <ref type=""bibr"" target=""#b9"">[10]</ref>, the lower layers of the networks handle raw-level acoustic features on the encoder side, and the character e",1
"model). In the case of speech recognition specifically, the positional encoding offers a clear advantage compared to learnable positional embeddings <ref type=""bibr"" target=""#b11"">[12]</ref>, because the speech signals can be arbitrarily long with a higher variance compared to text sequences.</p><p",0
"e audio features into high-level representations, which are then fed into an auto-regressive decoder which attentively generates the output sequences <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref>.</p><p>In this context, we aim at reconsidering acoustic modeling within dependencies between different timesteps.</p><p>Recently, self-attention has been shown to efficiently represent different structures including text <ref type=""bibr"" target=""#b0"">[1]</ref>, images <ref type=""bibr"" target=""#b4"">[5]</ref>, and even acoustic signals <ref type=""bibr"" target=""#b5"">[6]</",0
"revious approaches in general had long short-term memory neural networks (LSTM) <ref type=""bibr"" target=""#b2"">[3]</ref> or time-delay neural networks <ref type=""bibr"" target=""#b3"">[4]</ref> operating on top of frame-level features to learn sequence-level representation. These neural networks are abl experiments with similar depth suggest that self-attention performs competitively compared to LSTMs <ref type=""bibr"" target=""#b2"">[3]</ref> or TDNNs <ref type=""bibr"" target=""#b3"">[4]</ref>. The former benefits strongly from building deep residual networks, in which our main finding shows that depth",0
"work of <ref type=""bibr"" target=""#b9"">[10]</ref>, we propose to apply stochastic residual layers into our Transformers. The method resembles Dropout <ref type=""bibr"" target=""#b16"">[17]</ref>, in which the key idea is the layers are randomly dropped during training. The original residual connection LayerNorm(M * F (x) + x)<label>(3)</label></formula><p>Mask M only takes 0 or 1 as values, generated from a Bernoulli distribution similar to dropout <ref type=""bibr"" target=""#b16"">[17]</ref>. When M = 1, the inner function F is activated, while it is skipped when M = 0. These stochastic connections ep * warmup −1.5 ) <ref type=""bibr"" target=""#b5"">(6)</ref> in which the init lr is set to 2, and we warm up the learning rate for 8000 steps. Dropout <ref type=""bibr"" target=""#b16"">[17]</ref> (applied before residual connection and on the attention weights) is set at 0.2. We also apply character dro",0
"t residual networks have shown that during training the network consists of multiple sub-networks taking different paths through shortcut connections <ref type=""bibr"" target=""#b15"">[16]</ref>, and thus there are redundant layers. Motivated by the previous work of <ref type=""bibr"" target=""#b9"">[10]</ reated during training, while during inference the full network is presented, causing the effect of ensembling different sub-networks, as analyzed in <ref type=""bibr"" target=""#b15"">[16]</ref>. It is non-trivial regarding how to the parameter p for dropping layers, since the amount of residual connec",0
"/p><p>Our second discovery that the encoder requires deeper networks than the decoder for the Transformer. This is inline with the previous work from <ref type=""bibr"" target=""#b28"">[29]</ref> who increases depth for the CNN encoder. As shown above, the encoder has learn representations starting from",0
"layers. The presence of the residual layer massively increases the magnitude of the neuron values which is then alleviated by the layer-normalization <ref type=""bibr"" target=""#b12"">[13]</ref> layers placed after each residual connection.</p><p>The decoder is the standard Transformer decoder in the r ><p>In equation 3, the inner function F is either self-attention, feedforward layers or even decoder-encoder attention. The layer normalization as in <ref type=""bibr"" target=""#b12"">[13]</ref> keeps the magnitude of the hidden layers from growing large. Stochastic residual connections fundamentally a",0
"ibr"" target=""#b5"">[6]</ref> with impressive results. The Transformer model using self-attention achieved the state-of-the-art in mainstream NLP tasks <ref type=""bibr"" target=""#b6"">[7]</ref>. The attractiveness of self-attention networks originates from the ability to establish a direct connection be weighted sum of the values as following:</p><formula xml:id=""formula_0"">Attention(Q, K, V ) = softmax(QK T )V<label>(1)</label></formula><p>Recently, <ref type=""bibr"" target=""#b6"">[7]</ref> improves dot-product attention by scaling the queries before hand and introducing sub-space projection for key "" target=""#b5"">[6]</ref> by grouping consecutive frames into one step. Subsequently we combine the input features with sinusoidal positional encoding <ref type=""bibr"" target=""#b6"">[7]</ref>. While directly adding acoustic features to the positional encoding is harmful, potentially leading to diverge b12"">[13]</ref> layers placed after each residual connection.</p><p>The decoder is the standard Transformer decoder in the recent translation systems <ref type=""bibr"" target=""#b6"">[7]</ref>. The notable difference between the decoder and the encoder is that to maintain the auto-regressive nature of Details</head><p>Our hyperparameter search revolves around the Base configuration of the machine translation model in the original Transformer paper <ref type=""bibr"" target=""#b6"">[7]</ref>. For all of our experiments in this work, the embedding dimension d is set to 512 and the size of the hidden s",0
"e method of using a content-based information extractor from a set of queries Q, keys K and values K. The retrieval function is based on similarities <ref type=""bibr"" target=""#b10"">[11]</ref> between the queries and the keys, and in turn returns the the weighted sum of the values as following:</p><f",0
"type=""bibr"" target=""#b16"">[17]</ref> (applied before residual connection and on the attention weights) is set at 0.2. We also apply character dropout <ref type=""bibr"" target=""#b20"">[21]</ref> with p = 0.1 and label smoothing <ref type=""bibr"" target=""#b21"">[22]</ref> with = 0.1. The experiment result",0
"ref type=""bibr"" target=""#b16"">(Devlin et al., 2019;</ref><ref type=""bibr"">Kitaev et al., 2019, i.a.)</ref>.</p><p>In this context, the GLUE benchmark <ref type=""bibr"" target=""#b62"">(Wang et al., 2019a)</ref> has become a prominent evaluation framework for research towards general-purpose language un performances.</p><p>For tasks with multiple metrics, we use an average of the metrics. More information on the tasks included in GLUE can be found in <ref type=""bibr"" target=""#b62"">Wang et al. (2019a)</ref> and in <ref type=""bibr"">Warstadt et al. (2019, CoLA)</ref>, <ref type=""bibr"">Socher et al. (2",1
"as GLUE: We merge data from RTE1 <ref type=""bibr"" target=""#b13"">(Dagan et al., 2006</ref><ref type=""bibr"">), RTE2 (Bar Haim et al., 2006)</ref>, RTE3 <ref type=""bibr"" target=""#b21"">(Giampiccolo et al., 2007)</ref>, and RTE5 <ref type=""bibr"" target=""#b2"">(Bentivogli et al., 2009)</ref>. All datasets",0
""" target=""#b14"">Dai and Le, 2015;</ref><ref type=""bibr"" target=""#b28"">Kiros et al., 2015;</ref><ref type=""bibr"" target=""#b23"">Hill et al., 2016;</ref><ref type=""bibr"" target=""#b11"">Conneau and Kiela, 2018;</ref><ref type=""bibr"" target=""#b39"">McCann et al., 2017;</ref><ref type=""bibr"" target=""#b48"">P e affording straightforward comparison between such task-agnostic transfer learning techniques. Other similarly-motivated benchmarks include SentEval <ref type=""bibr"" target=""#b11"">(Conneau and Kiela, 2018)</ref>, which specifically evaluates fixed-size sentence embeddings, and DecaNLP <ref type=""bi",0
"ude SentEval <ref type=""bibr"" target=""#b11"">(Conneau and Kiela, 2018)</ref>, which specifically evaluates fixed-size sentence embeddings, and DecaNLP <ref type=""bibr"" target=""#b40"">(McCann et al., 2018)</ref>, which recasts a set of target tasks into a general question-answering format and prohibits",0
"=""#b48"">(Peters et al., 2018)</ref> and 63.7 for the strongest baseline with no multitask learning or pretraining above the word level. Recent models <ref type=""bibr"" target=""#b37"">(Liu et al., 2019d;</ref><ref type=""bibr"" target=""#b67"">Yang et al., 2019)</ref> have clearly surpassed estimates of no that benefits from transfer learning the most, with performance jumping from near random-chance (∼56%) at the time of GLUE's launch to 86.3% accuracy <ref type=""bibr"" target=""#b37"">(Liu et al., 2019d;</ref><ref type=""bibr"" target=""#b67"">Yang et al., 2019)</ref> at the time of writing.</p><p>Given th edictions. <ref type=""foot"" target=""#foot_0"">2</ref> In the past few months, several works <ref type=""bibr"" target=""#b30"">(Kocijan et al., 2019;</ref><ref type=""bibr"" target=""#b37"">Liu et al., 2019d)</ref> have made rapid progress via a hueristic data augmentation scheme, raising machine performance for COPA, MultiRC, and RTE are from <ref type=""bibr"" target=""#b56"">Sap et al. (2019</ref><ref type=""bibr"" target=""#b61"">), Trivedi et al. (2019</ref><ref type=""bibr"" target=""#b37"">), and Liu et al. (2019d)</ref> respectively.</p><p>Human Performance Pilehvar and Camacho-Collados (2019), <ref type=""",0
"ptic and post-synaptic spikes, enabling the learning ability of SNN. Several research efforts have explored STDP algorithms to enable learning in SNN <ref type=""bibr"" target=""#b2"">[3]</ref> [4] <ref type=""bibr"" target=""#b4"">[5]</ref>. Simulations of SNN has received significant attention in recent y istic STDP (defined as baseline) in ParallelSpikeSim shows comparable accuracy with the stateof-the-art SNN design with deterministic STDP from Diehl <ref type=""bibr"" target=""#b2"">[3]</ref>. In Diehl's work, the network yields an accuracy of 91.9% for the MNIST data set while our baseline test achie",1
"/ref> required cluster computers to run. The recent developments focused on achieving SNN simulation with higher accuracy and better simulation speed <ref type=""bibr"" target=""#b6"">[7]</ref>. In particular, advancements of Graphics Processing Units (GPUs) has led to feasibility of orders of magnitude",0
"-Dependent-Plasticity (STDP) in the synapse models. The STDP is a phenomenon observed in biology experiments <ref type=""bibr"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b1"">[2]</ref>, which can be used as the synapse model of SNN. With STDP, synapse changes its conductance based on the time d",0
"of unsupervised learning using Spike-Time-Dependent-Plasticity (STDP) in the synapse models. The STDP is a phenomenon observed in biology experiments <ref type=""bibr"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b1"">[2]</ref>, which can be used as the synapse model of SNN. With STDP, synapse ch",0
"asks such as MNIST-based digit recognition <ref type=""bibr"" target=""#b11"">[12]</ref>, the learning accuracy for difficult tasks such as Fashion-MNIST <ref type=""bibr"" target=""#b12"">[13]</ref> (images of apparel items that contains complex features) is much lower. Second, deterministic STDP provides ws good learning accuracy for both simple (MNIST <ref type=""bibr"" target=""#b11"">[12]</ref>, 96.1% accuracy) and complex (feature-rich) (Fashion-MNIST <ref type=""bibr"" target=""#b12"">[13]</ref>, 77.2% accuracy) data sets. ? We show that the stochastic STDP allows SNN to operate under input frequency r",0
"</p><p>In practice, it is difficult to program the conductances of memristors to exact values and thus the resistances are usually programmed instead <ref type=""bibr"" target=""#b12"">[13]</ref>, <ref type=""bibr"" target=""#b13"">[14]</ref>. After software training, the weights usually satisfy a quasi-nor",1
"ng can be recovered by reprogramming the conductance, while the aging effect described above is irreversible <ref type=""bibr"" target=""#b8"">[9]</ref>, <ref type=""bibr"" target=""#b9"">[10]</ref>.</p><p>To reduce the aging effect, several methods have been proposed. In <ref type=""bibr"" target=""#b8"">[9]</ he valid resistance range of the memristor degrades, an effect called aging in the memristor, as observed in <ref type=""bibr"" target=""#b8"">[9]</ref>, <ref type=""bibr"" target=""#b9"">[10]</ref>, <ref type=""bibr"" target=""#b16"">[17]</ref>, <ref type=""bibr"" target=""#b17"">[18]</ref>.</p><p>Assume that the nstants of the reference weight ? i shown in Fig. <ref type=""figure"">7</ref> and the penalty values ? 1 and ? 2 in ( <ref type=""formula"">9</ref>) and <ref type=""bibr"" target=""#b9"">(10)</ref> are shown in Table <ref type=""table"">II</ref>. By setting various combinations during software training, the",0
"[11]</ref>, a resistor is connected to a memristor in series to suppress irregular voltage drop on the memristor. Furthermore, the swapping method in <ref type=""bibr"" target=""#b11"">[12]</ref> uses rows of memristors that are slightly aged to replace the rows that are heavily aged to extend the lifet",0
"onal CMOS-based architectures are inefficient in implementing these computations. Although general-purpose and specific hardware platforms, e.g., GPU <ref type=""bibr"" target=""#b0"">[1]</ref> and FPGA <ref type=""bibr"" target=""#b1"">[2]</ref>, have been adopted to implement neural networks, power consum",0
"mristor, as observed in <ref type=""bibr"" target=""#b8"">[9]</ref>, <ref type=""bibr"" target=""#b9"">[10]</ref>, <ref type=""bibr"" target=""#b16"">[17]</ref>, <ref type=""bibr"" target=""#b17"">[18]</ref>.</p><p>Assume that the upper bound and the lower bound of the resistance of a memristor are written as R max the memristor is programmed. f (?) and g(?) are aging functions. Both aging functions are Arrhenius-based <ref type=""bibr"" target=""#b16"">[17]</ref>, <ref type=""bibr"" target=""#b17"">[18]</ref> and the parameters in the functions can be extracted from measurement data. Among various aging failure type",0
"is roof.</p><p>The third type of designs reduce the number of MAC operations by directly pruning the CNN model <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b7"">8]</ref>. Unimportant weights are forced to zero during the training (or fine-tuni ted performance was 271.6 GOP/s on a Zynq VC706 FPGA without further information disclosed on resource utilization and working frequency. The work of <ref type=""bibr"" target=""#b1"">[2]</ref> presented an algorithm-hardware codesign scheme to improve the efficiency of sparse convolutional layers execu",1
"ng resource to speedup multiply-and-accumulate (MAC) operations in the convolution (CONV) and fully-connected (FC) layers, which account for over 99% <ref type=""bibr"" target=""#b8"">[9]</ref> of the total operations for most CNNs. According to the way in which convolution computation is implemented, e",0
"ecome the dominate approach in many artificial intelligence (AI) applications, such as computer vision, speech recognition and robotics. Many studies <ref type=""bibr"" target=""#b10"">[11]</ref> have been carried out to design various CNN hardware accelerators for real-time processing. Being able to pr ix-V GXA7 FPGA.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2"">PRELIMINARIES</head><p>CNNs are composed of multiple functional layers <ref type=""bibr"" target=""#b10"">[11]</ref>, each of which performs certain type of arithmetic operations on the input image or feature map. As the most",0
"669.1 GOP/s, which is very close to this roof.</p><p>The third type of designs reduce the number of MAC operations by directly pruning the CNN model <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b7"">8]</ref>. Unimportant weights are forced to dvantage in performance density than all three designs.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""7"">RELATED WORK</head><p>Study of <ref type=""bibr"" target=""#b0"">[1]</ref> presented an energy-efficient accelerator that deployed sparse convolutional neural networks on a Artix-7 FPGA",0
"rence throughput is 204.8 GOP/s under frequency of 200 MHz. (each DSP can perform two 16/8-bit fixedpoint MACs).</p><p>The second category of designs <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b9"">10]</ref> perform convolution in the Frequency Domain (referred to as FDConv). By =""#b3"">[4,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b12"">13]</ref> are based on spatial convolution, while the works of <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b9"">10]</ref> use frequency domain convolution.</p><p>The latest work of <ref type=""b nv-based design by a factor of R mac , where R mac is the reduction rate in MAC operation. For instance, up to 69.2% of the MAC operation is saved in <ref type=""bibr"" target=""#b2"">[3]</ref>, resulting in a theoretical speedup of 3.3? in peak performance. The throughput achieved by <ref type=""bibr"" t ation is saved in <ref type=""bibr"" target=""#b2"">[3]</ref>, resulting in a theoretical speedup of 3.3? in peak performance. The throughput achieved by <ref type=""bibr"" target=""#b2"">[3]</ref> on a Intel HARP platform is 669.1 GOP/s, which is very close to this roof.</p><p>The third type of designs red ional roof of 2 ? R mac ? N mac ? Freq with FDConv-based ones. The performance of the reported SpConv-based FPGA accelerators have not exceed that of <ref type=""bibr"" target=""#b2"">[3]</ref>.</p><p>From an architecture view, existing FPGA accelerators tend to utilize a similar underlying architecture e=""bibr"" target=""#b6"">[7]</ref>. 83.6% of the total operations (accumulate and multiply) is saved compared to SDConv, while the reduction over FDConv <ref type=""bibr"" target=""#b2"">[3]</ref> and SpConv <ref type=""bibr"" target=""#b6"">[7]</ref> are 47.1% and 50%, respectively.</p></div> <div xmlns=""http he works of <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b9"">10]</ref> use frequency domain convolution.</p><p>The latest work of <ref type=""bibr"" target=""#b2"">[3]</ref> uses a frequency domain convolution scheme which gains 3.3? reduction in MAC operations for both CNN models. F cheme adopted in our design maintains a similar reduction rate of 3.06?. The implemented accelerator achieves 1.55? speedup in throughput compared to <ref type=""bibr"" target=""#b2"">[3]</ref> as a result of being able to utilize 1.6? accumulators to accelerate the convolution computation. Note that, a he convolution computation. Note that, although our scheme quantizes the CNN model in 8-bit, the precision of the datapath is of the same (16-bit) as <ref type=""bibr"" target=""#b2"">[3]</ref>. For AlexNet, the pruning scheme adopted by us only reduces the total MAC operations by 2.3? (30%  lower than e=""bibr"" target=""#b2"">[3]</ref>. For AlexNet, the pruning scheme adopted by us only reduces the total MAC operations by 2.3? (30%  lower than that of <ref type=""bibr"" target=""#b2"">[3]</ref>), but our scheme still improves the inference throughput by 5.4%. When compared with design that implements sp",0
"f type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b22"" is <ref type=""bibr"" target=""#b2"">[3]</ref>, gate-level power analysis <ref type=""bibr"" target=""#b20"">[21]</ref>, or a per-control-step ML power model <ref type=""bibr"" target=""#b13"">[14]</ref> for power estimation.</p><p>Compared to behavioral-level analysis, more implementation details are available",1
"ing Keras <ref type=""bibr"" target=""#b1"">[2]</ref>. Other ML models are realized in scikit-learn <ref type=""bibr"" target=""#b18"">[19]</ref> and XGBoost <ref type=""bibr"" target=""#b7"">[8]</ref>. We conduct our experiments on a server with an Intel Xeon E5-2630 v4 CPU and a 128GB RAM. We run neural netwo",0
"technique <ref type=""bibr"" target=""#b16"">[17]</ref>. For linear models and gradient tree boosting models, we apply principal component analysis (PCA) <ref type=""bibr"" target=""#b11"">[12]</ref> to the input data to reduce model complexity and avoid overtting. We also study the ecacy of deep learning m wn in Figure <ref type=""figure"" target=""#fig_5"">5b</ref>. We use node2vec <ref type=""bibr"" target=""#b9"">[10]</ref> for node embedding, then apply PCA <ref type=""bibr"" target=""#b11"">[12]</ref> and t-SNE <ref type=""bibr"" target=""#b15"">[16]</ref> to project the vector representations to 2D space. The r",0
"three-layer MLP models for the arithmetic unit and fourlayer MLP models for the NoC router and the RISC-V core. We use an open-source implementation <ref type=""bibr"" target=""#b0"">[1]</ref> of ShueNet V2 <ref type=""bibr"" target=""#b14"">[15]</ref> for CNN-based power estimation because of its paramete",0
""" target=""#tab_2"">1</ref> lists the benchmarks used to evaluate PRIMAL. Our benchmarks include a number of xed-and oating-point arithmetic units from <ref type=""bibr"" target=""#b17"">[18]</ref>. We also test our approach against two complex designs -a NoC router used in a CNN accelerator and a RISC-V",0
"feld et al., 2018)</ref>. At the same time, they also are sensitive to small, worst-case perturbations of the input, so-called ""adversarial examples"" <ref type=""bibr"" target=""#b32"">(Szegedy et al., 2014)</ref>. This latter phenomenon has struck many in the machine learning community as surprising an back to 2004 <ref type=""bibr"" target=""#b8"">(Dalvi et al., 2004;</ref><ref type=""bibr"" target=""#b3"">Biggio &amp; Roli, 2018)</ref>. Since the work of <ref type=""bibr"" target=""#b32"">Szegedy et al. (2014)</ref>, a subfield has focused specifically on the phenomenon of small adversarial perturbations o l. (2014)</ref>, a subfield has focused specifically on the phenomenon of small adversarial perturbations of the input, or ""adversarial examples."" In <ref type=""bibr"" target=""#b32"">Szegedy et al. (2014)</ref> it was proposed these adversarial examples occupy a dense, measure-zero subset of image spa d generalization in noise, and training on noise improved robustness to small perturbations.</p><p>In the introduction we referred to a question from <ref type=""bibr"" target=""#b32"">Szegedy et al. (2014)</ref> about why we find errors so close to our test points while the test error itself is so low. ustness perfectly requires solving an NP-hard problem for every point in the test set <ref type=""bibr"" target=""#b23"">(Katz et al., 2017)</ref>. Since <ref type=""bibr"" target=""#b32"">Szegedy et al. (2014)</ref>, hundreds of adversarial defense papers have been published. To our knowledge, only one <re",1
"r Gaussian noise are part of the same large set and show some visualizations that illustrate this relationship. (This analysis builds upon prior work <ref type=""bibr"" target=""#b15"">(Fawzi et al., 2018;</ref><ref type=""bibr"">2016)</ref> which makes smoothness assumptions on the decision boundary to r me pictures of two-dimensional slices of image space through several different triples of points. (Similar visualizations have previously appeared in <ref type=""bibr"" target=""#b15"">Fawzi et al. (2018)</ref>, and are called ""church window plots."")</p><p>We see some common themes. In the figure on the",0
"rget=""#b19"">(Guo et al., 2017;</ref><ref type=""bibr"" target=""#b13"">Dziugaite et al., 2016;</ref><ref type=""bibr"" target=""#b25"">Liu et al., 2018;</ref><ref type=""bibr"" target=""#b1"">Aydemir et al., 2018;</ref><ref type=""bibr"" target=""#b10"">Das et al., 2018;</ref><ref type=""bibr"">2017)</ref>, Pixel Def rget=""#b19"">(Guo et al., 2017;</ref><ref type=""bibr"" target=""#b13"">Dziugaite et al., 2016;</ref><ref type=""bibr"" target=""#b25"">Liu et al., 2018;</ref><ref type=""bibr"" target=""#b1"">Aydemir et al., 2018;</ref><ref type=""bibr"" target=""#b10"">Das et al., 2018;</ref> 2017), Pixel Deflection<ref type=""bibr",0
""">(Prakash et al., 2018)</ref>, total variance minimization <ref type=""bibr"" target=""#b19"">(Guo et al., 2017)</ref>, respresentation-guided denoising <ref type=""bibr"" target=""#b24"">(Liao et al., 2018)</ref>, and random resizing and random padding of the input image <ref type=""bibr"" target=""#b37"">(Xi 28"">(Prakash et al., 2018)</ref>, total variance minimization<ref type=""bibr"" target=""#b19"">(Guo et al., 2017)</ref>, respresentation-guided denoising<ref type=""bibr"" target=""#b24"">(Liao et al., 2018)</ref>, and random resizing and random padding of the input image<ref type=""bibr"" target=""#b37"">(Xie",0
"these goals, we introduce a novel instance feature-based softmax embedding method. Existing softmax embedding is usually built on classifier weights <ref type=""bibr"" target=""#b7"">[8]</ref> or memorized features <ref type=""bibr"" target=""#b45"">[46]</ref>, which has limited efficiency and discriminabi v xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.1."">Instance-wise Softmax Embedding</head><p>Softmax Embedding with Classifier Weights. Exemplar CNN <ref type=""bibr"" target=""#b7"">[8]</ref> treats each image as a distinct class. Following the conventional classifier training, it defines a matrix</p> se a softmax embedding variant for unsupervised embedding learning, which directly optimizes the real instance feature rather than classifier weights <ref type=""bibr"" target=""#b7"">[8]</ref> or memory bank <ref type=""bibr"" target=""#b45"">[46]</ref>. To achieve the goal that features of the same instan RandomCNN 32.1 DeepCluster (10) <ref type=""bibr"" target=""#b2"">[3]</ref> 44.4 DeepCluster (1000) <ref type=""bibr"" target=""#b2"">[3]</ref> 67.6 Exemplar <ref type=""bibr"" target=""#b7"">[8]</ref> 74.5 NPSoftmax <ref type=""bibr"" target=""#b45"">[46]</ref> 80.8 NCE <ref type=""bibr"" target=""#b45"">[46]</ref> 80 0) nearest neighbors based on cosine similarity, then apply weighted voting to predict its label <ref type=""bibr"" target=""#b45"">[46]</ref>   plar CNN <ref type=""bibr"" target=""#b7"">[8]</ref>, NPSoftmax <ref type=""bibr"" target=""#b45"">[46]</ref>, NCE <ref type=""bibr"" target=""#b45"">[46]</ref> and Triple ation drops dramatically. Meanwhile, it is also quite sensitive to cluster numbers, which is unsuitable for different tasks. Compared to Exemplar CNN <ref type=""bibr"" target=""#b7"">[8]</ref> which uses the classifier weights for training, the proposed method outperforms it by 9.1%. Compared to NPSoft _4"">3</ref>. The proposed method takes only 2 epochs to get a kNN accuracy of 60% while <ref type=""bibr"" target=""#b45"">[46]</ref> takes 25 epochs and <ref type=""bibr"" target=""#b7"">[8]</ref> takes 45 epochs to reach the same accuracy. It is obvious that our learning speed is much faster than the comp speed is much faster than the competitors. The efficiency is guaranteed by directly optimization on instance features rather than classifier weights <ref type=""bibr"" target=""#b7"">[8]</ref> or memory bank <ref type=""bibr"" target=""#b45"">[46]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0"">< et <ref type=""bibr"" target=""#b4"">[5]</ref>, HMP <ref type=""bibr"" target=""#b1"">[2]</ref>, Satck <ref type=""bibr"" target=""#b52"">[54]</ref> and Exemplar <ref type=""bibr"" target=""#b7"">[8]</ref>) are also reported. Those results are taken from <ref type=""bibr"" target=""#b32"">[33]</ref>.</p><p>As shown in ]</ref> is the only method that claims for unsupervised metric learning. We implement the other three state-of-the-art unsupervised methods (Exemplar <ref type=""bibr"" target=""#b7"">[8]</ref>, NCE <ref type=""bibr"" target=""#b45"">[46]</ref> and DeepCluster <ref type=""bibr"" target=""#b2"">[3]</ref>) on thr Table <ref type=""table"">3</ref>.</p><p>Generally, the instance-wise feature learning methods (NCE <ref type=""bibr"" target=""#b45"">[46]</ref>, Examplar <ref type=""bibr"" target=""#b7"">[8]</ref>, Ours) outperform non-instancewise feature learning methods (DeepCluster <ref type=""bibr"" target=""#b2"">[3]</re",1
". 2) Estimating Between-image Labels, it usually estimates between-image labels using the clustering technique <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b25"">26]</ref> or kNN-based methods <ref type=""bibr"" target=""#b40"">[41]</ref>, which p",1
"ure representation may not preserve visual similarity and its performance drops dramatically for similarity based tasks, e.g. nearest neighbor search <ref type=""bibr"" target=""#b45"">[46,</ref><ref type=""bibr"" target=""#b47"">48,</ref><ref type=""bibr"" target=""#b49"">50]</ref>.</p><p>The main challenge of max embedding method. Existing softmax embedding is usually built on classifier weights <ref type=""bibr"" target=""#b7"">[8]</ref> or memorized features <ref type=""bibr"" target=""#b45"">[46]</ref>, which has limited efficiency and discriminability. We propose to explicitly optimize the feature embedding h aims at learning a parameterized mapping between images and predefined noise signals, which constrains the distribution between raw data and noises <ref type=""bibr"" target=""#b45"">[46]</ref>. Bolztmann Machines (RBMs) <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b39"">40]</ref> features, which results in limited efficiency and discriminability. Softmax Embedding with Memory Bank. To improve the inferior efficiency, Wu et al. <ref type=""bibr"" target=""#b45"">[46]</ref> propose to set up a memory bank to store the instance features f i calculated in the previous step. The feat ng learning, which directly optimizes the real instance feature rather than classifier weights <ref type=""bibr"" target=""#b7"">[8]</ref> or memory bank <ref type=""bibr"" target=""#b45"">[46]</ref>. To achieve the goal that features of the same instance under different data augmentations are invariant, wh =""#b2"">[3]</ref> 44.4 DeepCluster (1000) <ref type=""bibr"" target=""#b2"">[3]</ref> 67.6 Exemplar <ref type=""bibr"" target=""#b7"">[8]</ref> 74.5 NPSoftmax <ref type=""bibr"" target=""#b45"">[46]</ref> 80.8 NCE <ref type=""bibr"" target=""#b45"">[46]</ref> 80. </p></div> <div xmlns=""http://www.tei-c.org/ns/1.0"">< ""bibr"" target=""#b2"">[3]</ref> 67.6 Exemplar <ref type=""bibr"" target=""#b7"">[8]</ref> 74.5 NPSoftmax <ref type=""bibr"" target=""#b45"">[46]</ref> 80.8 NCE <ref type=""bibr"" target=""#b45"">[46]</ref> 80. </p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4."">Experimental Results</head><p>We have c </div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.1."">Experiments on Seen Testing Categories</head><p>We follow the experimental settings in <ref type=""bibr"" target=""#b45"">[46]</ref> to conduct the experiments on CIFAR-10 <ref type=""bibr"" target=""#b22"">[23]</ref> and STL-10 [4] datasets, wh ation methods (RandomResizedCrop, RandomGrayscale, ColorJitter, RandomHorizontalFlip) in PyTorch with default parameters are adopted.</p><p>Following <ref type=""bibr"" target=""#b45"">[46]</ref>, we adopt weighted kNN classifier to evaluate the performance. Given a test sample, we retrieve its top-k (k e. Given a test sample, we retrieve its top-k (k = 200) nearest neighbors based on cosine similarity, then apply weighted voting to predict its label <ref type=""bibr"" target=""#b45"">[46]</ref>   plar CNN <ref type=""bibr"" target=""#b7"">[8]</ref>, NPSoftmax <ref type=""bibr"" target=""#b45"">[46]</ref>, NCE en apply weighted voting to predict its label <ref type=""bibr"" target=""#b45"">[46]</ref>   plar CNN <ref type=""bibr"" target=""#b7"">[8]</ref>, NPSoftmax <ref type=""bibr"" target=""#b45"">[46]</ref>, NCE <ref type=""bibr"" target=""#b45"">[46]</ref> and Triplet loss with and without hard mining. Triplet (hard) ref type=""bibr"" target=""#b45"">[46]</ref>   plar CNN <ref type=""bibr"" target=""#b7"">[8]</ref>, NPSoftmax <ref type=""bibr"" target=""#b45"">[46]</ref>, NCE <ref type=""bibr"" target=""#b45"">[46]</ref> and Triplet loss with and without hard mining. Triplet (hard) is the online hard negative sample within each aining <ref type=""bibr"" target=""#b15"">[16]</ref>, and the margin parameter is set to 0.5. DeepCluster <ref type=""bibr"" target=""#b2"">[3]</ref> and NCE <ref type=""bibr"" target=""#b45"">[46]</ref> represent the state-of-the-art unsupervised feature learning methods. The results are shown in Table <ref ty type=""bibr"" target=""#b7"">[8]</ref> which uses the classifier weights for training, the proposed method outperforms it by 9.1%. Compared to NPSoftmax <ref type=""bibr"" target=""#b45"">[46]</ref> and NCE <ref type=""bibr"" target=""#b45"">[46]</ref>, which use memorized feature for optimizing, the proposed classifier weights for training, the proposed method outperforms it by 9.1%. Compared to NPSoftmax <ref type=""bibr"" target=""#b45"">[46]</ref> and NCE <ref type=""bibr"" target=""#b45"">[46]</ref>, which use memorized feature for optimizing, the proposed method outperform by 2.8% and 3.2% respectively. T ds at different epochs in Fig. <ref type=""figure"" target=""#fig_4"">3</ref>. The proposed method takes only 2 epochs to get a kNN accuracy of 60% while <ref type=""bibr"" target=""#b45"">[46]</ref> takes 25 epochs and <ref type=""bibr"" target=""#b7"">[8]</ref> takes 45 epochs to reach the same accuracy. It i ncy is guaranteed by directly optimization on instance features rather than classifier weights <ref type=""bibr"" target=""#b7"">[8]</ref> or memory bank <ref type=""bibr"" target=""#b45"">[46]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.1.2"">STL-10 Dataset</head><p>STL-10 dataset < y designed with three splits: 1) train, 5K labelled images in ten The classifier is used to predict the label of test samples. We implement NPSoftmax <ref type=""bibr"" target=""#b45"">[46]</ref>, NCE <ref type=""bibr"" target=""#b45"">[46]</ref> and DeepCluster <ref type=""bibr"" target=""#b2"">[3]</ref> (clus lled images in ten The classifier is used to predict the label of test samples. We implement NPSoftmax <ref type=""bibr"" target=""#b45"">[46]</ref>, NCE <ref type=""bibr"" target=""#b45"">[46]</ref> and DeepCluster <ref type=""bibr"" target=""#b2"">[3]</ref> (cluster number 100) under the same settings with th images for learning, the proposed method achieves the best accuracy with both classifiers (kNN: 74.1%, Linear: 69.5%), which are much better than NCE <ref type=""bibr"" target=""#b45"">[46]</ref> and DeepCluster <ref type=""bibr"" target=""#b2"">[3]</ref> under the same evaluation protocol. Note that kNN me supervised metric learning. We implement the other three state-of-the-art unsupervised methods (Exemplar <ref type=""bibr"" target=""#b7"">[8]</ref>, NCE <ref type=""bibr"" target=""#b45"">[46]</ref> and DeepCluster <ref type=""bibr"" target=""#b2"">[3]</ref>) on three datasets with their released code under th et=""#b20"">[21]</ref>) on CUB200 dataset as shown in Table <ref type=""table"">3</ref>.</p><p>Generally, the instance-wise feature learning methods (NCE <ref type=""bibr"" target=""#b45"">[46]</ref>, Examplar <ref type=""bibr"" target=""#b7"">[8]</ref>, Ours) outperform non-instancewise feature learning method",1
"ng the inter-class variation <ref type=""bibr"" target=""#b31"">[32,</ref><ref type=""bibr"" target=""#b36"">37,</ref><ref type=""bibr"" target=""#b44"">45,</ref><ref type=""bibr"" target=""#b46"">47]</ref>. Most of them are designed on top of pairwise <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" targe",0
"pe=""bibr"" target=""#b28"">29]</ref>. In particular, several sampling strategies are widely investigated to improve the performance, such as hard mining <ref type=""bibr"" target=""#b15"">[16]</ref>, semihard mining <ref type=""bibr"" target=""#b34"">[35]</ref>, smart mining <ref type=""bibr"" target=""#b12"">[13] get=""#b45"">[46]</ref> and Triplet loss with and without hard mining. Triplet (hard) is the online hard negative sample within each batch for training <ref type=""bibr"" target=""#b15"">[16]</ref>, and the margin parameter is set to 0.5. DeepCluster <ref type=""bibr"" target=""#b2"">[3]</ref> and NCE <ref ty",0
"ut using labels for training. The performances of some state-of-the-art unsupervised methods (k-MeansNet <ref type=""bibr"" target=""#b4"">[5]</ref>, HMP <ref type=""bibr"" target=""#b1"">[2]</ref>, Satck <ref type=""bibr"" target=""#b52"">[54]</ref> and Exemplar <ref type=""bibr"" target=""#b7"">[8]</ref>) are als",0
"n Testing Categories</head><p>We follow the experimental settings in <ref type=""bibr"" target=""#b45"">[46]</ref> to conduct the experiments on CIFAR-10 <ref type=""bibr"" target=""#b22"">[23]</ref> and STL-10 [4] datasets, where training and testing set share the same categories. Specifically, ResNet18 ne",0
"CUB200-2011(CUB200) <ref type=""bibr"" target=""#b42"">[43]</ref>, Stanford Online Product (Product) <ref type=""bibr"" target=""#b31"">[32]</ref> and Car196 <ref type=""bibr"" target=""#b21"">[22]</ref>   Implementation Details. We implement the proposed method on PyTorch. The pre-trained Inception-V1 <ref typ",0
"formula><formula xml:id=""formula_4"">)</formula><p>where τ is the temperature parameter controlling the concentration level of the sample distribution <ref type=""bibr"" target=""#b16"">[17]</ref>. v T i f j measures the cosine similarity between the feature f j and the i-th memorized feature v i . For i",0
"ype=""bibr"" target=""#b22"">[23]</ref> and STL-10 [4] datasets, where training and testing set share the same categories. Specifically, ResNet18 network <ref type=""bibr"" target=""#b14"">[15]</ref> is adopted as the backbone and the output embedding feature dimension is set to 128. The initial learning ra",0
"default, we only use 5K training images without using labels for training. The performances of some state-of-the-art unsupervised methods (k-MeansNet <ref type=""bibr"" target=""#b4"">[5]</ref>, HMP <ref type=""bibr"" target=""#b1"">[2]</ref>, Satck <ref type=""bibr"" target=""#b52"">[54]</ref> and Exemplar <re",0
"<body> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""1."">Introduction</head><p>Deep embedding learning is a fundamental task in computer vision <ref type=""bibr"" target=""#b13"">[14]</ref>, which aims at learning a feature embedding that has the following properties: 1) positive concentrated, the",0
"overlapped. We follow the experimental settings described in <ref type=""bibr"" target=""#b31"">[32]</ref> to conduct experiments on CUB200-2011(CUB200) <ref type=""bibr"" target=""#b42"">[43]</ref>, Stanford Online Product (Product) <ref type=""bibr"" target=""#b31"">[32]</ref> and Car196 <ref type=""bibr"" tar",0
"sentation from unlabelled data <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b30"">31,</ref><ref type=""bibr"" target=""#b33"">34]</ref>. The learned feature is then generalized to different tasks by using a small set of labelled training data fr =""bibr"" target=""#b5"">[6]</ref>, the position of randomly rearranged patches <ref type=""bibr"" target=""#b30"">[31]</ref>, the missing pixels of an image <ref type=""bibr"" target=""#b33"">[34]</ref> or the color information from gray-scale images <ref type=""bibr"" target=""#b50"">[51]</ref>. Some attempts als",0
"ling diverse interests, as its dimension must be large in order to express the huge number of interest profiles at Tmall. Deep Interest Network (DIN) <ref type=""bibr"" target=""#b29"">[30]</ref> makes the user representation vary over different items with attention mechanisms to capture the diversity o mmendation algorithms <ref type=""bibr"" target=""#b2"">[3]</ref>. Besides the industrial applications proposed by <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b29"">30]</ref>, various types of deep models have gained significant attention. Neural Collaborative Filtering (NCF) <ref ty",1
"d items <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b20"">21]</ref>, keywords <ref type=""bibr"" target=""#b5"">[6]</ref> and topics <ref type=""bibr"" target=""#b27"">[28]</ref>. As the emergence of distributed representation learning, user embeddings obtained by neural networks are wi",0
"amic routing [20] is used to learn the weights on the connections between capsules, which is improved by utilizing Expectation-Maximization algorithm <ref type=""bibr"" target=""#b11"">[12]</ref> to overcome several deficiencies and achieves better accuracy. These two main differences to conventional ne or layer is inspired by the recently proposed dynamic routing for representation learning in capsule network <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b19"">20]</ref>, we firstly revisit essential basics in order to make this paper self",0
"t user interests with low-dimensional embedding vectors. For example, the deep neural network proposed for YouTube video recommendation (YouTube DNN) <ref type=""bibr"" target=""#b4"">[5]</ref> represents each user by one fixed-length vector transformed from the past behaviors of users, which can be a b to hidden factors of users and items. Recommendation is made based on compatibilities between hidden factors of users and target items. • YouTube DNN <ref type=""bibr"" target=""#b4"">[5]</ref> As mentioned above, YouTube DNN is one of the most successful deep learning method used for industrial recomme for developing deep learning-based recommendation algorithms <ref type=""bibr"" target=""#b2"">[3]</ref>. Besides the industrial applications proposed by <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b29"">30]</ref>, various types of deep models have gained significant attention. Neura",0
"re that directly generates the target signal, synthesizing it from scratch. It is most similar to recent work on sequence-tosequence voice conversion <ref type=""bibr"" target=""#b15"">[16]</ref><ref type=""bibr"" target=""#b16"">[17]</ref><ref type=""bibr"" target=""#b17"">[18]</ref>. <ref type=""bibr"" target="" equence voice conversion <ref type=""bibr"" target=""#b15"">[16]</ref><ref type=""bibr"" target=""#b16"">[17]</ref><ref type=""bibr"" target=""#b17"">[18]</ref>. <ref type=""bibr"" target=""#b15"">[16]</ref> uses a similar end-toend model, conditioned on speaker identities, to transform word segments from multiple ly accented or otherwise atypical speech into standard speech. In the future, we plan to test it on other speech disorders, and adopt techniques from <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b29"">30]</ref> to preserve the speaker identity.</p></div> <div xmlns=""http://www.t",1
"most similar to recent work on sequence-tosequence voice conversion <ref type=""bibr"" target=""#b15"">[16]</ref><ref type=""bibr"" target=""#b16"">[17]</ref><ref type=""bibr"" target=""#b17"">[18]</ref>. <ref type=""bibr"" target=""#b15"">[16]</ref> uses a similar end-toend model, conditioned on speaker identities ibr"" target=""#b16"">[17]</ref>. However, we do find it helpful to multitask train the model to predict source speech phonemes. Finally, in contrast to <ref type=""bibr"" target=""#b17"">[18]</ref>, we train the model without auxiliary alignment or auto-encoding losses.</p><p>Similar voice conversion tech",1
"nal, synthesizing it from scratch. It is most similar to recent work on sequence-tosequence voice conversion <ref type=""bibr"" target=""#b15"">[16]</ref><ref type=""bibr"" target=""#b16"">[17]</ref><ref type=""bibr"" target=""#b17"">[18]</ref>. <ref type=""bibr"" target=""#b15"">[16]</ref> uses a similar end-toend s a similar end-toend model, conditioned on speaker identities, to transform word segments from multiple speakers into multiple target voices. Unlike <ref type=""bibr"" target=""#b16"">[17]</ref>, which trained separate models for each source-target speaker pair, we focus on many-to-one conversion. Our t augmenting inputs with bottleneck features from a pretrained speech recognizer to more explicitly capture phonemic information in the source speech <ref type=""bibr"" target=""#b16"">[17]</ref>. However, we do find it helpful to multitask train the model to predict source speech phonemes. Finally, in",0
"verse set of tasks in speech and natural language processing, such as machine translation <ref type=""bibr"" target=""#b0"">[1]</ref>, speech recognition <ref type=""bibr"" target=""#b1"">[2]</ref>, and even combined speech translation <ref type=""bibr"" target=""#b2"">[3]</ref>. They have also achieved state-o re representation which the decoder consumes to predict a spectrogram. The core architecture is based on recent attention-based end-to-end ASR models <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b21"">22]</ref> and TTS models such as Tacotron <ref type=""bibr"" target=""#b3"">[4,</ref",0
"t graphemes leads to a significant improvement, reducing the WER from 27.1% to 19.9%. Extending the additive attention with a location sensitive term <ref type=""bibr"" target=""#b31"">[32]</ref> further improves results. This improves outputs on long utterances where additive attention sometimes failed",0
"o-encoding losses.</p><p>Similar voice conversion techniques have also been applied to improving intelligibility for speakers with vocal disabilities <ref type=""bibr"" target=""#b18"">[19,</ref><ref type=""bibr"" target=""#b19"">20]</ref>, and hearing-impaired speakers in particular <ref type=""bibr"" target",0
"ibr"" target=""#b6"">[7]</ref>, neural networks <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b8"">9]</ref>, dynamic frequency warping <ref type=""bibr"" target=""#b9"">[10]</ref>, and Gaussian mixture models <ref type=""bibr"" target=""#b10"">[11]</ref><ref type=""bibr"" target=""#b11"">[12]</re",0
"stics, and to retain only what is being said, not who, where or how it is said. This amounts to a text-independent, many-to-one voice conversion task <ref type=""bibr"" target=""#b5"">[6]</ref>. We evaluate the model on this voice normalization task using ASR and listening studies, verifying that it is",0
"ef type=""bibr"" target=""#b9"">[10]</ref>, and Gaussian mixture models <ref type=""bibr"" target=""#b10"">[11]</ref><ref type=""bibr"" target=""#b11"">[12]</ref><ref type=""bibr"" target=""#b12"">[13]</ref>. Recent work has also addressed accent conversion <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr""",0
"variety of techniques have been proposed for voice conversion, including mapping code books <ref type=""bibr"" target=""#b6"">[7]</ref>, neural networks <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b8"">9]</ref>, dynamic frequency warping <ref type=""bibr"" target=""#b9"">[10]</ref>, and",0
"uch as Word2Vec <ref type=""bibr"" target=""#b17"">(Mikolov et al., 2013)</ref>, ELMo <ref type=""bibr"" target=""#b21"">(Peters et al., 2018)</ref> and BERT <ref type=""bibr"" target=""#b3"">(Devlin et al., 2019)</ref> are trained and tested mainly on datasets containing general domain texts (e.g. Wikipedia), , while <ref type=""bibr"">CoVe (McCann et al., 2017)</ref> uses machine translation to embed context information into word representations.</p><p>BERT <ref type=""bibr"" target=""#b3"">(Devlin et al., 2019)</ref> is a contextualized word representation model that is based on a masked language model and p often exist in a biomedical corpus <ref type=""bibr"" target=""#b10"">(Krallinger et al., 2017)</ref>. Due to the space limitations, we refer readers to <ref type=""bibr"" target=""#b3"">Devlin et al. (2019)</ref> for a more detailed description of BERT.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0"">< tational efficiency, whenever the Wiki þ Books corpora were used for pre-training, we initialized BioBERT with the pre-trained BERT model provided by <ref type=""bibr"" target=""#b3"">Devlin et al. (2019)</ref>. We define BioBERT as a language representation model whose pre-training corpora includes bio Fine-tuning BioBERT on QA and RE tasks took less than an hour as the size of the training data is much smaller than that of the training data used by <ref type=""bibr"" target=""#b3"">Devlin et al. (2019)</ref>. On the other hand, it takes more than 20 epochs for BioBERT to reach its highest performance",1
"6"">Yoon et al., 2019)</ref>. Other deep learning based models have made improvements in biomedical text mining tasks such as relation extraction (RE) <ref type=""bibr"" target=""#b1"">(Bhasuran and Natarajan, 2018;</ref><ref type=""bibr"" target=""#b12"">Lim and Kang, 2018)</ref> and question answering (QA) LINAAEUS and Species-800 could not be found from <ref type=""bibr"" target=""#b6"">Giorgi and Bader (2018)</ref> and may be different. Like previous work <ref type=""bibr"" target=""#b1"">(Bhasuran and Natarajan, 2018)</ref>, we reported the performance of 10-fold cross-validation on datasets that do not ha on each dataset are reported. The best scores are in bold, and the second best scores are underlined. The scores on GAD and EU-ADR were obtained from <ref type=""bibr"" target=""#b1"">Bhasuran and Natarajan (2018)</ref>, and the scores on CHEMPROT were obtained from <ref type=""bibr"" target=""#b12"">Lim an",0
"nt models in biomedical text mining rely largely on adapted versions of word representations <ref type=""bibr"" target=""#b7"">(Habibi et al., 2017;</ref><ref type=""bibr"" target=""#b22"">Pyysalo et al., 2013)</ref>.</p><p>In this study, we hypothesize that current state-of-the-art word representation mode representation models, was trained on biomedical corpora which contain terms and expressions that are usually not included in a general domain corpus <ref type=""bibr"" target=""#b22"">(Pyysalo et al., 2013)</ref>. While ELMo and BERT have proven the effectiveness of contextualized word representations,",0
"transcripts to target language [1, <ref type=""bibr"" target=""#b0"">2,</ref><ref type=""bibr"" target=""#b1"">3,</ref><ref type=""bibr"" target=""#b2"">4,</ref><ref type=""bibr"" target=""#b3"">5]</ref>. This pipeline system usually suffers from time delay, parameter redundancy and error accumulation. In contrast sk, which adopts an encoder-decoder architecture and generates target words from left to right at each step [1, <ref type=""bibr"" target=""#b1"">3,</ref><ref type=""bibr"" target=""#b3"">5]</ref>. This model has also achieved promising results in ASR fields <ref type=""bibr"" target=""#b0"">[2,</ref><ref type= paper, we apply end-to-end models with the same architecture for all three tasks (ASR, ST and MT). The model architecture is similar with Transformer <ref type=""bibr"" target=""#b3"">[5]</ref>, which is the state-of-art model in MT task. Recently, this model also begins to be used in ASR task, showing",1
"8 filter sizes with 8 heads.The number of encoder layers and decoder layers in above models are all set to 6. We train our models with Adam optimizer <ref type=""bibr"" target=""#b27"">[29]</ref> with β1 = 0.9, β2 = 0.98 and = 10 −9 on 2 NVIDIA V100 GPUs.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.",0
"hus, a student model can be taught by imitating the behaviour of teacher model, such as output probabilities <ref type=""bibr"" target=""#b10"">[12,</ref><ref type=""bibr"" target=""#b11"">13]</ref>, hidden representation <ref type=""bibr"" target=""#b12"">[14,</ref><ref type=""bibr"" target=""#b13"">15]</ref>, or #b17"">19,</ref><ref type=""bibr"" target=""#b18"">20]</ref>, speech recognition <ref type=""bibr"" target=""#b10"">[12]</ref> and natural language processing <ref type=""bibr"" target=""#b11"">[13,</ref><ref type=""bibr"" target=""#b14"">16,</ref><ref type=""bibr"" target=""#b19"">21]</ref>. The teacher and student mod",0
"provides transcripts of source language utterances, and a text machine translation (MT) model which translates the transcripts to target language [1, <ref type=""bibr"" target=""#b0"">2,</ref><ref type=""bibr"" target=""#b1"">3,</ref><ref type=""bibr"" target=""#b2"">4,</ref><ref type=""bibr"" target=""#b3"">5]</re h step [1, <ref type=""bibr"" target=""#b1"">3,</ref><ref type=""bibr"" target=""#b3"">5]</ref>. This model has also achieved promising results in ASR fields <ref type=""bibr"" target=""#b0"">[2,</ref><ref type=""bibr"" target=""#b2"">4,</ref><ref type=""bibr"" target=""#b15"">17]</ref>. Recent works purpose a further",0
"this model has become a new trend in speech translation research studies <ref type=""bibr"" target=""#b4"">[6,</ref><ref type=""bibr"" target=""#b5"">7,</ref><ref type=""bibr"" target=""#b6"">8,</ref><ref type=""bibr"" target=""#b7"">9,</ref><ref type=""bibr"" target=""#b8"">10,</ref><ref type=""bibr"" target=""#b9"">11]</ target=""#b5"">[7]</ref> focus on the alignment between speech and translated phrase but not to directly predict the final translations. Bérard et al. <ref type=""bibr"" target=""#b6"">[8]</ref> give the first proof of the potential for end-to-end speech-to-text translation without using source language.",0
"sed criterion and an optimization procedure that is guaranteed to converge to a local optimum of it.</p><p>Inspired by the previous work on CycleGANs <ref type=""bibr"" target=""#b29"">(Zhu et al., 2017)</ref> and dual learning <ref type=""bibr"" target=""#b11"">(He et al., 2016)</ref>, our method takes two",1
"translation and, in the case of <ref type=""bibr"" target=""#b14"">Lample et al. (2018a)</ref>, adversarial training. This method was further improved by <ref type=""bibr"" target=""#b27"">Yang et al. (2018)</ref>, who use two language-specific encoders sharing only a subset of their parameters, and incorpo",0
"to a local optimum of it.</p><p>Inspired by the previous work on CycleGANs <ref type=""bibr"" target=""#b29"">(Zhu et al., 2017)</ref> and dual learning <ref type=""bibr"" target=""#b11"">(He et al., 2016)</ref>, our method takes two initial models in opposite directions, and defines an unsupervised optimi erlevel scores in our initial phrase-table. In addition to that, we would like to incorporate a language modeling loss during NMT training similar to <ref type=""bibr"" target=""#b11"">He et al. (2016)</ref>. Finally, we would like to adapt our approach to more relaxed scenarios with multiple languages",0
"ngs, which independently train word embeddings in two languages and learn a linear transformation to map them to a shared space through self-learning <ref type=""bibr"" target=""#b0"">(Artetxe et al., 2017</ref><ref type=""bibr"" target=""#b1"">(Artetxe et al., , 2018a) )</ref> or adversarial training <ref",0
"ning <ref type=""bibr"" target=""#b0"">(Artetxe et al., 2017</ref><ref type=""bibr"" target=""#b1"">(Artetxe et al., , 2018a) )</ref> or adversarial training <ref type=""bibr"" target=""#b4"">(Conneau et al., 2018)</ref>. The resulting crosslingual embeddings are used to initialize a shared encoder for both lan",0
"foot_0"">1</ref> .</p><p>Relational Graph Convolutional Networks (RGCNs) have been proposed as an extension of GCNs to the domain of relational graphs <ref type=""bibr"" target=""#b28"">(Schlichtkrull et al., 2018)</ref>. This model has achieved impressive performance on node classification and link pred risation. In RGCNs it was found that decomposing the kernels was beneficial for generalisation, although it comes at the cost of increased model bias <ref type=""bibr"" target=""#b28"">(Schlichtkrull et al., 2018)</ref>. We follow this approach, decomposing both the kernels W (r,k) as well as the kernel <ref type=""bibr"" target=""#b6"">de Vries and de Rooij, 2015)</ref>, RDF2Vec <ref type=""bibr"" target=""#b25"">(Ristoski and Paulheim, 2016)</ref> and RGCN <ref type=""bibr"" target=""#b28"">(Schlichtkrull et al., 2018)</ref>, and (mean and standard deviation over 200 seeds) for our implementation of RGCN, as n gives a relative mean performance drop of 1.63%.</p><p>We note that RGCN consistently outperforms RGAT on MUTAG, contrary to what might be expected <ref type=""bibr"" target=""#b28"">(Schlichtkrull et al., 2018)</ref>. The result is surprising given that RGCN lies within the parameter space of RGAT (w <ref type=""bibr"" target=""#b6"">de Vries and de Rooij, 2015)</ref>, RDF2Vec <ref type=""bibr"" target=""#b25"">(Ristoski and Paulheim, 2016)</ref> and RGCN <ref type=""bibr"" target=""#b28"">(Schlichtkrull et al., 2018)</ref>, and (mean and standard deviation over 200 runs) for our implementation of RGCN. Yel construction of the GAT layer in <ref type=""bibr"" target=""#b33"">Veli?kovi? et al. (2017)</ref>, extending to the relational setting, using ideas from <ref type=""bibr"" target=""#b28"">Schlichtkrull et al. (2018)</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>Layer input and output</hea med feature vector of the i th node.</p><p>Intermediate representations Different relations convey distinct pieces of information. The update rule of <ref type=""bibr"" target=""#b28"">Schlichtkrull et al. (2018)</ref> made this manifest by assigning each node i a distinct intermediate representation g< type=""formula"" target=""#formula_6"">6</ref>) or Equation ( <ref type=""formula"" target=""#formula_8"">7</ref>) with the neighborhood aggregation step of <ref type=""bibr"" target=""#b28"">Schlichtkrull et al. (2018)</ref> gives</p><formula xml:id=""formula_9"">h i = ? ? ? ? r?R j?N (r) i ? (r) i,j g (r) j ? www.tei-c.org/ns/1.0""><head n=""3.1"">Datasets</head><p>We evaluate the models on transductive and inductive tasks. Following the experimental setup of <ref type=""bibr"" target=""#b28"">Schlichtkrull et al. (2018)</ref> for the transductive tasks, we evaluate our model on the Resource Description Framewo r"" target=""#b35"">Wu et al. (2018)</ref> respectively.</p><p>Transductive baselines We consider as a baseline the recent state-of-the-art results from <ref type=""bibr"" target=""#b28"">Schlichtkrull et al. (2018)</ref> obtained with a two-layer RGCN model with 16 hidden units and basis function decompos ""figure"">5</ref> of Appendix D. To draw meaningful conclusions, we compare against our own implementation of RGCN rather than the results reported in <ref type=""bibr"" target=""#b28"">Schlichtkrull et al. (2018)</ref>; <ref type=""bibr"" target=""#b35"">Wu et al. (2018)</ref>.</p><p>We will occasionally em g</head><p>In Table <ref type=""table"">2a</ref> we evaluate RGAT on MUTAG and AIFB. With additive attention, WIRGAT outperforms ARGAT, consistent with <ref type=""bibr"" target=""#b28"">Schlichtkrull et al. (2018)</ref>. Interestingly, when employing multiplicative attention, the converse appears true. F",1
"y of tasks in Euclidean grid-like domains, such as image captioning <ref type=""bibr"" target=""#b8"">(Donahue et al., 2017)</ref> and classifying videos <ref type=""bibr"" target=""#b15"">(Karpathy et al., 2014)</ref>. CNNs are successful because they assume the data is locally stationary and compositional",0
"e=""bibr"" target=""#b16"">Kearnes et al. (2016)</ref>, and a RGCN model whose relational structure is determined by the degree of the node to be updated <ref type=""bibr"" target=""#b1"">Altae-Tran et al. (2016)</ref>. Specifically, up to and including some maximum degree D max ,</p><formula xml:id=""formul , 2015)</ref>, Bypass <ref type=""bibr"" target=""#b35"">(Wu et al., 2018)</ref>, Weave <ref type=""bibr"" target=""#b16"">(Kearnes et al., 2016)</ref>, RGCN <ref type=""bibr"" target=""#b1"">(Altae-Tran et al., 2016)</ref>, and (mean and standard deviation over 3 splits, 2 seeds per split) our implementation o tive</cell></row></table></figure> <figure xmlns=""http://www.tei-c.org/ns/1.0"" type=""table"" xml:id=""tab_2""><head></head><label></label><figDesc>, RGCN<ref type=""bibr"" target=""#b1"">(Altae-Tran et al., 2016)</ref>, our implementation of RGCN, additive and multiplicative attention versions of WIRGAT an",0
"petitive methods on Tox21 reported in <ref type=""bibr"" target=""#b35"">Wu et al. (2018)</ref>. Specifically, we compare against deep multitask networks <ref type=""bibr"" target=""#b24"">Ramsundar et al. (2015)</ref>, deep bypass multitask networks <ref type=""bibr"" target=""#b35"">Wu et al. (2018)</ref>, We mean Receiver Operating Characteristic (ROC) Area Under the Curve (AUC) across all 12 tasks (mean and standard deviation over 3 splits) for Multitask <ref type=""bibr"" target=""#b24"">(Ramsundar et al., 2015)</ref>, Bypass <ref type=""bibr"" target=""#b35"">(Wu et al., 2018)</ref>, Weave <ref type=""bibr"" t e=""table"">3</ref>: Graph classification mean Area Under the Curve (AUC) across all 12 tasks (mean and standard deviation over 3 splits) for Multitask <ref type=""bibr"" target=""#b24"">(Ramsundar et al., 2015)</ref>, Bypass <ref type=""bibr"" target=""#b35"">(Wu et al., 2018)</ref>, Weave <ref type=""bibr"" t e graph classification mean Receiver Operating Characteristic (ROC) AUC across all 12 tasks (mean and standard deviation over 3 splits) for Multitask <ref type=""bibr"" target=""#b24"">(Ramsundar et al., 2015)</ref>, Bypass <ref type=""bibr"" target=""#b35"">(Wu et al., 2018)</ref>, Weave <ref type=""bibr"" t",0
"b33"">(Veli?kovi? et al., 2017;</ref><ref type=""bibr"" target=""#b12"">Gong and Cheng, 2018;</ref><ref type=""bibr"" target=""#b38"">Zhang et al., 2018;</ref><ref type=""bibr"" target=""#b22"">Monti et al., 2018;</ref><ref type=""bibr"" target=""#b19"">Lee et al., 2018</ref>).</p><p>An alternative direction has bee re many types of attention mechanisms beyond vanilla additive and multiplicative. These include mechanisms leveraging the structure of the dual graph <ref type=""bibr"" target=""#b22"">Monti et al. (2018)</ref> as well as learned edge features <ref type=""bibr"" target=""#b12"">Gong and Cheng (2018)</ref>.<",0
"=""#b12"">11]</ref>. For blackbox attacks based on the transferability <ref type=""bibr"" target=""#b11"">[10,</ref><ref type=""bibr"" target=""#b20"">19,</ref><ref type=""bibr"" target=""#b8"">7]</ref>, an adversarial example is usually generated for a single input against a white-box model. So the generated adv n shown that BIM induces much more powerful white-box attacks than FGSM at the cost of worse transferability <ref type=""bibr"" target=""#b17"">[16,</ref><ref type=""bibr"" target=""#b8"">7]</ref>.</p><p>Momentum Iterative Fast Gradient Sign Method (MI-FGSM) <ref type=""bibr"" target=""#b8"">[7]</ref> proposes ients. The transferability <ref type=""bibr"" target=""#b20"">[19]</ref> of adversarial examples can be used to attack a black-box model. Several methods <ref type=""bibr"" target=""#b8"">[7,</ref><ref type=""bibr"" target=""#b38"">37]</ref> have been proposed to improve the transferability, which enable powerf like BIM.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.2."">Translation-Invariant Attack Method</head><p>Although many attack methods <ref type=""bibr"" target=""#b8"">[7,</ref><ref type=""bibr"" target=""#b38"">37]</ref> can generate adversarial examples with very high transferability acros ability <ref type=""bibr"" target=""#b17"">[16,</ref><ref type=""bibr"" target=""#b8"">7]</ref>.</p><p>Momentum Iterative Fast Gradient Sign Method (MI-FGSM) <ref type=""bibr"" target=""#b8"">[7]</ref> proposes to improve the transferability of adversarial examples by integrating a momentum term into the iterat y)). (<label>11</label></formula><formula xml:id=""formula_13"">)</formula><p>The translation-invariant method can be similarly integrated into MI-FGSM <ref type=""bibr"" target=""#b8"">[7]</ref> and DIM <ref type=""bibr"" target=""#b38"">[37]</ref> as TI-MI-FGSM and TI-DIM, respectively.</p></div> <div xmlns ur method into the fast gradient sign method (FGSM) <ref type=""bibr"" target=""#b11"">[10]</ref>, momentum iterative fast gradient sign method (MI-FGSM) <ref type=""bibr"" target=""#b8"">[7]</ref>, and diverse inputs method (DIM) <ref type=""bibr"" target=""#b38"">[37]</ref>. We do not include the basic iterat >[15]</ref> and C&amp;W's method <ref type=""bibr"" target=""#b6"">[5]</ref> since that they are not good at generating transferable adversarial examples <ref type=""bibr"" target=""#b8"">[7]</ref>. We denote the attacks combined with our translation-invariant method as TI-FGSM, TI-MI-FGSM, and TI-DIM, resp ple remains adversarial for multiple models, it is more likely to transfer to another black-box model.</p><p>We adopt the ensemble method proposed in <ref type=""bibr"" target=""#b8"">[7]</ref>, which fuses the logit activations of different models. We attack the ensemble of Inc-v3, Inc-v4, IncRes-v2, a",1
"n the input can be recognized in spite of its position. In practice, CNNs are not truly translation-invariant <ref type=""bibr"" target=""#b10"">[9,</ref><ref type=""bibr"" target=""#b15"">14]</ref>. So we make an assumption that the translation-invariant property is nearly held with very small translations",0
"the models produce correct predictions for adversarial examples, some methods attempt to detect them instead <ref type=""bibr"" target=""#b22"">[21,</ref><ref type=""bibr"" target=""#b24"">23]</ref>. However, most of the non-certified defenses demonstrate the robustness by causing obfuscated gradients, whic",0
"learning models. Besides directly making the models produce correct predictions for adversarial examples, some methods attempt to detect them instead <ref type=""bibr"" target=""#b22"">[21,</ref><ref type=""bibr"" target=""#b24"">23]</ref>. However, most of the non-certified defenses demonstrate the robustn",0
"get=""#b32"">Tokui et al., 2015;</ref><ref type=""bibr"" target=""#b19"">Maclaurin et al., 2015;</ref><ref type=""bibr"" target=""#b4"">Chen et al., 2015;</ref><ref type=""bibr"" target=""#b0"">Abadi et al., 2016;</ref><ref type=""bibr"" target=""#b23"">Paszke et al., 2017;</ref><ref type=""bibr"">The Gluon Team, 2017; n of parallelism, and simplifying deployment, distribution, and code generation (see, e.g., <ref type=""bibr"" target=""#b2"">Bergstra et al., 2010;</ref><ref type=""bibr"" target=""#b0"">Abadi et al., 2016)</ref>. But, because declarative DSLs prevent users from using arbitrary host-language constructs, th w is detailed in §4.1, and the mechanism is described in §4.6. TensorFlow graphs come with their own set of design principles, which are presented in <ref type=""bibr"" target=""#b0"">(Abadi et al., 2016)</ref>. The following terminology will be used in the sequel: a tensor is a multi-dimensional, typed in C++, automatically partitions subgraphs across devices and parallelizes operations when possible. Readers interested in the runtime should consult <ref type=""bibr"" target=""#b0"">(Abadi et al., 2016)</ref>.</p><p>The function decorator supports code generation via XLA <ref type=""bibr"" target=""#b31""",1
"b28"">(Sujeeth et al., 2011)</ref>. Outside of DSLs, there are several projects that provide just-in-time (JIT) compilation for Python, of which Numba <ref type=""bibr"" target=""#b16"">(Lam et al., 2015)</ref> and PyPy <ref type=""bibr"" target=""#b3"">(Bolz et al., 2009)</ref> are two examples.</p><p>Multi",0
"e=""bibr"" target=""#b23"">Paszke et al., 2017;</ref><ref type=""bibr"">The Gluon Team, 2017;</ref><ref type=""bibr"" target=""#b21"">Neubig et al., 2017;</ref><ref type=""bibr"" target=""#b10"">Innes, 2018;</ref><ref type=""bibr"">Frostig et al., 2018)</ref>. These software packages in fact more closely resemble d",0
"via XLA <ref type=""bibr"" target=""#b31"">(The XLA team, 2017)</ref>. TensorFlow Eager relies upon XLA to execute code on Tensor Processing Units (TPUs) <ref type=""bibr"" target=""#b25"">(Sato et al., 2017)</ref> (see §4.4). In addition to performance and hardware acceleration, dataflow graphs simplify di",0
"rget=""#b19"">Maclaurin et al., 2015;</ref><ref type=""bibr"" target=""#b4"">Chen et al., 2015;</ref><ref type=""bibr"" target=""#b0"">Abadi et al., 2016;</ref><ref type=""bibr"" target=""#b23"">Paszke et al., 2017;</ref><ref type=""bibr"">The Gluon Team, 2017;</ref><ref type=""bibr"" target=""#b21"">Neubig et al., 201 interpreted language like Pythonwhich is the case for popular DSLs like Chainer <ref type=""bibr"" target=""#b32"">(Tokui et al., 2015)</ref> and PyTorch <ref type=""bibr"" target=""#b23"">(Paszke et al., 2017)</ref>-performance is bottlenecked on the interpreter and serialization of models is difficult. To hainer <ref type=""bibr"" target=""#b32"">(Tokui et al., 2015)</ref>, Autograd <ref type=""bibr"" target=""#b19"">(Maclaurin et al., 2015)</ref>, and PyTorch <ref type=""bibr"" target=""#b23"">(Paszke et al., 2017)</ref>, but our API allows for more fine-grained control over which computations are traced.</p><p",0
"has traditionally been used for hyperparameter optimization <ref type=""bibr"" target=""#b3"">(Bengio, 2000)</ref>, or, more recently, few-shot learning <ref type=""bibr"" target=""#b11"">(Finn et al., 2017)</ref>. In essence, we turn the gradient-based optimization procedure of deep learning models upside tions <ref type=""bibr"" target=""#b3"">(Bengio, 2000)</ref> or initial weights that enable rapid adaptation to new tasks or domains in few-shot learning <ref type=""bibr"" target=""#b11"">(Finn et al., 2017)</ref>.</p><p>Meta-gradients (e.g., gradients w.r.t. hyperparameters) are obtained by backpropagatin ETA-GRADIENTS</head><p>Computing the meta gradients is expensive both from a computational and a memory point-of-view.</p><p>To alleviate this issue, <ref type=""bibr"" target=""#b11"">Finn et al. (2017)</ref> propose a first-order approximation, leading to</p><formula xml:id=""formula_10"">∇ meta A = ∇ A true class labels (train and test) available and thus more knowledge than all competing methods. First-order refers to the approximation proposed by <ref type=""bibr"" target=""#b11"">Finn et al. (2017)</ref>, i.e. ignoring all second-order derivatives. Note that Nettack is not designed for global atta",1
"s have also shown that such approaches are vulnerable to adversarial attacks both at test time (evasion) as well as training time (poisoning attacks) <ref type=""bibr"" target=""#b28"">(Zügner et al., 2018;</ref><ref type=""bibr"" target=""#b10"">Dai et al., 2018)</ref>. A core strength of models using grap strict the attacks to edge deletions only. Moreover, they focus on targeted attacks, i.e. attacks designed to change the prediction of a single node. <ref type=""bibr"" target=""#b28"">Zügner et al. (2018)</ref> consider both test-time and training-time attacks on node classification models. They circum t. In this work, for the first time, we propose an algorithm for global attacks on (deep) node classification models at training time. In contrast to <ref type=""bibr"" target=""#b28"">Zügner et al. (2018)</ref>, we explicitly tackle the bilevel optimization problem of poisoning attacks using meta learn ne minus the accuracy) of a node classification algorithm achieved after training on the data (i.e., graph) modified by our algorithm. In contrast to <ref type=""bibr"" target=""#b28"">Zügner et al. (2018)</ref> and <ref type=""bibr"" target=""#b10"">Dai et al. (2018)</ref>, our algorithm is designed for gl er's capability. In order to be effective and remain undiscovered, adversarial attacks should be unnoticeable. To account for this, we largely follow <ref type=""bibr"" target=""#b28"">Zügner et al. (2018)</ref>'s attacker capabilities. First, we impose a budget constraint ∆ on the attacks, i.e. limit t degree distribution. Any significant changes to it are very likely to be noticed; to prevent such large changes to the degree distribution, we employ <ref type=""bibr"" target=""#b28"">Zügner et al. (2018)</ref>'s unnoticeability constraint on the degree distribution. Essentially, it ensures that the gr ep learning models for node classification (e.g. a GCN) to evaluate the performance degradation due to the attack. We use the same surrogate model as <ref type=""bibr"" target=""#b28"">Zügner et al. (2018)</ref>, which is a linearized two-layer graph convolutional network:</p><formula xml:id=""formula_8"" he adjacency matrix, X are the node features, D the diagonal matrix of the node degrees, and θ = {W } the set of learnable parameters. In contrast to <ref type=""bibr"" target=""#b28"">Zügner et al. (2018)</ref> we do not linearize the output (softmax) layer.</p><p>Note that we only perform changes to t of the attack loss L atk w.r.t. the data, after training the model for T steps. We compare against this baseline in our experiments; as also done in <ref type=""bibr"" target=""#b28"">Zügner et al. (2018)</ref>. However, unlike the meta-gradient, this approximation completely disregards the training dy wing.</p><p>Comparison with competing methods. We compare our meta-gradient approach as well as its approximations with various baselines and Nettack <ref type=""bibr"" target=""#b28"">(Zügner et al., 2018)</ref>. DICE ('delete internally, connect externally') is a baseline where, for each perturbation, ><head>D UNNOTICEABILITY CONSTRAINT</head><p>In all our experiments, we enforce the unnoticeability constraint on the degree distribution proposed by <ref type=""bibr"" target=""#b28"">(Zügner et al., 2018)</ref>. In Fig. <ref type=""figure"" target=""#fig_2"">4</ref> we show that this constraint does not s",0
"neural networks are highly sensitive to these small adversarial perturbations to the data <ref type=""bibr"" target=""#b25"">(Szegedy et al., 2014;</ref><ref type=""bibr"" target=""#b12"">Goodfellow et al., 2015)</ref>. The vast majority of attacks and defenses assume the data instances to be independent a",0
"nt, and that direct multitask models are still heavily dependent on the end-to-end data.</p><p>As our second contribution, we apply a two-stage model <ref type=""bibr"" target=""#b23"">(Tu et al., 2017;</ref><ref type=""bibr"" target=""#b10"">Kano et al., 2017)</ref> as an alternative solution to our proble source speech and target text through a reasonable intermediate representation closely tied to the source text. The architecture has been proposed by <ref type=""bibr"" target=""#b23"">Tu et al. (2017)</ref> to realize a reconstruction objective, and a similar model was also applied to speech translatio tentially improved data efficiency when adding auxiliary ASR and MT training data ( §3). This model is similar to the architecture first described by <ref type=""bibr"" target=""#b23"">Tu et al. (2017)</ref>. It combines two encoder-decoder models in a cascade-like fashion, with the decoder of the first foot_7"">For two-stage and attention-passing models, we apply beam search only for the second stage decoder. We do not use the two-phase beam search of<ref type=""bibr"" target=""#b23"">Tu et al. (2017)</ref> because of its prohibitive memory requirements.</note> 			<note xmlns=""http://www.tei-c.org/ns/1",1
"baselines do not match their results, despite considerable efforts. We note that other research groups have encountered similar replicability issues <ref type=""bibr"" target=""#b4"">(Bansal et al., 2018)</ref>, explanations include the lack of a large GPU cluster to perform ASGD training, as well as t",0
"dels can be adopted for audio inputs by directly feeding speech features (here, Mel filterbank features) instead of word embeddings as encoder inputs <ref type=""bibr"" target=""#b7"">(Chorowski et al., 2015;</ref><ref type=""bibr"" target=""#b6"">Chan et al., 2016)</ref>. Such an encoder transforms M featu owever, the impact is still more significant than perhaps expected, suggesting that improved attention models that are more robust to decoding errors <ref type=""bibr"" target=""#b7"">(Chorowski et al., 2015;</ref><ref type=""bibr"" target=""#b21"">Tjandra et al., 2017)</ref> may serve to further improve ou",0
"</p><p>A large number of approaches to link prediction so far have been linear, based on various methods of factorizing the third-order binary tensor <ref type=""bibr"" target=""#b16"">(Nickel et al., 2011;</ref><ref type=""bibr"" target=""#b28"">Yang et al., 2015;</ref><ref type=""bibr"" target=""#b25"">Trouil erive a dimensionality bound which guarantees full expressiveness.</p><p>Finally, we show that several previous stateof-the-art linear models, RESCAL <ref type=""bibr"" target=""#b16"">(Nickel et al., 2011)</ref>, DistMult <ref type=""bibr"" target=""#b28"">(Yang et al., 2015)</ref>, ComplEx <ref type=""bibr xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2"">Related Work</head><p>Several linear models for link prediction have previously been proposed: RESCAL <ref type=""bibr"" target=""#b16"">(Nickel et al., 2011)</ref>  HypER <ref type=""bibr"" target=""#b0"">(Balažević et al., 2019)</ref> is a simplified convolu ad n=""5.2"">Relation to Previous Linear Models</head><p>Several previous tensor factorization models can be viewed as a special case of TuckER: RESCAL <ref type=""bibr"" target=""#b16"">(Nickel et al., 2011)</ref> Following the notation introduced in Section 3.2, the RESCAL scoring function (see Table <r",1
"</div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.2"">Tucker Decomposition</head><p>Tucker decomposition, named after Ledyard R.</p><p>Tucker <ref type=""bibr"" target=""#b26"">(Tucker, 1964)</ref>, decomposes a tensor into a set of matrices and a smaller core tensor. In a three-mode case, given",1
"ecently, state-of-the-art results have been achieved using non-linear convolutional models <ref type=""bibr"" target=""#b4"">(Dettmers et al., 2018;</ref><ref type=""bibr"" target=""#b0"">Balažević et al., 2019)</ref>. Despite achieving very good performance, the fundamental problem with deep, non-linear mo ><p>Several linear models for link prediction have previously been proposed: RESCAL <ref type=""bibr"" target=""#b16"">(Nickel et al., 2011)</ref>  HypER <ref type=""bibr"" target=""#b0"">(Balažević et al., 2019)</ref> is a simplified convolutional model, that uses a hypernetwork to generate 1D convolutiona",0
"f type=""bibr"" target=""#b9"">Kazemi and Poole, 2018)</ref>. Recently, state-of-the-art results have been achieved using non-linear convolutional models <ref type=""bibr"" target=""#b4"">(Dettmers et al., 2018;</ref><ref type=""bibr"" target=""#b0"">Balažević et al., 2019)</ref>. Despite achieving very good pe e=""bibr"" target=""#b2"">(Bordes et al., 2013)</ref> is a subset of Word-Net, a hierarchical database containing lexical relations between words. WN18RR <ref type=""bibr"" target=""#b4"">(Dettmers et al., 2018)</ref> is a subset of WN18, created by removing the inverse relations from validation and test se",0
"nd test sets.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""6.2"">Implementation and Experiments</head><p>We implement TuckER in PyTorch <ref type=""bibr"" target=""#b17"">(Paszke et al., 2017)</ref> and make our code available on GitHub. 1  We choose all hyper-parameters by random search b",0
"not model asymmetric relations; and transitive models such as TransE<ref type=""bibr"" target=""#b2"">(Bordes et al., 2013)</ref> and its variants FTransE<ref type=""bibr"" target=""#b6"">(Feng et al., 2016)</ref> andSTransE (Nguyen et al., 2016), because of certain contradictions that they impose between d",0
"ach mode. It can be thought of as a form of higherorder SVD in the special case where matrices are orthogonal and the core tensor is ""all-orthogonal"" <ref type=""bibr"" target=""#b12"">(Kroonenberg and De Leeuw, 1980)</ref>. In our case, rows of the matrices contain entity and relation embeddings, while",0
"ed Work</head><p>Most traditional solutions to single-person pose estimation adopt the probabilistic graphical model or the pictorial structure model <ref type=""bibr"" target=""#b78"">[79,</ref><ref type=""bibr"" target=""#b49"">50]</ref>, which is recently improved by exploiting deep learning for better m",1
"t epoch 30, 60 and 90. Our models can achieve comparable performance as those networks specifically designed for image classification, such as ResNet <ref type=""bibr"" target=""#b21"">[22]</ref>. Our HRNet-W32 has a singlemodel top-5 validation error of 6.5% and has a single-model top-1 validation erro",0
"""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" target=""#b38"">39,</ref><ref type=""bibr"" target=""#b46"">47,</ref><ref type=""bibr"" target=""#b56"">57,</ref><ref type=""bibr"" target=""#b40"">41,</ref><ref type=""bibr"" target=""#b45"">46,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b70""",0
"eek to use a bidirectional model to learn the representations for users' historical behavior sequences. Specifically, inspired by the success of BERT <ref type=""bibr"" target=""#b5"">[6]</ref> in text understanding, we propose to apply the deep bidirectional self-attention model to sequential recommend tional models on text sequence modeling tasks show that it is beneficial to incorporate context from both sides for sequence representations learning <ref type=""bibr"" target=""#b5"">[6]</ref>. For rigid order assumption, our model is more suitable than unidirectional models in modeling user behavior s at attention mechanism as an additional component to the original models. In contrast, Transformer <ref type=""bibr"" target=""#b52"">[52]</ref> and BERT <ref type=""bibr"" target=""#b5"">[6]</ref> are built solely on multi-head self-attention and achieve state-of-the-art results on text sequence modeling. e. In fact, these parameters are different from layer to layer. In this work, following OpenAI GPT <ref type=""bibr"" target=""#b38"">[38]</ref> and BERT <ref type=""bibr"" target=""#b5"">[6]</ref>, we use a smoother GELU <ref type=""bibr"" target=""#b12"">[13]</ref> activation rather than the standard ReLu act ly train our proposed model, we apply a new objective: Cloze task <ref type=""bibr"" target=""#b50"">[50]</ref> (also known as ""Masked Language Model"" in <ref type=""bibr"" target=""#b5"">[6]</ref>) to sequential recommendation. It is a test consisting of a portion of language with some words removed, where make predicting the future become trivial and the network would not learn anything useful.</p><p>To tackle this problem, we introduce the Cloze task <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b50"">50]</ref> to take the place of the objective in unidirectional models (i.e., seq uences. Previous work has shown that it is beneficial to jointly attend to information from different representation subspaces at different positions <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b29"">29,</ref><ref type=""bibr"" target=""#b52"">52]</ref>. Thus, we here adopt the multi",1
"arget=""#formula_5"">1</ref>.</p><p>The temperature d/h is introduced to produce a softer attention distribution for avoiding extremely small gradients <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b52"">52]</ref>.</p><p>Position-wise Feed-Forward Network. As described above, the s",0
"""><head n=""2.3"">Attention Mechanism</head><p>Attention mechanism has shown promising potential in modeling sequential data, e.g., machine translation <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b52"">52]</ref> and text classification <ref type=""bibr"">[? ]</ref>. Recently, some wo",0
"rformance by integrating the distributed item representations learned from auxiliary information, e.g., text <ref type=""bibr"" target=""#b23"">[23,</ref><ref type=""bibr"" target=""#b53"">53]</ref>, images <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b55"">55]</ref>, and acoustic featu",0
""">27,</ref><ref type=""bibr"" target=""#b41"">41]</ref>. Another line of work is item-based neighborhood methods <ref type=""bibr"" target=""#b19"">[20,</ref><ref type=""bibr"" target=""#b25"">25,</ref><ref type=""bibr"" target=""#b31"">31,</ref><ref type=""bibr"" target=""#b43"">43]</ref>. They estimate a user's prefe",0
"ds seeks to improve the recommendation performance by integrating the distributed item representations learned from auxiliary information, e.g., text <ref type=""bibr"" target=""#b23"">[23,</ref><ref type=""bibr"" target=""#b53"">53]</ref>, images <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" ta",0
"arious methods have been proposed to make sequential recommendations based on users' historical interactions <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b40"">40]</ref>. They aim to predict the successive item(s) that a user is likely to than recurrent neural networks, various deep learning models are also introduced for sequential recommendation <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b33"">33,</ref><ref type=""bibr"" target=""#b49"">49]</ref>. For example, Tang and Wang < ion records according to the timestamps. To ensure the quality of the dataset, following the common practice <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b40"">40,</ref><ref type=""bibr"" target=""#b49"">49]</ref>, we keep users with at least we adopted the leave-one-out evaluation (i.e., next item recommendation) task, which has been widely used in <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b49"">49]</ref>. For each user, we hold out the last item of the behavior sequence as and utilize the remaining items for training. For easy and fair evaluation, we follow the common strategy in <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b49"">49]</ref>, pairing each ground truth item in the test set with 100 randomly sam f type=""foot"" target=""#foot_3"">4</ref> : This is a dataset collected from Steam, a large online video game distribution platform, by Kang and McAuley <ref type=""bibr"" target=""#b21"">[22]</ref>. • MovieLens <ref type=""bibr"" target=""#b7"">[8]</ref>: This is a popular benchmark dataset for evaluating rec orm exceeds a threshold of 5. For fair comparison, we set the layer number L = 2 and head number h = 2 and use the same maximum sequence length as in <ref type=""bibr"" target=""#b21"">[22]</ref>, N = 200 for ML-1m and ML-20m, N = 50 for Beauty and Steam datasets. For head setting, we empirically set th =""#foot_4"">5</ref> and MovieLens 20m (ML-20m) <ref type=""foot"" target=""#foot_5"">6</ref> . For dataset preprocessing, we follow the common practice in <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b40"">40,</ref><ref type=""bibr"" target=""#b49"">49]</ref>. For all datasets, we conver 1"" xml:id=""foot_0"">https://www.netflixprize.com</note> 			<note xmlns=""http://www.tei-c.org/ns/1.0"" place=""foot"" n=""2"" xml:id=""foot_1"">Here, following<ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b40"">40]</ref>, we use the relative time index instead of absolute time index for n",0
"11]</ref>.</p><p>Recently, RNN and its variants, Gated Recurrent Unit (GRU) <ref type=""bibr"" target=""#b3"">[4]</ref> and Long Short-Term Memory (LSTM) <ref type=""bibr"" target=""#b16"">[17]</ref>, are becoming more and more popular for modeling user behavior sequences <ref type=""bibr"" target=""#b6"">[7,</",0
"the table' context contributes to recognizing the black blob). <ref type=""bibr"" target=""#b14"">[15]</ref>, <ref type=""bibr"" target=""#b22"">[23]</ref>, <ref type=""bibr"" target=""#b24"">[25]</ref>, <ref type=""bibr"" target=""#b25"">[26]</ref>, <ref type=""bibr"" target=""#b27"">[28]</ref>, <ref type=""bibr"" targ ity dimension introduced by Xie et al. <ref type=""bibr"" target=""#b55"">[56]</ref>, as well as squeeze and excitation (SE) block presented by Hu et al. <ref type=""bibr"" target=""#b24"">[25]</ref>. The proposed Res2Net module introduces the scale dimension that is orthogonal to these improvements. As sho hown in Fig. <ref type=""figure"">3</ref>, we can easily integrate the cardinality dimension <ref type=""bibr"" target=""#b55"">[56]</ref> and the SE block <ref type=""bibr"" target=""#b24"">[25]</ref> with the proposed Res2Net module.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.2.1"">Dimensi : The Res2Net module can be integrated with the dimension cardinality <ref type=""bibr"" target=""#b55"">[56]</ref> (replace conv with group conv) and SE <ref type=""bibr"" target=""#b24"">[25]</ref> blocks.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.2.2"">SE block.</head><p>A SE block ada .2.2"">SE block.</head><p>A SE block adaptively re-calibrates channel-wise feature responses by explicitly modelling inter-dependencies among channels <ref type=""bibr"" target=""#b24"">[25]</ref>. Similar to <ref type=""bibr"" target=""#b24"">[25]</ref>, we add the SE block right before the residual connect brates channel-wise feature responses by explicitly modelling inter-dependencies among channels <ref type=""bibr"" target=""#b24"">[25]</ref>. Similar to <ref type=""bibr"" target=""#b24"">[25]</ref>, we add the SE block right before the residual connections of the Res2Net module. Our Res2Net module can ben ement of 0.73% in terms of top-1 error over the  <ref type=""bibr"" target=""#b22"">[23]</ref>, ResNeXt <ref type=""bibr"" target=""#b55"">[56]</ref>, SE-Net <ref type=""bibr"" target=""#b24"">[25]</ref>, bLResNet <ref type=""bibr"" target=""#b4"">[5]</ref>, and DLA <ref type=""bibr"" target=""#b59"">[60]</ref> are the",1
"t=""#b25"">[26]</ref>, <ref type=""bibr"" target=""#b27"">[28]</ref>, <ref type=""bibr"" target=""#b46"">[47]</ref>, <ref type=""bibr"" target=""#b50"">[51]</ref>, <ref type=""bibr"" target=""#b55"">[56]</ref>, <ref type=""bibr"" target=""#b59"">[60]</ref>, have made significant advances in numerous vision tasks with sta s2Net block), as an essential factor in addition to existing dimensions of depth <ref type=""bibr"" target=""#b46"">[47]</ref>, width 2 , and cardinality <ref type=""bibr"" target=""#b55"">[56]</ref>. We state in Sec. 4.4 that increasing scale is more effective than increasing other dimensions.</p><p>Note t that the Res2Net module can further improve the performance of state-of-the-art CNNs, e.g., ResNet <ref type=""bibr"" target=""#b22"">[23]</ref>, ResNeXt <ref type=""bibr"" target=""#b55"">[56]</ref>, and DLA <ref type=""bibr"" target=""#b59"">[60]</ref>.</p><p>1. Convolutional operators and filters are used in t=""#b25"">[26]</ref>, <ref type=""bibr"" target=""#b27"">[28]</ref>, <ref type=""bibr"" target=""#b46"">[47]</ref>, <ref type=""bibr"" target=""#b50"">[51]</ref>, <ref type=""bibr"" target=""#b55"">[56]</ref>, <ref type=""bibr"" target=""#b59"">[60]</ref>, achieving state-of-theart performance in various vision tasks wi g_0"">2(a</ref>) is a basic building block in many modern backbone CNNs architectures, e.g., ResNet <ref type=""bibr"" target=""#b22"">[23]</ref>, ResNeXt <ref type=""bibr"" target=""#b55"">[56]</ref>, and DLA <ref type=""bibr"" target=""#b59"">[60]</ref>. Instead of extracting features using a group of 3×3 filt Modern Modules</head><p>Numerous neural network modules have been proposed in recent years, including cardinality dimension introduced by Xie et al. <ref type=""bibr"" target=""#b55"">[56]</ref>, as well as squeeze and excitation (SE) block presented by Hu et al. <ref type=""bibr"" target=""#b24"">[25]</re le dimension that is orthogonal to these improvements. As shown in Fig. <ref type=""figure"">3</ref>, we can easily integrate the cardinality dimension <ref type=""bibr"" target=""#b55"">[56]</ref> and the SE block <ref type=""bibr"" target=""#b24"">[25]</ref> with the proposed Res2Net module.</p></div> <div http://www.tei-c.org/ns/1.0""><head n=""3.2.1"">Dimension cardinality.</head><p>The dimension cardinality indicates the number of groups within a filter <ref type=""bibr"" target=""#b55"">[56]</ref>. This dimension changes filters from single-branch to multi-branch and improves the representation ability o dinality are presented in Sec. 4.2 and Sec. 4.4. Fig. <ref type=""figure"">3</ref>: The Res2Net module can be integrated with the dimension cardinality <ref type=""bibr"" target=""#b55"">[56]</ref> (replace conv with group conv) and SE <ref type=""bibr"" target=""#b24"">[25]</ref> blocks.</p></div> <div xmlns e can easily integrate the proposed Res2Net module into the state-ofthe-art models, such as ResNet <ref type=""bibr"" target=""#b22"">[23]</ref>, ResNeXt <ref type=""bibr"" target=""#b55"">[56]</ref>, DLA <ref type=""bibr"" target=""#b59"">[60]</ref> and Big-Little Net <ref type=""bibr"" target=""#b4"">[5]</ref>. T re referred to as Res2Net, Res2NeXt, Res2Net-DLA, and bLRes2Net-50, respectively.</p><p>The proposed scale dimension is orthogonal to the cardinality <ref type=""bibr"" target=""#b55"">[56]</ref> dimension and width <ref type=""bibr"" target=""#b22"">[23]</ref> dimension of prior work. Thus, after the scale on the ImageNet <ref type=""bibr"" target=""#b43"">[44]</ref> dataset, we mainly use the ResNet-50 <ref type=""bibr"" target=""#b22"">[23]</ref>, ResNeXt-50 <ref type=""bibr"" target=""#b55"">[56]</ref>, DLA-60 <ref type=""bibr"" target=""#b59"">[60]</ref>, and bLResNet-50 <ref type=""bibr"" target=""#b4"">[5]</ref> a s is around 4.2G for 50-layer networks. For experiments on the CIFAR <ref type=""bibr"" target=""#b26"">[27]</ref> dataset, we use the ResNeXt-29, 8c×64w <ref type=""bibr"" target=""#b55"">[56]</ref> as our baseline model. Empirical evaluations and discussions of the proposed models with respect to model co ing the Pytorch framework.</p><p>For fair comparisons, we use the Pytorch implementation of ResNet <ref type=""bibr"" target=""#b22"">[23]</ref>, ResNeXt <ref type=""bibr"" target=""#b55"">[56]</ref>, DLA <ref type=""bibr"" target=""#b59"">[60]</ref> as well as bLResNet-50 <ref type=""bibr"" target=""#b4"">[5]</ref ng, we use the same image cropping method as <ref type=""bibr"" target=""#b22"">[23]</ref>. On the CIFAR dataset, we use the implementation of ResNeXt-29 <ref type=""bibr"" target=""#b55"">[56]</ref>. For all tasks, we use the original implementations of baselines and only replace the backbone model with th .68% over the SENet-50. bLRes2Net-50 has an improvement of 0.73% in terms of top-1 error over the  <ref type=""bibr"" target=""#b22"">[23]</ref>, ResNeXt <ref type=""bibr"" target=""#b55"">[56]</ref>, SE-Net <ref type=""bibr"" target=""#b24"">[25]</ref>, bLResNet <ref type=""bibr"" target=""#b4"">[5]</ref>, and DLA g deeper with Res2Net.</head><p>Deeper networks have been shown to have stronger representation capability <ref type=""bibr"" target=""#b22"">[23]</ref>, <ref type=""bibr"" target=""#b55"">[56]</ref> for vision tasks. To validate our model with greater depth, we compare the classification performance of the dataset <ref type=""bibr"" target=""#b26"">[27]</ref>, which contains 50k training images and 10k testing images from 100 classes. The ResNeXt-29, 8c×64w <ref type=""bibr"" target=""#b55"">[56]</ref> is used as the baseline model. We only replace the original basic block with our proposed Res2Net module whi ing other configurations unchanged.  </p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.4"">Scale Variation</head><p>Similar to Xie et al. <ref type=""bibr"" target=""#b55"">[56]</ref>, we evaluate the test performance of the baseline model by increasing different CNN dimensions, including sc test performance of the baseline model by increasing different CNN dimensions, including scale (Equation ( <ref type=""formula"">1</ref>)), cardinality <ref type=""bibr"" target=""#b55"">[56]</ref>, and depth <ref type=""bibr"" target=""#b46"">[47]</ref>. While increasing model capacity using one dimension, w increasing model capacity using one dimension, we fix all other dimensions. A series of networks are trained and evaluated under these changes. Since <ref type=""bibr"" target=""#b55"">[56]</ref> has already shown that increasing cardinality is more effective than increasing width, we only compare the p",1
"ckbone network, i.e., VGGNet <ref type=""bibr"" target=""#b46"">[47]</ref>, to extract features of multiple scales. He et al. propose an SPP-Net approach <ref type=""bibr"" target=""#b21"">[22]</ref> that utilizes spatial pyramid pooling after the backbone network to enhance the multi-scale ability. The Fas",0
"target=""#b5"">[6]</ref>, salient object detection <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b23"">[24]</ref>, object proposal <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" target=""#b42"">[43]</ref>, skeleton extraction <ref type=""bibr"" target=""#b66"">[67]</ref>, s",0
""" target=""#b11"">[12]</ref>, <ref type=""bibr"" target=""#b42"">[43]</ref>, skeleton extraction <ref type=""bibr"" target=""#b66"">[67]</ref>, stereo matching <ref type=""bibr"" target=""#b41"">[42]</ref>, and edge detection <ref type=""bibr"" target=""#b36"">[37]</ref>, <ref type=""bibr"" target=""#b56"">[57]</ref>.</p",0
"), and their surrounding context (e.g., 'on the table' context contributes to recognizing the black blob). <ref type=""bibr"" target=""#b14"">[15]</ref>, <ref type=""bibr"" target=""#b22"">[23]</ref>, <ref type=""bibr"" target=""#b24"">[25]</ref>, <ref type=""bibr"" target=""#b25"">[26]</ref>, <ref type=""bibr"" targ bibr"" target=""#b49"">[50]</ref>, <ref type=""bibr"" target=""#b50"">[51]</ref>, <ref type=""bibr"" target=""#b51"">[52]</ref>), residual modules (e.g., ResNet <ref type=""bibr"" target=""#b22"">[23]</ref>), shortcut connections (e.g., DenseNet <ref type=""bibr"" target=""#b25"">[26]</ref>), and hierarchical layer ag architectures. Extensive experimental results show that the Res2Net module can further improve the performance of state-of-the-art CNNs, e.g., ResNet <ref type=""bibr"" target=""#b22"">[23]</ref>, ResNeXt <ref type=""bibr"" target=""#b55"">[56]</ref>, and DLA <ref type=""bibr"" target=""#b59"">[60]</ref>.</p><p rg/ns/1.0""><head n=""2.1"">Backbone Networks</head><p>Recent years have witnessed numerous backbone networks <ref type=""bibr"" target=""#b14"">[15]</ref>, <ref type=""bibr"" target=""#b22"">[23]</ref>, <ref type=""bibr"" target=""#b25"">[26]</ref>, <ref type=""bibr"" target=""#b27"">[28]</ref>, <ref type=""bibr"" targ 52]</ref> stack more filters in each path of the parallel paths in the GoogLeNet to further expand the receptive field. On the other hand, the ResNet <ref type=""bibr"" target=""#b22"">[23]</ref> introduces short connections to neural networks, thereby alleviating the gradient vanishing problem while ob ucture shown in Fig. <ref type=""figure"" target=""#fig_0"">2(a</ref>) is a basic building block in many modern backbone CNNs architectures, e.g., ResNet <ref type=""bibr"" target=""#b22"">[23]</ref>, ResNeXt <ref type=""bibr"" target=""#b55"">[56]</ref>, and DLA <ref type=""bibr"" target=""#b59"">[60]</ref>. Inste he layer-wise feature aggregation models of CNNs, we can easily integrate the proposed Res2Net module into the state-ofthe-art models, such as ResNet <ref type=""bibr"" target=""#b22"">[23]</ref>, ResNeXt <ref type=""bibr"" target=""#b55"">[56]</ref>, DLA <ref type=""bibr"" target=""#b59"">[60]</ref> and Big-Li t-50, respectively.</p><p>The proposed scale dimension is orthogonal to the cardinality <ref type=""bibr"" target=""#b55"">[56]</ref> dimension and width <ref type=""bibr"" target=""#b22"">[23]</ref> dimension of prior work. Thus, after the scale is set, we adjust the value of cardinality and width to maint ""bibr"" target=""#b13"">[14]</ref>.</p><p>For experiments on the ImageNet <ref type=""bibr"" target=""#b43"">[44]</ref> dataset, we mainly use the ResNet-50 <ref type=""bibr"" target=""#b22"">[23]</ref>, ResNeXt-50 <ref type=""bibr"" target=""#b55"">[56]</ref>, DLA-60 <ref type=""bibr"" target=""#b59"">[60]</ref>, and etails</head><p>We implement the proposed models using the Pytorch framework.</p><p>For fair comparisons, we use the Pytorch implementation of ResNet <ref type=""bibr"" target=""#b22"">[23]</ref>, ResNeXt <ref type=""bibr"" target=""#b55"">[56]</ref>, DLA <ref type=""bibr"" target=""#b59"">[60]</ref> as well as ibr"" target=""#b43"">[44]</ref>, each image is of 224×224 pixels randomly cropped from a re-sized image. We use the same data argumentation strategy as <ref type=""bibr"" target=""#b22"">[23]</ref>, <ref type=""bibr"" target=""#b51"">[52]</ref>. Similar to <ref type=""bibr"" target=""#b22"">[23]</ref>, we train t mage. We use the same data argumentation strategy as <ref type=""bibr"" target=""#b22"">[23]</ref>, <ref type=""bibr"" target=""#b51"">[52]</ref>. Similar to <ref type=""bibr"" target=""#b22"">[23]</ref>, we train the network using SGD with weight decay 0.0001, momentum 0.9, and a mini-batch of 256 on 4 Titan X osed models, are trained for 100 epochs with the same training and data argumentation strategy. For testing, we use the same image cropping method as <ref type=""bibr"" target=""#b22"">[23]</ref>. On the CIFAR dataset, we use the implementation of ResNeXt-29 <ref type=""bibr"" target=""#b55"">[56]</ref>. Fo -1 error. The SE-Res2Net-50 has an improvement of 1.68% over the SENet-50. bLRes2Net-50 has an improvement of 0.73% in terms of top-1 error over the  <ref type=""bibr"" target=""#b22"">[23]</ref>, ResNeXt <ref type=""bibr"" target=""#b55"">[56]</ref>, SE-Net <ref type=""bibr"" target=""#b24"">[25]</ref>, bLResN od against the InceptionV3 [52] model, which utilizes parallel filters with different kernel combinations. For fair comparisons, we use the ResNet-50 <ref type=""bibr"" target=""#b22"">[23]</ref> as the baseline model and train our model with the input image size of 299×299 pixels, as used in the Incept /www.tei-c.org/ns/1.0""><head n=""4.2.2"">Going deeper with Res2Net.</head><p>Deeper networks have been shown to have stronger representation capability <ref type=""bibr"" target=""#b22"">[23]</ref>, <ref type=""bibr"" target=""#b55"">[56]</ref> for vision tasks. To validate our model with greater depth, we co",0
"widely used in both conventional feature design <ref type=""bibr"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b38"">[39]</ref> and deep learning <ref type=""bibr"" target=""#b9"">[10]</ref>, <ref type=""bibr"" target=""#b50"">[51]</ref>. Obtaining multi-scale representations in vision tasks requires fe efficient network architecture is the key to further improving the performance of CNNs.</p><p>In the past few years, several backbone networks, e.g., <ref type=""bibr"" target=""#b9"">[10]</ref>,</p><p>*Equal contribution</p><p>• S.H. Gao, M.M. Cheng, K. Zhao, and X.Y Zhang are with the TKLNDST, College connected layers in the DenseNet <ref type=""bibr"" target=""#b25"">[26]</ref> enable the network to process objects in a very wide range of scales. DPN <ref type=""bibr"" target=""#b9"">[10]</ref> combines the ResNet with DenseNet to enable feature re-usage ability of ResNet and the feature exploration ab",0
"v xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.6"">Object Detection</head><p>For object detection task, we validate the Res2Net on the PAS-CAL VOC07 <ref type=""bibr"" target=""#b16"">[17]</ref> and MS COCO <ref type=""bibr"" target=""#b32"">[33]</ref> datasets, using Faster R-CNN <ref type=""bibr"" target=""",0
"target=""#b2"">[3]</ref> utilize handcrafted representations of global contrast <ref type=""bibr"" target=""#b12"">[13]</ref> or multiscale region features <ref type=""bibr"" target=""#b52"">[53]</ref>. Li et al. <ref type=""bibr"" target=""#b28"">[29]</ref> propose one of the earliest methods that enables multi-",0
"(e.g., sofa, table, and cup in this example), and their surrounding context (e.g., 'on the table' context contributes to recognizing the black blob). <ref type=""bibr"" target=""#b14"">[15]</ref>, <ref type=""bibr"" target=""#b22"">[23]</ref>, <ref type=""bibr"" target=""#b24"">[25]</ref>, <ref type=""bibr"" targ /head></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2.1"">Backbone Networks</head><p>Recent years have witnessed numerous backbone networks <ref type=""bibr"" target=""#b14"">[15]</ref>, <ref type=""bibr"" target=""#b22"">[23]</ref>, <ref type=""bibr"" target=""#b25"">[26]</ref>, <ref type=""bibr"" targ",0
"s formed by applying an equivariant linear operator, a pointwise non-linearity, and either an invariant or equivariant linear output layer. Recently, <ref type=""bibr"" target=""#b19"">Maron et al. (2019b)</ref> showed that by allowing higherorder tensorization inside the network, universal invariant GN ""bibr"" target=""#b6"">(Cybenko, 1989;</ref><ref type=""bibr"" target=""#b13"">Hornik et al., 1989;</ref><ref type=""bibr"" target=""#b20"">Pinkus, 1999)</ref>. <ref type=""bibr"" target=""#b19"">Maron et al. (2019b)</ref> recently proved that certain invariant GNNs were universal approximators of invariant contin r contributions show that they are dense in the spaces of continuous invariant (resp. equivariant) functions.</p><p>2 The case of invariant functions <ref type=""bibr"" target=""#b19"">Maron et al. (2019b)</ref> recently proved that invariant GNNs similar to (1) are universal approximators of continuous function is uniformly well approximated by a GNN on the whole set G inv. , that is, for all numbers of nodes n n max simultaneously. On the contrary, <ref type=""bibr"" target=""#b19"">Maron et al. (2019b)</ref> work with a fixed n, and it does not seem that their proof can extend easily to encompass se r proof is that it does not provide an upper bound on the order of tensorization k s . Indeed, through Noether's theorem on polynomials, the proof of <ref type=""bibr"" target=""#b19"">Maron et al. (2019b)</ref> shows that k s n d (n d − 1)/2 is sufficient for universality, which we cannot seem to deduc ve for future work. For the same reason, while the previous invariant case could be easily extended to invariance to subgroups of O n , as is done by <ref type=""bibr"" target=""#b19"">Maron et al. (2019b)</ref>, for the equivariant case our theorem only applies when considering the full permuation grou follows. After reviewing previous works and notations in the rest of the introduction, in Section 2 we provide an alternative proof of the result of <ref type=""bibr"" target=""#b19"">(Maron et al., 2019b)</ref> for invariant GNNs (Theorem 1), which will serve as a basis for the equivariant case. It re form convergence. We then have the following result. Theorem 1. For any ρ ∈ F MLP , N inv. (ρ) is dense in C(G inv. , d edit ).</p><p>Comparison with <ref type=""bibr"" target=""#b19"">(Maron et al., 2019b)</ref>. A variant of Theorem 1 was proved in <ref type=""bibr"" target=""#b19"">(Maron et al., 2019b)< dense in C(G inv. , d edit ).</p><p>Comparison with <ref type=""bibr"" target=""#b19"">(Maron et al., 2019b)</ref>. A variant of Theorem 1 was proved in <ref type=""bibr"" target=""#b19"">(Maron et al., 2019b)</ref>. The two proofs are however different: their proof relies on the construction of a basis of ss theorem for algebras of real-valued functions. See the next subsection for details.</p><p>One improvement of our result with respect to the one of <ref type=""bibr"" target=""#b19"">(Maron et al., 2019b)</ref> is that it can handle graphs of varying sizes. As mentioned in the introduction, a single s c graphs using recurrent architectures <ref type=""bibr"" target=""#b0"">(Battaglia et al., 2016)</ref>. Another outstanding open question, formulated in <ref type=""bibr"" target=""#b19"">(Maron et al., 2019b)</ref>, is the characterization of the approximation power of networks whose tensorization orders N case, the universality of architectures built using a single hidden layer of such equivariant operators followed by an invariant layer is proved in <ref type=""bibr"" target=""#b19"">(Maron et al., 2019b</ref>) (see also <ref type=""bibr"">(Kondor et al., 2018)</ref>). This is the closest work from our,",1
"ce prediction <ref type=""bibr"" target=""#b9"">(Fout et al., 2017)</ref>, among many others. See <ref type=""bibr"" target=""#b34"">(Zhou et al., 2018;</ref><ref type=""bibr"" target=""#b1"">Bronstein et al., 2017)</ref> for thorough reviews. It is therefore of great interest to increase our understanding of e ns <ref type=""bibr"" target=""#b3"">(Bruna et al., 2014)</ref> or polynomials <ref type=""bibr"" target=""#b8"">(Defferrard et al., 2016)</ref>. We refer to <ref type=""bibr"" target=""#b1"">(Bronstein et al., 2017;</ref><ref type=""bibr"" target=""#b30"">Xu et al., 2019)</ref> for recent reviews. For regular-grid",0
"with a metric that takes into account graphs with different number of nodes.</p><p>A distance often used in the literature is the graph edit distance <ref type=""bibr"" target=""#b25"">(Sanfeliu and Fu, 1983)</ref>. It relies on defining a set of elementary operations o and a cost c(o) associated to eac",0
"rve as a basis for the equivariant case in the Section 3. It relies on Stone-Weierstrass theorem, which we recall below. Theorem 2 (Stone-Weierstrass <ref type=""bibr"" target=""#b23"">(Rudin (1991)</ref>, Thm. 5.7)). Suppose X is a compact Hausdorff space and A is a subalgebra of the space of continuou",0
"ize better to unseen data. In much recent work, this loss term falls into one of three classes (discussed further in Section 2): entropy minimization <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b27"">28]</ref>-which encourages the model to output confident predictions on unlabe ibution.</p><p>One way to enforce this is to require that the classifier output low-entropy predictions on unlabeled data. This is done explicitly in <ref type=""bibr"" target=""#b17"">[18]</ref> with a loss term which minimizes the entropy of p model (y | x; θ) for unlabeled data x. This form of entrop",1
"eled example x should be classified the same as Augment(x), an augmentation of itself.</p><p>In the simplest case, for unlabeled points x, prior work <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b39"">40]</ref> adds the loss term</p><formula xml:id=""formula_0"">p model (y | Augme algorithm 1, line 7. Using data augmentation to obtain an artificial target for an unlabeled example is common in consistency regularization methods <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b39"">40,</ref><ref type=""bibr"" target=""#b43"">44]</ref>.</p><p>Algorithm 1 MixMatch ike the cross-entropy, it is bounded and less sensitive to incorrect predictions. For this reason, it is often used as the unlabeled data loss in SSL <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b43"">44]</ref> as well as a measure of predictive uncertainty <ref type=""bibr"" targ predictive uncertainty <ref type=""bibr"" target=""#b25"">[26]</ref>. We do not propagate gradients through computing the guessed labels, as is standard <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b43"">44,</ref><ref type=""bibr"" target=""#b30"">31,</ref><ref type=""bibr"" target=""#b34 <head n=""4.2.1"">Baseline Methods</head><p>As baselines, we consider the four methods considered in <ref type=""bibr"" target=""#b34"">[35]</ref> (Π-Model <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b39"">40]</ref>, Mean Teacher <ref type=""bibr"" target=""#b43"">[44]</ref>, Virtual Adv",1
"ame as Augment(x), an augmentation of itself.</p><p>In the simplest case, for unlabeled points x, prior work <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b39"">40]</ref> adds the loss term</p><formula xml:id=""formula_0"">p model (y | Augment(x); θ) − p model (y | Augment(x); θ) 2 >As baselines, we consider the four methods considered in <ref type=""bibr"" target=""#b34"">[35]</ref> (Π-Model <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b39"">40]</ref>, Mean Teacher <ref type=""bibr"" target=""#b43"">[44]</ref>, Virtual Adversarial Training <ref type=""bibr"" target tion to obtain an artificial target for an unlabeled example is common in consistency regularization methods <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b39"">40,</ref><ref type=""bibr"" target=""#b43"">44]</ref>.</p><p>Algorithm 1 MixMatch takes a batch of labeled data X and a bat",1
"h penalizes the L 2 norm of the model parameters <ref type=""bibr"" target=""#b29"">[30,</ref><ref type=""bibr"" target=""#b45"">46]</ref>. We also use MixUp <ref type=""bibr"" target=""#b46"">[47]</ref> in MixMatch to encourage convex behavior ""between"" examples. We utilize MixUp as both as a regularizer (appl type=""bibr"" target=""#b30"">[31]</ref>, and Pseudo-Label <ref type=""bibr"" target=""#b27"">[28]</ref>) which are described in section 2. We also use MixUp <ref type=""bibr"" target=""#b46"">[47]</ref> on its own as a baseline. MixUp is designed as a regularizer for supervised learning, so we modify it for SS",1
"s may contain private information. In comparison, in many tasks it is much easier or cheaper to obtain unlabeled data.</p><p>Semi-supervised learning <ref type=""bibr"" target=""#b5"">[6]</ref> (SSL) seeks to largely alleviate the need for labeled data by allowing a model to leverage unlabeled data. Man #b33"">34,</ref><ref type=""bibr"" target=""#b41"">42]</ref>, etc.). More comprehensive overviews are provided in <ref type=""bibr"" target=""#b48"">[49,</ref><ref type=""bibr"" target=""#b5"">6]</ref>. In the following, we will refer to a generic model p model (y | x; θ) which produces a distribution over class",0
"el distribution. In practice, for the sharpening function, we use the common approach of adjusting the ""temperature"" of this categorical distribution <ref type=""bibr"" target=""#b15"">[16]</ref>, which is defined as the operation</p><formula xml:id=""formula_7"">Sharpen(p, T ) i := p 1 T i L j=1 p 1 T j",0
"the median error rate of the last 20 checkpoints. This simplifies the analysis at a potential cost to accuracy by, for example, averaging checkpoints <ref type=""bibr"" target=""#b1"">[2]</ref> or choosing the checkpoint with the lowest validation error.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0 in 26 million parameters. We also evaluate MixMatch on this larger model on CIFAR-100 with 10000 labels, to compare to the corresponding result from <ref type=""bibr"" target=""#b1"">[2]</ref>. The results are shown in table 1. In general, MixMatch matches or outperforms the best results from <ref type ult from <ref type=""bibr"" target=""#b1"">[2]</ref>. The results are shown in table 1. In general, MixMatch matches or outperforms the best results from <ref type=""bibr"" target=""#b1"">[2]</ref>, though we note that the comparison still remains problematic due to the fact that the model from <ref type=""b thod's sample efficiency which is central to SSL. CIFAR-10 and CIFAR-100 with a larger model Some prior work <ref type=""bibr"" target=""#b43"">[44,</ref><ref type=""bibr"" target=""#b1"">2]</ref> has also considered the use of a larger, 26 million-parameter model. Our base model, as used in <ref type=""bibr >[2]</ref>, though we note that the comparison still remains problematic due to the fact that the model from <ref type=""bibr"" target=""#b43"">[44,</ref><ref type=""bibr"" target=""#b1"">2]</ref>   <ref type=""table"">3</ref>: Comparison of error rates for SVHN and SVHN+Extra for MixMatch. The last column (""",0
">[49,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b28"">29]</ref>, generative modeling <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" target=""#b40"">41,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b16"">1",0
"=""4.2"">Semi-Supervised Learning</head><p>First, we evaluate the effectiveness of MixMatch on four standard benchmark datasets: CIFAR-10 and CIFAR-100 <ref type=""bibr"" target=""#b23"">[24]</ref>, SVHN <ref type=""bibr"" target=""#b31"">[32]</ref>, and STL-10 <ref type=""bibr"" target=""#b7"">[8]</ref>. Standar",0
"e=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" target=""#b40"">41,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b37"">38,</ref><ref type=""bibr"" target=""#b33"">34,</ref><ref type=""bibr"" target=""#b41""",0
"pe=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" target=""#b40"">41,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b37"">38,</ref><ref type=""bibr"" target=""#b33""",0
"olumn (""All"") contains the fully-supervised performance with all labels in the corresponding training set. sophisticated ""shake-shake"" regularization <ref type=""bibr"" target=""#b14"">[15]</ref>. For this model, we used a weight decay of 0.0008. We used λ U = 75 for CIFAR-10 and λ U = 150 for CIFAR-100",0
"oise to an input image, which can dramatically change the pixel content of an image without altering its label <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b42"">43,</ref><ref type=""bibr"" target=""#b9"">10]</ref>. Roughly speaking, this can artificially expand the size of a training",0
"y state-of-the-art and that MixMatch builds on; there is a wide literature on SSL techniques that we do not discuss here (e.g., ""transductive"" models <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b20"">21]</ref>, graph-based methods <ref ty",0
"its parts;</p><p>• We demonstrate in section 4.3 that MixMatch is useful for differentially private learning, enabling students in the PATE framework <ref type=""bibr"" target=""#b35"">[36]</ref> to obtain new state-of-the-art results that simultaneously strengthen both privacy guarantees and accuracy.< input and its label. Hence, approaches for deep learning from private training data, such as DP-SGD <ref type=""bibr"" target=""#b0"">[1]</ref> and PATE <ref type=""bibr"" target=""#b35"">[36]</ref>, benefit from accessing as few labeled private training points as possible when computing updates to the mod",0
"dard benchmark datasets: CIFAR-10 and CIFAR-100 <ref type=""bibr"" target=""#b23"">[24]</ref>, SVHN <ref type=""bibr"" target=""#b31"">[32]</ref>, and STL-10 <ref type=""bibr"" target=""#b7"">[8]</ref>. Standard practice for evaluating semi-supervised learning on the first three datasets is to treat most of the",0
"gment(x) is a stochastic transformation, so the two terms in eq. ( <ref type=""formula"" target=""#formula_0"">1</ref>) are not identical. ""Mean Teacher"" <ref type=""bibr"" target=""#b43"">[44]</ref> replaces one of the terms in eq. ( <ref type=""formula"" target=""#formula_0"">1</ref>) with the output of the m ing points for tuning. In all experiments, we linearly ramp up λ U to its maximum value over the first 16,000 steps of training as is common practice <ref type=""bibr"" target=""#b43"">[44]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4"">Experiments</head><p>We test the effectivene in <ref type=""bibr"" target=""#b34"">[35]</ref> (Π-Model <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b39"">40]</ref>, Mean Teacher <ref type=""bibr"" target=""#b43"">[44]</ref>, Virtual Adversarial Training <ref type=""bibr"" target=""#b30"">[31]</ref>, and Pseudo-Label <ref type=""bibr"" t 7% is the error limit obtained on our model with fully supervised learning. In addition, at 4000 labels the next-best-performing method (Mean Teacher <ref type=""bibr"" target=""#b43"">[44]</ref>) obtains an error rate of 10.36%, which suggests that MixMatch can achieve similar performance with only 1/1 ethods) across all amounts of labeled data. Surprisingly, after additional tuning we were able to obtain extremely good performance from Mean Teacher <ref type=""bibr"" target=""#b43"">[44]</ref>, though its error rate was consistently slightly higher than MixMatch's.</p><p>Note that SVHN has two traini (i.e. setting T = 1)</p><p>• using an exponential moving average (EMA) of model parameters when producing guessed labels, as is done by Mean Teacher <ref type=""bibr"" target=""#b43"">[44]</ref> • performing MixUp between labeled examples only, unlabeled examples only, and without mixing across labeled an unlabeled example is common in consistency regularization methods <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b39"">40,</ref><ref type=""bibr"" target=""#b43"">44]</ref>.</p><p>Algorithm 1 MixMatch takes a batch of labeled data X and a batch of unlabeled data U and produces a co less sensitive to incorrect predictions. For this reason, it is often used as the unlabeled data loss in SSL <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b43"">44]</ref> as well as a measure of predictive uncertainty <ref type=""bibr"" target=""#b25"">[26]</ref>. We do not propagate target=""#b25"">[26]</ref>. We do not propagate gradients through computing the guessed labels, as is standard <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b43"">44,</ref><ref type=""bibr"" target=""#b30"">31,</ref><ref type=""bibr"" target=""#b34"">35]</ref> </p></div> <div xmlns=""http:/ beled data points since it reveals the method's sample efficiency which is central to SSL. CIFAR-10 and CIFAR-100 with a larger model Some prior work <ref type=""bibr"" target=""#b43"">[44,</ref><ref type=""bibr"" target=""#b1"">2]</ref> has also considered the use of a larger, 26 million-parameter model. O esults from <ref type=""bibr"" target=""#b1"">[2]</ref>, though we note that the comparison still remains problematic due to the fact that the model from <ref type=""bibr"" target=""#b43"">[44,</ref><ref type=""bibr"" target=""#b1"">2]</ref>   <ref type=""table"">3</ref>: Comparison of error rates for SVHN and SV",0
"pe=""bibr"" target=""#b18"">[19]</ref>. We use weight decay which penalizes the L 2 norm of the model parameters <ref type=""bibr"" target=""#b29"">[30,</ref><ref type=""bibr"" target=""#b45"">46]</ref>. We also use MixUp <ref type=""bibr"" target=""#b46"">[47]</ref> in MixMatch to encourage convex behavior ""betwee",0
"29]</ref>, generative modeling <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" target=""#b40"">41,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b37"">3",0
"instance normalization mechanism <ref type=""bibr"" target=""#b13"">[14]</ref>, which was shown to be useful in large-scale conditional generation tasks <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b32"">34]</ref>.</p><p>The model-agnostic meta-learner (MAML) <ref type=""bibr"" target= oposed by Johnson et. al. <ref type=""bibr"" target=""#b17"">[19]</ref>, but replace downsampling and upsampling layers with residual blocks similarly to <ref type=""bibr"" target=""#b5"">[6]</ref> (with batch normalization [15] replaced by instance normalization <ref type=""bibr"" target=""#b34"">[36]</ref>). pe=""bibr"" target=""#b31"">[33]</ref> for all convolutional and fully connected layers in all the networks. We also use self-attention blocks, following <ref type=""bibr"" target=""#b5"">[6]</ref> and <ref type=""bibr"" target=""#b40"">[42]</ref>. They are inserted at 32×32 spatial resolution in all downsampli",1
"tional generation tasks <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b32"">34]</ref>.</p><p>The model-agnostic meta-learner (MAML) <ref type=""bibr"" target=""#b9"">[10]</ref> uses meta-learning to obtain the initial state of an image classifier, from which it can quickly converge to",0
"Face layers for real and fake images. We sum these losses with the weights equal to 1.5•10 −1 for VGG19 and 2.5•10 −2 for VGGFace terms. We use Caffe <ref type=""bibr"" target=""#b15"">[17]</ref> trained versions for both of these networks. For L FM , we use activations after each residual block of the",0
"tation of it is rather different. Several works have further proposed to combine adversarial training with meta-learning. Thus, data-augmentation GAN <ref type=""bibr"" target=""#b1"">[2]</ref>, Meta-GAN <ref type=""bibr"" target=""#b41"">[43]</ref>, adversarial meta-learning <ref type=""bibr"" target=""#b39"">",0
"c.org/ns/1.0""><head n=""4."">Experiments</head><p>Two datasets with talking head videos are used for quantitative and qualitative evaluation: VoxCeleb1 <ref type=""bibr"" target=""#b24"">[26]</ref> (256p videos at 1 fps) and VoxCeleb2 <ref type=""bibr"" target=""#b7"">[8]</ref>  than the former. VoxCeleb1 is",0
">(Real et al., 2017;</ref><ref type=""bibr"" target=""#b19"">Suganuma et al., 2017;</ref><ref type=""bibr"">Zoph &amp; Le, 2017)</ref>. More recent studies <ref type=""bibr"" target=""#b3"">(Brock et al., 2018;</ref><ref type=""bibr"" target=""#b18"">Shirakawa et al., 2018;</ref><ref type=""bibr"" target=""#b16"">Pha ecture search phase, we retrain the network with the most likely architecture, ? = argmax c p ? (c), from scratch, which is a commonly used technique <ref type=""bibr"" target=""#b3"">(Brock et al., 2018;</ref><ref type=""bibr"" target=""#b9"">Liu et al., 2019;</ref><ref type=""bibr"" target=""#b16"">Pham et al into one-shot NAS. On this line of the research, three different existing approaches are reported. The 1st category is based on a meta-network. SMASH <ref type=""bibr"" target=""#b3"">(Brock et al., 2018)</ref> employs HyperNet that takes an architecture  <ref type=""bibr"" target=""#b15"">(Pathak et al., 2",1
"2017)</ref>. More recent studies <ref type=""bibr"" target=""#b3"">(Brock et al., 2018;</ref><ref type=""bibr"" target=""#b18"">Shirakawa et al., 2018;</ref><ref type=""bibr"" target=""#b16"">Pham et al., 2018;</ref><ref type=""bibr"" target=""#b9"">Liu et al., 2019;</ref><ref type=""bibr"" target=""#b22"">Xie et al., 19;</ref><ref type=""bibr"" target=""#b22"">Xie et al., 2019)</ref> or stochastic relaxation <ref type=""bibr"" target=""#b18"">(Shirakawa et al., 2018;</ref><ref type=""bibr"" target=""#b16"">Pham et al., 2018)</ref>. A gradient descent or a natural gradient descent strategy with an existing adaptive step-size atch, which is a commonly used technique <ref type=""bibr"" target=""#b3"">(Brock et al., 2018;</ref><ref type=""bibr"" target=""#b9"">Liu et al., 2019;</ref><ref type=""bibr"" target=""#b16"">Pham et al., 2018)</ref> to improve final performance. In the retraining stage, we can exclude the redundant (unused) w nd adopt the standard preprocessing and data augmentation as done in the previous works, e.g., <ref type=""bibr"" target=""#b9"">Liu et al. (2019)</ref>; <ref type=""bibr"" target=""#b16"">Pham et al. (2018)</ref>. During the architecture search, we split the training dataset into halves as D = {D x , D ? } calculating (12). Namely, the computational cost of the ? update is less than that of x.</p><p>Search Space: The search space is based on the one in <ref type=""bibr"" target=""#b16"">Pham et al. (2018)</ref>, which consists of models obtained by connecting two motifs (called normal cell and reduction ""#b20"">Suganuma et al. (2018)</ref>. The weights of HyperNet is then optimized by backprop while c is randomly chosen during architecture search. ENAS<ref type=""bibr"" target=""#b16"">(Pham et al., 2018)</ref> employs a recurrent neural network (RNN) to generate a sequence of categorical variables c re",0
"rogeneous networks. We also give the theoretical analysis to prove that our transductive model is a more general form than existing models (e.g., MNE <ref type=""bibr"" target=""#b42"">[43]</ref>). • Efficient and scalable learning algorithms for GATNE have been developed. Our learning algorithms are ab . MVE <ref type=""bibr"" target=""#b29"">[30]</ref> embeds networks with multiple views in a single collaborated embedding using attention mechanism. MNE <ref type=""bibr"" target=""#b42"">[43]</ref> uses one common embedding and several additional embeddings of each edge type for each node, which are joint dge embeddings towards the overall embedding and M r ∈ R s×d is a trainable transformation matrix.</p><p>Connection with Previous Work. We choose MNE <ref type=""bibr"" target=""#b42"">[43]</ref>, a recent representative work for MHEN, as the base model for multiplex heterogeneous networks to discuss th ing Methods.</head><p>The compared methods include PMNE <ref type=""bibr"" target=""#b21"">[22]</ref>, MVE <ref type=""bibr"" target=""#b29"">[30]</ref>, MNE <ref type=""bibr"" target=""#b42"">[43]</ref>. We denote the three methods of PMNE as PMNE(n), PMNE(r) and PMNE(c) respectively. MVE uses collaborated con",1
"ch as node classification <ref type=""bibr"" target=""#b0"">[1]</ref>, link prediction <ref type=""bibr"" target=""#b38"">[39]</ref>, and community detection <ref type=""bibr"" target=""#b7"">[8]</ref>. DeepWalk <ref type=""bibr"" target=""#b26"">[27]</ref>, LINE <ref type=""bibr"" target=""#b34"">[35]</ref>, and node2",0
"e, in the four datasets that we are working with, there are 20.3% (Twitter), 21.6% (YouTube), <ref type=""bibr"" target=""#b14"">15</ref>.1% (Amazon) and <ref type=""bibr"" target=""#b15"">16</ref>.3% (Alibaba) of the linked node pairs having more than one type of edges respectively. As an instance, in an e AHON) TADW <ref type=""bibr"" target=""#b40"">[41]</ref> </p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>Single</head><p>Single Attributed LANE <ref type=""bibr"" target=""#b15"">[16]</ref> AANE <ref type=""bibr"" target=""#b14"">[15]</ref> SNE <ref type=""bibr"" target=""#b19"">[20]</ref> DANE <ref type= arget=""#b40"">[41]</ref> incorporates text features of vertices into network representation learning under the framework of matrix factorization. LANE <ref type=""bibr"" target=""#b15"">[16]</ref> smoothly incorporates label information into the attributed network embedding while preserving their correla r ) i j , where r corresponds to a certain edge type. Definition 2 (Attributed Network). An attributed network <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b15"">16]</ref> is a network G endowed with an attribute representation A, i.e., G = (V, E, A). Each node v i ∈ V is associat",0
"tween their users and items for recommendations. We hide a set of edges/non-edges from the original graph and train on the remaining graph. Following <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b17"">18]</ref>, we create a validation/test set that contains 5%/10% randomly selecte",0
"ype=""bibr"" target=""#b34"">35]</ref>, we use random walk to generate node sequences and then perform skip-gram <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b24"">25]</ref> over the node sequences to learn embeddings. Since each view of the input network is heterogeneous, we use me",0
"vision, where applications are ranging from audio-to-video generation <ref type=""bibr"" target=""#b27"">[28,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b1"">2]</ref> to text-to-video generation <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b18"">19]</ref> a",1
"between facial movements and speech audio. However, recent researchers <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b16"">17]</ref> tended to formulate video generation as a temporally independent image generation problem. For example, Chung",0
"Face Synthesizing The success of traditional approaches has been mainly limited to synthesizing a talking face from speech audio of a specific person <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b28"">29]</ref>. For example, Suwajanakorn et",0
"/head><p>Attention mechanism is an emerging topic in natural language tasks <ref type=""bibr"" target=""#b19"">[20]</ref> and image/video generation task <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b36"">37,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b35 "">[26,</ref><ref type=""bibr"" target=""#b36"">37,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b35"">36]</ref>. Pumarola et al. <ref type=""bibr"" target=""#b25"">[26]</ref> generated facial expression conditioned on action units annotations. Instead of using a basic GAN structure, vel dynamic pixel-wise loss to enforce the generator to generate consistent pixels along temporal axis.</p><p>As mentioned in Sec. 2, Pumarola et al. <ref type=""bibr"" target=""#b25"">[26]</ref> exploited a generator that regresses an attention mask and a RGB color transformation over the entire image.",0
"space <ref type=""bibr"" target=""#b29"">[30]</ref>, will result in low-quality videos. For example, in web videos <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b23"">24]</ref> (e.g., LRW and VoxCeleb datasets), speakers move significantly when they are talking. Nonetheless, all the re he-art methods on several popular datasets (e.g., GRID <ref type=""bibr"" target=""#b5"">[6]</ref>, LRW <ref type=""bibr"" target=""#b4"">[5]</ref>, VoxCeleb <ref type=""bibr"" target=""#b23"">[24]</ref> and TCD <ref type=""bibr"" target=""#b12"">[13]</ref>). Experimental results show that our model outperforms all videos are selected from different sources: we randomly select samples from the testing set of LRW <ref type=""bibr"" target=""#b4"">[5]</ref>, VoxCeleb <ref type=""bibr"" target=""#b23"">[24]</ref>, TCD <ref type=""bibr"" target=""#b12"">[13]</ref>, GRID <ref type=""bibr"" target=""#b5"">[6]</ref> and realworld s",0
"</profileDesc> 	</teiHeader> 	<text xml:lang=""en""> 		<body> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""1"">INTRODUCTION</head><p>Score matching <ref type=""bibr"" target=""#b12"">(Hyvärinen, 2005)</ref> is particularly suitable for learning unnormalized statistical models, such as energy based one alized models with maximum likelihood estimation (MLE) can be difficult due to the intractable partition function Z θ . To avoid this, score matching <ref type=""bibr"" target=""#b12"">(Hyvärinen, 2005)</ref> minimizes the Fisher divergence between p d and p m (•, θ), which is defined as</p><formula xml nnormalized models, as we only have samples and do not have access to the score function of the data s d (x).</p><p>By applying integration by parts, <ref type=""bibr"" target=""#b12"">Hyvärinen (2005)</ref> shows that L(θ) can be written as L(θ) = J(θ) + C (cf ., Theorem 1 in Hyvärinen ( <ref type=""for _12"">)</formula><p>where C is a constant w.r.t. θ.</p><p>Other than our requirements on p v , the assumptions are exactly the same as in Theorem 1 of <ref type=""bibr"" target=""#b12"">Hyvärinen (2005)</ref>. We advise the interested readers to read Appendix B.2 for technical statements of the assumptio These are standard assumptions used for proving the consistency of <ref type=""bibr"">MLE (van der Vaart, 1998)</ref>. We also adopt the assumption in <ref type=""bibr"" target=""#b12"">Hyvärinen (2005)</ref> that all densities are strictly positive (Assumption 5). Finally, we assume that p m (x; θ) has (Lemma 3), which</formula><p>holds regardless of M . These two facts lead to consistency. For a complete proof, see Appendix B.3.</p><p>Remark 1. In <ref type=""bibr"" target=""#b12"">Hyvärinen (2005)</ref>, the authors only showed that J(θ) = 0 ⇔ θ = θ * , which leads to ""local consistency"" of score m ormula xml:id=""formula_38"">)</formula><p>where C is a constant w.r.t. θ.</p><p>Proof. The basic idea of this proof is similar to that of Theorem 1 in <ref type=""bibr"" target=""#b12"">Hyvärinen (2005)</ref>. First, note that L(θ, p v ) can be expanded to</p><formula xml:id=""formula_39"">L(θ, p v ) = 1 2",1
"thus can be easily and efficiently implemented in frameworks such as TensorFlow <ref type=""bibr"" target=""#b0"">(Abadi et al., 2016)</ref> and PyTorch <ref type=""bibr"" target=""#b1"">(Adam et al., 2017)</ref>.</p><p>Beyond training unnormalized models, sliced score matching can also be naturally adapte",0
"osteriors and the prior. In both situations we outperformed kernel-based score estimators <ref type=""bibr"" target=""#b16"">(Li &amp; Turner, 2018;</ref><ref type=""bibr"" target=""#b25"">Shi et al., 2018)</ref> by achieving better test likelihoods and better sample quality in image generation.</p></div> < ormula"" target=""#formula_31"">12</ref>) to obtain estimates of ∇ x log q(x) at the sample points. We refer to this method as Stein in the experiments. <ref type=""bibr"" target=""#b25"">Shi et al. (2018)</ref> adopt a different approach but also exploit (12). They build their estimator by expanding ∇ x l a single data point x, kernel score estimators need multiple samples from q φ (z | x) to estimate its score. On MNIST, we use 100 samples, as done in <ref type=""bibr"" target=""#b25"">Shi et al. (2018)</ref>. On CelebA, however, we can only take 20 samples due to GPU memory limitations. In contrast, SS ble options for q φ . We consider 3 score estimation techniques: SSM, Stein <ref type=""bibr"" target=""#b16"">(Li &amp; Turner, 2018)</ref> and Spectral <ref type=""bibr"" target=""#b25"">(Shi et al., 2018)</ref>.</p><p>For a single data point x, kernel score estimators need multiple samples from q φ (z |",0
"stribution is highly non-trivial and in practice the performance can be very sensitive to σ, and heuristics have to be used in practice. For example, <ref type=""bibr"" target=""#b23"">Saremi et al. (2018)</ref> propose a heuristic for choosing σ based on Parzen windows.</p><p>Approximate Backpropagatio tperforms all the other methods on the SM loss. DSM performs worse than SSM-VR, and σ is still hard to tune. Specifically, following the heuristic in <ref type=""bibr"" target=""#b23"">Saremi et al. (2018)</ref> leads to σ = 1.74, which performed the worst (on both log-likelihood and SM loss) of the eig eight values are used: <ref type=""bibr"">[0.01, 0.05, 0.10, 0.20, 0.28, 0.50, 1.00, 1.50]</ref>. We also evaluate σ = 1.74, chosen by the heuristic in <ref type=""bibr"" target=""#b23"">Saremi et al. (2018)</ref>. The model with the best performance on validation score matching loss is used. Only nine va",0
"on the graph clustering idea, we proposed Cluster-GCN, an algorithm to design the batches based on efficient graph clustering algorithms (e.g., METIS <ref type=""bibr"" target=""#b8"">[8]</ref>). We take this idea further by proposing a stochastic multi-clustering framework to improve the convergence of used in previous SGD-based training methods.</p><p>We use graph clustering algorithms to partition the graph. Graph clustering methods such as Metis <ref type=""bibr"" target=""#b8"">[8]</ref> and Graclus <ref type=""bibr"" target=""#b4"">[4]</ref> aim to construct the partitions over the vertices in the g",1
"eiHeader> 	<text xml:lang=""en""> 		<body> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""1"">INTRODUCTION</head><p>Graph convolutional network (GCN) <ref type=""bibr"" target=""#b9"">[9]</ref> has become increasingly popular in addressing many graph-based applications, including semi-supervised node cl br"" target=""#b9"">[9]</ref> has become increasingly popular in addressing many graph-based applications, including semi-supervised node classification <ref type=""bibr"" target=""#b9"">[9]</ref>, link prediction <ref type=""bibr"" target=""#b17"">[17]</ref> and recommender systems <ref type=""bibr"" target=""#b ension, and L the number of layers to analyze classic GCN training algorithms.</p><p>• Full-batch gradient descent is proposed in the first GCN paper <ref type=""bibr"" target=""#b9"">[9]</ref>. To compute the full gradient, it requires storing all the intermediate embeddings, leading to O(N F L) memory erwise. Also, each node is associated with an F -dimensional feature vector and X ∈ R N ×F denotes the feature matrix for all N nodes. An L-layer GCN <ref type=""bibr"" target=""#b9"">[9]</ref> consists of L graph convolution layers and each of them constructs embeddings for each node by mixing the embe y we omit the memory for storing the graph (GCN) or sub-graphs (other approaches) since they are fixed and usually not the main bottleneck.</p><p>GCN <ref type=""bibr"" target=""#b9"">[9]</ref> Vanilla SGD GraphSAGE <ref type=""bibr"" target=""#b5"">[5]</ref> FastGCN <ref type=""bibr"" target=""#b1"">[1]</ref> emory complexity O(LN F + LF 2 ) O(bd L F + LF 2 ) O(br L F + LF 2 ) O(brLF + LF 2 ) O(LN F + LF 2 ) O(bLF + LF 2 )</formula><p>In the original paper <ref type=""bibr"" target=""#b9"">[9]</ref>, full gradient descent is used for training GCN, but it suffers from high computational and memory cost. In te embedding utilization-each embedding will be reused d (average degree) times in the upper layer. As a consequence, the original full gradient descent <ref type=""bibr"" target=""#b9"">[9]</ref> only needs to compute O(N L) embeddings per epoch, which means on average only O(L) embedding computation is n 1.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.3"">Issues of training deeper GCNs</head><p>Previous attempts of training deeper GCNs <ref type=""bibr"" target=""#b9"">[9]</ref> seem to suggest that adding more layers is not helpful. However, the datasets used in the experiments may be t t that adding more layers is not helpful. However, the datasets used in the experiments may be too small to make a proper justification. For example, <ref type=""bibr"" target=""#b9"">[9]</ref> considered a graph with only a few hundreds of training nodes for which overfitting can be an issue. Moreover, bserve that the optimization of deep GCN models becomes difficult as it may impede the information from the first few layers being passed through. In <ref type=""bibr"" target=""#b9"">[9]</ref>, they adopt a technique similar to residual connections <ref type=""bibr"" target=""#b6"">[6]</ref> to enable the thod in PyTorch <ref type=""bibr"" target=""#b13"">[13]</ref>. For the other methods, we use all the original papers' code from their github pages. Since <ref type=""bibr"" target=""#b9"">[9]</ref> has difficulty to scale to large graphs, we do not compare with it here. Also as shown in <ref type=""bibr"" tar",1
"oposed by <ref type=""bibr"" target=""#b5"">[5]</ref> is adopted and the number of hidden units is the same for all methods. Note that techniques such as <ref type=""bibr"" target=""#b11"">(11)</ref> is not considered here. In each experiment, we consider the same GCN architecture for all methods. For VRGCN e show a detailed convergence of a 8-layer GCN in Figure <ref type=""figure"" target=""#fig_3"">5</ref>. With the proposed diagonal enhancement technique <ref type=""bibr"" target=""#b11"">(11)</ref>, the convergence can be improved significantly and similar accuracy can be achieved.</p><p>State-of-the-art GCN training algorithms, we constructed a much larger graph with over 2 millions of nodes and 61 million edges based on Amazon co-purchasing networks <ref type=""bibr"" target=""#b11"">[11,</ref><ref type=""bibr"" target=""#b12"">12]</ref>. The raw co-purchase data is from Amazon-3M <ref type=""foot"" target=",0
"Amazon from the website 8 . Note that for Amazon, we consider GCN in an inductive setting, meaning that the model only learns from training data. In <ref type=""bibr"" target=""#b3"">[3]</ref> they consider a transductive setting. Regarding software versions, we install CUDA 10.0 and cuDNN 7.0. TensorF",0
"much larger graph with over 2 millions of nodes and 61 million edges based on Amazon co-purchasing networks <ref type=""bibr"" target=""#b11"">[11,</ref><ref type=""bibr"" target=""#b12"">12]</ref>. The raw co-purchase data is from Amazon-3M <ref type=""foot"" target=""#foot_5"">6</ref> . In the graph, each no",0
"= 1 |V t | i ∈V t loss(y i , z (L) i ). (7)</formula><p>The Cluster-GCN is then based on the decomposition form in ( <ref type=""formula"">6</ref>) and <ref type=""bibr"" target=""#b7"">(7)</ref>. At each step, we sample a cluster V t and then conduct SGD to update based on the gradient of L Ā′ t t</p><p> ed together. Each node feature is generated by extracting bag-of-word features from the product descriptions followed by Principal Component Analysis <ref type=""bibr"" target=""#b7"">[7]</ref> to reduce the dimension to be 100. In addition, we use the top-level categories as the labels for that product",0
"low since the parameters are updated only once per epoch.</p><p>[memory: bad; time per epoch: good; convergence: bad] • Mini-batch SGD is proposed in <ref type=""bibr"" target=""#b5"">[5]</ref>. Since each update is only based on a mini-batch gradient, it can reduce the memory requirement and conduct ma ghbors' embeddings at layer L − 2 and recursive ones in the downstream layers. This leads to time complexity exponential to the GCN depth. Graph-SAGE <ref type=""bibr"" target=""#b5"">[5]</ref> proposed to use a fixed size of neighborhood samples during back-propagation through layers and FastGCN <ref t (other approaches) since they are fixed and usually not the main bottleneck.</p><p>GCN <ref type=""bibr"" target=""#b9"">[9]</ref> Vanilla SGD GraphSAGE <ref type=""bibr"" target=""#b5"">[5]</ref> FastGCN <ref type=""bibr"" target=""#b1"">[1]</ref> VR-GCN <ref type=""bibr"" target=""#b2"">[2]</ref> Cluster-GCN Tim ni-batch SGD work, previous approaches try to restrict the neighborhood expansion size, which however do not improve embedding utilization. GraphSAGE <ref type=""bibr"" target=""#b5"">[5]</ref> uniformly samples a fixed-size set of neighbors, instead of using a full-neighborhood set. We denote the sampl uggested in <ref type=""bibr"" target=""#b2"">[2]</ref> <ref type=""foot"" target=""#foot_3"">4</ref> . • GraphSAGE<ref type=""foot"" target=""#foot_4"">5</ref>  <ref type=""bibr"" target=""#b5"">[5]</ref>: It samples a fixed number of neighbors per node. We use the default settings of sampled sizes for each layer For all the methods we use the Adam optimizer with learning rate as 0.01, dropout rate as 20%, weight decay as zero. The mean aggregator proposed by <ref type=""bibr"" target=""#b5"">[5]</ref> is adopted and the number of hidden units is the same for all methods. Note that techniques such as <ref type= ve the training speed and memory requirement of GCN in some recent works <ref type=""bibr"" target=""#b1"">[1,</ref><ref type=""bibr"" target=""#b2"">2,</ref><ref type=""bibr"" target=""#b5"">5]</ref>. Instead of computing the full gradient, SGD only needs to calculate the gradient based on a mini-batch for eac",0
"-SAGE <ref type=""bibr"" target=""#b5"">[5]</ref> proposed to use a fixed size of neighborhood samples during back-propagation through layers and FastGCN <ref type=""bibr"" target=""#b1"">[1]</ref> proposed importance sampling, but the overhead of these methods is still large and will become worse when GCN ally not the main bottleneck.</p><p>GCN <ref type=""bibr"" target=""#b9"">[9]</ref> Vanilla SGD GraphSAGE <ref type=""bibr"" target=""#b5"">[5]</ref> FastGCN <ref type=""bibr"" target=""#b1"">[1]</ref> VR-GCN <ref type=""bibr"" target=""#b2"">[2]</ref> Cluster-GCN Time complexity</p><formula xml:id=""formula_3"">O(L∥ note the sample size as r . This leads to O(r L ) embedding computations for each loss term but also makes gradient estimation less accurate. FastGCN <ref type=""bibr"" target=""#b1"">[1]</ref> proposed an important sampling strategy to improve the gradient estimation. VR-GCN <ref type=""bibr"" target=""#b ore epochs to converge.</p><p>It has been shown that mini-batch SGD can improve the training speed and memory requirement of GCN in some recent works <ref type=""bibr"" target=""#b1"">[1,</ref><ref type=""bibr"" target=""#b2"">2,</ref><ref type=""bibr"" target=""#b5"">5]</ref>. Instead of computing the full gra Python wrapper 10 for METIS library.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""6.2"">Implementation details</head><p>Previous works <ref type=""bibr"" target=""#b1"">[1,</ref><ref type=""bibr"" target=""#b2"">2]</ref> propose to pre-compute the multiplication of AX in the first GCN layer. the same as in Section 4.1. The numbers marked red indicate poor convergence.2-layer 3-layer 4-layer 5-layer 6-layer 7-layer 8-layer Cluster-GCN with<ref type=""bibr"" target=""#b1"">(1)</ref> </figDesc><table><row><cell></cell><cell>90.3</cell><cell>97.6</cell><cell>98.2</cell><cell>98.3</cell><cell>9",0
"achieved excellent results on several benchmark datasets and continuously set new stateof-the-art performance <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b28"">29]</ref>. Started with the early succe x I − L with the feature matrix X . The product (I − L)X is understood as features averaging and propagation <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b27"">28]</ref>. In graph signal processing literature, such operation filters signal n to the true signal optimization problem. In contrast to the recent design principle of graph neural networks <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b28"">29]</ref>, our results suggest that the graph convolution layer is simply low-p t=""#b25"">26,</ref><ref type=""bibr"" target=""#b28"">29]</ref>. Started with the early success of ChebNet <ref type=""bibr"" target=""#b5"">[6]</ref> and GCN <ref type=""bibr"" target=""#b15"">[16]</ref> at vertex classification, many variants of GNN have been proposed to solve problems in social networks <ref </p><p>In semi-supervised vertex classification, we observe that the parameters of a graph convolutional layer in a Graph Convolutional Network (GCN) <ref type=""bibr"" target=""#b15"">[16]</ref> only contribute to overfitting. Similar observations have been reported in both simple architectures such as terize  the graph neural networks solution to this problem and provide insights to the mechanism underlying the most commonly used baseline model GCN <ref type=""bibr"" target=""#b15"">[16]</ref>, and its simplified variant SGC <ref type=""bibr"" target=""#b27"">[28]</ref>.</p><p>Graph signal processing (GS GCN, and gfNN are similar to those of the corresponding NNs using true features.</p><p>Theorem 7 implies that, under Assumption 1, both gfNN and GCN <ref type=""bibr"" target=""#b15"">[16]</ref> have similar high performance. Since gfNN does not require multiplications of the adjacency matrix during th ion ( ) simplifies the Graph Convolutional Neural Network model by removing nonlinearity in the neural network and only averaging features.</p><p>GCN <ref type=""bibr"" target=""#b15"">[16]</ref> Graph Convolutional Neural Network ($) is the most commonly used baseline.  The noise level is measured by s",1
"GCN model to statistical physics. They concluded that backpropagation improves neither accuracy nor detectability of a GCN-based GNN model. Li et al. <ref type=""bibr"" target=""#b17"">[18]</ref> empirically analyzed GCN models with many layers under the limited labeled data setting and stated that GCN",1
"tion networks, social networks, and biological networks) commonly used for graph neural network benchmarking <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b29"">30,</ref><ref type=""bibr"" target=""#b32"">33]</ref> and artificially synthesized random graphs from the two circles datas",0
"GCN <ref type=""bibr"" target=""#b15"">[16]</ref> at vertex classification, many variants of GNN have been proposed to solve problems in social networks <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b30"">31]</ref>, biology <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" t he (augmented) normalized adjacency matrix I − L with the feature matrix X . The product (I − L)X is understood as features averaging and propagation <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b27"">28]</ref>. In graph signal processing",0
"ication and graph isomorphism test have achieved excellent results on several benchmark datasets and continuously set new stateof-the-art performance <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b28""> -pass filters is the analytical solution to the true signal optimization problem. In contrast to the recent design principle of graph neural networks <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b28"">29]</ref>, our results suggest that the ts suggest that the graph convolutional layer should be thought of simply as a denoising filter. In contrast to the recent design trend involving GCN <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b23"">24]</ref> , our results imply that simply stacking GCN layers might only make th ) Z D ρ(W 1 )ρ(W 2 ).</figDesc></figure> <figure xmlns=""http://www.tei-c.org/ns/1.0"" type=""table"" xml:id=""tab_0""><head></head><label></label><figDesc><ref type=""bibr"" target=""#b0"">1</ref>. Compute the graph Fourier basis U from L; 2. Add Gaussian noise to the input features: X ← X + N (0, σ</figDesc",0
"ry <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b9"">10]</ref>, natural language processing <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b31"">32]</ref>, computer vision <ref type=""bibr"" target=""#b20"">[21]</ref>, and weakly-supervised learning <ref type=""bibr"" t",0
". SGC or GCN) fail to perform?</p><p>In this study, we provide an answer to the aforementioned questions from the graph signal processing perspective <ref type=""bibr"" target=""#b18"">[19]</ref>. Formally, we consider a semi-supervised learning problem on a graph. Given a graph G = (V, E), each vertex",0
"ince the maximum eigenvalue of the normalized Laplacian is 2 if and only if the graph contains a non-trivial bipartite graph as a connected component <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"">Lemma 1.7]</ref>. Therefore, for other graphs, multiplying the normalized (non-augmented) adja",0
"λ) = h(λ)x(λ). In the spatial domain, the above equation is equivalent to y = h( Lrw )x. where h( Lrw ) is defined via the Taylor expansion of h; see <ref type=""bibr"" target=""#b12"">[13]</ref> for the detail of matrix functions.</p><p>In a machine learning problem on a graph, each vertex i ∈ V has a",0
"7,</ref><ref type=""bibr"" target=""#b33"">34]</ref>, and hybrid methods <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b27"">28]</ref>. However, these approaches rely on manual feature engineering, are unable to perform end-to-end training, and /meta-graphs, which are hard to tune in practice. (3) Hybrid methods <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b27"">28]</ref> combine the above two categories and learn user/item embeddings by exploiting the structure of KGs. Our propo y, several works developed GNNs architecture for recommender systems <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b30"">31,</ref><ref type=""bibr"" target=""#b31"">32]</ref>, but these approaches are mos elationships between the items as well as personalized user preferences and interests. To account for the relational heterogeneity in KGs, similar to <ref type=""bibr"" target=""#b27"">[28]</ref>, we use a trainable and personalized relation scoring function that transforms the KG into a user-specific w us bipartite graphs or user/item-similarity graphs where GNNs can be used directly, while here we investigate GNNs for heterogeneous KGs. Wang et al. <ref type=""bibr"" target=""#b27"">[28]</ref> use GCNs in KGs for recommendation, but simply applying GCNs to KGs without proper regularization is prone t approach is to transform a heterogeneous KG into a user-personalized weighted graph, which characterizes user's preferences. To this end, similar to <ref type=""bibr"" target=""#b27"">[28]</ref>, we use a user-specific relation scoring function s u (r ) that provides the importance of relation r for us ef type=""figure"" target=""#fig_4"">5</ref>. It is clear that the performance of KGNN-LS with a non-zero λ is better than λ = 0 (the case of Wang et al. <ref type=""bibr"" target=""#b27"">[28]</ref>), which justifies our claim that LS regularization can assist learning the edge weights in a KG and achieve",1
"or regularization of edge weights during the learning process, which leads to better generalization. We develop an approach based on label smoothness <ref type=""bibr"" target=""#b34"">[35,</ref><ref type=""bibr"" target=""#b37"">38]</ref>, which assumes that adjacent entities in the KG are likely to have s 38]</ref>; (2) Edge weights are parameterized and therefore learnable <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b34"">35]</ref>. Inspired by these methods, we design a module of label smoothness regularization in our proposed model. The a large amount of prior works <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b34"">35,</ref><ref type=""bibr"" target=""#b37"">38]</ref>. Therefore, more regularization on edge weights is needed to assist t >Moreover, we do not know true relevancy labels for the unlabeled nodes l * u (E\V). To solve the issue, we propose minimizing the leave-one-out loss <ref type=""bibr"" target=""#b34"">[35]</ref>. Suppose we hold out a single item v and treat it unlabeled. Then we predict its label by using the rest of",1
"d><p>MovieLens-20M Book-Crossing Last.FM Dianping-Food R@2 R@10 R@50 R@100 R@2 R@10 R@50 R@100 R@2 R@10 R@50 R@100 R@2 R@10 R@50 R@100 SVD 0. • LibFM <ref type=""bibr"" target=""#b15"">[16]</ref> is a widely used feature-based factorization model for CTR prediction. We concatenate user ID and item ID as",0
"ly flexible in utilizing KGs to assist recommender systems, but the KGE algorithms focus more on modeling rigorous semantic relatedness (e.g., TransE <ref type=""bibr"" target=""#b1"">[2]</ref> assumes head +relation = tail), which are more suitable for graph applications such as link prediction rather , 8} and the number of training epochs is 50 for all datasets. • LibFM + TransE extends LibFM by attaching an entity representation learned by TransE <ref type=""bibr"" target=""#b1"">[2]</ref> to each user-item pair. The dimension of TransE is 32 for all datasets. • PER <ref type=""bibr"" target=""#b32"">[ model to select K items with highest predicted click probability for each user in the test set, and choose Recall@K to evaluate the recommended sets. <ref type=""bibr"" target=""#b1"">(2)</ref> In click-through rate (CTR) prediction, we apply the trained model to predict each piece of user-item pair in",0
"t been heavily explored by the users. The sparsity issue can be addressed by introducing additional sources of information such as user/item profiles <ref type=""bibr"" target=""#b22"">[23]</ref> or social networks <ref type=""bibr"" target=""#b21"">[22]</ref>.</p><p>Knowledge graphs (KGs) capture structure",0
"pe=""bibr"" target=""#b31"">32]</ref>. Traditional recommender systems that are based on collaborative filtering <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b21"">22]</ref> usually suffer from the coldstart problem and have trouble recommending brand new items that have not yet bee be addressed by introducing additional sources of information such as user/item profiles <ref type=""bibr"" target=""#b22"">[23]</ref> or social networks <ref type=""bibr"" target=""#b21"">[22]</ref>.</p><p>Knowledge graphs (KGs) capture structured information and relations between a set of entities <ref ty",0
"ization is prone to overfitting and leads to performance degradation as we will show later. Schlichtkrull et al. also propose using GNNs to model KGs <ref type=""bibr"" target=""#b16"">[17]</ref>, but not for the purpose of recommendations.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2.2",0
"ture for recommender systems <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b30"">31,</ref><ref type=""bibr"" target=""#b31"">32]</ref>, but these approaches are mostly designed for homogeneous bipartite u ""#b18"">[19]</ref> model recommender systems as matrix completion and design GNNs for representation learning on user-item bipartite graphs. Wu et al. <ref type=""bibr"" target=""#b30"">[31]</ref> use GNNs on user/item structure graphs to learn user/item representations. The difference between these work",0
"first two baselines are KG-free while the rest are all KG-aware methods. The hyperparameter setting of KGNN-LS is provided in Appendix B.</p><p>• SVD <ref type=""bibr"" target=""#b11"">[12]</ref> is a classic CF-based model using inner product to model user-item interactions. We use the unbiased version",0
"sers and items for reconstructing historical interactions, and predict user preference based on the parameters <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b13"">14]</ref>.</p><p>Generally speaking, there are two key components in learnable CF models -1) embedding, which transform "" target=""#b29"">[30]</ref>; neural collaborative filtering models replace the MF interaction function of inner product with nonlinear neural networks <ref type=""bibr"" target=""#b13"">[14]</ref>; and translation-based CF models instead use Euclidean distance metric as the interaction function <ref type eddings are insufficient in capturing CF, the methods have to rely on the interaction function to make up for the deficiency of suboptimal embeddings <ref type=""bibr"" target=""#b13"">[14]</ref>.</p><p>While intuitively useful to integrate user-item interactions into the embedding function, it is non-t mbeddings and item embeddings, to be optimized in an end-to-end fashion. In traditional recommender models like MF and neural collaborative filtering <ref type=""bibr"" target=""#b13"">[14]</ref>, these ID embeddings are directly fed into an interaction layer (or operator) to achieve the prediction scor thus only employ the simple interaction function of inner product. Other more complicated choices, such as neural network-based interaction functions <ref type=""bibr"" target=""#b13"">[14]</ref>, are left to explore in the future work.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2.4"">Op the interaction function, so as to capture the nonlinear feature interactions between users and items. For instance, neural CF models, such as NeuMF <ref type=""bibr"" target=""#b13"">[14]</ref>, employ nonlinear neural networks as the interaction function; meanwhile, translationbased CF models, such a Bayesian personalized ranking (BPR) loss, which exploits the user-item direct interactions only as the target value of interaction function. • NeuMF <ref type=""bibr"" target=""#b13"">[14]</ref>: The method is a state-of-the-art neural CF model which uses multiple hidden layers above the element-wise a s=""http://www.tei-c.org/ns/1.0""><head n=""2.1"">Embedding Layer</head><p>Following mainstream recommender models <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b25"">26]</ref>, we describe a user u (an item i) with an embedding vector e u ∈ R d v xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.1"">Model-based CF Methods</head><p>Modern recommender systems <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b31"">32]</ref> parameterize users and items by vectorized representations and recons pe=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b14"">15]</ref>. Towards this end, recent efforts <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b34"">35]</ref> focus on exploiting deep lear served interaction close to each other, its linearity makes it insufficient to reveal the complex and nonlinear relationships between users and items <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b14"">15]</ref>. Towards this end, recent efforts <ref type=""bibr"" target=""#b10"">[11 used in the training set. To evaluate the effectiveness of top-K recommendation and preference ranking, we adopt two widely-used evaluation protocols <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b38"">39]</ref>: recall@K and ndcg@K. By default, we set K = 20. We report the avera",1
"recommendation methods that also employ graph convolution operations <ref type=""bibr"" target=""#b28"">[29,</ref><ref type=""bibr"" target=""#b40"">41,</ref><ref type=""bibr"" target=""#b41"">42]</ref>. GC-MC <ref type=""bibr"" target=""#b28"">[29]</ref> applies the graph convolution network (GCN) <ref type=""bibr"" terest image recommendation. As such, the CF effect is captured on the level of item relations, rather than the collective user behaviors. SpectralCF <ref type=""bibr"" target=""#b41"">[42]</ref> proposes a spectral convolution operation to discover all possible connectivity between users and items in t the hidden dimension is set as the embedding size, is used as suggested in <ref type=""bibr"" target=""#b28"">[29]</ref>.</p><p>We also tried SpectralCF <ref type=""bibr"" target=""#b41"">[42]</ref> but found that the eigen-decomposition leads to high time cost and resource cost, especially when the number",1
">2 } L l =1 } denotes all trainable model parameters, and λ controls the L 2 regularization strength to prevent overfitting. We adopt mini-batch Adam <ref type=""bibr"" target=""#b16"">[17]</ref> to optimize the prediction model and update the model parameters. In particular, for a batch of randomly sam",0
"model parameters, we optimize the pairwise BPR loss <ref type=""bibr"" target=""#b25"">[26]</ref>, which has been intensively used in recommender systems <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b12"">13]</ref>.</p><p>It considers the relative order between observed and unobserved een them to predict an interaction. To enhance the embedding function, much effort has been devoted to incorporate side information like item content <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b29"">30]</ref>, social relations <ref type=""bibr"" target=""#b32"">[33]</ref>, item rela ns (i.e., e u ⊙ e i ), which makes messages being propagated dependent on the affinity between e i and e u and functions like the attention mechanism <ref type=""bibr"" target=""#b1"">[2]</ref>. Whereas, all variants only take linear transformation into consideration. It hence verifies the rationality a g the user-item graph structure into the embedding learning process. In future, we will further improve NGCF by incorporating the attention mechanism <ref type=""bibr"" target=""#b1"">[2]</ref> to learn variable weights for neighbors during embedding propagation and for the connectivities of different o",0
"ly (e.g., ID and attributes), without considering the user-item interactions -which are only used to define the objective function for model training <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b27"">28]</ref>. As a result, when the embeddings are insufficient in capturing CF, ""2.1"">Embedding Layer</head><p>Following mainstream recommender models <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b25"">26]</ref>, we describe a user u (an item i) with an embedding vector e u ∈ R d (e i ∈ R d ), where d denotes the embedd orized representations and reconstruct user-item interaction data based on model parameters. For example, MF <ref type=""bibr"" target=""#b19"">[20,</ref><ref type=""bibr"" target=""#b25"">26]</ref> projects the ID of each user and item as an embedding vector, and conducts inner product between them to pred .</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2.4"">Optimization</head><p>To learn model parameters, we optimize the pairwise BPR loss <ref type=""bibr"" target=""#b25"">[26]</ref>, which has been intensively used in recommender systems <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bi org/ns/1.0""><head n=""4.2.2"">Baselines.</head><p>To demonstrate the effectiveness, we compare our proposed NGCF with the following methods:</p><p>• MF <ref type=""bibr"" target=""#b25"">[26]</ref>: This is matrix factorization optimized by the Bayesian personalized ranking (BPR) loss, which exploits the",0
"v xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.2"">Graph-Based CF Methods</head><p>Another line of research <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b38"">39]</ref> exploits the user-item interaction graph to infer user preference. Ea",0
"la><p>where e</p><p>u denotes the representation of user u obtained after the first embedding propagation layer. The activation function of LeakyReLU <ref type=""bibr"" target=""#b22"">[23]</ref> allows messages to encode both positive and small negative signals. Note that in addition to the messages pr",0
"ion for propagation, and d ′ is the transformation size. Distinct from conventional graph convolution networks <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b28"">29,</ref><ref type=""bibr"" target=""#b40"">41]</ref> that consider the contributio n ability, but also boosts the performance for recommendation (evidences in our experiments Section 4.4.2). Following the graph convolutional network <ref type=""bibr"" target=""#b17"">[18]</ref>, we set p ui as the graph Laplacian norm 1/ |N u ||N i |, where N u and N i denote the first-hop neighbors o ""#b40"">41,</ref><ref type=""bibr"" target=""#b41"">42]</ref>. GC-MC <ref type=""bibr"" target=""#b28"">[29]</ref> applies the graph convolution network (GCN) <ref type=""bibr"" target=""#b17"">[18]</ref> on user-item graph, however it only employs one convolutional layer to exploit the direct connections betwee 40"">[41]</ref>, and the hidden dimension is set equal to the embedding size. • GC-MC <ref type=""bibr"" target=""#b28"">[29]</ref>: This model adopts GCN <ref type=""bibr"" target=""#b17"">[18]</ref> encoder to generate the representations for users and items, where only the first-order neighbors are consid",0
"/www.tei-c.org/ns/1.0""><head>2.2.1</head><p>First-order Propagation. Intuitively, the interacted items provide direct evidence on a user's preference <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b37"">38]</ref>; analogously, the users that consume an item can be treated as the i setting p ui ′ and p u ′ i as 1/ |N u | and 0 separately, we can exactly recover SVD++ model. Moreover, another widely-used item-based CF model, FISM <ref type=""bibr"" target=""#b15"">[16]</ref>, can be also seen as a special case of NGCF, wherein p iu ′ in Equation ( <ref type=""formula"" target=""#formu",0
"ts linearity makes it insufficient to reveal the complex and nonlinear relationships between users and items <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b14"">15]</ref>. Towards this end, recent efforts <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b13"">14, ype=""bibr"" target=""#b14"">15]</ref>. Towards this end, recent efforts <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b34"">35]</ref> focus on exploiting deep learning techniques to enhance the interacti",0
"pe=""bibr"" target=""#b3"">[4]</ref> showed that a user's short-term search history becomes more important as the search session progresses. White et al. <ref type=""bibr"" target=""#b50"">[51]</ref> reported the use of users' on-task behavior yielded promising gains in retrieval performance in the Microsof or statistical features are extracted from previous clicks or queries <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b42"">43,</ref><ref type=""bibr"" target=""#b50"">51]</ref>, or manually crafted rules are introduced to characterize the changes in a search sequence <ref type=""bibr"" t past queries and clicked documents in a search session to re-rank document for future queries. White et al. <ref type=""bibr"" target=""#b49"">[50,</ref><ref type=""bibr"" target=""#b50"">51]</ref> develop a rich set of statistical features to quantify context information from users' on-task search behavio their retrieval performance <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b29"">30,</ref><ref type=""bibr"" target=""#b50"">51]</ref>. However, as the users' information need and behavior pattern vary significantly from task to task, modeling",1
"lations and uses the learned embeddings to improve query prediction. <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b45"">46,</ref><ref type=""bibr"" target=""#b51"">52]</ref> exploited hierarchical neural architectures to model a sequence of qu ""4.1"">Dataset and Experimental Setups</head><p>We conduct experiments on the AOL search log data <ref type=""bibr"" target=""#b36"">[37]</ref>. Following <ref type=""bibr"" target=""#b45"">[46]</ref>, we use the first five weeks as background set, the next six weeks as training set, and the remaining two we r the query suggestion task, we evaluate a model's ability to discriminate and generate the next query. To test its discrimination ability, we follow <ref type=""bibr"" target=""#b45"">[46]</ref> and apply a testing model to rank a list of candidate queries that might follow an anchor query (the second #b2"">[3]</ref>, an enhanced Seq2seq model with attention mechanism <ref type=""bibr"" target=""#b30"">[31]</ref>, session-based suggestion models HRED-qs <ref type=""bibr"" target=""#b45"">[46]</ref>, M-Match Tensor <ref type=""bibr"" target=""#b1"">[2]</ref> and M-NSRF <ref type=""bibr"" target=""#b1"">[2]</ref>.",0
"and learning to rank approaches as our future work.</p><p>Various models can be employed here to predict click based on these two vectors. Following <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b32"">33]</ref>, we first create an extended matching vector to capture the similari type=""bibr"" target=""#b17"">[18]</ref> 0.283 0.307 0.188 0.231 0.341 CLSM <ref type=""bibr"" target=""#b43"">[44]</ref> 0.313 0.341 0.205 0.252 0.373 ARC-I <ref type=""bibr"" target=""#b15"">[16]</ref> 0.401 0.411 0.259 0.374 0.463 ARC-II <ref type=""bibr"" target=""#b15"">[16]</ref> 0.455 0.465 0.309 0.434 0.521 pe=""bibr"" target=""#b43"">[44]</ref> 0.313 0.341 0.205 0.252 0.373 ARC-I <ref type=""bibr"" target=""#b15"">[16]</ref> 0.401 0.411 0.259 0.374 0.463 ARC-II <ref type=""bibr"" target=""#b15"">[16]</ref> 0.455 0.465 0.309 0.434 0.521 DUET <ref type=""bibr"" target=""#b32"">[33]</ref> 0.479 0.490 0.332 0.462 0.546 M",0
"user skipped a top-ranked document, the suggestion for next query should be less related to such documents. Inspired by these scenarios, recent works <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" target=""#b33""> bibr"" target=""#b40"">[41]</ref> adopted semantic categorization of the query terms to improve query segmentation. From a different angle, Ahmad et al. <ref type=""bibr"" target=""#b1"">[2]</ref> proposed to train a document ranker and a query recommender jointly over a sequence of queries in a session. H ecom ? 2 ).</formula><formula xml:id=""formula_18"">L R2 = ? 3 w ?V P(w |q 1:i-1 , w 1:t -1 ) log P(w |q 1:i-1 , w 1:t -1 )</formula><p>as suggested in <ref type=""bibr"" target=""#b1"">[2]</ref> to smooth the predicted word distribution.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4"">EXPE x weeks as training set, and the remaining two weeks are divided into half to construct validation and test sets. Note this setting is different from <ref type=""bibr"" target=""#b1"">[2]</ref> that randomly splits search log. The background set is used to generate candidate queries for later query sugg log only contains clicked documents under each query and do not record other candidate documents returned to the users. Therefore, for a given query, <ref type=""bibr"" target=""#b1"">[2]</ref> aggregated a list of candidate documents, selected from the top documents ranked by BM25 <ref type=""bibr"" targ ]</ref> 0.479 0.490 0.332 0.462 0.546 Match Tensor <ref type=""bibr"" target=""#b18"">[19]</ref> 0.481 0.501 0.345 0.472 0.555 Multi-task Learning M-NSRF <ref type=""bibr"" target=""#b1"">[2]</ref> 0.491 0.502 0.348 0.474 0.557 M-Match Tensor <ref type=""bibr"" target=""#b1"">[2]</ref> 0.505 0.518 0.368 0.491 0 ]</ref> 0.481 0.501 0.345 0.472 0.555 Multi-task Learning M-NSRF <ref type=""bibr"" target=""#b1"">[2]</ref> 0.491 0.502 0.348 0.474 0.557 M-Match Tensor <ref type=""bibr"" target=""#b1"">[2]</ref> 0.505 0.518 0.368 0.491 0.567 CARS 0.531 0.542 0.391 0.517 0.596 between the resulting vectors. <ref type=""foo 43]</ref>, as our classical IR baselines for document ranking. To compare CARS with neural ranking models, we selected the same set of models used in <ref type=""bibr"" target=""#b1"">[2]</ref>, and trained and evaluated them using their publicly available implementations. To examine CARS's performance echanism <ref type=""bibr"" target=""#b30"">[31]</ref>, session-based suggestion models HRED-qs <ref type=""bibr"" target=""#b45"">[46]</ref>, M-Match Tensor <ref type=""bibr"" target=""#b1"">[2]</ref> and M-NSRF <ref type=""bibr"" target=""#b1"">[2]</ref>. We used the public implementation of these query suggestio session-based suggestion models HRED-qs <ref type=""bibr"" target=""#b45"">[46]</ref>, M-Match Tensor <ref type=""bibr"" target=""#b1"">[2]</ref> and M-NSRF <ref type=""bibr"" target=""#b1"">[2]</ref>. We used the public implementation of these query suggestion models. We carefully tuned the hyper-parameters f",0
"=""#b50"">51]</ref>, or manually crafted rules are introduced to characterize the changes in a search sequence <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b52"">53]</ref>. Those algorithms' exploration of contextual information is thus subjected by the capacity of their employed target=""#b50"">51]</ref> develop a rich set of statistical features to quantify context information from users' on-task search behavior. Xiang et al. <ref type=""bibr"" target=""#b52"">[53]</ref> craft a collection of rules to characterize the search context, e.g., specialization v.s., generalization, s",0
"the acoustic data.</p><p>Besides, CIF provides a concise calculation process by conducting the locating and integrating at the same time, rather than <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b11"">12]</ref> which need two separate steps of first using a hard monotonic attent",1
"two separate steps of first using a hard monotonic attention to decide when to stop and then performing soft attention to calculate, also rather than <ref type=""bibr"" target=""#b12"">[13]</ref> which needs a CTC trained model to conduct pre-partition before the attention decoding.</p><p>In <ref type="" e=""table"" target=""#tab_1"">1</ref>), which not only shows a clear performance advantage than other soft and monotonic models (e.g. triggered attention <ref type=""bibr"" target=""#b12"">[13]</ref>), but also matches or surpasses most of the published results of end-to-end models.</p><p>By fine-tuning the <ref type=""bibr"" target=""#b34"">[35]</ref> 26.8 Joint CTC-attention model / ESPNet <ref type=""bibr"" target=""#b15"">[16]</ref> 27.4 Triggered Attention <ref type=""bibr"" target=""#b12"">[13]</ref> 30 where the membrane potential Um is constantly simulated by the input spikes I in the period of dt, C is a",1
"ather than <ref type=""bibr"" target=""#b12"">[13]</ref> which needs a CTC trained model to conduct pre-partition before the attention decoding.</p><p>In <ref type=""bibr"" target=""#b13"">[14]</ref>, Li al. present the important Adaptive Computation Steps (ACS) algorithm whose motivation is to dynamically",1
">), current largest Mandarin read-speech corpus (AISHELL-2 <ref type=""bibr"" target=""#b17"">[18]</ref>) and the Mandarin telephone ASR benchmark (HKUST <ref type=""bibr"" target=""#b18"">[19]</ref>). For Librispeech, we use all the train data (960 hours) for training, mix the two development sets for vali",0
"ergoing an exciting pathway to be more simplified and accurate with the spring up of various end-to-end models. Among them, the attention-based model <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref>, which builds a soft alignment between each decoder step and every encod",0
"th our expectations, the CIFbased model performs very competitive on all test sets and significantly improves the results achieved by the Chain model <ref type=""bibr"" target=""#b32"">[33]</ref>. </p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>Model CER</head><p>Chain-TDNN <ref type=""bibr"" ta by the Chain model <ref type=""bibr"" target=""#b32"">[33]</ref>. </p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>Model CER</head><p>Chain-TDNN <ref type=""bibr"" target=""#b32"">[33]</ref> 23.7 Self-attention Aligner <ref type=""bibr"" target=""#b14"">[15]</ref> 24.1 Transformer <ref type=""bibr"" targ",0
"ective training set. We extract input features using the same setup as <ref type=""bibr"" target=""#b14"">[15]</ref> for all datasets. Speed perturbation <ref type=""bibr"" target=""#b19"">[20]</ref> with fixed ± 10% is applied for all training datasets. The frequency masking and time masking in <ref type=""",0
"also studied the soft and monotonic alignment in end-to-end ASR models. <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b9"">10]</ref>  to be a forward-moving window that fits gaussian distribution <ref type=""bibr"" target=""#b7"">[8,</ref><ref typ rd-moving window that fits gaussian distribution <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b8"">9]</ref> or even heuristic rule <ref type=""bibr"" target=""#b9"">[10]</ref>, where the center and width of the window are predicted by its decoder state. Comparing with them, CIF neithe",0
"c calculation but also locates acoustic boundaries. And we find inspirations from the integrate-and-fire model <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b5"">6]</ref>.</p><p>Integrate-and-fire is one of the earliest models in spiking neural networks (SNNs), which are more bio-p",0
"ions are applied: one of them is the quantity loss LQUA in section 3.2, the other is the CTC loss LCT C , which is applied on the encoder (similar to <ref type=""bibr"" target=""#b15"">[16]</ref>) and addresses the left-to-right acoustic encoding. When using these two optional loss, our model is trained sformer <ref type=""bibr"" target=""#b33"">[34]</ref> 26.6 Extended-RNA <ref type=""bibr"" target=""#b34"">[35]</ref> 26.8 Joint CTC-attention model / ESPNet <ref type=""bibr"" target=""#b15"">[16]</ref> 27.4 Triggered Attention <ref type=""bibr"" target=""#b12"">[13]</ref> 30 where the membrane potential Um is con",0
"ns/1.0""><head n=""2."">RELATION TO PRIOR WORK</head><p>Several prior works have also studied the soft and monotonic alignment in end-to-end ASR models. <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b9"">10]</ref>  to be a forward-moving window th [8,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b9"">10]</ref>  to be a forward-moving window that fits gaussian distribution <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b8"">9]</ref> or even heuristic rule <ref type=""bibr"" target=""#b9"">[10]</ref>, where t",0
"ere, we give these details and introduce some new characteristics of the CIF-based model:</p><p>Encoder: Our encoder follows the encoder structure in <ref type=""bibr"" target=""#b14"">[15]</ref>, which uses a two-layer convolutional front-end followed by a pyramid self-attention networks (SANs) and red etworks (SANs) and reduces the time resolution to 1/8. Forward encoding for online recognition is achieved by applying the chunk-hopping mechanism in <ref type=""bibr"" target=""#b14"">[15]</ref>. As an aside, adjusting the encoding resolution enables CIF suitable for various tasks, e.g. we could use up tworks.</p><p>Decoder: Two versions of decoder are introduced in this work: the one an autoregressive decoder, which follows the decoder structure in <ref type=""bibr"" target=""#b14"">[15]</ref>. Specifically, it first projects the concatenation of the embedding (ei−1) of the previous label and the pre nt sets for validation and use the three test sets for evaluation. For HKUST, we use the same training (∼168 hours), validation and evaluation set as <ref type=""bibr"" target=""#b14"">[15]</ref>. The training of LM on AISHELL-2 and HKUST uses the text from respective training set. We extract input feat "">[15]</ref>. The training of LM on AISHELL-2 and HKUST uses the text from respective training set. We extract input features using the same setup as <ref type=""bibr"" target=""#b14"">[15]</ref> for all datasets. Speed perturbation <ref type=""bibr"" target=""#b19"">[20]</ref> with fixed ± 10% is applied f lement our model on TensorFlow <ref type=""bibr"" target=""#b22"">[23]</ref>. For the selfattention networks (SANs) in our model, we use the structure in <ref type=""bibr"" target=""#b14"">[15]</ref> and set h = 4, d model = 640, d f f = 2560 for the two Mandarin datasets, and change (d model , d f f ) to ( (d model , d f f ) to (512, 2048), (1024, 4096) for the base, big model on Librispeech, respectively. For the encoder, we use the same configures as <ref type=""bibr"" target=""#b14"">[15]</ref>, where n in the pyramid structure is all set to 5. The chunk-hopping <ref type=""bibr"" target=""#b14"">[15]</re encoder, we use the same configures as <ref type=""bibr"" target=""#b14"">[15]</ref>, where n in the pyramid structure is all set to 5. The chunk-hopping <ref type=""bibr"" target=""#b14"">[15]</ref> for forward encoding uses the chunk size of 256 (frames) and the hop size of 128 (frames). For the 1-dimensi v> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>Model CER</head><p>Chain-TDNN <ref type=""bibr"" target=""#b32"">[33]</ref> 23.7 Self-attention Aligner <ref type=""bibr"" target=""#b14"">[15]</ref> 24.1 Transformer <ref type=""bibr"" target=""#b33"">[34]</ref> 26.6 Extended-RNA <ref type=""bibr"" target=""#b34"">",0
"vides a concise calculation process by conducting the locating and integrating at the same time, rather than <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b11"">12]</ref> which need two separate steps of first using a hard monotonic attention to decide when to stop and then perfo",0
"p rolling, and works in a more general setting.</p><p>Speech animation for rigged models. Several related methods produce animation curves for speech <ref type=""bibr"" target=""#b12"">[Edwards et al. 2016;</ref><ref type=""bibr"" target=""#b59"">Taylor et al. 2017;</ref><ref type=""bibr"" target=""#b73"">Zhou",1
"on et al. 2017;</ref><ref type=""bibr"" target=""#b53"">Sela et al. 2017]</ref>, or 3D displacements <ref type=""bibr"" target=""#b7"">[Cao et al. 2015;</ref><ref type=""bibr"" target=""#b22"">Guo et al. 2018;</ref><ref type=""bibr"" target=""#b61"">Tewari et al. 2018b</ref>].</p><p>Face reconstruction is the basis",0
"ess appears to be saying fox, even though that word was never spoken by her in the original recording.</p><p>to directly regress the model parameters <ref type=""bibr"" target=""#b11"">[Dou et al. 2017;</ref><ref type=""bibr"" target=""#b20"">Genova et al. 2018;</ref><ref type=""bibr"" target=""#b48"">Richardso",0
"pe=""bibr"" target=""#b69"">Wang et al. 2018b</ref>] or condition the synthesis on an input image <ref type=""bibr"" target=""#b26"">[Isola et al. 2017;</ref><ref type=""bibr"" target=""#b41"">Mirza and Osindero 2014]</ref>. High-resolution conditional video synthesis <ref type=""bibr"" target=""#b68"">[Wang et al.",0
"Garrido et al. 2014;</ref><ref type=""bibr"" target=""#b30"">Kemelmacher-Shlizerman et al. 2010;</ref><ref type=""bibr"" target=""#b35"">Li et al. 2014;</ref><ref type=""bibr"" target=""#b38"">Liu et al. 2001;</ref><ref type=""bibr"" target=""#b58"">Suwajanakorn et al. 2017;</ref><ref type=""bibr"" target=""#b67"">Vlas",0
"]</ref>, approximation methods are appropriate choices but it may incur the loss of information during the training process. To this end, inspired by <ref type=""bibr"" target=""#b3"">[Dai et al., 2016]</ref>, we transform the binary optimization problem to an equivalent continuous optimization problem",1
"lengings to address. Firstly, compared with explicit feedback, implicit feedback is more difficult to utilize because of the lack of negative samples <ref type=""bibr"" target=""#b10"">[Pan et al., 2008]</ref>. Secondly, generating top-k preferred items for each user is extremely time-consuming.</p><p>F be efficiently computed between binary codes via bit operation. Finding approximate top-K items can be even finished in sublinear or logarithmic time <ref type=""bibr"" target=""#b10"">[Wang et al., 2012;</ref><ref type=""bibr"" target=""#b10"">Muja and Lowe, 2009]</ref> by making use of index technique.</p ration. Finding approximate top-K items can be even finished in sublinear or logarithmic time <ref type=""bibr"" target=""#b10"">[Wang et al., 2012;</ref><ref type=""bibr"" target=""#b10"">Muja and Lowe, 2009]</ref> by making use of index technique.</p><p>Several methods applied hash techniques to recommend -stage approximation methods like BCCF <ref type=""bibr"">[Zhou and Zha, 2012]</ref>, PPH <ref type=""bibr"" target=""#b12"">[Zhang et al., 2014]</ref>, CH <ref type=""bibr"" target=""#b10"">[Liu et al., 2014]</ref> incur large quantization loss <ref type=""bibr"" target=""#b12"">[Zhang et al., 2016]</ref>, and a (3)</formula><p>where g(•):R d → R is a penalty term for f (x) and β is its penalty coefficient. <ref type=""bibr"">[Giannessi and Tardella, 1998;</ref><ref type=""bibr"" target=""#b10"">Lucidi and Rinaldi, 2010]</ref> show that the above two problems are equivalent when certain conditions hold.</p><p>Lem formula_13"">V T = [V (0) , V (1) , V (4) ]<label>(13)</label></formula><p>In terms of the loss function, we employ the popular and effective BPR loss <ref type=""bibr"" target=""#b10"">[Rendle et al., 2009]</ref>. In particular, given a user matrix U and an item matrix V as shown in Eqn.12 and Eqn.13, t",1
"""1"">Introduction</head><p>Nowadays, recommender systerms are widely used in people's daily life <ref type=""bibr"" target=""#b9"">[Liu et al., 2011;</ref><ref type=""bibr"" target=""#b8"">Lian et al., 2016;</ref><ref type=""bibr"" target=""#b8"">Li et al., 2018a]</ref>, but a growing scale of users and products ms are widely used in people's daily life <ref type=""bibr"" target=""#b9"">[Liu et al., 2011;</ref><ref type=""bibr"" target=""#b8"">Lian et al., 2016;</ref><ref type=""bibr"" target=""#b8"">Li et al., 2018a]</ref>, but a growing scale of users and products renders recommendation challenging. Because implicit mula><p>For the binary optimization problem, it is a direct method to use tanh(x/t) to approximate sign function, where t is a small temperature. But <ref type=""bibr"" target=""#b8"">[Li et al., 2018b]</ref> points that setting a small temperature will harm the optimization process. <ref type=""bibr"" ta",0
"e for the three items i, j, k is ranked as [i, j, k] in GCN-CF model, we hope that the rank keeps [i, j, k] in the binary model. According to ListNet <ref type=""bibr"" target=""#b1"">[Cao et al., 2007]</ref>, we can characterize sorting information in the following ways</p><formula xml:id=""formula_16"">",0
"h is crucial for implicit feedback. Some work used GCN to solve it such as SpectralCF <ref type=""bibr"" target=""#b13"">[Zheng et al., 2018]</ref>, GCMC <ref type=""bibr"" target=""#b0"">[Berg et al., 2017]</ref>, <ref type=""bibr"">RMGCNN [Monti et al., 2017]</ref>, GCNWSRS <ref type=""bibr"" target=""#b11"">[Y",0
"2008]</ref>. Secondly, generating top-k preferred items for each user is extremely time-consuming.</p><p>For the first problem, recently, Spec-tralCF <ref type=""bibr"" target=""#b13"">[Zheng et al., 2018]</ref> combined collaborative filtering model with graph convolutional network <ref type=""bibr"" tar , 2015]</ref> to mine hidden interactions between users and items from spectral domain, which showed enormous potential for implicit feedback problem <ref type=""bibr"" target=""#b13"">[Zheng et al., 2018]</ref>. However, SpectralCF ignores high-order feature interaction.</p><p>For the second problem, f f the rich linkage information from the user-item bipartite graph is crucial for implicit feedback. Some work used GCN to solve it such as SpectralCF <ref type=""bibr"" target=""#b13"">[Zheng et al., 2018]</ref>, GCMC <ref type=""bibr"" target=""#b0"">[Berg et al., 2017]</ref>, <ref type=""bibr"">RMGCNN [Mont datasets. The learned matrices U ,V in GCN-CF are used as the initialization of P ,Q in DGCN-BinCF. All parameters of SpectralCF are set according to <ref type=""bibr"" target=""#b13"">[Zheng et al., 2018]</ref>.</p><p>Besides, for DCF, BCCF and PPH, we held-out evaluation means on splits of training da",0
"al methods applied hash techniques to recommendation. Some two-stage approximation methods like BCCF <ref type=""bibr"">[Zhou and Zha, 2012]</ref>, PPH <ref type=""bibr"" target=""#b12"">[Zhang et al., 2014]</ref>, CH <ref type=""bibr"" target=""#b10"">[Liu et al., 2014]</ref> incur large quantization loss <r H <ref type=""bibr"" target=""#b12"">[Zhang et al., 2014]</ref>, CH <ref type=""bibr"" target=""#b10"">[Liu et al., 2014]</ref> incur large quantization loss <ref type=""bibr"" target=""#b12"">[Zhang et al., 2016]</ref>, and a direct optimization model DCF <ref type=""bibr"" target=""#b12"">[Zhang et al., 2016]</re [Liu et al., 2014]</ref> incur large quantization loss <ref type=""bibr"" target=""#b12"">[Zhang et al., 2016]</ref>, and a direct optimization model DCF <ref type=""bibr"" target=""#b12"">[Zhang et al., 2016]</ref> is easy to fall into a local optimum because it is based on local search. To this end, to im ef type=""bibr"" target=""#b12"">Zhang et al., 2014]</ref> which relaxed binary constraints at first and then quantified binary codes.</p><p>Nonetheless, <ref type=""bibr"" target=""#b12"">[Zhang et al., 2016]</ref> proposed that those two-stage methods suffered from large quantization loss. Therefore, DCF factors into Hamming space to obtain binary representation. Later, following this, some two stage methods <ref type=""bibr"">[Zhou and Zha, 2012;</ref><ref type=""bibr"" target=""#b12"">Zhang et al., 2014]</ref> which relaxed binary constraints at first and then quantified binary codes.</p><p>Nonetheless",0
"ody> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""1"">Introduction</head><p>Nowadays, recommender systerms are widely used in people's daily life <ref type=""bibr"" target=""#b9"">[Liu et al., 2011;</ref><ref type=""bibr"" target=""#b8"">Lian et al., 2016;</ref><ref type=""bibr"" target=""#b8"">Li et al., 2",0
"ve Hashing <ref type=""bibr"" target=""#b5"">[Datar et al., 2004]</ref> to generate binary codes for Google News readers according to their click history <ref type=""bibr"" target=""#b4"">[Das et al., 2007]</ref>. Then <ref type=""bibr"">[Karatzoglou et al., 2010]</ref> proposed a method mapping users and ite",0
"that, we distill the ranking information from the trained GCN-CF model into a binarized model (DGCN-BinCF) with knowledge distillation technique (KD <ref type=""bibr"" target=""#b7"">[Hinton et al., 2015]</ref>). To be more specific, we introduce a novel distillation loss, which penalizes not only the s binary codes via searching neighborhoods with the distance one. So it is easy to fall into local optima.</p><p>2.3 Distilling Knowledge for Ranking <ref type=""bibr"" target=""#b7"">[Hinton et al., 2015]</ref> was the first one that proposed method ""Knowledge Distilling"", which trained a complex neura nvert the items' score list to probability distributions via softmax function, and utilize cross entropy for penalizing the discrepancy. According to <ref type=""bibr"" target=""#b7"">[Hinton et al., 2015]</ref>, combining L Bin−CF with L rank as a multi-task learing problem can transfer the ranking kno",0
"f only supporting a limited coverage of deltas <ref type=""bibr"" target=""#b2"">[3]</ref>, it seems worthwhile to be unbiased, including negative deltas <ref type=""bibr"" target=""#b3"">[4]</ref> as well. 4) Matrices that are too sparse or empty (mcf_s1536B), indicate simple patterns or invalidated deltas",1
"div xmlns=""http://www.tei-c.org/ns/1.0""><head>Delta Cache (Markov-chain)</head><p>Associativity: 16</p><p>Set index: Delta ?{-63, ..., 63} Delta Next <ref type=""bibr"" target=""#b6"">(7)</ref>, LFU bits (8) With respect to the replacement, we use an approach similar to the Least Frequently Used (LFU) r ""#fig_4"">4</ref>, we can see an overview of the page cache. The page cache is set-associative, indexed by the page address. Page Tag (10), Delta Prev <ref type=""bibr"" target=""#b6"">(7)</ref>, Offset Prev <ref type=""bibr"" target=""#b5"">(6)</ref>, NRU bit (1) ? Page tag: to identify the page and disting uscated by 'unexpected', sometimes temporary, page transitions. There are similar approaches in related work <ref type=""bibr"" target=""#b4"">[5]</ref>, <ref type=""bibr"" target=""#b6"">[7]</ref>. This does not modify the decision part of Pangloss, as it only helps to increase the number of valid observat prefetcher to two state-of-the art prefetchers, the Best-Offset Prefetcher <ref type=""bibr"" target=""#b2"">[3]</ref> (BOP) and the prefetcher from KPC <ref type=""bibr"" target=""#b6"">[7]</ref> (KPCP). The first was the winner of the previous Data Prefetching Championship (DPC2) and was ported to work a",0
"d utilising address transition probabilities for subsequent accesses. Distance prefetching is a generalisation of the common Markov model prefetchers <ref type=""bibr"" target=""#b0"">[1]</ref>, that uses deltas instead of addresses to build more general models (originally for TLBs <ref type=""bibr"" targ on mechanisms have some weaknesses. When observing short repeating delta patterns, such as 1, 1, 2, 1, 3, 1, 1, 2, 1, 3, ..., the transitions (1, 1), <ref type=""bibr"" target=""#b0"">(1,</ref><ref type=""bibr"" target=""#b1"">2)</ref> and <ref type=""bibr"" target=""#b0"">(1,</ref><ref type=""bibr"" target=""#b2"" such as 1, 1, 2, 1, 3, 1, 1, 2, 1, 3, ..., the transitions (1, 1), <ref type=""bibr"" target=""#b0"">(1,</ref><ref type=""bibr"" target=""#b1"">2)</ref> and <ref type=""bibr"" target=""#b0"">(1,</ref><ref type=""bibr"" target=""#b2"">3)</ref> would yield an equal probability. This in combination with other factors",0
"ig_6"">6</ref> shows the weighted IPC speedups of 'Proposal L1 &amp; L2' and KPCP over the single-core runs with non-prefetch (i.e. ?(IPCi/IPCalone_i) <ref type=""bibr"" target=""#b7"">[8]</ref>). It is clear that the multi-level prefetcher performs generally better than KPCP, while the single-level vers",0
"lar strides (such as (1, 1), i.e. the model of the next line/sequential prefetcher).</p><p>3) Instead of only supporting a limited coverage of deltas <ref type=""bibr"" target=""#b2"">[3]</ref>, it seems worthwhile to be unbiased, including negative deltas <ref type=""bibr"" target=""#b3"">[4]</ref> as well cache replacement algorithm (LRU).</p><p>We compare the performance of our prefetcher to two state-of-the art prefetchers, the Best-Offset Prefetcher <ref type=""bibr"" target=""#b2"">[3]</ref> (BOP) and the prefetcher from KPC <ref type=""bibr"" target=""#b6"">[7]</ref> (KPCP). The first was the winner of ive lookup <ref type=""bibr"" target=""#b4"">[5]</ref> remains relatively efficient, although allowing a delay could also prove beneficial for timeliness <ref type=""bibr"" target=""#b2"">[3]</ref>.</p><p>According to the use case, many parameters that impact the space/logic complexity, can be explored furt ..., the transitions (1, 1), <ref type=""bibr"" target=""#b0"">(1,</ref><ref type=""bibr"" target=""#b1"">2)</ref> and <ref type=""bibr"" target=""#b0"">(1,</ref><ref type=""bibr"" target=""#b2"">3)</ref> would yield an equal probability. This in combination with other factors, like a low prefetch degree, could hav",0
"r of deep learning models were proposed to assist doctors in making medication recommendation <ref type=""bibr"" target=""#b5"">[Xiao et al., 2018a;</ref><ref type=""bibr"" target=""#b3"">Shang et al., 2019;</ref><ref type=""bibr"" target=""#b0"">Baytas et al., 2017;</ref><ref type=""bibr"">Choi et al., 2018;</re riteria are discarded before model training. For example, a large number of patients who only have one hospital visit were discarded from training in <ref type=""bibr"" target=""#b3"">[Shang et al., 2019]</ref>.</p><p>2. Lack of hierarchical knowledge: For medical knowledge such as diagnosis code ontolo n extremely effective in various areas such as image classification <ref type=""bibr"" target=""#b2"">[Hinton et al., 2006]</ref> and machine translation <ref type=""bibr"" target=""#b3"">[Ramachandran et al., 2016]</ref>. The unsupervised pre-training can be considered as a regularizer that supports better r"">[Velickovic et al., 2017]</ref>. GNNs have already been demonstrated useful on EHR modeling <ref type=""bibr"" target=""#b1"">[Choi et al., 2017;</ref><ref type=""bibr"" target=""#b3"">Shang et al., 2019]</ref>. <ref type=""bibr"">GRAM [Choi et al., 2017]</ref> represented a medical concept as a combinatio",1
"prove the performance on multiple NLP tasks. As the most widely used one, <ref type=""bibr"">BERT [Devlin et al., 2018]</ref> builds on the Transformer <ref type=""bibr"" target=""#b4"">[Vaswani et al., 2017]</ref> architecture and improves the pre-training using a masked language model for bidirectional G-BERT.</formula></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.1"">Background of BERT</head><p>Based on a multi-layer Transformer encoder <ref type=""bibr"" target=""#b4"">[Vaswani et al., 2017]</ref> (The transformer architecture has been ubiquitously used in many sequence modeling tasks re es).</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>Visit Embedding</head><p>Similar to BERT, we use a multi-layer Transformer architecture <ref type=""bibr"" target=""#b4"">[Vaswani et al., 2017]</ref> as our visit encoder. The model takes the ontology embedding as input and derive visit embe",1
", BERT) methods. We developed a new pre-training method based on BERT for Instance-based methods focus on current health conditions. Among them, Leap <ref type=""bibr"" target=""#b7"">[Zhang et al., 2017]</ref> formulates a multi-instance multi-label learning framework and proposes a variant of sequence",0
"rs in making medication recommendation <ref type=""bibr"" target=""#b5"">[Xiao et al., 2018a;</ref><ref type=""bibr"" target=""#b3"">Shang et al., 2019;</ref><ref type=""bibr"" target=""#b0"">Baytas et al., 2017;</ref><ref type=""bibr"">Choi et al., 2018;</ref><ref type=""bibr"">Ma et al., 2018]</ref>. They often l Despite that various considerations were handled in previous works for improving medical code representations <ref type=""bibr"">[Ma et al., 2018;</ref><ref type=""bibr"" target=""#b0"">Baytas et al., 2017;</ref><ref type=""bibr"">Choi et al., 2018]</ref>, there are two limitations with the existing work:</",0
"oses. Longitudinal-based methods leverage the temporal dependencies among clinical events, see <ref type=""bibr"" target=""#b1"">[Choi et al., 2016;</ref><ref type=""bibr"" target=""#b6"">Xiao et al., 2018b;</ref><ref type=""bibr"" target=""#b2"">Lipton et al., 2015]</ref>. Among them, <ref type=""bibr"">RETAIN [",0
"enabled and demonstrated by the following technical contributions:</p><p>1. Pre-training to leverage more data: Pre-training techniques, such as ELMo <ref type=""bibr"" target=""#b2"">[Peters et al., 2018]</ref>, OpenAI <ref type=""bibr"">GPT [Radford et al., 2018]</ref> and <ref type=""bibr"">BERT [Devlin dependencies among clinical events, see <ref type=""bibr"" target=""#b1"">[Choi et al., 2016;</ref><ref type=""bibr"" target=""#b6"">Xiao et al., 2018b;</ref><ref type=""bibr"" target=""#b2"">Lipton et al., 2015]</ref>. Among them, <ref type=""bibr"">RETAIN [Choi et al., 2016]</ref> uses a two-level neural attent is to provide model training with good initializations. Pre-training has been shown extremely effective in various areas such as image classification <ref type=""bibr"" target=""#b2"">[Hinton et al., 2006]</ref> and machine translation <ref type=""bibr"" target=""#b3"">[Ramachandran et al., 2016]</ref>. The better generalization from the training dataset <ref type=""bibr"">[Erhan et al., 2010]</ref>. Recently, language model pre-training techniques such as <ref type=""bibr"" target=""#b2"">[Peters et al., 2018;</ref><ref type=""bibr"">Radford et al., 2018;</ref><ref type=""bibr"" target=""#b1"">Devlin et al., 2018 data. Various graph neural networks have been proposed to encode the graph-structure information, including graph convolutional neural networks (GCN) <ref type=""bibr"" target=""#b2"">[Kipf and Welling, 2017]</ref>, message passing networks (MPNN) <ref type=""bibr"" target=""#b1"">[Gilmer et al., 2017]</ref a><p>5 Experiment</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""5.1"">Experimental Setting Data</head><p>We used EHR data from MIMIC-III <ref type=""bibr"" target=""#b2"">[Johnson et al., 2016]</ref> and conducted all our experiments on a cohort where patients have more than one visit. We u determine the dimension for medical embedding as 300 and thershold for final prediction as 0.3 for better performance. Training is done through Adam <ref type=""bibr"" target=""#b2"">[Kingma and Ba, 2014]</ref> at learning rate 5e-4. We fix the best model on evaluation set within 100 epochs and report",0
"predict combination of medicines given patient's diagnoses. Longitudinal-based methods leverage the temporal dependencies among clinical events, see <ref type=""bibr"" target=""#b1"">[Choi et al., 2016;</ref><ref type=""bibr"" target=""#b6"">Xiao et al., 2018b;</ref><ref type=""bibr"" target=""#b2"">Lipton et , language model pre-training techniques such as <ref type=""bibr"" target=""#b2"">[Peters et al., 2018;</ref><ref type=""bibr"">Radford et al., 2018;</ref><ref type=""bibr"" target=""#b1"">Devlin et al., 2018]</ref> have shown to largely improve the performance on multiple NLP tasks. As the most widely used on, including graph convolutional neural networks (GCN) <ref type=""bibr"" target=""#b2"">[Kipf and Welling, 2017]</ref>, message passing networks (MPNN) <ref type=""bibr"" target=""#b1"">[Gilmer et al., 2017]</ref>, graph attention networks (GAT) <ref type=""bibr"">[Velickovic et al., 2017]</ref>. GNNs have 7]</ref>, graph attention networks (GAT) <ref type=""bibr"">[Velickovic et al., 2017]</ref>. GNNs have already been demonstrated useful on EHR modeling <ref type=""bibr"" target=""#b1"">[Choi et al., 2017;</ref><ref type=""bibr"" target=""#b3"">Shang et al., 2019]</ref>. <ref type=""bibr"">GRAM [Choi et al., 20 s am a binary sentence prediction task to predict whether one sentence is the next sentence of the other.</p><p>A typical input to BERT is as follows <ref type=""bibr"" target=""#b1"">( [Devlin et al., 2018]</ref>):</p><formula xml:id=""formula_4"">Input = [CLS] the man went to [MASK] store [SEP] he bough from their connected ancestors (in stage 2).</p><p>It is worth mentioning that our graph embedding method on medical ontology is different from GRAM <ref type=""bibr"" target=""#b1"">[Choi et al., 2017]</ref> from the following two aspects: 1. Initialization: we initialize all the node embeddings from rparameter values which result in L1 norm penalty with weight as 1.1. For deep learning models, we implemented RNN using a gated recurrent unit (GRU) <ref type=""bibr"" target=""#b1"">[Cho et al., 2014]</ref> and utilize dropout with a probability of 0.4 on the output of embedding. We test several embed",0
"putational medication recommendation task.</p><p>A number of deep learning models were proposed to assist doctors in making medication recommendation <ref type=""bibr"" target=""#b5"">[Xiao et al., 2018a;</ref><ref type=""bibr"" target=""#b3"">Shang et al., 2019;</ref><ref type=""bibr"" target=""#b0"">Baytas et",0
"arget=""#b44"">[45]</ref> has been proposed to regularize the distribution of zero weights, so that more all-zero rows and columns can be found. SNrram <ref type=""bibr"" target=""#b43"">[44]</ref> seeks to enable fine-grained column compression at the cost of high output indexing overhead.</p><p>Existing hts, so that more all-zero rows and columns can be found. However, with ReCom, there are still many zero weights left in the compressed model. SNrram <ref type=""bibr"" target=""#b43"">[44]</ref> compresses the model at a finer level, i.e., allzero filters in structurally compressed neural networks. As lly-connected layer is all zeros, the corresponding OU rows are removed to reduce unnecessary computations. Note that we does not compare with SNrram <ref type=""bibr"" target=""#b43"">[44]</ref>, as SNrram uses model-based compression and its crossbar architecture is highly model-dependent. A quantitat In Figure <ref type=""figure"" target=""#fig_20"">20</ref>, we also use arrows to indicate the weight compression ratio that can be obtained from SNrram <ref type=""bibr"" target=""#b43"">[44]</ref>. As SNrram uses model-based compression (i.e., finegrained column-based compression that removes all-zero co es. However, with ReCom's coarse-grain compression, only all-zero rows can be removed; many zero weights still remain in the compressed model. SNrram <ref type=""bibr"" target=""#b43"">[44]</ref> is another ReRAMbased sparse DNN accelerator that compresses the model at a finer level, i.e., filter-sized e-grained column compression at the cost of high output indexing overhead.</p><p>Existing sparsity solutions <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b43"">44]</ref> are based on an over-idealized ReRAM crossbar architecture <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""",1
"es and bitlines in a crossbar array can be activated in a single cycle <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b41"">42,</ref><ref type=""bibr"" target=""#b46"">47]</ref>. The maximum number of wordlines that can be turned on concurrently depends on accuracy limitations, while th hus is commonly deployed in latest ReRAM-based DNN accelerator studies <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b41"">42,</ref><ref type=""bibr"" target=""#b46"">47]</ref>, we use one of the oxide-based ReRAM, WOx ReRAM <ref type=""bibr"" target=""#b21"">[22]</ref>, for our evaluation ssbar array <ref type=""bibr"" target=""#b34"">[35]</ref>), ADC sensing time brings a much larger impact on the operating speed of a ReRAM crossbar array <ref type=""bibr"" target=""#b46"">[47]</ref>. Hence, the cycle time of a ReRAM-based DNN accelerator is dictated by the slowest ADC sensing stage and the",0
"nly done in digital CMOS-based accelerators to improve energy efficiency <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" target=""#b48"">49]</ref>. Many studies have shown that <ref type=""bibr"" target=""#b48"">[49]</ref>, activation sparsity <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b7"">8]</ref>, or both <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b35"">36]</ref> in CMOS-based digital accelerators in order to improve the energy ef ier operands. Nevertheless, neither Eyeriss nor Cnvlutin skips computations with zero weights. To jointly exploit weight and activation sparsity, EIE <ref type=""bibr"" target=""#b15"">[16]</ref> uses a compressed representation for both activations and weights, and only delivers non-zero operands to th",0
"into wordlines sequentially. In this example, to compute the output neuron for the first input sliding window <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b0"">1]</ref>, the data we feed into the input re",0
".tei-c.org/ns/1.0""><head n=""6"">RELATED WORK</head><p>Datacenter-wide profiling. Modern systems for always-on profiling trace their beginnings to DCPI <ref type=""bibr"" target=""#b0"">[1]</ref>. Of these, Aut-oFDO <ref type=""bibr"" target=""#b4"">[5]</ref> (built on top of Google-wide-profiling (GWP) <ref",1
"uctions. This enables faithful and accurate reconstruction of dynamic program control flow. Collection is built on top of the data source for AutoFDO <ref type=""bibr"" target=""#b4"">[5]</ref>, which similarly uses LBRs to reconstruct basic block execution counts for compiler feedback-directed optimiza icularly susceptible to the accuracy of feedback profiles <ref type=""bibr"" target=""#b24"">[25]</ref>, especially with sampling approaches like AutoFDO <ref type=""bibr"" target=""#b4"">[5]</ref>.</p><p>The high degree of fragmentation we observed suggests there is significant opportunity to improve i-cac ter-wide profiling. Modern systems for always-on profiling trace their beginnings to DCPI <ref type=""bibr"" target=""#b0"">[1]</ref>. Of these, Aut-oFDO <ref type=""bibr"" target=""#b4"">[5]</ref> (built on top of Google-wide-profiling (GWP) <ref type=""bibr"" target=""#b29"">[30]</ref>) is perhaps the most si",1
"sed these misses with significant hardware architectural modifications <ref type=""bibr"">[10, 11, 18-20, 20, 22]</ref> or static control flow analysis <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b27"">28]</ref>. However, in WSC environments it has been commercially infeasible to ock will be used, and static code analysis can assume some control flow and determine constructs such as loops which might be amenable to prefetching <ref type=""bibr"" target=""#b23"">[24]</ref>. In contrast, our approach leverages AsmDB to augment heuristic information with empirical observations abou e (and doesn't arrive before the miss), as represented by w in Figure <ref type=""figure"" target=""#fig_4"">14</ref>. Existing compiler based approaches <ref type=""bibr"" target=""#b23"">[24]</ref> insert prefetches at a fixed number of instructions before the miss to roughly match the memory access laten 50 instructions. Both situations can be addressed by prefetching, but low-fan-in targets will incur significantly smaller overheads. Prior approaches <ref type=""bibr"" target=""#b23"">[24]</ref> had to aggressively insert prefetches in all paths to obtain high coverage, and then resort to filtering out e have not made it into datacenter-scale processors so far, and each assumes significant complexity, cost, and risk to implement.</p><p>Luk and Mowry <ref type=""bibr"" target=""#b23"">[24]</ref> have the most similar approach to reducing i-cache misses compared to this work. They also insert code prefe",1
"). We further correlate AsmDB with hardware performance counter profiles collected by a datacenter-wide profiling system -Google-Wide Profiling (GWP) <ref type=""bibr"" target=""#b29"">[30]</ref> -in order to reason about specific patterns that affect front-end performance. Collecting and processing pro echniques to improve i-cache hit rates.</p><p>AsmDB is a specialization of generic datacenter-wide profiling systems like Google-wide-profiling (GWP) <ref type=""bibr"" target=""#b29"">[30]</ref>, which collect many different types of performance profiles. AsmDB is implemented in the GWP framework in or DCPI <ref type=""bibr"" target=""#b0"">[1]</ref>. Of these, Aut-oFDO <ref type=""bibr"" target=""#b4"">[5]</ref> (built on top of Google-wide-profiling (GWP) <ref type=""bibr"" target=""#b29"">[30]</ref>) is perhaps the most similar to AsmDB. Both AutoFDO and this work use continuously-collected LBR samples for",0
", is now being served by services hosted in huge datacenters, which are dubbed Warehouse-Scale Computers (WSC) <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b25"">26]</ref>. The continued growth in cloud-based, digital services has led WSCs t et=""#fig_0"">1</ref> presents a Top-Down <ref type=""bibr"" target=""#b32"">[33]</ref> breakdown of a web search loadtest running on an Intel Haswell CPU. <ref type=""bibr"" target=""#b12"">13</ref>.8% of total performance potential is wasted due to ""Front-end latency,"" which is dominated by instruction cach",0
"footprint of WSC workloads in particular is often over 100 times larger than the size of a L1 instruction cache (i-cache) and can easily overwhelm it <ref type=""bibr"" target=""#b1"">[2]</ref>. Studies show it expanding at rates of over 20% per year <ref type=""bibr"" target=""#b15"">[16]</ref>. This resul",0
"ilar gains is in the domain of compilers. Compiler optimizations like hot/cold splitting <ref type=""bibr"" target=""#b5"">[6]</ref> and partial inlining <ref type=""bibr"" target=""#b31"">[32]</ref> aim to address fragmentation by only inlining the hot basic blocks of a function. However, they have recentl",0
"lobytes per core <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b19"">20]</ref> or per chip <ref type=""bibr"" target=""#b17"">[18]</ref>. Recent streaming prefetchers have reduced the required amount of on-chip storage <ref type=""bibr"" target=""# 19"">20]</ref> or per chip <ref type=""bibr"" target=""#b17"">[18]</ref>. Recent streaming prefetchers have reduced the required amount of on-chip storage <ref type=""bibr"" target=""#b17"">[18]</ref><ref type=""bibr"" target=""#b18"">[19]</ref><ref type=""bibr"" target=""#b19"">[20]</ref>. However, they still requi",0
"s <ref type=""bibr"" target=""#b15"">[16]</ref> and on isolated benchmarks <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b16"">17]</ref>, have previously identified i-cache misses as a significant performance bottleneck. This work stems from the",0
"-end stalls. We corroborate this challenge for our WSCs on a web search binary. Figure <ref type=""figure"" target=""#fig_0"">1</ref> presents a Top-Down <ref type=""bibr"" target=""#b32"">[33]</ref> breakdown of a web search loadtest running on an Intel Haswell CPU. <ref type=""bibr"" target=""#b12"">13</ref>.",0
"t-processing discovers basic block predecessors and successors and identifies loops using Havlak's algorithm <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b13"">14]</ref> for full control-flow-graph (CFG) reconstruction.</p><p>With tens of thousands of binaries in a WSC, this ste",0
"cache miss rates which are orders of magnitude higher than the worst cases in desktop-class applications, commonly represented by SPEC CPU benchmarks <ref type=""bibr"" target=""#b8"">[9]</ref>.</p><p>Because the performance of a general-purpose processor is critically dependent on its ability to feed i insertion on the binary level.</p><p>Profiling efforts, both on production WSCs <ref type=""bibr"" target=""#b15"">[16]</ref> and on isolated benchmarks <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b16"">17]</ref>, have previously identified i-",0
"plications which leads to untimely prefetch injections. (For instance, in SPEC CPU2006, mcf has an IPC of 0.2 whereas dealII has an IPC of almost 2.0 <ref type=""bibr"" target=""#b26"">[27]</ref>.) Our approach addresses this issue by leveraging per-application IPC obtained from profiling data to calcul",0
"inder scalability.</p><p>In addition, post-processing discovers basic block predecessors and successors and identifies loops using Havlak's algorithm <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b13"">14]</ref> for full control-flow-graph (CFG) reconstruction.</p><p>With tens of",0
"e architectural modifications <ref type=""bibr"">[10, 11, 18-20, 20, 22]</ref> or static control flow analysis <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b27"">28]</ref>. However, in WSC environments it has been commercially infeasible to implement these large hardware changes,",0
"i-c.org/ns/1.0""><head n=""2.3"">Perceptron Learning</head><p>Perceptron learning for microarchitectural prediction was introduced for branch prediction <ref type=""bibr"" target=""#b19"">[20]</ref>. Our predictor uses a version of microarchitectural perceptron prediction known as the ""hashed perceptron"" o ching is done. The LRU replacement policy is used on all levels of cache hierarchies. Branch prediction is done using the perceptron branch predictor <ref type=""bibr"" target=""#b19"">[20]</ref>. The page size is 4KB. ChampSim operates all the prefetchers strictly in the physical address space.</p></di rformance.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""7.5"">Perceptrons in Cache Management</head><p>In addition to branch prediction <ref type=""bibr"" target=""#b19"">[20]</ref>, perceptron-based learning has been applied to the area of cache management. Teran et al. propose using perc",1
"=""#b5"">[6]</ref>. Early prefetchers detected stride access patterns in order to predict future memory requests <ref type=""bibr"" target=""#b6"">[7]</ref><ref type=""bibr"" target=""#b7"">[8]</ref><ref type=""bibr"" target=""#b8"">[9]</ref>. Modern prefetching mechanisms are more sophisticated as they look into",0
"r"" target=""#b11"">[12]</ref><ref type=""bibr"" target=""#b12"">[13]</ref><ref type=""bibr"" target=""#b13"">[14]</ref><ref type=""bibr"" target=""#b14"">[15]</ref><ref type=""bibr"" target=""#b15"">[16]</ref><ref type=""bibr"" target=""#b16"">[17]</ref>, controlflow speculation <ref type=""bibr"" target=""#b17"">[18,</ref><",0
"efetch coverage.</p><p>Figure <ref type=""figure"">1</ref> illustrates the above scenario. Here we consider a stateof-the-art lookahead prefetcher -SPP <ref type=""bibr"" target=""#b1"">[2]</ref>. Lookahead prefetchers such as SPP provide a mechanism to speculate an arbitrary number of references ahead of ained neural model to be inaccurate. The state-of-the-art prefetcher that we use to evaluate PPF in this paper is the Signature Path Prefetcher (SPP) <ref type=""bibr"" target=""#b1"">[2]</ref>, however as we describe, PPF can be designed to benefit any prefetcher. In this design, PPF replaces SPP's exi f><ref type=""bibr"" target=""#b18"">19]</ref>, and other other aspects to detect complex memory access patterns. See Section 7 for other relevant work.  <ref type=""bibr"" target=""#b1"">[2]</ref>, a confidencebased lookahead prefetcher. SPP creates a signature associated with a page address by compressing -AMPM is the enhanced version of AMPM, modified to account for DRAM row buffer locality. SPP has been shown to outperform BOP on SPEC CPU 2006 traces <ref type=""bibr"" target=""#b1"">[2]</ref>. For each of these, we compare their speedups taking the no prefetching case as the baseline.</p></div> <div x get=""#b34"">[35]</ref>, which correlates histories of deltas between cache line accesses within memory pages with the next delta within that page. SPP <ref type=""bibr"" target=""#b1"">[2]</ref> and KPC's prefetching component <ref type=""bibr"" target=""#b35"">[36]</ref> are more recent examples of lookahea ""><p>Components of Prefetch Table can be found in Table</p></note> 			<note xmlns=""http://www.tei-c.org/ns/1.0"" place=""foot"" n=""2"" xml:id=""foot_1""><p><ref type=""bibr"" target=""#b1"">2</ref> The Reject Table does not need to maintain the useful bit as that only applies for prefetches that ultimately ma",0
"n of PPF against prior work techniques. ChampSim is an enhanced version of the framework that was used for the 2nd Data Prefetch Championship (DPC-2) <ref type=""bibr"" target=""#b25"">[26]</ref>, also used in the 2nd Cache Replacement Competition (CRC2) <ref type=""bibr"" target=""#b26"">[27]</ref>. We mod",0
"et=""#b22"">[23]</ref>.</p><p>We provide insight into the root causes of relatively low IPC using the Top-down Microarchitecture Analysis Method (TMAM) <ref type=""bibr"" target=""#b62"">[63]</ref> to categorize processor pipelines' execution stalls, as reported in Fig. <ref type=""figure"" target=""#fig_5""> CloudSuite <ref type=""bibr"" target=""#b20"">[21]</ref> comprises both latency-sensitive and throughput-oriented scale-out cloud workloads. Yasin et al. <ref type=""bibr"" target=""#b62"">[63]</ref> perform a microarchitectural characterization of several CloudSuite workloads. However, our findings on prod services differ from those of academic cloud benchmark suite studies <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b62"">63,</ref><ref type=""bibr"" target=""#b95"">96,</ref><ref type=""bibr"" target=""#b96"">97]</ref>. For example, unlike these be",1
"s2 and Feed1), the largest working set is too large to be captured. Hence, some services might benefit from trading LLC capacity for additional cores <ref type=""bibr"" target=""#b67"">[68]</ref>.</p><p>2.4.4 TLB misses. We report instruction and data TLB MPKI in Fig. <ref type=""figure"" target=""#fig_9"">",0
"ific soft SKUs. Other works reduce coscheduled job interference <ref type=""bibr"" target=""#b108"">[109]</ref><ref type=""bibr"" target=""#b109"">[110]</ref><ref type=""bibr"" target=""#b110"">[111]</ref><ref type=""bibr"" target=""#b111"">[112]</ref><ref type=""bibr"" target=""#b112"">[113]</ref><ref type=""bibr"" targ",0
"bibr"" target=""#b97"">[98]</ref><ref type=""bibr"" target=""#b98"">[99]</ref><ref type=""bibr"" target=""#b99"">[100]</ref>, exploiting multicore heterogeneity <ref type=""bibr"" target=""#b100"">[101]</ref><ref type=""bibr"" target=""#b101"">[102]</ref><ref type=""bibr"" target=""#b102"">[103]</ref>, trading memory late",0
"quency. ?SKU varies uncore (LLC, memory controller, etc.) frequency from 1.4 GHz to 1.8 GHz (default) by overriding uncore frequency-controlling MSRs <ref type=""bibr"" target=""#b79"">[80]</ref>.</p><p>(3) Core count. ?SKU scales core count from 2 physical cores to the platform-specific maximum (defaul",0
"cent innovations in gesture recognition with Wi-Fi have explored cross-domain generalization ability of recognition models. For example, recent works <ref type=""bibr"" target=""#b19"">[20,</ref><ref type=""bibr"" target=""#b49"">50]</ref> borrow the ideas from machine learning, such as transfer learning an ss-domain learning methods. Crossdomain learning methods such as transfer learning <ref type=""bibr"" target=""#b49"">[50]</ref> and adversarial learning <ref type=""bibr"" target=""#b19"">[20]</ref> latently generate features of data samples in the target domain, either by translating samples from the sour training the classifier each time new target domains are added. As an example, we evaluate the performance of an adversarial learning based model, EI <ref type=""bibr"" target=""#b19"">[20]</ref> over different domain factors (e.g., environment, location and orientation of the person). Specifically, the f-the-arts works. We compare Widar3.0 against several alternative state-of-the-arts methodologies, CARM <ref type=""bibr"" target=""#b43"">[44]</ref>, EI <ref type=""bibr"" target=""#b19"">[20]</ref> and CrossSense <ref type=""bibr"" target=""#b49"">[50]</ref>, where the latter two are feasible for cross-domain atter type, the idea of adversarial learning is usually adopted to shift the task of separating gesture-related features from domain-related ones. EI <ref type=""bibr"" target=""#b19"">[20]</ref> incorporates an adversarial network to obtain domain-independent features from CSI. However, crossdomain lea br"" target=""#b49"">50,</ref><ref type=""bibr"" target=""#b52"">53]</ref> and developing domain-independent features <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" target=""#b36"">37]</ref>. In the former type, WiAG <ref type=""bibr"" target=""#b38"">[39]</ref> d",1
"endencies of the whole series.</p><p>CNN is a useful technique to extract spatial features and compress data <ref type=""bibr"" target=""#b26"">[27,</ref><ref type=""bibr"" target=""#b46"">47]</ref>, and it is especially suitable for handling the single BVP, which is highly sparse but preserves spatial loca ticated activities with longer durations, the GRU-based models can be transformed into more complex versions <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b46"">47]</ref>. In ? 6.4, we will verify that single-layer GRUs are sufficient for capturing temporal dependencies for short",0
"s the core enabler for a wide range of applications such as smart home, security surveillance and virtual reality. Traditional approaches use cameras <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b41"">42]</ref>, wearable devices and phones",0
"y human, including DFS <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b31"">32,</ref><ref type=""bibr"" target=""#b43"">44]</ref>, ToF <ref type=""bibr"" target=""#b1"">[2]</ref><ref type=""bibr"" target=""#b2"">[3]</ref><ref type=""bibr"" target=""#b3"">[4]</ref><ref type=""bibr"" target=""#b20"">21 target=""#b1"">[2]</ref><ref type=""bibr"" target=""#b2"">[3]</ref><ref type=""bibr"" target=""#b3"">[4]</ref><ref type=""bibr"" target=""#b20"">21]</ref>, AoA/AoD <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b24"">25 [26,</ref><ref type=""bibr"" target=""#b31"">32]</ref>, gait <ref type=""bibr"" target=""#b42"">[43,</ref><ref type=""bibr"" target=""#b48"">49]</ref> and figure <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b18"">19]</ref>. Though not detailed enough, they provide coarse human movement inform",0
"features for target domains <ref type=""bibr"" target=""#b38"">[39,</ref><ref type=""bibr"" target=""#b39"">40,</ref><ref type=""bibr"" target=""#b49"">50,</ref><ref type=""bibr"" target=""#b52"">53]</ref> and developing domain-independent features <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b",0
"ign will be.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.2.2"">Predicting Performance in Solo-Mode.</head><p>Referring to prior work <ref type=""bibr"" target=""#b11"">[12]</ref>, we design shadow solo-cycle accounting (SSCA) approach to estimate workloads' execution time in solo mode b still introduce certain amount of inaccuracy. • The prediction method of QoSMT is inspired by PTA. PTA uses MLP correction to achieve higher accuracy <ref type=""bibr"" target=""#b11"">[12]</ref>. However, we can not get an application's MLP without offline profiling, so this correctness mechanism is no 7]</ref> providing solutions on improving overall SMT throughput and fairness, but they did not take performance control into account. Eyerman et al. <ref type=""bibr"" target=""#b11"">[12]</ref> proposed the per-thread cycle accounting (PTA) mechanism that is able to estimate a workload's solo performa",1
"ll throughput, but they did not investigate the impact of static partitioning on guaranteeing performance of specific threads.</p><p>Cakarevic et al. <ref type=""bibr"" target=""#b28"">[31]</ref> analyzed the impact of shared resources of UltraSPARC T2 that is a somewhat different fine-grain multithread",0
"avoid performance degradation of HPT.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""6.3"">Software Based Polices</head><p>Eyerman et al. <ref type=""bibr"" target=""#b12"">[13]</ref> proposed a job scheduler based on sampling mechanism along with hardware modification to achieve better thro",0
"bled. This is because unpredictable performance on a single server brought by SMT can make load balancing very difficult in large scale data centers. <ref type=""bibr"" target=""#b0"">[1]</ref> In this paper, we ask the following question: Can we obtain guaranteed performance of a high-priority workload ch stage and will increment the contention cycle value corresponding to ROB contention entry in the CRT.</p><p>There are two reasons for this design: <ref type=""bibr"" target=""#b0"">(1)</ref> The PEM needs to use contention cycles as input. Since contention information is distributed at serval stages,",0
"n SMT processor. Recent work has shown that applications may suffer varying performance degradation by as high as 70% due to SMT induced interference <ref type=""bibr"" target=""#b30"">[33]</ref>. To confirm this, we conduct some experiments on an Intel i7-4770 server with Hyper-Threading <ref type=""bib .</p><p>However, resource sharing within a physical core may cause performance degradation for logical threads. As illustrated in previous literature <ref type=""bibr"" target=""#b30"">[33]</ref>, a logical thread may suffer performance degradation by up to 70% when its demanding resources are occupied applicable to mainstream SMT processors because UltraSPARC T2's architecture is pretty different from Intel and AMD's design.</p><p>Recent work SMiTe <ref type=""bibr"" target=""#b30"">[33]</ref> uses a set of carefully designed microbenchmarks to perform offline sensitive analysis on various shared res",0
"rds in character sequence can provide rich boundary information for character-based model. To integrate words information into character-based model, <ref type=""bibr"" target=""#b40"">Zhang and Yang (2018)</ref> propose a lattice-structured LSTM model to encode a sequence of input characters as well as ock show the performance of the lattice model and characterbased model. The character baseline denotes the original character-based BiLSTM-CRF model. <ref type=""bibr"" target=""#b40"">Zhang and Yang (2018)</ref> propose a lattice LSTM to exploit word information in character sequence, giving the F1 sco ramework to integrate the task-shared word boundary information into Chinese NER task. Another way to obtain word boundary information is proposed by <ref type=""bibr"" target=""#b40"">(Zhang and Yang, 2018)</ref>, using a lattice LSTM to integrate word information into character-based model, which is s which are a set of character subsequences c b,i , where b &lt; i and c b,i matches a word in lexicon D. The lexicon D is the same as the one used in <ref type=""bibr"" target=""#b40"">(Zhang and Yang, 2018)</ref>, which is built by using automatically segmented large raw text. Similarly, we use ←− ws i 36"">Yang et al., 2017)</ref>, we concatenate each x c i with x − → ws i to utilize word information. And this is quite different from the way used in <ref type=""bibr"" target=""#b40"">(Zhang and Yang, 2018)</ref>, since they use extra shortcut paths to integrate word information into the hidden layer o el></formula><p>Given N manually labeled data {(s j , y j )}| N j=1 , we minimize the sentence-level negative loglikelihood loss to train the model:  <ref type=""bibr"" target=""#b40"">(Zhang and Yang, 2018)</ref>.</p><formula xml:id=""formula_16"">L = − j log(p(y j |s j ))<label>(15)</label></formula><p> mmarize the datasets in Table <ref type=""table"" target=""#tab_0"">1</ref>. Implementation Details. We utilize the character and word embeddings used in <ref type=""bibr"" target=""#b40"">(Zhang and Yang, 2018)</ref>, both of which are pre-trained on Chinese Giga-Word using word2vec model. Following <ref t ed in <ref type=""bibr"" target=""#b40"">(Zhang and Yang, 2018)</ref>, both of which are pre-trained on Chinese Giga-Word using word2vec model. Following <ref type=""bibr"" target=""#b40"">(Zhang and Yang, 2018)</ref>, we use the word embedding dictionary as Lexicon D in our model. For characters and words embeddings and word embeddings are updated along with other parameters.</p><p>For hyper-parameter configurations, we mostly refer to the settings in <ref type=""bibr"" target=""#b40"">(Zhang and Yang, 2018)</ref>. We set both character embedding size and word embedding size to 50. The dimensionality of f type=""bibr"" target=""#b7"">Dong et al. (2016)</ref> 91.28 90.62 90.95 <ref type=""bibr"" target=""#b0"">Cao et al. (2018)</ref> 91.73 89.58 90.64 Lattice <ref type=""bibr"" target=""#b40"">(Zhang and Yang, 2018)</ref>   approach to integrating word information is more reasonable than lattice model.</p></div table"" target=""#tab_6"">5</ref> shows the results on Chinese Resume dataset. Consistent with the previous results, our models outperform lattice model <ref type=""bibr"" target=""#b40"">(Zhang and Yang, 2018)</ref>. The above experimental results strongly verify that our method to utilize word informatio er explore the efficiency of our model, we conduct some comparative experiments on training time and convergence speed. The lattice model proposed in <ref type=""bibr"" target=""#b40"">(Zhang and Yang, 2018)</ref> is our principal comparison object, since it also utilizes the word information in charact e /></figure> <figure xmlns=""http://www.tei-c.org/ns/1.0"" type=""table"" xml:id=""tab_5""><head></head><label></label><figDesc>are the most common methods<ref type=""bibr"" target=""#b40"">Zhang and Yang, 2018)</ref> 94.81 94.11 94.46 Character baseline 93.26 93.44 93.35 WC-LSTM + shortest 94.97 94.91 94.94",1
"Ng, 2003)</ref>, Support Vector Machines(SVM) <ref type=""bibr"" target=""#b8"">(Ekbal and Bandyopadhyay, 2010)</ref> and Conditional Random Fields(CRF) <ref type=""bibr"" target=""#b9"">(Feng et al., 2006)</ref>. → ""水(Water)"", the model incorrectly predicts that ""江(River)"" and ""水(Water)"" belong to the sam",0
"ne) denote predicted labels, and blue labels(with underline) denote gold labels.</p><p>works <ref type=""bibr"" target=""#b13"">(Huang et al., 2015;</ref><ref type=""bibr"" target=""#b15"">Lample et al., 2016;</ref><ref type=""bibr"" target=""#b11"">Habibi et al., 2017)</ref> have been introduced to NER task. T arget=""#b21"">Ma and Hovy (2016)</ref> and <ref type=""bibr"" target=""#b4"">Chiu and Nichols (2016)</ref> use CNN to capture spelling characteristics and <ref type=""bibr"" target=""#b15"">Lample et al. (2016)</ref> use LSTM instead. When applied to Chinese NER, the above models all suffer from segmentation",0
"xml:id=""formula_16"">L = − j log(p(y j |s j ))<label>(15)</label></formula><p>For OntoNotes, we use the same training, development and test splits as <ref type=""bibr"" target=""#b1"">(Che et al., 2013)</ref>. For other datasets which have already been split, and we don't change them. We summarize the d </p><p>The first block in Table <ref type=""table"">2</ref> are the results of wordbased models <ref type=""bibr"" target=""#b31"">(Wang et al., 2013;</ref><ref type=""bibr"" target=""#b1"">Che et al., 2013;</ref><ref type=""bibr"" target=""#b34"">Yang et al., 2016)</ref>. By using gold-standard segmentation and",0
", the above models all suffer from segmentation errors, since Chinese word segmentation is compulsory for those models.</p><p>Character-Based Models. <ref type=""bibr"" target=""#b24"">Peng and Dredze (2015)</ref> propose to add segmentation features for better recognition of entity boundary. <ref type=",0
"br"" target=""#b9"">[10]</ref><ref type=""bibr"" target=""#b10"">[11]</ref><ref type=""bibr"" target=""#b11"">[12]</ref><ref type=""bibr"" target=""#b12"">[13]</ref><ref type=""bibr"" target=""#b13"">[14]</ref><ref type=""bibr"" target=""#b14"">[15]</ref><ref type=""bibr"" target=""#b15"">[16]</ref>. Adversarial examples can t augmented by adversarial examples from multiple static pre-trained models <ref type=""bibr"" target=""#b16"">[17]</ref>. In recent work, Brendel et al. <ref type=""bibr"" target=""#b13"">[14]</ref> proposed Boundary Attack, which generates adversarial examples via rejection sampling. While relying neither ://www.tei-c.org/ns/1.0""><head>A. Decision-based attacks</head><p>Most related to our work is the Boundary Attack method introduced by Brendel et al. <ref type=""bibr"" target=""#b13"">[14]</ref>. Boundary Attack is an iterative algorithm based on rejective sampling, initialized at an image that lies in models.</p><p>A. Efficiency evaluation a) Baselines: We compare HopSkipJumpAttack with three state-of-the-art decision-based attacks: Boundary Attack <ref type=""bibr"" target=""#b13"">[14]</ref>, Limited Attack <ref type=""bibr"" target=""#b8"">[9]</ref> and Opt Attack <ref type=""bibr"" target=""#b15"">[16]</ ious work, such as Carlini and Wagner <ref type=""bibr"" target=""#b5"">[6]</ref>. A version normalized by image dimension was employed by Brendel et al. <ref type=""bibr"" target=""#b13"">[14]</ref> for evaluating Boundary Attack. The As an alternative metric, we also plot the success rate at various dista rbed samples at the two sides of the boundary:</p><formula xml:id=""formula_38"">|E[φ x (x t + δ t u)]| &gt; 0,</formula><p>as we can see from Equation <ref type=""bibr"" target=""#b13"">(14)</ref>. To attempt to control the variance, we introduce a baseline φ x into the estimate:</p><formula xml:id=""form",1
"ype=""bibr"" target=""#b5"">[6]</ref><ref type=""bibr"" target=""#b6"">[7]</ref><ref type=""bibr"" target=""#b7"">[8]</ref><ref type=""bibr"" target=""#b8"">[9]</ref><ref type=""bibr"" target=""#b9"">[10]</ref><ref type=""bibr"" target=""#b10"">[11]</ref><ref type=""bibr"" target=""#b11"">[12]</ref><ref type=""bibr"" target=""#b1 e generation of adversarial examples under the score-based threat model <ref type=""bibr"" target=""#b7"">[8]</ref><ref type=""bibr"" target=""#b8"">[9]</ref><ref type=""bibr"" target=""#b9"">[10]</ref>.</p><p>Subsequent work <ref type=""bibr"" target=""#b21"">[22]</ref> proposed and analyzed an algorithm based on odel for each of the three threat models. Chen et al. <ref type=""bibr"" target=""#b7"">[8]</ref> and Ilyas et al. <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b9"">10]</ref> introduced score-based methods using zeroth-order gradient estimation to craft adversarial examples.</p><p>The",0
"ing data. Moreover, they can be defended against via training on a data set augmented by adversarial examples from multiple static pre-trained models <ref type=""bibr"" target=""#b16"">[17]</ref>. In recent work, Brendel et al. <ref type=""bibr"" target=""#b13"">[14]</ref> proposed Boundary Attack, which ge ut the label. Adversarial training <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b16"">17]</ref> is known to be one of the most effective defense mechanisms against adversarial perturbation <ref type=""bibr""",0
"years have witnessed a flurry of research on the design of new algorithms for generating adversarial examples <ref type=""bibr"" target=""#b0"">[1]</ref><ref type=""bibr"" target=""#b1"">[2]</ref><ref type=""bibr"" target=""#b2"">[3]</ref><ref type=""bibr"" target=""#b3"">[4]</ref><ref type=""bibr"" target=""#b4"">[5] cluding C&amp;W Attack <ref type=""bibr"" target=""#b5"">[6]</ref>, DeepFool <ref type=""bibr"" target=""#b3"">[4]</ref> for minimizing 2 -distance, and FGSM <ref type=""bibr"" target=""#b1"">[2]</ref>, and BIM <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b40"">41]</ref> for minimizing ∞ -dis rization <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b5"">6]</ref> or via tackling the dual as a constrained optimization problem <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b6"">7]</ref>. In the black-box setting, an atta hat φ x (x ) = 1,<label>(2)</label></formula><p>where d is a distance function that quantifies similarity. Standard choices of d studied in past work <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b5"">6]</ref> include the usual p -norms, for p 0, and ""sign"" returns the element-wise sign of a vector. We use the sign of the gradient for faster convergence in practice, similar to previous work <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b6"">7]</ref>.</p><formula xml:id=""formula_18"">I input image, predicts the label for each sampled point with the base model, and then takes a majority vote to output the label. Adversarial training <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b16"">17]< . . .. (<label>5</label></formula><formula xml:id=""formula_11"">)</formula><p>In particular, the algorithm converges to a stationary point of problem <ref type=""bibr"" target=""#b1"">(2)</ref>.</p><p>Theorem 1 suggests a scheme for choosing the step size in the algorithm that we present in the next sec",0
"ine. 2 Our algorithm is also 2 See https://github.com/Jianbo-Lab/HSJA/. available on CleverHans <ref type=""bibr"" target=""#b24"">[25]</ref> and Foolbox <ref type=""bibr"" target=""#b25"">[26]</ref>, which are two popular Python packages to craft adversarial examples for machine learning models.</p><p>A. E with uniform random noise, and increasing the weight of uniform noise gradually until it is misclassified, a procedure which is available on Foolbox <ref type=""bibr"" target=""#b25"">[26]</ref>, as the default initialization of Boundary Attack.</p><p>For targeted attack, the target class is sampled un",0
"script and Chinese vocab released in BERT. For text generation, we implement truncated top-k sampling instead of beam-search to generate diverse text <ref type=""bibr"" target=""#b5"">[6]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2.2"">Data processing</head><p>The training proces",1
"/ref><ref type=""bibr"" target=""#b7"">8]</ref> on Chinese poetry generation have been mostly rulebased or template-based. Recurrent Neural Network (RNN) <ref type=""bibr"" target=""#b10"">[11]</ref> was recently introduced as it has been proved to be effective in generation tasks such as machine translatio",1
"as a theme clue for generating the next line. <ref type=""bibr"" target=""#b13"">Yang et al. (2017)</ref>  <ref type=""bibr"" target=""#b13"">[14]</ref> and <ref type=""bibr"" target=""#b12"">Wang et al. (2016)</ref>  <ref type=""bibr"" target=""#b12"">[13]</ref> employ a two-stage approach,</p><formula xml:id=""fo ""bibr"" target=""#b13"">Yang et al. (2017)</ref>  <ref type=""bibr"" target=""#b13"">[14]</ref> and <ref type=""bibr"" target=""#b12"">Wang et al. (2016)</ref>  <ref type=""bibr"" target=""#b12"">[13]</ref> employ a two-stage approach,</p><formula xml:id=""formula_3"">五绝(Wujue)•秋思 暮燕翻惊户， 飞鸿却唤人。 西风卷梧叶， 触落一庭秋。 七绝(Qiju",1
"云霞岭上日倾杯</formula></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4"">Related Work</head><p>Early works <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b7"">8]</ref> on Chinese poetry generation have",0
"ipai(词牌) including Xijiangyue(西江 月), Manjianghong(满江红), Shuidiaogetou(水调歌头), etc .</p><p>Various methods e.g., <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref> have been proposed to generate classical Chinese poetry. However, these methods are somewhat complicated so as",0
"allenge of natural language generation. Unlike free text generation, a classical Chinese poem should normally meet both form and content requirements <ref type=""bibr"" target=""#b0"">[1]</ref>. The form requirements includes the regulations on the number of words (字数), rhyming (押韵), tone patterns (平仄), me other works have been investigating the coherence of content throughout a poem. For example, <ref type=""bibr"" target=""#b0"">Yi et al. (2018)</ref>  <ref type=""bibr"" target=""#b0"">[1]</ref> propose a salientclue mechanism which automatically selects the most salient characters from the so-far genera (七 绝), Wulü(五律), Qilü(七律), and vaious Cipai(词牌) including Xijiangyue(西江 月), Manjianghong(满江红), Shuidiaogetou(水调歌头), etc .</p><p>Various methods e.g., <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref> have been proposed to generate classical Chinese poetry. However, these il a well-formed one is obtained. In the mean time, some other works have been investigating the coherence of content throughout a poem. For example, <ref type=""bibr"" target=""#b0"">Yi et al. (2018)</ref>  <ref type=""bibr"" target=""#b0"">[1]</ref> propose a salientclue mechanism which automatically sele",0
"://www.tei-c.org/ns/1.0""><head n=""2.1"">Model Details</head><p>We refer the readers to the blog<ref type=""foot"" target=""#foot_0"">3</ref> or the papers <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b3"">4]</ref> to get a basic understanding of Tr",0
"烟雨江南春纵笔 花红柳绿，桃源粤地燕裁云 诗词歌赋，风花塞外夏抒怀 天光风韵，云霞岭上日倾杯</formula></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4"">Related Work</head><p>Early works <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b7"">8]</",0
".0""><p>We present a simple yet effective method for generating high quality classical Chinese poetry with Generative Pre-trained Language Model (GPT) <ref type=""bibr"" target=""#b4"">[5]</ref>. The method adopts a simple GPT model, without using any human crafted rules or features, or designing any add Model Details</head><p>We refer the readers to the blog<ref type=""foot"" target=""#foot_0"">3</ref> or the papers <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b3"">4]</ref> to get a basic understanding of Transformer, which is the underlying mode",0
"aders to the blog<ref type=""foot"" target=""#foot_0"">3</ref> or the papers <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b3"">4]</ref> to get a basic understanding of Transformer, which is the underlying model of the proposed method. Figure <ref",0
"Related Work</head><p>Early works <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b7"">8]</ref> on Chinese poetry generation have been mostly rulebased or template-based. Recurrent Neural Network (RNN) <ref",0
"o make sure the rhyming, the tone patterns and the pairing of a generation are correct, various strategies have been adopted. For example, Yan (2016) <ref type=""bibr"" target=""#b11"">[12]</ref> proposes an iterative polishing schema, which refines the generated poem until a well-formed one is obtained",0
"tclue mechanism which automatically selects the most salient characters from the so-far generated lines as a theme clue for generating the next line. <ref type=""bibr"" target=""#b13"">Yang et al. (2017)</ref>  <ref type=""bibr"" target=""#b13"">[14]</ref> and <ref type=""bibr"" target=""#b12"">Wang et al. (201 ent characters from the so-far generated lines as a theme clue for generating the next line. <ref type=""bibr"" target=""#b13"">Yang et al. (2017)</ref>  <ref type=""bibr"" target=""#b13"">[14]</ref> and <ref type=""bibr"" target=""#b12"">Wang et al. (2016)</ref>  <ref type=""bibr"" target=""#b12"">[13]</ref> emplo",0
"p://www.tei-c.org/ns/1.0""><head n=""4"">Related Work</head><p>Early works <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b7"">8]</ref> on Chinese poetry generation have been mostly rulebased or template-based",0
"el is demanding, as these dynamics are very distinct and have high intersubject variability. To learn personalized models for each student, we follow <ref type=""bibr"" target=""#b3"">(Jaques et al., 2017)</ref> and use a Multitask approach which comprises of a LSTM to model sequence of histograms follo",1
", heterogeneity in granularity and inter subject variability in behavioural and environmental patterns have stymied predictive modeling of this kind. <ref type=""bibr"" target=""#b8"">(Mikelsons et al., 2018)</ref> have tried to predict stress of students in the StudentLife dataset by novel feature engi ><head n=""3.2."">Models</head></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.2.1."">LOCATION FEATURE BASED MLP</head><p>In the work done by <ref type=""bibr"" target=""#b8"">(Mikelsons et al., 2018)</ref>, a Multilayer Perceptron (MLP) with 4 fully connected layers was employed to perform stre /p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4."">Result</head><p>Due to a heavy imbalance of class labels on a scale of 1-5, we follow <ref type=""bibr"" target=""#b8"">(Mikelsons et al., 2018)</ref>, converting the five stress label scale to a scale of three stress labels by defining our .1. Used Features</head><p>In all, our data comprises of 23 students, totaling to 1183 data points achieving roughly equal amount of training data in <ref type=""bibr"" target=""#b8"">(Mikelsons et al., 2018)</ref>. These 1183 data points have the following label distribution -263 below median stress, 5",1
"ser's stress through self-reporting tools e.g., <ref type=""bibr"" target=""#b9"">(Rahman et al., 2014)</ref> and surveys like the Perceived Stress Scale <ref type=""bibr"" target=""#b0"">(Cohen et al., 1983)</ref>.</p><p>With the induction of high quality, robust sensors in wear-Proceedings of the 1 st Ada",0
"em <ref type=""bibr"" target=""#b6"">(Khansari et al., 1990)</ref>, and poor academic performance <ref type=""bibr"" target=""#b12"">(Sano et al., 2015;</ref><ref type=""bibr"" target=""#b16"">Trokel et al., 2000)</ref>. With the efforts of researchers at various institutions several technologies for detecting of stress in <ref type=""bibr"" target=""#b15"">(Stults-Kolehmainen &amp; Sinha, 2014;</ref><ref type=""bibr"" target=""#b11"">Sano &amp; Picard, 2013;</ref><ref type=""bibr"" target=""#b16"">Trokel et al., 2000;</ref><ref type=""bibr"" target=""#b12"">Sano et al., 2015)</ref> etc.</p><p>Among the discrete sequenc",0
">(Setz et al., 2010)</ref>. Other techniques do not depend on sensors but simply try to discover the user's stress through self-reporting tools e.g., <ref type=""bibr"" target=""#b9"">(Rahman et al., 2014)</ref> and surveys like the Perceived Stress Scale <ref type=""bibr"" target=""#b0"">(Cohen et al., 198",0
"nged exposure to stressful academic and social environment causes cardiovascular diseases <ref type=""bibr"" target=""#b10"">(Rozanski et al., 1999;</ref><ref type=""bibr"" target=""#b5"">Kario et al., 2003)</ref>, alterations of the brain causing differences in memory and cognition <ref type=""bibr"" target=",0
"students with assignments, tests and part-time work. A prolonged exposure to stressful academic and social environment causes cardiovascular diseases <ref type=""bibr"" target=""#b10"">(Rozanski et al., 1999;</ref><ref type=""bibr"" target=""#b5"">Kario et al., 2003)</ref>, alterations of the brain causing",0
"observed trade-offs, and indeed this appears to be true. In particular we demonstrate that the recently proposed AutoAugment data augmentation policy <ref type=""bibr"" target=""#b5"">[6]</ref> achieves state-of-the-art results on the CIFAR-10-C benchmark. In addition, a follow-up work has utilized Auto tive solution is to train on a variety of data augmentation strategies. Towards this end, we investigated the learned augmentation policy AutoAugment <ref type=""bibr"" target=""#b5"">[6]</ref>. AutoAugment applies a learned mixture of image transformations during training and achieves the state-of-thea",1
"e. These qualify as simple, single query <ref type=""foot"" target=""#foot_0"">3</ref> black box attacks that satisfy the content preserving threat model <ref type=""bibr"" target=""#b12"">[13]</ref>. This observation was also made in concurrent work <ref type=""bibr"" target=""#b25"">[26]</ref>.</p><p>Finally,",1
"end on how the adversary constructs the perturbation. This difficulty has led to many false claims of methods for detecting adversarial perturbations <ref type=""bibr"" target=""#b4"">[5]</ref>. Thus the analysis presented here is to better understand common hypothesis about adversarial perturbations, r",0
"challenging given the tradeoffs we commonly observe. Naively augmenting on different corruptions often will not transfer well to held out corruptions <ref type=""bibr"" target=""#b11"">[12]</ref>. However, the impressive robustness of AutoAugment gives us hope that data augmentation done properly can pl",0
"esults on the CIFAR-10-C benchmark. In addition, a follow-up work has utilized AutoAugment in a way to achieve state-of-the-art results on ImageNet-C <ref type=""bibr"" target=""#b0"">[1]</ref>. Some of our observations could be of interest to research on security. For example, we observe perturbations ppendix. As for the compressed ImageNet-C images, we note that a follow-up work has utilized AutoAugment in a way to achieve state-of-the-art results <ref type=""bibr"" target=""#b0"">[1]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.4"">Adversarial examples are not strictly a high",0
"tional shift simply because there is no reason for them to be robust <ref type=""bibr"" target=""#b19"">[20,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b17"">18]</ref>. In naturally occurring data there are many correlations between the input and target that models can utilize hat models leverage these non-robust statistics.</p><p>It seems likely that these invisible high frequency features are related to the experiments of <ref type=""bibr"" target=""#b17"">[18]</ref>, which show that certain imperceptibly perturbed images contain features which are useful for generalization",0
"mentation and adversarial training improve robustness to noise and blurring corruptions on the CIFAR-10-C and ImageNet-C common corruption benchmarks <ref type=""bibr"" target=""#b16"">[17]</ref>, while significantly degrading performance on the fog and contrast corruptions. This begs a natural question mong the four models, AutoAugment achieves the best average corruption test accuracy of 86%. Using the mean corruption error (mCE) metric proposed in <ref type=""bibr"" target=""#b16"">[17]</ref> with the naturally trained model being the baseline (see a formal definition of mCE in the appendix), we obs eves the best performance among the four models.</p><p>As for the ImageNet-C benchmark, instead of using the compressed ImageNet-C images provided in <ref type=""bibr"" target=""#b16"">[17]</ref>, we evaluate the models on corruptions applied in memory, <ref type=""foot"" target=""#foot_2"">5</ref> and obse",0
"mproves robustness across all corruption types. Performance gains on some corruptions may be met with dramatic reduction on others. As an example, in <ref type=""bibr"" target=""#b9"">[10]</ref> it was observed that Gaussian data augmentation and adversarial training improve robustness to noise and blur",0
"Data augmentation is a natural and sometimes effective approach to learning robust models. Examples of data augmentation include adversarial training <ref type=""bibr"" target=""#b13"">[14]</ref>, applying image transformations to the training data, such as flipping, cropping, adding random noise, and e",0
"f ImageNet models when severe filtering is performed on the input in the frequency domain. While modest filtering has been used for model compression <ref type=""bibr"" target=""#b8"">[9]</ref>, we experiment with extreme filtering in order to test the limits of model generalization. The results are sho",0
"""> 		<body> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""1"">Introduction</head><p>Bidirectional Encoder Representations from Transformers (BERT) <ref type=""bibr"" target=""#b8"">(Devlin et al., 2019)</ref> has become enormously popular and proven to be effective in recent NLP studies which utilize from the pre-training data (Wikipedia for BERT/BERT-wwm), we suggest taking another pre-training steps on the task data, which was also suggested by <ref type=""bibr"" target=""#b8"">(Devlin et al., 2019)</ref>.</p><p>• As there are so many possibilities in pretraining stage (such as initial learning r 1 :</head><label>1</label><figDesc>Hyper-parameter settings and data statistics in different task. † represents the dataset was also evaluated by BERT<ref type=""bibr"" target=""#b8"">(Devlin et al., 2019)</ref>. ‡ represents the dataset was also evaluated by ERNIE<ref type=""bibr"" target=""#b19"">(Sun et d English models (BERTlarge-wwm) for the community, which is beneficial for the researcher to design more powerful models based on them.</p><p>Before <ref type=""bibr"" target=""#b8"">Devlin et al. (2019)</ref> releasing BERT with whole word masking, <ref type=""bibr"" target=""#b19"">Sun et al. (2019)</ref g</head><p>We downloaded the latest Wikipedia dump<ref type=""foot"" target=""#foot_1"">3</ref> , and pre-processed with WikiExtractor.py as suggested by <ref type=""bibr"" target=""#b8"">Devlin et al. (2019)</ref>  <ref type=""bibr"" target=""#b8"">Devlin et al. (2019)</ref>, for computation efficiency and lea e=""foot"" target=""#foot_1"">3</ref> , and pre-processed with WikiExtractor.py as suggested by <ref type=""bibr"" target=""#b8"">Devlin et al. (2019)</ref>  <ref type=""bibr"" target=""#b8"">Devlin et al. (2019)</ref>, for computation efficiency and learning long-range dependencies.</p></div> <div xmlns=""http:",1
"n BERT and ERNIE. We adopt additional datasets for testing their performance in a wider range.</p><p>• Machine Reading Comprehension (MRC): CMRC 2018 <ref type=""bibr"" target=""#b6"">(Cui et al., 2019)</ref>, DRCD <ref type=""bibr"" target=""#b18"">(Shao et al., 2018</ref><ref type=""bibr"">), CJRC (Duan et",0
"QuAC <ref type=""bibr"" target=""#b3"">(Choi et al., 2018)</ref>, NaturalQuestions <ref type=""bibr"" target=""#b10"">(Kwiatkowski et al., 2019)</ref>, RACE <ref type=""bibr"" target=""#b11"">(Lai et al., 2017)</ref>, we can see that most of the top performing models are based on BERT and its variants <ref typ",0
"e model comparisons are depicted in Table <ref type=""table"" target=""#tab_2"">2</ref>.</p><p>We carried out all experiments under Tensor-Flow framework <ref type=""bibr"" target=""#b0"">(Abadi et al., 2016)</ref>. Note that, ERNIE only provides PaddlePaddle version<ref type=""foot"" target=""#foot_7"">9</ref>",0
"r"" target=""#b5"">(Cui et al., 2017;</ref><ref type=""bibr"" target=""#b7"">Dai et al., 2019;</ref><ref type=""bibr"" target=""#b22"">Zhang et al., 2019a;</ref><ref type=""bibr"" target=""#b16"">Ran et al., 2019)</ref>.</p><p>Recently, the authors of BERT have released an updated version of BERT, which is called",0
"aphical structure data can be dated back to the research of graph embedding ( <ref type=""bibr"" target=""#b15"">Perozzi, Al-Rfou, and Skiena, 2014;</ref><ref type=""bibr"" target=""#b4"">Grover and Leskovec, 2016)</ref>, where graph topology and node relations are embedded as vector space. In light of the mulate as the depth increases.</p><p>We contrast the performance of our algorithm against several unsupervised graph learning counter-parts: Node2Vec <ref type=""bibr"" target=""#b4"">(Grover and Leskovec, 2016)</ref>, VGAE <ref type=""bibr"" target=""#b9"">(Kipf and Welling, 2016)</ref>, Graph-SAGE <ref ty and graph convolutional networks (GCN)based methods. DeepWalk <ref type=""bibr"" target=""#b15"">(Perozzi, Al-Rfou, and Skiena, 2014)</ref> and Node2vec <ref type=""bibr"" target=""#b4"">(Grover and Leskovec, 2016)</ref> are representative random walk-based methods to model homogeneous graphs. Some follow- s indicates a naive classification model that learns from nodes' raw features, without using any graph structure information incorporated. • Node2Vec <ref type=""bibr"" target=""#b4"">(Grover and Leskovec, 2016)</ref>: This approach is an extension of Word2Vec <ref type=""bibr"" target=""#b13"">(Mikolov et",1
"over and Leskovec, 2016)</ref>, where graph topology and node relations are embedded as vector space. In light of the rapid advances of deep learning <ref type=""bibr"" target=""#b11"">(LeCun, Bengio, and Hinton, 2015)</ref>, current research attention has been paid to the application of deep neural net",0
"and even high-order proximities between vertices on homogeneous graphs, such as LINE <ref type=""bibr"" target=""#b19"">(Tang et al., 2015)</ref>, GraRep <ref type=""bibr"" target=""#b0"">(Cao, Lu, and Xu, 2015)</ref>, SDNE <ref type=""bibr"" target=""#b21"">(Wang, Cui, and Zhu, 2016)</ref>, DVNE <ref type=""bib",0
"r algorithm against several unsupervised graph learning counter-parts: Node2Vec <ref type=""bibr"" target=""#b4"">(Grover and Leskovec, 2016)</ref>, VGAE <ref type=""bibr"" target=""#b9"">(Kipf and Welling, 2016)</ref>, Graph-SAGE <ref type=""bibr"" target=""#b5"">(Hamilton, Ying, and Leskovec, 2017)</ref>, and l Networks (GCN)-based methods have a rapid development recently. Among them, GCN <ref type=""bibr"" target=""#b10"">(Kipf and Welling, 2017)</ref>, VGAE <ref type=""bibr"" target=""#b9"">(Kipf and Welling, 2016)</ref>, GraphSAGE <ref type=""bibr"" target=""#b5"">(Hamilton, Ying, and Leskovec, 2017)</ref> show g a biased random walks on the graph. We run Node2Vec on the bipartite graph and then concatenate the node embeddings with their own features. • VGAE <ref type=""bibr"" target=""#b9"">(Kipf and Welling, 2016)</ref>: This method is based on variational auto-encoder, where GCN is used as an encoder and a",0
"ref type=""bibr"" target=""#b33"">[34]</ref> with 66M parameters using 600⇥600 images for training. In this paper, we focus on the ResNet-50 architecture <ref type=""bibr"" target=""#b10"">[11]</ref> due to its good accuracy/cost tradeoff (25.6M parameters) and its popularity. We also conduct some experimen ning results. Architectures. We use standard state-of-the-art neural network architectures with no modifications, We consider in particular ResNet-50 <ref type=""bibr"" target=""#b10"">[11]</ref>. For larger experiments, we use PNASNet-5-Large <ref type=""bibr"" target=""#b20"">[21]</ref>, learned using neu ombining multiple data augmentations at test time, although this means that sev- eral forward passes are required to classify one image. For example, <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b31"">32]</ref> used ten crops (one central, corner of the image and their mirrored versions). Another performanceboosting strategy is to classify an image by feeding it at multiple resolutions <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b29"">30,</ref><ref type=""bibr"" target=""#b31"">32]</ref>, again averaging the predict",1
"olutional Neural Networks <ref type=""bibr"" target=""#b17"">[18]</ref> (CNNs) are used extensively in computer vision tasks such as image classification <ref type=""bibr"" target=""#b16"">[17]</ref>, object detection <ref type=""bibr"" target=""#b26"">[27]</ref>, inpainting <ref type=""bibr"" target=""#b36"">[37]< est time, although this means that sev- eral forward passes are required to classify one image. For example, <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b31"">32]</ref> used ten crops (one central, and one for each corner of the image and",0
"e initial learning rate is 0.008 same decay, B = 512, 60 epochs. For ResNeXt-101 32x48d we use the pretrained version from the PyTorch hub repository <ref type=""bibr"" target=""#b1"">[2]</ref>. We also use a ten times smaller learning rate and a batch size two times smaller. For PNASNet-5-Large we use",0
"ons drawn by Boureau et al. <ref type=""bibr"" target=""#b5"">[6]</ref>. Similar pooling techniques have been employed in image retrieval for a few years <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b34"">35]</ref>, where high-resolution images are required to achieve a competitive",0
"ained for image classification, usually on the ImageNet database <ref type=""bibr"" target=""#b7"">[8]</ref>, transfer to a variety of other applications <ref type=""bibr"" target=""#b23"">[24]</ref>. Furthermore, advances in image classification translate to improved results on many other tasks <ref type=""",0
"ict the structure of the computed network. Recently, ""feature-wise linear modulations"" (FiLM) were introduced in the visual question answering domain <ref type=""bibr"" target=""#b10"">(Perez et al., 2017)</ref>. Here, the hypernetwork is fed with an encoding of a question and produces an element-wise a e chunks of the node representation at a time. In the extreme case in which the dimension of each chunk is 1, this method coincides with the ideas of <ref type=""bibr"" target=""#b10"">Perez et al. (2017)</ref>, who propose to use layers of element-wise affine transformations to modulate feature maps in",1
"s of the model are the edge-type-dependent weights W and the recurrent cell parameters ? r .</p><p>In Relational Graph Convolutional Networks (R-GCN) <ref type=""bibr"" target=""#b12"">(Schlichtkrull et al., 2018)</ref>, the gated unit is replaced by a simple non-linearity ? (e.g., the hyperbolic tangen nied by implementations of a range of baseline methods. These include GGNN <ref type=""bibr"" target=""#b8"">(Li et al., 2016</ref>) (see Eq. (1)), R-GCN <ref type=""bibr"" target=""#b12"">(Schlichtkrull et al., 2018</ref>) (see Eq. ( <ref type=""formula"">2</ref>)), R-GAT <ref type=""bibr"" target=""#b16"">(Veli",0
"p://www.tei-c.org/ns/1.0""><head n=""3.3.3"">VARIABLE USAGE IN PROGRAMS (VARMISUSE)</head><p>Finally, the models were evaluated on the VarMisuse task of <ref type=""bibr"" target=""#b0"">Allamanis et al. (2018)</ref>. This task requires to process a graph representing an abstraction of a program fragment a t ranking of model architectures as the results on PPI and QM9, with R-GCN performing best. All re-implemented baselines beat the results reported by <ref type=""bibr"" target=""#b0"">Allamanis et al. (2018)</ref>, who also reported that R-GCN and GGNN show very similar performance. This is in spite of (2018)</ref>. ? The results in Tab. 3 indicate that R-GCNs are outperforming GGNNs substantially on the VarMisuse task, contradicting the findings of <ref type=""bibr"" target=""#b0"">Allamanis et al. (2018)</ref>. ? The GNN-MLP models are obvious extensions that are often alluded to, but are not part o rg/ns/1.0"" type=""table"" xml:id=""tab_3""><head>Table 3 :</head><label>3</label><figDesc>Accuracy on VarMisuse task. GGNN * result taken from appendix of<ref type=""bibr"" target=""#b0"">Allamanis et al. (2018)</ref>.</figDesc><table><row><cell>Model</cell><cell>TRAIN</cell><cell>VALID</cell><cell cols=""3""",0
"actical because the prediction of weights of non-trivial neural networks is computationally expensive.</p><p>Approaches to mitigate this exist (e.g., <ref type=""bibr"" target=""#b17"">Wu et al. (2019)</ref> handle this in natural language processing), but are often domain-specific.</p><p>A more general f, ,C . The learnable parameters of the model are only the hypernetwork parameters ? f, ,c . This is somewhat less desirable than the related idea of <ref type=""bibr"" target=""#b17"">Wu et al. (2019)</ref>, which operates on sequences, where sharing between neighbouring elements of the sequence has an",0
"tentially the edge type) and the target node is not taken into consideration. A (partial) exception to this is the family of Graph Attention Networks <ref type=""bibr"" target=""#b16"">(Veli?kovi? et al., 2018)</ref>, where the agreement between source and target representation of an edge is used to det ed to contain a special edge type 0 for self-loops v 0 ? v, allowing state associated with a node to be kept.</p><p>In Graph Attention Networks (GAT) <ref type=""bibr"" target=""#b16"">(Veli?kovi? et al., 2018)</ref>, new node representations are computed from a weighted sum of neighbouring node represe ., 2016</ref>) (see Eq. (1)), R-GCN <ref type=""bibr"" target=""#b12"">(Schlichtkrull et al., 2018</ref>) (see Eq. ( <ref type=""formula"">2</ref>)), R-GAT <ref type=""bibr"" target=""#b16"">(Veli?kovi? et al., 2018)</ref> (see Eq. (3)), and R-GIN (Hamilton et al., 2017) (see Eq. (4))<ref type=""foot"" target="" eviations and training times in seconds computed over the ten runs. The results for all re-implemented models are better than the results reported by <ref type=""bibr"" target=""#b16"">Veli?kovi? et al. (2018)</ref> for the GAT model (without edge types). A cursory exploration of the reasons yielded thr cantly. Second, using dropout between layers significantly improved the results. Third, the larger node representation sizes (compared to 256 used by <ref type=""bibr"" target=""#b16"">Veli?kovi? et al. (2018)</ref>) improved the results again. However, the new GNN-FiLM improves slightly over these four elines:</p><p>? The results in Tab. 1 indicate that GATs have no advantage over GGNNs or R-GCNs on the PPI task, which does not match the findings by <ref type=""bibr"" target=""#b16"">Veli?kovi? et al. (2018)</ref>. ? The results in Tab. 3 indicate that R-GCNs are outperforming GGNNs substantially on t tp://www.tei-c.org/ns/1.0"" type=""table"" xml:id=""tab_1""><head>Table 1 :</head><label>1</label><figDesc>GNN results on PPI task. GAT * result taken from<ref type=""bibr"" target=""#b16"">Veli?kovi? et al. (2018)</ref>.</figDesc><table><row><cell>Model</cell><cell cols=""2"">Avg. Micro-F1 Time (s)</cell></ro",0
"p><p>In contrast to existing works on provable robustness for classical neural networks/robust training (e.g. <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b19"">20]</ref>), we tackle various additional challenges: Being the first work for g As an alternative, recent works have considered certifiable robustness <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b19"">20]</ref> providing guarantees that no perturbation w.r.t. a specific perturbat l examples to lie on a small ϵ-ball around the original sample measured by, e.g., the infinity-norm or L2-norm <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b19"">20]</ref>, often e.g. ϵ &lt; 0.1 This is clearly not practical in our binary se ation space will change an instance's prediction.</p><p>For this work, specifically the class of methods based on convex relaxations are of relevance <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b19"">20]</ref>. They construct a convex relaxation for computing a lower bound on t urbations.</p><p>The proof is in the appendix. For the remaining layers, since the input to them is no longer binary, we adapt the bounds proposed in <ref type=""bibr"" target=""#b17"">[18]</ref>. Generalized to the GNN we therefore obtain:</p><formula xml:id=""formula_26"">R (l ) = A (l −1) R (l −1) W (l",1
"also the unlabeled nodes into account.</p><p>In contrast to existing works on provable robustness for classical neural networks/robust training (e.g. <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b19"">20]</ref>), we tackle various additiona ttack methods, leading to an arms race. As an alternative, recent works have considered certifiable robustness <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b19"">20]</ref> providing guarantees that no p",0
"omer types in e-commerce networks <ref type=""bibr"" target=""#b5"">[6]</ref>, or the assignment of scientific papers from a citation network into topics <ref type=""bibr"" target=""#b11"">[12]</ref>. While there exist many classical approaches to node classification <ref type=""bibr"" target=""#b1"">[2,</ref>< ility of assigning node v to class c. The Â(l) are the message passing matrices that define how the activations are propagated in the network. In GCN <ref type=""bibr"" target=""#b11"">[12]</ref>, for example,</p><formula xml:id=""formula_2"">Â(1) = ... = Â(L−1) = D− 1 2 Ã D− 1 2</formula><p>, where Ã = A ><p>While our method can be used for any GNN of the form in Eq. ( <ref type=""formula"" target=""#formula_0"">1</ref>), we study the well-established GCN <ref type=""bibr"" target=""#b11"">[12]</ref>, which has shown to outperform many more complicated models. Following <ref type=""bibr"" target=""#b11"">[12]</ ), we study the well-established GCN <ref type=""bibr"" target=""#b11"">[12]</ref>, which has shown to outperform many more complicated models. Following <ref type=""bibr"" target=""#b11"">[12]</ref>, we consider GCNs with one hidden layer (i.e. L = 3), and choose a latent dimensionality of 32. We split the much attention and improved the state of the art in node classification <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b12"">13]</ref>.</p><p>However, there is one big catch: Recently it has been shown th s negative and X nd = 1). Thus, the maximal increase of Ĥ (2) mj based on X nd one can achieve is A d j ] − • X nd ) , which matches the terms in Eq. <ref type=""bibr"" target=""#b11"">(12)</ref>. To obtain the maximal overall increase in Ĥ (2) mj , and, thus, an upper bound, one simply picks the larges",0
"networks <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b21"">22]</ref> and node embeddings <ref type=""bibr"" target=""#b0"">[1]</ref>. All of these works focus on generating adversarial examples. In contrast, we provide the first work to certif",0
"ure work.</p><p>For 'classical' neural networks various heuristic approaches have been proposed to improve the the robustness to adversarial examples <ref type=""bibr"" target=""#b16"">[17]</ref>. However, such heuristics are often broken by new attack methods, leading to an arms race. As an alternative",0
"earn a hierarchy of translation invariant feature detectors.</p><p>The success of CNNs in Computer Vision and other domains has motivated researchers <ref type=""bibr"" target=""#b4"">(Bruna et al., 2014;</ref><ref type=""bibr"" target=""#b7"">Defferrard et al., 2016;</ref><ref type=""bibr"" target=""#b13"">Kip o learn location-invariant node and neighborhood features.</p><p>Early extensions of the graph convolution (GC) operator were theoretically motivated <ref type=""bibr"" target=""#b4"">(Bruna et al., 2014)</ref>, but (1) required quadratic computational complexity in number of nodes and therefore were no",0
"rvised node classification experiments on synthetic and real-world datasets.</p><p>Synthetic Datasets: Our synthetic datasets are generated following <ref type=""bibr"" target=""#b12"">Karimi et al. (2017)</ref>. We generate 10 graphs, each with a different homophily coefficient (ranging from 0.0 to 0.9",0
"mlns=""http://www.tei-c.org/ns/1.0""><head n=""5.2."">Training</head><p>For all experiments, we construct a 2-layer network of our model using TensorFlow <ref type=""bibr"" target=""#b0"">(Abadi et al., 2016)</ref>. We train our models using a Gradient Descent optimizer for a maximum of 2000 steps, with an",0
"ors.</p><p>The success of CNNs in Computer Vision and other domains has motivated researchers <ref type=""bibr"" target=""#b4"">(Bruna et al., 2014;</ref><ref type=""bibr"" target=""#b7"">Defferrard et al., 2016;</ref><ref type=""bibr"" target=""#b13"">Kipf &amp; Welling, 2017)</ref> to extend the convolutional ore were not scalable to large graphs, and (2) required the graph to be completely observed during training, targeting only the transductive setting. <ref type=""bibr"" target=""#b7"">Defferrard et al. (2016)</ref> and <ref type=""bibr"" target=""#b13"">Kipf &amp; Welling (2017)</ref> propose GC approximati e, enabling our method to learn neighborhood mixing e.g. delta operators, which contrast the features of immediate neighbors from those further away. <ref type=""bibr"" target=""#b7"">Defferrard et al. (2016)</ref> uses more Chebyshev polynomials (i.e. higher-rank) Graph Convolution, but their model und",0
"f>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.3."">Geometric scattering compared to other feed forward graph ConvNets</head><p>We  <ref type=""bibr"" target=""#b16"">Gatys et al., 2015)</ref>.</p><p>A graph wavelet transform Ψ (J) x decomposes the geometry of G through the lens of x,",0
"cortices, thus supporting the neural terminology in deep learning. An alternative approach towards such universal feature extraction was presented in <ref type=""bibr"" target=""#b23"">Mallat (2012)</ref>, where a deep filter bank, known as the scattering transform, is designed, rather than trained, bas t=""#b28"">(Perlmutter et al., 2018)</ref>, which we will generally term ""geometric scattering."" These works mostly focus on following the footsteps of <ref type=""bibr"" target=""#b23"">Mallat (2012)</ref> in establishing the stability of their respective constructions to deformations of input signals or ttering on Graphs</head><p>A geometric wavelet scattering transform follows a similar construction as the (Euclidean) wavelet scattering transform of <ref type=""bibr"" target=""#b23"">Mallat (2012)</ref>, but leverages a graph wavelet transform.</p><p>In this paper we utilize the wavelet transform defi /p><p>We note that even in the classical Euclidean case, while the stability of scattering transforms to deformations can be established analytically <ref type=""bibr"" target=""#b23"">(Mallat, 2012)</ref>, their capacity is typically examined by empirical evidence when applied to machine learning tasks",0
"tion (PTI), which was introduced to mitigate against Meltdown, a different speculative execution vulnerability <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b33"">34]</ref>. PTI introduced a new trampoline used during system calls to switch between the user and kernel page tables.",1
"ition occurs. IBRS has an extremely high overhead compared to retpolines (observed to be as high as 25-53%+) <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b46"">47,</ref><ref type=""bibr"" target=""#b49"">50]</ref>.</p><p>IBRS is the only mechanism which completely protects Skylake a",0
"cessors <ref type=""bibr"" target=""#b36"">[37]</ref>. This includes technologies such as Enhanced Indirect Branch Restricted Speculation (Enhanced IBRS) <ref type=""bibr"" target=""#b15"">[16]</ref> and control flow enforcement technology (CET) <ref type=""bibr"" target=""#b13"">[14]</ref>. Enhanced IBRS elimi",0
"pe=""bibr"" target=""#b20"">[21]</ref>.</p><p>A similar solution was introduced by Abeni to reduce the overhead of indirect branches in the network stack <ref type=""bibr"" target=""#b1"">[2]</ref>. This solution is also static, requiring the developer to determine at development time what the likely target",0
"adopted the use of retpolines on all vulnerable architectures instead, using RSB stuffing to protect the RSB <ref type=""bibr"" target=""#b50"">[51,</ref><ref type=""bibr"" target=""#b52"">53]</ref>.</p><p>Hardware mitigations. Finally, Intel has proposed mitigations for Spectre in new microarchitectures wh",0
"more detail in upcoming sections.</p><p>We have released our implementation of ApproxNDCG in Tensorflow in the open-source Tensorflow Ranking library <ref type=""bibr"" target=""#b11"">[12]</ref>. <ref type=""foot"" target=""#foot_0"">1</ref></p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.3"">",1
"methods like LambdaMART <ref type=""bibr"" target=""#b16"">[17]</ref>.</p><p>A more direct approach to LTR metric optimization was proposed by Qin et al. <ref type=""bibr"" target=""#b13"">[14]</ref>, where the rank variable in the definition of metrics like NDCG was approximated by a sum of sigmoids, there ns of the metrics offered no immediate advantage. Recent hardware and software advances in the training of neural networks, however, make the work in <ref type=""bibr"" target=""#b13"">[14]</ref> relevant again and potentially allow us to harvest the effectiveness and the scalability of deep neural netw at are comparable with those obtained using existing state-of-the-art LTR algorithms such as LambdaMART. We give an overview of LTR and in particular <ref type=""bibr"" target=""#b13"">[14]</ref> in Section 2. We discuss experimental results in Section 3 and conclude the paper in Section 4.</p></div> <d ]</ref>, smoothing scores <ref type=""bibr"" target=""#b14"">[15]</ref>, boosting <ref type=""bibr"" target=""#b18"">[19]</ref>, and approximating the metric <ref type=""bibr"" target=""#b13"">[14]</ref>. It is the latter that can tightly bound any ranking metric such as NDCG <ref type=""bibr"" target=""#b13"">[14] ref>, and approximating the metric <ref type=""bibr"" target=""#b13"">[14]</ref>. It is the latter that can tightly bound any ranking metric such as NDCG <ref type=""bibr"" target=""#b13"">[14]</ref> and can be easily optimized with gradient descent.</p><p>Surprisingly, despite its attractive theoretical pr 13"">[14]</ref> and can be easily optimized with gradient descent.</p><p>Surprisingly, despite its attractive theoretical properties, the framework in <ref type=""bibr"" target=""#b13"">[14]</ref> has received little attention in LTR studies in the decade since the original publication. In this paper, we perparameters of that work and reproduce experiments to optimize NDCG-referred to as ApproxNDCG. Our results show that  the theoretical guarantees in <ref type=""bibr"" target=""#b13"">[14]</ref> materialize in practice. Before we go any further, we give a brief overview of ApproxNDCG in the next sectio scoring function from Equation 2, and I s &lt;t is the indicator which is 1 if s &lt; t and 0 otherwise. <ref type=""bibr"">Qin et al.</ref> propose in <ref type=""bibr"" target=""#b13"">[14]</ref> a smooth approximation of Equation <ref type=""formula"" target=""#formula_4"">5</ref>where I is estimated by a r, remains a challenge due to the discontinuous nature of ranking utility functions.</p><p>In this work, we set out to revisit the work of Qin et al. <ref type=""bibr"" target=""#b13"">[14]</ref> which formulates a smooth approximation to any ranking metric such as NDCG. Unlike many other existing surro ]</ref> which formulates a smooth approximation to any ranking metric such as NDCG. Unlike many other existing surrogate LTR losses, the framework in <ref type=""bibr"" target=""#b13"">[14]</ref> offers a way to directly optimize ranking metrics. Because the objective is differentiable, it is also a goo e but more appropriate to directly optimize ranking metrics rather than loosely related surrogate losses; and (b) that the approximation framework in <ref type=""bibr"" target=""#b13"">[14]</ref> could lay out the foundation of deep neural networks in LTR. We wish to encourage research in this direction",1
"s=""http://www.tei-c.org/ns/1.0""><head n=""3.1"">Datasets</head><p>We conduct exhaustive experiments on two publicly available LTR datasets: MSLR-WEB30K <ref type=""bibr"" target=""#b12"">[13]</ref> and Yahoo! LTR Challenge <ref type=""bibr"" target=""#b4"">[5]</ref>. Both datasets contain roughly 30,000 queri",0
"ementation (denoted by λMART GBM ). We also used the legacy RankLib implementation (λMART RankLib ). We implemented ListMLE and RankNet in Tensorflow <ref type=""bibr"" target=""#b0"">[1]</ref>, a deep learning framework. In all of our experiments, we run 10 trials of each experiment and report mean met",0
"e when tree-based LTR models were making great strides, as demonstrated for instance by LambdaMART's winning of the Yahoo! Learning to Rank Challenge <ref type=""bibr"" target=""#b4"">[5]</ref>, and since regression trees cannot be optimized globally, such differentiable approximations of the metrics of conduct exhaustive experiments on two publicly available LTR datasets: MSLR-WEB30K <ref type=""bibr"" target=""#b12"">[13]</ref> and Yahoo! LTR Challenge <ref type=""bibr"" target=""#b4"">[5]</ref>. Both datasets contain roughly 30,000 queries. Web30K has an average of 120 documents per query, each represen",0
"umbling block, the LTR community has produced a plethora of schemes to improve metrics like NDCG, including metric smoothing methods such as SoftRank <ref type=""bibr"" target=""#b14"">[15]</ref> and indirect boosting methods like LambdaMART <ref type=""bibr"" target=""#b16"">[17]</ref>.</p><p>A more direct exceptions that attempt to directly maximize a ranking metric by using coordinate ascent <ref type=""bibr"" target=""#b10"">[11]</ref>, smoothing scores <ref type=""bibr"" target=""#b14"">[15]</ref>, boosting <ref type=""bibr"" target=""#b18"">[19]</ref>, and approximating the metric <ref type=""bibr"" target=""#",0
"b17"">[18]</ref>, RankNet and LambdaMART <ref type=""bibr"" target=""#b2"">[3]</ref>. To train LambdaMART models, we used the recent open-source Light-GBM <ref type=""bibr"" target=""#b9"">[10]</ref> implementation (denoted by λMART GBM ). We also used the legacy RankLib implementation (λMART RankLib ). We i",0
"target=""#b8"">[9]</ref>, boosted weak learners <ref type=""bibr"" target=""#b18"">[19]</ref>, gradientboosted trees <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b5"">6]</ref>, support vector machines <ref type=""bibr"" target=""#b8"">[9]</ref>, and neural networks <ref type=""bibr"" target=""",0
"ef type=""bibr"">[2-4, 9, 18]</ref>. There exist a few notable exceptions that attempt to directly maximize a ranking metric by using coordinate ascent <ref type=""bibr"" target=""#b10"">[11]</ref>, smoothing scores <ref type=""bibr"" target=""#b14"">[15]</ref>, boosting <ref type=""bibr"" target=""#b18"">[19]</r",0
"r"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b5"">6]</ref>, support vector machines <ref type=""bibr"" target=""#b8"">[9]</ref>, and neural networks <ref type=""bibr"" target=""#b1"">[2]</ref>. In this paper, we model f using the latter.</p><p>The loss function, ℓ, is ideally derived from a utility of",0
"y each neighbor of u. z i ( t) ∈ R d is the most recent embedding for node i. The use of max operator is inspired from learning on general point sets <ref type=""bibr"" target=""#b34"">(Qi et al., 2017)</ref>. By applying max-pooling operator element-wise, the model effectively captures different aspect",1
"ude Poisson Process, Hawkes processes <ref type=""bibr"" target=""#b10"">(Farajtabar et al., 2014;</ref><ref type=""bibr"" target=""#b18"">Hawkes, 1971;</ref><ref type=""bibr"" target=""#b46"">Wang et al., 2016b;</ref><ref type=""bibr"" target=""#b39"">Tabibian et al., 2017)</ref>, Self-Correcting Process <ref type",0
"network), that relates to activities between (not necessarily connected) nodes which leads to temporary information flow between them (Farine, 2017; <ref type=""bibr"" target=""#b1"">Artime et al., 2017)</ref>. We, then, posit our goal of learning node representations as modeling a latent mediation pro",0
"mporal Point Processes have previously been used to model both -dynamics on the network <ref type=""bibr"" target=""#b12"">(Farajtabar et al., 2016;</ref><ref type=""bibr"" target=""#b52"">Zarezade et al., 2017;</ref><ref type=""bibr"" target=""#b13"">Farajtabar et al., 2017)</ref> and dynamics of the network <",0
"which is represented as a graph over the residues (amino acids). Specifically, we augment the autoregressive self-attention of recent sequence models <ref type=""bibr"" target=""#b6"">[7]</ref> with graph-based descriptions of the 3D structure. By composing multiple layers of structured self-attention, our architecture, am encoder develops sequence-independent, contextual embeddings of each residue in the 3D structure with multi-head self-attention <ref type=""bibr"" target=""#b6"">[7]</ref> on the spatial k-nearest neigbors graph. An autoregressive decoder then predicts amino acid s i given the full =""2"">Methods</head><p>In this work, we introduce a Structured Transformer model that draws inspiration from the selfattention based Transformer model <ref type=""bibr"" target=""#b6"">[7]</ref> and is augmented for scalable incorporation of relational information (Figure <ref type=""figure"" target=""#fig_ -head self-attention component, where head ∈ [L] can attend to a separate subspace of the embeddings via learned query, key and value transformations <ref type=""bibr"" target=""#b6"">[7]</ref>.</p><p>The queries are derived from the current embedding at node i while the keys and values from the relatio embeddings with this residual, and alternate between these self-attention layers and position-wise feedforward layers as in the original Transformer <ref type=""bibr"" target=""#b6"">[7]</ref>. We stack multiple layers atop each other, and thereby obtain continually refined embeddings as we traverse th a hidden dimension of 128.</p><p>Optimization We trained models using the learning rate schedule and initialization of the original Transformer paper <ref type=""bibr"" target=""#b6"">[7]</ref>, a dropout rate of 10% <ref type=""bibr"" target=""#b41"">[42]</ref>, a label smoothing rate of 10%, and early sto n' protein sequences given a graph specification of their structure. Our model augments the traditional sequence-level self-attention of Transformers <ref type=""bibr"" target=""#b6"">[7]</ref> with relational 3D structural encodings and is able to leverage the spatial locality of dependencies in molecu n our architecture, am encoder develops sequence-independent, contextual embeddings of each residue in the 3D structure with multi-head self-attention<ref type=""bibr"" target=""#b6"">[7]</ref> on the spatial k-nearest neigbors graph. An autoregressive decoder then predicts amino acid s i given the full",1
"erating protein sequence designs requires a sampling scheme for drawing high-likelihood sequences from the model. While beam-search or top-k sampling <ref type=""bibr"" target=""#b45"">[46]</ref> are commonly used heuristics for decoding, we found that simple biased sampling from the temperature adjuste",0
"ntroduced a generative model for protein sequences conditioned on a 1D, context-free grammar based specification of the fold topology. Multiple works <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b16"">17]</ref> have modeled the conditional distribution of single amino acids give",0
"rties. This field has seen tremendous progess in the past two decades <ref type=""bibr"" target=""#b0"">[1]</ref>, including the design of novel 3D folds <ref type=""bibr"" target=""#b1"">[2]</ref>, enzymes <ref type=""bibr"" target=""#b2"">[3]</ref>, and complexes <ref type=""bibr"" target=""#b3"">[4]</ref>. Howev",0
"l) protein language models <ref type=""bibr"" target=""#b21"">[22]</ref><ref type=""bibr"" target=""#b22"">[23]</ref><ref type=""bibr"" target=""#b23"">[24]</ref><ref type=""bibr"" target=""#b24"">[25]</ref> to learn sequence representations that transfer well to supervised tasks. While serving different purposes,",0
"finding paragraph-level comparative relation, where two algorithm mention candidates lie across sentences. We base on recent Transformer architecture <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b35"">36]</ref> to build this module, due to its better performance in encoding long-d al connection follows the self-attention layer:</p><formula xml:id=""formula_4"">H Ak = A k + Conv( A k ).<label>(4)</label></formula><p>We follow BERT <ref type=""bibr"" target=""#b6"">[7]</ref> which recently achieves great success in multiple NLP tasks, to add a special &lt;CLS&gt; token at the start o",1
"w types other than algorithm exists. In addition, the abbreviation, as a short form of text, is prone to ambiguity. Word sense disambiguation methods <ref type=""bibr"" target=""#b22"">[23]</ref> have been studied to disambiguate word senses, however, deciding the sense for the abbreviation in the scien entific publications, but the process is known to be expensive and the collected datasets are normally small in size.</p><p>Word sense disambiguation <ref type=""bibr"" target=""#b22"">[23]</ref> is a type of technique used to distinguish ambiguous word senses. They either disambiguate word senses with ut also a television channel (Cable News Network).</p><p>Inspired by word sense disambiguation methods that label super sense types for word clusters <ref type=""bibr"" target=""#b22"">[23]</ref>, we jointly predict the types for abbreviation candidates with relation extraction task to distinguish abbre",1
"ee, where the tree roots of different sentences are linked together. <ref type=""bibr"" target=""#b36"">[37]</ref> proposes a method using self-attention <ref type=""bibr"" target=""#b35"">[36]</ref> and bi-affine scoring algorithm to predict biological relations between all mention pairs in the abstract si emory Networks (LSTMs) <ref type=""bibr"" target=""#b14"">[15]</ref> and Convolutional neural networks (CNNs).</p><p>Self-Attention. We adapt Transformer <ref type=""bibr"" target=""#b35"">[36]</ref> to encode word sequences in a paragraph, where we calculate the self-attention of the words, and use a convo ation, where two algorithm mention candidates lie across sentences. We base on recent Transformer architecture <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b35"">36]</ref> to build this module, due to its better performance in encoding long-distance context compared to Long Short",1
"hes for the supervised relation extraction focus on single sentence relation extraction with an exception of <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b36"">37]</ref>, which focus on general documents while not targeting on a specific narration of algorithm abbreviations and use a Graph-LSTM to encode the shortest path in the extracted dependency parse tree, where the tree roots of different sentences are linked together. <ref type=""bibr"" target=""#b36"">[37]</ref> proposes a method using self-attention <ref type=""bibr"" target=""#b35"">[36]</ref> and bi-affine scoring algor word sequences in a paragraph, where we calculate the self-attention of the words, and use a convolutional layer in self-attention blocks similar to <ref type=""bibr"" target=""#b36"">[37]</ref> to alleviate the burden on the model to attend to local features. We add residual connections <ref type=""bib",1
"bibr"" target=""#b28"">29]</ref>. Recent neural network based methods have achieved great success in relation extraction, including CNN-based approaches <ref type=""bibr"" target=""#b39"">[40,</ref><ref type=""bibr"" target=""#b40"">41]</ref> and LSTMbased approaches <ref type=""bibr"" target=""#b30"">[31]</ref>. s-sentence information, and consider all abbreviations in a paragraph. To this end, our model consists of a single-sentence module with Piecewise CNN <ref type=""bibr"" target=""#b39"">[40]</ref>, and a cross-sentence module which leverages self-attention to attend to all words capturing the paragraphle e=""bibr"" target=""#b9"">[10]</ref>, word-similarity based methods <ref type=""bibr"" target=""#b19"">[20]</ref>, and supervised relation extraction methods <ref type=""bibr"" target=""#b39"">[40]</ref>. The pattern-based method <ref type=""bibr"" target=""#b13"">[14]</ref> is not compared due to its low recall in ics following the criteria of our weak supervision. In the following we introduce evaluated methods in detail.</p><p>PCNN_single: Piecewise CNN model <ref type=""bibr"" target=""#b39"">[40]</ref>, which is one of the state of art single-sentence relation extraction methods. PCNN_single only uses single-",1
"out rate 0.3. The number of Transfomer block layer is set to 1, since we did not observe performance gain in increasing layers. We use Adam optimizer <ref type=""bibr"" target=""#b15"">[16]</ref> with a learning rate 0.001. In training, the batch size is set to be 32, and for each positive example, we s",0
"entence module, and the same number of filters as input dimension for the convolutional layer in each Transformer block. We apply layer normalization <ref type=""bibr"" target=""#b1"">[2]</ref> to each component of transformer block, and adopt dropout <ref type=""bibr"" target=""#b32"">[33]</ref> to the inp",0
"a or well-covered knowledge bases.</p><p>Beside single-sentence relation approaches, some previous works exist on cross-sentence relation extraction. <ref type=""bibr"" target=""#b25"">[26]</ref> proposes to construct cross-sentence relation data for entities with minimal-span assumption. <ref type=""bib as negative examples in training. To reduce the huge number of unrelated and non-informative negative examples, we follow the minimumspan strategy in <ref type=""bibr"" target=""#b25"">[26]</ref>, and limit sampled negative candidate pairs to the co-occurred pairs shown in a limited length of continuous",0
"er than the threshold are cut off.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>A.2 Training</head><p>The model is implemented in pytorch <ref type=""bibr"" target=""#b23"">[24]</ref> and trained on a single GeForce GTX 1080 GPU. The dimensions of word embedding, character embedding, and pos",0
"r"" target=""#b29"">[30]</ref>. The low coverage of knowledge base can also influence the availability of relation labels when using distant supervision <ref type=""bibr"" target=""#b21"">[22]</ref>.</p><p>Entity: General entity recognition does not directly separate the algorithms with the others. Though relation extraction works assume entities and relation sets are given in the datasets, while others apply distant supervision to link entity mentions <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b27"">28]</ref> in the text to the knowledge base entities <ref type=""bibr"" target=""",0
"t assumptions, different weighting strategies have been proposed such as maxpooling <ref type=""bibr"" target=""#b34"">[35]</ref> and selective attention <ref type=""bibr"" target=""#b16"">[17]</ref>. We follow at-least-one assumption, where a positive example has at least one instance implies the comparati",0
"extraction, including CNN-based approaches <ref type=""bibr"" target=""#b39"">[40,</ref><ref type=""bibr"" target=""#b40"">41]</ref> and LSTMbased approaches <ref type=""bibr"" target=""#b30"">[31]</ref>. These approaches all consider relations lying in a single sentence. On the other hand, most relation extrac",0
"ks similar to <ref type=""bibr"" target=""#b36"">[37]</ref> to alleviate the burden on the model to attend to local features. We add residual connections <ref type=""bibr"" target=""#b11"">[12]</ref> to both multihead attention and convolutional layers. The Transformer contains stacked layers of Transformer",0
"f type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b18"">19]</ref> or weakly supervised phrase extraction approaches relying on frequency <ref type=""bibr"" target=""#b29"">[30]</ref>. The low coverage of knowledge base can also influence the availability of relation labels when using distan reviations. However, this could be extended to general forms of entities and relations by integrating our model with general phrase mining algorithms <ref type=""bibr"" target=""#b29"">[30]</ref>, entity linking <ref type=""bibr"" target=""#b18"">[19]</ref>, and general cross-sentence relations with corresp",0
"dia info-boxes <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b33"">34]</ref> or harvest knowledge with specific linguistic patterns <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b38"">39]</ref>. Taxonomy can be viewed as a tree-structure knowledge graph, where lin",0
"ork related to ours is relation extraction, which has attracted much attention from the community, while most of the works focus on news and web data <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b28"">29]</ref>. Recent neural network based methods have achieved great success in re",0
"ome up with keywords, retrieve and read relevant papers and iterate this process.</p><p>One step assisting with this process is taxonomy construction <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b31"">32,</ref><ref type=""bibr"" target=""#b41 3"">[14,</ref><ref type=""bibr"" target=""#b31"">32]</ref> which extract hierarchical relation leveraging linguistic features, or clustering-based methods <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b41"">42]</ref>, which cluster concepts to induce an implicit hierarchy.</p><p>In th",0
"such as GAN (Generative Adversarial Networks) <ref type=""bibr"" target=""#b8"">[9]</ref> and DCGAN (Deep Convolutional Generative Adversarial Networks) <ref type=""bibr"" target=""#b26"">[27]</ref>. A directed edge GAN → DCGAN represents ""DCGAN"" is a successor and is evolved from ""GAN"". Comparative Relati",0
"r scientific taxonomy construction, <ref type=""bibr"" target=""#b12"">[13]</ref> studied the evolution of scientific topics through dynamic topic models <ref type=""bibr"" target=""#b20"">[21]</ref> modeling implicit topics and obscure relations, and some technical reports 1 2 manually analyzed the develop",0
"a graph connecting mined algorithms. In Figure <ref type=""figure"">1</ref>, for example, a roadmap for algorithm Generative Adversarial Network (GAN) <ref type=""bibr"" target=""#b8"">[9]</ref>, describes its successors and competitors in the scientific literature. The generated algorithm roadmap captur een two algorithm nodes e 1 and e 2 . For example, in the computer science domain, there are algorithms such as GAN (Generative Adversarial Networks) <ref type=""bibr"" target=""#b8"">[9]</ref> and DCGAN (Deep Convolutional Generative Adversarial Networks) <ref type=""bibr"" target=""#b26"">[27]</ref>. A di ublication time, as the algorithm is non-necessarily published in this conference. ""GAN"" (Generative Adversarial Networks) is a deep generative model <ref type=""bibr"" target=""#b8"">[9]</ref>, which has been extensively cited since proposed. Researchers even maintain a ""GAN zoo""<ref type=""foot"" target",0
"algorithm entities and relation labels in scientific publications are prohibitively expensive. Existing datasets or curated in-domain knowledge bases <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b3"">4]</ref> are rather small and frequently outdated with the development of science he general domain, harvesting common knowledge with pattern or statistics.</p><p>Many works focus on for mining scientific publications, for example, <ref type=""bibr"" target=""#b0"">[1]</ref> proposed a keyphrase and relation extraction competition for scientific publications, <ref type=""bibr"" target=",0
"ach Transformer block. We apply layer normalization <ref type=""bibr"" target=""#b1"">[2]</ref> to each component of transformer block, and adopt dropout <ref type=""bibr"" target=""#b32"">[33]</ref> to the input layer, piece-wise max-pooling and Transformer block with a dropout rate 0.3. The number of Tran",0
""">36]</ref> to build this module, due to its better performance in encoding long-distance context compared to Long Short Term Memory Networks (LSTMs) <ref type=""bibr"" target=""#b14"">[15]</ref> and Convolutional neural networks (CNNs).</p><p>Self-Attention. We adapt Transformer <ref type=""bibr"" target",0
"whether given candidate pairs are comparative or not. Evaluated methods can be divided to unsupervised methods including co-occurrence based methods <ref type=""bibr"" target=""#b9"">[10]</ref>, word-similarity based methods <ref type=""bibr"" target=""#b19"">[20]</ref>, and supervised relation extraction as PCNN_single where cross-sentence instances are also used.</p><p>Sent_cooccur: A method similar to co-occurrence method used in hypernym detection <ref type=""bibr"" target=""#b9"">[10]</ref>. Sent_cooccur calculates the co-occurrence frequency of candidate pairs in one sentence. A threshold that dec",0
"ompared algorithms. Most existing researches for the supervised relation extraction focus on single sentence relation extraction with an exception of <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b36"">37]</ref>, which focus on general documents while not targeting on a specific n extraction. <ref type=""bibr"" target=""#b25"">[26]</ref> proposes to construct cross-sentence relation data for entities with minimal-span assumption. <ref type=""bibr"" target=""#b24"">[25]</ref> proposes to use a Graph-LSTM to encode the shortest path in the extracted dependency parse tree, where the t",0
"al systems, and it is really critical to capture user intent.</p><p>As a general information modeling method, Heterogeneous Information Network (HIN) <ref type=""bibr"" target=""#b17"">[18]</ref>, consisting of multiple types of objects and links, has been widely applied to many data mining tasks <ref t objects and links, has been widely applied to many data mining tasks <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b17"">18]</ref>. In this paper, we propose to model the intent recommendation system with a HIN, through which we can flexibl",1
"mendation used in industry, such as Taobao and Amazon, usually extract handcrafted features, and then feed these features to a classifier, e.g., GBDT <ref type=""bibr"" target=""#b6"">[7]</ref> and XG-Boost <ref type=""bibr"" target=""#b3"">[4]</ref>. These methods heavily rely on domain knowledge and need near model with static features.</p><p>• DNN: With the same input setting as LR, we implement the deep neural network with 3 layers MLP.</p><p>• GBDT <ref type=""bibr"" target=""#b6"">[7]</ref>: It is a scalable tree-based model for feature learning and classification task. We feed static features into",0
"-guided Heterogeneous Graph Neural Network</head><p>Inspired by the basic idea of the GCNs which generates object embeddings based on local neighbors <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b20"">21]</ref>, we first propose a UIQ path UQI path metapath-guided heterogeneous",0
"p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.3"">Detailed Implementation</head><p>We implement the proposed method based on Tensorflow <ref type=""bibr"" target=""#b0"">[1]</ref>. For our method, we set the dimension of term embedding as 64. We use a single-layer LSTM with 64 hidden neuro",0
"(HIN) <ref type=""bibr"" target=""#b17"">[18]</ref>, consisting of multiple types of objects and links, has been widely applied to many data mining tasks <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b17"">18]</ref>. In this paper, we propose to",0
"ic tasks such as node classification.</p><p>Despite their remarkable performance, recent studies show that GCNs are vulnerable to adversarial attacks <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b36"">37]</ref>, i.e. carefully designed small perturbations in graph structures and n ef type=""bibr"" target=""#b36"">[37,</ref><ref type=""bibr"" target=""#b37"">38]</ref> try to attack the model by changing training data and evasion attacks <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b36"">37]</ref> try to generate fake samples for a trained model, i.e. the attacks are by whether they are before (poisoning attacks) or after (evasion attacks) the training phase of GCNs. • Targeted or Non-targeted. In targeted attacks <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b36"">37]</ref>, the attacker focus on misclassifying some target nodes while in non-t all model performance. • Direct or Influence. Targeted attacks can be further divided into two categories based on attack settings. In direct attacks <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b36"">37]</ref>, the attacker can directly manipulate the edges or features of the tar abnormal compared to the existing information. For example, the attacker tends to connect nodes from different communities to confuse the classifier <ref type=""bibr"" target=""#b6"">[7]</ref>. While plain vectors cannot adapt to such changes, Gaussian distributions can automatically absorb the effects tack: We randomly generate fake edges and add them into the graph. We regard this method as an illustrating example of non-targeted attacks. • RL-S2V <ref type=""bibr"" target=""#b6"">[7]</ref> <ref type=""foot"" target=""#foot_2"">3</ref> : This method generates adversarial attacks on graph-structured data",1
"model for graphs that recently attracts considerable research attention <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b17"">18]</ref>. State-of-the-art GCNs usually follow a ""message-passing"" framework <ref type=""bibr"" target=""#b9"">[10,</ref>< Chebyshev polynomials, the exact eigen-decomposition of the Laplacian matrix is avoided, thus reducing the overall time complexity. Kipf and Welling <ref type=""bibr"" target=""#b17"">[18]</ref> further propose to simplify the graph convolution using only the 1 st order polynomial, i.e. the immediate n nal Networks</head><p>Though a number of different GCN methods have been proposed, here we focus on a representative one proposed by Kipf and Welling <ref type=""bibr"" target=""#b17"">[18]</ref>. Here, the (l + 1) t h convolutional layer is defined as:</p><formula xml:id=""formula_2"">h (l +1) i = ρ j ∈n ze H (1) as deeper layers are naturally Gaussian distributions by using our Gaussian-based Graph Convolutions.</p><p>Following the original GCN model <ref type=""bibr"" target=""#b17"">[18]</ref>, we also impose L 2 regularization on parameters of the first layer as follows:</p><formula xml:id=""formula_ aselines and Adversarial Attack Methods.</head><p>To evaluate the robustness of RGCN, we compare it with two state-of-the-art GCN models:</p><p>• GCN <ref type=""bibr"" target=""#b17"">[18]</ref>: As introduced in Section 3.2 , this is the original GCN model which defines graph convolution as aggregatin n and early-stopping. The performance of different methods is evaluated on a separate test set of 1000 labels. We adopt the same dataset splits as in <ref type=""bibr"" target=""#b17"">[18]</ref> and report the average results of 10 runs. In experiments, we set the number of layers as two for all method ad><p>In order to comprehensively evaluate the effectiveness of our proposed method, we adopt three citation networks commonly used in previous works <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b29"">30]</ref>: Cora, Citeseer and Pubmed <ref type=""bibr"" target=""#b23"">[24]</ref> of the datasets are summarized in Table <ref type=""table"" target=""#tab_0"">1</ref>.</p><p>We closely follow the experimental setting in previous works <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b29"">30]</ref>. Specifically, we use all node features and 20 labels per class as t 8]</ref> and report the average results of 10 runs. In experiments, we set the number of layers as two for all methods as suggested by previous works <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b29"">30]</ref>. For GCN and RGCN, we set the number of hidden units as 32.</p><p>No",1
"ns and variances respectively since variances are required to be non-negative while means can take negative values. For the optimization, we use Adam <ref type=""bibr"" target=""#b15"">[16]</ref> with a fixed learning rate of 0.01 and set epoch number as 200 with early stopping on the validation set. Fo",0
"h data, have drawn increasing research interests in the past few years. Next, we briefly review some representative GCNs, and readers are referred to <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b34"">35]</ref> for some comprehensive surveys. Bruna et al. <ref type=""bibr"" target=""",0
"<ref type=""bibr"" target=""#b32"">33]</ref> considering edge attributes <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b25"">26]</ref>, disentangling node representations <ref type=""bibr"" target=""#b19"">[20]</ref> and automatically selecting hyp",0
"networks commonly used in previous works <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b29"">30]</ref>: Cora, Citeseer and Pubmed <ref type=""bibr"" target=""#b23"">[24]</ref>, where nodes represent documents and edges represent citations. Nodes are also associated with bag-of-words",0
"<ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b32"">33]</ref> considering edge attributes <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b25"">26]</ref>, disentangling node representations <ref type=""bibr"" target=""#b19"">[2",0
"lution is very high. To solve the efficiency problem, Cheb-Net <ref type=""bibr"" target=""#b7"">[8]</ref> proposes to use a K-order Chebyshev polynomial <ref type=""bibr"" target=""#b12"">[13]</ref> to approximate the convolutional filters in the spectral domain. By using the recurrence relation of Chebysh",0
"hanism to assign different weights in aggregating node neighborhoods <ref type=""bibr"" target=""#b27"">[28,</ref><ref type=""bibr"" target=""#b29"">30,</ref><ref type=""bibr"" target=""#b30"">31,</ref><ref type=""bibr"" target=""#b33"">34]</ref>, adding residual and jumping connections <ref type=""bibr"" target=""#b3",0
"al graph autoencoder using GCN as the encoder <ref type=""bibr"" target=""#b16"">[17]</ref>. MPNNs <ref type=""bibr"" target=""#b9"">[10]</ref> and GraphSAGE <ref type=""bibr"" target=""#b11"">[12]</ref> unify these approaches using the ""message-passing"" framework, i.e. defining the graph convolution as nodes a",0
"=""bibr"" target=""#b23"">24]</ref>. In recent years, with the success of deep neural networks (DNNs), some researches have applied DNNs to precipitation <ref type=""bibr"" target=""#b20"">[21]</ref> and radar echo <ref type=""bibr"" target=""#b31"">[32]</ref> nowcast. These methods can nowcast weather distribu r"" and applied it to short-term forecasting of location and intensity of rain and snow. To solve the problem of spatiotemporal dependency, Shi et al. <ref type=""bibr"" target=""#b20"">[21]</ref> developed the conventional LSTM and propose convolutional LSTM (ConvLSTM). Their experiments demonstrate tha connected long short-term memory (FC-LSTM) network, with a goal to overcome the drawbacks of FC-LSTM in handling spatial-temporal data such as videos <ref type=""bibr"" target=""#b20"">[21]</ref>. Specifically, in the ConvLSTM network, the fully-connected gates of the FC-LSTM module are replaced by conv",1
"Prediction Model in Deep</head><p>Learning. There are also some deep learning models focusing on spatiotemporal prediction tasks such as 3D ConvNets <ref type=""bibr"" target=""#b28"">[29]</ref>, PredCNN <ref type=""bibr"" target=""#b26"">[27]</ref>, DCRNN <ref type=""bibr"" target=""#b25"">[26]</ref> and Step type=""bibr"" target=""#b25"">[26]</ref> and StepDeep <ref type=""bibr"" target=""#b19"">[20]</ref>. To better model spatiotemporal information, 3D ConvNets <ref type=""bibr"" target=""#b28"">[29]</ref> extend conventional 2-dimensional convolution, which slides only in the spatial dimension, to 3-dimensional",0
"target=""#b6"">[7]</ref>. Preliminary studies focus on understanding the electrification mechanism of lightning <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b17"">18]</ref>. These researches provide basic theory and routes for the prediction",0
"of this natural hazard <ref type=""bibr"" target=""#b6"">[7]</ref>. Preliminary studies focus on understanding the electrification mechanism of lightning <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b17"">18]</ref>. These researches provide basi",0
"g/ns/1.0""><head n=""2"">RELATED WORKS</head><p>Lightning Prediction. Generally speaking, modern lightning prediction methods fall into three categories <ref type=""bibr"" target=""#b30"">[31]</ref>: numerical diagnosis prediction based on synoptic background filed statistical relations; the numerical weat",0
"6,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" target=""#b33"">34]</ref>, and hybrid methods <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b27"">28]</ref>. However, these approaches rely on manual feature engineering, are un y heavily on manually designed metapaths/meta-graphs, which are hard to tune in practice. (3) Hybrid methods <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b27"">28]</ref> combine the above two categories and learn user/item embeddings by ex ur datasets are 64, 128, 64, 64. The training weight for KG part is 0.1 for all datasets. The learning rate are the same as in SVD.</p><p>• RippleNet <ref type=""bibr"" target=""#b23"">[24]</ref> is a representative of hybrid methods, which is a memory-network-like approach that propagates users' prefer",1
"7,</ref><ref type=""bibr"" target=""#b33"">34]</ref>, and hybrid methods <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b27"">28]</ref>. However, these approaches rely on manual feature engineering, are unable to perform end-to-end training, and /meta-graphs, which are hard to tune in practice. (3) Hybrid methods <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b27"">28]</ref> combine the above two categories and learn user/item embeddings by exploiting the structure of KGs. Our propo y, several works developed GNNs architecture for recommender systems <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b30"">31,</ref><ref type=""bibr"" target=""#b31"">32]</ref>, but these approaches are mos elationships between the items as well as personalized user preferences and interests. To account for the relational heterogeneity in KGs, similar to <ref type=""bibr"" target=""#b27"">[28]</ref>, we use a trainable and personalized relation scoring function that transforms the KG into a user-specific w us bipartite graphs or user/item-similarity graphs where GCNs can be used directly, while here we investigate GCNs for heterogeneous KGs. Wang et al. <ref type=""bibr"" target=""#b27"">[28]</ref> use GCNs in KGs for recommendation, but simply applying GCNs to KGs without proper regularization is prone t ur approach is to transform a heterogeneous KG into a user-personalized weighted graph that characterizes user's preferences. To this end, similar to <ref type=""bibr"" target=""#b27"">[28]</ref>, we use a user-specific relation scoring function s u (r ) that provides the importance of relation r for us ef type=""figure"" target=""#fig_6"">5</ref>. It is clear that the performance of KGNN-LS with a non-zero λ is better than λ = 0 (the case of Wang et al. <ref type=""bibr"" target=""#b27"">[28]</ref>), which justifies our claim that LS regularization can assist learning the edge weights in a KG and achieve",1
"l</head><p>MovieLens-20M Book-Crossing Last.FM Dianping-Food R@2 R@10 R@50 R@100 R@2 R@10 R@50 R@100 R@2 R@10 R@50 R@100 R@2 R@10 R@50 SVD 0. • LibFM <ref type=""bibr"" target=""#b15"">[16]</ref> is a widely used feature-based factorization model for CTR prediction. We concatenate user ID and item ID as",0
"ly flexible in utilizing KGs to assist recommender systems, but the KGE algorithms focus more on modeling rigorous semantic relatedness (e.g., TransE <ref type=""bibr"" target=""#b1"">[2]</ref> assumes head +relation = tail), which are more suitable for graph applications such as link prediction rather , 8} and the number of training epochs is 50 for all datasets. • LibFM + TransE extends LibFM by attaching an entity representation learned by TransE <ref type=""bibr"" target=""#b1"">[2]</ref> to each user-item pair. The dimension of TransE is 32 for all datasets.</p><p>• PER <ref type=""bibr"" target=""# act that: (1) r 1 and r 2 are the inverse of each other and semantically related; (2) Treating A u symmetric will greatly increase the matrix density.<ref type=""bibr"" target=""#b1"">2</ref> There are several candidate designs for the architecture of our model, e.g., GCN<ref type=""bibr"" target=""#b10"">[",0
"/ref><ref type=""bibr"" target=""#b35"">36]</ref>, embedding-based methods <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" target=""#b33"">34]</ref>, and hybrid methods <ref type=""bibr"" target=""#b17"">[18,</ref><ref typ s can be classified into three categories: (1) Embedding-based methods <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" target=""#b33"">34]</ref> pre-process a KG with knowledge graph embedding (KGE) <ref type=""bibr",0
"t been heavily explored by the users. The sparsity issue can be addressed by introducing additional sources of information such as user/item profiles <ref type=""bibr"" target=""#b22"">[23]</ref> or social networks <ref type=""bibr"" target=""#b21"">[22]</ref>.</p><p>Knowledge graphs (KGs) capture structure",0
"pe=""bibr"" target=""#b31"">32]</ref>. Traditional recommender systems that are based on collaborative filtering <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b21"">22]</ref> usually suffer from the coldstart problem and have trouble recommending brand new items that have not yet bee be addressed by introducing additional sources of information such as user/item profiles <ref type=""bibr"" target=""#b22"">[23]</ref> or social networks <ref type=""bibr"" target=""#b21"">[22]</ref>.</p><p>Knowledge graphs (KGs) capture structured information and relations between a set of entities <ref ty",0
"ype=""bibr"" target=""#b28"">29,</ref><ref type=""bibr"" target=""#b31"">32]</ref>. Traditional recommender systems that are based on collaborative filtering <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b21"">22]</ref> usually suffer from the coldstart problem and have trouble recommend",0
"elatedness captured by the KG. Existing KG-aware recommender systems can be classified into path-based methods <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" target=""#b35"">36]</ref>, embedding-based methods <ref type=""bibr"" target=""#b8"">[9,</ref><ref tions. In addition, embedding-based methods usually lack an end-to-end way of training. (2) Path-based methods <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" target=""#b35"">36]</ref> explore various patterns of connections among items in a KG (a.k.a me ntation learned by TransE <ref type=""bibr"" target=""#b1"">[2]</ref> to each user-item pair. The dimension of TransE is 32 for all datasets.</p><p>• PER <ref type=""bibr"" target=""#b32"">[33]</ref> is a representative of path-based methods, which treats the KG as heterogeneous information networks and ext",0
"first two baselines are KG-free while the rest are all KG-aware methods. The hyperparameter setting of KGNN-LS is provided in Appendix B.</p><p>• SVD <ref type=""bibr"" target=""#b11"">[12]</ref> is a classic CF-based model using inner product to model user-item interactions. We use the unbiased version",0
"ture for recommender systems <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b30"">31,</ref><ref type=""bibr"" target=""#b31"">32]</ref>, but these approaches are mostly designed for homogeneous bipartite u ""#b18"">[19]</ref> model recommender systems as matrix completion and design GCNs for representation learning on user-item bipartite graphs. Wu et al. <ref type=""bibr"" target=""#b30"">[31]</ref> use GCNs on user/item structure graphs to learn user/item representations. The difference between these work",0
"he value of l * u at each non-item entity e i ∈ E\V is the average of its neighboring entities, which leads to the following label propagation scheme <ref type=""bibr"" target=""#b38"">[39]</ref>: Theorem 2. Repeating the following two steps: <ref type=""bibr"" target=""#b0"">(1)</ref> Propagate labels for",0
"ow to capture user-specific item-item relatedness captured by the KG. Existing KG-aware recommender systems can be classified into path-based methods <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" target=""#b35"">36]</ref>, embedding-based methods <ref link prediction rather than recommendations. In addition, embedding-based methods usually lack an end-to-end way of training. (2) Path-based methods <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" target=""#b35"">36]</ref> explore various patterns of co",0
"d et al. <ref type=""bibr"" target=""#b4"">[5]</ref> approximate the convolutional filters by Chebyshev expansion of the graph Laplacian, and Kipf et al. <ref type=""bibr"" target=""#b10"">[11]</ref> propose a convolutional architecture via a first-order approximation. In contrast to these spectral GCNs, no tly increase the matrix density.<ref type=""bibr"" target=""#b1"">2</ref> There are several candidate designs for the architecture of our model, e.g., GCN<ref type=""bibr"" target=""#b10"">[11]</ref> or GraphSAGE<ref type=""bibr"" target=""#b6"">[7]</ref>. Here we use GCN<ref type=""bibr"" target=""#b10"">[11]</ref the architecture of our model, e.g., GCN<ref type=""bibr"" target=""#b10"">[11]</ref> or GraphSAGE<ref type=""bibr"" target=""#b6"">[7]</ref>. Here we use GCN<ref type=""bibr"" target=""#b10"">[11]</ref> as our base model.</note> 			<note xmlns=""http://www.tei-c.org/ns/1.0"" place=""foot"" n=""3"" xml:id=""foot_2"">ht",0
"ethods such as ELMo <ref type=""bibr"" target=""#b35"">(Peters et al., 2018)</ref>, GPT <ref type=""bibr"" target=""#b39"">(Radford et al., 2018)</ref>, BERT <ref type=""bibr"" target=""#b11"">(Devlin et al., 2019)</ref>, XLM <ref type=""bibr"" target=""#b25"">(Lample &amp; Conneau, 2019)</ref>, and XLNet <ref type be done, and modeling advances are often conflated with changes in data size or composition.</p><p>We present a replication study of BERT pretraining <ref type=""bibr"" target=""#b11"">(Devlin et al., 2019)</ref>, which includes a careful evaluation of the effects of hyperparameter tuning and training s t design choices, is competitive with all other recently published methods. We release our model, pretraining and fine-tuning code.</p><p>Setup: BERT <ref type=""bibr"" target=""#b11"">(Devlin et al., 2019)</ref> takes as input a concatenation of two segments (sequences of tokens), x 1 , . . . , x N and tand this discrepancy, we compare several alternative training formats:</p><p>• SEGMENT-PAIR+NSP: This follows the original input format used in BERT <ref type=""bibr"" target=""#b11"">(Devlin et al., 2019)</ref>, with the NSP loss. Each input has a pair of segments, which can each contain multiple natu portion of this vocabulary when modeling large and diverse corpora, such as the ones considered in this work.</p><p>The original BERT implementation <ref type=""bibr"" target=""#b11"">(Devlin et al., 2019)</ref>  Early experiments revealed only minor differences between these encodings, with the byte-l xmlns=""http://www.tei-c.org/ns/1.0""><head n=""5.2"">SQUAD RESULTS</head><p>We adopt a much simpler approach for SQuAD compared to past work. While BERT <ref type=""bibr"" target=""#b11"">(Devlin et al., 2019)</ref> and XLNet <ref type=""bibr"" target=""#b56"">(Yang et al., 2019)</ref> augment their training d 0.4 points (EM) and 0.6 points (F1).</p><p>We also submit RoBERTa to the public SQuAD 2.0 leaderboard. Most of the top systems build upon either BERT <ref type=""bibr"" target=""#b11"">(Devlin et al., 2019)</ref> or XLNet <ref type=""bibr"" target=""#b56"">(Yang et al., 2019)</ref> and therefore rely on add f><ref type=""bibr"" target=""#b56"">Yang et al., 2019)</ref>. This formulation significantly simplifies the task, but is not directly comparable to BERT <ref type=""bibr"" target=""#b11"">(Devlin et al., 2019)</ref>. Following recent work, we adopt the ranking approach for our test submission, but for dire We also set β 2 = 0.98 to improve stability when training with large batch sizes.</p><p>We pretrain with sequences of at most T = 512 tokens. Unlike <ref type=""bibr"" target=""#b11"">Devlin et al. (2019)</ref>, we do not randomly inject short sequences, and we do not train with a reduced sequence leng ural Language Inference <ref type=""bibr"" target=""#b4"">(Bowman et al., 2015)</ref>, which require predicting relationships between pairs of sentences. <ref type=""bibr"" target=""#b11"">Devlin et al. (2019)</ref> observe that removing NSP hurts performance, with significant performance degradation on QNL <ref type=""table"" target=""#tab_0"">1</ref> shows results for the four different settings. We first compare the original SEGMENT-PAIR input format from <ref type=""bibr"" target=""#b11"">Devlin et al. (2019)</ref> to the SENTENCE-PAIR format; both formats retain the NSP loss, but the latter uses single se ms the originally published BERT BASE results and that removing the NSP loss matches or slightly improves downstream task performance, in contrast to <ref type=""bibr"" target=""#b11"">Devlin et al. (2019)</ref>. It is possible that the original BERT implementation may only have removed the loss term wh et=""#tab_1"">2</ref> shows the masked LM perplexity and end-task accuracy for BERT BASE as we increase the batch size, while tuning the learning rate. <ref type=""bibr"" target=""#b11"">Devlin et al. (2019)</ref> originally trained BERT BASE for 1M steps with a batch size of 256 sequences; however a batc n BERT, with a batch size eight times larger for half as many optimization steps, thus seeing four times as many sequences in pretraining compared to <ref type=""bibr"" target=""#b11"">Devlin et al. (2019)</ref>.</p><p>To help disentangle the importance of these factors from other modeling choices (e.g. chitecture (L = 24, H = 1024, A = 16, 355M parameters). We pretrain for 100K steps over a comparable BOOKCORPUS plus WIKIPEDIA dataset as was used in <ref type=""bibr"" target=""#b11"">Devlin et al. (2019)</ref>. We pretrain our model using 1024 V100 GPUs, which takes approximately one day per 100K step rning rate scheduled used by <ref type=""bibr"" target=""#b56"">Yang et al. (2019)</ref>.</p><p>For SQuAD v1.1 we follow the same finetuning procedure as <ref type=""bibr"" target=""#b11"">Devlin et al. (2019)</ref>. For SQuAD v2.0, we additionally classify whether a given question is answerable; we train t w.tei-c.org/ns/1.0"" type=""table"" xml:id=""tab_7""><head>Table 7 :</head><label>7</label><figDesc>Comparison between the published BERT BASE results from<ref type=""bibr"" target=""#b11"">Devlin et al. (2019)</ref> to our reimplementation with either static or dynamic masking. We report F1 for SQuAD and ac =""#b19"">Howard &amp; Ruder, 2018)</ref>, machine translation <ref type=""bibr"" target=""#b30"">(McCann et al., 2017)</ref>, and masked language modeling <ref type=""bibr"" target=""#b11"">(Devlin et al., 2019;</ref><ref type=""bibr"" target=""#b25"">Lample &amp; Conneau, 2019)</ref>. Many recent papers have us l., 2019;</ref><ref type=""bibr"" target=""#b56"">Yang et al., 2019)</ref>. Performance is also typically improved by training bigger models on more data <ref type=""bibr"" target=""#b11"">(Devlin et al., 2019;</ref><ref type=""bibr"" target=""#b1"">Baevski et al., 2019;</ref><ref type=""bibr"" target=""#b56"">Yang",1
"""11"" xml:id=""foot_10"">  11  The GLUE datasets are: CoLA<ref type=""bibr"" target=""#b53"">(Warstadt et al., 2018)</ref>, Stanford Sentiment Treebank (SST)<ref type=""bibr"" target=""#b46"">(Socher et al., 2013)</ref>, Microsoft Research Paragraph Corpus (MRPC)<ref type=""bibr"" target=""#b12"">(Dolan &amp; Broc",0
"underlying data collection or preprocessing.</note> 			<note xmlns=""http://www.tei-c.org/ns/1.0"" place=""foot"" n=""4"" xml:id=""foot_3"">We use news-please<ref type=""bibr"" target=""#b16"">(Hamborg et al., 2017)</ref> to collect and extract CC-NEWS. CC-NEWS is similar to the REALNEWS dataset described in<re",0
".</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.2"">DATA</head><p>BERT-style pretraining crucially relies on large quantities of text. <ref type=""bibr"" target=""#b1"">Baevski et al. (2019)</ref> demonstrate that increasing data size can result in improved end-task performance. Several e 2019)</ref>. Performance is also typically improved by training bigger models on more data <ref type=""bibr"" target=""#b11"">(Devlin et al., 2019;</ref><ref type=""bibr"" target=""#b1"">Baevski et al., 2019;</ref><ref type=""bibr"" target=""#b56"">Yang et al., 2019;</ref><ref type=""bibr"" target=""#b40"">Radford",0
"ning set size effects.</p><p>When controlling for training data, our improved training procedure improves upon the published BERT results on the GLUE <ref type=""bibr"" target=""#b52"">(Wang et al., 2019b)</ref> and SQuAD <ref type=""bibr"" target=""#b41"">(Rajpurkar et al., 2016)</ref> benchmarks. When tra rk, we evaluate our pretrained models by finetuning on downstream tasks:</p><p>• GLUE: The General Language Understanding Evaluation (GLUE) benchmark <ref type=""bibr"" target=""#b52"">(Wang et al., 2019b)</ref> is a collection of 9 datasets for evaluating natural language understanding systems. Tasks a",0
"econd, the exposure-aware indicator function 1 Eu is nondifferentiable. Therefore, for S with model parameters Φ, we use the policy gradient based RL <ref type=""bibr"" target=""#b13"">[Sutton et al., 2000]</ref> to derive its gradient:</p><formula xml:id=""formula_9"">∇ Φ L S = ∇ Φ (u,i)∈C E j∼ ΨS (j|u)",1
"ure. Due to lack of data, most works use probabilistic approach to model user exposure as a latent variable and infer its value from interaction data <ref type=""bibr"" target=""#b11"">[Liang et al., 2016]</ref> or social relationship <ref type=""bibr"" target=""#b3"">[Chen et al., 2019]</ref>. Previous wor",0
"""1"">Introduction</head><p>The prevalence of implicit feedback has boosted the research and development of implicit feedback-based recommender systems <ref type=""bibr"" target=""#b0"">[Bayer et al., 2017;</ref><ref type=""bibr"" target=""#b14"">Yang et al., 2018]</ref>. The key challenge in learning from im",0
"ich focus on choosing more reliable negative instances by leveraging the auxiliary data such as clicked but non-purchased items in Ecommerce websites <ref type=""bibr"" target=""#b4"">[Ding et al., 2018a]</ref>.</p><p>In real-world scenarios, platforms can easily collect whether the recommended (i.e., e gatives only from exposure data. We use it to investigate the impact of selecting negatives from an incomplete candidate set.</p><p>-EBPR. Similar to <ref type=""bibr"" target=""#b4"">[Ding et al., 2018a]</ref>, we consider to weight those exposed but unclicked items differently (compared with other une cause selection bias, making the model under-trained and resulting in suboptimal performance <ref type=""bibr"" target=""#b10"">[Lian et al., 2017;</ref><ref type=""bibr"" target=""#b4"">Ding et al., 2018a</ref>]. • Difficulty of optimizing the negative sampler. Due to the discrete sampling process on item",0
"et al., 2018]</ref>. The key challenge in learning from implicit feedback lies in the natural scarcity of negative signal, known as one-class problem <ref type=""bibr"" target=""#b12"">[Pan et al., 2008]</ref>. To address this issue, negative sampling has been widely adopted in previous works <ref type= data (i.e., the unobserved interactions) <ref type=""bibr"" target=""#b8"">[Jiang et al., 2018;</ref><ref type=""bibr"" target=""#b7"">He et al., 2018;</ref><ref type=""bibr"" target=""#b12"">Lin et al., 2019]</ref>. This process, with no doubt, plays a critical role in training recommender models from implici r systems with negative sampling. In the future, we plan to learn more general negative samplers for social-aware or context-rich recommender systems <ref type=""bibr"" target=""#b12"">[Lin et al., 2019;</ref><ref type=""bibr"" target=""#b16"">Zhang et al., 2019b]</ref>, and other related fields such as net",0
"tion is that when the window size of the convolution operation is set to 2, then all the potential words can easily fuse into corresponding positions <ref type=""bibr"" target=""#b4"">[Kim, 2014]</ref>. As shown in Figure <ref type=""figure"" target=""#fig_0"">2</ref>, words with certain lengths correspond ore efficient and effective. Improving NER computational efficiency. In general, end-to-end CNNs in NLP have mainly been used for text classification <ref type=""bibr"" target=""#b4"">[Kim, 2014]</ref>. For sequence labeling tasks, CNNs have been mainly used for low-level feature extraction <ref type=""b hidden state − → h w i and a right-to-left hidden state ← − h w i , which are concatenated for the NER prediction.</p><p>CNN. We apply a standard CNN <ref type=""bibr"" target=""#b4"">[Kim, 2014]</ref> structure on the character or word sequence to obtain its multiple gram representation for the NER pre mula></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.3"">Hyper-Parameter Settings</head><p>For all four of the datasets, we used the Adamax <ref type=""bibr"" target=""#b4"">[Kingma and Ba, 2014]</ref> optimization to train our networks. The initial learning rate was set at 0.0015, with a deca",1
"are redundant (i.e., unigram X 1 m , bigram X 2 m , . . . , and L-gram X L m ). In this work, we propose the use of the multi-scale feature attention <ref type=""bibr"" target=""#b6"">[Wang et al., 2018]</ref> to adaptively selects the features of different scales in each position of a sentence, as foll s. In particular, these models can achieve results that are competitive to the state-of-the-art <ref type=""bibr"" target=""#b1"">[Che et al., 2013;</ref><ref type=""bibr"" target=""#b6"">Wang et al., 2013]</ref>. However, due to the influence of the word segmentation errors, the models on the automatic seg",0
"plays an important role in many downstream NLP tasks, including information retrieval <ref type=""bibr"">[Chen et al., 2015]</ref>, relation extraction <ref type=""bibr"" target=""#b0"">[Bunescu and Mooney, 2005]</ref>, question answering systems <ref type=""bibr"" target=""#b2"">[Diefenbach et al., 2018]</re",0
"n and Zheng, 2011]</ref>. Hence, one intuitive way to perform Chinese NER is to first perform word segmentation and then apply word sequence labeling <ref type=""bibr"" target=""#b7"">[Yang et al., 2016;</ref><ref type=""bibr"">He and Sun, 2017]</ref>.</p><p>However, gold-standard segmentation is rarely a ways to use lexicon features to better leverage word information for NER has attracted research attention <ref type=""bibr"">[Passos et al., 2014;</ref><ref type=""bibr"" target=""#b7"">Zhang and Yang, 2018]</ref>. In particular, to exploit explicit word information, Zhang and Yang <ref type=""bibr"">[2018] the recurrent structure, which limits their computational efficiency <ref type=""bibr"">[Strubell et al., 2017]</ref>.</p><p>Specifically, lattice LSTM <ref type=""bibr"" target=""#b7"">[Zhang and Yang, 2018]</ref> employs double recurrent transition computation across the length of the input, one for eac n that due to the limited performance of current Chinese word segmentation, character-based name taggers can outperform their word-based counterparts <ref type=""bibr"" target=""#b7"">[Zhang and Yang, 2018]</ref>. <ref type=""bibr"" target=""#b7"">Zhang and Yang [2018]</ref> exploit an RNNbased lattice stru s that applied a dilated window skips over every dilation. This method achieves great performance in the English NER task. Lattice LSTM. Lattice LSTM <ref type=""bibr"" target=""#b7"">[Zhang and Yang, 2018]</ref> can model the characters in sequence and explicitly leverages word information through Gate ei-c.org/ns/1.0"" type=""table"" xml:id=""tab_2""><head></head><label></label><figDesc>, Weibo NER[Peng and Dredze, 2015; He and Sun, 2016], and Resume NER<ref type=""bibr"" target=""#b7"">[Zhang and Yang, 2018]</ref>. The splitting methods follow those in<ref type=""bibr"" target=""#b7"">[Zhang and Yang, 2018]< g and Dredze, 2015; He and Sun, 2016], and Resume NER<ref type=""bibr"" target=""#b7"">[Zhang and Yang, 2018]</ref>. The splitting methods follow those in<ref type=""bibr"" target=""#b7"">[Zhang and Yang, 2018]</ref>.The OntoNotes and MSRA are the newswire domain, where gold-standard segmentation is availab d segmentation, character-based name taggers can outperform their word-based counterparts <ref type=""bibr"" target=""#b7"">[Zhang and Yang, 2018]</ref>. <ref type=""bibr"" target=""#b7"">Zhang and Yang [2018]</ref> exploit an RNNbased lattice structure to simultaneously model characters and corresponding w",0
"type=""bibr"">[Chen et al., 2015]</ref>, relation extraction <ref type=""bibr"" target=""#b0"">[Bunescu and Mooney, 2005]</ref>, question answering systems <ref type=""bibr"" target=""#b2"">[Diefenbach et al., 2018]</ref>, and other applications. Compared with English NER, Chinese named entities are more diff the word w l m (Figure <ref type=""figure"" target=""#fig_0"">2</ref>). To incorporate the lexicon feature effectively, we use the vectorbased attention <ref type=""bibr"" target=""#b2"">[Chen et al., 2018]</ref> to combine the l gram feature with the word feature, as follows:</p><formula xml:id=""formula_4",0
"ion at the pixel level. Recently, there are many fabulous semantic segmentation models such as <ref type=""bibr"">FCN [Long et al., 2015]</ref>, SegNet <ref type=""bibr"" target=""#b0"">[Badrinarayanan et al., 2017]</ref>, <ref type=""bibr"">DeepLab-v3 [Chen et al., 2018a]</ref>, PSPNet <ref type=""bibr"" tar ead><p>Network Architecture</p><p>The proposed network is based on the classic encoderdecoder network architecture without the fully-connected layers <ref type=""bibr"" target=""#b0"">[Badrinarayanan et al., 2017]</ref> and we improve it by adding the refined cross connections as shown in Figure <ref ty e of our network with comparison to other classic semantic segmentation networks -FCN <ref type=""bibr"" target=""#b7"">[Long et al., 2015]</ref>, SegNet <ref type=""bibr"" target=""#b0"">[Badrinarayanan et al., 2017]</ref>, and DeepLab-V3 <ref type=""bibr"" target=""#b2"">[Chen et al., 2018b]</ref>. Second, to ompare the semantic segmentation performance of our network with that of <ref type=""bibr"">FCN-32s, FCN-16s, FCN-8s [Long et al., 2015]</ref>, Seg-Net <ref type=""bibr"" target=""#b0"">[Badrinarayanan et al., 2017], and</ref><ref type=""bibr"">DeepLab-v3 [Chen et al., 2018b]</ref>. The evaluation results a",1
"tial dimensions by pooling layers, while the decoder restores the target details and spatial dimensions step by step. Among such architectures, U-Net <ref type=""bibr"" target=""#b8"">[Ronneberger et al., 2015]</ref> is a very efficient one, whose semantic segmentation model employs the architecture of",0
"egradation because of climatic variations and overgrazing, thus resulting in multifarious ecological problems, such as desertification and sandstorms <ref type=""bibr"" target=""#b10"">[Zhan et al., 2017]</ref>. Therefore, how to estimate the stage of grassland degradation accurately is of top priority ith enough samples to train the network. In addition, the aerial or satellite images used in the studies of remote sensing and environmental sciences <ref type=""bibr"" target=""#b10"">[Wang et al., 2018]</ref> are not high-resolution enough to capture such a tiny target as SC.Moreover, due to the parti ging to each of the five semantic categories -grassland (background), SC, sky, water, and road -for every image with an open annotation tool -LabelMe <ref type=""bibr"" target=""#b10"">[Russell et al., 2008]</ref>. Due to the limitation on the plateau, there are inevitably quantitative differences in di",0
"n. First, existing public datasets <ref type=""bibr"" target=""#b7"">[Mottaghi et al., 2014;</ref><ref type=""bibr"" target=""#b2"">Cordts et al., 2016;</ref><ref type=""bibr"" target=""#b1"">Caesar et al., 2018;</ref><ref type=""bibr"" target=""#b9"">Ros et al., 2016]</ref> contain substantially insufficient grass ification is one of the most promising solutions towards furthering botanical taxonomy, as illustrated by the wealth of research regarding this topic <ref type=""bibr"" target=""#b1"">[Cerutti et al., 2011;</ref><ref type=""bibr"" target=""#b5"">Kebapci et al., 2011;</ref><ref type=""bibr"">Goëau et al., 2016",0
"e way of achieving an automatic estimation of degradation. First, existing public datasets <ref type=""bibr"" target=""#b7"">[Mottaghi et al., 2014;</ref><ref type=""bibr"" target=""#b2"">Cordts et al., 2016;</ref><ref type=""bibr"" target=""#b1"">Caesar et al., 2018;</ref><ref type=""bibr"" target=""#b9"">Ros et a FCN <ref type=""bibr"" target=""#b7"">[Long et al., 2015]</ref>, SegNet <ref type=""bibr"" target=""#b0"">[Badrinarayanan et al., 2017]</ref>, and DeepLab-V3 <ref type=""bibr"" target=""#b2"">[Chen et al., 2018b]</ref>. Second, to test the performance of our scheme on grassland degradation stage estimation, we",0
"The anomaly detection in dynamic graph, especially anomalous edges detection, is then highly needed before the data are fed into the following tasks <ref type=""bibr"" target=""#b1"">[Akoglu et al., 2015;</ref><ref type=""bibr"" target=""#b4"">Ranshous et al., 2015]</ref>.</p><p>It is not trivial to detect n of nodes, we apply the contextual attention-based model which is inspired by <ref type=""bibr"" target=""#b3"">[Liu et al., 2017]</ref> and proposed by <ref type=""bibr"" target=""#b1"">[Cui et al., 2017]</ref>. In our framework, we construct short state of local window as follow:</p><formula xml:id=""form "">H t = GRU(Current t , Short t )<label>(11)</label></formula><p>GRU is a variant of LSTM network. It is simpler and more effective than LSTM network <ref type=""bibr"" target=""#b1"">[Chung et al., 2014]</ref>. GRU can record long-term information, and avoid gradient vanishing and exploding problems. T l, we cannot use a strict loss function such as cross entropy to distinguish the existing edges and the generated ones. We then take the same idea in <ref type=""bibr"" target=""#b1"">[Bordes et al., 2013]</ref> and use marginbased pairwise loss in training of AddGraph:</p><formula xml:id=""formula_16"">L each node as its content feature. We need to manually build the required datasets because the ground-truth for the test phase is difficult to obtain <ref type=""bibr"" target=""#b1"">[Akoglu et al., 2015]</ref>, and we follow the approach used in <ref type=""bibr"" target=""#b7"">[Yu et al., 2018]</ref> to",1
"combine short-term and long-term states. To catch the short-term pattern of nodes, we apply the contextual attention-based model which is inspired by <ref type=""bibr"" target=""#b3"">[Liu et al., 2017]</ref> and proposed by <ref type=""bibr"" target=""#b1"">[Cui et al., 2017]</ref>. In our framework, we co",1
""">Graph Embedding</head><p>Graph embedding maps the nodes into a K-dimensional vector space, which preserves certain properties among nodes. Deepwalk <ref type=""bibr"" target=""#b6"">[Tang et al., 2015]</ref>, LINE <ref type=""bibr"" target=""#b6"">[Tang et al., 2015]</ref> and Node2vec <ref type=""bibr"" ta a K-dimensional vector space, which preserves certain properties among nodes. Deepwalk <ref type=""bibr"" target=""#b6"">[Tang et al., 2015]</ref>, LINE <ref type=""bibr"" target=""#b6"">[Tang et al., 2015]</ref> and Node2vec <ref type=""bibr"" target=""#b2"">[Grover and Leskovec, 2016]</ref> are the methods t",1
"o detect anomaly on embeddings.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2.1"">Anomaly Detection in Dynamic Graph</head><p>Goutlier <ref type=""bibr"" target=""#b0"">[Aggarwal et al., 2011]</ref> is proposed with an observation that anomalous edges always appear between two different n 2018]</ref> to inject anomalous edges into two datasets.</p><p>Baselines. We compare AddGraph with three anomaly detection methods.</p><p>• GOutlier <ref type=""bibr"" target=""#b0"">[Aggarwal et al., 2011]</ref>. It builds a generative model for edges in a node cluster, and the model can also be used",0
"/ref> mainly relies on the structural features. They define a density function and discover the target mainly us-ing structural features. Other works <ref type=""bibr"" target=""#b7"">[Zhao and Yu, 2013;</ref><ref type=""bibr"" target=""#b4"">McConville et al., 2015]</ref> consider content feature or even t ce is used to detect anomalous changes in dynamic graph, while the method mainly focuses on structural features and cannot catch long-term anomalies. <ref type=""bibr"" target=""#b7"">[Yu et al., 2018]</ref> proposed NetWalk, a dynamic graph embedding model based on random walks. The anomaly detection i ground-truth for the test phase is difficult to obtain <ref type=""bibr"" target=""#b1"">[Akoglu et al., 2015]</ref>, and we follow the approach used in <ref type=""bibr"" target=""#b7"">[Yu et al., 2018]</ref> to inject anomalous edges into two datasets.</p><p>Baselines. We compare AddGraph with three ano l., 2016]</ref>. It uses the local structural feature and historical behavior near an edge to measure whether the edge is anomalous or not. • NetWalk <ref type=""bibr"" target=""#b7"">[Yu et al., 2018]</ref>. The method first builds node embeddings based on random walks, and then detects anomaly using t o the test data of different datasets.</p><p>The results are shown in Table <ref type=""table"">1</ref>, in which the data of baselines are reported by <ref type=""bibr"" target=""#b7"">[Yu et al., 2018]</ref>. We can see that Ad-dGraph beats all baselines on the two datasets with varying anomaly proporti e margin γ Figure <ref type=""figure"" target=""#fig_3"">2</ref> illustrates the results on dynamic graph, in which the data of baselines are reported by <ref type=""bibr"" target=""#b7"">[Yu et al., 2018]</ref>. The results indicate that AddGraph beats baselines on almost all snapshots except the last one",0
"lous edges detection, is then highly needed before the data are fed into the following tasks <ref type=""bibr"" target=""#b1"">[Akoglu et al., 2015;</ref><ref type=""bibr"" target=""#b4"">Ranshous et al., 2015]</ref>.</p><p>It is not trivial to detect the anomaly due to its flexible and dynamic nature. Some fine a density function and discover the target mainly us-ing structural features. Other works <ref type=""bibr"" target=""#b7"">[Zhao and Yu, 2013;</ref><ref type=""bibr"" target=""#b4"">McConville et al., 2015]</ref> consider content feature or even temporal factor. However, the way taking the content, st ibility as patterns are given in advance.</p><p>Besides the structural features, the temporal ones are considered in the anomaly detection. CM-Sketch <ref type=""bibr"" target=""#b4"">[Ranshous et al., 2016]</ref> is a sketch-based method, which uses the local structural information and historical behav f>. It builds a generative model for edges in a node cluster, and the model can also be used to produce anomalous score for a given edge. • CM-Sketch <ref type=""bibr"" target=""#b4"">[Ranshous et al., 2016]</ref>. It uses the local structural feature and historical behavior near an edge to measure whet",0
"ng-term anomalous behavior and increases the difficulty of detection. The similar anomaly pattern appears in the network attack against IP-IP network <ref type=""bibr"" target=""#b2"">[Eswaran et al., 2018]</ref>, where there are sudden large number of connections, forming a very dense subgraph in the n licit rules from the given data. GCN(Graph Convolutional Network) is a representative model to combine the content and structural features in a graph <ref type=""bibr"" target=""#b2"">[Kipf and Welling, 2017]</ref>. Compared with traditional graph methods, GCN can automatically propagate the information ng nodes. Deepwalk <ref type=""bibr"" target=""#b6"">[Tang et al., 2015]</ref>, LINE <ref type=""bibr"" target=""#b6"">[Tang et al., 2015]</ref> and Node2vec <ref type=""bibr"" target=""#b2"">[Grover and Leskovec, 2016]</ref> are the methods to yield node embeddings so that two structural similar nodes have the ss structural features and content features. GCN extends the idea of convolution model over regular graphs (i.e., image) to general graphs. The works <ref type=""bibr"" target=""#b2"">[Defferrard et al., 2016;</ref><ref type=""bibr"" target=""#b2"">Kipf and Welling, 2017]</ref>improve the performance of bas idea of convolution model over regular graphs (i.e., image) to general graphs. The works <ref type=""bibr"" target=""#b2"">[Defferrard et al., 2016;</ref><ref type=""bibr"" target=""#b2"">Kipf and Welling, 2017]</ref>improve the performance of basic GCN from different viewpoints, like the optimization in ti",0
"ores produced by the model can be used as an important measure in detecting anomalous edges.</p><p>The works <ref type=""bibr"">[Sun et al., 2006;</ref><ref type=""bibr"" target=""#b5"">Shin et al., 2016;</ref><ref type=""bibr"" target=""#b5"">Shin et al., 2017]</ref> view the anomaly as a dense sub-graph. <r measure in detecting anomalous edges.</p><p>The works <ref type=""bibr"">[Sun et al., 2006;</ref><ref type=""bibr"" target=""#b5"">Shin et al., 2016;</ref><ref type=""bibr"" target=""#b5"">Shin et al., 2017]</ref> view the anomaly as a dense sub-graph. <ref type=""bibr"" target=""#b5"">Shin [2016;</ref><ref type f><ref type=""bibr"" target=""#b5"">Shin et al., 2016;</ref><ref type=""bibr"" target=""#b5"">Shin et al., 2017]</ref> view the anomaly as a dense sub-graph. <ref type=""bibr"" target=""#b5"">Shin [2016;</ref><ref type=""bibr"">2017]</ref> define a density function in the dynamic bipartite graph, and employ a gre",0
"eatedly fusing the representations from multi-resolution streams. This paper represents a very substantial extension of our previous conference paper <ref type=""bibr"" target=""#b105"">[105]</ref> with an additional material added from our unpublished technical report <ref type=""bibr"" target=""#b106"">[1 on results under recentlydeveloped start-of-the-art object detection and instance segmentation frameworks. The main technical novelties compared with <ref type=""bibr"" target=""#b105"">[105]</ref> lie in threefold. <ref type=""bibr"" target=""#b0"">(1)</ref> We extend the network (named as HRNetV1) propose type=""bibr"" target=""#b105"">[105]</ref> lie in threefold. <ref type=""bibr"" target=""#b0"">(1)</ref> We extend the network (named as HRNetV1) proposed in <ref type=""bibr"" target=""#b105"">[105]</ref>, to two versions: HRNetV2 and HRNetV2p, which explore all the four-resolution representations. <ref type=""",1
"e=""bibr"" target=""#b2"">[3]</ref>, DeconvNet <ref type=""bibr"" target=""#b85"">[85]</ref>, U-Net <ref type=""bibr"" target=""#b95"">[95]</ref>, SimpleBaseline <ref type=""bibr"" target=""#b124"">[124]</ref>, and encoder-decoder <ref type=""bibr"" target=""#b90"">[90]</ref>. In addition, dilated convolutions are used 95"">[95]</ref> and Hourglass <ref type=""bibr"" target=""#b83"">[83]</ref>, encoder-decoder <ref type=""bibr"" target=""#b90"">[90]</ref>, and SimpleBaseline <ref type=""bibr"" target=""#b124"">[124]</ref>.</p><p>plexity. We observe similar performance for HRNetV1 and HRNetV2 over COCO pose estimation, and the rks include: light upsample process <ref type=""bibr"">[5]</ref>, <ref type=""bibr"" target=""#b19"">[19]</ref>, <ref type=""bibr"" target=""#b72"">[72]</ref>, <ref type=""bibr"" target=""#b124"">[124]</ref>, possibly with dilated convolutions used in the backbone <ref type=""bibr"" target=""#b47"">[47]</ref>, <ref t a augmentation is also involved.</p><p>We use the Adam optimizer <ref type=""bibr"" target=""#b54"">[54]</ref>. The learning schedule follows the setting <ref type=""bibr"" target=""#b124"">[124]</ref>. The base learning rate is set as 2. http://cocodataset.org/#keypoints-eval 1e−3, and is dropped to 1e−4 a ochs.</p><p>Testing. The two-stage top-down paradigm similar as <ref type=""bibr"" target=""#b19"">[19]</ref>, <ref type=""bibr"" target=""#b87"">[87]</ref>, <ref type=""bibr"" target=""#b124"">[124]</ref> is used: detect the person instance using a person detector, and then predict detection keypoints.</p><p>W Sim-pleBaseline 3 for both the val and test-dev sets. Following <ref type=""bibr"" target=""#b19"">[19]</ref>, <ref type=""bibr"" target=""#b83"">[83]</ref>, <ref type=""bibr"" target=""#b124"">[124]</ref>, we compute the heatmap by averaging the heatmaps of the original and flipped images. Each keypoint locati del size and slightly higher complexity, achieves 4.8 and 4.0 points gain, respectively. (iii) Compared to the previous best-performed SimpleBaseline <ref type=""bibr"" target=""#b124"">[124]</ref>, our HRNetV1-W32 obtains significant improvements: 3.0 points gain for the backbone ResNet-50 with a simil HRNetV1-W48, get the 75.8 and 76.3 AP, which have 1.4 and 1.2 improvements compared to the input size 256 × 192. In comparison to the SimpleBaseline <ref type=""bibr"" target=""#b124"">[124]</ref> that uses ResNet-152 as the backbone, our HRNetV1-W32 and HRNetV1-W48 attain 1.5 and 2.0 points gain in te of model size (#Params) and computation complexity (GFLOPs). Our big model, HRNetV1-W48, achieves the highest 75.5 AP. Compared to the SimpleBaseline <ref type=""bibr"" target=""#b124"">[124]</ref> with the same input size, our small and big networks receive 1.2 and 1.8 improvements, respectively. With",0
"""#b77"">[77]</ref>, <ref type=""bibr"" target=""#b88"">[88]</ref>, <ref type=""bibr"" target=""#b93"">[93]</ref>, <ref type=""bibr"" target=""#b102"">[102]</ref>, <ref type=""bibr"" target=""#b103"">[103]</ref>. In the Faster R-CNN framework, our networks perform better than ResNets with similar parameter and comput",0
"ASCAL-Context dataset</head><p>The data augmentation and learning rate policy are the same as Cityscapes. Following the widely-used training strategy <ref type=""bibr"" target=""#b25"">[25]</ref>, <ref type=""bibr"" target=""#b137"">[137]</ref>, we resize the images to 480×480 and set the initial learning r ng rate to 0.004 and weight decay to 0.0001. The batch size is 16 and the number of iterations is 60K.</p><p>We follow the standard testing procedure <ref type=""bibr"" target=""#b25"">[25]</ref>, <ref type=""bibr"" target=""#b137"">[137]</ref>. The image is resized to 480 × 480 and then fed into our networ",0
""">[34]</ref>; studying the details of the upsample process <ref type=""bibr"" target=""#b120"">[120]</ref>; combining multi-scale pyramid representations <ref type=""bibr"" target=""#b18"">[18]</ref>, <ref type=""bibr"" target=""#b125"">[125]</ref>; stacking multiple DeconvNets/U-Nets/Hourglass <ref type=""bibr""",0
"nto consideration. Therefore, the produced architectures (e.g., NASNet, AmoebaNet) are not efficient for inference. Recent hardware-aware NAS methods <ref type=""bibr"" target=""#b3"">(Cai et al., 2019;</ref><ref type=""bibr"" target=""#b32"">Tan et al., 2019;</ref><ref type=""bibr"" target=""#b35"">Wu et al., at originally has N layers, we keep the first D layers and skip the last N -D layers, rather than keeping any D layers as done in current NAS methods <ref type=""bibr"" target=""#b3"">(Cai et al., 2019;</ref><ref type=""bibr"" target=""#b35"">Wu et al., 2019)</ref>. As such, one depth setting only correspon t the accuracy of a model given its architecture and input image size<ref type=""foot"" target=""#foot_1"">2</ref> . We also build a latency lookup table <ref type=""bibr"" target=""#b3"">(Cai et al., 2019)</ref> on each target hardware platform to predict the latency. Given the target hardware and latency ctiveness of OFA on six additional hardware platforms (Figure <ref type=""figure"" target=""#fig_10"">11</ref>) using the ProxylessNAS architecture space <ref type=""bibr"" target=""#b3"">(Cai et al., 2019)</ref>. OFA consistently improves the trade-off between accuracy and latency by a significant margin, ency constraints by training only once (green curve). It is impossible for previous NAS methods<ref type=""bibr"" target=""#b32"">(Tan et al., 2019;</ref><ref type=""bibr"" target=""#b3"">Cai et al., 2019)</ref> due to the prohibitive training cost.</figDesc></figure> <figure xmlns=""http://www.tei-c.org/ns/",1
"rent hardware platforms and efficiency constraints (defined as deployment scenarios), researchers either design compact models specialized for mobile <ref type=""bibr"" target=""#b15"">(Howard et al., 2017;</ref><ref type=""bibr"" target=""#b30"">Sandler et al., 2018;</ref><ref type=""bibr"" target=""#b40"">Zha itectures are proposed to improve the hardware efficiency, such as SqueezeNet <ref type=""bibr"" target=""#b18"">(Iandola et al., 2016)</ref>, MobileNets <ref type=""bibr"" target=""#b15"">(Howard et al., 2017;</ref><ref type=""bibr"" target=""#b30"">Sandler et al., 2018)</ref>, ShuffleNets <ref type=""bibr"" tar",0
"ref type=""bibr"" target=""#b43"">Zoph et al., 2018;</ref><ref type=""bibr"" target=""#b29"">Real et al., 2019;</ref><ref type=""bibr"">Cai et al., 2018a;</ref><ref type=""bibr"" target=""#b24"">Liu et al., 2019)</ref>. Early NAS methods <ref type=""bibr"" target=""#b43"">(Zoph et al., 2018;</ref><ref type=""bibr"" tar",0
"pling period are 5 billion and 100 million core cycles respectively, which are determined to be a good set up experimentally. In fact, Jim?nez et al. <ref type=""bibr"" target=""#b2"">[3]</ref> shows a 50:1 of ""execution epoch vs sampling period"" is a reasonable choice. Other lengths (2 billion, 50 mill cessors. Kang et al. <ref type=""bibr"" target=""#b30"">[31]</ref> studies the effect of hardware prefetching in virtualized environments. Jimenez et al. <ref type=""bibr"" target=""#b2"">[3]</ref> proposed an adaptive prefetch control to independently adjust each core's prefetch aggressiveness on Fig. <ref",1
"ing and a dynamic algorithm. They find that measurements on real machines provide different observations than past simulation-based work. Wang et al. <ref type=""bibr"" target=""#b46"">[47]</ref> proposes a combined cache partition method (SWAP) to take into account both of set and way partitioning. Thi",0
"are controllable, way-based cache partitioning of LLC called Cache Allocation Technology (CAT) <ref type=""bibr"" target=""#b14"">[15]</ref>. Recent work <ref type=""bibr"" target=""#b15"">[16]</ref>- <ref type=""bibr"" target=""#b17"">[18]</ref> utilized CAT for performance isolation. However, it did not consi ystems from various perspectives. Like recent architecture work ( <ref type=""bibr"" target=""#b7"">[8]</ref>, <ref type=""bibr"" target=""#b10"">[11]</ref>, <ref type=""bibr"" target=""#b15"">[16]</ref>- <ref type=""bibr"" target=""#b17"">[18]</ref>), we use harmonic speedup (HS) and normalized weighted speedup ov allocate various amounts of cache ways to each application. It should be noted that different partitions cannot overlap with each other. Cook et al. <ref type=""bibr"" target=""#b15"">[16]</ref> evaluates the effect of cache partitioning on real Intel multi-core processors, including an optimal static",0
"nd prefetch throttling. It targets Intel Server processors, but can be applied to any processor with similar capabilities.</p><p>Extensive prior work <ref type=""bibr"" target=""#b1"">[2]</ref>- <ref type=""bibr"" target=""#b10"">[11]</ref> proposed reducing prefetchercaused inter-core interference by contr -c.org/ns/1.0""><head>VI. RELATED WORK</head></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>A. Local/Global Prefetch Control</head><p>Wu et al. <ref type=""bibr"" target=""#b1"">[2]</ref> proposes a dynamic control of prefetching to mitigate intra-application prefetch interference on Intel archite",0
"tp://www.tei-c.org/ns/1.0""><head>A. Processor used</head><p>All measurements were performed on an Intel Broadwell-EP Xeon server processor E5-2620 V4 <ref type=""bibr"" target=""#b25"">[26]</ref>. It contains 8 physical cores and 16 hardware threads and each physical core contains private 32KB L1 I/D-Ca",0
"<div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2."">MODEL</head><p>Our sequence-to-sequence model is an encoder-decoder architecture with attention <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b6"">7]</ref>. Let X = [X 1 , . . . , X T ] be t",1
"""><head n=""2."">MODEL</head><p>Our sequence-to-sequence model is an encoder-decoder architecture with attention <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b6"">7]</ref>. Let X = [X 1 , . . . , X T ] be the frames of an utterance with correspo y u−1 , Q u−1 ) (2) S u = attend(Q u , K, V ) (3) P (y u | X, y &lt;u ) = h(S u , Q u ).<label>(4)</label></formula><p>The function g(•) is a GRU RNN <ref type=""bibr"" target=""#b5"">[6]</ref> which encodes the previous token and query vector Q u−1 to produce the next query vector. The attention mechan",1
"lns=""http://www.tei-c.org/ns/1.0""><head n=""4.1."">Data</head><p>All experiments are performed on the publicly available Lib-riSpeech audio book corpus <ref type=""bibr"" target=""#b10"">[11]</ref>. We use the ""train-clean-100"" set as the paired data set, which consists of around 100 hours of clean speech t used with Lib-riSpeech is derived from 14,476 public domain books. The books were selected such that there is no overlap with the dev and test sets <ref type=""bibr"" target=""#b10"">[11]</ref>. On the other hand, the training data set transcriptions are almost entirely contained in the LM training te ing to lower case and removing punctuation except for the apostrophe in contractions (we replace hyphens with a space). Unlike the original LM corpus <ref type=""bibr"" target=""#b10"">[11]</ref> we take no steps to replace non-standard words with a canonical verbalized form. However, we find that LMs t",1
"s; the architecture and parameters remain the same as in the ""trainclean-100"" baseline. All experiments are implemented in the wav2letter++ framework <ref type=""bibr"" target=""#b16"">[17]</ref>.</p><p>Prior to generating pseudo-labels with any new combination of acoustic and language model, we optimiz",0
"ne such approach, self-training, uses noisy labels generated from a model trained on a much smaller labelled data set.</p><p>We revisit self-training <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b2"">3]</ref> in the context of sequenceto-sequence models with attention. We show rel ns/1.0""><head n=""5."">RELATED WORK</head><p>Self-training has been applied to tasks in natural language processing including word-sense disambiguation <ref type=""bibr"" target=""#b1"">[2]</ref>, noun identification <ref type=""bibr"" target=""#b22"">[23]</ref> and parsing <ref type=""bibr"" target=""#b2"">[3]</",0
"n computer vision such as object detection <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b24"">25]</ref> and image classification <ref type=""bibr"" target=""#b25"">[26]</ref>.</p><p>In automatic speech recognition, self-training-style approaches have seen some success in hybrid, ali",0
"in two ways: (1) the attention can loop causing long outputs and (2) the model can predict the EOS token too early leading to an overly short output <ref type=""bibr"" target=""#b9"">[10]</ref>.</p><p>We filter for the first failure case by removing examples which contain an n-gram repeated more than c",0
"ence-to-sequence model is an encoder-decoder architecture with attention <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b6"">7]</ref>. Let X = [X 1 , . . . , X T ] be the frames of an utterance with corresponding transcription Y = [y 1 , . . . ,",0
"itectural details are the same as <ref type=""bibr"" target=""#b7"">[8]</ref>. We predict 5,000 sub-word targets generated with the SentencePiece toolkit <ref type=""bibr"" target=""#b13"">[14]</ref> using only ""train-clean-100"" as training data.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>Dev<",0
"takes advantage of outputs from multiple systems, and the data selection process can take place at different levels ranging from frames to utterances <ref type=""bibr"" target=""#b30"">[31,</ref><ref type=""bibr"" target=""#b31"">32]</ref>.</p><p>Using pseudo-labels with a hybrid system can also give an imp",0
"orpus for LM training. First, we detect sentence boundaries using the ""punkt"" tokenizer <ref type=""bibr"" target=""#b11"">[12]</ref> implemented in NLTK <ref type=""bibr"" target=""#b12"">[13]</ref>. We normalize the text by converting everything to lower case and removing punctuation except for the apostr",0
"ref type=""bibr"" target=""#b26"">[27,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b28"">29]</ref> and agreementbased selection <ref type=""bibr"" target=""#b29"">[30]</ref> that also takes advantage of outputs from multiple systems, and the data selection process can take place at",0
"can compare.</p><p>Recently-proposed semi-supervised approaches for endto-end speech recognition have applied techniques similar to back-translation <ref type=""bibr"" target=""#b34"">[35]</ref>. These use unpaired text to generate a synthetic data set, but target hidden state representations instead o",0
"K T ] are the keys and V = [V 1 . . . , V T ]</formula><p>are the values. We use a fully convolutional encoder with time-depth separable (TDS) blocks <ref type=""bibr"" target=""#b7"">[8]</ref>. The decoder maps the query vectors generated from previous output tokens and the key-value pairs into summary ntion limit, which does not allow the beam search to propose new hypotheses which attend more than t max frames away from the previous attention peak <ref type=""bibr"" target=""#b7"">[8]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3."">SEMI-SUPERVISED SELF-TRAINING</head><p>In a s he encoder. The TDS groups contain 10, 14 and 16 channels respectively all with a kernel width of 21. All other architectural details are the same as <ref type=""bibr"" target=""#b7"">[8]</ref>. We predict 5,000 sub-word targets generated with the SentencePiece toolkit <ref type=""bibr"" target=""#b13"">[14 a.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>Dev</head><p>During optimization we pre-train for three epochs with a soft-window (σ = 4) <ref type=""bibr"" target=""#b7"">[8]</ref>. Other than 20% dropout, we use 1% uniform target sampling, 10% label smoothing <ref type=""bibr"" target=""#b14"" ence (EOS) token is proposed. We use several techniques to improve the efficiency and stability of the decoder <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b7"">8]</ref>. One such technique, used to discourage early stopping, is to only propose EOS when the corresponding probabili",0
"simple steps to pre-process and normalize the resulting text corpus for LM training. First, we detect sentence boundaries using the ""punkt"" tokenizer <ref type=""bibr"" target=""#b11"">[12]</ref> implemented in NLTK <ref type=""bibr"" target=""#b12"">[13]</ref>. We normalize the text by converting everythin",0
"h as ELMo <ref type=""bibr"" target=""#b13"">(Peters et al., 2018)</ref>, OpenAI GPT <ref type=""bibr"" target=""#b15"">(Radford et al., 2018)</ref> and BERT <ref type=""bibr"" target=""#b4"">(Devlin et al., 2018)</ref>. It has been shown that language modeling pre-training significantly improves the performanc as been commonly used in NER task <ref type=""bibr"" target=""#b2"">(Castro et al., 2018;</ref><ref type=""bibr"" target=""#b1"">de Araujo et al., 2018;</ref><ref type=""bibr"" target=""#b4"">Fernandes et al., 2018)</ref>. The model is composed of two bidirectional LSTM networks that extract and combine charact is then performed by the CRF layer. Several pre-trained word embeddings were explored by <ref type=""bibr"" target=""#b2"">Castro et al. (2018)</ref> and <ref type=""bibr"" target=""#b4"">Fernandes et al. (2018)</ref> compared it to 3 other architectures. This is the first work to explore a model that benef ummation in Eq. 1 is computed using dynamic programming. During evaluation, the most likely sequence is obtained by Viterbi decoding. As described in <ref type=""bibr"" target=""#b4"">Devlin et al. (2018)</ref>, WordPiece tokenization requires predictions and losses to be computed only for the first sub odel consists of a BiLSTM and a Linear layer. Instead of using only the last hidden representation layer of BERT, we sum the last 4 layers, following <ref type=""bibr"" target=""#b4"">Devlin et al. (2018)</ref>. The resulting architecture resembles the LSTM-CRF model <ref type=""bibr"" target=""#b10"">Lampl he tokens' encoded representation provided by BERT, we use document context for input examples instead of sentence context. Following the approach of <ref type=""bibr"" target=""#b4"">Devlin et al. (2018)</ref> on the SQuAD dataset, examples larger than S tokens are broken into spans of length up to S u CRF. The number of epochs was found using a development set comprised of 10% of the First HAREM training set. We use the customized Adam optimizer of <ref type=""bibr"" target=""#b4"">Devlin et al. (2018)</ref>.</p><p>For the feature-based approach, we use a BiLSTM with 1 layer and hidden dimension of 1",1
"cs and on its surrounding context. Moreover, there are many definitions of named entity and evaluation criteria, introducing evaluation complications <ref type=""bibr"" target=""#b12"">(Marrero et al., 2013)</ref>.</p><p>Current state-of-the-art NER systems employ neural architectures that have been pre",0
"ref type=""bibr"" target=""#b10"">(Lample et al., 2016)</ref> has been commonly used in NER task <ref type=""bibr"" target=""#b2"">(Castro et al., 2018;</ref><ref type=""bibr"" target=""#b1"">de Araujo et al., 2018;</ref><ref type=""bibr"" target=""#b4"">Fernandes et al., 2018)</ref>. The model is composed of two b",0
"g/ns/1.0""><head n=""4.1"">Datasets</head><p>The standard datasets for training and evaluating Portuguese NER task are the HAREM Golden Collections (GC) <ref type=""bibr"" target=""#b17"">(Santos et al., 2006;</ref><ref type=""bibr"" target=""#b5"">Freitas et al., 2010)</ref>. We use the GCs of the First HAREM single named entity. These criteria were adopted in order to take into account vagueness and indeterminacy that can arise in sentence interpretation <ref type=""bibr"" target=""#b17"">(Santos et al., 2006)</ref>.</p><p>Despite enriching the datasets by including such realistic information, these aspect",0
"icantly improves the performance of many natural language processing tasks and also reduces the amount of labeled data needed for supervised learning <ref type=""bibr"" target=""#b9"">(Howard and Ruder, 2018)</ref>.</p><p>For Portuguese NER, there are few available corpora and the annotated datasets are",0
"ith class imbalance, we initialize the linear layer bias term of the ""O"" tag with value of 6 in order to promote a better stability in early training <ref type=""bibr"" target=""#b11"">(Lin et al., 2017)</ref>. We also use a weight of 0.01 for ""O"" tag losses when not using a CRF layer. When evaluating,",0
"uential classification.</p><p>The LSTM-CRF architecture <ref type=""bibr"" target=""#b10"">(Lample et al., 2016)</ref> has been commonly used in NER task <ref type=""bibr"" target=""#b2"">(Castro et al., 2018;</ref><ref type=""bibr"" target=""#b1"">de Araujo et al., 2018;</ref><ref type=""bibr"" target=""#b4"">Fern r-level and word-level features. A sequential classification is then performed by the CRF layer. Several pre-trained word embeddings were explored by <ref type=""bibr"" target=""#b2"">Castro et al. (2018)</ref> and <ref type=""bibr"" target=""#b4"">Fernandes et al. (2018)</ref> compared it to 3 other archit and Date) and a Total scenario, that considers all 10 classes. This is the same setup used by <ref type=""bibr"">Santos and Guimaraes (2015)</ref> and <ref type=""bibr"" target=""#b2"">Castro et al. (2018)</ref>.</p><p>Vagueness and indeterminacy: some text segments of the GCs contain &lt;ALT&gt; tags th -based and fine-tuning based models on the two scenarios (total and selective) to the works of <ref type=""bibr"">Santos and Guimaraes (2015)</ref> and <ref type=""bibr"" target=""#b2"">Castro et al. (2018)</ref>. To make the results comparable to both works, all metrics are computed using CoNLL 2003 eval perform significantly worse compared to the ones of the fine-tuning approach, as expected. BERT-LSTM-CRF outperforms the reported results of LSTM-CRF <ref type=""bibr"" target=""#b2"">(Castro et al., 2018)</ref> by about 1.5 points on the selective scenario and 1.8 points on the total scenario. The main",0
"These features were concatenated with pre-trained word embeddings and then used to perform sequential classification.</p><p>The LSTM-CRF architecture <ref type=""bibr"" target=""#b10"">(Lample et al., 2016)</ref> has been commonly used in NER task <ref type=""bibr"" target=""#b2"">(Castro et al., 2018;</ref hat A i,j represents the score of transitioning from tag i to tag j. A includes 2 additional states: start and end of sequence.</p><p>As described by <ref type=""bibr"" target=""#b10"">Lample et al. (2016)</ref>, for an input sequence X = (x 1 , x 2 , ..., x n ) and a sequence of tag predictions y = (y we sum the last 4 layers, following <ref type=""bibr"" target=""#b4"">Devlin et al. (2018)</ref>. The resulting architecture resembles the LSTM-CRF model <ref type=""bibr"" target=""#b10"">Lample et al. (2016)</ref> replacing its embedding techniques by BERT.</p><p>As for the fine-tuning approach, the class",0
"f><ref type=""bibr"" target=""#b18"">Hamilton et al., 2017;</ref><ref type=""bibr"" target=""#b39"">Veličković et al., 2018;</ref><ref type=""bibr"">2019;</ref><ref type=""bibr"" target=""#b33"">Qu et al., 2019;</ref><ref type=""bibr"" target=""#b13"">Gao &amp; Ji, 2019;</ref><ref type=""bibr"" target=""#b28"">Ma et al., es in these datasets correspond to the bitcoin users and the edge weights between them correspond to the degree of trust between the users. Following <ref type=""bibr"" target=""#b33"">(Qu et al., 2019)</ref>, we treat edges with weights greater than 3 as positive instances, and edges with weights less CN (Kipf &amp; Welling, 2017)</ref>, and the recent state-of-the-art methods GAT <ref type=""bibr"" target=""#b39"">(Veličković et al., 2018)</ref>, GMNN <ref type=""bibr"" target=""#b33"">(Qu et al., 2019)</ref> and Graph U-Net <ref type=""bibr"" target=""#b13"">(Gao &amp; Ji, 2019)</ref>. We additionally use ne GCN method for both the datasets. Furthermore, the results of GraphMix(GCN) are comparable with the recently proposed state-of-the-art method GMNN <ref type=""bibr"" target=""#b33"">(Qu et al., 2019)</ref>. Since GraphMix consists of various components, some of which are common with the existing lite ć et al., 2018)</ref> 83.0 ± 0.7% 72.5 ± 0.7% 79.0 ± 0.3% GraphScan <ref type=""bibr"" target=""#b11"">(Ding et al., 2018)</ref> 83.3 ±1.3 73.1±1.8 -GMNN <ref type=""bibr"" target=""#b33"">(Qu et al., 2019)</ref> 83.7% 73.1% 81.8% DisenGCN <ref type=""bibr"" target=""#b28"">(Ma et al., 2019)</ref> 83.7% 73.4% 8 tecture as used in <ref type=""bibr"">GCN (Kipf &amp; Welling, 2017)</ref>, GAT <ref type=""bibr"" target=""#b39"">(Veličković et al., 2018)</ref> and GMNN <ref type=""bibr"" target=""#b33"">(Qu et al., 2019)</ref>, among others. This architecture has one hidden layer and the graph convolution is applied twic lts for semi-supervised learning over graph data, hence much of the recent attention is dedicated to proposing architectural changes to these methods <ref type=""bibr"" target=""#b33"">(Qu et al., 2019;</ref><ref type=""bibr"" target=""#b13"">Gao &amp; Ji, 2019;</ref><ref type=""bibr"" target=""#b28"">Ma et al.",1
"red. Our proposed method GraphMix<ref type=""foot"" target=""#foot_0"">1</ref> is a unified framework that utilizes interpolation based data augmentation <ref type=""bibr"" target=""#b46"">(Zhang et al., 2018;</ref><ref type=""bibr"" target=""#b41"">Verma et al., 2019a)</ref> and self-targetprediction based dat objective is to propose an efficient data augmentation technique for graph datasets.</p><p>Recent work based on interpolation-based data augmentation <ref type=""bibr"" target=""#b46"">(Zhang et al., 2018;</ref><ref type=""bibr"" target=""#b41"">Verma et al., 2019a)</ref> has seen sizable improvements in re us part of the input is zeroed out. DropBlock further extends Cutout to the hidden states. In another line of research, such as Mixup and BC-learning <ref type=""bibr"" target=""#b46"">(Zhang et al., 2018;</ref><ref type=""bibr"" target=""#b38"">Tokozume et al., 2017)</ref>, additional training samples are ecently, interpolation-based techniques have been proposed for regularizing neural networks. We briefly describe some of these techniques here. Mixup <ref type=""bibr"" target=""#b46"">(Zhang et al., 2018)</ref> trains a neural network on the convex combination of input and targets, whereas Manifold Mix",0
""">Henaff et al., 2015;</ref><ref type=""bibr"" target=""#b8"">Defferrard et al., 2016;</ref><ref type=""bibr"" target=""#b20"">Kipf &amp; Welling, 2016;</ref><ref type=""bibr"" target=""#b15"">Gilmer et al., 2017;</ref><ref type=""bibr"" target=""#b18"">Hamilton et al., 2017;</ref><ref type=""bibr"" target=""#b39"">Vel amp; Welling, 2017)</ref>, Graph Attention Layer <ref type=""bibr"" target=""#b39"">(Veličković et al., 2018)</ref>, or any general message passing layer <ref type=""bibr"" target=""#b15"">(Gilmer et al., 2017)</ref>. Formally, let H l ∈ R n×k be a matrix containing the k-dimensional representation of n nod",0
"local smoothness over the predicted targets for the nodes <ref type=""bibr"">(Zhu &amp; Ghahramani, 2002;</ref><ref type=""bibr"">Zhu et al., 2003;</ref><ref type=""bibr"" target=""#b1"">Belkin et al., 2006)</ref>. Another line of work learns node embedding in an unsupervised way <ref type=""bibr"" target=""# ""2"" xml:id=""foot_1"">https://github.com/shchur/gnn-benchmark</note> 		</body> 		<back> 			<div type=""annex""> <div xmlns=""http://www.tei-c.org/ns/1.0""> <ref type=""bibr"" target=""#b1"">(Belkin et al., 2006)</ref> <p>59.5% 60.1% 70.7% SemiEmb <ref type=""bibr"" target=""#b43"">(Weston et al., 2012)</ref> 59.0",0
"extract local visual features from multiple local regions at multiple scales of the original image.</p><p>Lastly, inspired by the recent Transformer <ref type=""bibr"" target=""#b22"">[23]</ref> model in dealing with a number of difficult NLP tasks such as machine translation successfully without attac viding the entire visual feature blocks in a pyramidal way, following by corresponding pooling strategy. Moreover, inspired by the recent Transformer <ref type=""bibr"" target=""#b22"">[23]</ref> model in dealing with a number of difficult NLP tasks such as machine translation successfully, we implant a ined in pyramidal segmentation. However, the association between these local regions are still not explored. Inspired by the recent Transformer model <ref type=""bibr"" target=""#b22"">[23]</ref>, we implant a self attention module adapted to image features. Through the self attention module, we mine th layer, swaps it to the output of the corresponding sentiment class number of datasets, and then tunes the parameters of all layers. • Self-Attention <ref type=""bibr"" target=""#b22"">[23]</ref> is a variant derived from the Transformer model. Transformer is a simple network architecture that based sol",1
"methods exhibit higher performance and robustness than the traditional hand-crafted feature based methods <ref type=""bibr"" target=""#b14"">[15]</ref>, <ref type=""bibr"" target=""#b15"">[16]</ref>, making fundamental changes in the research of visual sentiment analysis fields.</p><p>Motivated by the psyc",0
"ne-grained emotions (such as love, joy, surprise, sadness, fear, anger, disgust, and anxiety) are considered <ref type=""bibr"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b1"">[2]</ref>. Traditional studies on visual sentiment analysis are mainly based on hand-crafted features, such as global an",0
"neutral, or negative) of subjective elements within a text through text analysis, and computational linguistics techniques, and achieved good results <ref type=""bibr"" target=""#b25"">[26]</ref>- <ref type=""bibr"" target=""#b27"">[28]</ref>. In the field of computer vision (CV), there are also some good r",0
"ion V.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>II. RELATED WORK</head><p>Sentiment analysis is a very important and challenging task <ref type=""bibr"" target=""#b23"">[24]</ref>, <ref type=""bibr"" target=""#b24"">[25]</ref>. Most of the previous research has focused on detecting the senti",0
"ions for anomaly detection.</p><p>To address the challenges above, we propose to model the attributed networks with graph convolutional network (GCN) <ref type=""bibr"" target=""#b15"">[16]</ref>. GCN, which takes the topological structure and nodal attributes as input, is able to learn discriminative n nteractions) simultaneously. To this end, we propose a new type of attributed network encoder inspired by the graph convolutional network (GCN) model <ref type=""bibr"" target=""#b15"">[16]</ref>. Specifically, GCN considers the high-order node proximity when learning the embedding representations, thus to the number of edges on the network.</p><p>For a particular layer, the convolution operation is D − 1 2 A D − 1 2 XW, and its complexity is O(mdh) <ref type=""bibr"" target=""#b15"">[16]</ref> as AX can be efficiently implemented using sparse-dense matrix multiplications, where m is the number of non "" target=""#b11"">12]</ref> demonstrate superior learning performance by considering neighbors of nodes that are multiple hops away. In particular, GCN <ref type=""bibr"" target=""#b15"">[16]</ref> takes the structure and attribute information as input, and extends the operation of convolution on network node content and label information via a tri-party autoencoder architecture. Meanwhile, recent research advances on graph convolutional network (GCN) <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b11"">12]</ref> demonstrate superior learning",1
"bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b24"">25]</ref>. Recently, residual analysis has emerged as another way to find anomalous nodes <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b22"">23]</ref>, where anomalies are defined as the nodes that cannot be approximate the performance of different anomaly detection algorithms:</p><p>• ROC-AUC: As a widely used evaluation metric in previous anomaly detection methods <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b22"">23]</ref>, the ROC curve is a plot of true positive rate (an anomaly is recogn presented by an adjacency matrix A, where A i,j = 1 if there is a link between node v i and node v j . Otherwise, A i,j = 0. We follow the setting of <ref type=""bibr"" target=""#b16"">[17]</ref> to obtain the adjacency matrix A = max(A, A T ) for directed networks. To make the results more interpretabl level and only considers nodal attributes.</p><p>• SCAN <ref type=""bibr"" target=""#b33"">[34]</ref> is a structure based detection method</p><p>• Radar <ref type=""bibr"" target=""#b16"">[17]</ref> is the state-of-the-art unsupervised anomaly detection framework for attributed networks. It detects anomali mentioned above, residual analysis has emerged as another common way to measure the abnormality of nodes on attributed networks. In particular, Radar <ref type=""bibr"" target=""#b16"">[17]</ref> characterizes the residuals of attribute information and its coherence with network information for anomaly <head n=""3.1"">Preliminary -Deep Autoencoder</head><p>As suggested by <ref type=""bibr"" target=""#b31"">[32,</ref><ref type=""bibr"" target=""#b36"">37,</ref><ref type=""bibr"" target=""#b16"">17]</ref>, the disparity between the original data and the estimated data (i.e., reconstruction errors) is a strong ind",0
"traditional plain networks in which only node-to-node interactions are observed, attributed networks also encode a rich set of features for each node <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b17"">18]</ref>. They are increasingly used to easingly used to model a wide range of complex systems, such as social media networks, critical infrastructure networks, and gene regulatory networks <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b25"">26]</ref>. For example, in social networks, users not only are connected with ea",0
"complex systems, such as social media networks, critical infrastructure networks, and gene regulatory networks <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b25"">26]</ref>. For example, in social networks, users not only are connected with each other by performing various social a",0
"ound truth anomalies, existing efforts are mostly unsupervised. Among them, one family of methods study the problem at the mesoscope with ego-network <ref type=""bibr"" target=""#b23"">[24]</ref> or community analysis <ref type=""bibr"" target=""#b9"">[10]</ref> and then identify anomalies by measuring the multaneously find communities as well as spot community anomalies within a unified probabilistic model <ref type=""bibr"" target=""#b9"">[10]</ref>. AMEN <ref type=""bibr"" target=""#b23"">[24]</ref> considers the ego-network information for each node and discovers anomalous neighborhoods on attributed netw </p></div> 			</div>  			<div type=""annex""> <div xmlns=""http://www.tei-c.org/ns/1.0""><p>which detects anomalies at the structural level.</p><p>• AMEN <ref type=""bibr"" target=""#b23"">[24]</ref> uses both attribute and network structure information to detect anomalous neighborhoods. Specifically, it an",0
"t suggests that providing negative signals throughout the learning process would be beneficial to enhance the OOD discriminative power of the system. <ref type=""bibr"" target=""#b3"">Hendrycks et al. (2019)</ref> demonstrate that utilizing auxiliary datasets as OOD examples (as a supervised signal) sig tinguishing OOD data from the training data, yielding higher likelihood estimates on unseen OOD samples. The ominous observation is presented also by <ref type=""bibr"" target=""#b3"">Hendrycks et al. (2019)</ref>, but they concentrate on improving the OOD data detection with Outlier Exposure. Their wor =""#b12"">(Shafaei et al., 2018;</ref><ref type=""bibr"" target=""#b8"">Nalisnick et al., 2019a;</ref><ref type=""bibr"" target=""#b0"">Choi et al., 2018;</ref><ref type=""bibr"" target=""#b3"">Hendrycks et al., 2019)</ref>. These works report that despite intuitive expectations, generative models -including but",1
"hypothesize that the reason behind this is that obtained negative samples are richer in features and semantically more meaningful for the task. (See <ref type=""bibr"" target=""#b6"">Lee et al. (2018)</ref> for an incarnation of this idea in the context of classification and generative adversarial netw this approach: one should choose the auxiliary datasets carefully to obtain robust OOD detection.</p><p>Within the context of uncertainty estimation, <ref type=""bibr"" target=""#b6"">Lee et al. (2018)</ref> demonstrate that adversarially generated samples improve the confidence of classifiers in their t OOD dataset. Their method of utilizing generated samples of GANs is closest to our approach of using generated data points as negative samples, but <ref type=""bibr"" target=""#b6"">Lee et al. (2018)</ref> work within a classification setting. <ref type=""bibr"" target=""#b9"">Nalisnick et al. (2019b)</re",0
"ses • Fashion-MNIST <ref type=""bibr"" target=""#b14"">(Xiao et al., 2017)</ref>: 28x28x1, 60.000 train + 10.000 test, 10 classes</p><p>• Kuzushiji-MNIST <ref type=""bibr"" target=""#b1"">(Clanuwat et al., 2018)</ref>: 28x28x1, 60.000 train + 10.000 test, 10 classes  <ref type=""bibr"">(2018)</ref>, all of th",0
"onstrates the effectiveness of deep generative models in this regard, recent work of <ref type=""bibr"" target=""#b8"">Nalisnick et al. (2019a)</ref> and <ref type=""bibr"" target=""#b0"">Choi et al. (2018)</ref> show that these models often fail even at a task that is supposed to be close to their original e phenomenon The results suggest that the intriguing phenomenon in VAEs discussed by <ref type=""bibr"" target=""#b8"">Nalisnick et al. (2019a)</ref> and <ref type=""bibr"" target=""#b0"">Choi et al. (2018)</ref> is highly dependent on modelling choices. In the case of grayscale images, when changing the no generative models on OOD data <ref type=""bibr"" target=""#b12"">(Shafaei et al., 2018;</ref><ref type=""bibr"" target=""#b8"">Nalisnick et al., 2019a;</ref><ref type=""bibr"" target=""#b0"">Choi et al., 2018;</ref><ref type=""bibr"" target=""#b3"">Hendrycks et al., 2019)</ref>. These works report that despite int",0
"d n=""5"">RELATED WORK</head><p>Our investigations are mostly inspired by and related to recent work on the evaluation of generative models on OOD data <ref type=""bibr"" target=""#b12"">(Shafaei et al., 2018;</ref><ref type=""bibr"" target=""#b8"">Nalisnick et al., 2019a;</ref><ref type=""bibr"" target=""#b0"">C",0
"ng and relational reasoning <ref type=""bibr"" target=""#b13"">(Kipf and Welling, 2017;</ref><ref type=""bibr"" target=""#b27"">Velickovic et al., 2018;</ref><ref type=""bibr"" target=""#b19"">Palm et al., 2018)</ref>, we propose an evidence reasoning network (ERNet) to propagate information among the evidence",1
"n extracted.</p><p>How to automatically verify the data becomes a vital problem for various datadriven applications, e.g., knowledge graph completion <ref type=""bibr"" target=""#b28"">(Wang et al., 2017)</ref> and open domain question answering <ref type=""bibr"" target=""#b2"">(Chen et al., 2017a)</ref>.",0
"final verification. Then, <ref type=""bibr"" target=""#b10"">Hanselowski et al. (2018)</ref>; <ref type=""bibr"" target=""#b31"">Yoneda et al. (2018)</ref>; <ref type=""bibr"" target=""#b11"">Hidey and Diab (2018)</ref> adopt the enhanced sequential inference model (ESIM) <ref type=""bibr"" target=""#b3"">(Chen et",0
"at the evidence can support, refute, or is not sufficient for the claim.</p><p>Existing FV methods formulate FV as a natural language inference (NLI) <ref type=""bibr"" target=""#b0"">(Angeli and Manning, 2014)</ref> task. However, they utilize simple evidence combination methods such as concatenating t",0
"the neural semantic matching network (NSMN), which is a modification of ESIM and achieves the best results in the competition. Unlike these methods, <ref type=""bibr"" target=""#b30"">Yin and Roth (2018)</ref> propose the TWOWINGOS system which trains the evidence identification and claim verification",0
"orks <ref type=""bibr"" target=""#b21"">(Zeng et al., 2014</ref><ref type=""bibr"" target=""#b20"">(Zeng et al., , 2015) )</ref> and recurrent neural network <ref type=""bibr"" target=""#b22"">(Zhang et al., 2015;</ref><ref type=""bibr"" target=""#b23"">Zhou et al., 2016)</ref>. To automatically obtain a large trai f type=""bibr"" target=""#b20"">(Zeng et al., 2015)</ref> is a revision of CNN which uses piecewise max-pooling to extract more relation features. BiLSTM <ref type=""bibr"" target=""#b22"">(Zhang et al., 2015)</ref> is also commonly used for RE with the help of position embeddings. BiLSTM+ATT <ref type=""bib",1
"ef><ref type=""bibr"" target=""#b20"">(Zeng et al., , 2015) )</ref> and recurrent neural network <ref type=""bibr"" target=""#b22"">(Zhang et al., 2015;</ref><ref type=""bibr"" target=""#b23"">Zhou et al., 2016)</ref>. To automatically obtain a large training dataset, DS has been proposed <ref type=""bibr"" targe order to capture the key feature words for identifying relations, we apply an attention mechanism over a BiLSTM Encoder, which is first introduced in <ref type=""bibr"" target=""#b23"">(Zhou et al., 2016)</ref> for RC. The model architecture is illustrated on the left side of Figure <ref type=""figure"" t tures. BiLSTM <ref type=""bibr"" target=""#b22"">(Zhang et al., 2015)</ref> is also commonly used for RE with the help of position embeddings. BiLSTM+ATT <ref type=""bibr"" target=""#b23"">(Zhou et al., 2016)</ref> adds an attention mechanism into BiLSTM to capture the most important features for identifyin",1
"y three kinds of methods for dealing with such noise problem. First, multiinstance learning <ref type=""bibr"" target=""#b15"">(Riedel et al., 2010;</ref><ref type=""bibr"" target=""#b8"">Lin et al., 2016;</ref><ref type=""bibr"" target=""#b16"">Surdeanu et al., 2012;</ref><ref type=""bibr"" target=""#b20"">Zeng et lve the noise problem. The first widely studied method is based on multi-instance  learning <ref type=""bibr"" target=""#b15"">(Riedel et al., 2010;</ref><ref type=""bibr"" target=""#b8"">Lin et al., 2016;</ref><ref type=""bibr"" target=""#b16"">Surdeanu et al., 2012;</ref><ref type=""bibr"" target=""#b20"">Zeng et org/ns/1.0""><head n=""4.2"">Baselines</head><p>We compare our ARNOR framework with several strong baselines for noise reduction as follows: PCNN+SelATT <ref type=""bibr"" target=""#b8"">(Lin et al., 2016)</ref> is a bag-level RC model. It adopts an attention mechanism over all sentences in a bag and thus",0
"arget=""#b10"">Qin et al., 2018a;</ref><ref type=""bibr"" target=""#b4"">Han et al., 2018;</ref><ref type=""bibr"" target=""#b18"">Xiangrong et al., 2018;</ref><ref type=""bibr"" target=""#b11"">Qin et al., 2018b)</ref>. This line of research selects confident relation labels by matching the predicted label of th cond kind of approach utilizes RL <ref type=""bibr"" target=""#b2"">(Feng et al., 2018b;</ref><ref type=""bibr"" target=""#b18"">Xiangrong et al., 2018;</ref><ref type=""bibr"" target=""#b11"">Qin et al., 2018b)</ref> or adversarial training <ref type=""bibr"" target=""#b10"">(Qin et al., 2018a;</ref><ref type=""bib (RL) based model for RC from noisy data. It jointly trains a CNN model for RC as well as an instance selector to remove unconfident samples. CNN+RL 1 <ref type=""bibr"" target=""#b11"">(Qin et al., 2018b</ref>) also introduces RL to heuristically recognize false positive instances. Different from <ref t",0
"ffectiveness of our method on more tasks, including open information extraction and event extraction, and also overlapping relation extraction models <ref type=""bibr"" target=""#b0"">(Dai et al., 2019)</ref>.</p></div><figure xmlns=""http://www.tei-c.org/ns/1.0"" xml:id=""fig_0""><head>Figure 1 :</head><la",0
"problem. First, multiinstance learning <ref type=""bibr"" target=""#b15"">(Riedel et al., 2010;</ref><ref type=""bibr"" target=""#b8"">Lin et al., 2016;</ref><ref type=""bibr"" target=""#b16"">Surdeanu et al., 2012;</ref><ref type=""bibr"" target=""#b20"">Zeng et al., 2015)</ref> relaxes the DS assumption as at-lea d is based on multi-instance  learning <ref type=""bibr"" target=""#b15"">(Riedel et al., 2010;</ref><ref type=""bibr"" target=""#b8"">Lin et al., 2016;</ref><ref type=""bibr"" target=""#b16"">Surdeanu et al., 2012;</ref><ref type=""bibr"" target=""#b20"">Zeng et al., 2015)</ref>. However, it models noise problem o",0
"e=""figure"" target=""#fig_1"">2</ref>).</p><p>There are mainly three kinds of methods for dealing with such noise problem. First, multiinstance learning <ref type=""bibr"" target=""#b15"">(Riedel et al., 2010;</ref><ref type=""bibr"" target=""#b8"">Lin et al., 2016;</ref><ref type=""bibr"" target=""#b16"">Surdeanu p>Previous studies make attempts on kinds of methods to solve the noise problem. The first widely studied method is based on multi-instance  learning <ref type=""bibr"" target=""#b15"">(Riedel et al., 2010;</ref><ref type=""bibr"" target=""#b8"">Lin et al., 2016;</ref><ref type=""bibr"" target=""#b16"">Surdeanu",0
">. The phrase ""was born in"" explains the relation type ""place of birth"" for ""Bill Lockyer"" and ""California"". Such indicating words is called patterns <ref type=""bibr"" target=""#b5"">(Hearst, 1992;</ref><ref type=""bibr"" target=""#b3"">Hamon and Nazarenko, 2001)</ref>. The bold words ""was born in"" in s 1 d DS-generated labels are both wrong. The third method relies on relation patterns. Pattern-based extraction is widely used in information extraction <ref type=""bibr"" target=""#b5"">(Hearst, 1992;</ref><ref type=""bibr"" target=""#b3"">Hamon and Nazarenko, 2001)</ref>. Among them, the generative model <re n et al., 2018a;</ref><ref type=""bibr"" target=""#b4"">Han et al., 2018)</ref> to select trustable instances. The third research line relies on patterns <ref type=""bibr"" target=""#b5"">(Hearst, 1992;</ref><ref type=""bibr"" target=""#b3"">Hamon and Nazarenko, 2001)</ref>. <ref type=""bibr"" target=""#b17"">Takam",0
"ng process of DS and finds noisy patterns that mistakenly label a relation. Data programming <ref type=""bibr"" target=""#b13"">(Ratner et al., 2016</ref><ref type=""bibr"" target=""#b12"">(Ratner et al., , 2017) )</ref> fuses DS-based labels and manual relation patterns for reducing noise.</p><p>In this pa amatsu et al. (2012)</ref> directly models the labeling process of DS to find noisy patterns. <ref type=""bibr"" target=""#b13"">Ratner et al. (2016</ref><ref type=""bibr"" target=""#b12"">Ratner et al. ( , 2017) )</ref> proposes to fuse DS-based labels and manual relation patterns for reducing noise. <ref",0
"ication (RC) is a fundamental task in natural language processing (NLP) and is particularly important for knowledge base construction. The goal of RC <ref type=""bibr"" target=""#b19"">(Zelenko et al., 2003)</ref> is to identify the relation type of a given entity pair in a sentence. Generally, a relati",0
"h an adapted Gated Graph Sequence Neural Networks (GGNN) <ref type=""bibr"" target=""#b10"">(Li et al., 2016)</ref> and a standard bidirectional LSTM-CRF <ref type=""bibr"" target=""#b7"">(Lample et al., 2016)</ref> (BiLSTM-CRF), our model learns a weighted combination of the information from different gaze of OntoNotes, MSRA, Weibo-NER and E-commerce-NER are respec-tively 39.70%, 44.75%, 36.10% and 46.05%.</p><p>Models for Comparison. We use BiLSTM-CRF <ref type=""bibr"" target=""#b7"">(Lample et al., 2016)</ref> with character+bigram embedding without using any gazetteer as the comparison baseline<ref t",1
"which can explicitly model the interaction of the characters and the gazetteers. Combined with an adapted Gated Graph Sequence Neural Networks (GGNN) <ref type=""bibr"" target=""#b10"">(Li et al., 2016)</ref> and a standard bidirectional LSTM-CRF <ref type=""bibr"" target=""#b7"">(Lample et al., 2016)</ref> tual information compared to other GNNs such as GCN <ref type=""bibr"" target=""#b6"">(Kipf and Welling, 2017)</ref>.</p><p>However, the traditional GGNN <ref type=""bibr"" target=""#b10"">(Li et al., 2016</ref>) is unable to distinguish edges with different labels. We adapt GGNN so as to learn a weighted c",1
"need a way to resolve the conflicting matches. Existing methods often rely on hand-crafted templates or predefined selection strategies. For example, <ref type=""bibr"" target=""#b12"">Qi et al. (2019)</ref> defined several n-gram templates to construct features for each character based on dictionaries",0
"<ref type=""bibr"" target=""#b19"">(Zamin and Oxley, 2011)</ref>. Besides, gazetteers can also be easily constructed from knowledge bases (e.g., Freebase <ref type=""bibr"" target=""#b0"">(Bollacker et al., 2008)</ref>) or com- While such background knowledge can be helpful, in practice the gazetteers may a",0
"=""http://www.tei-c.org/ns/1.0""><head n=""3.1"">Experimental Setup</head><p>Dataset. The three public datasets used in our experiments are OntoNotes 4.0 <ref type=""bibr"" target=""#b17"">(Weischedel et al., 2010)</ref>, MSRA <ref type=""bibr"" target=""#b8"">(Levow, 2006)</ref>, and Weibo-NER <ref type=""bibr""",0
"et al., 2005)</ref>. On the one hand, the use of NE gazetteers alleviates the need of manually labeling the data and can handle rare and unseen cases <ref type=""bibr"" target=""#b16"">(Wang et al., 2018)</ref>. On the other hand, resources of gazetteers are abundant. Many gazetteers have been manually n-Independent Entity Type (PIET) features and Position-Dependent Entity Type (PDET) features. These feature construction processes follow the work of <ref type=""bibr"" target=""#b16"">Wang et al. (2018)</ref>. We refer the readers to their paper for further details.</p><p>To show the effect of adding g",0
"e OntoNotes 4.0 <ref type=""bibr"" target=""#b17"">(Weischedel et al., 2010)</ref>, MSRA <ref type=""bibr"" target=""#b8"">(Levow, 2006)</ref>, and Weibo-NER <ref type=""bibr"" target=""#b11"">(Peng and Dredze, 2016)</ref>. OntoNotes and MSRA are two datasets consisting of newswire text. Weibo-NER is in the dom ting of newswire text. Weibo-NER is in the domain of social media. We use the same split as <ref type=""bibr"" target=""#b1"">Che et al. (2013)</ref> and <ref type=""bibr"" target=""#b11"">Peng and Dredze (2016)</ref> on OntoNotes and on Weibo-NER. To demonstrate the effectiveness of our model in the e-comm",0
"teer the node represents. In the case of character nodes, a bigram embedding table W bi is used since it has been shown to be useful for the NER task <ref type=""bibr"" target=""#b2"">(Chen et al., 2015)</ref>.</p><p>The structural information of the graph is stored in the adjacency matrix A which serve",0
"e, 2016)</ref>. OntoNotes and MSRA are two datasets consisting of newswire text. Weibo-NER is in the domain of social media. We use the same split as <ref type=""bibr"" target=""#b1"">Che et al. (2013)</ref> and <ref type=""bibr"" target=""#b11"">Peng and Dredze (2016)</ref> on OntoNotes and on Weibo-NER. T",0
"umber of input frames <ref type=""bibr"" target=""#b26"">(Yao et al. 2015)</ref> and extracting the last hidden state of recurrent visual feature encoder <ref type=""bibr"" target=""#b20"">(Venugopalan et al. 2015)</ref>.</p><p>Those feature encoding methods mentioned above are only based on visual cues. Ho 1.0""><head>Basic Video Caption Framework</head><p>Our basic video caption framework is extended from S2VT (sequence to sequence: video to text) model <ref type=""bibr"" target=""#b20"">(Venugopalan et al. 2015)</ref> and M 3 (multimodal memory modeling) model <ref type=""bibr"" target=""#b22"">(Wang et al.",1
"(sequence to sequence: video to text) model <ref type=""bibr"" target=""#b20"">(Venugopalan et al. 2015)</ref> and M 3 (multimodal memory modeling) model <ref type=""bibr"" target=""#b22"">(Wang et al. 2016)</ref>, which is shown in Figure <ref type=""figure"" target=""#fig_0"">1</ref>. As in Figure <ref type="" across visual and audio modalities during encoding stage.</p><p>We compare our feature fusion models with several video caption models, including M 3 <ref type=""bibr"" target=""#b22"">(Wang et al. 2016)</ref>, Visual model (our basic video caption model), Audio model (our basic video caption model with Framework Figure <ref type=""figure"" target=""#fig_9"">6</ref> presents some sentences generated by GA (models with only generated audio features), M 3 <ref type=""bibr"" target=""#b22"">(Wang et al. 2016</ref>), V-ShaWei-GA models and human-annotated ground truth based on the test set of MSVD.</p><p>Conc",1
"</ref><ref type=""bibr"" target=""#b9"">2016b)</ref>, which take inspiration from image caption <ref type=""bibr"" target=""#b21"">(Vinyals et al. 2015;</ref><ref type=""bibr"" target=""#b3"">Donahue et al. 2015)</ref>.</p><p>We argue that these video caption methods only rely on visual information while ignori",0
"t, they were lack of richness and flexibility.</p><p>The second category treat video caption as a retrieval problem. They tagged videos with metadata <ref type=""bibr"" target=""#b0"">(Aradhye, Toderici, and Yagnik 2009)</ref> and then clustered videos and captions based on these tags. Although the gene",0
"ods. They first identified the semantic attributes hidden in videos and then derived a sentence structure based on some predefined sentence templates <ref type=""bibr"" target=""#b6"">(Krishnamoorthy et al. 2013;</ref><ref type=""bibr"" target=""#b18"">Thomason et al. 2014)</ref>. Then, probabilistic graphi",0
"tly, benefiting from extraordinary abilities of convolutional neural networks (CNN) <ref type=""bibr"" target=""#b14"">(Simonyan and Zisserman 2014;</ref><ref type=""bibr"" target=""#b16"">Szegedy et al. 2015;</ref><ref type=""bibr"">2016)</ref>, recurrent neural networks (RNN) <ref type=""bibr"" target=""#b5"">(",0
"ncy.</p><p>To enhance the memory ability of RNN, an external memory has been utilized to extend RNN in some works, such as Neural Turing Machine(NTM) <ref type=""bibr"" target=""#b4"">(Graves, Wayne, and Danihelka 2014)</ref>, memory network <ref type=""bibr"" target=""#b23"">(Weston, Chopra, and Bordes 201",0
"head></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2.6.1"">Implementation</head><p>We implement the neural network using the torch7 library <ref type=""bibr"" target=""#b7"">(Collobert et al., 2011a)</ref>. Training and inference are done on a per-sentence level. The initial states of the LSTM",1
"e sophisticated optimization algorithms such as momentum <ref type=""bibr"" target=""#b24"">(Nesterov, 1983)</ref>, AdaDelta (Zeiler, 2012), and RM-SProp <ref type=""bibr"" target=""#b15"">(Hinton et al., 2012)</ref>, and in preliminary experiments they did not improve upon plain SGD.</p></div> <div xmlns=""",0
"categories (Person, Organization, Location, Misc) defined by the CoNLL 2003 NER shared task, we compiled a list of known named entities from DBpedia <ref type=""bibr"" target=""#b1"">(Auer et al., 2007)</ref>, by extracting all descendants of DBpedia types corresponding to the CoNLL categories. <ref ty",0
"2014)</ref> was quite effective in reducing overfitting (Section 4.4). We explored other more sophisticated optimization algorithms such as momentum <ref type=""bibr"" target=""#b24"">(Nesterov, 1983)</ref>, AdaDelta (Zeiler, 2012), and RM-SProp <ref type=""bibr"" target=""#b15"">(Hinton et al., 2012)</ref",0
"e=""bibr"" target=""#b13"">(Graves et al., 2013)</ref>, machine translation <ref type=""bibr"" target=""#b3"">(Cho et al., 2014)</ref>, and language modeling <ref type=""bibr"" target=""#b22"">(Mikolov et al., 2011)</ref>. The long-short term memory (LSTM) unit with the forget gate allows highly non-trivial lon",0
"al representation <ref type=""bibr"" target=""#b23"">(Zeiler and Fergus, 2014)</ref> of the input. The best networks are using more than 150 layers as in <ref type=""bibr"" target=""#b6"">(He et al., 2016a;</ref><ref type=""bibr"" target=""#b7"">He et al., 2016b)</ref>.</p><p>Many NLP approaches consider words our architecture is inspired by recent progress in computer vision, in particular <ref type=""bibr"" target=""#b17"">(Simonyan and Zisserman, 2015;</ref><ref type=""bibr"" target=""#b6"">He et al., 2016a)</ref>.</p><p>This paper is structured as follows. There have been previous attempts to use ConvNets fo #b12"">(Krizhevsky et al., 2012)</ref>, namely 19 layers <ref type=""bibr"" target=""#b17"">(Simonyan and Zisserman, 2015)</ref>, or even up to 152 layers <ref type=""bibr"" target=""#b6"">(He et al., 2016a)</ref>. In the remainder of this paper, we describe our very deep convolutional architecture and repor k <ref type=""bibr"" target=""#b17"">(Simonyan and Zisserman, 2015)</ref>. We have also investigated the same kind of ""ResNet shortcut"" connections as in <ref type=""bibr"" target=""#b6"">(He et al., 2016a)</ref>, namely identity and 1 × 1 convolutions (see Figure <ref type=""figure"" target=""#fig_0"">1</ref>) to documents (with multiple sentences).</p><p>Going even deeper degrades accuracy. Shortcut connections help reduce the degradation. As described in <ref type=""bibr"" target=""#b6"">(He et al., 2016a)</ref>, the gain in accuracy due to the the increase of the depth is limited when using standard ConvN the model, the ResNet model introduced shortcut connections between convolutional blocks that allow the gradients to flow more easily in the network <ref type=""bibr"" target=""#b6"">(He et al., 2016a)</ref>.</p><p>We evaluate the impact of shortcut connections by increasing the number of convolutions lan to further explore adaptations of residual networks to temporal convolutions as we think this a milestone for going deeper in NLP. Residual units <ref type=""bibr"" target=""#b6"">(He et al., 2016a)</ref>  work and ImageNet is that the latter deals with 1000 classes and thus much more information is",1
"layers to approach this goal, using up to 29 layers. The design of our architecture is inspired by recent progress in computer vision, in particular <ref type=""bibr"" target=""#b17"">(Simonyan and Zisserman, 2015;</ref><ref type=""bibr"" target=""#b6"">He et al., 2016a)</ref>.</p><p>This paper is structur gnificant improvements have been reported using much deeper networks <ref type=""bibr"" target=""#b12"">(Krizhevsky et al., 2012)</ref>, namely 19 layers <ref type=""bibr"" target=""#b17"">(Simonyan and Zisserman, 2015)</ref>, or even up to 152 layers <ref type=""bibr"" target=""#b6"">(He et al., 2016a)</ref>. ine these different ""3-gram features"" in a deep hierarchical manner. Our architecture can be in fact seen as a temporal adaptation of the VGG network <ref type=""bibr"" target=""#b17"">(Simonyan and Zisserman, 2015)</ref>. We have also investigated the same kind of ""ResNet shortcut"" connections as in <r",1
"tum of 0.9. We follow the same training procedure as in <ref type=""bibr"">(Zhang et al., 2015)</ref>. We initialize our convolutional layers following <ref type=""bibr"" target=""#b5"">(He et al., 2015)</ref>. One epoch took from 24 minutes to 2h45 for depth 9, and from 50 minutes to 7h (on the largest d",0
"rt and Weston, 2008;</ref><ref type=""bibr"" target=""#b3"">Collobert et al., 2011)</ref>. They have been subsequently applied to sentence classification <ref type=""bibr"" target=""#b11"">(Kim, 2014;</ref><ref type=""bibr"" target=""#b10"">Kalchbrenner et al., 2014;</ref><ref type=""bibr"">Zhang et al., 2015)</r therwise stated, all approaches operate on words which are projected into a high-dimensional space.</p><p>A rather shallow neural net was proposed in <ref type=""bibr"" target=""#b11"">(Kim, 2014)</ref>: one convolutional layer (using multiple widths and filters) followed by a max pooling layer over tim",0
"<ref type=""bibr"" target=""#b16"">(Sennrich et al., 2016)</ref> or words up to whole sentences or even paragraphs.</p><p>After a couple of pioneer works <ref type=""bibr"" target=""#b0"">(Bengio et al. (2001)</ref>, <ref type=""bibr"" target=""#b2"">Collobert and Weston (2008)</ref>, <ref type=""bibr"" target=""#",0
"content (e.g. machine translation, summarization). The level of granularity of this processing can range from individual characters to subword units <ref type=""bibr"" target=""#b16"">(Sennrich et al., 2016)</ref> or words up to whole sentences or even paragraphs.</p><p>After a couple of pioneer works",0
"bination of recurrent and convolutional networks in one architecture has also been investigated, with the goal to ""get the best of both worlds"", e.g. <ref type=""bibr"" target=""#b15"">(Pinheiro and Collobert, 2014)</ref>. The same idea was recently applied to sentence classification <ref type=""bibr"" ta",0
"mance. This is in sharp contrast to the current trend in computer vision where significant improvements have been reported using much deeper networks <ref type=""bibr"" target=""#b12"">(Krizhevsky et al., 2012)</ref>, namely 19 layers <ref type=""bibr"" target=""#b17"">(Simonyan and Zisserman, 2015)</ref>,",0
"any other graph neural models <ref type=""bibr"" target=""#b32"">(Monti et al., 2017;</ref><ref type=""bibr"" target=""#b12"">Duran &amp; Niepert, 2017;</ref><ref type=""bibr"" target=""#b27"">Li et al., 2018)</ref>; we refer to <ref type=""bibr"" target=""#b57"">Zhou et al. (2018)</ref>; <ref type=""bibr"" target=""#",1
"r geolocation locates the ""home"" position of users on social media given users' posts, connections among users, and a small number of labelled users. <ref type=""bibr"" target=""#b34"">Rahimi et al. (2018)</ref> apply GCNs with highway connections on this task and achieve close to state-of-the-art resul",1
"-Haija et al. (2018</ref><ref type=""bibr"" target=""#b29"">), and Liao et al. (2019)</ref> exploit multi-scale information by raising S to higher order. <ref type=""bibr"" target=""#b48"">Xu et al. (2019)</ref> study the expressiveness of graph neural networks in terms of their ability to distinguish any t t al., 2019)</ref> using the publicly released implementations. Since GIN is not initially evaluated on citation networks, we implement GIN following <ref type=""bibr"" target=""#b48"">Xu et al. (2019)</ref> and use hyperopt to tune weight decay and learning rate for 60 iterations. Moreover, we tune the l. (2018)</ref>'s observation that GCNs may not be necessary.</p><p>Graph classification requires models to use graph structure to categorize graphs. <ref type=""bibr"" target=""#b48"">Xu et al. (2019)</ref> theoretically show that GCNs are not sufficient to distinguish certain graph structures and show",0
"d propagation matrix S. <ref type=""bibr"" target=""#b10"">Chen et al. (2018)</ref> propose an efficient variant of GCN based on importance sampling, and <ref type=""bibr"" target=""#b15"">Hamilton et al. (2017)</ref> propose a framework based on sampling and aggregation. <ref type=""bibr"" target=""#b1"">Atwoo SGC to the reported performance of GaAN <ref type=""bibr"" target=""#b52"">(Zhang et al., 2018a)</ref>, supervised and unsupervised variants of GraphSAGE <ref type=""bibr"" target=""#b15"">(Hamilton et al., 2017)</ref>, FastGCN, and DGI. Table <ref type=""table"">3</ref> also highlights the setting of the fea ts. Previous approaches tackle this limitation by either sampling to reduce neighborhood size <ref type=""bibr"" target=""#b10"">(Chen et al., 2018;</ref><ref type=""bibr"" target=""#b15"">Hamilton et al., 2017)</ref> or limiting their model sizes <ref type=""bibr"" target=""#b43"">(Velikovi et al., 2019)</ref>",0
"h SGC (K = 2) and call the resulting model C-SGC. Table <ref type=""table"" target=""#tab_4"">6</ref> shows that C-SGR sets new state-ofthe-art on TACRED <ref type=""bibr"" target=""#b54"">(Zhang et al., 2017)</ref>.</p><p>Zero-shot image classification consists of learning an image classifier without acces",0
"hens, 1988)</ref> eventually gave rise to nonlinear CNNs with learned convolutional kernels <ref type=""bibr"" target=""#b44"">(Waibel et al., 1989;</ref><ref type=""bibr"" target=""#b25"">LeCun et al., 1989)</ref>. As additional algorithmic complexity tends to complicate theoretical analysis and obfuscates",0
"l., 2012;</ref><ref type=""bibr"" target=""#b55"">Zhang et al., 2018c)</ref>, and computer vision <ref type=""bibr"" target=""#b45"">(Wang et al., 2018;</ref><ref type=""bibr"" target=""#b21"">Kampffmeyer et al., 2018)</ref>.</p><p>Historically, the development of machine learning algo-rithms has followed a cle lickovic et al., 2018;</ref><ref type=""bibr"" target=""#b41"">Thekumparampil et al., 2018;</ref><ref type=""bibr"" target=""#b52"">Zhang et al., 2018a;</ref><ref type=""bibr"" target=""#b21"">Kampffmeyer et al., 2018)</ref>. However, the attention mechanism usually adds significant overhead to computation and",0
"hemistry <ref type=""bibr"" target=""#b29"">(Liao et al., 2019)</ref>, natural language processing <ref type=""bibr"" target=""#b51"">(Yao et al., 2019;</ref><ref type=""bibr"" target=""#b16"">Han et al., 2012;</ref><ref type=""bibr"" target=""#b55"">Zhang et al., 2018c)</ref>, and computer vision <ref type=""bibr""",0
"c regression is a well studied convex optimization problem and can be performed with any efficient second order method or stochastic gradient descent <ref type=""bibr"" target=""#b6"">(Bottou, 2010)</ref>. Provided the graph connectivity pattern is sufficiently sparse, SGD naturally scales to very large",0
"methods. Graph embedding methods <ref type=""bibr"" target=""#b46"">(Weston et al., 2008;</ref><ref type=""bibr"" target=""#b33"">Perozzi et al., 2014;</ref><ref type=""bibr"" target=""#b50"">Yang et al., 2016;</ref><ref type=""bibr"" target=""#b43"">Velikovi et al., 2019)</ref> represent nodes as high-dimensional",0
"earn to assign different edge weights at each layer based on node features and have achieved state-of-the-art results on several graph learning tasks <ref type=""bibr"" target=""#b42"">(Velickovic et al., 2018;</ref><ref type=""bibr"" target=""#b41"">Thekumparampil et al., 2018;</ref><ref type=""bibr"" target and validation accuracy. Baselines. For citation networks, we compare against GCN <ref type=""bibr"" target=""#b23"">(Kipf &amp; Welling, 2017)</ref> GAT <ref type=""bibr"" target=""#b42"">(Velickovic et al., 2018)</ref> FastGCN <ref type=""bibr"" target=""#b10"">(Chen et al., 2018)</ref> LNet, AdaLNet <ref typ",0
"Lafferty, 2001)</ref>. Recent research efforts in neural network models have shown that end-to-end learning like convolutional neural networks (CNNs) <ref type=""bibr"" target=""#b24"">(Ma and Hovy, 2016a)</ref> or bidirectional long short-term memory (BLSTMs) <ref type=""bibr"" target=""#b13"">(Lample et a",1
"ion 4.4.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.1"">The Base Model: BLSTM-CRF</head><p>Many recent sequence labeling frameworks <ref type=""bibr"" target=""#b25"">(Ma and Hovy, 2016b;</ref><ref type=""bibr"" target=""#b27"">Misawa et al., 2017)</ref> share a very basic structure: a bid",1
"0""><head n=""4.1"">The Base Model: BLSTM-CRF</head><p>Many recent sequence labeling frameworks <ref type=""bibr"" target=""#b25"">(Ma and Hovy, 2016b;</ref><ref type=""bibr"" target=""#b27"">Misawa et al., 2017)</ref> share a very basic structure: a bidirectional LSTM network followed by a CRF tagging layer (",1
"knowledge between source and target domains. Adversarial penalty can be added to the loss function to make models learn domain-invariant feature only <ref type=""bibr"" target=""#b7"">(Fernando et al., 2015;</ref><ref type=""bibr"" target=""#b22"">Long et al., 2014;</ref><ref type=""bibr"" target=""#b26"">Ming",0
"ling the quality of crowd-annotation in annotation frameworks such as AlpacaTag <ref type=""bibr"" target=""#b18"">(Lin et al., 2019)</ref> and LEAN-LIFE <ref type=""bibr"" target=""#b15"">(Lee et al., 2020)</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""5"">Experiments</head><p>We evalua",0
"learn domain-invariant feature only <ref type=""bibr"" target=""#b7"">(Fernando et al., 2015;</ref><ref type=""bibr"" target=""#b22"">Long et al., 2014;</ref><ref type=""bibr"" target=""#b26"">Ming Harry Hsu et al., 2015)</ref>. However, it does not exploit domain-specific information.</p></div> <div xmlns=""htt",0
"nalty can be added to the loss function to make models learn domain-invariant feature only <ref type=""bibr"" target=""#b7"">(Fernando et al., 2015;</ref><ref type=""bibr"" target=""#b22"">Long et al., 2014;</ref><ref type=""bibr"" target=""#b26"">Ming Harry Hsu et al., 2015)</ref>. However, it does not exploit",0
"ld be combined in two ways: (1) aggregating labels on crowd-sourced training set then feeding the generated labels to a Sequence Labeling Model (SLM) <ref type=""bibr"" target=""#b21"">(Liu et al., 2017)</ref>; (2) feeding multi-source data to a Multi-Task Learning (MTL) <ref type=""bibr"" target=""#b42"">(",0
"d from high-resource domains (source domains) to boost performance on lowresource domains (target domains) of interests such as social media messages <ref type=""bibr"" target=""#b16"">(Lin et al., 2017)</ref>. Different from supervised adaptation (Lin and Lu, 2018), we assume there is no labels at all",0
"learn the model and annotator behavior on classification.</p><p>Recent research shows the strength of multitask framework in semi-supervised learning <ref type=""bibr"" target=""#b14"">(Lan et al., 2018;</ref><ref type=""bibr"" target=""#b2"">Clark et al., 2018)</ref>, cross-type learning <ref type=""bibr"" t",0
"pervised learning <ref type=""bibr"" target=""#b14"">(Lan et al., 2018;</ref><ref type=""bibr"" target=""#b2"">Clark et al., 2018)</ref>, cross-type learning <ref type=""bibr"" target=""#b42"">(Wang et al., 2018)</ref>, and learning with entity triggers <ref type=""bibr"">(Lin et al., 2020)</ref>. <ref type=""bibr a Sequence Labeling Model (SLM) <ref type=""bibr"" target=""#b21"">(Liu et al., 2017)</ref>; (2) feeding multi-source data to a Multi-Task Learning (MTL) <ref type=""bibr"" target=""#b42"">(Wang et al., 2018)</ref> model then aggregating multiple predicted labels. We investigate multiple label aggregation s",0
"like convolutional neural networks (CNNs) <ref type=""bibr"" target=""#b24"">(Ma and Hovy, 2016a)</ref> or bidirectional long short-term memory (BLSTMs) <ref type=""bibr"" target=""#b13"">(Lample et al., 2016)</ref> can largely eliminate humancrafted features.</p><p>BLSTM-CRF models have achieved promising br"" target=""#b13"">(Lample et al., 2016)</ref> can largely eliminate humancrafted features.</p><p>BLSTM-CRF models have achieved promising performance <ref type=""bibr"" target=""#b13"">(Lample et al., 2016)</ref> and are used as our base sequence tagging model in this paper. Crowd-sourced Annotation. Cr",0
"d adaptation (Lin and Lu, 2018), we assume there is no labels at all for target corpora. <ref type=""bibr"" target=""#b37"">Saito et al. (2017)</ref> and <ref type=""bibr"" target=""#b36"">Ruder and Plank (2018)</ref> explored bootstrapping with multitask tri-training approach, which requires unlabeled data trapping with multi-task Tri-Training approach for unsupervised one-to-one domain adaptation <ref type=""bibr"" target=""#b37"">(Saito et al., 2017;</ref><ref type=""bibr"" target=""#b36"">Ruder and Plank, 2018)</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""5.4"">Learning with Crowd Anno",0
"14)</ref>  domains: academic, bio, fiction, news, voyage, wiki, and interview. For NER task, we select the English portion of the OntoNotes v5 corpus <ref type=""bibr"" target=""#b10"">(Hovy et al., 2006)</ref>. The corpus is annotated with 9 named entities with data from 6 domains: broadcast conversati",0
"l prefetchers explore designs that reduce the traffic and latency costs of accessing an off-chip Markov table <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b42"">43]</ref> GHB-Based Temporal Prefetchers. Wenisch et al. find that tables are not ideal for organizing off-chip metadat",1
"cheme was subsequently revised with a new metadata management scheme that includes a metadata prefetcher, reducing metadata traffic from 482% to 156% <ref type=""bibr"" target=""#b46"">[47]</ref>.</p><p>Unfortunately, the presence of off-chip metadata in these solutions presents three issues. First, eve nication with the OS, and (3) methods for managing the metadata, which can include both a metadata cache replacement policy and a metadata prefetcher <ref type=""bibr"" target=""#b46"">[47]</ref>.</p><p>In this paper, we present a new temporal data prefetcher that addresses these issues by not maintaini e Triage prefetcher using a highly accurate proprietary simulator for single-core simulations and the ChampSim simulator for multi-core simulations.  <ref type=""bibr"" target=""#b46"">[47]</ref>). ? On a 4-core system running CloudSuite server benchmarks, BO+Triage improves performance by 13.7%, compar ffic overheads of 200-400%, and its metadata cache utilization is quite poor due to the absence of spatial locality in the metadata cache.</p><p>MISB <ref type=""bibr"" target=""#b46"">[47]</ref> addresses these issues by divorcing ISB's metadata cache from the TLB. In particular, MISB manages the metad namely, Sampled Temporal Memory Streaming (STMS) <ref type=""bibr"" target=""#b44"">[45]</ref>, Domino <ref type=""bibr"" target=""#b3"">[4]</ref>, and MISB <ref type=""bibr"" target=""#b46"">[47]</ref>. STMS, Domino, and MISB represent the state-of-the-art in temporal prefetching. For simplicity, we model ide ure"" target=""#fig_0"">11</ref> compares Triage against overly optimistic idealized versions of STMS and Domino and against a realistic version of MISB <ref type=""bibr"" target=""#b46"">[47]</ref>. We see that Triage outperforms idealized STMS and Domino (23.5% for Triage vs 14.5% for Domino and 15.3% fo work is motivated by two observations. First, most of the coverage for state-of-the-art temporal prefetchers <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b46"">47]</ref> comes from a small number of metadata entries, so it is possible to get substantial coverage without storing s exploit address correlation by storing metadata in off-chip memory <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b44"">45,</ref><ref type=""bibr"" target=""#b46"">47]</ref>, but the use of off-chip metadata has limited the commercial viability of such prefetchers. Triage is the fir er (ISB) combines address correlation with PC-localization by proposing a new off-chip metadata organization <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b46"">47]</ref>. In particular, ISB maps PClocalized correlated address pairs to consecutive addresses in a new address space ip metadata. In particular, compared to other metadata organizations <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b44"">45,</ref><ref type=""bibr"" target=""#b46"">47]</ref>, our table-based organization avoids metadata redundancy by representing each correlated address pair only on",0
"and run for 50 million instructions, and we generate at most 10 SimPoints for each SPEC benchmark.</p><p>We present multi-core results for CloudSuite <ref type=""bibr"" target=""#b15"">[16]</ref> and multiprogrammed SPEC benchmarks. For CloudSuite, we use the traces provided with the 2 nd Cache Replacem",0
"tely.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>Core</head><p>Out-of-order, 2GHz, For multi-core evaluation of Triage, we use ChampSim <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b28"">29]</ref>, a trace-based simulator that includes an out-of-order core model and",0
"ny metadata that cannot be kept in the metadata store is simply discarded. To identify important metadata, Triage uses the Hawkeye replacement policy <ref type=""bibr"" target=""#b24"">[25]</ref>, which provides significant performance benefits for small metadata stores, as it identifies frequently acce granularity because Triage targets irregular memory accesses, which exhibit poor spatial locality.</p><p>To accomplish these goals, we modify Hawkeye <ref type=""bibr"" target=""#b24"">[25]</ref>, a state-ofthe-art cache replacement policy, which learns from the optimal solution for past memory referenc h for B. If an entry is not found, no prefetch is issued.</p><p>Metadata Replacement Updates. Our metadata replacement is based on the Hawkeye policy <ref type=""bibr"" target=""#b24"">[25]</ref>, and like the Hawkeye policy, our metadata replacement policy is trained on the behavior of a few sampled se isses, to update the per-metadataentry replacement state. For more details on how the Hawkeye policy works, we refer the reader to the original paper <ref type=""bibr"" target=""#b24"">[25]</ref>.</p><p>Metadata Partition Updates. Triage partitions the cache between data and metadata by using way partit",0
"3]</ref>. Functional Warming (FW) on the other hand does not incur any storage overhead and is transferable across both hardware and software changes <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b33"">34]</ref>. Functional warming  warms up the microarchitecture state by simulat ef><ref type=""bibr"" target=""#b32"">33]</ref> is fast, requires huge storage overhead, and does not allow for software changes. Functional warming (FW) <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b33"">34]</ref> does not incur any storage overhead, allows for software changes, bu a snapshot of the architecture state for each region, which is fast but does not allow for changes to the software. Virtualized Fast-Forwarding (VFF) <ref type=""bibr"" target=""#b25"">[26]</ref> leverages hardware virtualization to quickly get to the next region, while enabling software changes. Time T n Ertvelde et al. <ref type=""bibr"" target=""#b30"">[31]</ref> extend on the concept of BLRL using a form of hardware state checkpoints. Sandberg et al. <ref type=""bibr"" target=""#b25"">[26]</ref> propose a method that uses two parallel simulations, pessimistic and optimistic, to bound the maximum error",1
", i.e., evaluate (a) small region(s) of the execution in detail and then extrapolate to the entire execution <ref type=""bibr"" target=""#b27"">[28,</ref><ref type=""bibr"" target=""#b33"">34]</ref>. A major challenge in sampled evaluation however is to quickly and accurately warm up the microarchitecture s other hand does not incur any storage overhead and is transferable across both hardware and software changes <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b33"">34]</ref>. Functional warming  warms up the microarchitecture state by simulating the microarchitecture structures in t f> is fast, requires huge storage overhead, and does not allow for software changes. Functional warming (FW) <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b33"">34]</ref> does not incur any storage overhead, allows for software changes, but is slow. Randomized statistical warming ecture performance evaluation by considering a select number of representative detailed regions that are evaluated in detail to then extrapolate from <ref type=""bibr"" target=""#b33"">[34]</ref>. The key challenge in sampling is to get (i) the correct architecture state and (ii) an accurate microarchit ues to obtain the correct architecture state are functional fast-forwarding, checkpointing and virtualized fastforwarding. Functional fast-forwarding <ref type=""bibr"" target=""#b33"">[34]</ref> leverages functional simulation to get to the next representative region, which is slow. Checkpointing <ref ipeline, predictors, prefetchers, caches) through a detailed warm-up using a small number of instructions (e.g., 30,000) prior to the detailed region <ref type=""bibr"" target=""#b33"">[34]</ref>. With this small amount of warming, only a small part of the cache state is warm, which we refer to as the l e gem5's out-oforder CPU model and run for 10,000 instructions. Prior research shows that the highest accuracy is achieved for small detailed regions <ref type=""bibr"" target=""#b33"">[34]</ref>; larger detailed regions will likely make DeLorean even more accurate since small regions make the penalty f on:</p><p>• SMARTS: Functional Warming (FW) is used to keep the caches warm using functional simulation in-between detailed regions as done in SMARTS <ref type=""bibr"" target=""#b33"">[34]</ref>. • CoolSim: Randomized Statistical Warming (RSW) is employed to collect randomly selected reuse distances at ionally, functional warming warms up microarchitecture state using all memory references between two consecutive detailed regions, which is very slow <ref type=""bibr"" target=""#b33"">[34]</ref>. Various approaches have been proposed to reduce the warm-up length prior to each detailed region. Haskins a",1
"uracy while improving evaluation speed by an order of magnitude compared to the state-of-the-art.</p><p>gem5 and KVM: We implement DeLorean using KVM <ref type=""bibr"" target=""#b15"">[16]</ref> and the gem5 cycle-accurate full-system simulation infrastructure <ref type=""bibr"" target=""#b7"">[8]</ref>. D h sets watchpoints by leveraging the operating system's page protection mechanism, as previously explained in Section 2.3. This is implemented in KVM <ref type=""bibr"" target=""#b15"">[16]</ref>. VDP advances between watchpoints at near-native speed. A watchpoint stops the native execution when there i",0
"hrough Just-in-Time optimization, etc.) nor hardware changes -although there exist solutions to make checkpoints transferable across cache structures <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b31"">32,</ref><ref type=""bibr"" target=""#b32"">33]</ref>. Functional Warming (FW) on th ch called memory hierarchy state (MHS) to minimize checkpoint size, in the context of SimPoint <ref type=""bibr"" target=""#b27"">[28]</ref>. Barr et al. <ref type=""bibr"" target=""#b0"">[1]</ref> propose the Memory Timestamp Record (MTR), a method to record memory patterns, compress and store them for use",0
"oundary Line Reuse Latency (BLRL) which extends the MRRL concept. They apply similar heuristics to find a shorter warm-up period. Van Ertvelde et al. <ref type=""bibr"" target=""#b30"">[31]</ref> extend on the concept of BLRL using a form of hardware state checkpoints. Sandberg et al. <ref type=""bibr"" t",0
"distance. Other works <ref type=""bibr"" target=""#b28"">[29]</ref> have proposed hardware-accelerated stack distance collection. Liu and Mellor-Crummey <ref type=""bibr"" target=""#b16"">[17]</ref> use a technique based on shadow profiling that forks off a redundant copy of an application, instrumented by dow profiling that forks off a redundant copy of an application, instrumented by Pin, to measure the stack distances for a selected set of references <ref type=""bibr"" target=""#b16"">[17]</ref>.</p><p>A major limitation of stack distance analysis is that measuring stack distances is computationally de",0
"gorithms are already available or can be designed.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.2"">Multi-Programming</head><p>StatCC <ref type=""bibr"" target=""#b9"">[10]</ref> is a method in which sparse reuse information, collected separately for each application, can be used to mode successfully demonstrated to model caches with different replacement policies <ref type=""bibr"" target=""#b24"">[25]</ref>, multi-programming workloads <ref type=""bibr"" target=""#b9"">[10]</ref> as well as multithreaded workloads <ref type=""bibr"" target=""#b6"">[7]</ref>. Randomized statistical warming le",0
"monstrating how sparse (approximate) reuse distance distributions can be used to statistically model caches considered caches with random replacement <ref type=""bibr"" target=""#b5"">[6]</ref>. Follow-on work demonstrates similar accuracy for LRU caches <ref type=""bibr"" target=""#b10"">[11]</ref>. More r",0
"ecause it simulates the cache for every memory access within the warm-up interval prior to the detailed region <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b12"">13]</ref>. More recently, Randomized Statistical Warming (RSW) <ref type=""bibr"" target=""#b22"">[23]</ref> takes a random ibr"" target=""#b18"">[19]</ref> use heuristics to find the minimum number of instructions needed to warm a cache of specified size. Haskins and Skadron <ref type=""bibr"" target=""#b12"">[13]</ref> introduce the concept of Memory Reference Reuse Latencies (MRRLs) which is the number of completed instructi",0
"stics, e.g., generate working set curves to visualize cache miss rate as a function of cache size. Cycle-accurate architecture simulation, e.g., gem5 <ref type=""bibr"" target=""#b7"">[8]</ref>, models cycle-by-cycle execution behavior to predict a workload's performance on a particular microprocessor c and KVM: We implement DeLorean using KVM <ref type=""bibr"" target=""#b15"">[16]</ref> and the gem5 cycle-accurate full-system simulation infrastructure <ref type=""bibr"" target=""#b7"">[8]</ref>. De-Lorean switches and exchanges full-system state between KVM and gem5 at the boundaries of each detailed re",0
"/ref> , thus the IPC is degraded. The path composed of these three operations is the critical path of the IQ, and is a critical path of the processor <ref type=""bibr"" target=""#b17"">[21]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2.2"">Circuits</head><p>In this section, we brie pe=""bibr"" target=""#b8"">[12,</ref><ref type=""bibr"" target=""#b26"">30]</ref>.</p><p>The most widely known circuit used as select logic is a tree arbiter <ref type=""bibr"" target=""#b17"">[21]</ref>. This circuit uses a tree structure to connect multiple small arbiter cells. For example, if the small arbit cked and the grants are determined from the highest priority in order from the bottom of the stacked arbiters <ref type=""bibr"" target=""#b8"">[12,</ref><ref type=""bibr"" target=""#b17"">21]</ref>.</p><p>Note that the priority in arbitration is not flexible but is fixed with respect to the IQ position to hat ?rant 0 , . . . , ?rant IW -1 correspond to the instructions in descending order in terms of the priority <ref type=""bibr"" target=""#b8"">[12,</ref><ref type=""bibr"" target=""#b17"">21]</ref>; that is, ?rant 0 is the grant signal of the instruction with the highest priority, ?rant 1 is that of the in",1
"investigated the effect of several select policies of the IQ, including random selection and selection based on the number of dependent instructions <ref type=""bibr"" target=""#b4"">[8]</ref>. Their evaluation results showed that the select policies they investigated delivered almost the same performa",0
"es the issue priority, further degrading performance. CIRC was often assumed in previous studies on IQs (e.g., <ref type=""bibr"" target=""#b2"">[6,</ref><ref type=""bibr"" target=""#b11"">15]</ref>), but is not used in current processors.</p><p>The last type of IQ is the random queue (RAND), where instruct large amounts of power.</p><p>Henry et al. addressed the problem caused by wrap-around (i.e., incorrect issue priority) in CIRC at the circuit level <ref type=""bibr"" target=""#b11"">[15]</ref>. However, in their proposed circuits, the wires traverse vertically within the IQ to transfer information fr",0
"-c.org/ns/1.0""><head n=""5"">RELATED WORK</head><p>The IQ was extensively studied around 2000, and a comprehensive survey was performed by Abella et al <ref type=""bibr"" target=""#b0"">[4]</ref>.</p><p>One processor that implemented SHIFT was the DEC Alpha 21264 <ref type=""bibr"" target=""#b5"">[9]</ref>. A",0
"2,</ref><ref type=""bibr"" target=""#b20"">24]</ref>, which selects the single oldest ready instruction, is used together with RAND in current processors <ref type=""bibr"" target=""#b7"">[11,</ref><ref type=""bibr"" target=""#b18"">22,</ref><ref type=""bibr"" target=""#b22"">26]</ref>. We call this organization AG st of our knowledge in this section. Although all processor vendors do not publish their IQ organizations, AGE is generally used in modern processors <ref type=""bibr"" target=""#b7"">[11,</ref><ref type=""bibr"" target=""#b18"">22,</ref><ref type=""bibr"" target=""#b22"">26]</ref>. </p></div> <div xmlns=""http: nd RAM <ref type=""bibr"" target=""#b9"">[13]</ref>. These are both used in commercial processors. For example, the CAM type is used in the AMD Bulldozer <ref type=""bibr"" target=""#b7"">[11]</ref>, whereas the RAM type is used in the IBM POWER8 <ref type=""bibr"" target=""#b22"">[26]</ref>. We assume the CAM ,</ref><ref type=""bibr"" target=""#b20"">24]</ref> is used. We refer to this type of IQ as AGE. The age matrix is used in parallel with the select logic <ref type=""bibr"" target=""#b7"">[11]</ref>, and selects only the single oldest ready instruction. The circuit of the age matrix is a logic matrix, where",0
"the literature, the definition of this patch size is usually performed using two strategies: (i) empirically <ref type=""bibr"" target=""#b7"">[8]</ref>, <ref type=""bibr"" target=""#b19"">[20]</ref>, by evaluating several sizes and selecting the best one, which is a very expensive process, given that, for multi-context information <ref type=""foot"" target=""#foot_0"">1</ref> . Multi-context paradigm has been proven to be essential for segmentation methods <ref type=""bibr"" target=""#b19"">[20]</ref>, <ref type=""bibr"" target=""#b22"">[23]</ref>, given that it allows the model to extract and capture patterns o r"" target=""#b29"">[30]</ref>- <ref type=""bibr"" target=""#b32"">[33]</ref>. Towards a better understanding of the Earth's surface, a myriad of techniques <ref type=""bibr"" target=""#b19"">[20]</ref>- <ref type=""bibr"" target=""#b21"">[22]</ref>, <ref type=""bibr"" target=""#b23"">[24]</ref>- <ref type=""bibr"" targ y argue that these different scales smooth the final predictions due to the combination of distinct fields of view and spatial context.</p><p>Sherrah <ref type=""bibr"" target=""#b19"">[20]</ref> proposed methods based on fully convolutional networks <ref type=""bibr"" target=""#b16"">[17]</ref>. The first bibr"" target=""#b37"">[38]</ref> and employed in the deep learning context (as an alternative to deconvolution layers) mainly for semantic segmentation <ref type=""bibr"" target=""#b19"">[20]</ref>, <ref type=""bibr"" target=""#b27"">[28]</ref>, <ref type=""bibr"" target=""#b38"">[39]</ref>. In dilated convolutio sumption and expensive training phase. Furthermore, the obtained results, that do not have any post-processing, are better than others, such as DST 2 <ref type=""bibr"" target=""#b19"">[20]</ref>, that employ CRF as  Aside from this, the proposed work (UFMG 5) achieved the best result (82.5% of F1 Score and 12</ref>, respectively.</p><p>The proposed work achieved competitive results, appearing in third place according to the overall accuracy. DST 5 <ref type=""bibr"" target=""#b19"">[20]</ref> and RIT L7 <ref type=""bibr"" target=""#b49"">[50]</ref> are the best result in terms of overall accuracy. Howev",1
"the best patch size for the inference stage. Specifically, this technique is based upon an architecture composed exclusively on dilated convolutions <ref type=""bibr"" target=""#b27"">[28]</ref>, which are capable of processing input patch of varying sizes without distinction, given that they learn the sample the input data (a common process performed in most works <ref type=""bibr"" target=""#b16"">[17]</ref>, <ref type=""bibr"" target=""#b21"">[22]</ref>, <ref type=""bibr"" target=""#b27"">[28]</ref>), (ii) the multi-context strategy is exploited during the training process without any modification of the n in the deep learning context (as an alternative to deconvolution layers) mainly for semantic segmentation <ref type=""bibr"" target=""#b19"">[20]</ref>, <ref type=""bibr"" target=""#b27"">[28]</ref>, <ref type=""bibr"" target=""#b38"">[39]</ref>. In dilated convolutional layers, filter weights are employed dif ss has several advantages, such as: (i) supports the expansion of the receptive field without increasing the number of trainable parameters per layer <ref type=""bibr"" target=""#b27"">[28]</ref>, which reduces the computational burden, and (ii) preserves the feature map resolution, which may help the n div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>B. Architectures</head><p>As presented in Section III, the properties of the dilated convolutions <ref type=""bibr"" target=""#b27"">[28]</ref> make them fit perfectly into the proposed multi-context methodology, given that a network composed of such l uation where it is not possible to create the coarse map and, consequently, the final upsampled map. Such problem is overcome by dilated convolutions <ref type=""bibr"" target=""#b27"">[28]</ref>, which are allowed to process patches of any size, without distinction, always outputting results with the s based on the wellknown SegNet <ref type=""bibr"" target=""#b17"">[18]</ref> architecture) and exploited as a baseline in this work. (iii) dilated network <ref type=""bibr"" target=""#b27"">[28]</ref>, which is, in this case, the Dilated6Pooling (Figure <ref type=""figure"">4c</ref>). All these networks were t between these convolution operations is the possibility to have gaps between the filter weights, a special characteristic of the dilated convolutions <ref type=""bibr"" target=""#b27"">[28]</ref>. Such aspect makes all the difference since dilated convolution can expand the exploited context (by enlargi 6 annotated images were used to train the network. The 5 remaining images (with <ref type=""bibr"">IDs 11,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b29"">30,</ref><ref type=""bibr"" target=""#b33"">34)</ref> were employed to validate and considering the largest range <ref type=""bibr"" target=""#b6"">(7,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b34"">35,</ref><ref type=""bibr"" target=""#b41"">42,</ref><ref type=""bibr"" target=""#b48""",1
"p://www.tei-c.org/ns/1.0""><head>III. DILATED CONVNETS</head><p>Dilated convolutions were originally proposed for the computation of wavelet transform <ref type=""bibr"" target=""#b37"">[38]</ref> and employed in the deep learning context (as an alternative to deconvolution layers) mainly for semantic se",1
"ative to deconvolution layers) mainly for semantic segmentation <ref type=""bibr"" target=""#b19"">[20]</ref>, <ref type=""bibr"" target=""#b27"">[28]</ref>, <ref type=""bibr"" target=""#b38"">[39]</ref>. In dilated convolutional layers, filter weights are employed differently when compared to standard convolut",1
"fifth place by yielding 89.4% of overall accuracy, outperforming several methods, such as ADL 3 <ref type=""bibr"" target=""#b48"">[49]</ref> and RIT L8 <ref type=""bibr"" target=""#b49"">[50]</ref>, that also tried to aggregate multi-context information. However, as can be seen in Table <ref type=""table"" achieved competitive results, appearing in third place according to the overall accuracy. DST 5 <ref type=""bibr"" target=""#b19"">[20]</ref> and RIT L7 <ref type=""bibr"" target=""#b49"">[50]</ref> are the best result in terms of overall accuracy. However, they have a larger number of trainable parameters",0
"</ref>, such as lowlevel <ref type=""bibr"" target=""#b12"">[13]</ref>- <ref type=""bibr"" target=""#b14"">[15]</ref> and mid-level (e.g. Bag of Visual Words <ref type=""bibr"" target=""#b15"">[16]</ref>) descriptors.</p><p>Among all networks, a specific type, called Convolutional (Neural) Networks, ConvNets or",0
"s opened new horizons to the remote sensing community <ref type=""bibr"" target=""#b0"">[1]</ref>, allowing a better understanding of the Earth's surface <ref type=""bibr"" target=""#b1"">[2]</ref>. Towards such understanding, one of the most important task is semantic labeling (or segmentation) <ref type=""",0
"in their architectures using different approaches. Some of them <ref type=""bibr"" target=""#b20"">[21]</ref>, <ref type=""bibr"" target=""#b23"">[24]</ref>, <ref type=""bibr"" target=""#b25"">[26]</ref> train several distinct layers or networks, one for each context, and combine them for the final prediction. ly, in an end-to-end process, the number of parameters is really huge allowing them to use only small values of batch size (10 patches per batch). In <ref type=""bibr"" target=""#b25"">[26]</ref>, the authors proposed a multi-context semantic segmentation by combining ConvNets, hand-crafted descriptors, est patch size, in training time, avoiding lots of experiments to adjust such size (as done in several works <ref type=""bibr"" target=""#b7"">[8]</ref>, <ref type=""bibr"" target=""#b25"">[26]</ref>, <ref type=""bibr"" target=""#b40"">[41]</ref>), in this section, the patch size range is analyzed in order to e",0
"ones can be useful in other scenarios. Therefore, several works <ref type=""bibr"" target=""#b20"">[21]</ref>, <ref type=""bibr"" target=""#b21"">[22]</ref>, <ref type=""bibr"" target=""#b23"">[24]</ref>- <ref type=""bibr"" target=""#b26"">[27]</ref> incorporate the benefits of the multi-context paradigm in their a the benefits of the multi-context paradigm in their architectures using different approaches. Some of them <ref type=""bibr"" target=""#b20"">[21]</ref>, <ref type=""bibr"" target=""#b23"">[24]</ref>, <ref type=""bibr"" target=""#b25"">[26]</ref> train several distinct layers or networks, one for each context, er understanding of the Earth's surface, a myriad of techniques <ref type=""bibr"" target=""#b19"">[20]</ref>- <ref type=""bibr"" target=""#b21"">[22]</ref>, <ref type=""bibr"" target=""#b23"">[24]</ref>- <ref type=""bibr"" target=""#b26"">[27]</ref> have been proposed to perform semantic segmentation in remote sen several input patch sizes with final architectures processing patches with 128 × 128 or 256 × 256 pixels depending on the dataset.</p><p>Marcu et al. <ref type=""bibr"" target=""#b23"">[24]</ref> combined the outputs of a dual-stream network in order to aggregate multi-context information for semantic s",0
"target=""#foot_0"">1</ref> . Multi-context paradigm has been proven to be essential for segmentation methods <ref type=""bibr"" target=""#b19"">[20]</ref>, <ref type=""bibr"" target=""#b22"">[23]</ref>, given that it allows the model to extract and capture patterns of varying granularities, helping the method ixels with green, red, and near-infrared bands (in this order), which are the most useful and representative ones for discriminating vegetation areas <ref type=""bibr"" target=""#b22"">[23]</ref>. More specifically, the dataset consists of 1,250,000 pixels classified into two classes: coffee (637,544 pi",0
"nother image (usually with the same resolution of the input) that has each pixel associated to a semantic class. Based on this idea, several networks <ref type=""bibr"" target=""#b16"">[17]</ref>- <ref type=""bibr"" target=""#b18"">[19]</ref> achieved stateof-the-art for the labeling task in the computer vi </head><p>As introduced, deep learning has made its way into the remote sensing community, mainly due to its success in several computer vision tasks <ref type=""bibr"" target=""#b16"">[17]</ref>, <ref type=""bibr"" target=""#b29"">[30]</ref>- <ref type=""bibr"" target=""#b32"">[33]</ref>. Towards a better unde ype=""bibr"" target=""#b26"">[27]</ref> have been proposed to perform semantic segmentation in remote sensing images. Based on previous successful models <ref type=""bibr"" target=""#b16"">[17]</ref>, <ref type=""bibr"" target=""#b33"">[34]</ref>, several of the proposed works exploit the benefits of the multi- ct fields of view and spatial context.</p><p>Sherrah <ref type=""bibr"" target=""#b19"">[20]</ref> proposed methods based on fully convolutional networks <ref type=""bibr"" target=""#b16"">[17]</ref>. The first architecture was purely based on the fully convolutional paradigm, i.e., the network has several predictions, which are aggregated, in a subsequent fusion stage, generating the final label. They also experimented fully convolutional architectures <ref type=""bibr"" target=""#b16"">[17]</ref> (with several skip layers in order to aggregate multi-context information) and an ensemble of several archit thod that performs labeling segmentation based on upsampled and concatenated features extracted from distinct layers of a fully convolutional network <ref type=""bibr"" target=""#b16"">[17]</ref>. Specifically, the network, that receives as input patches of 256 × 256 or 512 × 512 pixels (depending on th : (i) the proposed technique exploits a fully convolutional network that does not downsample the input data (a common process performed in most works <ref type=""bibr"" target=""#b16"">[17]</ref>, <ref type=""bibr"" target=""#b21"">[22]</ref>, <ref type=""bibr"" target=""#b27"">[28]</ref>), (ii) the multi-conte .</p><p>Though, in this work, we explore networks composed of dilated convolutions, other types of ConvNets could be used, such as fully convolutions <ref type=""bibr"" target=""#b16"">[17]</ref> and deconvolutions <ref type=""bibr"" target=""#b17"">[18]</ref>, <ref type=""bibr"" target=""#b18"">[19]</ref>. The pixel is classified independently by the classifier. Also, for these two datasets, we considered as baselines: (i) Fully Convolutional Networks (FCN) <ref type=""bibr"" target=""#b16"">[17]</ref>. In this case, the pixelwise architectures proposed by <ref type=""bibr"" target=""#b40"">[41]</ref> were conver chieved state-of-the-art results in two datasets (Coffee and GRSS Data Fusion datasets) outperforming several techniques (such as Fully Convolutional <ref type=""bibr"" target=""#b16"">[17]</ref> and deconvolutional networks <ref type=""bibr"" target=""#b17"">[18]</ref>) that also exploit the multicontext p",0
"compared to previous state-of-the-art methods <ref type=""bibr"" target=""#b11"">[12]</ref>, such as lowlevel <ref type=""bibr"" target=""#b12"">[13]</ref>- <ref type=""bibr"" target=""#b14"">[15]</ref> and mid-level (e.g. Bag of Visual Words <ref type=""bibr"" target=""#b15"">[16]</ref>) descriptors.</p><p>Among plications to benefit from the patterns extracted by our models, a very interesting feature specially when working with small amounts of labeled data <ref type=""bibr"" target=""#b14"">[15]</ref>.</p><p>In practice, these are the contributions of this work:</p><p>• Our main contribution is a novel appro convolutions, allowing the network to maintain the full resolution of the image. Finally, the last strategy evaluated by the authors was to fine-tune <ref type=""bibr"" target=""#b14"">[15]</ref> pretrained networks over the remote sensing datasets. None of the aforementioned strategies exploit the bene er applications to benefit from the patterns extracted by our models, a very important process mainly when working with small amounts of labeled data <ref type=""bibr"" target=""#b14"">[15]</ref>.</p><note type=""other"">Image nDSM Ground-Truth Dilated6 DenseDilated6 Dilated6 Pooling Dilated8 Pooling Dila r the Vaihingen dataset, 11 out of the 16 annotated images were used to train the network. The 5 remaining images (with <ref type=""bibr"">IDs 11,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b29"">30,</ref><ref type=""bibr"" target=""#b33""",0
"ion of 0.9 meter. A Digital Surface Model (DSM) coregistered to the image data was also provided, allowing the creation of a normalized DSM (nDSM) by <ref type=""bibr"" target=""#b43"">[44]</ref>. In this work, we use the spectral information (NIR-R-G) and the nDSM, i.e., the input data for the method h",0
"set <ref type=""bibr"" target=""#b41"">[42]</ref>, the best result was obtained when considering the largest range <ref type=""bibr"" target=""#b6"">(7,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b34""",0
"technique. The remaining 6 images (with IDs 02 12, 03 12, 04 12, 05 12, 06 12, 07 12) were employed for validation of the method.</p><p>Four metrics <ref type=""bibr"" target=""#b46"">[47]</ref> were considered to assess the performance of the proposed algorithm: overall and average accuracy, kappa ind n terms of the ground-truths and the network predictions.</p><p>In the second case, the score is represented by the pixelwise classification accuracy <ref type=""bibr"" target=""#b46"">[47]</ref> of the images.</p><p>To analyze the most appropriate score function, experiments were performed varying only",0
"yers are, then, upsampled, concatenated and finally classified by another convolutional layer. This proposed strategy resembles somehow the DenseNets <ref type=""bibr"" target=""#b35"">[36]</ref>, with the final layer having connections to the previous ones. Wang et al. <ref type=""bibr"" target=""#b26"">[2 feature extraction, it will be referenced as Dilated6. The second network (Figure <ref type=""figure"">4b</ref>) is based on densely connected networks <ref type=""bibr"" target=""#b35"">[36]</ref>, which recently achieved outstanding results on the image classification task. This network is very similar",0
"tation of remote sensing images. The current state-of-the-art method for semantic segmentation is based on a resurgent approach, called deep learning <ref type=""bibr"" target=""#b10"">[11]</ref>, that can learn specific and adaptable spatial features directly from the images. Specifically, deep learnin ype=""bibr"" target=""#b15"">[16]</ref>) descriptors.</p><p>Among all networks, a specific type, called Convolutional (Neural) Networks, ConvNets or CNNs <ref type=""bibr"" target=""#b10"">[11]</ref>, is the most traditional one for learning visual features in computer vision applications, as well as remote f the approach, to aggregate multi-context information, more parameters are included in the final model, resulting in a more complex learning process <ref type=""bibr"" target=""#b10"">[11]</ref>.</p><p>In this work, we propose a novel technique to perform semantic segmentation of remote sensing images phasize that this training process (performed by the sampled batch) represents only a single step (iteration) of the mini-batch optimization strategy <ref type=""bibr"" target=""#b10"">[11]</ref> (and not the full train) that processes one whole batch to then update the network weights W. As aforementio unctions that could be employed in this step: the loss and the accuracy.</p><p>In the first case, the loss is a measure (obtained using cross entropy <ref type=""bibr"" target=""#b10"">[11]</ref>, in this case) that represents the error generated in terms of the ground-truths and the network predictions sed algorithm, several experiments were conducted comparing the same network trained using two distinct methods: (i) the traditional training process <ref type=""bibr"" target=""#b10"">[11]</ref>, in which the network is trained using patches of constant size, without any variation. This method is the s",0
"type=""bibr"" target=""#b4"">[5]</ref>, disaster relief <ref type=""bibr"" target=""#b5"">[6]</ref>, <ref type=""bibr"" target=""#b6"">[7]</ref>, urban planning <ref type=""bibr"" target=""#b7"">[8]</ref>. K. Nogueira, W. R. Schwartz, and J. A. dos Santos are with the Department of Computer Science, Universidade F ffect the performance of the ConvNet. In the literature, the definition of this patch size is usually performed using two strategies: (i) empirically <ref type=""bibr"" target=""#b7"">[8]</ref>, <ref type=""bibr"" target=""#b19"">[20]</ref>, by evaluating several sizes and selecting the best one, which is a hingen <ref type=""bibr"" target=""#b42"">[43]</ref> and Potsdam <ref type=""bibr"" target=""#b44"">[45]</ref> datasets, we followed the protocol proposed by <ref type=""bibr"" target=""#b7"">[8]</ref>. For the Vaihingen dataset, 11 out of the 16 annotated images were used to train the network. The 5 remaining is proposed to select automatically the best patch size, in training time, avoiding lots of experiments to adjust such size (as done in several works <ref type=""bibr"" target=""#b7"">[8]</ref>, <ref type=""bibr"" target=""#b25"">[26]</ref>, <ref type=""bibr"" target=""#b40"">[41]</ref>), in this section, the p ver the latter dataset are also applicable to the Potsdam one. Furthermore, in order to evaluate such dataset, a validation set, created according to <ref type=""bibr"" target=""#b7"">[8]</ref>, was employed. Experiments were conducted varying only the patch size range but maintaining the remaining conf type=""table"" target=""#tab_4"">VI</ref> presents the obtained results. Each dataset was evaluated over several ranges, selected based on previous works <ref type=""bibr"" target=""#b7"">[8]</ref>, <ref type=""bibr"" target=""#b40"">[41]</ref>. Specifically, each dataset was evaluated in a large range (compris tion VI-G. TABLE VI: Results of the proposed approach when varying the input range of patch sizes. For Vaihingen, a validation set (created according <ref type=""bibr"" target=""#b7"">[8]</ref>) is employed. Bold patch size ranges were selected for all further experiments. </p></div> <div xmlns=""http:// ingen and Potsdam datasets are very similar and analysis performed over one can also be extended to the other. A validation set, created according to <ref type=""bibr"" target=""#b7"">[8]</ref>, was used to evaluate the Vaihingen dataset. Experiments were executed preserving all configurations and varyi atch size for testing. For the Coffee dataset, only the fold 1 is reported. For Vaihingen and Potsdam datasets, the validation set (created according <ref type=""bibr"" target=""#b7"">[8]</ref>) is reported.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>F. Performance Analysis</head><p>To ana imilar to Potsdam one and, therefore, allows the conclusions to be applied to this one. To evaluate this dataset, a validation set, created according <ref type=""bibr"" target=""#b7"">[8]</ref>, was employed. Specifically, in these experiments, Dilated6 network (Figure <ref type=""figure"">4a</ref>) is tr twork for all datasets. For the Coffee dataset, only the fold 1 is reported. For Vaihingen and Potsdam datasets, the validation set (created according<ref type=""bibr"" target=""#b7"">[8]</ref>) is reported.</figDesc></figure> <figure xmlns=""http://www.tei-c.org/ns/1.0"" xml:id=""fig_5""><head>Fig. 6 :</he",0
"fectiveness of the proposed method.</p><p>The proposed method and network <ref type=""foot"" target=""#foot_1"">4</ref> were implemented using TensorFlow <ref type=""bibr"" target=""#b47"">[48]</ref>, a framework conceived to allow efficient exploitation of deep learning with Graphics Processing Units (GPUs ep influences the current value of the network weights, starts with a high value and is reduced during the training phase using the exponential decay <ref type=""bibr"" target=""#b47"">[48]</ref> with parameters defined according to the last column of Table <ref type=""table"" target=""#tab_2"">III</ref>.</",0
", a new network must be trained (without any guarantee for the best patch configuration), and (ii) imposed <ref type=""bibr"" target=""#b20"">[21]</ref>, <ref type=""bibr"" target=""#b21"">[22]</ref>, in which the patch size is defined by network constraints (i.e., changing the patch size implies modifying preferable in some situations while larger ones can be useful in other scenarios. Therefore, several works <ref type=""bibr"" target=""#b20"">[21]</ref>, <ref type=""bibr"" target=""#b21"">[22]</ref>, <ref type=""bibr"" target=""#b23"">[24]</ref>- <ref type=""bibr"" target=""#b26"">[27]</ref> incorporate the benefi pe=""bibr"" target=""#b25"">[26]</ref> train several distinct layers or networks, one for each context, and combine them for the final prediction. Others <ref type=""bibr"" target=""#b21"">[22]</ref>, <ref type=""bibr"" target=""#b24"">[25]</ref>, <ref type=""bibr"" target=""#b26"">[27]</ref> extract and merge feat r"" target=""#b32"">[33]</ref>. Towards a better understanding of the Earth's surface, a myriad of techniques <ref type=""bibr"" target=""#b19"">[20]</ref>- <ref type=""bibr"" target=""#b21"">[22]</ref>, <ref type=""bibr"" target=""#b23"">[24]</ref>- <ref type=""bibr"" target=""#b26"">[27]</ref> have been proposed to nal Random Fields <ref type=""bibr"" target=""#b34"">[35]</ref> are used as a post-processing method in an attempt to improve the final results.</p><p>In <ref type=""bibr"" target=""#b21"">[22]</ref>, the authors proposed multi-context methods that combine boundary detection with deconvolution networks (spe ly convolutional network that does not downsample the input data (a common process performed in most works <ref type=""bibr"" target=""#b16"">[17]</ref>, <ref type=""bibr"" target=""#b21"">[22]</ref>, <ref type=""bibr"" target=""#b27"">[28]</ref>), (ii) the multi-context strategy is exploited during the trainin is possible to notice that the proposed work yielded competitive results. The best result, in terms of overall accuracy, was 90.3% achieved by DLR 9 <ref type=""bibr"" target=""#b21"">[22]</ref> and GSN3 <ref type=""bibr"" target=""#b26"">[27]</ref>. Our best result (UFMG 4) appears in fifth place by yield sing limited architectures (in terms of parameters). In fact, the number of parameters of the network is so relevant that authors of DLR 9 submission <ref type=""bibr"" target=""#b21"">[22]</ref>, one of the best results but with a higher number of parameters, do not recommend their proposed method for",0
", several trials have embeded attention processing to improve the performance of CNNs for various tasks, such as image and video classification tasks <ref type=""bibr"" target=""#b9"">[9,</ref><ref type=""bibr"" target=""#b33"">33]</ref>. Wang et al. <ref type=""bibr"" target=""#b33"">[ 33]</ref> proposed non-l ile our SAN is built on the basis of residual blocks. The second one is the way of enhancing discriminative ability of the network. Channel attention <ref type=""bibr"" target=""#b9"">[9,</ref><ref type=""bibr"" target=""#b38"">38]</ref> has been shown to be effective for better discriminative representatio /ref> proposed non-local neural network to incorporate non-local operations for spatial attention in video classification. On the contrary, Hu et al. <ref type=""bibr"" target=""#b9"">[ 9]</ref> proposed SENet to exploit channel-wise relationships to achieve significant performance gain for image classi Channel Attention (SOCA)</head><p>Most previous CNN-based SR models do not consider the feature interdependencies. To utilize such information, SENet <ref type=""bibr"" target=""#b9"">[9]</ref> was introduced in CNNs to rescale the channelwise features for image SR. However, SENet only exploits first-or >To fully exploit feature interdependencies from the aggregated information by global covariance pooling, we apply a gating mechanism. As explored in <ref type=""bibr"" target=""#b9"">[9]</ref>, the simple sigmoid function can serve as a proper gating function</p><formula xml:id=""formula_17"">w = f (W U",1
"f type=""bibr"" target=""#b14"">[14]</ref> (d) SRMD <ref type=""bibr"" target=""#b36"">[36]</ref> (e) EDSR <ref type=""bibr"" target=""#b36"">[36]</ref> (f) DBPN <ref type=""bibr"" target=""#b20"">[20]</ref> (g) RDN <ref type=""bibr"" target=""#b6"">[6]</ref> (h) Ours</p><p>Figure <ref type=""figure"">1</ref>. Zoom visua nd DRCN <ref type=""bibr"" target=""#b13"">[13]</ref> with more than 16 layers based on residual learning. To further improve the performance, Lim et al. <ref type=""bibr"" target=""#b20"">[20]</ref> proposed a very deep and wide network EDSR by stacking modified residual blocks. The significant performance <ref type=""bibr"" target=""#b12"">[12]</ref>, LapSRN <ref type=""bibr"" target=""#b14"">[14]</ref>, Mem-Net <ref type=""bibr"" target=""#b30"">[30]</ref>, EDSR <ref type=""bibr"" target=""#b20"">[20]</ref>, SRMD <ref type=""bibr"" target=""#b36"">[36]</ref>, NLRN <ref type=""bibr"" target=""#b22"">[22]</ref>, DBPN <ref t oup (NLRG) based deep feature extraction, upscale module, and reconstruction part. Given I LR and I SR as the input and output of SAN. As explored in <ref type=""bibr"" target=""#b20"">[20,</ref><ref type=""bibr"" target=""#b39"">39]</ref>, we apply only one convolutional layer to extract the shallow featur tention (SOCA) module to exploit feature interdependencies.</p><p>It has been verified that stacking residual blocks is helpful to form a deep CNN in <ref type=""bibr"" target=""#b20"">[20,</ref><ref type=""bibr"" target=""#b39"">39]</ref>. However, very deep network built in such way would suffer from trai ution filter outside SOCA, the size and number of filter are set as 3 × 3 and C =6 4 , respectively. For upscale part H ↑ (•), we follow the works in <ref type=""bibr"" target=""#b20"">[20,</ref><ref type=""bibr"" target=""#b39"">39]</ref> and apply ESPCNN <ref type=""bibr"" target=""#b28"">[28]</ref> to upscal =""http://www.tei-c.org/ns/1.0""><head n=""4."">Experiments</head></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.1."">Setup</head><p>Following <ref type=""bibr"" target=""#b20"">[20,</ref><ref type=""bibr"" target=""#b6"">6,</ref><ref type=""bibr"" target=""#b39"">39,</ref><ref type=""bibr"" target=""#b38""> DBPN <ref type=""bibr"" target=""#b6"">[6]</ref>, RDN <ref type=""bibr"" target=""#b39"">[39]</ref> and RCAN <ref type=""bibr"" target=""#b38"">[38]</ref>. As in <ref type=""bibr"" target=""#b20"">[20,</ref><ref type=""bibr"" target=""#b39"">39,</ref><ref type=""bibr"" target=""#b38"">38]</ref>, we also adopt self-ensemble target=""#b29"">29,</ref><ref type=""bibr"" target=""#b30"">30]</ref>, L 1 <ref type=""bibr"" target=""#b14"">[14,</ref><ref type=""bibr"" target=""#b15"">15,</ref><ref type=""bibr"" target=""#b20"">20,</ref><ref type=""bibr"" target=""#b39"">39]</ref>, perceptual losses <ref type=""bibr"" target=""#b11"">[11,</ref><ref type",1
"""bibr"" target=""#b12"">12,</ref><ref type=""bibr"" target=""#b14"">14,</ref><ref type=""bibr"" target=""#b36"">36,</ref><ref type=""bibr"" target=""#b39"">39,</ref><ref type=""bibr"" target=""#b38"">38]</ref>. Although considerable progress has been achieved in image SR, existing CNNbased SR models are still faced wi ""bibr"" target=""#b29"">29,</ref><ref type=""bibr"" target=""#b17"">17,</ref><ref type=""bibr"" target=""#b30"">30,</ref><ref type=""bibr"" target=""#b39"">39,</ref><ref type=""bibr"" target=""#b38"">38]</ref>. Due to space limitation, we here briefly review works related to CNN-based SR methods and attention mechanis e=""bibr"" target=""#b12"">12,</ref><ref type=""bibr"" target=""#b13"">13,</ref><ref type=""bibr"" target=""#b6"">6,</ref><ref type=""bibr"" target=""#b39"">39,</ref><ref type=""bibr"" target=""#b38"">38]</ref>. For example, Dong et al. <ref type=""bibr"" target=""#b2"">[ 2]</ref> first introduced a shallow three-layer con sidual blocks. The second one is the way of enhancing discriminative ability of the network. Channel attention <ref type=""bibr"" target=""#b9"">[9,</ref><ref type=""bibr"" target=""#b38"">38]</ref> has been shown to be effective for better discriminative representations. However, RDN does not consider such 4.1."">Setup</head><p>Following <ref type=""bibr"" target=""#b20"">[20,</ref><ref type=""bibr"" target=""#b6"">6,</ref><ref type=""bibr"" target=""#b39"">39,</ref><ref type=""bibr"" target=""#b38"">38]</ref>, we use 800 high-resolution images from DIV2K dataset <ref type=""bibr"" target=""#b31"">[31]</ref> as training s [39]</ref> and RCAN <ref type=""bibr"" target=""#b38"">[38]</ref>. As in <ref type=""bibr"" target=""#b20"">[20,</ref><ref type=""bibr"" target=""#b39"">39,</ref><ref type=""bibr"" target=""#b38"">38]</ref>, we also adopt self-ensemble method to further improve our SAN denoted as SAN+. All the quantitative results In addition to focusing on increasing the depth of the network, some other networks, such as NLRN <ref type=""bibr"" target=""#b22"">[22]</ref> and RCAN <ref type=""bibr"" target=""#b38"">[38]</ref>, improve the performance by considering feature correlations in spatial or channel dimension. Attention mech to achieve significant performance gain for image classification.</p><p>Recently, SENet was introduced to deep CNNs to further improve SR performance <ref type=""bibr"" target=""#b38"">[38]</ref>. However, SENet only explores first-order statistics (e.g., global average pooling), while ignoring the stat n (SOCA) mechanism to effectively learn channel-wise feature interdependencies. Difference to Residual Channel Attention Network (RCAN). Zhang et al. <ref type=""bibr"" target=""#b38"">[38]</ref> proposed a residual in residual structure to form a very deep network. RCAN is close to our SAN, and the mai ns the convolution layers with 20 LSRAGs and 10 residual blocks in each LSRAG, thus resulting in deep network with over 400 convolution layers. As in <ref type=""bibr"" target=""#b38"">[38]</ref>, we also add long and short skip connections in Base model. From Table 1 we can see that Base reaches PSNR=3 NLRN <ref type=""bibr"" target=""#b22"">[22]</ref>, DBPN <ref type=""bibr"" target=""#b6"">[6]</ref>, RDN <ref type=""bibr"" target=""#b39"">[39]</ref> and RCAN <ref type=""bibr"" target=""#b38"">[38]</ref>. As in <ref type=""bibr"" target=""#b20"">[20,</ref><ref type=""bibr"" target=""#b39"">39,</ref><ref type=""bibr"" tar NN <ref type=""bibr"" target=""#b35"">[35]</ref>, SRMD <ref type=""bibr"" target=""#b36"">[36]</ref>, RDN <ref type=""bibr"" target=""#b39"">[39]</ref>, and RCAN <ref type=""bibr"" target=""#b38"">[38]</ref>. All the results on 3× are shown in Table <ref type=""table"" target=""#tab_2"">3</ref>, from which we can obser",1
"target=""#b12"">12,</ref><ref type=""bibr"" target=""#b29"">29,</ref><ref type=""bibr"" target=""#b30"">30]</ref>, L 1 <ref type=""bibr"" target=""#b14"">[14,</ref><ref type=""bibr"" target=""#b15"">15,</ref><ref type=""bibr"" target=""#b20"">20,</ref><ref type=""bibr"" target=""#b39"">39]</ref>, perceptual losses <ref type= r from training difficulty and performance bottleneck due to the problem of gradient vanishing and exploding in deep network. Inspired by the work in <ref type=""bibr"" target=""#b15"">[15]</ref>, we propose local-source residual attention group (LSRAG) as the fundamental unit. It is known that simply s",0
"/p><p>Deep convolution neural networks (CNNs) have recently achieved unprecedented success in various problems <ref type=""bibr"" target=""#b7"">[7,</ref><ref type=""bibr"" target=""#b25"">25]</ref>. The powerful feature representation and end-to-end training paradigm of CNN makes it a promising approach to",0
". The learning rate is initialized as 10 −4 and then reduced to half every 200 epochs. Our proposed SAN has been implemented on the Pytorch framework <ref type=""bibr"" target=""#b23"">[23]</ref> on an Nvidia 1080Ti GPU.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.2."">Ablation Study</h",0
"rchical features from all the convolutional layers. In addition to focusing on increasing the depth of the network, some other networks, such as NLRN <ref type=""bibr"" target=""#b22"">[22]</ref> and RCAN <ref type=""bibr"" target=""#b38"">[38]</ref>, improve the performance by considering feature correlati 2) it is empirically shown that non-local operations at a proper neighborhood size are preferable for low-level tasks (e.g., image super-resolution) <ref type=""bibr"" target=""#b22"">[22]</ref>. Thus for feature with higher spatial resolution or degradation, it is natural to perform region-level non-l mages (RGB channels).</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.6."">Discussions</head><p>Difference to Non-local RNN (NLRN). NLRN <ref type=""bibr"" target=""#b22"">[22]</ref> introduces non-local operations to capture long-distance spatial contextual information in image restoration m-Net <ref type=""bibr"" target=""#b30"">[30]</ref>, EDSR <ref type=""bibr"" target=""#b20"">[20]</ref>, SRMD <ref type=""bibr"" target=""#b36"">[36]</ref>, NLRN <ref type=""bibr"" target=""#b22"">[22]</ref>, DBPN <ref type=""bibr"" target=""#b6"">[6]</ref>, RDN <ref type=""bibr"" target=""#b39"">[39]</ref> and RCAN <ref t",0
"n mechanism. Attention in human perception generally means that human visual systems adaptively process visual information and focus on salient areas <ref type=""bibr"" target=""#b16"">[16]</ref>. In recent years, several trials have embeded attention processing to improve the performance of CNNs for va",0
"""bibr"" target=""#b14"">14,</ref><ref type=""bibr"" target=""#b13"">13,</ref><ref type=""bibr"" target=""#b29"">29,</ref><ref type=""bibr"" target=""#b17"">17,</ref><ref type=""bibr"" target=""#b30"">30,</ref><ref type=""bibr"" target=""#b39"">39,</ref><ref type=""bibr"" target=""#b38"">38]</ref>. Due to space limitation, we residual blocks. The significant performance gain indicates the depth of representation plays a key role in image SR. Other recent works like MemNet <ref type=""bibr"" target=""#b30"">[30]</ref> and RDN <ref type=""bibr"" target=""#b39"">[39]</ref>, are based on dense blocks <ref type=""bibr"" target=""#b10""> NN <ref type=""bibr"" target=""#b3"">[3]</ref>, VDSR <ref type=""bibr"" target=""#b12"">[12]</ref>, LapSRN <ref type=""bibr"" target=""#b14"">[14]</ref>, Mem-Net <ref type=""bibr"" target=""#b30"">[30]</ref>, EDSR <ref type=""bibr"" target=""#b20"">[20]</ref>, SRMD <ref type=""bibr"" target=""#b36"">[36]</ref>, NLRN <ref t been widely used, such as L 2 <ref type=""bibr"" target=""#b2"">[2,</ref><ref type=""bibr"" target=""#b12"">12,</ref><ref type=""bibr"" target=""#b29"">29,</ref><ref type=""bibr"" target=""#b30"">30]</ref>, L 1 <ref type=""bibr"" target=""#b14"">[14,</ref><ref type=""bibr"" target=""#b15"">15,</ref><ref type=""bibr"" target",0
"[14,</ref><ref type=""bibr"" target=""#b15"">15,</ref><ref type=""bibr"" target=""#b20"">20,</ref><ref type=""bibr"" target=""#b39"">39]</ref>, perceptual losses <ref type=""bibr"" target=""#b11"">[11,</ref><ref type=""bibr"" target=""#b26"">26]</ref>. To verify the effectiveness of our SAN, we adopt the same loss func =""bibr"" target=""#b18"">[18]</ref>, we utilize Newton-Schulz iteration to speed up the computation of covariance normalization. Specifically, from Equ. <ref type=""bibr"" target=""#b11"">(11)</ref>, the Σ has square root as Σ 1/2 = Y = Udiag(λ 1/2 i )U T .G i v e nY 0 = Σ, Z 0 = I, for n =1 , ••• ,N, as s",0
"=""bibr"" target=""#b37"">[37]</ref>, model-based <ref type=""bibr"" target=""#b34"">[34]</ref>, and CNN-based methods <ref type=""bibr"" target=""#b2"">[2,</ref><ref type=""bibr"" target=""#b29"">29,</ref><ref type=""bibr"" target=""#b14"">14,</ref><ref type=""bibr"" target=""#b13"">13,</ref><ref type=""bibr"" target=""#b29"" =""bibr"" target=""#b2"">[2,</ref><ref type=""bibr"" target=""#b29"">29,</ref><ref type=""bibr"" target=""#b14"">14,</ref><ref type=""bibr"" target=""#b13"">13,</ref><ref type=""bibr"" target=""#b29"">29,</ref><ref type=""bibr"" target=""#b17"">17,</ref><ref type=""bibr"" target=""#b30"">30,</ref><ref type=""bibr"" target=""#b39"" loss function. Some loss functions have been widely used, such as L 2 <ref type=""bibr"" target=""#b2"">[2,</ref><ref type=""bibr"" target=""#b12"">12,</ref><ref type=""bibr"" target=""#b29"">29,</ref><ref type=""bibr"" target=""#b30"">30]</ref>, L 1 <ref type=""bibr"" target=""#b14"">[14,</ref><ref type=""bibr"" target",0
"nce multiple HR solutions can map to any LR input. Therefore, a great number of SR methods have been proposed, ranging from early interpolation-based <ref type=""bibr"" target=""#b37"">[37]</ref> and model-based <ref type=""bibr"" target=""#b4"">[4]</ref>, to recent learning-based methods <ref type=""bibr"" t rk</head><p>During the past decade, a plenty of image SISR methods have been proposed in the computer vision community, including interpolation-based <ref type=""bibr"" target=""#b37"">[37]</ref>, model-based <ref type=""bibr"" target=""#b34"">[34]</ref>, and CNN-based methods <ref type=""bibr"" target=""#b2"">",0
"ef type=""bibr"" target=""#b36"">[36]</ref> (e) EDSR <ref type=""bibr"" target=""#b36"">[36]</ref> (f) DBPN <ref type=""bibr"" target=""#b20"">[20]</ref> (g) RDN <ref type=""bibr"" target=""#b6"">[6]</ref> (h) Ours</p><p>Figure <ref type=""figure"">1</ref>. Zoom visual results for 4× SR on ""img 092"" from Urban100. Ou EDSR <ref type=""bibr"" target=""#b20"">[20]</ref>, SRMD <ref type=""bibr"" target=""#b36"">[36]</ref>, NLRN <ref type=""bibr"" target=""#b22"">[22]</ref>, DBPN <ref type=""bibr"" target=""#b6"">[6]</ref>, RDN <ref type=""bibr"" target=""#b39"">[39]</ref> and RCAN <ref type=""bibr"" target=""#b38"">[38]</ref>. As in <ref per or wider network structure <ref type=""bibr"" target=""#b2"">[2,</ref><ref type=""bibr"" target=""#b12"">12,</ref><ref type=""bibr"" target=""#b13"">13,</ref><ref type=""bibr"" target=""#b6"">6,</ref><ref type=""bibr"" target=""#b39"">39,</ref><ref type=""bibr"" target=""#b38"">38]</ref>. For example, Dong et al. <ref between computational burden and performance, and thus is preferable to be used in recent CNN-based SR models <ref type=""bibr"" target=""#b3"">[3,</ref><ref type=""bibr"" target=""#b6"">6,</ref><ref type=""bibr"" target=""#b39"">39]</ref>. The upscaled feature is then mapped into SR image via one convolution ."">Experiments</head></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.1."">Setup</head><p>Following <ref type=""bibr"" target=""#b20"">[20,</ref><ref type=""bibr"" target=""#b6"">6,</ref><ref type=""bibr"" target=""#b39"">39,</ref><ref type=""bibr"" target=""#b38"">38]</ref>, we use 800 high-resolution ima",0
"Other recent works like MemNet <ref type=""bibr"" target=""#b30"">[30]</ref> and RDN <ref type=""bibr"" target=""#b39"">[39]</ref>, are based on dense blocks <ref type=""bibr"" target=""#b10"">[10]</ref> to form deep networks and focus on utilizing all the hierarchical features from all the convolutional layers",0
"ata with adversarially crafted examples generated as the training progresses <ref type=""bibr"" target=""#b7"">[8]</ref>. Madry, Makelov, Schmidt, et al. <ref type=""bibr"" target=""#b24"">[25]</ref> used adversarial training on the cifar dataset, which still has the best empirical robustness to attack <ref d toward the L ∞ metric and is currently the strongest known attack for this metric. PGD has been conjectured to be a near-optimal first-order attack <ref type=""bibr"" target=""#b24"">[25]</ref>. We use the Fo olBox library for the implementation of these attacks <ref type=""bibr"" target=""#b34"">[35]</re f our adversary.</p><p>As part of our evaluation, we also wish to address ac o n c e r nr a i s e db yM a d r y ,M a k e l o v ,S c h m i d t ,et al. <ref type=""bibr"" target=""#b24"">[25]</ref>, which is the computational cost of a threat model. They argue that the strength of an adversary should be i",1
"r denoising. Prakash, Moran, Garber, et al. <ref type=""bibr"" target=""#b14"">[15]</ref> claimed 81% accuracy under attack and Liao, Liang, Dong, et al. <ref type=""bibr"" target=""#b15"">[16]</ref> 75%, but both were reduced to 0% under just ✏ =4when obfuscated gradients were accounted for <ref type=""bibr or a max perturbation of ✏ = 16. We will show that our defense outperforms adversarial training across all ✏ 2 <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b15"">16]</ref>, and even continues to provide a robust defense up to ✏ = 32. We are not aware of any prior work which has co",0
"rning techniques within the realm of computer vision tasks and the surprising ease with which such systems are fooled into giving incorrect decisions <ref type=""bibr"" target=""#b1"">[2]</ref>. In particular, there are concerns about the safety of self-driving cars, as they could be fooled into misread of defenses have all been defeated in the white-box threat model, either by correctly incorporating the defense into the adversary's search procedure <ref type=""bibr"" target=""#b1"">[2]</ref>, or by properly accounting for obfuscated gradients <ref type=""bibr"" target=""#b3"">[4]</ref>. Obfuscated gradie 5.5% for Top-1 and Top-5 respectively for a max perturbation of ✏ = 16. We will show that our defense outperforms adversarial training across all ✏ 2 <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b15"">16]</ref>, and even continues to provide a robust defense up to ✏ = 32. We are n hich the adversary will attempt to modify the input x to a new input x such that kx xk ∞ &lt;✏.</p><p>In our experiments, we will test a range of ✏ 2 <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b31"">32]</ref>.</p><p>We will operate in the white-box scenario, and assume that our",0
"defending against the best known adversaries under the whitebox threat model <ref type=""bibr"" target=""#b3"">[4]</ref>. Kurakin, Goodfellow, and Bengio <ref type=""bibr"" target=""#b25"">[26]</ref> attempted to scale adversarial training up to the ImageNet dataset, which they found especially difficult. A d (FGSM) <ref type=""bibr"" target=""#b7"">[8]</ref> because it is a common baseline. More importantly, we will also use Projected Gradient Descent (PGD) <ref type=""bibr"" target=""#b25"">[26]</ref>, which is also targeted toward the L ∞ metric and is currently the strongest known attack for this metric. P state-ofthe-art results on the ImageNet dataset, as well as the associated costs in achieving such performance.</p><p>Kurakin, Goodfellow, and Bengio <ref type=""bibr"" target=""#b25"">[26]</ref> provide the strongest results on the full ImageNet dataset that we are aware of. They do this with adversari i-c.org/ns/1.0"" type=""table"" xml:id=""tab_1""><head>Table 1 :</head><label>1</label><figDesc>Accuracy (%) of baseline prior work on adversarial training<ref type=""bibr"" target=""#b25"">[26]</ref> and BaRT. 'Clean Images' is the results of classifying non-attacked images without any transforms; 'Attacked",0
"v e l o ping provably secure training procedures for deep learning <ref type=""bibr"" target=""#b20"">[21]</ref><ref type=""bibr"" target=""#b21"">[22]</ref><ref type=""bibr"" target=""#b22"">[23]</ref>. We believe that in the long term this is the most encouraging and desirable path toward defending against a",0
"ir with 1000 unobserved micro-videos that the user has not interacted before. We use the widely-used protocols <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b18"">19]</ref>: Precision@K, Recall@K, and NDCG@K to evaluate the performance of top-K recommendation. Here we set K = 10 an",1
"contents into user representations. Such signals can be summarized as the paths connecting the target user and item based on historical interactions <ref type=""bibr"" target=""#b38"">[38,</ref><ref type=""bibr"" target=""#b41"">41]</ref>. For example, given two paths p 1 u 1 → i 1 → u 2 → i 2 and p 2 u 1 graph on each modality. Intuitively, the historical behaviors of users reflect personal interests; meanwhile, the user groups can also profile items <ref type=""bibr"" target=""#b38"">[38,</ref><ref type=""bibr"" target=""#b41"">41]</ref>. Hence, in each modality (e.g., visual), we aggregate signals from t formation matrix to distill useful knowledge, where d ′ m is the transformation size; and we select LeakyReLU(•) as the nonlinear activation function <ref type=""bibr"" target=""#b38"">[38,</ref><ref type=""bibr"" target=""#b41"">41]</ref>. Such aggregation method assumes that different neighbors would have early multi-modal recommendation algorithms mainly based on CF models <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b38"">[38]</ref><ref type=""bibr"" target=""#b39"">[39]</ref><ref type=""bibr"" target=""#b40"">[40]</ref>. CF-based models leverage",1
"signals can be summarized as the paths connecting the target user and item based on historical interactions <ref type=""bibr"" target=""#b38"">[38,</ref><ref type=""bibr"" target=""#b41"">41]</ref>. For example, given two paths p 1 u 1 → i 1 → u 2 → i 2 and p 2 u 1 → i 1 → u 2 → i 3 ; this would suggest th historical behaviors of users reflect personal interests; meanwhile, the user groups can also profile items <ref type=""bibr"" target=""#b38"">[38,</ref><ref type=""bibr"" target=""#b41"">41]</ref>. Hence, in each modality (e.g., visual), we aggregate signals from the corresponding contents (e.g., frames) dge, where d ′ m is the transformation size; and we select LeakyReLU(•) as the nonlinear activation function <ref type=""bibr"" target=""#b38"">[38,</ref><ref type=""bibr"" target=""#b41"">41]</ref>. Such aggregation method assumes that different neighbors would have the same contributions to the representa s in the neighborhood. For a fair comparison, we integrate multi-modal features as the node features to learn the representation of each node. • NGCF <ref type=""bibr"" target=""#b41"">[41]</ref>. This method represent a novel recommendation framework to integrate the user-item interactions into the emb",1
"b8"">[9,</ref><ref type=""bibr"" target=""#b32"">32]</ref>, neural networks are increasingly used in the multi-modal domain, on multimodal representations <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b35"">[35]</ref><ref type=""bibr"" target=""#b36",0
"m the MoiveLens-10M dataset and crawled the corresponding trailers instead of the full-length videos from Youtube 5 . We use the pre-trained ResNet50 <ref type=""bibr"" target=""#b15"">[16]</ref> models to extract the visual features from key frames extracted from micro-video. In terms of acoustic modal",0
"The simplest implementation of the joint representation is the concatenation of single-modal features. Recently, with its success in computer vision <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b22"">23]</ref> and natural language processin",0
"sers and micro-videos, which is widespread in recommendation systems <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b28"">29]</ref>. Towards video recommendation, Hamilton et al. <ref type=""bibr"" targe",0
"s contain rich multimedia information -frames, sound tracks, and descriptions -that involve multiple modalities of visual, acoustic, and textual ones <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b27"">28]</ref>.</p><p>Incorporating such mu",0
"f single-modal features. Recently, with its success in computer vision <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b22"">23]</ref> and natural language processing <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b32"">32]</re",0
"guish and consider modal-specific user preferences.</p><p>However, existing works on multimedia recommendation <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b16"">17]</ref> mainly treat multi-modal information as a whole and incorporate them into a collaborative filtering (CF) fram e incorporated with user and item representations derived from CF framework, such as MF <ref type=""bibr"" target=""#b29"">[30]</ref>. For instance, VBPR <ref type=""bibr"" target=""#b16"">[17]</ref> leverages visual features to enrich ID embeddings of items; ACF <ref type=""bibr"" target=""#b7"">[8]</ref> empl the-art baselines. The baselines can be grouped into two categories: CF-based (VBPR and ACF) and GCN-based (NGCF and GraphSAGE) methods.</p><p>• VBPR <ref type=""bibr"" target=""#b16"">[17]</ref>. Such model integrates the content features and ID embeddings of each item as its representation, and uses t",0
"nt representation is the concatenation of single-modal features. Recently, with its success in computer vision <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b22"">23]</ref> and natural language processing <ref type=""bibr"" target=""#b8"">[9,</re",0
"focus on information interchange between users and items in multiple modalities. Inspired by the recent success of graph convolution networks (GCNs) <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b21"">22]</ref>, we use the information-propagation mechanism to encode high-order c feature and the interaction among different modalities. However, existing GNN efforts (e.g., GCN <ref type=""bibr"" target=""#b21"">[22]</ref>, GraphSage <ref type=""bibr"" target=""#b13"">[14]</ref>, GAT <ref type=""bibr"" target=""#b33"">[33]</ref>) only consider homogeneous features from one data source. Hen icro-video characteristic, we treat each modality as one component of the micro-video, which is consistent with the idea of standard ACF. • GraphSAGE <ref type=""bibr"" target=""#b13"">[14]</ref>. Such model is based on the general inductive framework that leverages node feature information to update no arget=""#b21"">22,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b28"">29]</ref>. Towards video recommendation, Hamilton et al. <ref type=""bibr"" target=""#b13"">[14]</ref> proposed a general inductive framework which leverages the content information to generate node representati",0
"tic information into a common space constrained by distance between the visual embedding and the corresponding word embedding. Similarly, Wang et al. <ref type=""bibr"" target=""#b34"">[34]</ref> constructed a coordinated space which enforces images with similar meanings to be closer to each other. Howe",0
", it is of crucial importance to distinguish and consider modal-specific user preferences.</p><p>However, existing works on multimedia recommendation <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b16"">17]</ref> mainly treat multi-modal information as a whole and incorporate them i set, we pair each observed user-item pair with 1000 unobserved micro-videos that the user has not interacted before. We use the widely-used protocols <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b18"">19]</ref>: Precision@K, Recall@K, and NDCG@K to evaluate the performance of top- arget=""#b29"">[30]</ref>. For instance, VBPR <ref type=""bibr"" target=""#b16"">[17]</ref> leverages visual features to enrich ID embeddings of items; ACF <ref type=""bibr"" target=""#b7"">[8]</ref> employs the attention mechanism on a user's history to encode two-level personal tastes on historical items an s, we use the concatenation of multi-modal features as the content information to predict the interactions between users and microvideos.</p><p>• ACF <ref type=""bibr"" target=""#b7"">[8]</ref>. This is the first framework that is designed to tackle the implicit feedback in multimedia recommendation. It uery tripartite graph and performed graph propagation to combine the content and feedback information between users and videos. Recently, Chen et al. <ref type=""bibr"" target=""#b7"">[8]</ref> explored the fine-grained user preference on the items and introduced a novel attention mechanism to address t",0
"and adopt VGGish <ref type=""bibr"" target=""#b19"">[20]</ref> to learn the acoustic deep learning features. For textual modality, we use Sentence2Vector <ref type=""bibr"" target=""#b0"">[1]</ref> to derive the textual features from microvideos' descriptions. Baselines. To evaluate the effectiveness of our",0
"type=""bibr"" target=""#b36"">Howard and Ruder, 2018]</ref>, but it has recently become more common to use models based on the ""Transformer"" architecture <ref type=""bibr"" target=""#b108"">[Vaswani et al., 2017]</ref>. The Transformer was initially shown to be effective for machine translation, but it has hitecture as originally proposed. Instead of providing a comprehensive definition of this model, we refer the interested reader to the original paper <ref type=""bibr"" target=""#b108"">[Vaswani et al., 2017]</ref> or follow-up tutorials 3,4 for a more detailed introduction.</p><p>The primary building b ese architectural variants in Section 3.2.</p><p>Overall, our encoder-decoder Transformer implementation closely follows its originally-proposed form <ref type=""bibr"" target=""#b108"">[Vaswani et al., 2017]</ref>. First, an input sequence of tokens is mapped to a sequence of embeddings, which is then odel is fed the question and its context and asked to generate the answer token-by-token. For WMT English to German, we use the same training data as <ref type=""bibr"" target=""#b108"">[Vaswani et al., 2017]</ref> (i.e. News Commentary v13, Common Crawl, Europarl v7) and newstest2013 as a validation se > <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.1.1"">Model</head><p>For our model, we use a standard encoder-decoder Transformer as proposed by <ref type=""bibr"" target=""#b108"">Vaswani et al. [2017]</ref>. While many modern approaches to transfer learning for NLP use a Transformer architecture",1
"orrupted tokens as targets; we compare these two approaches in Section 3.3.2. Finally, we also consider a basic deshuffling objective as used e.g. in <ref type=""bibr"" target=""#b61"">[Liu et al., 2019a]</ref> where it was applied to a denoising sequential autoencoder. This approach takes a sequence of",0
"ang et al., 2019;</ref><ref type=""bibr"" target=""#b65"">Liu et al., 2019c;</ref><ref type=""bibr"" target=""#b120"">Zellers et al., 2019]</ref>, benchmarks <ref type=""bibr"" target=""#b111"">[Wang et al., 2019b</ref><ref type=""bibr"" target=""#b109"">[Wang et al., , 2018;;</ref><ref type=""bibr"" target=""#b17"">Co rom TensorFlow Datasets.<ref type=""foot"" target=""#foot_7"">9</ref> </p><p>GLUE <ref type=""bibr"" target=""#b109"">[Wang et al., 2018]</ref> and SuperGLUE <ref type=""bibr"" target=""#b111"">[Wang et al., 2019b]</ref> each comprise a collection of text classification tasks meant to test general language unde esigned to comprise of tasks that were ""beyond the scope of current state-of-the-art systems, but solvable by most college-educated English speakers"" <ref type=""bibr"" target=""#b111"">[Wang et al., 2019b]</ref>. We nearly match the human performance of 89.8 <ref type=""bibr"" target=""#b111"">[Wang et al. by most college-educated English speakers"" <ref type=""bibr"" target=""#b111"">[Wang et al., 2019b]</ref>. We nearly match the human performance of 89.8 <ref type=""bibr"" target=""#b111"">[Wang et al., 2019b]</ref>. Interestingly, on the reading comprehension tasks (MultiRC and ReCoRD) we exceed human per",0
"target=""#b64"">Yang et al., 2019;</ref><ref type=""bibr"" target=""#b63"">Liu et al., 2019b;</ref><ref type=""bibr"" target=""#b110"">Wang et al., 2019a;</ref><ref type=""bibr"" target=""#b100"">Song et al., 2019;</ref><ref type=""bibr"" target=""#b25"">Dong et al., 2019;</ref><ref type=""bibr"" target=""#b44"">Joshi et ens in the input with a mask token and the model is trained to reconstruct the original uncorrupted sequence. A similar masking objective was used by <ref type=""bibr"" target=""#b100"">Song et al. [2019]</ref> where it was referred to as ""MASS"", so we call this variant the ""MASS-style"" objective. Secon",0
"ss all subtasks (as stipulated by the official benchmarks) under the headings ""GLUE"" and ""SGLUE"". For all translation tasks, we report the BLEU score <ref type=""bibr"" target=""#b73"">[Papineni et al., 2002]</ref> as provided by SacreBLEU v1.3.0 <ref type=""bibr"" target=""#b80"">[Post, 2018]</ref> with ""e",0
"e intensiveness in developing hand-crafted features, etc.</p><p>Inspired by the current trend of formalizing NLP problems as question answering tasks <ref type=""bibr"" target=""#b21"">(Levy et al., 2017;</ref><ref type=""bibr"" target=""#b28"">McCann et al., 2018;</ref><ref type=""bibr"" target=""#b22"">Li et sitions of the answer spans. Over the past one or two years, there has been a trend of transforming NLP tasks to MRC question answering. For example, <ref type=""bibr"" target=""#b21"">Levy et al. (2017)</ref> transformed the task of relation extraction to a QA task: each relation type R(x, y) can be pa",1
"problems as question answering tasks <ref type=""bibr"" target=""#b21"">(Levy et al., 2017;</ref><ref type=""bibr"" target=""#b28"">McCann et al., 2018;</ref><ref type=""bibr"" target=""#b22"">Li et al., 2019)</ref>, we propose a new framework that is capable of handling both flat and nested NER. Instead of tre ing. For example, the task of summarization can be formalized as answering the question ""What is the summary?"". Our work is significantly inspired by <ref type=""bibr"" target=""#b22"">Li et al. (2019)</ref>, which formalized the task of entity-relation extraction as a multi-turn question answering task 2"">Li et al. (2019)</ref>, which formalized the task of entity-relation extraction as a multi-turn question answering task. Different from this work, <ref type=""bibr"" target=""#b22"">Li et al. (2019)</ref> focused on relation extraction rather than NER. Additionally, <ref type=""bibr"" target=""#b22"">Li g task. Different from this work, <ref type=""bibr"" target=""#b22"">Li et al. (2019)</ref> focused on relation extraction rather than NER. Additionally, <ref type=""bibr"" target=""#b22"">Li et al. (2019)</ref> utilized a template-based procedure for constructing queries to extract semantic relations betwe prior knowledge about labels and have a significant influence on the final results. Different ways have been proposed for question generation, e.g., <ref type=""bibr"" target=""#b22"">Li et al. (2019)</ref> utilized a template-based procedure for constructing queries to extract semantic relations betwe",1
".</p><p>Inspired by the current trend of formalizing NLP problems as question answering tasks <ref type=""bibr"" target=""#b21"">(Levy et al., 2017;</ref><ref type=""bibr"" target=""#b28"">McCann et al., 2018;</ref><ref type=""bibr"" target=""#b22"">Li et al., 2019)</ref>, we propose a new framework that is cap x study?"". Given a question q(x), if a non-null answer y can be extracted from a sentence, it means the relation label for the current sentence is R. <ref type=""bibr"" target=""#b28"">McCann et al. (2018)</ref> transformed NLP tasks such as summarization or sentiment analysis into question answering. F",1
"chine Reading Comprehension (MRC)</head><p>MRC models <ref type=""bibr"" target=""#b40"">(Seo et al., 2016;</ref><ref type=""bibr"">Wang et al., 2016;</ref><ref type=""bibr"" target=""#b48"">Wang and Jiang, 2016;</ref><ref type=""bibr"" target=""#b51"">Xiong et al., 2016</ref><ref type=""bibr"" target=""#b52"">Xiong",0
"le span to detect overlapping mentions and Katiyar and Cardie (2018) used neural models to learn the hyper-graph representations for nested entities. <ref type=""bibr"" target=""#b15"">Ju et al. (2018)</ref> dynamically stacked flat NER layers in a hierarchical manner. <ref type=""bibr"" target=""#b23"">Lin",0
"e entities. Many attempts have been made to reconcile sequence labeling models with nested NER <ref type=""bibr"" target=""#b0"">(Alex et al., 2007;</ref><ref type=""bibr"" target=""#b1"">Byrne, 2007;</ref><ref type=""bibr"" target=""#b10"">Finkel and Manning, 2009;</ref><ref type=""bibr"" target=""#b25"">Lu and Ro",0
"raining, we compare the LSTM-CRF tagging model <ref type=""bibr"" target=""#b45"">(Strubell et al., 2017)</ref> with other MRC based models such as QAnet <ref type=""bibr"" target=""#b54"">(Yu et al., 2018)</ref> and BiDAF <ref type=""bibr"">(Seo et al., 2017)</ref>, which do not rely on large-scale pretraini",0
"/ref> proposed the Anchor-Region Networks (ARNs) architecture by modeling and leveraging the head-driven phrase structures of nested entity mentions. <ref type=""bibr"" target=""#b26"">Luan et al. (2019)</ref> built a span enumeration approach by selecting the most confident entity spans and linking the",0
"set consists of RichERE annotated datasets, which include LDC2015E29, LDC2015E68, LDC2016E31 and LDC2017E02. We follow the dataset split strategy in <ref type=""bibr"" target=""#b24"">Lin et al. (2019b)</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.1.2"">Baselines</head><p>We use",0
"resentations for nested entities. <ref type=""bibr"" target=""#b15"">Ju et al. (2018)</ref> dynamically stacked flat NER layers in a hierarchical manner. <ref type=""bibr"" target=""#b23"">Lin et al. (2019a)</ref> proposed the Anchor-Region Networks (ARNs) architecture by modeling and leveraging the head-dr 4.1.3"">Results</head><p>Table <ref type=""table"" target=""#tab_5"">2</ref>   <ref type=""bibr"" target=""#b47"">(Wang and Lu, 2018)</ref> 76.8 72.3 74.5 ARN <ref type=""bibr"" target=""#b23"">(Lin et al., 2019a)</ref> 76.2 73.6 74.9 Path-BERT <ref type=""bibr"" target=""#b42"">(Shibuya and Hovy, 2019)</ref> 82.98",0
""" target=""#b1"">Byrne, 2007;</ref><ref type=""bibr"" target=""#b10"">Finkel and Manning, 2009;</ref><ref type=""bibr"" target=""#b25"">Lu and Roth, 2015;</ref><ref type=""bibr"" target=""#b16"">Katiyar and Cardie, 2018)</ref>, mostly based on the pipelined systems. However, pipelined systems suffer from the disa as follows: GENIA <ref type=""bibr"" target=""#b30"">(Ohta et al., 2002)</ref> For the GENIA dataset, we use GENIAcorpus3.02p. We follow the protocols in <ref type=""bibr"" target=""#b16"">Katiyar and Cardie (2018)</ref>.</p><formula xml:id=""formula_7"">L = αL start + βL end + γL span (6) α, β, γ ∈ [0,</form ormula_7"">L = αL start + βL end + γL span (6) α, β, γ ∈ [0,</formula></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>KBP2017</head><p>We follow <ref type=""bibr"" target=""#b16"">Katiyar and Cardie (2018)</ref> and evaluate our model on the 2017 English evaluation dataset (LDC2017D55). Training se",0
"achos (2019)</ref> introduced a BERT-based model that first merges tokens and/or entities into entities, and then assigned labeled to these entities. <ref type=""bibr"" target=""#b42"">Shibuya and Hovy (2019)</ref> provided inference model that extracts entities iteratively from outermost ones to inner e=""bibr"" target=""#b47"">(Wang and Lu, 2018)</ref> 76.8 72.3 74.5 ARN <ref type=""bibr"" target=""#b23"">(Lin et al., 2019a)</ref> 76.2 73.6 74.9 Path-BERT <ref type=""bibr"" target=""#b42"">(Shibuya and Hovy, 2019)</ref> 82.98 82.42 82.70 Merge-BERT <ref type=""bibr"" target=""#b11"">(Fisher and Vlachos, 2019)</",0
"le tagging class to each unit within a sequence of tokens. This formulation is unfortunately incapable of handling overlapping entities in nested NER <ref type=""bibr"" target=""#b13"">(Huang et al., 2015;</ref><ref type=""bibr"" target=""#b3"">Chiu and Nichols, 2015)</ref>, where multiple categories need t",0
"e shown to be vulnerable to attacks taking such defense strategies into consideration <ref type=""bibr"">(Athalye et al., 2018b)</ref>.</p><p>Recently, <ref type=""bibr"" target=""#b13"">Shafahi et al. (2019)</ref> showed that, for two classes of data distributed with bounded probability densities on a co of these results on studying adversarial examples for general classifiers, and their relationship to some recent works in literature.</p><p>Recently, <ref type=""bibr"" target=""#b13"">Shafahi et al. (2019)</ref> shows that no classifier can achieve low misclassification rate and also be adversarialrobu",1
"obust training <ref type=""bibr"" target=""#b4"">(Madry et al., 2018;</ref><ref type=""bibr"" target=""#b10"">Sinha et al., 2018)</ref>; Input transformation <ref type=""bibr"" target=""#b11"">Xu et al. (2017)</ref>. However, many of the defenses are shown to be vulnerable to attacks taking such defense strateg",0
"rt machining learning classification systems due to its great performance gains in recent years. Meanwhile adversarial examples, first pointed out by <ref type=""bibr"" target=""#b0"">Szegedy et al. (2014)</ref>, emerges as a novel peculiar security threat against such systems: a small perturbation that ype=""bibr"">Athalye et al., 2018a)</ref>. Various defense methods have also been proposed to prevent adversarial example attacks: Adversarial training <ref type=""bibr"" target=""#b0"">(Szegedy et al., 2014;</ref><ref type=""bibr"" target=""#b1"">Goodfellow et al., 2015)</ref>; Defensive distillation <ref ty",0
"y et al., 2018)</ref>. The adversarial examples have also been demonstrated to misled DNN based classification systems in physical world applications <ref type=""bibr"" target=""#b5"">(Sharif et al., 2016;</ref><ref type=""bibr"" target=""#b6"">Brown et al., 2017;</ref><ref type=""bibr"" target=""#b7"">Kurakin",0
"the exact and soft matches, we adopt the interaction-based models <ref type=""bibr"" target=""#b4"">[5]</ref>, <ref type=""bibr"" target=""#b12"">[12]</ref>, <ref type=""bibr"" target=""#b52"">[43]</ref> widely used in information retrieval. The interaction-based models first build a similarity matrix between e to 500, and the maximal number of papers published by each person to 100.</p><p>The hyper-parameters of the RBF kernel functions are set the same as <ref type=""bibr"" target=""#b52"">[43]</ref>. We use 11 RBF kernels, with the hyper-parameters m = f1; 0:9; 0:7; 0:5; 0:3; 0:1; À0:1; À0:3; À0:5; À0:7; À",1
"get=""#b47"">[40]</ref>, <ref type=""bibr"" target=""#b61"">[49]</ref>, DBSCAN <ref type=""bibr"" target=""#b13"">[13]</ref> or 1016 semi-supervised clustering <ref type=""bibr"" target=""#b21"">[21]</ref> to partition these papers.</p><p>1017</p><p>Embedding models further include graph auto-encoder <ref type=""b",0
"f>. Feature-based <ref type=""bibr"" target=""#b17"">[17]</ref> or neural models such as skip-gram <ref type=""bibr"" target=""#b53"">[44]</ref>, autoencoder <ref type=""bibr"" target=""#b11"">[11]</ref>, CNN <ref type=""bibr"" target=""#b36"">[33]</ref>,</p><p>LSTM <ref type=""bibr"" target=""#b16"">[16]</ref> are pro",0
"ion encourages the matching component to float the right person at the top and push the wrong person away from the top. The policy gradient algorithm <ref type=""bibr"" target=""#b37"">[34]</ref> is adopted to optimize the expected reward in Eq. ( <ref type=""formula"" target=""#formula_7"">9</ref>), whose",0
"lized language model pre-training <ref type=""bibr"" target=""#b23"">(Peters et al., 2018;</ref><ref type=""bibr"" target=""#b24"">Radford et al., 2018;</ref><ref type=""bibr"" target=""#b5"">Devlin et al., 2019)</ref>, which has improved performances on various tasks <ref type=""bibr"" target=""#b25"">(Rajpurkar e irst split into words {w 1 , . . . , w n } with length of n by the same WordPiece tokenizer <ref type=""bibr"" target=""#b35"">(Wu et al., 2016)</ref> in <ref type=""bibr"" target=""#b5"">Devlin et al. (2019)</ref>. Next, as shown in Fig. <ref type=""figure"">1</ref>, the word w i and its index i (w i 's abso nguage and vision outputs are the feature sequences generated by the cross-modality encoder. For the cross-modality output, following the practice in <ref type=""bibr"" target=""#b5"">Devlin et al. (2019)</ref>, we append a special token [CLS] (denoted as the top yellow block in the bottom branch of Fig guage en-coder and an object-relationship encoder, and each of them only focuses on a single modality (i.e., language or vision). Different from BERT <ref type=""bibr"" target=""#b5"">(Devlin et al., 2019)</ref>, which applies the transformer encoder only to language inputs, we apply it to vision inputs ity language model (LM) task. As shown in the bottom branch of Fig. <ref type=""figure"" target=""#fig_0"">2</ref>, the task setup is almost same to BERT <ref type=""bibr"" target=""#b5"">(Devlin et al., 2019)</ref>: words are randomly masked with a probability of 0.15 and the model is asked to predict thes . Then, we train a classifier to predict whether an image and a sentence match each other. This task is similar to 'Next Sentence Prediction' in BERT <ref type=""bibr"" target=""#b5"">(Devlin et al., 2019)</ref>.</p><p>Image Question Answering (QA) In order to enlarge the pre-training dataset (see detai are in the Appendix. The input sentences are split by the WordPiece tokenizer <ref type=""bibr"" target=""#b35"">(Wu et al., 2016)</ref> provided in BERT <ref type=""bibr"" target=""#b5"">(Devlin et al., 2019)</ref> image to maximize the pre-training compute utilization by avoiding padding. For the model ar cover 90% questions in all three image QA datasets. We take Adam (Kingma and Ba, 2014) as the optimizer with a linear-decayed learning-rate schedule <ref type=""bibr"" target=""#b5"">(Devlin et al., 2019)</ref> and a peak learning rate at 1e − 4. We train the model for 20 epochs (i.e., roughly 670K<ref g certain model components/pre-training strategies.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""5.1"">BERT versus LXMERT</head><p>BERT <ref type=""bibr"" target=""#b5"">(Devlin et al., 2019</ref>) is a pre-trained language encoder which improves several language tasks. As shown in Table <",1
"h intra-modality and cross-modality relationships.</p><p>Empirically, we first evaluate LXMERT on two popular visual question-answering datasets, VQA <ref type=""bibr"" target=""#b1"">(Antol et al., 2015)</ref> and GQA <ref type=""bibr"" target=""#b13"">(Hudson and Manning, 2019)</ref>. Our model outperform al., 2017)</ref>. Besides the two original captioning datasets, we also aggregate three large image question answering (image QA) datasets: VQA v2.0 <ref type=""bibr"" target=""#b1"">(Antol et al., 2015)</ref>, GQA balanced version (Hudson and Manning, 2019), and VG-QA <ref type=""bibr"" target=""#b40"">(Z luated Datasets Description</head><p>We use three datasets for evaluating our LXMERT framework.</p><p>VQA The goal of visual question answering (VQA) <ref type=""bibr"" target=""#b1"">(Antol et al., 2015</ref>) is to answer a natural language question related to an image. We take VQA v2.0 dataset <ref t",0
"ding a universal backbone model with large-scale contextualized language model pre-training <ref type=""bibr"" target=""#b23"">(Peters et al., 2018;</ref><ref type=""bibr"" target=""#b24"">Radford et al., 2018;</ref><ref type=""bibr"" target=""#b5"">Devlin et al., 2019)</ref>, which has improved performances on",0
"hr et al., 2019)</ref> is a challenging visual reasoning dataset where some existing approaches <ref type=""bibr"" target=""#b12"">(Hu et al., 2017;</ref><ref type=""bibr"" target=""#b22"">Perez et al., 2018)</ref> fail, and the SotA method is 'MaxEnt' in <ref type=""bibr"" target=""#b30"">Suhr et al. (2019)</r",0
"inal but causes the classifier to produce a different label prediction <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b54"">55,</ref><ref type=""bibr"" target=""#b23"">24]</ref>. Adversarial examples have been shown to be ubiquitous beyond classification, ranging from object detection < type=""bibr"" target=""#b71"">72,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b70"">71]</ref>. Among them, adversarial training <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b35"">36]</ref> is one of the most popular technique <ref type=""bibr"" target=""#b1"">[ the data manifold. As the adversarial examples reside in a large, contiguous region and a significant portion of the adversarial subspaces is shared <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b58"">59,</ref><ref type=""bibr"" target=""#b39 t many existing defence methods suffer from a false sense of robustness against adversarial attacks due to gradient masking, and adversarial training <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b31"">32,</ref><ref type=""bibr"" target=""#b57"">58,</ref><ref type=""bibr"" target=""#b35 get=""#b35"">36]</ref> is one of the effective defense method against adversarial attacks. It improves model robustness by solving a minimax problem as <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b35"">36]</ref>:</p><formula xml:id=""formula_0"">min θ max x ∈Sx L(x , y; θ)<label>(1 max ν∈Sµ D(µ, ν).<label>(7)</label></formula><p>The proposed formulation deviates from the conventional minimax formulation for adversarial training <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b35"">36]</ref>. More specifically, it can be regarded as an instance of the more ge and takes the form of a supervised loss, e.g., D θ ≡ i L θ (x i , y i ), the proposed approach reduces to the conventional adversarial training setup <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b35"">36]</ref>. The overall procedure for the proposed approach is in Algorithm 1.< bibr"" target=""#b54"">[55]</ref>. A fast gradient sign method (FGSM) for adversarial attack generation is developed and used in adversarial training in <ref type=""bibr"" target=""#b23"">[24]</ref>. Many variants of attacks have been developed later <ref type=""bibr"" target=""#b40"">[41,</ref><ref type=""bibr cks <ref type=""bibr"" target=""#b35"">[36]</ref>. The inner maximization can be solved approximately, using for example a one-step approach such as FGSM <ref type=""bibr"" target=""#b23"">[24]</ref>, or a multi-step projected gradient descent (PGD) method <ref type=""bibr"" target=""#b35"">[36]</ref> x t+1 = P ng an upper bound of robustness on the test set, by measuring the accuracy of the model under different adversarial attacks, including white-box FGSM <ref type=""bibr"" target=""#b23"">[24]</ref>, PGD <ref type=""bibr"" target=""#b35"">[36]</ref>, CW <ref type=""bibr"" target=""#b7"">[8]</ref> (CW-loss <ref typ",1
"ng images as samples from it <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b50"">51,</ref><ref type=""bibr"" target=""#b43"">44,</ref><ref type=""bibr"" target=""#b55"">56]</ref>. Modern classifiers are over-complete in terms of parameterizations and different local minima have been show ef>. However, the classification boundary that extends beyond the manifold is less constrained, contributing to the existence of adversarial examples <ref type=""bibr"" target=""#b55"">[56,</ref><ref type=""bibr"" target=""#b58"">59]</ref>. For examples, it has been pointed out that some clean trained model he decision boundary lies close to the manifold for its out of manifold part, adversarial perturbations lead to a tilting effect on the data manifold <ref type=""bibr"" target=""#b55"">[56]</ref>; at places where the classification boundary is far from the manifold for its out of manifold part, the adve",1
"fold and neighborhood structure have been proven to be effective in capturing the inter-sample relationships <ref type=""bibr"" target=""#b50"">[51,</ref><ref type=""bibr"" target=""#b21"">22]</ref>. Natural images live on a low-dimensional manifold, with the training and testing images as samples from it < on boundary. The idea of leveraging inter-sample relationship for learning dates back to the seminal work of <ref type=""bibr"" target=""#b50"">[51,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b47"">48]</ref>. This type of local structure is also exploited in this work, but for",0
"ollapsing <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b0"">1]</ref>. OTrelated distances <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b11"">12]</ref> have been used for overcoming the difficulties encountered in the original GAN training <ref type=""bibr"" targ er T is intractable in general <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b48"">49,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b11"">12]</ref>. Here we compare two practical solvers, the Sinkhorn algorithm <ref type=""bibr"" target=""#b11"">[12]</ref> and used in training following literature <ref type=""bibr"" target=""#b35"">[36]</ref>. Label smoothing of 0.5, attack iteration T=1 and Sinkhorn algorithm <ref type=""bibr"" target=""#b11"">[12]</ref> with regularization of 0.01 is used. For testing, model robustness is evaluated by approximately computing a </ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b11"">12]</ref>. Here we compare two practical solvers, the Sinkhorn algorithm <ref type=""bibr"" target=""#b11"">[12]</ref> and the Inexact Proximal point method for Optimal Transport (IPOT) algorithm <ref type=""bibr"" target=""#b65""> hod for Optimal Transport (IPOT) algorithm <ref type=""bibr"" target=""#b65"">[66]</ref>. More details on them can be found in the supplementary file and <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b65"">66,</ref><ref type=""bibr"" target=""#b44"">45]</ref>. The results are summarized",0
"s several benchmark datasets including CIFAR10 <ref type=""bibr"" target=""#b30"">[31]</ref>, CIFAR100 <ref type=""bibr"" target=""#b30"">[31]</ref> and SVHN <ref type=""bibr"" target=""#b41"">[42]</ref>. We use Wide ResNet (WRN-28-10) <ref type=""bibr"" target=""#b67"">[68]</ref> as the network structure following PGD/CW attackers with =8 and attack step 20 and 100 in the sequel as part of the threat models.  SVHN. We further report results on the SVHN dataset <ref type=""bibr"" target=""#b41"">[42]</ref>. SVHN is a 10-way house number classification dataset, with 73257 training images and 26032 test images. The",0
"d inputs for adversarial training.</p><p>Inter-sample Regularization <ref type=""bibr"" target=""#b69"">[70,</ref><ref type=""bibr"" target=""#b29"">30,</ref><ref type=""bibr"" target=""#b38"">39]</ref>. Mixup <ref type=""bibr"" target=""#b69"">[70]</ref> generates training examples by linear interpolation between dversarial images. The idea is to suppress spurious logits responses using the natural logits as a reference. Similarly, virtual adversarial training <ref type=""bibr"" target=""#b38"">[39]</ref> proposed a regularization term based on the KL divergence of the prediction probability of original and adve",0
"ogresses been made towards improving model robustness against adversarial examples under different scenarios <ref type=""bibr"" target=""#b57"">[58,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" target=""#b66"">67,</ref><ref type=""bibr"" target=""#b71"" ype=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b70"">71]</ref>. Among them, adversarial training <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b35"">36]</ref> is one of the most popular technique <ref type=""bibr"" target=""#b1"">[2]</ref>, which conducts model training u ng, and adversarial training <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b31"">32,</ref><ref type=""bibr"" target=""#b57"">58,</ref><ref type=""bibr"" target=""#b35"">36]</ref> is one of the effective defense method against adversarial attacks. It improves model robustness by solving a ive defense method against adversarial attacks. It improves model robustness by solving a minimax problem as <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b35"">36]</ref>:</p><formula xml:id=""formula_0"">min θ max x ∈Sx L(x , y; θ)<label>(1)</label></formula><p>where the inner max mula><p>The proposed formulation deviates from the conventional minimax formulation for adversarial training <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b35"">36]</ref>. More specifically, it can be regarded as an instance of the more general bilevel optimization problem <ref t e.g., D θ ≡ i L θ (x i , y i ), the proposed approach reduces to the conventional adversarial training setup <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b35"">36]</ref>. The overall procedure for the proposed approach is in Algorithm 1.</p></div> <div xmlns=""http://www.tei-c.or training <ref type=""bibr"" target=""#b31"">[32]</ref>. Currently available remedies either increase the number of iterations for generating the attacks <ref type=""bibr"" target=""#b35"">[36]</ref> or use classes other than the ground-truth for attack generation <ref type=""bibr"" target=""#b31"">[32,</ref><r maximization essentially generates attacks while the outer minimization corresponds to minimizing the ""adversarial loss"" induced by the inner attacks <ref type=""bibr"" target=""#b35"">[36]</ref>. The inner maximization can be solved approximately, using for example a one-step approach such as FGSM <ref using for example a one-step approach such as FGSM <ref type=""bibr"" target=""#b23"">[24]</ref>, or a multi-step projected gradient descent (PGD) method <ref type=""bibr"" target=""#b35"">[36]</ref> x t+1 = P Sx x t + α • sign ∇ x L(x t , y; θ) ,</p><p>where P Sx (•) is a projection operator projecting the ref type=""bibr"" target=""#b41"">[42]</ref>. We use Wide ResNet (WRN-28-10) <ref type=""bibr"" target=""#b67"">[68]</ref> as the network structure following <ref type=""bibr"" target=""#b35"">[36]</ref>. We compare the performance of the proposed method with a number of baseline methods, including: i) the mode ach using clean images (Standard) <ref type=""bibr"" target=""#b30"">[31]</ref>, ii) PGD-based approach from Madry et al. <ref type=""bibr"">(Madry)</ref>  <ref type=""bibr"" target=""#b35"">[36]</ref>, which is one of the most effective defense method <ref type=""bibr"" target=""#b1"">[2]</ref>, iii) another rec m horizontal flips <ref type=""bibr"" target=""#b30"">[31]</ref> during training. The perturbation budget of = 8 is used in training following literature <ref type=""bibr"" target=""#b35"">[36]</ref>. Label smoothing of 0.5, attack iteration T=1 and Sinkhorn algorithm <ref type=""bibr"" target=""#b11"">[12]</re , by measuring the accuracy of the model under different adversarial attacks, including white-box FGSM <ref type=""bibr"" target=""#b23"">[24]</ref>, PGD <ref type=""bibr"" target=""#b35"">[36]</ref>, CW <ref type=""bibr"" target=""#b7"">[8]</ref> (CW-loss <ref type=""bibr"" target=""#b7"">[8]</ref> within the PGD ttacker under different attack budgets with a fixed attack step of 20, with the results shown in Figure <ref type=""figure"" target=""#fig_3"">3</ref>    <ref type=""bibr"" target=""#b35"">[36]</ref> and Bilateral <ref type=""bibr"" target=""#b60"">[61]</ref> methods on CIFAR10 under different threat models.</p <ref type=""bibr"" target=""#b60"">[61]</ref> methods on CIFAR10 under different threat models.</p><p>further boosts the performance over the Madry model <ref type=""bibr"" target=""#b35"">[36]</ref> by a large margin under different attack budgets. We also conduct experiments using PGD attacker with differ n in Figure <ref type=""figure"" target=""#fig_3"">3 (b-c</ref>) and also Table <ref type=""table"" target=""#tab_1"">1</ref>. It is observed that both Madry <ref type=""bibr"" target=""#b35"">[36]</ref> and Proposed can maintain a fairly stable performance when the number of attack iterations is increased. Not tain a fairly stable performance when the number of attack iterations is increased. Notably, the proposed approach consistently outperforms the Madry <ref type=""bibr"" target=""#b35"">[36]</ref> model across a wide range of attack iterations. From Table <ref type=""table"" target=""#tab_1"">1</ref>, it is f type=""table"" target=""#tab_3"">2</ref>(b), the proposed approach outperforms all baseline methods significantly, which is about 20% better than Madry <ref type=""bibr"" target=""#b35"">[36]</ref> and Bilateral <ref type=""bibr"" target=""#b60"">[61]</ref> under PGD attack and about 10% better under CW attac ct experiments on CIFAR10 <ref type=""bibr"" target=""#b30"">[31]</ref>, which is a popular dataset that is widely use in adversarial training literature <ref type=""bibr"" target=""#b35"">[36,</ref><ref type=""bibr"" target=""#b60"">61]</ref> with 10 classes, 5K training images per class and 10K test images. W r class and 10K test images. We report the accuracy on the original test images (Clean) and under PGD and CW attack with T iterations (PGDT and CWT ) <ref type=""bibr"" target=""#b35"">[36,</ref><ref type=""bibr"" target=""#b7"">8]</ref>. The evaluation results are summarized in Table <ref type=""table"" targ",0
"><ref type=""bibr"" target=""#b21"">22]</ref>. Natural images live on a low-dimensional manifold, with the training and testing images as samples from it <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b50"">51,</ref><ref type=""bibr"" target=""#b43"">44,</ref><ref type=""bibr"" target=""#b55",0
"<ref type=""bibr"" target=""#b48"">49]</ref>, which is the case for natural images. It has been widely applied to many tasks, such as generative modeling <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b0"">1,</ref><ref type=""bibr"" target=""#b48"">49,</ref><ref type=""bibr"" target=""#b19""> olvers. Exact minimization of Eqn.(4) over T is intractable in general <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b48"">49,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b11"">12]</ref>. Here we compare two practical solvers, the Sinkhorn algorithm <ref t",0
"""bibr"" target=""#b36"">37,</ref><ref type=""bibr"" target=""#b62"">63,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" target=""#b49"">50,</ref><ref type=""bibr"" target=""#b52"">53,</ref><ref type=""bibr"" target=""#b45"">46,</ref><ref type=""bibr"" target=""#b34""",0
"pe=""bibr"" target=""#b45"">Zhao et al., 2017)</ref> inject high-level variations with latent variables. Variational Hierarchical Conversation RNN (VHCR) <ref type=""bibr"" target=""#b30"">(Park et al., 2018)</ref> is a most similar model to ours, which also adopts a hierarchical latent structure. Our metho",1
"is range of input length (accounting for 6.5% of the entire training set).</p><p>To further verify the planning diversity, we also computed self-BLEU <ref type=""bibr"" target=""#b46"">(Zhu et al., 2018)</ref> to evaluate how different planning results (or texts) for the same input overlap (by taking on",0
"emplate-based <ref type=""bibr"" target=""#b24"">(McRoy et al., 2003;</ref><ref type=""bibr"" target=""#b6"">van Deemter et al., 2005)</ref> or grammar-based <ref type=""bibr"" target=""#b1"">(Bateman, 1997;</ref><ref type=""bibr"" target=""#b8"">Espinosa et al., 2008)</ref>. As these models are shallow and the two",0
"bibr"" target=""#b7"">(Duboue and McKeown, 2003)</ref>. Surface realization generates natural language by carrying out the plan, which is template-based <ref type=""bibr"" target=""#b24"">(McRoy et al., 2003;</ref><ref type=""bibr"" target=""#b6"">van Deemter et al., 2005)</ref> or grammar-based <ref type=""bib",0
"""figure"" target=""#fig_0"">1</ref> are representative for the 'happy' category while the others not. In this paper, inspired by the attention mechanism <ref type=""bibr"" target=""#b13"">[14]</ref> of machine translation and the neural aggregation networks <ref type=""bibr"" target=""#b14"">[15]</ref> of vide",1
"of the methods focus on improving static face based CNN models and combine scores video-level FER. Both <ref type=""bibr"" target=""#b27"">[28]</ref> and <ref type=""bibr"" target=""#b28"">[29]</ref> input two LBP maps and a gray image for CNN models. Deeplysupervised networks are used in <ref type=""bibr"" t 7"">[28]</ref> and <ref type=""bibr"" target=""#b28"">[29]</ref> input two LBP maps and a gray image for CNN models. Deeplysupervised networks are used in <ref type=""bibr"" target=""#b28"">[29]</ref> and <ref type=""bibr"" target=""#b29"">[30]</ref>, which add supervision on intermediate layers. For static meth",0
"10"">11]</ref>. C3D, which is originally developed for video action recognition, is also popular in the EmotiW challenge.</p><p>Geometry based methods <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b10"">11]</ref> aim to model the motions of key points in faces which only leverage ef> aim to model the motions of key points in faces which only leverage the geometry locations of facial landmarks in every video frames. Jung et al. <ref type=""bibr"" target=""#b11"">[12]</ref> propose a deep temporal appearance-geometry network (DTAGN) which first alternately concatenates the xcoordi #b22"">[23]</ref> propose to combine a spatial CNN model and a temporal network, where the spatial CNN model only uses the last peak frame. Jung et al <ref type=""bibr"" target=""#b11"">[12]</ref> select a fixed length sequence for each video with a lipreading method <ref type=""bibr"" target=""#b25"">[26]</ Test data Acc.</p><p>ST network <ref type=""bibr"" target=""#b22"">[23]</ref> S: the last frame T: all frames S: the last frame T: all frames 98.50 DTAGN <ref type=""bibr"" target=""#b11"">[12]</ref> Fixed length Fixed length 97.25</p><p>CNN+Island loss <ref type=""bibr"" target=""#b23"">[24]</ref> The last thr",0
"hods.</p><p>Static-based feature extraction methods mainly inherit those methods from static image emotion recognition which can be both hand-crafted <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref> and learned <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" targ target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b4"">5]</ref>. For the hand-crafted features, Littlewort et al. <ref type=""bibr"" target=""#b0"">[1]</ref> propose to use a bank of 2D Gabor filters to extract facial features for videobased FER. Shan et al. <ref type",0
"type=""bibr"" target=""#b28"">[29]</ref> and <ref type=""bibr"" target=""#b29"">[30]</ref>, which add supervision on intermediate layers. For static methods, <ref type=""bibr"" target=""#b30"">[31]</ref> gets slightly better performance than ours. However, <ref type=""bibr"" target=""#b30"">[31]</ref> uses DenseNet upervision on intermediate layers. For static methods, <ref type=""bibr"" target=""#b30"">[31]</ref> gets slightly better performance than ours. However, <ref type=""bibr"" target=""#b30"">[31]</ref> uses DenseNet-161 and pretrains it on both large-scale face datasets and their own Situ emotion video datase target=""#b30"">[31]</ref> uses DenseNet-161 and pretrains it on both large-scale face datasets and their own Situ emotion video dataset. Additionally, <ref type=""bibr"" target=""#b30"">[31]</ref> applies complicated post-processing which extracts frame features and compute their mean vector, max-pooled",0
"ion</p><formula xml:id=""formula_0"">⇤ Equal contributions.</formula><p>as a means for, e.g., model debugging or architecture selection. A recent paper <ref type=""bibr"" target=""#b3"">(Jain and Wallace, 2019)</ref> points to possible pitfalls that may cause researchers to misapply attention scores as ex lns=""http://www.tei-c.org/ns/1.0""><head n=""2"">Attention Might be Explanation</head><p>In this section, we briefly describe the experimental design of <ref type=""bibr"" target=""#b3"">Jain and Wallace (2019)</ref> and look at the results they provide to support their claim that 'Attention is not explana",1
"their suitability for providing explanations for model predictions is a topic of high interest <ref type=""bibr"" target=""#b17"">(Xu et al., 2015;</ref><ref type=""bibr"" target=""#b11"">Rocktäschel et al., 2015;</ref><ref type=""bibr"" target=""#b8"">Mullenbach et al., 2018;</ref><ref type=""bibr"" target=""#b1",0
"ileDesc> 	</teiHeader> 	<text xml:lang=""en""> 		<body> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""1"">Introduction</head><p>Attention mechanisms <ref type=""bibr"" target=""#b0"">(Bahdanau et al., 2014)</ref> are nowadays ubiquitous in NLP, and their suitability for providing explanations for model ef type=""table"" target=""#tab_0"">1</ref>.</p><p>We use a single-layer bidirectional LSTM with tanh activation, followed by an additive attention layer <ref type=""bibr"" target=""#b0"">(Bahdanau et al., 2014)</ref> and softmax prediction, which is equivalent to the LSTM setup of Jain and Wallace. We use",0
"tional rationale-generation capability. The authors show that rationales generated in a post-hoc manner increase user trust in a system.</p><p>Citing <ref type=""bibr"" target=""#b12"">Ross et al. (2017)</ref>, Jain and Wallace's requisite for attention distributions to be used as explanation is that th",0
"eddings were analyzed in <ref type=""bibr"" target=""#b11"">[12]</ref>, which also proposed a combinatorial algorithm to compute embeddings. Ganea et al. <ref type=""bibr"" target=""#b16"">[17]</ref> and Gulcehre et al. <ref type=""bibr"" target=""#b20"">[21]</ref> proposed hyperbolic neural networks and hyperb e input features of the GCN and an exponential map taking its outputs. An alternative to prevent such collapse would be to introduce bias terms as in <ref type=""bibr"" target=""#b16"">[17]</ref>. Importantly, when applying the non-linearity directly on a manifold M, we need to ensure that its applicati ormula_13"">x ⊕ y = (1 + 2 x, y + y 2 )x + (1 − x 2 )y 1 + 2 x, y + x 2 y 2<label>(8)</label></formula><p>Similar to the Euclidean case, and following <ref type=""bibr"" target=""#b16"">[17]</ref>, we use x = x 0 . On the Poincaré ball, we employ pointwise non-linearities which are norm decreasing, i.e.,",1
"kchain graph and its dynamics over time have been used as a way of quantifying the ""true value"" of a network <ref type=""bibr"" target=""#b48"">[49,</ref><ref type=""bibr"" target=""#b46"">47]</ref>. Blockchain networks have uncharacteristic dynamics <ref type=""bibr"" target=""#b29"">[30]</ref>, but the distri",0
"perbolic geometry has attractive properties for stochastic optimization and leads to substantially improved embeddings, especially in low dimensions. <ref type=""bibr"" target=""#b15"">[16]</ref> extended Poincaré embeddings to directed graphs using hyperbolic entailment cones. The representation trade-",0
"rlying generation algorithm. We choose 3 distinct graph generation algorithms: Erdős-Rényi <ref type=""bibr"" target=""#b14"">[15]</ref>, Barabási-Albert <ref type=""bibr"" target=""#b1"">[2]</ref> and Watts-Strogatz <ref type=""bibr"" target=""#b45"">[46]</ref> (see Figure <ref type=""figure"">1</ref>).</p><p>Th",0
"ceived increased attention in machine learning and artificial intelligence due to their attractive properties for learning from graph-structured data <ref type=""bibr"" target=""#b6"">[7]</ref>. Originally proposed by <ref type=""bibr"" target=""#b18"">[19,</ref><ref type=""bibr"" target=""#b40"">41]</ref> as a",0
"ale annotated data for NLP tasks in the scientific domain is challenging and expensive. We release SCIBERT, a pretrained language model based on BERT <ref type=""bibr"" target=""#b5"">(Devlin et al., 2019)</ref> to address the lack of highquality, large-scale labeled scientific data. SCIBERT leverages u own through ELMo <ref type=""bibr"" target=""#b24"">(Peters et al., 2018)</ref>, GPT <ref type=""bibr"" target=""#b25"">(Radford et al., 2018)</ref> and BERT <ref type=""bibr"" target=""#b5"">(Devlin et al., 2019)</ref>, unsupervised pretraining of language models on large corpora significantly improves perform esults on many of these tasks.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2"">Methods</head><p>Background The BERT model architecture <ref type=""bibr"" target=""#b5"">(Devlin et al., 2019)</ref> is based on a multilayer bidirectional Transformer <ref type=""bibr"" target=""#b28"">(Vaswani e </div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.3"">Pretrained BERT Variants</head><p>BERT-Base We use the pretrained weights for BERT-Base <ref type=""bibr"" target=""#b5"">(Devlin et al., 2019)</ref> released with the original BERT code. <ref type=""foot"" target=""#foot_3"">4</ref> The vocabula ed triangular schedule <ref type=""bibr"" target=""#b9"">(Howard and Ruder, 2018)</ref> which is equivalent to the linear warmup followed by linear decay <ref type=""bibr"" target=""#b5"">(Devlin et al., 2019)</ref>. For each dataset and BERT variant, we pick the best learning rate and number of epochs on t s (Sections 3.4 and 3.5) are implemented in PyTorch using AllenNLP <ref type=""bibr"" target=""#b8"">(Gardner et al., 2017)</ref>.</p><p>Casing We follow <ref type=""bibr"" target=""#b5"">Devlin et al. (2019)</ref> in using the cased models for NER and the uncased models for all other tasks. We also use the www.tei-c.org/ns/1.0""><head n=""3.4"">Finetuning BERT</head><p>We mostly follow the same architecture, optimization, and hyperparameter choices used in <ref type=""bibr"" target=""#b5"">Devlin et al. (2019)</ref>. For text classification (i.e. CLS and REL), we feed the final BERT vector for the [CLS] toke",1
"Extension, etc.)</ref> to sentences from scientific papers that cite other papers. The Paper Field dataset is built from the Microsoft Academic Graph <ref type=""bibr"" target=""#b27"">(Sinha et al., 2015)</ref> <ref type=""foot"" target=""#foot_2"">3</ref> and maps paper titles to one of 7 fields of study.",0
"ted Work</head><p>Recent work on domain adaptation of BERT includes BIOBERT <ref type=""bibr"" target=""#b18"">(Lee et al., 2019)</ref> and CLINI-CALBERT <ref type=""bibr"" target=""#b0"">(Alsentzer et al., 2019;</ref><ref type=""bibr"" target=""#b10"">Huang et al., 2019)</ref>. BIOBERT is trained on PubMed abs",0
"target=""#b23"">(Li et al., 2010)</ref>, hybrid segmentation <ref type=""bibr"" target=""#b37"">(Wang et al., 2018)</ref> and multi-resolution segmentation <ref type=""bibr"" target=""#b0"">(Benz et al., 2004)</ref> have been widely used to generate image objects in GEOBIA. Most of these segmentation algorith nd Argialas, 2006</ref>) is used to generate the initial segmentation results for HR images. MSEG is developed from the multi-resolution segmentation <ref type=""bibr"" target=""#b0"">(Benz et al., 2004)</ref>. It initially generates an oversegmentation result and then merge the segmentation objects in segmented phenomenon could be observed.</p><p>Although the segmentation algorithm used in this study is based on multi-resolution segmentation (MSEG) <ref type=""bibr"" target=""#b0"">(Benz et al. (2004)</ref>; <ref type=""bibr"" target=""#b36"">Tzotsos and Argialas, 2006)</ref>, the proposed method is impl",1
"ocal spectral heterogeneity measure</head><p>Spectral angle is a commonly used distance metric to measure the difference between two spectral vectors <ref type=""bibr"" target=""#b22"">(Kruse et al., 1993)</ref>. The reflectance of individual pixel is represented as a B dimensional spectral vector, wher",1
"<div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2.1."">Multiscale segmentation used</head><p>A region based multiscale image segmentation named MSEG <ref type=""bibr"" target=""#b36"">(Tzotsos and Argialas, 2006</ref>) is used to generate the initial segmentation results for HR images. MSEG is develope has a size of × 700 700 pixels. Both Scene C and D have a size of × 500 500 pixels. The multiscale segmentation used in this study is MSEG algorithm <ref type=""bibr"" target=""#b36"">(Tzotsos and Argialas, 2006)</ref>, which is implemented under C++ environment in a computer with an intel i7-6700 CPU e segmentation algorithm used in this study is based on multi-resolution segmentation (MSEG) <ref type=""bibr"" target=""#b0"">(Benz et al. (2004)</ref>; <ref type=""bibr"" target=""#b36"">Tzotsos and Argialas, 2006)</ref>, the proposed method is implemented independently and can be applied in other multisc",1
"meter, the cross-scale strategy has emerged and drawn attention in the scale optimization <ref type=""bibr"" target=""#b20"">(Johnson and Xie, 2011;</ref><ref type=""bibr"" target=""#b48"">Zhang et al., 2015b;</ref><ref type=""bibr"" target=""#b5"">Chen et al., 2009)</ref>. This strategy aims at fusing the mult",0
"y ignored. Therefore, the (Geographic) Object-Based Image Analysis (OBIA or GEOBIA) has emerged and become an effective approach in HR image analysis <ref type=""bibr"" target=""#b16"">(Hossain and Chen, 2019;</ref><ref type=""bibr"" target=""#b27"">Ma et al., 2017;</ref><ref type=""bibr"" target=""#b1"">Blasch",0
"unsupervised approaches do not use reference objects and tend to design a measure to evaluate the quality of segmentation results at different scales <ref type=""bibr"" target=""#b10"">(Espindola et al., 2006;</ref><ref type=""bibr"" target=""#b21"">Karl and Maurer, 2010;</ref><ref type=""bibr"" target=""#b31""",0
"t al., 2018)</ref>. The purpose of GEOBIA is to address the complicated spatial and hierarchical relationships in the HR image classification process <ref type=""bibr"" target=""#b26"">(Ma et al., 2015;</ref><ref type=""bibr"" target=""#b9"">Dronova, 2015;</ref><ref type=""bibr"" target=""#b7"">Costa et al., 20",0
"ype=""bibr"" target=""#b13"">(Gonçalves et al., 2019)</ref>.</p><p>There are a number of HR image segmentation algorithms proposed and utilized in GEOBIA <ref type=""bibr"" target=""#b24"">(Liu et al., 2015;</ref><ref type=""bibr"" target=""#b14"">Grinias et al., 2016;</ref><ref type=""bibr"" target=""#b35"">Troya-",0
"ibr"" target=""#b35"">Troya-Galvis et al., 2018;</ref><ref type=""bibr"" target=""#b6"">Comaniciu and Meer, 2002)</ref>. For example, watershed segmentation <ref type=""bibr"" target=""#b23"">(Li et al., 2010)</ref>, hybrid segmentation <ref type=""bibr"" target=""#b37"">(Wang et al., 2018)</ref> and multi-resolut ther multiscale segmentation algorithms, such as mean shift <ref type=""bibr"" target=""#b6"">(Comaniciu and Meer, 2002)</ref> and watershed segmentation <ref type=""bibr"" target=""#b23"">(Li et al., 2010)</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""5."">Conclusion</head><p>Segmentati",0
"1"">Stein and de Beurs, 2005;</ref><ref type=""bibr"" target=""#b49"">Zhou et al., 2017;</ref><ref type=""bibr"" target=""#b12"">Georganos et al., 2018b;</ref><ref type=""bibr"" target=""#b17"">Hu et al., 2016)</ref>. Therefore, it is a key step to design an appropriate measurement for optimization methods. For",0
""" target=""#b19"">Hu et al., 2018)</ref>. Theses scale optimization methods usually can be divided into two groups: supervised and unsupervised methods <ref type=""bibr"" target=""#b15"">(Grybas et al., 2017)</ref>. Supervised scale optimization methods are designed to measure the similarity between segme",0
"regions. These regions are recognized as objects of GEOBIA and usually represent a kind of land covers such as farm lands, buildings and water bodies <ref type=""bibr"" target=""#b42"">(Yang et al., 2015a)</ref>. Therefore, the segmentation performance has a critical effect on the later steps of GEOBIA,",0
".</p><p>There are a number of HR image segmentation algorithms proposed and utilized in GEOBIA <ref type=""bibr"" target=""#b24"">(Liu et al., 2015;</ref><ref type=""bibr"" target=""#b14"">Grinias et al., 2016;</ref><ref type=""bibr"" target=""#b35"">Troya-Galvis et al., 2018;</ref><ref type=""bibr"" target=""#b6""",0
"thms proposed and utilized in GEOBIA <ref type=""bibr"" target=""#b24"">(Liu et al., 2015;</ref><ref type=""bibr"" target=""#b14"">Grinias et al., 2016;</ref><ref type=""bibr"" target=""#b35"">Troya-Galvis et al., 2018;</ref><ref type=""bibr"" target=""#b6"">Comaniciu and Meer, 2002)</ref>. For example, watershed s",0
"ds on the trial-and-error method.</p><p>To select an appropriate scale for segmentation, researchers proposed a variety of scale optimization methods <ref type=""bibr"" target=""#b11"">(Georganos et al., 2018a;</ref><ref type=""bibr"" target=""#b19"">Hu et al., 2018)</ref>. Theses scale optimization methods",0
"antly, there are fast approximations for both PPR <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b75"">76]</ref> and the heat kernel <ref type=""bibr"" target=""#b33"">[34]</ref>, with which GDC achieves a linear runtime O(N ). Furthermore, top-k truncation generates a regular graph, wh f><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" target=""#b36"">37]</ref>, especially for clustering <ref type=""bibr"" target=""#b33"">[34]</ref>, semi-supervised classification <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b21"">22]<",1
"cases.</head><p>Two popular examples of graph diffusion are personalized PageRank (PPR) <ref type=""bibr"" target=""#b55"">[56]</ref> and the heat kernel <ref type=""bibr"" target=""#b35"">[36]</ref>. PPR corresponds to choosing T = T rw and θ PPR k = α(1 − α) k , with teleport probability α ∈ (0, 1) <ref t ndom walks have been extensively studied in classical graph learning <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" target=""#b36"">37]</ref>, especially for clustering <ref type=""bibr"" target=""#b33"">[34]</ref>,",1
"iciently and accurately in linear time and space. Most importantly, there are fast approximations for both PPR <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b75"">76]</ref> and the heat kernel <ref type=""bibr"" target=""#b33"">[34]</ref>, with which GDC achieves a linear runtime O(N )",1
"]</ref>, especially for clustering <ref type=""bibr"" target=""#b33"">[34]</ref>, semi-supervised classification <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b21"">22]</ref>, and recommendation systems <ref type=""bibr"" target=""#b42"">[43]</ref>. For an overview of existing methods se ype=""bibr"" target=""#b42"">[43]</ref>. For an overview of existing methods see Masuda et al. <ref type=""bibr"" target=""#b44"">[45]</ref> and Fouss et al. <ref type=""bibr"" target=""#b21"">[22]</ref>.</p><p>The first models similar in structure to current Graph Neural Networks (GNNs) were proposed by Sperdu sification and regression. Promising extensions include other diffusion coefficients θ k such as those given by the methods presented in Fouss et al. <ref type=""bibr"" target=""#b21"">[22]</ref> and more advanced random walks and operators that are not defined by powers of a transition matrix.</p></div",0
"6,</ref><ref type=""bibr"" target=""#b36"">37]</ref>, especially for clustering <ref type=""bibr"" target=""#b33"">[34]</ref>, semi-supervised classification <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b21"">22]</ref>, and recommendation systems <ref type=""bibr"" target=""#b42"">[43]</ref",0
"overcome. One way of extending GDC to heterophily, i.e. ""opposites attract"", might be negative edge weights <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b42"">43]</ref>. Furthermore, we suspect that GDC does not perform well in settings with more complex edges (e.g. knowledge g ]</ref>, semi-supervised classification <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b21"">22]</ref>, and recommendation systems <ref type=""bibr"" target=""#b42"">[43]</ref>. For an overview of existing methods see Masuda et al. <ref type=""bibr"" target=""#b44"">[45]</ref> and Fouss e",0
"bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b31"">32,</ref><ref type=""bibr"" target=""#b39"">40]</ref>, which are based on the eigendecomposition of the graph Laplacian, and spatial-based methods <ref type=""bibr""",0
"in classical graph learning <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" target=""#b36"">37]</ref>, especially for clustering <ref type=""bibr"" target=""#b33"">[34]</ref>, semi-supervised classification <ref typ",0
"and finally generalized to consider all exponents at once, using the adjacency matrix's dominant eigenvector <ref type=""bibr"" target=""#b37"">[38,</ref><ref type=""bibr"" target=""#b73"">74]</ref>. The field of Graph Neural Networks (GNNs) is currently in a similar state. Graph Convolutional Networks (GCN",0
"h features and edges in real graphs are often noisy. Previous works also highlighted the effectiveness of graph denoising. Berberidis &amp; Giannakis <ref type=""bibr"" target=""#b6"">[7]</ref> showed that PPR is able to reconstruct the underlying probability matrix of a sampled stochastic block model (",0
"ents analogous to θ k as part of their training process. We investigated this approach using label propagation <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b12"">13]</ref> and node embedding models <ref type=""bibr"" target=""#b0"">[1]</ref>. However, we found that the simple coeffici e also tried obtaining θ k from models that learn analogous coefficients <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b12"">13]</ref>. However, we found that θ k obtained by these models tend to converge to a minimal neighborhood, i.e. they co ://www.tei-c.org/ns/1.0""><head n=""5"">Related work</head><p>Graph diffusion and random walks have been extensively studied in classical graph learning <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" target=""#b36",0
"in many graph-related tasks <ref type=""bibr"" target=""#b18"">[19,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" target=""#b40"">41,</ref><ref type=""bibr"" target=""#b80"">81]</ref>. In general, GNNs are classified into spectral-based models <ref type=""bibr"" target=""#b10"">[11,</ref><ref typ nd PAN <ref type=""bibr"" target=""#b43"">[44]</ref> the transition matrix of maximal entropy random walks to aggregate over nodes in each layer, PinSage <ref type=""bibr"" target=""#b80"">[81]</ref> uses random walks for neighborhood aggregation, and MixHop <ref type=""bibr"" target=""#b1"">[2]</ref> concatena",0
"ef type=""bibr"" target=""#b17"">(Hochreiter and Schmidhuber, 1997)</ref> system can readily outperform the previous state-of-the-art system, CogCompTime <ref type=""bibr"" target=""#b32"">(Ning et al., 2018d)</ref>, by a large margin. The fact that a standard LSTM system can significantly improve over a fe .0"" type=""table"" xml:id=""tab_3""><head>Table 2 :</head><label>2</label><figDesc>Performances on the MATRES test set (i.e., the PT section). CogCompTime<ref type=""bibr"" target=""#b32"">(Ning et al., 2018d)</ref> is the previous state-of-the-art feature-based system.</figDesc><table /><note>Position indi S.<ref type=""foot"" target=""#foot_6"">7</ref> Note that in Table <ref type=""table"" target=""#tab_2"">2</ref>, CogCompTime performed slightly different to <ref type=""bibr"" target=""#b32"">Ning et al. (2018d)</ref>: Cog-CompTime reportedly had F 1 =65.9 (Table <ref type=""table"" target=""#tab_2"">2</ref> Line : Cog-CompTime reportedly had F 1 =65.9 (Table <ref type=""table"" target=""#tab_2"">2</ref> Line 3 therein) and here we obtained F 1 =66.6. In addition, <ref type=""bibr"" target=""#b32"">Ning et al. (2018d)</ref> only reported F 1 scores, while we also use another two metrics for a more thorough compariso",1
"n=""1"">Introduction</head><p>Temporal relation (TempRel) extraction has been considered as a major component of understanding time in natural language <ref type=""bibr"" target=""#b15"">(Do et al., 2012;</ref><ref type=""bibr"">Uz-Zaman et al., 2013;</ref><ref type=""bibr"" target=""#b26"">Minard et al., 2015; f><ref type=""bibr"" target=""#b33"">O'Gorman et al., 2016)</ref>, structured inference <ref type=""bibr"" target=""#b8"">(Chambers and Jurafsky, 2008a;</ref><ref type=""bibr"" target=""#b15"">Do et al., 2012;</ref><ref type=""bibr"" target=""#b7"">Chambers et al., 2014;</ref><ref type=""bibr"" target=""#b29"">Ning et procedure used in many existing works to enforce the transitivity property of time <ref type=""bibr"" target=""#b9"">(Chambers and Jurafsky, 2008b;</ref><ref type=""bibr"" target=""#b15"">Do et al., 2012;</ref><ref type=""bibr"" target=""#b28"">Ning et al., 2017</ref>). An overview of the proposed network stru",0
"s to TempRel extraction include <ref type=""bibr"" target=""#b23"">Mani et al. (2006)</ref>; <ref type=""bibr"" target=""#b10"">Chambers et al. (2007)</ref>; <ref type=""bibr"" target=""#b1"">Bethard et al. (2007)</ref>; <ref type=""bibr"" target=""#b41"">Verhagen and Pustejovsky (2008)</ref>, which aimed at buildi",0
"or showed only moderate improvements <ref type=""bibr"" target=""#b14"">(Dligach et al., 2017;</ref><ref type=""bibr"" target=""#b21"">Lin et al., 2017;</ref><ref type=""bibr"" target=""#b24"">Meng and Rumshisky, 2018)</ref>. We think it is important for to understand: is it because we missed a ""magic"" neural a ., 2017;</ref><ref type=""bibr"" target=""#b38"">Tourille et al., 2017)</ref> and in newswire <ref type=""bibr"" target=""#b11"">(Cheng and Miyao, 2017;</ref><ref type=""bibr"" target=""#b24"">Meng and Rumshisky, 2018;</ref><ref type=""bibr"" target=""#b20"">Leeuwenberg and Moens, 2018)</ref>. However, their improv",0
""">Han et al. (2018)</ref> present a relation classification dataset -FewRel, and adapt most recent state-of-the-art few-shot learning methods for it, <ref type=""bibr"" target=""#b3"">Gao et al. (2019)</ref> propose a hybrid attention-based prototypical networks for noisy few-shot relation classificatio ture level space, and other features are confusing and useless at the same time.</p><p>So we apply a CNN-based feature attention mechanism similar to <ref type=""bibr"" target=""#b3"">Gao et al. (2019)</ref> proposed as a class feature extractor. It depends on all the instances in the support set of eac by <ref type=""bibr"" target=""#b13"">Snell et al. (2017)</ref> which includes Finetune, kNN, MetaN, GNN, and SNAIL, then we cite the results reported by <ref type=""bibr"" target=""#b3"">Gao et al. (2019)</ref> which includes Proto and PHATT. For a fair comparison, in our model, we use the same word embedd 0 way 5 shot settings on FewRel test set.</figDesc><table /><note>* reported by<ref type=""bibr"" target=""#b5"">Han et al. (2018)</ref> and ◇ reported by<ref type=""bibr"" target=""#b3"">Gao et al. (2019)</ref>.</note></figure> 			<note xmlns=""http://www.tei-c.org/ns/1.0"" place=""foot"" n=""1"" xml:id=""foot_0"" chieved excellent performance in few-shot image classification and few-shot text classification <ref type=""bibr"" target=""#b5"">(Han et al., 2018;</ref><ref type=""bibr"" target=""#b3"">Gao et al., 2019)</ref> tasks respectively, so our model is based on prototypical networks and aims to get promotion. Th 8)</ref>, SNAIL <ref type=""bibr"" target=""#b10"">(Mishra et al., 2018)</ref>, Proto <ref type=""bibr"" target=""#b13"">(Snell et al., 2017)</ref> and PHATT <ref type=""bibr"" target=""#b3"">(Gao et al., 2019)</ref> respectively.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""5.3"">Implementation d",1
"he best weighted combination from a set of metrics obtained from meta-training tasks for a newly seen few-shot task such as intention classification, <ref type=""bibr"" target=""#b5"">Han et al. (2018)</ref> present a relation classification dataset -FewRel, and adapt most recent state-of-the-art few-sh and speed of the model, we achieve this part with one layer convolutional neural networks (CNN). For ease of comparison, its details are the same as <ref type=""bibr"" target=""#b5"">Han et al. (2018)</ref> proposed. Hierarchical Attention In order to get more important information from rare data, we a ines, they learn the parameters on the support set with the CNN encoder. For the neural networks based baselines, we use the same hyper parameters as <ref type=""bibr"" target=""#b5"">Han et al. (2018)</ref> proposed.</p><p>For our hierarchical attention prototypical networks, the window size of the CNN :</head><label>4</label><figDesc>Accuracies (%) for 5 way 5 shot and 10 way 5 shot settings on FewRel test set.</figDesc><table /><note>* reported by<ref type=""bibr"" target=""#b5"">Han et al. (2018)</ref> and ◇ reported by<ref type=""bibr"" target=""#b3"">Gao et al. (2019)</ref>.</note></figure> 			<note =""bibr"" target=""#b13"">(Snell et al., 2017)</ref> has achieved excellent performance in few-shot image classification and few-shot text classification <ref type=""bibr"" target=""#b5"">(Han et al., 2018;</ref><ref type=""bibr"" target=""#b3"">Gao et al., 2019)</ref> tasks respectively, so our model is based rgence speed of the model.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""5.1"">Datasets</head><p>FewRel Few-Shot Relation Classification <ref type=""bibr"" target=""#b5"">(Han et al., 2018)</ref>  </p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""5.2"">Baselines</head><p>Firstly,",1
"train a neural network with a few parameters using few data but achieve good performance. A typical example of this approach is prototypical networks <ref type=""bibr"" target=""#b13"">(Snell et al., 2017)</ref>, which averages the vector of few support instances as the class prototype and computes dist s parameters to be learned.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.3"">Prototypical Networks</head><p>The prototypical networks <ref type=""bibr"" target=""#b13"">(Snell et al., 2017)</ref> has achieved excellent performance in few-shot image classification and few-shot text classi 017)</ref>, GNN <ref type=""bibr"" target=""#b4"">(Garcia and Bruna, 2018)</ref>, SNAIL <ref type=""bibr"" target=""#b10"">(Mishra et al., 2018)</ref>, Proto <ref type=""bibr"" target=""#b13"">(Snell et al., 2017)</ref> and PHATT <ref type=""bibr"" target=""#b3"">(Gao et al., 2019)</ref> respectively.</p></div> <di l labeled support set and an unlabelled example to its label, and obviate the need for fine-tuning to adapt to new class types. Prototypical networks <ref type=""bibr"" target=""#b13"">(Snell et al., 2017</ref>) learns a metric space in which the model can perform well by computing distance between quer target query q</p><formula xml:id=""formula_9"">p θ (y = l i q) = exp(−d(g θ (q), c i ) Σ L l=1 exp(−d(g θ (q), c l )<label>(9)</label></formula><p>As <ref type=""bibr"" target=""#b13"">Snell et al. (2017)</ref> mentioned, squared Euclidean distance is a reasonable choice, however, we will introduce a mo >We compare our models with seven baselines, and the implementation details are as follows.</p><p>For FewRel dataset, we cite the results reported by <ref type=""bibr"" target=""#b13"">Snell et al. (2017)</ref> which includes Finetune, kNN, MetaN, GNN, and SNAIL, then we cite the results reported by <re",1
"t text classification models in deep learning <ref type=""bibr"" target=""#b7"">(Kim, 2014;</ref><ref type=""bibr"" target=""#b23"">Zhang et al., 2015a;</ref><ref type=""bibr"" target=""#b20"">Yang et al., 2016;</ref><ref type=""bibr"" target=""#b18"">Wang et al., 2018)</ref> require a considerable amount of labele an instance is unequal, thus it is worth pointing out which words are useful and which words are useless. Therefore, we apply an attention mechanism <ref type=""bibr"" target=""#b20"">(Yang et al., 2016)</ref> to get those important words and assemble them to compose a more informative instance vector",1
"bibr"" target=""#b1"">Caruana (1994)</ref> and <ref type=""bibr"" target=""#b0"">Bengio (2011)</ref> adopt the target task from the pre-trained models. Then <ref type=""bibr"" target=""#b8"">Koch et al. (2015)</ref> explore a method for learning siamese neural networks which employs an unique structure to rank",0
"odels such as Finetune and kNN, Then we compare our model with five state-of-the-art fewshot learning models based on neural networks, they are MetaN <ref type=""bibr"" target=""#b11"">(Munkhdalai and Yu, 2017)</ref>, GNN <ref type=""bibr"" target=""#b4"">(Garcia and Bruna, 2018)</ref>, SNAIL <ref type=""bib",0
", and it can apply to unseen classes. The early works aim to use transfer learning approaches, <ref type=""bibr"" target=""#b1"">Caruana (1994)</ref> and <ref type=""bibr"" target=""#b0"">Bengio (2011)</ref> adopt the target task from the pre-trained models. Then <ref type=""bibr"" target=""#b8"">Koch et al. (2",0
""">Koch et al. (2015)</ref> explore a method for learning siamese neural networks which employs an unique structure to rank similarity between inputs. <ref type=""bibr"" target=""#b17"">Vinyals et al. (2016)</ref> use matching networks to map a small labeled support set and an unlabelled example to its l in to optimize parameters, D validation to select best hyper parameters, and D test to evaluate the model.</p><p>The ""episode"" training strategy that <ref type=""bibr"" target=""#b17"">Vinyals et al. (2016)</ref> proposed has proved to be effective. For each training episode, we first sample a label set",0
"nal methods mainly focus on feature engineerings such as bagof-words or n-grams <ref type=""bibr"" target=""#b19"">(Wang and Manning, 2012)</ref> or SVMs <ref type=""bibr"" target=""#b16"">(Tang et al., 2015)</ref>. The neural network based methods like <ref type=""bibr"" target=""#b7"">Kim (2014)</ref>  </p></",0
"state-of-the-art fewshot learning models based on neural networks, they are MetaN <ref type=""bibr"" target=""#b11"">(Munkhdalai and Yu, 2017)</ref>, GNN <ref type=""bibr"" target=""#b4"">(Garcia and Bruna, 2018)</ref>, SNAIL <ref type=""bibr"" target=""#b10"">(Mishra et al., 2018)</ref>, Proto <ref type=""bibr""",0
"fair comparison, in our model, we use the same word embeddings and hyperparameters of instance encoder as PHATT proposed. In detail, we use the Glove <ref type=""bibr"" target=""#b12"">(Pennington et al., 2014)</ref> consisting of 6B tokens and 400K vocabulary as our initialized word representation, and",0
"2018)</ref> propose a two-branch relation networks, which learns to compare query against few-shot labeled sample support data. Dual TriNet structure <ref type=""bibr"" target=""#b2"">(Chen et al., 2018)</ref> can efficiently and directly augment multi-layer visual features to boost the few-shot classif",0
"training a classifier with few instances in each class, and it can apply to unseen classes. The early works aim to use transfer learning approaches, <ref type=""bibr"" target=""#b1"">Caruana (1994)</ref> and <ref type=""bibr"" target=""#b0"">Bengio (2011)</ref> adopt the target task from the pre-trained mo",0
"age Processing, and many models are proposed to solve it. The traditional methods mainly focus on feature engineerings such as bagof-words or n-grams <ref type=""bibr"" target=""#b19"">(Wang and Manning, 2012)</ref> or SVMs <ref type=""bibr"" target=""#b16"">(Tang et al., 2015)</ref>. The neural network bas",0
"org/ns/1.0""><head n=""1"">Introduction</head><p>The dominant text classification models in deep learning <ref type=""bibr"" target=""#b7"">(Kim, 2014;</ref><ref type=""bibr"" target=""#b23"">Zhang et al., 2015a;</ref><ref type=""bibr"" target=""#b20"">Yang et al., 2016;</ref><ref type=""bibr"" target=""#b18"">Wang et",0
"mension.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.2.2"">Encoding Layer</head><p>Following we apply a convolutional neural network <ref type=""bibr"" target=""#b22"">Zeng et al. (2014)</ref> as encoding layer to get the hidden annotations of each word by a convolution kernel with the",0
"perform well by computing distance between query and prototype representations of each class and classify the query to the nearest prototype's class. <ref type=""bibr"" target=""#b15"">Sung et al. (2018)</ref> propose a two-branch relation networks, which learns to compare query against few-shot labeled",0
"ng=""en""> 		<body> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""1"">Introduction</head><p>The dominant text classification models in deep learning <ref type=""bibr"" target=""#b7"">(Kim, 2014;</ref><ref type=""bibr"" target=""#b23"">Zhang et al., 2015a;</ref><ref type=""bibr"" target=""#b20"">Yang et al., 20 target=""#b19"">(Wang and Manning, 2012)</ref> or SVMs <ref type=""bibr"" target=""#b16"">(Tang et al., 2015)</ref>. The neural network based methods like <ref type=""bibr"" target=""#b7"">Kim (2014)</ref>  </p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2.2"">Few-Shot Learning</head><p>Few-Shot",0
"://www.tei-c.org/ns/1.0""><head n=""4.1"">Encoder Module</head><p>This module is a bi-direction recurrent neural network with self-attention as shown in <ref type=""bibr"" target=""#b10"">Lin et al. (2017)</ref>. Given an input text x = (w 1 , w 2 , ..., w T ), represented by a sequence of word embeddings.",1
"Munkhdalai and Yu, 2017)</ref>, and fine-tuning the target problem <ref type=""bibr"" target=""#b14"">(Qi et al., 2018)</ref>. The approaches proposed by <ref type=""bibr"" target=""#b18"">Snell et al. (2017)</ref> and <ref type=""bibr"" target=""#b20"">Sung et al. (2018)</ref>, which combine non-parametric met w.tei-c.org/ns/1.0""><head n=""3.1"">Few-Shot Classification</head><p>Few-shot classification <ref type=""bibr"" target=""#b21"">(Vinyals et al., 2016;</ref><ref type=""bibr"" target=""#b18"">Snell et al., 2017)</ref> is a task in which a classifier must be adapted to accommodate new classes not seen in traini ate 5-shot learning models on this dataset. Matching Networks <ref type=""bibr"" target=""#b21"">(Vinyals et al., 2016)</ref> 65.73 Prototypical Networks <ref type=""bibr"" target=""#b18"">(Snell et al., 2017)</ref> 68.17 Graph Network <ref type=""bibr"" target=""#b5"">(Garcia and Bruna, 2017)</ref> 82.61 Relat ""bibr"" target=""#b21"">(Vinyals et al., 2016)</ref>.</p><p>• Prototypical Networks: a deep metric-based method using sample average as class prototypes <ref type=""bibr"" target=""#b18"">(Snell et al., 2017)</ref>.</p><p>• Graph Network: a graph-based few-shot learning model that implements a task-driven experiments.</p><p>Evaluation Methods We evaluate the performance by few-shot classification accuracy following previous studies in few-shot learning <ref type=""bibr"" target=""#b18"">(Snell et al., 2017;</ref><ref type=""bibr"" target=""#b20"">Sung et al., 2018)</ref>. To evaluate the proposed model with",1
"techniques to alleviate the overfitting problem caused by data sparseness, only to a limited extent. Instead, researchers have explored meta-learning <ref type=""bibr"" target=""#b4"">(Finn et al., 2017)</ref> to leverage the distribution over similar tasks, inspired by human learning. Contemporary appr )</ref>. The authors combined generative models with complex iterative inference strategies. More recently, many approaches have used a meta-learning <ref type=""bibr"" target=""#b4"">(Finn et al., 2017;</ref><ref type=""bibr"" target=""#b11"">Mishra et al., 2018)</ref> strategy in the sense that they extra",0
"ant problems, including the imposed strong priors <ref type=""bibr"" target=""#b3"">(Fei-Fei et al., 2006)</ref>, complex gradient transfer between tasks <ref type=""bibr"" target=""#b12"">(Munkhdalai and Yu, 2017)</ref>, and fine-tuning the target problem <ref type=""bibr"" target=""#b14"">(Qi et al., 2018)</r",0
"tion (ARSC) Following <ref type=""bibr"" target=""#b27"">Yu et al. (2018)</ref>, we use the multiple tasks with the multi-domain sentiment classification <ref type=""bibr"" target=""#b0"">(Blitzer et al., 2007)</ref> dataset. The dataset comprises English reviews for 23 types of products on Amazon. For each",0
"very few labeled examples. The limitation of only one or very few examples challenges the standard fine-tuning method in deep learning. Early studies <ref type=""bibr"" target=""#b17"">(Salamon and Bello, 2017)</ref> applied data augmentation and regularization techniques to alleviate the overfitting pr",0
"hierarchical representations of support sets and dynamically induce sample representations to class representations.</p><p>Recently, capsule network <ref type=""bibr"" target=""#b16"">(Sabour et al., 2017)</ref> has been proposed, which possesses the exciting potential to address the aforementioned iss ss vector c i :</p><formula xml:id=""formula_3"">e s ij ∈ R 2u i=1,...C,j=1...K → c i ∈ R 2u C i=1 .</formula><p>We apply the dynamic routing algorithm <ref type=""bibr"" target=""#b16"">(Sabour et al., 2017)</ref> in this module, in the situation where the number of the output capsule is one. In order to on two datasets.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2.2"">Capsule Network</head><p>The Capsule Network was first proposed by <ref type=""bibr"" target=""#b16"">Sabour et al. (2017)</ref>, which allowed the network to learn robustly the invariants in part-whole relationships. Lat",0
"target=""#b21"">(Vinyals et al., 2016)</ref> 65.73 Prototypical Networks <ref type=""bibr"" target=""#b18"">(Snell et al., 2017)</ref> 68.17 Graph Network <ref type=""bibr"" target=""#b5"">(Garcia and Bruna, 2017)</ref> 82.61 Relation Network <ref type=""bibr"" target=""#b20"">(Sung et al., 2018)</ref> 83.07 SNA )</ref>.</p><p>• Graph Network: a graph-based few-shot learning model that implements a task-driven message passing algorithm on the samplewise level <ref type=""bibr"" target=""#b5"">(Garcia and Bruna, 2017)</ref>.</p><p>• Relation Network: a few-shot learning model which uses a neural network as the d",0
"and we implemented the baseline models on ODIC with the same text encoder module.</p><p>Implementation Details We use 300-dimension Glove embeddings <ref type=""bibr"" target=""#b13"">(Pennington et al., 2014)</ref> for ARSC dataset and 300-dimension Chinese word embeddings trained by <ref type=""bibr""",0
"uction</head><p>Deep learning has achieved a great success in many fields such as computer vision, speech recognition and natural language processing <ref type=""bibr"" target=""#b8"">(Kuang et al., 2018)</ref>. However, supervised deep learning is notoriously greedy for large labeled datasets, which li",0
"s small. Figure <ref type=""figure"" target=""#fig_0"">1</ref> depicts an example of 3-shot link prediction in KGs.</p><p>To do few-shot link prediction, <ref type=""bibr"" target=""#b22"">Xiong et al. (2018)</ref> made the first trial and proposed GMatching, learning a matching metric by considering both l is performed over parameters by using above updated parameters, it's like ""a gradient through a gradient"".</p><p>As far as we know, work proposed by <ref type=""bibr"" target=""#b22"">Xiong et al. (2018)</ref> is the first research on few-shot learning for knowledge graphs. It's a metric-based model wh tp://www.tei-c.org/ns/1.0""><head n=""5.1"">Datasets and Evaluation Metrics</head><p>We use two datasets, NELL-One and Wiki-One which are constructed by <ref type=""bibr"" target=""#b22"">Xiong et al. (2018)</ref>. NELL-One and Wiki-One are derived from NELL <ref type=""bibr"" target=""#b3"">(Carlson et al., 2 ion meta further which makes the model rebase to a simple TransE embedding model, denoted as -g -r. The result under the third setting is copied from <ref type=""bibr"" target=""#b22"">Xiong et al. (2018)</ref>. It uses the triples from background graph, training tasks and one-shot training triples from ta which will be used to make a rapid update before transferring relation meta to incomplete triples during prediction.</p><p>Compared with GMatching <ref type=""bibr"" target=""#b22"">(Xiong et al., 2018</ref>) which relies on a background knowledge graph, our MetaR is independent with them, thus is mo ing methods.</p><p>Traditional embedding models are heavily rely on rich training instances <ref type=""bibr"" target=""#b26"">(Zhang et al., 2019b;</ref><ref type=""bibr"" target=""#b22"">Xiong et al., 2018)</ref>, thus are limited to do few-shot link prediction. Our MetaR is designed to fill this vulnerab rget=""#b7"">(Koch et al., 2015;</ref><ref type=""bibr"" target=""#b17"">Vinyals et al., 2016;</ref><ref type=""bibr"" target=""#b15"">Snell et al., 2017;</ref><ref type=""bibr"" target=""#b22"">Xiong et al., 2018)</ref>, which tries to learn a matching metric between query and support set generalized to all task f type=""bibr"" target=""#b7"">(Koch et al., 2015)</ref> is a typical method using symmetric twin networks to compute the metric of two inputs. GMatching <ref type=""bibr"" target=""#b22"">(Xiong et al., 2018)</ref>, the first trial on one-shot link prediction in knowledge graphs, learns a matching metric b 1-shot and 5-shot, on NELL-One and Wiki-One are shown in Table <ref type=""table"" target=""#tab_6"">4</ref>. The baseline in our experiment is GMatching <ref type=""bibr"" target=""#b22"">(Xiong et al., 2018)</ref>, which made the first trial on few-shot link prediction task and is the only method that we",1
"any KGs have been proposed <ref type=""bibr"" target=""#b18"">(Vrandei and Krtzsch, 2014;</ref><ref type=""bibr"" target=""#b0"">Bollacker et al., 2008;</ref><ref type=""bibr"" target=""#b3"">Carlson et al., 2010)</ref> and applied to various applications <ref type=""bibr"" target=""#b2"">(Bordes et al., 2014;</ref NELL-One and Wiki-One which are constructed by <ref type=""bibr"" target=""#b22"">Xiong et al. (2018)</ref>. NELL-One and Wiki-One are derived from NELL <ref type=""bibr"" target=""#b3"">(Carlson et al., 2010)</ref> and Wikidata <ref type=""bibr"" target=""#b18"">(Vrandei and Krtzsch, 2014)</ref>   graph origi",0
"ethod. (2) Model-based method <ref type=""bibr"" target=""#b14"">(Santoro et al., 2016;</ref><ref type=""bibr"" target=""#b12"">Munkhdalai and Yu, 2017;</ref><ref type=""bibr"" target=""#b11"">Mishra et al., 2018)</ref>, which uses a specially designed part like memory to achieve the ability of learning rapidly",0
"t) in short), encoding knowledge and facts in the world. Many KGs have been proposed <ref type=""bibr"" target=""#b18"">(Vrandei and Krtzsch, 2014;</ref><ref type=""bibr"" target=""#b0"">Bollacker et al., 2008;</ref><ref type=""bibr"" target=""#b3"">Carlson et al., 2010)</ref> and applied to various applicatio",0
"head><p>During training, mini-batch gradient descent is applied with batch size set as 64 and 128 for NELL-One and Wiki-One respectively. We use Adam <ref type=""bibr"" target=""#b6"">(Kingma and Ba, 2015)</ref> with the initial learning rate as 0.001 to update parameters. We set γ = 1 and β = 1. The nu",0
"of learning with hypergraphs <ref type=""bibr"" target=""#b51"">[52,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b48"">49,</ref><ref type=""bibr"" target=""#b16"">17]</ref>. A popular learning paradigm is graphbased / hypergraph-based semi-supervised learning (SSL) where the goal i pergraph structure G = (V, E) implicitly via a neural network f (G, X) <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b16"">17]</ref> (X contains the initial features on the vertices for example, text attributes for documents). While explicit eredge and treats the learning problem as a graph learning problem on the approximation. While the state-of-the-art hypergraph neural networks (HGNN) <ref type=""bibr"" target=""#b16"">[17]</ref> approximates each hyperedge by a clique and hence requires s C 2 (quadratic number of) edges for each hypere ial optimisation on realworld hypergraphs. Through detailed experimentation, we demonstrate their effectiveness compared to the state-of-the art HGNN <ref type=""bibr"" target=""#b16"">[17]</ref> and other baselines (Sections 5, and 7). • We thoroughly discuss when we prefer our methods to HGNN (Section ><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b49"">50,</ref><ref type=""bibr"" target=""#b42"">43]</ref>. Hypergraph neural networks <ref type=""bibr"" target=""#b16"">[17]</ref> and their variants <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b23"">24]</ref> use the argmax i,j∈e |S i − S j |, breaking ties randomly 2 . Our approach requires at most a linear number of edges (1 and 2|e| − 3 respectively) while HGNN <ref type=""bibr"" target=""#b16"">[17]</ref> requires a quadratic number of edges for each hyperedge. </p></div> <div xmlns=""http://www.tei-c.org/ns/1.0"" ""5.1"">Baselines</head><p>We compared HyperGCN, 1-HyperGCN and FastHyperGCN against the following baselines:</p><p>• Hypergraph neural networks (HGNN) <ref type=""bibr"" target=""#b16"">[17]</ref> uses the clique expansion <ref type=""bibr"" target=""#b51"">[52,</ref><ref type=""bibr"" target=""#b0"">1]</ref> to he generalised hypergraph Laplacian with mediators [7].Our approach requires at most a linear number of edges (1 and 2|e| − 3 respectively) while HGNN<ref type=""bibr"" target=""#b16"">[17]</ref> requires a quadratic number of edges for each hyperedge.</figDesc><graphic url=""image-2.png"" coords=""5,108.0",1
"get=""#b49"">50,</ref><ref type=""bibr"" target=""#b42"">43]</ref>. Hypergraph neural networks <ref type=""bibr"" target=""#b16"">[17]</ref> and their variants <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b23"">24]</ref> use the clique expansion to extend GCNs for hypergraphs. Powerset co",1
"ularisation in the objective <ref type=""bibr"" target=""#b50"">[51,</ref><ref type=""bibr"" target=""#b52"">53,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b47"">48]</ref>, the state-of-the-art neural methods encode the graph / hypergraph structure G = (V, E) implicitly via a neur",0
"""bibr"" target=""#b51"">[52]</ref> and has become a popular approach for learning on hypergraph-structured data <ref type=""bibr"" target=""#b38"">[39,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b49"">50,</ref><ref type=""bibr"" target=""#b42"">43]</ref>. Hypergraph neural networks <",0
"peredges do not encode similarity. Combinatorial optimisation on hypergraphs has recently been highlighted as crucial for real-world network analysis <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b35"">36]</ref>.</p><p>Methodologically, HyperGCN approximates each hyperedge of the h <ref type=""bibr"" target=""#b14"">[15]</ref>. NP-hard problems on hypergraphs have recently been highlighted as crucial for real-world network analysis <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b35"">36]</ref>. Our problem is, given a hypergraph (V, E), to find a subset W ⊆ V of",0
"ed learning (SSL) where the goal is to assign labels to initially unlabelled vertices in a graph / hypergraph <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b53"">54,</ref><ref type=""bibr"" target=""#b41"">42]</ref>. While many techniques have used explicit Laplacian regularisation in training can improve learning accuracy significantly. This topic is so popular that it has influential books <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b53"">54,</ref><ref type=""bibr"" target=""#b41"">42]</ref>.</p><p>Graph neural networks for combinatorial optimisation: Graph-ba",0
"s for different hyperedge cuts) <ref type=""bibr"" target=""#b29"">[30]</ref> and submodular function minimisation (generalises hypergraph SSL objective) <ref type=""bibr"" target=""#b26"">[27,</ref><ref type=""bibr"" target=""#b28"">29]</ref>.</p><p>• Laplacian that considers all vertices in each hyperedge (in ""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b48"">49,</ref><ref type=""bibr"" target=""#b29"">30,</ref><ref type=""bibr"" target=""#b28"">29,</ref><ref type=""bibr"" target=""#b26"">27]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""5.1"">Baselines</head><p>We compared HyperGCN, 1-",0
"get=""#b47"">48]</ref>, the state-of-the-art neural methods encode the graph / hypergraph structure G = (V, E) implicitly via a neural network f (G, X) <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b16"">17]</ref> (X contains the initial featur t Laplacian regularisation assumes similarity among vertices in each edge / hyperedge, implicit regularisation of graph convolutional networks (GCNs) <ref type=""bibr"" target=""#b24"">[25]</ref> avoids this restriction and enables application to a broader range of problems in combinatorial optimisation mpting to generalise (structured) deep neural network models to non-Euclidean domains such as graphs and manifolds. Graph convolutional network (GCN) <ref type=""bibr"" target=""#b24"">[25]</ref> defines the convolution using a simple linear function of the graph Laplacian and is shown to be effective o matrix X ∈ R N ×p . which has p-dimensional real-valued vector representations for each node v ∈ V.</p><p>The basic formulation of graph convolution <ref type=""bibr"" target=""#b24"">[25]</ref> stems from the convolution theorem <ref type=""bibr"" target=""#b32"">[33]</ref> and it can be shown that the co rned weights Θ ∈ R p×r with r hidden units is ĀXΘ , Ā = D− 1 2 Ã D− 1 2 , Ã = A + I, and Dii = N j=1 Ãij . The proof involves a renormalisation trick <ref type=""bibr"" target=""#b24"">[25]</ref> and is in the supplementary.</p><p>GCN <ref type=""bibr"" target=""#b24"">[25]</ref> The forward model for a sim I, and Dii = N j=1 Ãij . The proof involves a renormalisation trick <ref type=""bibr"" target=""#b24"">[25]</ref> and is in the supplementary.</p><p>GCN <ref type=""bibr"" target=""#b24"">[25]</ref> The forward model for a simple two-layer GCN takes the following simple form:</p><formula xml:id=""formula_2"" re details about datasets, please refer to the supplementary. We trained all methods for 200 epochs and used the same hyperparameters of a prior work <ref type=""bibr"" target=""#b24"">[25]</ref>. We report the mean test error and standard deviation over 100 different train-test splits. We sampled sets",0
"et=""#b42"">43]</ref>. Hypergraph neural networks <ref type=""bibr"" target=""#b16"">[17]</ref> and their variants <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b23"">24]</ref> use the clique expansion to extend GCNs for hypergraphs. Powerset convolutional networks <ref type=""bibr"" tar",0
"on in Equation ( <ref type=""formula"" target=""#formula_2"">1</ref>), when applied to a hypernode v ∈ V in G S , in the neural message-passing framework <ref type=""bibr"" target=""#b17"">[18]</ref> is h</p><formula xml:id=""formula_5"">(τ +1) v = σ (Θ (τ ) ) T u∈N (v) ([ Ā(τ) S ] v,u • h (τ ) u ) .</formula",0
"red computation, for example by using an acoustic model with fewer parameters. The recently-introduced factorised time-delay neural networks (TDNN-F) <ref type=""bibr"" target=""#b12"">[13]</ref> utilise half the number of parameters than the hybrid networks with comparable performance, in particular in rthogonal lowrank matrix factorisation is applied to the parameter matrices of TDNN layers, ASR performance can be improved in lowresource situations <ref type=""bibr"" target=""#b12"">[13]</ref>. Consequently, a TDNN-F acoustic model (10 time-delay layers followed by a rank reduction layer) was trained",1
"to compute acoustic features for neural network training. However, in this case three-fold data augmentation was applied prior to feature extraction <ref type=""bibr"" target=""#b20"">[21]</ref> and the acoustic features comprised 40-dimensional MFCCs (without derivatives), 3-dimensional pitch features",1
"nal LSTM). Hence, the TDNN-F models are faster to train. Our TDNN-F was trained using the lattice-free maximum mutual information objective criterion <ref type=""bibr"" target=""#b21"">[22]</ref>. No parameter tuning was performed during neural network training and the default recipe parameters were use",1
"lingual training set. To our knowledge, only one other study on Somali automatic speech recognition (ASR) has so far been described in the literature <ref type=""bibr"" target=""#b11"">[12]</ref>.</p><p>Somali is an Afroasiatic language. It is the official language of Somalia and widely used its neighbo",0
"/head><p>In countries with a well established internet infrastructure, social media has become an accepted platform for sharing opinions and concerns <ref type=""bibr"" target=""#b0"">[1]</ref><ref type=""bibr"" target=""#b1"">[2]</ref><ref type=""bibr"" target=""#b2"">[3]</ref>. Surveys conducted by the United",0
"nts consist of raw, unfiltered text <ref type=""bibr"" target=""#b8"">[9]</ref>. Furthermore, two corpora taken from the Leipzig Corpora Collection (LCC) <ref type=""bibr"" target=""#b17"">[18]</ref> were included in our language model (LM) data collection. Finally, it has been shown by some researchers tha",0
"phone-in talk shows, the South African data was compiled from national radio news bulletins and consists of a mix of prepared and spontaneous speech <ref type=""bibr"" target=""#b16"">[17]</ref>. The total transcribed multilingual speech data available for acoustic model training (ManT), comprising Som",0
"""bibr"" target=""#b8"">[9]</ref>. By leveraging available resources from better-resourced but unrelated languages <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b10"">11]</ref>, a system using a hybrid neural network acoustic model was able to ach",0
"aging available resources from better-resourced but unrelated languages <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b10"">11]</ref>, a system using a hybrid neural network acoustic model was able to achieve a word error rate (WER) of 53.75%",0
"media has become an accepted platform for sharing opinions and concerns <ref type=""bibr"" target=""#b0"">[1]</ref><ref type=""bibr"" target=""#b1"">[2]</ref><ref type=""bibr"" target=""#b2"">[3]</ref>. Surveys conducted by the United Nations (UN) in places lacking sufficient internet infrastructure indicate th",0
"lished internet infrastructure, social media has become an accepted platform for sharing opinions and concerns <ref type=""bibr"" target=""#b0"">[1]</ref><ref type=""bibr"" target=""#b1"">[2]</ref><ref type=""bibr"" target=""#b2"">[3]</ref>. Surveys conducted by the United Nations (UN) in places lacking suffici",0
"iv xmlns=""http://www.tei-c.org/ns/1.0""><head n=""6."">Acoustic modelling</head><p>The Kaldi speech recognition toolkit was used for all ASR experiments <ref type=""bibr"" target=""#b19"">[20]</ref>. All experiments were performed using a PC with an 8-core Intel i7 CPU, 32GB of RAM and a 12GB NVIDIA Tesla",0
"under-developed parts of Uganda, the UN has piloted radio browsing systems in three of the country's languages <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b7"">8]</ref>.</p><p>The success in Uganda served as a motivator for the development of a corresponding system for Somalia, a ining set of just 1.57 hours <ref type=""bibr"" target=""#b8"">[9]</ref>. By leveraging available resources from better-resourced but unrelated languages <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b10"">11]</ref>, a system using a hybrid neural al but unannotated Somali speech data by semisupervised training, an approach which has been applied successfully in some other low-resource settings <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b13"">[14]</ref><ref type=""bibr"" target=""#b14"">[15]</ref><ref type=""bibr"" target=""#b15 ef><ref type=""bibr"" target=""#b15"">[16]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2."">Radio browsing system</head><p>Figure 1 <ref type=""bibr"" target=""#b7"">[8]</ref> shows the components of the radio browsing system. The preprocessed audio stream is passed to the ASR system w ation of the confidence threshold.</p></div><figure xmlns=""http://www.tei-c.org/ns/1.0"" xml:id=""fig_0""><head>2</head><label>2</label><figDesc>Figure 1<ref type=""bibr"" target=""#b7"">[8]</ref> shows the components of the radio browsing system. The preprocessed audio stream is passed to the ASR system w",0
"cted by the United Nations (UN) in places lacking sufficient internet infrastructure indicate that this function is fulfilled by radio phone-in shows <ref type=""bibr"" target=""#b3"">[4]</ref><ref type=""bibr"" target=""#b4"">[5]</ref><ref type=""bibr"" target=""#b5"">[6]</ref>. Therefore, to support its human",0
"other low-resource settings <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b13"">[14]</ref><ref type=""bibr"" target=""#b14"">[15]</ref><ref type=""bibr"" target=""#b15"">[16]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2."">Radio browsing system</head><p>Figure 1 <re",0
"cess in a wide range of NLP tasks <ref type=""bibr"" target=""#b17"">(Mikolov et al., 2013;</ref><ref type=""bibr"" target=""#b20"">Peters et al., 2018;</ref><ref type=""bibr"" target=""#b2"">Devlin et al., 2019;</ref><ref type=""bibr"" target=""#b11"">Joshi et al., 2019;</ref><ref type=""bibr"" target=""#b15"">Yang et we use representations from the final hidden state of the decoder.</p><p>Figure <ref type=""figure"">1</ref>: A schematic comparison of BART with BERT <ref type=""bibr"" target=""#b2"">(Devlin et al., 2019)</ref> and GPT <ref type=""bibr"" target=""#b21"">(Radford et al., 2018)</ref>.</p><p>English, by propa mations we used are summarized below, and examples are shown in Figure <ref type=""figure"" target=""#fig_0"">2</ref>.</p><p>Token Masking Following BERT <ref type=""bibr"" target=""#b2"">(Devlin et al., 2019)</ref>, random tokens are sampled and replaced with [MASK] elements.</p><p>Token Deletion Random to er models, we do not implement the relative positional embeddings or attention across segments from XLNet.</p><p>Masked Language Model Following BERT <ref type=""bibr"" target=""#b2"">(Devlin et al., 2019)</ref>, we replace 15% of tokens with [MASK] symbols, and train the model to independently predict /ref>a an extractive question answering task on Wikipedia paragraphs. Answers are text spans extracted from a given document context. Similar to BERT <ref type=""bibr"" target=""#b2"">(Devlin et al., 2019)</ref>, we use concatenated question and context as input to the encoder of BART, and additionally de left-to-right auto-regressive language modelling during pre-training.</p><p>Bidirectional encoders are crucial for SQuAD As noted in previous work <ref type=""bibr"" target=""#b2"">(Devlin et al., 2019)</ref>, just left-to-right decoder performs poorly on SQuAD, because future context is crucial in c pe=""bibr"" target=""#b22"">Radford et al. (2019)</ref> demonstrated that very large language models can act as unsupervised multitask models.</p><p>BERT <ref type=""bibr"" target=""#b2"">(Devlin et al., 2019)</ref> introduced masked language modelling, which allows pre-training to learn interactions betwee",1
".</p><p>Figure <ref type=""figure"">1</ref>: A schematic comparison of BART with BERT <ref type=""bibr"" target=""#b2"">(Devlin et al., 2019)</ref> and GPT <ref type=""bibr"" target=""#b21"">(Radford et al., 2018)</ref>.</p><p>English, by propagation through BART, thereby using BART as a pre-trained target-si as also trained for 1M steps on a combination of books and Wikipedia data. We compare the following approaches:</p><p>Language Model Similarly to GPT <ref type=""bibr"" target=""#b21"">(Radford et al., 2018)</ref>, we train a left-to-right Transformer language model. This model is equivalent to the BART /p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""7"">Related Work</head><p>Early methods for pretraining were based on language models. GPT <ref type=""bibr"" target=""#b21"">(Radford et al., 2018)</ref> only models leftward context, which is problematic for some tasks. ELMo <ref type=""bibr"" t",1
"rget=""#b20"">Peters et al., 2018;</ref><ref type=""bibr"" target=""#b2"">Devlin et al., 2019;</ref><ref type=""bibr"" target=""#b11"">Joshi et al., 2019;</ref><ref type=""bibr"" target=""#b15"">Yang et al., 2019;</ref><ref type=""bibr"">Liu et al., 2019)</ref>. The most successful approaches have been variants of improving the distribution of masked tokens <ref type=""bibr"" target=""#b11"">(Joshi et al., 2019)</ref>, the order in which masked tokens are predicted <ref type=""bibr"" target=""#b15"">(Yang et al., 2019)</ref>, and the available context for replacing masked tokens <ref type=""bibr"" target=""#b5"">(Dong et right Transformer language model. This model is equivalent to the BART decoder, without cross-attention.</p><p>Permuted Language Model Based on XLNet <ref type=""bibr"" target=""#b15"">(Yang et al., 2019)</ref>, we sample 1/6 of the tokens, and generate them in a random order autoregressively. For consi a sequence to sequence model to predict the masked tokens.</p><p>For the Permuted LM, Masked LM and Multitask Masked LM, we use two-stream attention <ref type=""bibr"" target=""#b15"">(Yang et al., 2019)</ref> to efficiently compute likelihoods of the output part of the sequence (using a diagonal self- of bidirectional layers.</p><p>The pre-training objective is not the only important factor Our Permuted Language Model performs less well than XLNet <ref type=""bibr"" target=""#b15"">(Yang et al., 2019)</ref>. Some of this difference is likely due to not including other architectural improvements, suc e missing tokens. MASS is less effective for discriminative tasks, because disjoint sets of tokens are fed into the encoder and decoder.</p><p>XL-Net <ref type=""bibr"" target=""#b15"">(Yang et al., 2019)</ref> extends BERT by pre-Source Document (abbreviated)</p></div> <div xmlns=""http://www.tei-c.org/ ning Experiments</head><p>Recent work has shown that downstream performance can dramatically improve when pre-training is scaled to large batch sizes <ref type=""bibr"" target=""#b15"">(Yang et al., 2019;</ref><ref type=""bibr"">Liu et al., 2019)</ref> and corpora. To test how well BART performs in this r",0
"der of BART, and additionally pass them to the decoder. The model includes classifiers to predict the start and end indices of each token.</p><p>MNLI <ref type=""bibr"" target=""#b31"">(Williams et al., 2017)</ref>, a bitext classification task to predict whether one sentence entails another. The fine-t",0
"in which masked tokens are predicted <ref type=""bibr"" target=""#b15"">(Yang et al., 2019)</ref>, and the available context for replacing masked tokens <ref type=""bibr"" target=""#b5"">(Dong et al., 2019)</ref>. However, these methods typically focus on particular types of end tasks (e.g. span prediction 5% of tokens with [MASK] symbols, and train the model to independently predict the original tokens.</p><p>Multitask Masked Language Model As in UniLM <ref type=""bibr"" target=""#b5"">(Dong et al., 2019)</ref>, we train a Masked Language Model with additional self-attention masks. Self attention masks a t=""#b11"">(Joshi et al., 2019)</ref>. Predictions are not made auto-regressively, reducing the effectiveness of BERT for generation tasks.</p><p>UniLM <ref type=""bibr"" target=""#b5"">(Dong et al., 2019)</ref> fine-tunes BERT with an ensemble of masks, some of which allow only leftward context. Like BAR",0
"on, we set beam size as 5, remove duplicated trigrams in beam search, and tuned the model with min-len, max-len, length penalty on the validation set <ref type=""bibr"" target=""#b7"">(Fan et al., 2017)</ref> Summarization To provide a comparison with the state-of-the-art in summarization, we present re",0
"motion-aware chatbot that is capable of perceiving/expressing emotions and emotionally interacting with the interlocutors. In literature, Zhou et al. <ref type=""bibr"" target=""#b45"">[46]</ref> successfully build an emotional chat machine (ECM) that is capable of generating emotional responses accordi o problematic to design a unified model that can generate plausible emotional sentence without sacrificing grammatical fluency and semantic coherence <ref type=""bibr"" target=""#b45"">[46]</ref>. Hence, the response generation problem faces a significant challenge: that is, how to effectively leverage study the conversation generation problem with emotional factors, which are most related to our proposed conversation generation problem. Zhou et al. <ref type=""bibr"" target=""#b45"">[46]</ref> develop an Emotional Chat Machine (ECM) model using three different mechanisms (i.e., emotion embedding, int nversation (ESTC) derived from STC <ref type=""bibr"" target=""#b32"">[33]</ref>, to evaluate our experimental results. In particular, we follow the work <ref type=""bibr"" target=""#b45"">[46]</ref> to train an emotion classifier for assigning emotional labels to the sentences in the dataset.</p><p>ESTC, w ype=""bibr"" target=""#b5"">[6]</ref> model over two different datasets, i.e., NLPCC 2013 2 and NLPCC 2014 3 emotion classification datasets by following <ref type=""bibr"" target=""#b45"">[46]</ref>, which contain 29, 417 manually annotated data in total, and the best performance(accuracy) of 0.7257 is ach happy, other), note that here ""other"" indicates no any emotion information, and rare emotion categories like fear are removed. In particular, unlike <ref type=""bibr"" target=""#b45"">[46]</ref> using solely one label for classification, we consider both of the emotion labels and thus regard it as a mu owing baselines.</p><p>Seq2seq <ref type=""bibr"" target=""#b35"">[36]</ref>, the traditional Seq2seq model is adopted as one of our baselines.</p><p>ECM <ref type=""bibr"" target=""#b45"">[46]</ref>, as mentioned, ECM model is improper to directly be as the baseline since it cannot automatically select an alized. The emotion embedding is a 6 × 200-dimensional matrix (if used). The parameters of imemory and ememory in ECM are the same as the settings in <ref type=""bibr"" target=""#b45"">[46]</ref>. We use stochastic gradient descent (SGD) with mini-batch for optimization when training, and the batch size ll></cell><cell></cell></row></table></figure> 			<note xmlns=""http://www.tei-c.org/ns/1.0"" place=""foot"" n=""1"" xml:id=""foot_0"">Here we follow the work<ref type=""bibr"" target=""#b45"">[46]</ref>, where the emotion categories are {Angry, Disgust, Happy, Like, Sad, Other}.</note> 			<note xmlns=""http://w egory using the most frequent response's emotion to the detected post's emotion over EIPs.</p><p>Seq2seq-emb <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b45"">46]</ref>, Seq2seq with emotion embedding (Seq2seqemb) is also adopted in the same manner. This model encode the emotio",1
"""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b38"">39]</ref>. Several approaches are also proposed for specific tasks, such as Zhou et al. <ref type=""bibr"" target=""#b46"">[47]</ref> take account of static graph attention to incorporate commonsense knowledge for chatbots. Zhang et al. <ref",0
"mative responses, such as Maximum Mutual Information (MMI) based model <ref type=""bibr"" target=""#b13"">[14]</ref> and enhanced beam-search based model <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b38"">39]</ref>. Several approaches are also proposed for specific tasks, such as Zh",0
"ing issues. Following the success of Seq2seq model, methods based on such framework have been applied to various domains, such as machine translation <ref type=""bibr"" target=""#b1"">[2]</ref> and image caption generation <ref type=""bibr"" target=""#b0"">[1]</ref>. Indeed, there exist some attempts on imp Indeed, there exist some attempts on improving the performance of such encoder-decoder architecture for machine translation problem. Bahdanau et al. <ref type=""bibr"" target=""#b1"">[2]</ref> utilize Bi-directional Long Short-Term Memory (Bi-LSTM) network with attention mechanism for longsentence gene arget=""#b4"">5,</ref><ref type=""bibr"" target=""#b35"">36]</ref>. In order to promote the quality of the generated sentences, the Seq2seq-attention model <ref type=""bibr"" target=""#b1"">[2]</ref> is proposed for dynamically attending on the key information of the input post during decoding. In this paper, tail, the context representation 𝒄 𝑡 of the post sequence 𝒙 is computed by parameterizing the encoder hidden vector 𝒉 with different attention scores <ref type=""bibr"" target=""#b1"">[2]</ref>, that is,</p><formula xml:id=""formula_2"">𝒄 𝑡 = 𝑇 ∑︁ 𝑗=1 𝜶 (𝒔 𝑡 −1 , 𝒉 𝒋 ) • 𝒉 𝒋 ,<label>(1)</label></formula>< nary: Sequence-to-Sequence Attention Model</head><p>In the literature, sequence-to-sequence (Seq2seq) model is widely adopted for dialogue generation <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b35"">36]</ref>. In order to promote the quality",0
"motional responses according to a pre-defined emotion category, and several similar efforts are also made by <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b24"">25]</ref>, such as <ref type=""bibr"" target=""#b47"">[48]</ref> proposed by <ref type=""bibr"">Zhou et al.</ref> that utiliz >[11]</ref>, Huang et al. propose three different models that are capable of injecting different emotion factors for response generation. Peng et al. <ref type=""bibr"" target=""#b24"">[25]</ref> utilize Latent Dirichlet allocation (LDA) models to extract topic information for emotional conversation gen",0
"it may be beneficial to consider the higher-order structure in graphs <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" target=""#b34"">35,</ref><ref type=""bibr"" target=""#b45"">46]</ref>. In this work, we introduce a account for higher-order structures in different graph-based ML tasks <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" target=""#b45"">46]</ref>. DeepGL <ref type=""bibr"" target=""#b33"">[34]</ref> uses motifs as a ba ormula xml:id=""formula_4"">R i = 1, otherwise R i = -1.</formula><p>Our work differs from previous approaches <ref type=""bibr"" target=""#b27"">[28,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" target=""#b33"">34,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" target=""#b45"" ""#b35"">36,</ref><ref type=""bibr"" target=""#b45"">46</ref>] in several key points. Specifically, in contrast to <ref type=""bibr"" target=""#b27"">[28,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" target=""#b33"">34,</ref><ref type=""bibr"" target=""#b45"">46]</ref>, we propose a new class of hi =""#b45"">46]</ref>. In this work, we introduce a general class of graph convolution networks which utilize weighted multi-hop motif adjacency matrices <ref type=""bibr"" target=""#b32"">[33]</ref> to capture higher-order neighborhoods in graphs. The weighted adjacency matrices are computed using various "" target=""#b32"">[33]</ref> to capture higher-order neighborhoods in graphs. The weighted adjacency matrices are computed using various network motifs <ref type=""bibr"" target=""#b32"">[33]</ref>. Fig. <ref type=""figure"" target=""#fig_3"">1</ref> shows an example of the node neighborhoods that are induced ctive relational functions that represent compositions of relational operators applied to a base graph function such as triangle counts. Rossi et al. <ref type=""bibr"" target=""#b32"">[33]</ref> proposed the notion of higher-order network embeddings and demonstrated that one can learn better embeddings ""importance. ""</p><p>Hence, we introduce a motif-based matrix formulation as a function ? : R N ?N ? R N ?N over a motif adjacency A t ? A similar to <ref type=""bibr"" target=""#b32"">[33]</ref>. Given a function ?, we can obtain motif-based matrices ?t = ?(A t ), for t = 1, ? ? ? ,T . Below, we summar sitioning from node i to node j in k steps.</p><p>While the K-step motif-based adjacencies defined here share some similarity to that of Rossi et al. <ref type=""bibr"" target=""#b32"">[33]</ref> we would like to point out that there is an important distinction with our formulation. In particular, since",1
"b7"">[8]</ref>: A graph convolution approach which uses Chebyshev polynomials to approximate a smooth filter in the spectral domain.</p><p>? Planetoid <ref type=""bibr"" target=""#b46"">[47]</ref>: A method which integrates graph embedding techniques into graph-based semi-supervised learning.</p><p>? MoN established benchmark datasets, these are: Cora, Citeseer, and Pubmed. Specifically, we use the pre-processed versions made available by Yang et al. <ref type=""bibr"" target=""#b46"">[47]</ref>. The aforementioned graphs are undirected citation networks where nodes represent documents and edges denote lished in previous work, we use only 20 nodes per class for training <ref type=""bibr"" target=""#b19"">[20,</ref><ref type=""bibr"" target=""#b41"">42,</ref><ref type=""bibr"" target=""#b46"">47]</ref>. Again, following previous work, we take 1,000 nodes per dataset for testing and utilize an additional 500 fo ets as these are the standard graph benchmarks used by previous work <ref type=""bibr"" target=""#b19"">[20,</ref><ref type=""bibr"" target=""#b41"">42,</ref><ref type=""bibr"" target=""#b46"">47]</ref> and also because these datasets have ground-truth node labels. However, the approach is fast and scalable for",0
"is may be for reasons similar as to why skip-connections are needed in deep architectures since the signal starts to degrade as the model gets deeper <ref type=""bibr"" target=""#b15"">[16]</ref>.</p><p>As another example, we set ? to Eq. 6. Now for an arbitrary motif, we see that ( ?(k) ) i, j encodes",0
"t which contains both i and j. In this paper, we use a loose definition for motifs and it can also mean induced subgraphs (e.g., graphlets or orbits <ref type=""bibr"" target=""#b1"">[2]</ref>). Motifs of sizes 2-4 are shown in Fig. <ref type=""figure"">4</ref>. As shown in Fig. <ref type=""figure"" target",0
"ssification <ref type=""bibr"" target=""#b20"">[21]</ref>, image super-resolution <ref type=""bibr"" target=""#b18"">[19]</ref>, and video action recognition <ref type=""bibr"" target=""#b9"">[10]</ref>, among many others. CNNs, however, are designed to work for data that can be represented as grids (e.g., vide",0
"ly considered as time series.</p><p>The item transition pattern is more complicated. • The recent approaches <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b37"">38]</ref>, which divide the user's preference into the long-term (global) and t uitable for sessionbased recommendation.</p><p>For the fairness and the convenience of comparison, we follow <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b37"">38]</ref> to filter out sessions of length 1 and items which occur less than 5 in the session S, it is defined as [ s, 0 , . . . , s,i −1 ] with the last item s,i −1 as l abel . Following <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b37"">38]</ref>, for the Yoochoose dataset, the most recent portions 1/64 and 1/4 of =""bibr"" target=""#b15"">[16]</ref> applies a self-attention on the last item after encoding with a RNN. To alleviate the influence of time order, STAMP <ref type=""bibr"" target=""#b18"">[19]</ref> only utilizes the self-attention mechanism without RNN. SR-GNN <ref type=""bibr"" target=""#b37"">[38]</ref> pro ention layer to assign weight to each hidden state to sum up as the session embedding. To further alleviate the bias introduced by time series, STAMP <ref type=""bibr"" target=""#b18"">[19]</ref> entirely replaces the recurrent encoder with an attention layer. SR-GNN <ref type=""bibr"" target=""#b37"">[38]< ntion layer to combine encoded states of RNN, which enables the model to explicitly emphasize on the more important parts of the input.</p><p>• STAMP <ref type=""bibr"" target=""#b18"">[19]</ref> uses attention layers to replace all RNN encoders in previous work to even make the model more powerful by f t to look deep into how they perform on sessions with different lengths because the length varies greatly within one dataset. Following previous work <ref type=""bibr"" target=""#b18"">[19,</ref><ref type=""bibr"" target=""#b37"">38]</ref>, sessions in Yoochoose 1/64 are separated into two groups, i.e., Sho",1
"recent years, many GNN methods <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" target=""#b38"">39]</ref> work under the mechanism that is similar to message passing network < Figure <ref type=""figure"">1</ref>. In recent years, some baseline methods on GNN, for example, GCN <ref type=""bibr"" target=""#b11"">[12]</ref> and GAT <ref type=""bibr"" target=""#b32"">[33]</ref>, are demonstrated to be capable of extracting features of the graph. However, most of them are only well-sui ned above, there are many different GNN layers that can be used to generate node embeddings, e.g., GCN <ref type=""bibr"" target=""#b11"">[12]</ref>, GAT <ref type=""bibr"" target=""#b32"">[33]</ref> and gated graph networks <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b37"">38]</ref>. experiments, we use the ReLU <ref type=""bibr"" target=""#b19"">[20]</ref>.</p><p>As suggested in previous work <ref type=""bibr"" target=""#b31"">[32,</ref><ref type=""bibr"" target=""#b32"">33]</ref>, the multi-head attention can help to stabilize the training of the self-attention layers. Therefore, we appl",1
"of RNN <ref type=""bibr"" target=""#b9"">[10]</ref> applied on the sequence data, for instance, GRU4REC <ref type=""bibr"" target=""#b8"">[9]</ref> and NARM <ref type=""bibr"" target=""#b15"">[16]</ref> mainly model the time order of items and encode these items using a RNN like neural networks. After encoding items for the long-term (global) preference. This setting directly ignores the pattern of item choices, which introduces the bias to the model. NARM <ref type=""bibr"" target=""#b15"">[16]</ref> applies a self-attention on the last item after encoding with a RNN. To alleviate the influence of time orde proposed a list-wise deep neural network model to train a ranking model. Some recent work uses the attention mechanism to avoid the time order. NARM <ref type=""bibr"" target=""#b15"">[16]</ref> stacks GRU as the encoder to extract information and</p><formula xml:id=""formula_0"">! "" # "" $ WGAT Readout % sited items. • BPR-MF <ref type=""bibr"" target=""#b21"">[22]</ref> proposes a BPR objective function which calculates a pairwise ranking loss. Following <ref type=""bibr"" target=""#b15"">[16]</ref>, Matrix Factorization is modified to session-based recommendation by using mean latent vectors of items in a is a hybrid model for the next-basket recommendation and it achieves state-of-the-art results. For anonymous session-based recommendation, following <ref type=""bibr"" target=""#b15"">[16]</ref>, we omit the user feature directly because of the unavailability. • GRU4REC <ref type=""bibr"" target=""#b8"">[9 b8"">[9]</ref> stacks multiple GRU layers to encode the session sequence into a final state. It also applies a ranking loss to train the model. • NARM <ref type=""bibr"" target=""#b15"">[16]</ref> extends to use an attention layer to combine encoded states of RNN, which enables the model to explicitly em espect to all baseline methods and our proposed FGNN model. Due to the insufficient memory of hardware, we can not initialize FPMC on Yoochoose1/4 as <ref type=""bibr"" target=""#b15"">[16]</ref>, which is not reported in Table <ref type=""table"" target=""#tab_3"">2</ref>. For more detailed comparisons, in n indicates that items should not be simply considered as time series.</p><p>The item transition pattern is more complicated. • The recent approaches <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b37"">38]</ref>, which divide the user's pre contains the transaction data which is suitable for sessionbased recommendation.</p><p>For the fairness and the convenience of comparison, we follow <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b37"">38]</ref> to filter out sessions of le ets. For the partial session of length i in the session S, it is defined as [ s, 0 , . . . , s,i −1 ] with the last item s,i −1 as l abel . Following <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b37"">38]</ref>, for the Yoochoose dataset,",0
"lity between items.</p><p>Deep learning models are popular recently with the boom of recurrent neural networks <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b28"">29,</ref><ref type=""bibr"" target=""#b35"">36 designed for processing sequential data. Hidasi et al. <ref type=""bibr"" target=""#b8"">[9]</ref> proposed the GRU4REC, which applies a multi layer GRU <ref type=""bibr"" target=""#b1"">[2]</ref> to simply treat the data as time series. Based on the RNN model, some work makes improvements on the architect",0
"ch interest in the deep learning society. Initially, GNN is applied to the simple situation on directed graphs <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b25"">26]</ref>. In recent years, many GNN methods <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b11"">12,<",0
"strong baseline in some situations although it is simple.</p><p>• S-POP always recommends the most popular items for the current session. • Item-KNN <ref type=""bibr"" target=""#b23"">[24]</ref> computes the similarity of items by the cosine distance of two item vectors in sessions. Regularization is a",0
"target=""#b15"">[16]</ref>, Matrix Factorization is modified to session-based recommendation by using mean latent vectors of items in a session. • FPMC <ref type=""bibr"" target=""#b22"">[23]</ref> is a hybrid model for the next-basket recommendation and it achieves state-of-the-art results. For anonymous",0
"tionally, the graph level feature representation learning is essential for graph level tasks, for example, graph classification and graph isomorphism <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b38"">39]</ref>. Set2Set <ref type=""bibr"" target=""#b33"">[34]</ref> assigns each node",0
"ds the incorporation of different deep learning tools, for instance, zeroshot learning and domain adaptation <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b14"">15]</ref>. These methods all depend on the identification of users and the whole record of interactions for every user.",0
"ation process. More subsequent work extends the incorporation of different deep learning tools, for instance, zeroshot learning and domain adaptation <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b14"">15]</ref>. These methods all depend on the identification of users and the who",0
"ader> 	<text xml:lang=""en""> 		<body> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""1"">Introduction</head><p>The Transformer neural sequence model <ref type=""bibr"" target=""#b4"">[Vaswani et al., 2017]</ref> has emerged as a popular alternative to recurrent sequence models. Transformer relies on at ut shape.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2.2"">Multi-head Attention</head><p>The ""Transformer"" seuqence-to-sequence model <ref type=""bibr"" target=""#b4"">[Vaswani et al., 2017]</ref> uses h different attention layers (heads) in parallel, which the authors refer to as ""Multi . so ftma x ( l o g i t s ) o = t f . einsum ( ""hm, hmv-&gt;hv "" , weig hts , V) y = t f . einsum ( "" hv , hdv-&gt;d "" , o , P_o) r e t u r n y Note: <ref type=""bibr"" target=""#b4"">[Vaswani et al., 2017]</ref> include a constant scaling factor on the logits. We omit this in our code, as it can be fol These queries all interact with the same keys and values. In addition, we process a batch of b different non-interacting sequences at once. Following <ref type=""bibr"" target=""#b4"">[Vaswani et al., 2017]</ref>, in an autoregressive model, we can prevent backward-information-flow by adding a ""mask"" to erformance analysis, we will make several simplifying assumptions:</p><formula xml:id=""formula_0"">? m = n ? k = v = d</formula><p>h , as suggested by <ref type=""bibr"" target=""#b4"">[Vaswani et al., 2017]</ref> ? n ? d</p><p>The total number of arithmetic operations is ?(bnd 2 ). (Since the complexity to process queries from multiple positions in parallel. An example is a self-attention layer in an autoregressive language model such as Transformer <ref type=""bibr"" target=""#b4"">[Vaswani et al., 2017]</ref>. The queries produced at each position attend to key-value pairs produced at all positions ei-c.org/ns/1.0""><head n=""3"">Multi-Query Attention</head><p>We introduce multi-query Attention as a variation of multi-head attention as described in <ref type=""bibr"" target=""#b4"">[Vaswani et al., 2017]</ref>. Multi-head attention consists of multiple attention layers (heads) in parallel with differ ns/1.0""><head n=""4"">Experiments and Results</head></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.1"">Experimental Setup</head><p>Following <ref type=""bibr"" target=""#b4"">[Vaswani et al., 2017]</ref>, we evaluate on the WMT 2014 English-German translation task. As a baseline, we use an enco",1
"ity degradation.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2"">Background: Neural Attention</head><p>Neural Attention, introduced by <ref type=""bibr"" target=""#b0"">[Bahdanau et al., 2014]</ref>, is a powerful tool for manipulating variable-length representations. A neural attention f",0
"ng to a local neighborhood, or by otherwise compressing the number of memory positions, as in <ref type=""bibr"" target=""#b2"">[Liu et al., 2018]</ref>, <ref type=""bibr"" target=""#b5"">[Zhang et al., 2018]</ref>, <ref type=""bibr"" target=""#b3"">[Povey et al., 2018]</ref>. In this paper we present an orthog",0
"number of positions being attended-to, either by attending to a local neighborhood, or by otherwise compressing the number of memory positions, as in <ref type=""bibr"" target=""#b2"">[Liu et al., 2018]</ref>, <ref type=""bibr"" target=""#b5"">[Zhang et al., 2018]</ref>, <ref type=""bibr"" target=""#b3"">[Povey",0
"baseline.</p><p>We preformed a similar set of experiments using ""transformer-decoder"" language models on the Billion-Word Language Modeling Benchmark <ref type=""bibr"" target=""#b1"">[Chelba et al., 2013]</ref>. For the baseline, we use a model with 6 layers,</p><formula xml:id=""formula_5"">d model = 10",0
"ptimal (up to the factor of √ S/( √ S + 1 − 1)) for any combination of input parameters ( § 3).</p><p>• Based on the red-blue pebble game abstraction <ref type=""bibr"" target=""#b34"">[34]</ref>, we provide a new method of deriving I/O lower bounds (Lemma 4), which may be used to generate optimal sched and outputs, that is, sets of vertices that have no parents (or no children, respectively). Red-Blue Pebble Game Hong and Kung's red-blue pebble game <ref type=""bibr"" target=""#b34"">[34]</ref> models an execution of an algorithm in a two-level memory structure with a small-and-fast as well as large-a ER BOUNDS</head><p>We now present a mathematical machinery for deriving I/O lower bounds for general CDAGs. We extend the main lemma by Hong and Kung <ref type=""bibr"" target=""#b34"">[34]</ref>, which provides a method to nd an I/O lower bound for a given CDAG. at lemma, however, does not give a tight on to bound the number of I/O operations of the corresponding pebbling. Hong and Kung use a speci c variant of this partition, denoted as S-partition <ref type=""bibr"" target=""#b34"">[34]</ref>.</p><p>We rst introduce our generalization of S-partition, called Xpartition, that is the base of our analys in a valid 2S-partition, together with the observation that each V i performs at least S I/O operations, we phrase the lemma by Hong and Kung: L 1 ( <ref type=""bibr"" target=""#b34"">[34]</ref>). e minimal number Q of I/O operations for any valid execution of a CDAG of any I/O computation is bounded b >P . Assume that we know the optimal complete calculation of the CDAG, where a calculation is a sequence of allowed moves in the red-blue pebble game <ref type=""bibr"" target=""#b34"">[34]</ref>. Divide the complete calculation into h consecutive subcomputations V 1 , V 2 , ..., V h , such that during .tei-c.org/ns/1.0""><head>4.2.2</head><p>Reuse-Based Lemma. We now use the above de nitions and observations to generalize the result of Hong and Kung <ref type=""bibr"" target=""#b34"">[34]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>L</head><p>2. An optimal complete calculation of a X . To show that S(X ) = {V 1 . . . V h } meets the remaining properties of a valid X -partition S(X ), we use the same reasoning as originally done <ref type=""bibr"" target=""#b34"">[34]</ref>.</p><p>erefore, a complete calculation performing q &gt; (X − R(S)</p><formula xml:id=""formula_15"">+ T (S)) l:id=""formula_23"">√ S/( √ S + 1 − 1)</formula><p>), and therefore may be seen as the last step in the long sequence of improved bounds. Hong and Kung <ref type=""bibr"" target=""#b34"">[34]</ref> derived an asymptotic bound Ω n 3 / √ S for the sequential case. Irony et al. <ref type=""bibr"" target=""#b33"" O optimal schedule (shared memory).</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""10.1"">General I/O Lower Bounds</head><p>Hong and Kung <ref type=""bibr"" target=""#b34"">[34]</ref> analyzed the I/O complexity for general CDAGs in their the red-blue pebble game, on which we base our work.",1
"979p 1 3 , k =1.184p<label>2 3</label></formula><p>q q q q q q q q 512 2,048 q CARMA <ref type=""bibr"" target=""#b21"">[21]</ref> COSMA (this work) CTF <ref type=""bibr"" target=""#b46"">[46]</ref> ScaLAPACK <ref type=""bibr"" target=""#b51"">[51]</ref> (c) Extra memory, m = n = 979p   </p></div> <div xmlns="" ine. Ballard et al. <ref type=""bibr"" target=""#b5"">[5]</ref> proved analogous results for Strassen's algorithm. is analysis was extended by Sco et al. <ref type=""bibr"" target=""#b46"">[46]</ref> to a general class of Strassen-like algorithms.</p><p>Although we consider only dense matrices, there is an",0
"MM) is one of the most fundamental building blocks in scienti c computing, used in linear algebra algorithms <ref type=""bibr"" target=""#b13"">[13,</ref><ref type=""bibr"" target=""#b15"">15,</ref><ref type=""bibr"" target=""#b42"">42]</ref>, (Cholesky and LU decomposition <ref type=""bibr"" target=""#b42"">[42]</ nd LU decomposition <ref type=""bibr"" target=""#b42"">[42]</ref>, eigenvalue factorization <ref type=""bibr"" target=""#b13"">[13]</ref>, triangular solvers <ref type=""bibr"" target=""#b15"">[15]</ref>), machine learning <ref type=""bibr"" target=""#b6"">[6]</ref>, graph processing <ref type=""bibr"" target=""#b4"">[",0
"f type=""bibr"" target=""#b12"">[12]</ref> studied di erent variants of pebble games in the context of memory space and parallel time. Aggarwal and Vi er <ref type=""bibr"" target=""#b1"">[2]</ref> introduced a two-memory machine that models a blocked access and latency in an external storage. Arge et al. < ensive literature on sparse matrix I/O optimizations. Bender et al. <ref type=""bibr"" target=""#b7"">[7]</ref> extended Aggarwal's external memory model <ref type=""bibr"" target=""#b1"">[2]</ref> and showed I/O complexity of the sparse matrix-vector (SpMV) multiplication. Greiner <ref type=""bibr"" target=""",0
"e=""bibr"" target=""#b8"">8,</ref><ref type=""bibr"" target=""#b18"">18,</ref><ref type=""bibr"" target=""#b36"">36,</ref><ref type=""bibr"" target=""#b44"">44,</ref><ref type=""bibr"" target=""#b52"">52]</ref>, computational chemistry <ref type=""bibr"" target=""#b21"">[21]</ref>, and others. us, accelerating MMM routines",0
"ativity in the LLC was indeed beneficial.</p><p>Ten years later, Zheng et al. evaluated the performance of exclusive caches with respect to inclusive <ref type=""bibr"" target=""#b39"">[40]</ref>. They found that exclusive caching is beneficial for most of the benchmarks they tried <ref type=""bibr"">(SPE",1
"clusive L2 <ref type=""bibr"" target=""#b14"">[15]</ref>. The Intel Knights Landing has an L2 (LLC) that is inclusive of the L1D and non-inclusive of L1I <ref type=""bibr"" target=""#b15"">[16]</ref>. However, Intel recently announced the Skylake-X which has a mostly exclusive LLC (which they call non-inclu",0
"che (LLC). The core has support to be attached to exclusive L2 caches as long as that is properly configured both in the core and L2 controller sides <ref type=""bibr"" target=""#b1"">[2]</ref>.</p><p>The processors in the IBM POWER series had mostly L3 (LLC) exclusive caches. The POWER5 has an L3 exclu",0
"policy. There has been a lower effort in exclusive caches.</p><p>Inclusive Non-Incl. Exclusive RRIP <ref type=""bibr"" target=""#b20"">[21]</ref> X SDBP <ref type=""bibr"" target=""#b24"">[25]</ref> X SHiP <ref type=""bibr"" target=""#b35"">[36]</ref> X GIPPR <ref type=""bibr"" target=""#b21"">[22]</ref> X MDPP <r ory <ref type=""bibr"" target=""#b19"">[20]</ref>.</p><p>Signature-based. Other RPs make use of the program counter as a signature to predict dead blocks <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b35"">36]</ref>.</p><p>Evicted Address Filter. Seshadri et al. proposed evicted addr",0
"gher-level cache. Only data evicted from higher levels is present in the exclusive cache, effectively making the last-level cache into a victim cache <ref type=""bibr"" target=""#b22"">[23]</ref>. Thus, there is no data replication and there cannot be inclusiveness-induced invalidations.</p></div> <div creasing the total amount of data blocks that can fit in the whole cache hierarchy.</p><p>The exclusive inclusion policy is similar to a victim cache <ref type=""bibr"" target=""#b22"">[23]</ref>. In a three-level cache hierarchy, the LLC would be the victim cache of a two-level cache hierarchy. Victim",0
"s a scalable system with minimal performance loss due to maintaining cache coherency. A notable proposed scalable system architecture is described in <ref type=""bibr"" target=""#b33"">[34]</ref> in which, NAND flash is used as main memory technology and DRAM as a cache for flash, allowing a substantial",1
"dopted due to its proprietary nature. Newer non-proprietary interconnection protocols provide the semantics for easier adaptation. For example, Gen-Z <ref type=""bibr"" target=""#b21"">[22]</ref> is a scalable, universal system interconnect that uses a memory-semantic (Load/Store) protocol and enables m",0
"proprietary nature constrain, with attempts to standardize RDMA access to lower latency, or adding additional layers of abstraction such as Portals 4 <ref type=""bibr"" target=""#b11"">[12]</ref> in order to standardize underlying hardware and provide low latency from either the language semantics or so",0
"the area allocated to the capacitor in the cell, such that a 3D structured capacitor is used to obtain the necessary capacitance in the limited area <ref type=""bibr"" target=""#b41"">[42]</ref>. The aspect ratio of the capacitor has sharply increased and will reach ~100 shortly because of the aggressi e time being, DRAM manufacturers claim to have hit the limit in how many electrons are required to hold a charge within an acceptable refresh period. <ref type=""bibr"" target=""#b41"">[42]</ref>. The complication being that DRAM data cannot be accessed during a refresh, resulting in performance loss.</",0
"tting up an RDMA transfer for small amounts of data. In the latter case, a more traditional approach of data copying should be adopted.</p><p>ArgoDSM <ref type=""bibr"" target=""#b38"">[39]</ref> is another new, highly-scalable, software DSM system for HPC and Big Data applications, meant to be run on t",0
"methods like ELMo <ref type=""bibr"" target=""#b25"">(Peters et al., 2018)</ref>, GPT-2 <ref type=""bibr"" target=""#b26"">(Radford et al., 2019)</ref>, BERT <ref type=""bibr"" target=""#b4"">(Devlin et al., 2019)</ref>, <ref type=""bibr"">XLNet (Yang et al., 2019)</ref> and RoBERTa <ref type=""bibr"" target=""#b19"" Despite its conceptual simplicity, this paradigm has reestablished the new state-ofthe-art baselines across various tasks, such as question answering <ref type=""bibr"" target=""#b4"">(Devlin et al., 2019)</ref>, coreference resolution <ref type=""bibr"" target=""#b14"">(Joshi et al., 2019b)</ref>, relation d-hoc handling of long sequences is also required in the pre-training stage, such as updating the model using only short sequences in the early stage <ref type=""bibr"" target=""#b4"">(Devlin et al., 2019)</ref>.</p><p>Common strategies for reducing memory consumption, unfortunately, do not work. For in BERT and introduce its memory profiling in this section. Following the paradigm of language model pre-training and down-stream task fine-tuning, BERT <ref type=""bibr"" target=""#b4"">(Devlin et al., 2019)</ref> consists of multiple layers of bidirectional Transformers <ref type=""bibr"">(Vaswani et al., 17)</ref>, where each Transformer encoder has a multi-head self-attention layer and a position-wise feed-forward layer. Using the same notation as in <ref type=""bibr"" target=""#b4"">(Devlin et al., 2019)</ref>, we denote the number of Transformer layers by L, the number of hidden units by H, the numbe h models in practice, however, is an extremely resource-intensive process. For instance, the training of BERT-family models is notoriously expensive. <ref type=""bibr"" target=""#b4"">Devlin et al. (2019)</ref> report that it takes four days for pre-training BERT-Base/BERT-Large on 4/16 Cloud TPUs, resp ation (1, 2, 3), (2, 3, 1), and (3, 1, 2), resp. We compare BlockBERT with the following baselines:</p><p>Google BERT The pre-trained base model from <ref type=""bibr"" target=""#b4"">Devlin et al. (2019)</ref>.</p><p>RoBERTa-2seq and RoBERTa-1seq We compare with two versions of RoBERTa <ref type=""bibr"" th distributions can be found in Figure <ref type=""figure"">6</ref>.</p><p>For all the pre-trained models, we adopt the same fine-tuning QA setup from <ref type=""bibr"" target=""#b4"">Devlin et al. (2019)</ref>.</p><p>The tokenized paragraph (p 1 , • • • , p s ) and question (q 1 , • • • , q t ) are con he number of layers L, attention heads A, or hidden units H leads to significant performance degradation <ref type=""bibr"">(Vaswani et al., 2017;</ref><ref type=""bibr"" target=""#b4"">Devlin et al., 2019)</ref> and does not address the long sequence issue. Alternatively, general low-memory training tech bibr"">(Vaswani et al., 2017;</ref><ref type=""bibr"" target=""#b3"">Dai et al., 2019)</ref> and its successful application on language model pre-training <ref type=""bibr"" target=""#b4"">(Devlin et al., 2019;</ref><ref type=""bibr"" target=""#b26"">Radford et al., 2019;</ref><ref type=""bibr"" target=""#b3"">Yang",1
"bles attention heads to capture either short-or long-range contextual information. Compared to the previous method that also enforces sparsity (e.g., <ref type=""bibr"" target=""#b1"">Child et al. (2019)</ref>), our approach is much simpler mathematically and very easy to implement. More importantly, th re-training model with only MLM task.</p><p>SparseBERT We pre-train BERT models with its Transformer encoder replaced by a Sparse Transformer encoder <ref type=""bibr"" target=""#b1"">(Child et al., 2019)</ref>. We set its sparsity hyper-parameters stride = 128 and expressivity c = 32. The attention mas "">4</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>A.3 SPARSEBERT</head><p>The sparse masking matrices we use for Sparse Transformer <ref type=""bibr"" target=""#b1"">(Child et al., 2019)</ref>  </p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>A.4 FINE-TUNING SETTINGS</head><p> earch focuses on attention matrix sparsification, such as Star Transformer <ref type=""bibr"" target=""#b8"">(Guo et al., 2019)</ref>, Sparse Transformer <ref type=""bibr"" target=""#b1"">(Child et al., 2019</ref><ref type=""bibr"">), Adaptive Sparse Transformer (Correia et al., 2019;</ref><ref type=""bibr"" ta",0
": Low precision is to use half-precision or mixed-precision for training neural networks. This technique has been widely used in Transformer training <ref type=""bibr"" target=""#b24"">(Ott et al., 2019;</ref><ref type=""bibr"" target=""#b19"">Liu et al., 2019)</ref>. In this work, we already assume to use",0
"lving long sequences such as coreference resolution <ref type=""bibr"" target=""#b14"">(Joshi et al., 2019b</ref>) and document-level machine translation <ref type=""bibr"" target=""#b22"">(Miculicich et al., 2018)</ref>, and also non-NLP tasks such as protein sequence modeling <ref type=""bibr"" target=""#b28",0
"ple et al., 2016;</ref><ref type=""bibr"">Zhang et al., 2018;</ref><ref type=""bibr"" target=""#b12"">Gui et al., 2019b)</ref>.</p><p>Recently, Transformer <ref type=""bibr"" target=""#b34"">(Vaswani et al., 2017</ref>) began to prevail in various NLP tasks, like machine translation <ref type=""bibr"" target=""# ><p>Recently, Transformer <ref type=""bibr"" target=""#b34"">(Vaswani et al., 2017</ref>) began to prevail in various NLP tasks, like machine translation <ref type=""bibr"" target=""#b34"">(Vaswani et al., 2017)</ref>, language modeling <ref type=""bibr"" target=""#b29"">(Radford et al., 2018)</ref>, and pretra #b7"">(Devlin et al., 2018)</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2.2"">Transformer</head><p>Transformer was introduced by <ref type=""bibr"" target=""#b34"">(Vaswani et al., 2017)</ref>, which was mainly based on self-attention. It achieved great success in various NLP tasks. arget=""#b34"">(Vaswani et al., 2017;</ref><ref type=""bibr"" target=""#b7"">Devlin et al., 2018)</ref>. Instead of using the sinusoidal position embedding <ref type=""bibr"" target=""#b34"">(Vaswani et al., 2017)</ref> and learned absolute position embedding, <ref type=""bibr"" target=""#b31"">Shaw et al. (2018) xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2.2.1"">Transformer Encoder Architecture</head><p>We first introduce the Transformer encoder proposed in <ref type=""bibr"" target=""#b34"">(Vaswani et al., 2017)</ref>. The Transformer encoder takes in an matrix H ∈ R l×d , where l is the sequence length, d yper-parameter.</formula><p>Other components of the Transformer encoder includes layer normalization and Residual connection, we use them the same as <ref type=""bibr"" target=""#b34"">(Vaswani et al., 2017)</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2.2.2"">Position Embedding</he t aware of the positions of different tokens, making it unable to capture the sequential characteristic of languages. In order to solve this problem, <ref type=""bibr"" target=""#b34"">(Vaswani et al., 2017)</ref> suggested to use position embeddings generated by sinusoids of varying frequency. The tth l the position of a token and the distance of each two tokens. For any fixed offset k, P E t+k can be represented by a linear transformation of P E t <ref type=""bibr"" target=""#b34"">(Vaswani et al., 2017)</ref>. In TENER, Transformer encoder is used not only to extract the word-level contextual infor NLP tasks. Since the self-attention mechanism used in the Transformer is unaware of positions, to avoid this shortage, position embeddings were used <ref type=""bibr"" target=""#b34"">(Vaswani et al., 2017;</ref><ref type=""bibr"" target=""#b7"">Devlin et al., 2018)</ref>. Instead of using the sinusoidal p",1
"target=""#b40"">(Zhou et al., 2017)</ref>, relation extraction <ref type=""bibr"" target=""#b24"">(Miwa and Bansal, 2016)</ref>, and coreference resolution <ref type=""bibr"" target=""#b9"">(Fragkou, 2017)</ref>. Since <ref type=""bibr"" target=""#b5"">(Collobert et al., 2011)</ref>, various neural models have be",0
"ith the ability of direction-and distanceawareness, we adopt the relative positional encoding <ref type=""bibr"" target=""#b31"">(Shaw et al., 2018;</ref><ref type=""bibr"" target=""#b15"">Huang et al., 2019;</ref><ref type=""bibr"" target=""#b6"">Dai et al., 2019)</ref>. instead of the absolute position encodi bibr"" target=""#b31"">Shaw et al. (2018)</ref> argued that the distance between two tokens should be considered when calculating their attention score. <ref type=""bibr"" target=""#b15"">Huang et al. (2019)</ref> reduced the computation complexity of relative positional encoding from O(l 2 d) to O(ld), wh",0
"essing (NLP) because of its potential assistance in question generation <ref type=""bibr"" target=""#b40"">(Zhou et al., 2017)</ref>, relation extraction <ref type=""bibr"" target=""#b24"">(Miwa and Bansal, 2016)</ref>, and coreference resolution <ref type=""bibr"" target=""#b9"">(Fragkou, 2017)</ref>. Since <r",0
"previous works in this arena <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b36"">37,</ref><ref type=""bibr"" target=""#b7"">8]</ref>, regularizes the spectral norm of the weight matrix at each individual layer based on certain Lyapunov conditio ork are the results given in <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b36"">37,</ref><ref type=""bibr"" target=""#b7"">8]</ref>. <ref type=""bibr"" target=""#b7"">[8]</ref> utilizes Lipschitz properties of the DNN to improve robustness against evious works on this subject <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b36"">37,</ref><ref type=""bibr"" target=""#b7"">8]</ref>, keep β constant across layers. These harder constraints over-regularize and thus impair the DNNs ability again bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b36"">37,</ref><ref type=""bibr"" target=""#b7"">8]</ref>. <ref type=""bibr"" target=""#b7"">[8]</ref> utilizes Lipschitz properties of the DNN to improve robustness against adversarial attacks. Unlike <ref type="" >8]</ref>. <ref type=""bibr"" target=""#b7"">[8]</ref> utilizes Lipschitz properties of the DNN to improve robustness against adversarial attacks. Unlike <ref type=""bibr"" target=""#b7"">[8]</ref>, our approach does not require a predetermined set of hyper-parameters to prove robustness. Our analysis provi he works given in <ref type=""bibr"" target=""#b10"">[11]</ref>, <ref type=""bibr"" target=""#b27"">[28]</ref>, <ref type=""bibr"" target=""#b36"">[37]</ref> and <ref type=""bibr"" target=""#b7"">[8]</ref>. <ref type=""bibr"" target=""#b27"">[28]</ref>, <ref type=""bibr"" target=""#b10"">[11]</ref> propose training network and β is a constant. We select 3 values of β from their papers: β = 1.0, 1.6, 2.0. The 2 works given in <ref type=""bibr"" target=""#b36"">[37]</ref> and <ref type=""bibr"" target=""#b7"">[8]</ref> may be seen as subsets of the works given in <ref type=""bibr"" target=""#b27"">[28]</ref> and <ref type=""bibr"" ta",1
"arget=""#b33"">[34,</ref><ref type=""bibr"" target=""#b9"">10]</ref>. Finally, our proposed robust training method, unlike the previous works in this arena <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b36"">37,</ref><ref type=""bibr"" target=""#b7"" obustness for a DNN by calculating the bounds for the activation functions' responses to the inputs. The closest to our work are the results given in <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b36"">37,</ref><ref type=""bibr"" target=""#b7"" his is not true for DNNs trained by weight-decay or spectral norm regularization against a single threshold β. All the previous works on this subject <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b36"">37,</ref><ref type=""bibr"" target=""#b7"" ectral norm of all layers by 1. As we will show, this approach limits the performance on the clean data set and does not produce the most robust DNN. <ref type=""bibr"" target=""#b10"">[11]</ref> uses PAC-Bayes generalization analysis to estimate the robustness of DNNs trained by spec-tral regularizatio network. We leave this to our future work in this area. When we consider the accuracy under attack, our Lyapunov approach dominates prior baselines. <ref type=""bibr"" target=""#b10"">[11]</ref> for instance, obtained an accuracy of 62% at = 0.1 with adversarial training on CIFAR10 under the 2 PGD adve ntal outputs. We use the power iteration method to estimate the spectral norm of the weight matrix at a specific layer during training as proposed in <ref type=""bibr"" target=""#b10"">[11]</ref>. Further, the pooling layers inside a DNN do not affect the conic behavior of the sub-systems given the prop /head><p>This appendix details a set of experiments performed to provide a robustness comparison between our proposed approach and the works given in <ref type=""bibr"" target=""#b10"">[11]</ref>, <ref type=""bibr"" target=""#b27"">[28]</ref>, <ref type=""bibr"" target=""#b36"">[37]</ref> and <ref type=""bibr"" t =""#b27"">[28]</ref>, <ref type=""bibr"" target=""#b36"">[37]</ref> and <ref type=""bibr"" target=""#b7"">[8]</ref>. <ref type=""bibr"" target=""#b27"">[28]</ref>, <ref type=""bibr"" target=""#b10"">[11]</ref> propose training networks with the same spectral regularization enforced across the entire network where ρ(W 36"">[37]</ref> and <ref type=""bibr"" target=""#b7"">[8]</ref> may be seen as subsets of the works given in <ref type=""bibr"" target=""#b27"">[28]</ref> and <ref type=""bibr"" target=""#b10"">[11]</ref>, where ρ(W l ) ≤ β for l = 1, ..., n and β = 1.0 are selected. All the aforementioned works are special case",1
">The following represents the input-output mapping of a building block l for incremental inputs u l2 , u l1 and outputs y l2 , y l1 in a ResNet layer <ref type=""bibr"" target=""#b16"">[17]</ref>, where y li = u li + F(u li , {W l }),</p><formula xml:id=""formula_29"">M l = (u l2 − u l1 ) T (y l2 − y l1 )",0
"""#b3"">[4]</ref>, attacks of this form have proven difficult to prevent <ref type=""bibr"" target=""#b31"">[32,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b30"">31]</ref>. While optimal defenses have been developed for simple linear models <r",0
"FGM) and projected gradient descent (PGD) method may be modeled by ∆ <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b25"">26]</ref>. Given a DNN, we are interested in the (local) robustness of an arbitrary natural example u by ensuring that",0
"><ref type=""bibr"" target=""#b30"">31]</ref>. While optimal defenses have been developed for simple linear models <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b23"">24]</ref>, the over-parameterized nature of DNNs and the complexity of surfaces learned during training make the develo",0
"majority of -based attacks of different norms such as the fast gradient method (FGM) and projected gradient descent (PGD) method may be modeled by ∆ <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b25"">26]</ref>. Given a DNN, we are interes",0
"rically shown in the paper, our approach theoretically should provide a level of robustness against intermediate level attacks recently introduced in <ref type=""bibr"" target=""#b17"">[18]</ref>. Our work provides a theoretical backing for the empirical findings in <ref type=""bibr"" target=""#b27"">[28,</",0
"xploited for the convolutional layers to have the same final relationships as given in Theorem 3. More specifically, we can follow the steps given in <ref type=""bibr"" target=""#b15"">[16]</ref> to define the transformation occurring at a convolutional layer l for output feature i with any padding and",0
"ion of the input, U l,i = [u l,1,i , ..., u l,M l−1 ,i ] and then representing the filter coefficients in the form of a doubly block circulant matrix <ref type=""bibr"" target=""#b29"">[30]</ref>. Specifically, if F j,i is a matrix that encompasses convolution of f j,i with the j-th feature map in a vec",0
"aw signal is in a continuous, high-dimensional space and is not structured for human communication (e.g., unlike words).</p><p>Several recent studies <ref type=""bibr"" target=""#b60"">[61,</ref><ref type=""bibr"" target=""#b45"">46,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" target=""#b65 mic dictionaries for contrastive learning, and can be used with various pretext tasks. In this paper, we follow a simple instance discrimination task <ref type=""bibr"" target=""#b60"">[61,</ref><ref type=""bibr"" target=""#b62"">63,</ref><ref type=""bibr"" target=""#b1"">2]</ref>: a query matches a key if they computed by a network <ref type=""bibr"" target=""#b28"">[29]</ref>. Contrastive learning is at the core of several recent works on unsupervised learning <ref type=""bibr"" target=""#b60"">[61,</ref><ref type=""bibr"" target=""#b45"">46,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" target=""#b65 are in Figure <ref type=""figure"" target=""#fig_5"">3</ref>. Overall, all three mechanisms benefit from a larger K. A similar trend has been observed in <ref type=""bibr"" target=""#b60"">[61,</ref><ref type=""bibr"" target=""#b55"">56]</ref> under the memory bank mechanism, while here we show that this trend rastive learning vs. pretext tasks. Various pretext tasks can be based on some form of contrastive loss functions. The instance discrimination method <ref type=""bibr"" target=""#b60"">[61]</ref> is related to the exemplar-based task <ref type=""bibr"" target=""#b16"">[17]</ref> and NCE <ref type=""bibr"" tar </p><formula xml:id=""formula_0"">Lq = − log exp(q•k+/τ ) K i=0 exp(q•ki/τ )<label>(1)</label></formula><p>where τ is a temperature hyper-parameter per <ref type=""bibr"" target=""#b60"">[61]</ref>. The sum is over one positive and K negative samples. Intuitively, this loss is the log loss of a (K+1)-way "">[2]</ref>, which may complicate the transfer of these networks to downstream tasks.</p><p>Another mechanism is the memory bank approach proposed by <ref type=""bibr"" target=""#b60"">[61]</ref> (Figure <ref type=""figure"">2b</ref>). A memory bank consists of the representations of all samples in the da out the encoders at multiple different steps all over the past epoch and thus are less consistent. A momentum update is adopted on the memory bank in <ref type=""bibr"" target=""#b60"">[61]</ref>. Its momentum update is on the representations of the same sample, not the encoder. This momentum update is xt tasks. As the focus of this paper is not on designing a new pretext task, we use a simple one mainly following the instance discrimination task in <ref type=""bibr"" target=""#b60"">[61]</ref>, to which some recent works <ref type=""bibr"" target=""#b62"">[63,</ref><ref type=""bibr"" target=""#b1"">2]</ref> "">[61]</ref>, to which some recent works <ref type=""bibr"" target=""#b62"">[63,</ref><ref type=""bibr"" target=""#b1"">2]</ref> are related.</p><p>Following <ref type=""bibr"" target=""#b60"">[61]</ref>, we consider a query and a key as a positive pair if they originate from the same image, and otherwise as a ""bibr"" target=""#b32"">[33]</ref> as the encoder, whose last fully-connected layer (after global average pooling) has a fixed-dimensional output (128-D <ref type=""bibr"" target=""#b60"">[61]</ref>). This output vector is normalized by its L2-norm <ref type=""bibr"" target=""#b60"">[61]</ref>. This is the rep al average pooling) has a fixed-dimensional output (128-D <ref type=""bibr"" target=""#b60"">[61]</ref>). This output vector is normalized by its L2-norm <ref type=""bibr"" target=""#b60"">[61]</ref>. This is the representation of the query or key. The temperature τ in Eqn.( <ref type=""formula"" target=""#for 61]</ref>. This is the representation of the query or key. The temperature τ in Eqn.( <ref type=""formula"" target=""#formula_0"">1</ref>) is set as 0.07 <ref type=""bibr"" target=""#b60"">[61]</ref>. The data augmentation setting follows <ref type=""bibr"" target=""#b60"">[61]</ref>: a 224×224-pixel crop is ta qn.( <ref type=""formula"" target=""#formula_0"">1</ref>) is set as 0.07 <ref type=""bibr"" target=""#b60"">[61]</ref>. The data augmentation setting follows <ref type=""bibr"" target=""#b60"">[61]</ref>: a 224×224-pixel crop is taken from a randomly resized image, and then undergoes random color jittering, ran Algorithm 1) in 8 GPUs, and an initial learning rate of 0.03. We train for 200 epochs with the learning rate multiplied by 0.1 at 120 and 160 epochs <ref type=""bibr"" target=""#b60"">[61]</ref>, taking ∼53 hours training ResNet-50. For IG-1B, we use a mini-batch size of 1024 in 64 GPUs, and a learning nd MoCo, and is K−1 in end-to-end (offset by one because the positive key is in the same mini-batch). The network is ResNet-50.</p><p>The memory bank <ref type=""bibr"" target=""#b60"">[61]</ref> mechanism can support a larger dictionary size. But it is 2.6% worse than MoCo. This is inline with our hypo the larger-scale data may not be fully exploited. We hope an advanced pretext task will improve this. Beyond the simple instance discrimination task <ref type=""bibr"" target=""#b60"">[61]</ref>, it is possible to adopt MoCo for pretext tasks like masked auto-encoding, e.g., in language <ref type=""bibr tei-c.org/ns/1.0"" place=""foot"" n=""2"" xml:id=""foot_1"">Here 58.0% is with InfoNCE and K=65536. We reproduce 54.3% when using NCE and K=4096 (the same as<ref type=""bibr"" target=""#b60"">[61]</ref>), close to 54.0% in<ref type=""bibr"" target=""#b60"">[61]</ref>.</note> 			<note xmlns=""http://www.tei-c.org/ns 0% is with InfoNCE and K=65536. We reproduce 54.3% when using NCE and K=4096 (the same as<ref type=""bibr"" target=""#b60"">[61]</ref>), close to 54.0% in<ref type=""bibr"" target=""#b60"">[61]</ref>.</note> 			<note xmlns=""http://www.tei-c.org/ns/1.0"" place=""foot"" n=""3"" xml:id=""foot_2"">Parameters are of th as k + . Contrastive loss functions can also be based on other forms <ref type=""bibr"" target=""#b28"">[29,</ref><ref type=""bibr"" target=""#b58"">59,</ref><ref type=""bibr"" target=""#b60"">61,</ref><ref type=""bibr"" target=""#b35"">36]</ref>, such as margin-based losses and variants of NCE losses. The contrast = f k (x k )). Their instantiations depend on the specific pretext task. The input x q and x k can be images <ref type=""bibr"" target=""#b28"">[29,</ref><ref type=""bibr"" target=""#b60"">61,</ref><ref type=""bibr"" target=""#b62"">63]</ref>, patches <ref type=""bibr"" target=""#b45"">[46]</ref>, or context consis acy depends on the detector structure. For the C4 backbone, by default used in existing ResNet-based results <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b60"">61,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b65"">66]</ref>, the advantage of unsupervise",1
", and can be used with various pretext tasks. In this paper, we follow a simple instance discrimination task <ref type=""bibr"" target=""#b60"">[61,</ref><ref type=""bibr"" target=""#b62"">63,</ref><ref type=""bibr"" target=""#b1"">2]</ref>: a query matches a key if they are encoded views (e.g., different crops s a natural mechanism (e.g., <ref type=""bibr"" target=""#b28"">[29,</ref><ref type=""bibr"" target=""#b45"">46,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" target=""#b62"">63,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b34"">35]</ref>, Figure <ref type=""figure"">2a</ nd on the specific pretext task. The input x q and x k can be images <ref type=""bibr"" target=""#b28"">[29,</ref><ref type=""bibr"" target=""#b60"">61,</ref><ref type=""bibr"" target=""#b62"">63]</ref>, patches <ref type=""bibr"" target=""#b45"">[46]</ref>, or context consisting a set of patches <ref type=""bibr"" t target=""#b45"">[46]</ref>. The networks f q and f k can be identical <ref type=""bibr"" target=""#b28"">[29,</ref><ref type=""bibr"" target=""#b58"">59,</ref><ref type=""bibr"" target=""#b62"">63]</ref>, partially shared <ref type=""bibr"" target=""#b45"">[46,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type= task, we use a simple one mainly following the instance discrimination task in <ref type=""bibr"" target=""#b60"">[61]</ref>, to which some recent works <ref type=""bibr"" target=""#b62"">[63,</ref><ref type=""bibr"" target=""#b1"">2]</ref> are related.</p><p>Following <ref type=""bibr"" target=""#b60"">[61]</ref> 61]</ref>, we consider a query and a key as a positive pair if they originate from the same image, and otherwise as a negative sample pair. Following <ref type=""bibr"" target=""#b62"">[63,</ref><ref type=""bibr"" target=""#b1"">2]</ref>, we take two random ""views"" of the same image under random data augmen",1
"are the gaps of at least +0.5 point.</p><p>ity of features, so our experiments are on controlled schedules, e.g., the 1× (∼12 epochs) or 2× schedules <ref type=""bibr"" target=""#b21"">[22]</ref> for COCO, in contrast to 6×∼9× in <ref type=""bibr"" target=""#b30"">[31]</ref>. On smaller datasets like VOC, t We fine-tune all layers end-to-end. We finetune on the train2017 set (∼118k images) and evaluate on val2017. The schedule is the default 1× or 2× in <ref type=""bibr"" target=""#b21"">[22]</ref>.</p><p>Results. Table <ref type=""table"" target=""#tab_7"">5</ref> shows the results on COCO with the FPN (Tabl",0
"""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" target=""#b65"">66,</ref><ref type=""bibr"" target=""#b34"">35,</ref><ref type=""bibr"" target=""#b55"">56,</ref><ref type=""bibr"" target=""#b1"">2]</ref> present promising results on unsupervised visual representation learning using approaches related to the contra asks. In this paper, we follow a simple instance discrimination task <ref type=""bibr"" target=""#b60"">[61,</ref><ref type=""bibr"" target=""#b62"">63,</ref><ref type=""bibr"" target=""#b1"">2]</ref>: a query matches a key if they are encoded views (e.g., different crops) of the same image. Using this pretext ""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" target=""#b65"">66,</ref><ref type=""bibr"" target=""#b34"">35,</ref><ref type=""bibr"" target=""#b55"">56,</ref><ref type=""bibr"" target=""#b1"">2]</ref>, which we elaborate on later in context (Sec. 3.1).</p><p>Adversarial losses <ref type=""bibr"" target=""#b23"">[24 >59,</ref><ref type=""bibr"" target=""#b62"">63]</ref>, partially shared <ref type=""bibr"" target=""#b45"">[46,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" target=""#b1"">2]</ref>, or different <ref type=""bibr"" target=""#b55"">[56]</ref>. x q encoder encoder q encoder k q</p><formula xml:id="" ation <ref type=""bibr"" target=""#b24"">[25]</ref>. Some recent methods <ref type=""bibr"" target=""#b45"">[46,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" target=""#b1"">2]</ref> are based on pretext tasks driven by local positions, where the dictionary size can be made larger by multiple ng the instance discrimination task in <ref type=""bibr"" target=""#b60"">[61]</ref>, to which some recent works <ref type=""bibr"" target=""#b62"">[63,</ref><ref type=""bibr"" target=""#b1"">2]</ref> are related.</p><p>Following <ref type=""bibr"" target=""#b60"">[61]</ref>, we consider a query and a key as a posi as a positive pair if they originate from the same image, and otherwise as a negative sample pair. Following <ref type=""bibr"" target=""#b62"">[63,</ref><ref type=""bibr"" target=""#b1"">2]</ref>, we take two random ""views"" of the same image under random data augmentation to form a positive pair. The queri bibr"" target=""#b28"">[29,</ref><ref type=""bibr"" target=""#b45"">46,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" target=""#b62"">63,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b34"">35]</ref>, Figure <ref type=""figure"">2a</ref>). It uses samples in the current mi s may require special network designs such as patchifying the input <ref type=""bibr"" target=""#b45"">[46]</ref> or customizing the receptive field size <ref type=""bibr"" target=""#b1"">[2]</ref>, which may complicate the transfer of these networks to downstream tasks.</p><p>Another mechanism is the memor abels.</p><p>patchified inputs <ref type=""bibr"" target=""#b45"">[46,</ref><ref type=""bibr"" target=""#b34"">35]</ref>, carefully tailored receptive fields <ref type=""bibr"" target=""#b1"">[2]</ref>, or combining two networks <ref type=""bibr"" target=""#b55"">[56]</ref>. By using an architecture that is not cus",0
"g in downstream tasks (e.g., <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" target=""#b42"">43,</ref><ref type=""bibr"" target=""#b51"">52]</ref>). Next we compare MoCo with ImageNet supervised pre-training, transferred to various tasks including PASCAL V r search.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.2.1"">PASCAL VOC Object Detection</head><p>Setup. The detector is Faster R-CNN <ref type=""bibr"" target=""#b51"">[52]</ref> with a backbone of R50-dilated-C5 or R50-C4 <ref type=""bibr"" target=""#b31"">[32]</ref> (details in appendix),",0
". But the dictionary size is coupled with the mini-batch size, limited by the GPU memory size. It is also challenged by large mini-batch optimization <ref type=""bibr"" target=""#b24"">[25]</ref>. Some recent methods <ref type=""bibr"" target=""#b45"">[46,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref t Here the largest mini-batch a high-end machine (8 Volta 32GB GPUs) can afford is 1024. More essentially, large mini-batch training is an open problem <ref type=""bibr"" target=""#b24"">[25]</ref>: we found it necessary to use the linear learning rate scaling rule <ref type=""bibr"" target=""#b24"">[25]</ref mini-batch training is an open problem <ref type=""bibr"" target=""#b24"">[25]</ref>: we found it necessary to use the linear learning rate scaling rule <ref type=""bibr"" target=""#b24"">[25]</ref> here, without which the accuracy drops (by ∼2% with a 1024 mini-batch). But optimizing with a larger mini-ba r"" target=""#b24"">[25]</ref> here, without which the accuracy drops (by ∼2% with a 1024 mini-batch). But optimizing with a larger mini-batch is harder <ref type=""bibr"" target=""#b24"">[25]</ref>, and it is questionable whether the trend can be extrapolated into a larger K even if memory is sufficient.",0
"=""#b45"">46,</ref><ref type=""bibr"" target=""#b34"">35]</ref>, and R170 is made wider <ref type=""bibr"" target=""#b34"">[35]</ref>; Rv50 is a reversible net <ref type=""bibr"" target=""#b22"">[23]</ref>, RX50 is ResNeXt-50-32×8d <ref type=""bibr"" target=""#b61"">[62]</ref>. † : Pre-training uses FastAutoAugment <",0
"oCo with the better supervised pre-training variant in this task. MoCo with IG-1B surpasses it in all metrics.</p><p>Cityscapes instance segmentation <ref type=""bibr"" target=""#b9"">[10]</ref>: MoCo with IG-1B is on par with its supervised pre-training counterpart in AP mk , and is higher in AP mk 50 >: MoCo with IG-1B is on par with its supervised pre-training counterpart in AP mk , and is higher in AP mk 50 . Semantic segmentation: On Cityscapes <ref type=""bibr"" target=""#b9"">[10]</ref>, MoCo outperforms its supervised pre-training counterpart by up to 0.9 point. But on VOC semantic segmentatio",0
"3"">[64,</ref><ref type=""bibr"" target=""#b64"">65]</ref>. Some pretext tasks form pseudo-labels by, e.g., transformations of a single (""exemplar"") image <ref type=""bibr"" target=""#b16"">[17]</ref>, patch orderings <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b44"">45]</ref>, tracking orm of contrastive loss functions. The instance discrimination method <ref type=""bibr"" target=""#b60"">[61]</ref> is related to the exemplar-based task <ref type=""bibr"" target=""#b16"">[17]</ref> and NCE <ref type=""bibr"" target=""#b27"">[28]</ref>. The pretext task in contrastive predictive coding (CPC) <",0
"ntrastive learning; we do not explore orthogonal factors (such as specific pretext tasks) that may further improve accuracy. As an example, ""MoCo v2"" <ref type=""bibr"" target=""#b7"">[8]</ref>, an extension of a preliminary version of this manuscript, achieves 71.1% accuracy with R50 (up from 60.6%), g",0
"-source 100 Gbit/s TCP/IP network stack on an FPGA. Limago explores the changes needed to upgrade an existing open-source TCP/IP stack from 10 Gbit/s <ref type=""bibr"" target=""#b10"">[11]</ref> to 100 Gbit/s, but maintaining the same high-productivity design methodology, based on Vivado-HLS, that was nd up to 40 Gbit/s but only receive up to 4 Gbit/s. The starting point for Limago is a 10 Gbit/s TOE written by Sidler et al. in C++ using Vivado-HLS <ref type=""bibr"" target=""#b10"">[11]</ref>, <ref type=""bibr"" target=""#b28"">[29]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>IV. LIM /ref>). It is divided into three parts, the incoming data path (Rx Engine), the outgoing data path (Tx Engine), and the state-keeping data structures <ref type=""bibr"" target=""#b10"">[11]</ref>, <ref type=""bibr"" target=""#b28"">[29]</ref>. The dash boxes are optional modules that can be enabled at synth",1
"s up the possibility of tailored network designs through Smart Network Interface Cards (NICs), which push application-level processing to the network <ref type=""bibr"" target=""#b1"">[2]</ref>. As a result, we are witnessing a flurry of activity around programmable networks based on a variety of design",0
"ofer HHI 10 GbE TCP/IP) with kernel-bypassing, concluding that the hardware solution has less latency and a more deterministic behaviour. The work in <ref type=""bibr"" target=""#b25"">[26]</ref> presents a complete TOE implementation supporting jumbo frames and configurable Maximum Segment Size (MSS) a",0
"size is configurable from 4 KiB to 64 KiB and it supports up to 128 connections per instantiated IP-Core and out-of-order packet delivery. Algo-Logic <ref type=""bibr"" target=""#b23"">[24]</ref> supports full duplex rates up to 20 Gbit/s per instance, claiming more than 200 Gbit/s can be achieved with",0
"ntroduction</head><p>Pre-trained language representation models (PLMs) such as ELMo <ref type=""bibr"" target=""#b29"">(Peters et al., 2018a)</ref>, BERT <ref type=""bibr"" target=""#b9"">(Devlin et al., 2019a)</ref> and XLNet <ref type=""bibr"" target=""#b46"">(Yang et al., 2019)</ref> learn effective language",1
"f> as its encoder, <ref type=""bibr"" target=""#b33"">Radford et al. (2018)</ref> demonstrate a pre-trained generative model (GPT) and its effects, while <ref type=""bibr"" target=""#b10"">Devlin et al. (2019b)</ref> release a pre-trained deep Bidirectional Encoder Representation from Transformers (BERT), a se a pre-trained deep Bidirectional Encoder Representation from Transformers (BERT), achieving state-of-the-arts on dozens of benchmarks.</p><p>After <ref type=""bibr"" target=""#b10"">Devlin et al. (2019b)</ref>, similar pre-trained encoders spring up recently. <ref type=""bibr"" target=""#b46"">Yang et al ntations from their descriptions.</p><p>For L LM , many alternatives for pre-trained language representation can be used, e.g., masked language model <ref type=""bibr"" target=""#b10"">(Devlin et al., 2019b)</ref>. Note that those two tasks only share the text encoder and for each mini-batch, text sampl here [CLS] and [EOS] are two special tokens. Model output at [CLS] is often used as the sentence representation.</p><p>PLM Objective Inspired by BERT <ref type=""bibr"" target=""#b10"">(Devlin et al., 2019b)</ref>, MLM randomly selects 15% of input tokens, among which 80% are masked with the special mar we fine-tune KEPLER on downstream tasks and use [CLS] output for sentence-level prediction and the outputs of all tokens for sequence labelling tasks <ref type=""bibr"" target=""#b10"">(Devlin et al., 2019b)</ref> </p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4"">Wikidata5m</head><p>We con ei-c.org/ns/1.0""><head>Model Structure</head><p>We use the transformer architecture <ref type=""bibr"" target=""#b41"">(Vaswani et al., 2017)</ref> as in <ref type=""bibr"" target=""#b10"">(Devlin et al., 2019b;</ref><ref type=""bibr"" target=""#b20"">Liu et al., 2019c)</ref>, which we will not address in detai",1
")</ref> and dialogue system <ref type=""bibr"" target=""#b22"">(Madotto et al., 2018)</ref>, but also some early works use text as additional information <ref type=""bibr"" target=""#b44"">(Xie et al., 2016;</ref><ref type=""bibr"" target=""#b22"">An et al., 2018)</ref> or jointly train the knowledge and text e x operations based on it. RotatE <ref type=""bibr"" target=""#b37"">(Sun et al., 2019a)</ref> combines the advantages of both of them. Among these works, <ref type=""bibr"" target=""#b44"">Xie et al. (2016)</ref> propose to utilize entity descriptions as an external information source and introduce an entit",1
"e are not only many works integrating knowledge embeddings into NLP models to improve the performance of NLP applications such as machine translation <ref type=""bibr"" target=""#b47"">(Zaremoodi et al., 2018)</ref>, reading comprehension <ref type=""bibr"">(Mihaylov and Frank, 2018;</ref><ref type=""bibr""",0
"rget=""#tab_6"">6</ref>, for both frameworks, our models have superior performance over others. We have also compared with current state-of-the-art MTP <ref type=""bibr"" target=""#b1"">(Baldini Soares et al., 2019)</ref>, which outperforms us a little. But note that MTP uses a large version of BERT while _0""><head>Table 1 :</head><label>1</label><figDesc>. For supervised relation extraction and fewshot relation extraction, we follow the approaches from<ref type=""bibr"" target=""#b1"">(Baldini Soares et al., 2019)</ref> and<ref type=""bibr"" target=""#b11"">(Gao et al., 2019)</ref> respectively. Statistics",0
"history of pre-training in NLP. Early works focus on distributed word representation <ref type=""bibr"" target=""#b6"">(Collobert and Weston, 2008;</ref><ref type=""bibr"" target=""#b25"">Mikolov et al., 2013;</ref><ref type=""bibr"" target=""#b27"">Pennington et al., 2014)</ref>, many of which are still often ive</head><p>We use the loss formula from <ref type=""bibr"" target=""#b38"">(Sun et al., 2019b)</ref> as our KE objective, which takes negative sampling <ref type=""bibr"" target=""#b25"">(Mikolov et al., 2013)</ref> for efficient optimization:</p><formula xml:id=""formula_3"">L = − log σ(γ − d r (h, t)) − n",0
"d checkpoint of RoBERTaBASE for the initialization of our model. However, we still keep MLM as one of our objectives to avoid catastrophic forgetting <ref type=""bibr"" target=""#b23"">(McCloskey and Cohen, 1989)</ref> while training towards the KRL loss. Note that experiments show that only further pre",0
", reading comprehension <ref type=""bibr"">(Mihaylov and Frank, 2018;</ref><ref type=""bibr"" target=""#b50"">Zhong et al., 2019)</ref> and dialogue system <ref type=""bibr"" target=""#b22"">(Madotto et al., 2018)</ref>, but also some early works use text as additional information <ref type=""bibr"" target=""#b4 22"">(Madotto et al., 2018)</ref>, but also some early works use text as additional information <ref type=""bibr"" target=""#b44"">(Xie et al., 2016;</ref><ref type=""bibr"" target=""#b22"">An et al., 2018)</ref> or jointly train the knowledge and text embedding in the same space <ref type=""bibr"" target=""#b4",0
"type=""bibr"" target=""#b7"">Dai and Le (2015)</ref> first propose to train an auto-encoder on unlabeled data, and then fine-tune it on downstream tasks. <ref type=""bibr"" target=""#b15"">Howard and Ruder (2018)</ref> propose a universal language model (ULMFiT) based on AWD-LSTM <ref type=""bibr"" target=""#b",0
"nslations of heads, while DistMult <ref type=""bibr"" target=""#b45"">(Yang et al., 2015)</ref> use matrix multiplications as score functions and ComplEx <ref type=""bibr"" target=""#b40"">(Trouillon et al., 2016)</ref> adopt complex operations based on it. RotatE <ref type=""bibr"" target=""#b37"">(Sun et al., , 2013)</ref> 109370 0.253 0.170 0.311 0.392 DistMult <ref type=""bibr"" target=""#b45"">(Yang et al., 2015)</ref> 211030 0.253 0.208 0.278 0.334 ComplEx <ref type=""bibr"" target=""#b40"">(Trouillon et al., 2016)</ref>  prediction task.</p><p>We conduct 5 knowledge graph embedding models , including TransE cluding TransE <ref type=""bibr"" target=""#b2"">(Bordes et al., 2013)</ref>, Dist-Mult <ref type=""bibr"" target=""#b45"">(Yang et al., 2015)</ref>, ComplEx <ref type=""bibr"" target=""#b40"">(Trouillon et al., 2016)</ref>, SimplE (Kazemi and Poole, 2018) and Ro-tatE <ref type=""bibr"" target=""#b38"">(Sun et al.,",0
"44"">(Xie et al., 2016;</ref><ref type=""bibr"" target=""#b22"">An et al., 2018)</ref> or jointly train the knowledge and text embedding in the same space <ref type=""bibr"" target=""#b43"">(Wang et al., 2014;</ref><ref type=""bibr"" target=""#b39"">Toutanova et al., 2015;</ref><ref type=""bibr"" target=""#b12"">Han",0
"ed noise and enhance its ability to generalize.</p><p>Inspired by the success of augmentation methods in ASR <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b16"">17]</ref>, as a remedy to avoid overfitting while using lowresource translated speech data, we study the use of spectro",1
"r"" target=""#b5"">[6]</ref> and MT <ref type=""bibr"" target=""#b6"">[7]</ref><ref type=""bibr"" target=""#b7"">[8]</ref><ref type=""bibr"" target=""#b8"">[9]</ref><ref type=""bibr"" target=""#b9"">[10]</ref><ref type=""bibr"" target=""#b10"">[11]</ref> have inspired the end-to-end direct ST models which can be trained u f> similar to <ref type=""bibr"" target=""#b18"">[19]</ref>. We only focus on LSTMbased models and leave the transformer architecture as our future study <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" target=""#b20"">21]</ref>. An abstract overview of the",0
"ten make the model invariant to the applied noise and enhance its ability to generalize.</p><p>Inspired by the success of augmentation methods in ASR <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b16"">17]</ref>, as a remedy to avoid overfitting while using lowresource translated erfitting while using lowresource translated speech data, we study the use of spectrogram augmentation (SpecAugment) for direct ST model. SpecAugment <ref type=""bibr"" target=""#b15"">[16]</ref> is a simple and low-implementation cost approach. Unlike traditional speech augmentation methods that direct ing data.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2."">Spectrogram Augmentation</head><p>In spectrogram augmentation (SpecAugment) <ref type=""bibr"" target=""#b15"">[16]</ref>, we randomly apply masking in consecutive frames in the time axis as well as consecutive dimensions in the f asks. We note that we standardize the 1 We choose time position differently from the original paper where they select t in the interval of [0, T − τ) <ref type=""bibr"" target=""#b15"">[16]</ref>. 2 It is not clear whether the original paper allows replacement or not. 3 Again we note the difference betw lows replacement or not. 3 Again we note the difference between our implementation and the original paper where the selection interval is [0, ν − φ ) <ref type=""bibr"" target=""#b15"">[16]</ref>. features to zero mean and variance of one. Therefore, masking to zero is equivalent to setting it to the me BLEU and 1.8% in TER (see Table <ref type=""table"" target=""#tab_4"">6</ref>). For IWSLT, we also apply a policy similar to the LD policy of main paper <ref type=""bibr"" target=""#b15"">[16]</ref> as listed in the last row of Table <ref type=""table"" target=""#tab_4"">6</ref>. As shown, this setup is not th",0
"ibed data and a machine translation (MT) model trained on bilingual text data. Recent advancements in both ASR <ref type=""bibr"" target=""#b0"">[1]</ref><ref type=""bibr"" target=""#b1"">[2]</ref><ref type=""bibr"" target=""#b2"">[3]</ref><ref type=""bibr"" target=""#b3"">[4]</ref><ref type=""bibr"" target=""#b4"">[5]",0
"dataset package. Hence, we end up to 200h of clean speech corresponding to 94.5k segments for the ST task. We apply 40-dimensional Gammatone features <ref type=""bibr"" target=""#b25"">[26]</ref> using the RASR feature extractor <ref type=""bibr"" target=""#b26"">[27]</ref>. For MT training, we utilize no a",0
"=""#b39"">[40]</ref>, many efforts <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b28"" g <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b29"">30]</ref>. Meanwhile, recent works <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b44"">45]</ref> also suggest that training with adversarial examples on large dataset this distribution mismatch between clean examples and adversarial examples is a key factor that causes the performance degradation in previous works <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b44"">45]</ref>.</p><p>In this paper, we pro noise (e.g., Tab. 5 in <ref type=""bibr"" target=""#b17"">[18]</ref> shows the result of training with random normal perturbations) or adversarial noise <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b41"">42]</ref>, fail to improve accuracy on lla training baseline. These results contradict previous conclusions <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b41"">42,</ref><ref type=""bibr"" target=""#b15"">16]</ref> that the performance degradation is always observed if adversarial examples are used for training. We hereby 85.1% top-1 accuracy on ImageNet, which beats the vanilla training baseline by 0.6%. However, previous works <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b15"">16]</ref> show adversarial training always degrades performance.</p><p>Compared to <ref type=""bibr"" target=""#b17"">[18,< type=""bibr"" target=""#b15"">16]</ref> show adversarial training always degrades performance.</p><p>Compared to <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b15"">16]</ref>, we make two changes in our reimplementation: (1) using stronger networks; and (2) training with weaker attac",1
"pe=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b28"">29,</ref><ref type=""bibr"" target=""#b35"" examples and adversarial examples is a key factor that causes the performance degradation in previous works <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b44"">45]</ref>.</p><p>In this paper, we propose AdvProp, short for Adversarial Propa al examples, constitutes the current foundation of state-of-the-arts for defending against adversarial attacks <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b44"">45]</ref>. Although adversarial trainin target=""#b17"">[18]</ref> shows the result of training with random normal perturbations) or adversarial noise <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b41"">42]</ref>, fail to improve accuracy on clean images. ial examples to effectivel atasets but in the semi-supervised setting <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b29"">30]</ref>. Meanwhile, recent works <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b44"">45]</ref> also suggest that training w >4</ref> and Tab. 1, AdvProp improves models for better recognition than the vanilla training baseline. These results contradict previous conclusions <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b41"">42,</ref><ref type=""bibr"" target=""#b15"">16]</ref> that the performance degrada dversarially trained EfficientNet-B7 has 85.1% top-1 accuracy on ImageNet, which beats the vanilla training baseline by 0.6%. However, previous works <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b15"">16]</ref> show adversarial training always degrades performance.</p><p>Compare type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b15"">16]</ref> show adversarial training always degrades performance.</p><p>Compared to <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b15"">16]</ref>, we make two changes in our reimplementation: (1) using stronger net ion: creating additional training samples by injecting noise. However, all previous attempts, by augmenting either with random noise (e.g., Tab. 5 in <ref type=""bibr"" target=""#b17"">[18]</ref> shows the result of training with random normal perturbations) or adversarial noise <ref type=""bibr"" target= cker to Gradient Descent (GD) as it removes the projection step in PGD; or (2) we skip the random noise initialization step in PGD, turn it to I-FGSM <ref type=""bibr"" target=""#b17"">[18]</ref>. Other attack hyper-parameters are unchanged: the maximum perturbation size =4 (if applicable), number of at els with different adversarial attacker. B3 B5 B7 Vanilla Training 81.7 83.7 84.5 PGD <ref type=""bibr"" target=""#b22"">[23]</ref> 81.8 84.3 85.2 I-FGSM <ref type=""bibr"" target=""#b17"">[18]</ref> 81.9 84. In Sec. 5.3, we show that adversarial training can improve performance if large EfficientNets are u l training samples and train networks with a mixture of adversarial examples and clean images, as suggested in <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b17"">18]</ref>,</p><formula xml:id=""formula_3"">arg min θ E (x,y)∼D L(θ, x, y) + max ∈S L(θ, x + , y) .<label>(3)</label></fo dels should enjoy the benefits from both adversarial and clean domains. However, as observed in former studies <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b17"">18]</ref>, directly optimizing Eq. ( <ref type=""formula"" target=""#formula_3"">3</ref>) generally yields lower performanc ed learning framework enables networks to get much stronger performance than the adversarial training baseline <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b17"">18]</ref>. Besides, compared to the fine-tuning strategy in Sec. 3, Ad-vProp also demonstrates superior performance as",1
"=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" target=""#b33"">34,</ref><ref type=""bibr"" target=""#b43"">44,</ref><ref type=""bibr"" target=""#b47"">48]</ref>, or network overparameterization <ref type=""bibr"" target=""#b30"">[31]</ref>.</p><p>This paper focuses on stand",0
"on, which applies a set of label-preserving transformations to images, serves as an important and effective role to prevent networks from overfitting <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b34"">35,</ref><ref type=""bibr"" target=""#b7"">8]</ref>. Besides traditional methods l",0
"ese models. Since the first discovery of the vulnerability of ConvNets to adversarial attacks <ref type=""bibr"" target=""#b39"">[40]</ref>, many efforts <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b15"">16",0
"f the vulnerability of ConvNets to adversarial attacks <ref type=""bibr"" target=""#b39"">[40]</ref>, many efforts <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b17"">1 tion but are restricted to certain situations-the improvement is only observed either on small datasets (e.g., MNIST) in the fully-supervised setting <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b19"">20]</ref>, or on larger datasets but in the semi-supervised setting <ref type=""b g, which trains networks with adversarial examples, constitutes the current foundation of state-of-the-arts for defending against adversarial attacks <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b44""> e treat adversarial images as additional training samples and train networks with a mixture of adversarial examples and clean images, as suggested in <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b17"">18]</ref>,</p><formula xml:id=""formula_3"">arg min θ E (x,y)∼D L(θ, x, y) + max ∈ l></formula><p>Ideally, such trained models should enjoy the benefits from both adversarial and clean domains. However, as observed in former studies <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b17"">18]</ref>, directly optimizing Eq. ( <ref type=""formula"" target=""#formula_3"">3</ p>Experiments show that such disentangled learning framework enables networks to get much stronger performance than the adversarial training baseline <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b17"">18]</ref>. Besides, compared to the fine-tuning strategy in Sec. 3, Ad-vProp als ref type=""bibr"" target=""#b4"">[5]</ref> AdvProp (ours)</p><p>Figure <ref type=""figure"">5</ref>. AdvProp substantially outperforms adversarial training <ref type=""bibr"" target=""#b6"">[7]</ref> on ImageNet, especially for small models.</p><p>Comparison Results. We compare AdvProp to traditional adversar br"" target=""#b6"">[7]</ref> on ImageNet, especially for small models.</p><p>Comparison Results. We compare AdvProp to traditional adversarial training <ref type=""bibr"" target=""#b6"">[7]</ref>, and report evaluation results on Im-ageNet validation set in Fig. <ref type=""figure"">5</ref>. Compared to the",0
"pe=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b27"">28]</ref>, the limited amount of training data <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" target=""#b33"">34,</ref><ref type=""bibr"" target=""#b43"">44,</ref><ref type=""bibr"" target=""#b47""",0
"try to explain this tradeoff phenomenon from the perspective of the increased sample complexity of adversary <ref type=""bibr"" target=""#b36"">[37,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b27"">28]</ref>, the limited amount of training data <ref type=""bibr"" target=""#b0"">[1",0
"icientNet-B8, our method achieves the state-of-the-art 85.5% ImageNet top-1 accuracy without extra data. This result even surpasses the best model in <ref type=""bibr"" target=""#b23"">[24]</ref> which is trained with 3.5B Instagram images (∼3000× more than ImageNet) and ∼9.4× more parameters. Models ar ntNet-B8 achieves the state-of-the-art 85.5% top-1 accuracy on ImageNet without any extra data. This result even surpasses the best model reported in <ref type=""bibr"" target=""#b23"">[24]</ref>, which is pretrained on 3.5B extra Instagram images (∼3000× more than ImageNet) and requires ∼9.4× more para 84.8% to 85.5%, achieving a new state-of-the-art accuracy on ImageNet without using extra data. This result even surpasses the best model reported in <ref type=""bibr"" target=""#b23"">[24]</ref>, which is pretrained on 3.5B extra Instagram images (∼3000× more than ImageNet) and requires ∼9.4× more para e are the best results so far if models are not allowed to train with corresponding distortions <ref type=""bibr"" target=""#b5"">[6]</ref> or extra data <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b45"">46]</ref>.</p><p>To summarize, the results suggest that AdvProp significantly",0
"s automatically for achieving better performance on image classification <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b51"">52]</ref> and object detection <ref typ",0
"k. Gray highlights task learning, blue metalearning, and orange NAS components. Top: gradientbased meta-learning with fixed architecture such as MAML <ref type=""bibr"" target=""#b15"">[16]</ref> or REPTILE <ref type=""bibr"" target=""#b36"">[37]</ref>. Middle: applying NAS to metalearning such as AutoMeta igure"">3</ref>. The key contributions of this work are as follows:</p><p>1. We show that model-agnostic, gradient-based metalearning methods (such as <ref type=""bibr"" target=""#b15"">[16]</ref>) can very naturally be combined with recently proposed gradient-based NAS methods, such as DARTS <ref type="" 4.</p><p>In our work, we choose DARTS for neural architecture search because of conceptual similarities to gradient-based meta-learning, such as MAML <ref type=""bibr"" target=""#b15"">[16]</ref>, which will allow us to combine the two kinds of methods.</p><p>Neural Architecture Search for Meta-Learning org/ns/1.0""><head n=""3.2."">Gradient-based Meta-Learning of Neural Architectures</head><p>Similar to MAML's meta-learning strategy in the weight space <ref type=""bibr"" target=""#b15"">[16]</ref>, our goal is to meta-learn an architecture with corresponding weights which is able to quickly adapt to new This means we can use any gradient-based meta-learning algorithm ? not only for w but also for the architecture ?. As an example, one could use MAML <ref type=""bibr"" target=""#b15"">[16]</ref> as a meta-learning algorithm, which runs SGD on the meta-objective, yielding meta-updates</p><formula xml:id ></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>MiniImagenet</head><p>Omniglot Method # params 1-shot, 5-way 5-shot, 5-way 1-shot, 20 way MAML <ref type=""bibr"" target=""#b15"">[16]</ref> 30k 48. which might be of independent interest. Empirical results on standard few-shot learning benchmarks s rk. Gray highlights task learning, blue metalearning, and orange NAS components. Top: gradientbased meta-learning with fixed architecture such as MAML<ref type=""bibr"" target=""#b15"">[16]</ref> or REPTILE<ref type=""bibr"" target=""#b36"">[37]</ref>. Middle: applying NAS to metalearning such as AutoMeta<r t learning, i.e., learning new tasks from just a few examples. Prior work has proposed meta-learning methods for this problem that are model-agnostic <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b36"">37]</ref> and allow meta-learning weights of fixed neural architectures (see < ef><ref type=""bibr"" target=""#b18"">19]</ref>.</p><p>In this work, we focus on a particular class of approaches denoted as model-agnostic meta-learning <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b36"">37,</ref><ref type=""bibr"" target=""#b0"" Ti train ) the task learning algorithm or simply task-learner, where k refers to k iterations of learning/ weight updates (e.g., by SGD). Prior work <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b36"">37,</ref><ref type=""bibr"" target=""#b24"">25]</ref> considered a fixed, predefin -? i meta ) 15: end while 16: return w meta , ? meta 3. This is in contrast to prior work where the architecture is always fixed during meta-testing <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b36"">37]</ref>. Also prior work using NAS for meta-learning <ref type=""bibr"" target ly over-parameterized and therefore prone to overfitting when only very little data is available. Prior work <ref type=""bibr"" target=""#b39"">[40,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b18"">19]</ref> often approaches few-shot lea s for fewshot learning without regularization does not improve performance due to overfitting as reported by <ref type=""bibr"" target=""#b39"">[40,</ref><ref type=""bibr"" target=""#b15"">16]</ref>.</p><p>Results are presented in Table <ref type=""table"" target=""#tab_2"">2</ref>. Again, METANAS improves over",1
"s, e.g., via network morphisms <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b46"">47]</ref>, weight sharing <ref type=""bibr"" target=""#b44"">[45,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bib",0
"f>.</p><p>In this work, we focus on a particular class of approaches denoted as model-agnostic meta-learning <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b36"">37,</ref><ref type=""bibr"" target=""#b0"">1,</ref><ref type=""bibr"" target=""#b17"">1",0
"l. <ref type=""bibr"" target=""#b13"">[14]</ref> for a more thorough literature overview. Researchers often frame NAS as a reinforcement learning problem <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b58"">59,</ref><ref type=""bibr"" target=""#b56"">57,</ref><ref type=""bibr"" target=""#b59"">",0
"o kinds of methods.</p><p>Neural Architecture Search for Meta-Learning There has been some recent work on combining NAS and metalearning. Wong et al. <ref type=""bibr"" target=""#b52"">[53]</ref> train an automated machine learning (AutoML <ref type=""bibr"" target=""#b22"">[23]</ref>) system via reinforcem",0
"work's decision.</p><p>Yet, almost all these gradient obfuscation based defenses have proven vulnerable. In their recent seminal work, Athalye et al. <ref type=""bibr"" target=""#b1"">[2]</ref> presented a suite of strategies for estimating network gradients in the presence of gradient obfuscation. Adve ng our defense model thoroughly. We examine a wide range of possible attacks, including those having successfully circumvented many previous defenses <ref type=""bibr"" target=""#b1"">[2]</ref>. Under these attacks, we compare the worst-case robustness of our method with state-of-the-art defense methods target=""#b38"">39,</ref><ref type=""bibr"" target=""#b32"">33]</ref>.</p><p>Unfortunately, many of these methods have proven vulnerable by Athalye et al. <ref type=""bibr"" target=""#b1"">[2]</ref>, who introduced a set of attacking strategies, including a method called Backward Pass Differentiable Approxim >42,</ref><ref type=""bibr"" target=""#b21"">22]</ref>.</p><p>Thus far, gradient obfuscation is generally considered vulnerable (and at least incomplete) <ref type=""bibr"" target=""#b1"">[2]</ref>. We revisit gradient obfuscation, and our defense demonstrates unprecedented robustness against BPDA and other 13"">[14]</ref>.</p><p>These input-transformation-based defense mechanisms seem plausible. Yet they are all fragile. As demonstrated by Athalye et al. <ref type=""bibr"" target=""#b1"">[2]</ref>, with random input transformation, adversarial examples can still be found using Expectation over Transformati tiple trials (more details in Sec. 3.3). The noise-removal transformation is also ineffective. One can use Backward Pass Differentiable Approximation <ref type=""bibr"" target=""#b1"">[2]</ref> to easily construct effective adversarial examples. In short, the current consensus is that input transformati s/1.0""><head>Backward Pass Differentiable Approximation (BPDA).</head><p>To circumvent the defense using non-differentiable operators, Athalye et al. <ref type=""bibr"" target=""#b1"">[2]</ref> introduced a strategy called Backward Pass Differentiable Approximation (BPDA) to estimate the defense model's value h.</p><p>Our defense inherently thwarts this attacking strategy. It causes the adversary to suffer from either exploding or vanishing gradients <ref type=""bibr"" target=""#b1"">[2]</ref>. Figure <ref type=""figure"" target=""#fig_4"">3</ref>-left shows a 1D depiction illustrating this phenomenon in o target=""#b38"">[39,</ref><ref type=""bibr"" target=""#b32"">33]</ref>. Yet those defenses have been proven vulnerable under a reparameterization strategy <ref type=""bibr"" target=""#b1"">[2]</ref>. This strategy aims to find some differentiable function h(•) for a change-of-variable x = h(z) such that g(h( /ref><ref type=""bibr"" target=""#b32"">33]</ref>. In those defenses, the transformed image g(x) remain similar to the input x. Consequently, as shown in <ref type=""bibr"" target=""#b1"">[2]</ref>, those defenses can be easily circumvented by replacing g(•) with the identity mapping in the backward pass of of adversarially crafted images.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.1."">BPDA Attack and the Variants</head><p>BPDA attack <ref type=""bibr"" target=""#b1"">[2]</ref>, as reviewed in Sec. 3.3, is a powerful way to estimate network gradients that are obfuscated by defense metho acks. We also evaluate our defense against the black-box attacks, including the black-box transfer attack <ref type=""bibr"" target=""#b29"">[30]</ref>   <ref type=""bibr"" target=""#b1"">[2]</ref>. We evaluate other methods using the code provided in the original papers, training them using the same networ s, and A rob is the worst-case robust accuracy under all tested attacks. The methods indicated by a star (*) are those circumvented by Athalye et al. <ref type=""bibr"" target=""#b1"">[2]</ref>. We include their results therein as a reference. The other defense methods (including ours) all use ResNet18 not new; prior defenses also employ randomized transformations to the input. But they have been circumvented by Expectation Over Transformation (EOT) <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b2"">3]</ref>. EOT attack first estimates the gradient of expected f (g(x)) with respe gy, one must find an h(•) that constructs the adversarial examples of f b directly (so that g(h(•)) = h(•)), without solving the optimization problem <ref type=""bibr"" target=""#b1"">(2)</ref>. We argue that finding such an h(•) is extremely hard. If h(•) could be constructed, we would have a direct wa",1
"irst-order derivatives) Dg(x). A straightforward attempt to this end is by unrolling the iterative steps (3) and using automatic differentiation (AD) <ref type=""bibr"" target=""#b46"">[47]</ref> to compute Dg(x). Yet, this is infeasible. As shown in (3), the iterative steps involves non-differentiable",0
", a few other gradient obfuscation based defenses have been proposed <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b30"">31,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b41"">42,</ref><ref type=""bibr"" target=""#b21"">22]</ref>. But those works either repor under BPDA attacks <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b30"">31]</ref> or neglected the evaluation against BPDA attacks <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b41"">42,</ref><ref type=""bibr"" target=""#b21"">22]</ref>.</p><p>Thus far, gradient ob",0
"zed the problem of adversarial attacks and proposed Projected Gradient Descent (PGD) method, which further inspires many subsequent attacking methods <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b18"">",0
"""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b45"">46,</ref><ref type=""bibr"" target=""#b34"">35,</ref><ref type=""bibr"" target=""#b52"">53,</ref><ref type=""bibr"" target=""#b28"">29,</ref><ref type=""bibr"" target=""#b26"">27]</ref>. In comparison to adversarial",0
"ng Bayes' theorem is not tractable. 4 Words along the dependency path between the entities might be even more indicative of the relation, as noted by <ref type=""bibr"" target=""#b40"">Toutanova et al. (2015)</ref>. It is quite possible that using these techniques may further improve results, but we did",1
"eature extractors,  where the hidden vectors learned through a language modeling objective are then used in downstream language understanding systems <ref type=""bibr"" target=""#b6"">(Dai and Le, 2015;</ref><ref type=""bibr"" target=""#b22"">Melamud et al., 2016;</ref><ref type=""bibr"" target=""#b27"">Peters",0
"this problem of finding text sequences that optimize some continuous objective has been studied both in the context of end-to-end sequence generation <ref type=""bibr"" target=""#b15"">(Hoang et al., 2017)</ref>, and in the context of making small changes to an existing input in the context of adversari ue to the large size of vocabulary, it can only be approximated with beam search, or computed with more complicated continuous optimization algorithms<ref type=""bibr"" target=""#b15"">(Hoang et al., 2017)</ref>.</note> 		</body> 		<back>  			<div type=""acknowledgement""> <div xmlns=""http://www.tei-c.org",0
"35"">Sap et al., 2019)</ref>, or extract factual knowledge about relations between entities <ref type=""bibr"" target=""#b29"">(Petroni et al., 2019;</ref><ref type=""bibr"" target=""#b1"">Baldini Soares et al., 2019)</ref>. Regardless of the end task, the knowledge contained in LMs is probed by providing a",0
"nguage models (LM) transition from generating or evaluating the fluency of natural text <ref type=""bibr"" target=""#b25"">(Mikolov and Zweig, 2012;</ref><ref type=""bibr"" target=""#b24"">Merity et al., 2018;</ref><ref type=""bibr"" target=""#b23"">Melis et al., 2018;</ref><ref type=""bibr"" target=""#b10"">Gamon",0
"trategies include: modifying the training phase such that it covers a predefined set of downsampling kernels <ref type=""bibr"" target=""#b37"">[38,</ref><ref type=""bibr"" target=""#b12"">13]</ref>; using DNNs to capture only a natural-image prior which is decoupled from the SISR task <ref type=""bibr"" targ that gets as inputs both the LR image and the degradation model and assumes that the downsampling kernels belong to a certain set of Gaussian filters <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b37"">38]</ref>. Another approach builds on the structural prior of CNNs, which prom dered the blind SISR setting and developed kernel estimation methods <ref type=""bibr"" target=""#b26"">[27,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b1"">2]</ref>.</p><p>Finally, we would like to highlight major differences be-tween t b12"">13,</ref><ref type=""bibr"" target=""#b1"">2]</ref>.</p><p>Finally, we would like to highlight major differences be-tween this paper and the work in <ref type=""bibr"" target=""#b12"">[13]</ref>, whose ""kernel correction"" approach may be misunderstood as our ""correction filter"". In <ref type=""bibr"" tar per and the work in <ref type=""bibr"" target=""#b12"">[13]</ref>, whose ""kernel correction"" approach may be misunderstood as our ""correction filter"". In <ref type=""bibr"" target=""#b12"">[13]</ref>, three different DNNs (super-resolver, kernel estimator, and kernel corrector) are offline trained under the ence is that contrary to our approach, no pre-trained existing DNN methods (other than SRMD <ref type=""bibr"" target=""#b37"">[38]</ref>) can be used in <ref type=""bibr"" target=""#b12"">[13]</ref>. Secondly, their approach is restricted by the offline training assumptions to very certain type of downsamp ertain type of downsampling kernels, contrary to our approach. Thirdly, the concepts of these works are very different: The (iterative) correction in <ref type=""bibr"" target=""#b12"">[13]</ref> modifies the estimated downsampling kernel, while our correction filter modifies the LR image.</p></div> <di deferred to Appendix A. All the experiments are performed with the official code of each method. Unfortunately, such code has not been available for <ref type=""bibr"" target=""#b12"">[13]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.1."">The non-blind setting</head><p>In this se",1
"zes the classical WhittakerNyquistKotelnikovShannon sampling theorem <ref type=""bibr"" target=""#b33"">[34,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b24"">25]</ref>, which considers signals that are bandlimited in the frequency domain",0
"ed kernel estimation methods <ref type=""bibr"" target=""#b26"">[27,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b1"">2]</ref>.</p><p>Finally, we would like to highlight major differences be-tween this paper and the work in <ref type=""bib and then use this estimation to restore the HR image by <ref type=""bibr"" target=""#b9"">(10)</ref>. In this setting we compare our method to kernelGAN <ref type=""bibr"" target=""#b1"">[2]</ref>, which estimates the downsampling kernel using adversarial training (in test-time) and then uses ZSSR to resto robust to different kernels. From left to right and top to bottom: original image, cropped zoom of the original image, bicubic upsampling, kernelGAN <ref type=""bibr"" target=""#b1"">[2]</ref>, proSR <ref type=""bibr"" target=""#b32"">[33]</ref>, RCAN <ref type=""bibr"" target=""#b40"">[41]</ref>, DBPN <ref ty th standard deviation 1.5/ √ 2.From left to right and top to bottom: original image, cropped zoom of the original image, bicubic upsampling, kernelGAN<ref type=""bibr"" target=""#b1"">[2]</ref>, proSR<ref type=""bibr"" target=""#b32"">[33]</ref>, RCAN<ref type=""bibr"" target=""#b40"">[41]</ref>, DBPN<ref type=",0
"where the test data mismatch the training data, the leading DNN methods suffer from a huge performance drop <ref type=""bibr"" target=""#b36"">[37,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b28"">29]</ref>. Such scenarios include a downsampling kernel which is not the bicubi >; or completely avoid any offline training and instead train a CNN super-resolver from scratch at test time <ref type=""bibr"" target=""#b29"">[30,</ref><ref type=""bibr"" target=""#b25"">26]</ref>.</p><p>Contribution. In this work we take a different strategy, inspired by the generalized sampling literatu right and top to bottom: original image, cropped zoom of the original image, bicubic upsampling, SRMD <ref type=""bibr"" target=""#b37"">[38]</ref>, ZSSR <ref type=""bibr"" target=""#b25"">[26]</ref>, proSR <ref type=""bibr"" target=""#b32"">[33]</ref>, RCAN <ref type=""bibr"" target=""#b40"">[41]</ref>, DBPN <ref 33]</ref>. We compare our approach to other methods that receive the downsampling kernel k (or its estimation in the blind setting) as an input: ZSSR <ref type=""bibr"" target=""#b25"">[26]</ref> and SRMD <ref type=""bibr"" target=""#b37"">[38]</ref>. We also compare our method to DPSR <ref type=""bibr"" targ right and top to bottom: original image, cropped zoom of the original image, bicubic upsampling, SRMD <ref type=""bibr"" target=""#b37"">[38]</ref>, ZSSR <ref type=""bibr"" target=""#b25"">[26]</ref>, proSR <ref type=""bibr"" target=""#b32"">[33]</ref>, RCAN <ref type=""bibr"" target=""#b40"">[41]</ref>, DBPN <ref NR as produced by its official code is lower by more than 10 dB than the other examined methods (SRMD <ref type=""bibr"" target=""#b37"">[38]</ref>, ZSSR <ref type=""bibr"" target=""#b25"">[26]</ref>, DBPN <ref type=""bibr"" target=""#b13"">[14]</ref>, proSR <ref type=""bibr"" target=""#b32"">[33]</ref>, RCAN <ref o right and top to bottom: original image, cropped zoom of the original image, bicubic upsampling, SRMD<ref type=""bibr"" target=""#b37"">[38]</ref>, ZSSR<ref type=""bibr"" target=""#b25"">[26]</ref>, proSR<ref type=""bibr"" target=""#b32"">[33]</ref>, RCAN<ref type=""bibr"" target=""#b40"">[41]</ref>, DBPN<ref typ e formation model. A network performance tends to drop significantly if it has been trained for one acquisition model and then been tested on another <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b28"">29,</ref><ref type=""bibr"" target=""#b36"">37]</ref>.</p><p>Recently, different S hich promotes signals with spatially recurring patterns (e.g. natural images) and thus allows to train a super-resolver CNN from scratch at test time <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b29"">30]</ref>. Another line of work recovers the latent HR image by minimizing a c",0
"ef><ref type=""bibr"" target=""#b40"">41]</ref> and the perceptual quality <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b31"">32]</ref>. However, one main disadvantage of DNN-based SISR methods is their se",0
"is to embed all conflicting requirements in a constrained RL problem and to use a primal-dual algorithm as in <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b10"">11]</ref> that chooses the parameters automatically. The main advantage of this approach is that constraints ensure sat al results to establish that the family of primal-dual algorithms for constrained reinforcement learning, e.g. <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b10"">11]</ref>, in fact converge to the optimal solution under mild assumptions.</p></div> <div xmlns=""http://www.tei-c.org/ ensures the satisfaction of the constraints <ref type=""bibr"" target=""#b20"">[21]</ref>. Primal-dual algorithms <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b10"">11]</ref>, allow us to choose dynamically the multipliers by find the best policy for the current set of parameters and an be applied in the context of reinforcement learning as well, where a policy gradient -or actor critic as in <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b10"">11]</ref> -update is followed by an update of the multipliers along the direction of the constraint violation. In these rate from a theoretical point of view as <ref type=""bibr"" target=""#b0"">(1)</ref>. In particular, the proofs in <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b10"">11]</ref> rely on the fact that this different time-scale is such that allows to consider the multiplier as constant.</ bution.</p><p>Regardless of these limitations, the primal dual algorithm considered here and those proposed in <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b10"">11]</ref> provide a manner to solve constrained policy optimization problems without the need to perform an exhaustive",1
"</ref>, networking <ref type=""bibr"" target=""#b14"">[15]</ref>, robotics <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b16"">17]</ref> and finance <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b18"">19]</ref>. The most commo",0
"ng modular value functions that encode them individually, by multiplying each signal by its own coefficient, which controls the emphasis placed on it <ref type=""bibr"" target=""#b1"">[2]</ref><ref type=""bibr"" target=""#b2"">[3]</ref><ref type=""bibr"" target=""#b3"">[4]</ref>. Although effective, the multi-o e multipliers: constrained Reinforcement Learning problems can be solved through by maximizing an unconstrained Lagrangian, for a specific multiplier <ref type=""bibr"" target=""#b1"">[2]</ref>. The combination of different rewards with manually selected Lagrange multipliers has been applied for instanc",0
"stance, a neural network-which are universal function approximators <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b24"">[25]</ref><ref type=""bibr"" target=""#b25"">[26]</ref><ref type=""bibr"" target=""#b26"">[27]</ref><ref type=""bibr"" target=""#b27"">[28]</ref>-the loss in optimality can",0
"ghly similar spellings to different people entities. Plenty of researches have been conducted to solve the name ambiguity problem. Supervised methods <ref type=""bibr"" target=""#b0"">[1]</ref>,</p><p>The research is supported by the National Key Research and Development Plan (2017YFC1601504), the Natur ete features (e.g., authors, venue). High quality representations play a critical role to quantify distinctions and similarities between publications <ref type=""bibr"" target=""#b0"">[1]</ref>. The majority of existing solutions utilize biographical features such as title, abstract, organization, autho few dominant authors. Hierarchical Agglomerative Clustering (HAC) method works well for skewed data and is widely in many name disambiguation methods <ref type=""bibr"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b4"">[5]</ref>, <ref type=""bibr"" target=""#b8"">[9]</ref>, <ref type=""bibr"" target=""#b This method simply partitions each PHNet into connected components to generate the clustering results for each name to be disambiguated. Zhang et al. <ref type=""bibr"" target=""#b0"">[1]</ref>: This method uses a global metric learning and local linkage learning based on a graph auto-encoder method to methods use different kinds of cluster strategy. For example, <ref type=""bibr"" target=""#b4"">[5]</ref> need to specify the number of distinct author, <ref type=""bibr"" target=""#b0"">[1]</ref> need labeled data to estimate the number. For a fair comparison, we assume the number of clusters is set to re e text information, and Xu et al. just use the word co-occurrence information of text and loss a certain amount of semantic information. Zhang et al. <ref type=""bibr"" target=""#b0"">[1]</ref> also use a graph convolutional network based encoder-decoder model but on homogeneous graph that can not extra et=""#b7"">[8]</ref> propose a Markov random fields based framework to extract multiple types of characteristics and relations in publication database. <ref type=""bibr"" target=""#b0"">[1]</ref> use a global metric learning and local linkage graph auto-encoder algorithm to learn the representation of pub",1
"est similarity) are merged in each step until the number of clusters reaches the specified K.</p><p>When K is unknown, we adopt an optimal modularity <ref type=""bibr"" target=""#b21"">[22]</ref> partitioning mechanism to determine the partition of publications. The modularity M of a partition of nodes",1
"<ref type=""bibr"" target=""#b34"">[35]</ref> introduces a relational graph convolutional network to link prediction task and entity classification task. <ref type=""bibr"" target=""#b35"">[36]</ref> propose a heterogeneous graph neural network model which considers both types and heterogeneous attributes o",1
"PHNet to a high quality representation. Inspired by the network embedding methods DeepWalk <ref type=""bibr"" target=""#b15"">[16]</ref> and Metapath2Vec <ref type=""bibr"" target=""#b16"">[17]</ref> which use a random walk strategy and the skip-gram model to learning node representation in network. First, alk strategy avoids the biased influence caused by central nodes and deviation of different relation quantities via the ordered guidance of meta-path <ref type=""bibr"" target=""#b16"">[17]</ref>. Besides, our designed random walk strategy considers the multiple possibilities of arrangement of a path, t ]</ref>: LINE can preserve both the first-order and secondorder proximities of nodes and is applicable for homogeneous weighted network. Metapath2Vec <ref type=""bibr"" target=""#b16"">[17]</ref>: Metapath2Vec is applicable for heterogeneous network with binary edges. It uses meta-path-based random walk by highly visible relation types and concentrated nodes, and generate incorporated node paths <ref type=""bibr"" target=""#b31"">[32]</ref>. Metapath2Vec <ref type=""bibr"" target=""#b16"">[17]</ref> proposes an embedding method on heterogeneous network based on meta-path. Some other methods have offered di",0
"ublications when we do not exactly know the number of distinct person for an ambiguous name. Some researches <ref type=""bibr"" target=""#b4"">[5]</ref>, <ref type=""bibr"" target=""#b10"">[11]</ref> assume that the cluster number of ambiguous author K is known in advance, but in real world we actually have",0
"random walks ignore the types of relations, are biased by highly visible relation types and concentrated nodes, and generate incorporated node paths <ref type=""bibr"" target=""#b31"">[32]</ref>. Metapath2Vec <ref type=""bibr"" target=""#b16"">[17]</ref> proposes an embedding method on heterogeneous networ",0
"he embedding vector of p i encoded from its initial features u (0) i by HGCN in eq.2. Then, we adopt the popular negative sampling method proposed in <ref type=""bibr"" target=""#b17"">[18]</ref> to sample negative nodes to increase the optimization efficiency. Then, the probability can be approximately",0
"7]</ref> to learn the similarity between publications. Some methods <ref type=""bibr"" target=""#b3"">[4]</ref>, <ref type=""bibr"" target=""#b7"">[8]</ref>- <ref type=""bibr"" target=""#b9"">[10]</ref> use the the word co-occurrence information in publications' content to design relations between publications,",0
"ruct publication networks, then use graph-based <ref type=""bibr"" target=""#b2"">[3]</ref>- <ref type=""bibr"" target=""#b4"">[5]</ref> or heuristic methods <ref type=""bibr"" target=""#b5"">[6]</ref>, <ref type=""bibr"" target=""#b6"">[7]</ref> to learn the similarity between publications. Some methods <ref type= ications that may contain potentially ambiguous names. Many methods <ref type=""bibr"" target=""#b2"">[3]</ref>, <ref type=""bibr"" target=""#b4"">[5]</ref>, <ref type=""bibr"" target=""#b5"">[6]</ref>, <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" target=""#b12"">[13]</ref> are implemented on a sta",0
"liation, citation maybe not available or incomplete in some digital libraries, especially the affiliation information usually has the synonym problem <ref type=""bibr"" target=""#b13"">[14]</ref> and hard to be cleaned, so in this paper, we only use the three common attributes mentioned above. Except th number of clusters is set to real value and choose HAC as the clustering method of <ref type=""bibr"" target=""#b3"">[4]</ref>. We use pairwise F1-score <ref type=""bibr"" target=""#b13"">[14]</ref> to evaluate the clustering results of our method and the compared ones. We also calculate the Macro-F1 score",0
"6]</ref>, <ref type=""bibr"" target=""#b6"">[7]</ref> to learn the similarity between publications. Some methods <ref type=""bibr"" target=""#b3"">[4]</ref>, <ref type=""bibr"" target=""#b7"">[8]</ref>- <ref type=""bibr"" target=""#b9"">[10]</ref> use the the word co-occurrence information in publications' content of ambiguous author K is known in advance, but in real world we actually have no clue about the number. Some <ref type=""bibr"" target=""#b3"">[4]</ref>, <ref type=""bibr"" target=""#b7"">[8]</ref> propose different strategies to estimate the number. However, there are usually pre-defined parameters in thes es and use them to quantify the similarity of publications, then cluster them into disjoint clusters, each of the which belongs to a distinct person. <ref type=""bibr"" target=""#b7"">[8]</ref> propose a Markov random fields based framework to extract multiple types of characteristics and relations in p",0
"target=""#b2"">[3]</ref>, <ref type=""bibr"" target=""#b4"">[5]</ref>, <ref type=""bibr"" target=""#b5"">[6]</ref>, <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" target=""#b12"">[13]</ref> are implemented on a static collection extracted from DLs, and need to run the author disambiguation process ly by co-author relation and devise a novel similarity metric. Then they use affinity propagation clustering algorithm to group result into clusters. <ref type=""bibr"" target=""#b12"">[13]</ref> introduce a pairwise factor graph model which can be extended by incorporation various features. The vast ma",0
"c methods <ref type=""bibr"" target=""#b5"">[6]</ref>, <ref type=""bibr"" target=""#b6"">[7]</ref> to learn the similarity between publications. Some methods <ref type=""bibr"" target=""#b3"">[4]</ref>, <ref type=""bibr"" target=""#b7"">[8]</ref>- <ref type=""bibr"" target=""#b9"">[10]</ref> use the the word co-occurre 11]</ref> assume that the cluster number of ambiguous author K is known in advance, but in real world we actually have no clue about the number. Some <ref type=""bibr"" target=""#b3"">[4]</ref>, <ref type=""bibr"" target=""#b7"">[8]</ref> propose different strategies to estimate the number. However, there a d-to-end model to estimate the number of clusters using a recurrent neural network and use HAC to determine the assignment of publications. Xu et al. <ref type=""bibr"" target=""#b3"">[4]</ref>: For an ambiguous name, this method build several publication networks for different kinds of relations, then data to estimate the number. For a fair comparison, we assume the number of clusters is set to real value and choose HAC as the clustering method of <ref type=""bibr"" target=""#b3"">[4]</ref>. We use pairwise F1-score <ref type=""bibr"" target=""#b13"">[14]</ref> to evaluate the clustering results of our g publication networks. <ref type=""bibr"" target=""#b4"">[5]</ref> utilize a network representation learning based approach on three anonymized network, <ref type=""bibr"" target=""#b3"">[4]</ref> construct five relationship networks among publications and use a network embedding algorithm to learn represe",0
"are conducted on static datasets. To disambiguate the new introduced publications in DLs, some works design incremental name disambiguation methods, <ref type=""bibr"" target=""#b26"">[27]</ref> propose a probabilistic model that use a rich set of metadata and classifies new publications to existing au",0
"iguous names. Many methods <ref type=""bibr"" target=""#b2"">[3]</ref>, <ref type=""bibr"" target=""#b4"">[5]</ref>, <ref type=""bibr"" target=""#b5"">[6]</ref>, <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" target=""#b12"">[13]</ref> are implemented on a static collection extracted from DLs, and ne",0
"s asynchronously, and prevents it from being applied for online speech recognition. There are some works trying to solve this problem. Tjandra et al. <ref type=""bibr"" target=""#b6"">[7]</ref> propose a local monotonic attention mechanism that forces the model to predict a central position at every dec",1
"ition.</p><p>However, it's difficult to accurately predict the next central position just based on limited information. Monotonic chunkwise attention <ref type=""bibr"" target=""#b7"">[8]</ref> is proposed to adaptively split the encoded state sequence into small chunks based on the predicted selection",1
"o small chunks based on the predicted selection probabilities. But complex and tricky training methods make it hard to implement. Triggered attention <ref type=""bibr"" target=""#b8"">[9]</ref> utilizes the spikes produced by connectionist temporal classification (CTC) model to split the sequence into m",1
"produced by the encoder, the decoder begins to predict symbols immediately. Similar to the Neural Transducer <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b12"">13]</ref>, the decoder generates the output sequence chunk by chunk. However, r",0
"<text xml:lang=""en""> 		<body> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""1."">INTRODUCTION</head><p>Attention-based sequence-to-sequence models <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b3"">4,</r ef>, especially transformer model <ref type=""bibr"" target=""#b1"">[2]</ref>, have shown great success in various tasks, e.g. neural machine translation <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref>, image captioning <ref type=""bibr"" target=""#b2"">[3]</ref> and speech rec",0
"eech frames.</p><p>The experiments show that the Sync-Transformer can achieve a comparable result with the best transformer, which is better than LAS <ref type=""bibr"" target=""#b19"">[20]</ref>, RNN-T and our previous (Chunk-Flow) SA-T <ref type=""bibr"" target=""#b9"">[10]</ref>. By contrast, the Sync-Tr",0
"ins to predict symbols immediately. Similar to the Neural Transducer <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b12"">13]</ref>, the decoder generates the output sequence chunk by chunk. However, restricted by the time-dependent property",0
"decoding at the same time. The Sync-Transformer combines the transformer <ref type=""bibr"" target=""#b5"">[6]</ref> and self-attention transducer (SA-T) <ref type=""bibr"" target=""#b9"">[10]</ref> in great depth. Similar to the original transformer, the Sync-Transformer has an encoder and a decoder. In or ble alignment paths and calculate the negative log loss function as the same as the RNN-Transducer <ref type=""bibr"" target=""#b13"">[14]</ref> and SA-T <ref type=""bibr"" target=""#b9"">[10]</ref>. We evaluate our Sync-Transformer on a Mandarin dataset AISHELL-1. The experiments show that the Sync-Transfo arable result with the best transformer, which is better than LAS <ref type=""bibr"" target=""#b19"">[20]</ref>, RNN-T and our previous (Chunk-Flow) SA-T <ref type=""bibr"" target=""#b9"">[10]</ref>. By contrast, the Sync-Transformer can achieve online decoding with only a little degradation of the performa nd decoder are composed of multi-head attentions and feed-forward layers <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b9"">10]</ref>.</p><p>As shown in Fig. <ref type=""figure"" target=""#fig_1"">1</ref>(a), we put a 2D convolution front end at th ity p(y 1:U |x 1,T ) by enumerating all possible alignment paths. Therefore, like transducer-based models in <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b9"">10]</ref>, we introduce a forward-backward algorithm to calculate the probabilities efficiently.</p><p>The forward varia",0
"n=""1."">INTRODUCTION</head><p>Attention-based sequence-to-sequence models <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b5"">6]</re ess in various tasks, e.g. neural machine translation <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref>, image captioning <ref type=""bibr"" target=""#b2"">[3]</ref> and speech recognition <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=",0
"fixed-length chunk of state sequence is produced by the encoder, the decoder begins to predict symbols immediately. Similar to the Neural Transducer <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b12"">13]</ref>, the decoder generates the o",0
"s=""http://www.tei-c.org/ns/1.0""><head n=""1."">INTRODUCTION</head><p>Attention-based sequence-to-sequence models <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b4"">5,</re /ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b5"">6]</ref>, especially transformer model <ref type=""bibr"" target=""#b1"">[2]</ref>, have shown great success in various tasks, e.g. neural machine translation <ref type=""bibr"" target=""#b0"">[1,< ""><head n=""2."">SYNCHRONOUS TRANSFORMER</head></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2.1."">Model</head><p>Similar to the transformer <ref type=""bibr"" target=""#b1"">[2]</ref>, a Sync-Transformer consists of an encoder and a decoder, as depicted in Fig. <ref type=""figure"" target=""#fig_ ncluding dimension transformation(transform feature dimensions from 40 to 256), time-axis down-sampling and adding sine-cosine positional information <ref type=""bibr"" target=""#b1"">[2]</ref>. Let x 1:T be the input feature sequence, the processed sequence can be expressed as s 1:L , where T and L are settings will be explored in the future. What's more, we adopt an Adam optimizer with warmup steps 25000 and the learning rate scheduler reported in <ref type=""bibr"" target=""#b1"">[2]</ref>.</p><p>During decoding, we use a beam search with a width of 5 for all the experiments. And set the maximum le type=""bibr"" target=""#b1"">[2]</ref>, have shown great success in various tasks, e.g. neural machine translation <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref>, image captioning <ref type=""bibr"" target=""#b2"">[3]</ref> and speech recognition <ref type=""bibr"" target=""#b3""> epicted in Fig. <ref type=""figure"" target=""#fig_1"">1</ref>(a). Both encoder and decoder are composed of multi-head attentions and feed-forward layers <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b9"">10]</ref>.</p><p>As shown in Fig. <ref type",0
""">Dataset</head><p>In this work, all experiments are conducted on a public Mandarin speech corpus AISHELL-1<ref type=""foot"" target=""#foot_0"">1</ref>  <ref type=""bibr"" target=""#b16"">[17]</ref>. The training set contains about 150 hours of speech (120,098 utterances) recorded by 340 speakers. The deve",0
"' ) as model units.</p><p>We utilize Kaldi<ref type=""foot"" target=""#foot_1"">2</ref> for data preparation. And our Sync-Transformer is built on ESPNet <ref type=""bibr"" target=""#b17"">[18]</ref> and warp-rnnt <ref type=""foot"" target=""#foot_2"">3</ref> . It consists of 6 encoder blocks and 6 decoder bloc models</head><p>We also compare the Sync-Transformer with other end-toend models. The transformer model is trained according to the recipe in ESPnet <ref type=""bibr"" target=""#b17"">[18]</ref>, which has the same settings as our Sync-Transformer. The second column indicates whether the model can deco",0
"-based sequence-to-sequence models <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b5"">6]</ref>, especially transformer model <ref =""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref>, image captioning <ref type=""bibr"" target=""#b2"">[3]</ref> and speech recognition <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b5"">6]</ref>.</p><p>For conventional attention-",0
"f type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b5"">6]</ref>, especially transformer model <ref type=""bibr"" target=""#b1"">[2]</ref>, ha =""bibr"" target=""#b1"">2]</ref>, image captioning <ref type=""bibr"" target=""#b2"">[3]</ref> and speech recognition <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b5"">6]</ref>.</p><p>For conventional attention-based sequence-to-sequence models, the",0
"partitioning strategy described in this section is application-agnostic and is implemented in the Customizable Streaming Partitioner (CuSP) framework <ref type=""bibr"" target=""#b35"">[36]</ref>. The experimental results in this paper are based on the partitions created by CuSP.</p><p>IV. DISTRIBUTED T s/1.0""><head>A. Experimental Setup</head><p>We implemented the proposed graph partitioning policy using the Customizable Streaming Partitioner (CuSP) <ref type=""bibr"" target=""#b35"">[36]</ref> framework. We modified CuSP to create edge proxies in addition to vertex proxies. We also modified it to tre",1
"-2015, and clueweb12 <ref type=""bibr"" target=""#b43"">[44]</ref>, <ref type=""bibr"" target=""#b44"">[45]</ref>, <ref type=""bibr"" target=""#b45"">[46]</ref>, <ref type=""bibr"" target=""#b46"">[47]</ref> are web crawls. Input graphs are symmetric, have no self-loops, and have no duplicated edges. We represent t",0
"e faster performance on CPUs, but it does give better performance on GPUs due to coalesced memory accesses <ref type=""bibr"" target=""#b14"">[15]</ref>, <ref type=""bibr"" target=""#b15"">[16]</ref>.</p><p>Triangle counting on single GPU: Green et al. <ref type=""bibr"" target=""#b16"">[17]</ref> and Voegele e matching, programmable graph analytics with a setintersection approach, and a matrix formulation based on sparse matrix-matrix multiplies. Hu et al. <ref type=""bibr"" target=""#b15"">[16]</ref> presented a distributed implementation of triangle counting, and they noted that even on a single GPU, a bin computation on GPUs due to improved exploitation of memory bandwidth.</p><p>Our implementation leverages the binary search based intersection method <ref type=""bibr"" target=""#b15"">[16]</ref> to improve performance on a single host which in turn improves overall distributed runtime. We also use the multiple GPUs and streams the edgelist from the CPU memory to the GPU memory on the fly. It uses binary search to increase coalesced memory accesses <ref type=""bibr"" target=""#b15"">[16]</ref> and employs load balancing by dynamically assigning independent units of work (created during preprocessing)",0
"nd it is used in applications such as social network analysis <ref type=""bibr"" target=""#b0"">[1]</ref>, graph statistics (e.g. clustering coefficients <ref type=""bibr"" target=""#b1"">[2]</ref>), and k-truss identification <ref type=""bibr"" target=""#b2"">[3]</ref>. The problem is to count the number of tr",0
"ral other techniques <ref type=""bibr"" target=""#b21"">[22]</ref>, <ref type=""bibr"" target=""#b22"">[23]</ref>, <ref type=""bibr"" target=""#b23"">[24]</ref>, <ref type=""bibr"" target=""#b24"">[25]</ref> were also proposed to improve the performance of triangle counting using MapReduce framework. PDTL <ref type",0
"eld inefficient computation which will be great challenge to process massive remote sensing images. In our work, two lightweight attention mechanisms <ref type=""bibr"" target=""#b16"">[17]</ref> which contains spatial attention and channel attention are adopted for the semantic segmentation of HRRSIs. label>(2)</label></formula><p>where f 7×7 represents a convolution operation with 7 × 7 kernel size.</p><p>In this study, we follow the method of Woo <ref type=""bibr"" target=""#b16"">[17]</ref>to integrate the two attention mechanisms. First, we use channel attention to capture good semantic informati",1
"connection and the network achieved better results compared with the reference networks on the ISPRS Vaihingen and ISPRS Potsdam datasets. Guo et al. <ref type=""bibr"" target=""#b9"">[10]</ref> learned from the spatial pyramid pooling model to capture multiscale features in HRRSIs, and then used condit",0
"t. Kampffmeyer et al. <ref type=""bibr"" target=""#b10"">[11]</ref> used a weighted loss function to solve the problem of category imbalance. Chen et al. <ref type=""bibr"" target=""#b11"">[12]</ref> designed two deep networks based on residual module, and achieved better results compared with FCN-8s <ref t",0
"spatial position relationships of an object to extract features and then use clustering, classification, and threshold algorithms to segment an image <ref type=""bibr"" target=""#b0"">[1]</ref> <ref type=""bibr"" target=""#b1"">[2]</ref>. However, these methods depend heavily on artificial design features a",0
"1.0""><head>C. Backbone for feature extraction</head><p>In this study, we use two representative backbones for feature extraction: SegNet and ResNet50 <ref type=""bibr"" target=""#b17"">[18]</ref>. On this basis, we proposed two networks: SCAttNet V1 with SegNet as backbone and SCAttNet V2 with ResNet50",0
"ctive, inputagnostic attacks can provide new insights into global model behavior.</p><p>Triggers are a new form of universal adversarial perturbation <ref type=""bibr"" target=""#b18"">(Moosavi-Dezfooli et al., 2017)</ref> adapted to discrete textual inputs. To find them, we design a gradient-guided sea ary: trigger sequences can be widely distributed for anyone to fool machine learning models. Moreover, universal attacks often transfer across models <ref type=""bibr"" target=""#b18"">(Moosavi-Dezfooli et al., 2017)</ref>, which further decreases attack requirements: the adversary does not need white-b cause a target prediction.</p><p>Why Universal? The adversarial threat is higher if an attack is universal: using the exact same attack for any input <ref type=""bibr"" target=""#b18"">(Moosavi-Dezfooli et al., 2017;</ref><ref type=""bibr"" target=""#b4"">Brown et al., 2017)</ref>. Universal attacks are adv",1
"e its prediction from the underlined span to a desired target span inside the trigger. For language modeling, triggers are prefixes that prompt GPT-2 <ref type=""bibr"" target=""#b24"">(Radford et al., 2019)</ref> to generate racist outputs, even when conditioned on non-racist user inputs. <ref type=""bi specific target no matter the input. We thus relax the attack to targets of similar content. particular, our trigger causes the GPT-2 language model <ref type=""bibr"" target=""#b24"">(Radford et al., 2019)</ref> to output racist content. We maximize the likelihood of racist outputs when conditioned on",1
"s to entailment. We suspect that this arises from a bias towards entailment when there is high lexical overlap between the premise and the hypothesis <ref type=""bibr"" target=""#b15"">(McCoy et al., 2019)</ref>. Since triggers are premise-and hypothesisagnostic, they cannot increase overlap for a parti",0
"""bibr"" target=""#b13"">(Jia and Liang, 2017;</ref><ref type=""bibr"" target=""#b26"">Ribeiro et al., 2018)</ref> and stress test neural machine translation <ref type=""bibr"" target=""#b1"">(Belinkov and Bisk, 2018)</ref>. Adversarial attacks also facilitate interpretation, e.g., by analyzing a model's sensit",0
"://www.tei-c.org/ns/1.0""><head n=""1"">Introduction</head><p>Adversarial attacks modify inputs in order to cause machine learning models to make errors <ref type=""bibr"" target=""#b31"">(Szegedy et al., 2014)</ref>. From an attack perspective, they expose system vulnerabilities, e.g., a spammer may use a",0
"g the optimal e i can be efficiently computed in brute-force with |V| d-dimensional dot products where d is the dimensionality of the token embedding <ref type=""bibr"" target=""#b16"">(Michel et al., 2019)</ref>. This brute-force solution is trivially parallelizable and less expensive than running a fo",0
"0""><head n=""6.1"">Triggers Align With SNLI Artifacts</head><p>The construction of NLP datasets can lead to dataset biases or ""artifacts"". For example, <ref type=""bibr"" target=""#b9"">Gururangan et al. (2018)</ref> and <ref type=""bibr"" target=""#b23"">Poliak et al. (2018)</ref> show that spurious correlat correlations exist between the hypothesis words and the labels in SNLI. We investigate whether triggers are caused by such artifacts.</p><p>Following <ref type=""bibr"" target=""#b9"">Gururangan et al. (2018)</ref>, we identify dataset artifacts by ranking all the hypothesis words according to their poi",0
"2019)</ref> and <ref type=""bibr"" target=""#b6"">Cheng et al. (2018)</ref> do the same for text generation. Other attack methods are based on generative <ref type=""bibr"" target=""#b12"">(Iyyer et al., 2018)</ref> or human-in-the-loop approaches <ref type=""bibr"" target=""#b32"">(Wallace et al., 2019)</ref>.",0
"also facilitate interpretation, e.g., by analyzing a model's sensitivity to local perturbations <ref type=""bibr"" target=""#b14"">(Li et al., 2016;</ref><ref type=""bibr"" target=""#b7"">Feng et al., 2018)</ref>.</p><p>These attacks are typically generated for a specific input; are there attacks that work",0
"p models for ranking or recommendation with a pairwise approach.</p><p>• Combining the proposed model with densityweighted Expected Loss Optimization <ref type=""bibr"" target=""#b16"">[17]</ref>, we introduce active learning into POLAR <ref type=""bibr"" target=""#b15"">[16]</ref>, an attentionbased CNN co et=""#b39"">[40]</ref>. Unlabeled instances whose prediction the parameters under the posterior disagree about are selected. Expected Loss Optimization <ref type=""bibr"" target=""#b16"">[17]</ref> selects the instance that maximizes the expected loss based on Bayesian decision theory. In <ref type=""bibr"" =""http://www.tei-c.org/ns/1.0""><head n=""4.2.2"">Expected Loss Optimization</head><p>The active learning metric we choose is Expected Loss Optimization <ref type=""bibr"" target=""#b16"">[17]</ref>. The basic idea is to choose the instance that maximizes the expected loss of the current best action. For t",1
"y. In <ref type=""bibr"" target=""#b40"">[41]</ref> a Bayesian active learning algorithm for deep learning in image data is proposed based on the idea in <ref type=""bibr"" target=""#b41"">[42]</ref>.</p><p>Most works that apply active learning to recommender systems are based on collaborative filtering <re ))dω − KL(q(ω)||p(ω))</formula><p>(2) which is the basic equation of Variational Inference <ref type=""bibr"" target=""#b51"">[52]</ref>.</p><p>Following <ref type=""bibr"" target=""#b41"">[42]</ref>, we use the distribution of the network parameter with dropout <ref type=""bibr"" target=""#b52"">[53]</ref> as ro as f + − f − increases. Therefore, it can be considered as the smoothed version of hinge loss.</p><p>As for the second term in (2), it's proved in <ref type=""bibr"" target=""#b41"">[42]</ref> that it can be approximated by L2 regularization term</p><formula xml:id=""formula_30"">Wi∈ω λ i ||W i || 2 ,<",1
"while the terms that occur more than once are stopwords like of and on. Therefore, a better mechanism for local weights is needed.</p><p>Inspired by <ref type=""bibr"" target=""#b59"">[60]</ref>, we propose a local weight network based on distributed word representations. The basic idea is that, becaus",1
"e Coverage strategy are two representative heuristic methods, but they are not personalized. More advanced methods are based on uncertainty reduction <ref type=""bibr"" target=""#b47"">[48]</ref> or error reduction <ref type=""bibr"" target=""#b48"">[49]</ref>, <ref type=""bibr"" target=""#b49"">[50]</ref>. For",0
"the proposed model with densityweighted Expected Loss Optimization <ref type=""bibr"" target=""#b16"">[17]</ref>, we introduce active learning into POLAR <ref type=""bibr"" target=""#b15"">[16]</ref>, an attentionbased CNN combined with one-shot learning for personalized article recommendation to utilize ex rical results show that our framework can perform stably and significantly better than comparative methods.</p><p>2. A prior version was published in <ref type=""bibr"" target=""#b15"">[16]</ref> Organization The rest of the paper is organized as follows: Section 2 reviews related work. Section 3 and 4 define the one-shot personalized article recommendation problem as follows.</p><p>Definition 1 (One-shot Personalized Article Recommendation Problem <ref type=""bibr"" target=""#b15"">[16]</ref>). The input of the problem is a query article d q , the set of candidate articles D = {d i } N i=1 , and a s",0
"matically in the past years. The recently released Open Academic Graph (OAG) 1 consists of 208,915,369 papers, 52,678 venues, and 253,144,301 authors <ref type=""bibr"" target=""#b1"">[2]</ref>. Many digital library providers article recommendations to help users find recent or related articles. These r",0
"here w mi and w nj are the word embeddings of term t mi and t nj . Since the cosine similarity of word embeddings can capture the semantic similarity <ref type=""bibr"" target=""#b53"">[54]</ref>,</p><formula xml:id=""formula_7"">M (m,n) i,j</formula><p>represents the similarity between t mi and t nj .</p embeddings in all the models above are 256 dimensions trained on Wikipedia via the skip-gram model, using hierarchical softmax and negative sampling <ref type=""bibr"" target=""#b53"">[54]</ref>.</p><p>In the Local Weight Network there are two hidden layers, with 64 and 32 hidden units respectively. Th",0
"</ref>.</p><p>Our work is also related to information retrieval <ref type=""bibr"" target=""#b10"">[11]</ref>, <ref type=""bibr"" target=""#b24"">[25]</ref>, <ref type=""bibr"" target=""#b25"">[26]</ref> and semantic matching <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" target=""#b12"">[13]</ref>. 11"">[12]</ref>, <ref type=""bibr"" target=""#b12"">[13]</ref>. Traditional methods for measuring the similarity between two pieces of texts, such as BM25 <ref type=""bibr"" target=""#b25"">[26]</ref> and TF-IDF <ref type=""bibr"" target=""#b24"">[25]</ref>, are based on the bag-ofwords model and do not perform tion of the frequency that the term occurs in a document, such as the term frequency (TF) in TF-IDF <ref type=""bibr"" target=""#b24"">[25]</ref> or BM25 <ref type=""bibr"" target=""#b25"">[26]</ref> ranking function:</p><formula xml:id=""formula_11"">BM25(d, q) = n i=1 IDF(q i ) f (q i , d)(k 1 + 1) f (q i ,",0
"=""#b9"">[10]</ref>. Some works also use graph-based methods to explore the inherent connections in academia <ref type=""bibr"" target=""#b22"">[23]</ref>, <ref type=""bibr"" target=""#b23"">[24]</ref>.</p><p>Our work is also related to information retrieval <ref type=""bibr"" target=""#b10"">[11]</ref>, <ref typ",0
"cold-start problem <ref type=""bibr"" target=""#b9"">[10]</ref>. Some works also use graph-based methods to explore the inherent connections in academia <ref type=""bibr"" target=""#b22"">[23]</ref>, <ref type=""bibr"" target=""#b23"">[24]</ref>.</p><p>Our work is also related to information retrieval <ref typ c(d i , d j )<label>(23)</label></formula><p>where c(d i , d j ) is the similarity between d i and d j predicted by our CNN model. The latter part in <ref type=""bibr"" target=""#b22"">(23)</ref> is the average similarity between d i and articles in U. <ref type=""bibr"" target=""#b22"">(23)</ref> aims to f d j predicted by our CNN model. The latter part in <ref type=""bibr"" target=""#b22"">(23)</ref> is the average similarity between d i and articles in U. <ref type=""bibr"" target=""#b22"">(23)</ref> aims to find the instance that is representative of most unlabeled articles while maximizing the expected lo",0
"ibraries and has attracted a lot of research interest. Giles et al. introduced the first research-article recommender as part of the CiteSeer project <ref type=""bibr"" target=""#b17"">[18]</ref>. Content-based filtering <ref type=""bibr"" target=""#b3"">[4]</ref> is one of the most widely used and research s <ref type=""bibr"" target=""#b19"">[20]</ref>, topics <ref type=""bibr"" target=""#b8"">[9]</ref>, <ref type=""bibr"" target=""#b20"">[21]</ref>, and citations <ref type=""bibr"" target=""#b17"">[18]</ref>. Collaborative filtering <ref type=""bibr"" target=""#b4"">[5]</ref> makes recommendation predictions by utilizi",0
"each article via Paragraph Vector model. The similarity score between two articles is produced by the cosine similarity of their representations.• WMD<ref type=""bibr"" target=""#b65"">[66]</ref>:The Word Mover's Distance (WMD) is the minimum distance required to transport words from one document to ano",0
"t=""#b10"">[11]</ref>, <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" target=""#b12"">[13]</ref>, <ref type=""bibr"" target=""#b13"">[14]</ref>, <ref type=""bibr"" target=""#b14"">[15]</ref>, but they often treat all the words in an article indiscriminately. Therefore, they cannot distinguish essen orks to learn the patterns in the word-level interaction of two articles, usually based on word embeddings <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" target=""#b14"">[15]</ref>, <ref type=""bibr"" target=""#b27"">[28]</ref>. These models lack the explicit expressions of word weights but r",0
"]</ref> or Collaborative Filtering <ref type=""bibr"" target=""#b4"">[5]</ref> in this scenario. Inspired by the recent success of one-shot deep learning <ref type=""bibr"" target=""#b5"">[6]</ref>, <ref type=""bibr"" target=""#b6"">[7]</ref>, <ref type=""bibr"" target=""#b7"">[8]</ref>, we propose to learn a one-s s based on deep learning fall into two categories. Metric-based approaches try to learn a similarity metric to help predict the label of instance. In <ref type=""bibr"" target=""#b5"">[6]</ref>, a Siamese network is learned with several convolutional layers used before the fully-connected layers and the",0
"0""><head n=""4.2.1"">Pairwise Loss for Bayesian Learning</head><p>We formulate the training set in the setting of Pairwise Approach of Learning to Rank <ref type=""bibr"" target=""#b60"">[61]</ref>. It means that D train = {(d q and support set S (i) . We transform it into the constraint on the recommenda",0
"se data contain valuable information for predicting future lightning occurrences. However, although extrapolationbased methods for weather nowcasting <ref type=""bibr"" target=""#b0"">[1]</ref>- <ref type=""bibr"" target=""#b2"">[3]</ref> can be migrated to lightning forecast tasks, they encounter rapid dec ype=""bibr"" target=""#b30"">[31]</ref> filters that are sensitive to different dimensions are assembled to predict mobile events in the city. Shi et al. <ref type=""bibr"" target=""#b0"">[1]</ref> proposed convolutional LSTM (ConvLSTM) for precipitation nowcasting. As a landmark structure, ConvLSTM is used eration </p><formula xml:id=""formula_5"">H t−1 , C t−1 .</formula><p>The ConvLSTM in this paper does not include peephole connections, as mentioned in <ref type=""bibr"" target=""#b0"">[1]</ref>. The data first enters the CNN modules, where sequentially arranged 2D convolutional layers expand the recepti",1
"attention uses a sliding time step window to find the best matching pattern of the current predicted trend from the historical sequence. Shao et al. <ref type=""bibr"" target=""#b9"">[10]</ref> added the target-side attention to a Seq2Seq conversation response generation model, which makes up for the d olution and denotes the depthwise convolution. W att ∈ R h ×w ×(c 4 +c 3 ) and U att ∈ R h ×w ×m are learnable weights. The bias terms are omitted in <ref type=""bibr"" target=""#b9"">(10)</ref>. • is Hadamard product and refers specifically here to the summation of all elements of the vector. According",0
"r structure facilitates sequence-to-sequence (Seq2Seq) conversion between data series with different lengths <ref type=""bibr"" target=""#b8"">[9]</ref>- <ref type=""bibr"" target=""#b10"">[11]</ref>. Since both recent lightning observations and numerical simulations can be organized in the form of spatiote b36"">[37]</ref>, we scale the weights before normalizing them to avoid getting values into the saturation region of the softmax function, as shown in <ref type=""bibr"" target=""#b10"">(11)</ref>. </p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>Applying the attention weights, we get the new in",0
"br"" target=""#b5"">[6]</ref> proposed the famous PR92 scheme which discovers the relation between maximum vertical velocity and lightning frequency. In <ref type=""bibr"" target=""#b25"">[26]</ref>, a formula is proposed for converting the radar reflectivity at two height levels to the flash rates. Consid",0
"and R represent observed mixture images with reflections, background, and reflection images, respectively. Here, α and β are the mixing coefficients <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" target=""#b25"">26]</ref>.</p><formula xml:id=""formula_0 reflection, which may contribute to more realistic generation results and clearer separation results. Moreover, we introduce the gradient constraints <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b25"">26]</ref> to make the model learning more effective, in which the edge map estim /p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.1."">Framework of the Proposed Scheme</head><p>In contrast to the conventional pipelines <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b28"">29,</ref><ref type=""bibr"" target=""#b23"">24]</ref> that treat the image generatio more realistic reflection appearances (first to third columns in Figure <ref type=""figure"" target=""#fig_3"">4</ref> 1 ) than previous linear functions <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b0"">1 ask learning to estimate the background, reflection, and the background edge map (E) concurrently. Instead of one-toone framework in previous methods <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b28"">29]</ref>, our separator learns the mapping function as S : M → (B, R, E), where It's not trivial to learn this function accurately. Recently, deep learning based reflection removal methods <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b4"">5]</ref> with better generalization ability have been proposed to address the limitations arising from the handcrafted i o phases in training models.</p><p>In contrast with previous methods <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b4"">5]</ref>, that heavily rely on the simplified model in Equation 1 and regard the image generation and separation as two f> proposed a two-stage deep learning approach to learn the edge features of the reflections with the light field camera. The framework introduced in <ref type=""bibr"" target=""#b4"">[5]</ref> exploited the edge information when training the whole network to better preserve the image details. Though th ork to separate the mixture image into background and reflection, and three discriminator networks to produce the adversarial losses. Existing method <ref type=""bibr"" target=""#b4"">[5]</ref> can be treated as a special instance of our method when the generator is simplified as a linear function.</p>< type=""bibr"" target=""#b23"">[24]</ref>, Zhang18 <ref type=""bibr"" target=""#b28"">[29]</ref>, CycleGAN <ref type=""bibr"" target=""#b29"">[30]</ref>, and FY17 <ref type=""bibr"" target=""#b4"">[5]</ref>. The yellow boxes highlight some noticeable differences.</p><p>Table <ref type=""table"">1</ref>. Quantitative e .801 0.829 21.77 WS16 <ref type=""bibr"" target=""#b25"">[26]</ref> 0.833 0.877 22.39 NR17 <ref type=""bibr"" target=""#b0"">[1]</ref> 0.832 0.882 23.70 FY17 <ref type=""bibr"" target=""#b4"">[5]</ref> 0.820 0.871 22.51 CycleGAN <ref type=""bibr"" target=""#b29"">[30]</ref> 0.794 0.813 20.10 Zhang18 <ref type=""bibr ref type=""bibr"" target=""#b23"">[24]</ref>, Zhang18 <ref type=""bibr"" target=""#b28"">[29]</ref>, CycleGAN <ref type=""bibr"" target=""#b29"">[30]</ref>, FY17 <ref type=""bibr"" target=""#b4"">[5]</ref>, NR17 <ref type=""bibr"" target=""#b0"">[1]</ref>, WS16 <ref type=""bibr"" target=""#b25"">[26]</ref>, and LB14 <ref t figure xmlns=""http://www.tei-c.org/ns/1.0"" type=""table"" xml:id=""tab_3""><head>Table 3 .</head><label>3</label><figDesc>Efficiency comparisons with FY17<ref type=""bibr"" target=""#b4"">[5]</ref>, Zhang18<ref type=""bibr"" target=""#b28"">[29]</ref> and Wan18<ref type=""bibr"" target=""#b23"">[24]</ref> of an ima",1
"ection images, respectively. Here, α and β are the mixing coefficients <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" target=""#b25"">26]</ref>.</p><formula xml:id=""formula_0"">M = αB + βR,<label>(1)</label></formula><p>Reflection removal aims at removin e realistic generation results and clearer separation results. Moreover, we introduce the gradient constraints <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b25"">26]</ref> to make the model learning more effective, in which the edge map estimation is elegantly dealt with as an aux visibility of the background scenes B is enhanced. In this scenario, image priors such as different blur levels between the background and reflection <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b16"">17]</ref>, ghosting effects <ref type=""bibr"" target=""#b21"">[22]</ref>, and the first to third columns in Figure <ref type=""figure"" target=""#fig_3"">4</ref> 1 ) than previous linear functions <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b0"">1]</ref> with fixed coefficients.</p><p> s methods using three different error metrics.</p><p>SSIM r SSIM PSNR(dB)</p><p>LB14 <ref type=""bibr"" target=""#b16"">[17]</ref> 0.801 0.829 21.77 WS16 <ref type=""bibr"" target=""#b25"">[26]</ref> 0.833 0.877 22.39 NR17 <ref type=""bibr"" target=""#b0"">[1]</ref> 0.832 0.882 23.70 FY17 <ref type=""bibr"" targe CycleGAN <ref type=""bibr"" target=""#b29"">[30]</ref>, FY17 <ref type=""bibr"" target=""#b4"">[5]</ref>, NR17 <ref type=""bibr"" target=""#b0"">[1]</ref>, WS16 <ref type=""bibr"" target=""#b25"">[26]</ref>, and LB14 <ref type=""bibr"" target=""#b16"">[17]</ref>. For a fair comparison, we use the codes provided by the",1
"org/ns/1.0""><head n=""4.1."">Framework of the Proposed Scheme</head><p>In contrast to the conventional pipelines <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b28"">29,</ref><ref type=""bibr"" target=""#b23"">24]</ref> that treat the image generation and separation as two independent sta , reflection, and the background edge map (E) concurrently. Instead of one-toone framework in previous methods <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b28"">29]</ref>, our separator learns the mapping function as S : M → (B, R, E), where the multi-task learning framework mode in the SIR<ref type=""foot"" target=""#foot_0"">2</ref> dataset. The comparison methods include Wan18 <ref type=""bibr"" target=""#b23"">[24]</ref>, Zhang18 <ref type=""bibr"" target=""#b28"">[29]</ref>, CycleGAN <ref type=""bibr"" target=""#b29"">[30]</ref>, and FY17 <ref type=""bibr"" target=""#b4"">[5]</ref>. The y 882 23.70 FY17 <ref type=""bibr"" target=""#b4"">[5]</ref> 0.820 0.871 22.51 CycleGAN <ref type=""bibr"" target=""#b29"">[30]</ref> 0.794 0.813 20.10 Zhang18 <ref type=""bibr"" target=""#b28"">[29]</ref> 0.842 0.885 24.01 Wan18 <ref type=""bibr"" target=""#b23"">[24]</ref> 0.854 0.891 24.08 Eq. ( <ref type=""formula o 4 and 0.9, respectively. Figure <ref type=""figure"">7</ref>. Perceptual study results on the whole SIR 2 dataset for the three best methods (Zhang18 <ref type=""bibr"" target=""#b28"">[29]</ref>, Wan18 <ref type=""bibr"" target=""#b23"">[24]</ref>, and ours) in terms of the quantitative scores in Table <re od is compared with seven state-ofthe-art single image reflection removal methods, including Wan18 <ref type=""bibr"" target=""#b23"">[24]</ref>, Zhang18 <ref type=""bibr"" target=""#b28"">[29]</ref>, CycleGAN <ref type=""bibr"" target=""#b29"">[30]</ref>, FY17 <ref type=""bibr"" target=""#b4"">[5]</ref>, NR17 <ref across datasets with different collecting protocols (e.g., the dataset of SIR 2 <ref type=""bibr"" target=""#b22"">[23]</ref> and the dataset of Zhang18 <ref type=""bibr"" target=""#b28"">[29]</ref>), we have observed that the dataset gap problem is worth further investigating to achieve consistently good ""table"" xml:id=""tab_3""><head>Table 3 .</head><label>3</label><figDesc>Efficiency comparisons with FY17<ref type=""bibr"" target=""#b4"">[5]</ref>, Zhang18<ref type=""bibr"" target=""#b28"">[29]</ref> and Wan18<ref type=""bibr"" target=""#b23"">[24]</ref> of an image with size 224 × 288 on a single Titan XP GPU.",0
"ed by the interactions of various factors and much beyond the straightforward linear combination. For example, either non-uniform lighting conditions <ref type=""bibr"" target=""#b11"">[12]</ref> or the non-flat surface of glass <ref type=""bibr"" target=""#b26"">[27]</ref> may make Equation 1 invalid. As s",0
"layer <ref type=""bibr"" target=""#b16"">[17]</ref>, the ghosting effects <ref type=""bibr"" target=""#b21"">[22]</ref> and the Laplacian data fidelity term <ref type=""bibr"" target=""#b0"">[1]</ref>. Other methods in this area remove reflections by virtue of multiple images. By exploiting different image cor PSNR(dB)</p><p>LB14 <ref type=""bibr"" target=""#b16"">[17]</ref> 0.801 0.829 21.77 WS16 <ref type=""bibr"" target=""#b25"">[26]</ref> 0.833 0.877 22.39 NR17 <ref type=""bibr"" target=""#b0"">[1]</ref> 0.832 0.882 23.70 FY17 <ref type=""bibr"" target=""#b4"">[5]</ref> 0.820 0.871 22.51 CycleGAN <ref type=""bibr"" tar g18 <ref type=""bibr"" target=""#b28"">[29]</ref>, CycleGAN <ref type=""bibr"" target=""#b29"">[30]</ref>, FY17 <ref type=""bibr"" target=""#b4"">[5]</ref>, NR17 <ref type=""bibr"" target=""#b0"">[1]</ref>, WS16 <ref type=""bibr"" target=""#b25"">[26]</ref>, and LB14 <ref type=""bibr"" target=""#b16"">[17]</ref>. For a fai than previous linear functions <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b0"">1]</ref> with fixed coefficients.</p><p>Separator (S). We perform a disentanglement in the separator for the mixture ima",0
"s in this area remove reflections by virtue of multiple images. By exploiting different image correlation cues <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b5"">6]</ref>, the modelling based methods using the multiple images show more reliable results. However, the requirements fo",0
"enhanced. In this scenario, image priors such as different blur levels between the background and reflection <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b16"">17]</ref>, ghosting effects <ref type=""bibr"" target=""#b21"">[22]</ref>, and the non-local similarity in the images <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b13"">14]</ref>, the blur level differences between the background and reflection layer <ref type=""bibr"" target=""#b16"">[17]</ref>, the ghosting effects <ref type=""bibr"" target=""#b21"">[22]</ref> and the Laplacian data fidelity term <ref ty Quantitative evaluation results on SIR 2 with the state-ofthe-arts methods using three different error metrics.</p><p>SSIM r SSIM PSNR(dB)</p><p>LB14 <ref type=""bibr"" target=""#b16"">[17]</ref> 0.801 0.829 21.77 WS16 <ref type=""bibr"" target=""#b25"">[26]</ref> 0.833 0.877 22.39 NR17 <ref type=""bibr"" tar FY17 <ref type=""bibr"" target=""#b4"">[5]</ref>, NR17 <ref type=""bibr"" target=""#b0"">[1]</ref>, WS16 <ref type=""bibr"" target=""#b25"">[26]</ref>, and LB14 <ref type=""bibr"" target=""#b16"">[17]</ref>. For a fair comparison, we use the codes provided by their authors and set the parameters as suggested in th pe=""figure"" target=""#fig_3"">4</ref> 1 ) than previous linear functions <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b0"">1]</ref> with fixed coefficients.</p><p>Separator (S). We perform a disentanglem",0
"separator to improve the reflection removal ability. For the discriminator networks, we use 70 × 70 PatchGANs <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b15"">16]</ref> which can be applied to arbitrarily-sized images in a fully convolutional fashion.</p></div> <div xmlns=""http",0
"ve modeling ability of deep learning has benefited the reflection removal problems and shown very promising results. For example, Chandramouli et al. <ref type=""bibr"" target=""#b3"">[4]</ref> proposed a two-stage deep learning approach to learn the edge features of the reflections with the light field",0
"operly handling the mutual effects of two phases in training models.</p><p>In contrast with previous methods <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b4"">5]</ref>, that heavily rely on the simplified model in Equation 1 and regard the",0
"is fundamentally a matching task, most recent neural architectures, such as DRMM <ref type=""bibr"" target=""#b8"">(Guo et al., 2016)</ref> and Co-PACRR <ref type=""bibr"" target=""#b13"">(Hui et al., 2018)</ref>, adopt an interaction-based design. They operate directly on the similarity matrix obtained fr",1
"similarity learning. Various neural network architectures, e.g., Siamese networks <ref type=""bibr"" target=""#b9"">(He et al., 2016)</ref> and attention <ref type=""bibr"" target=""#b26"">(Seo et al., 2017;</ref><ref type=""bibr"" target=""#b31"">Tay et al., 2019b)</ref>, have been proposed to model semantic s emantic understanding and contextual reasoning rather than specific term matches. Context-aware representation learning, such as co-attention methods <ref type=""bibr"" target=""#b26"">(Seo et al., 2017)</ref>, has been proved effective in many benchmarks. Though improvements have been shown from adding erts the input vector to a R n×m matrix by repeating elements in the missing dimen-sions. Softmax col is the column-wise softmax operator. Similar to <ref type=""bibr"" target=""#b26"">Seo et al. (2017)</ref>, we perform co-attention from two directions: query-to-context and context-to-query, as follows sentence order for better sentence representations; (2) interaction and attention mecha-nisms <ref type=""bibr"" target=""#b31"">(Tay et al., 2019b;</ref><ref type=""bibr"" target=""#b26"">Seo et al., 2017;</ref><ref type=""bibr"" target=""#b21"">Parikh et al., 2016;</ref><ref type=""bibr"" target=""#b4"">Conneau e",1
"the window size for phrase modeling, which has been shown to be critical for relevance matching <ref type=""bibr"" target=""#b5"">(Dai et al., 2018;</ref><ref type=""bibr"" target=""#b25"">Rao et al., 2019)</ref>. On the other hand, the contextual encoder enables us to obtain long-distance contextual repres g 2013-2014 datasets <ref type=""bibr"" target=""#b17"">(Lin and Efron, 2013;</ref><ref type=""bibr"" target=""#b18"">Lin et al., 2014)</ref>, as prepared by <ref type=""bibr"" target=""#b25"">Rao et al. (2019)</ref>, where each dataset contains around 50 queries and 40k query-tweet pairs. We report MAP and pre bibr"" target=""#b6"">(Devlin et al., 2019)</ref> results on each dataset.</p><p>For the tweet search task, we mostly follow the experimental setting in <ref type=""bibr"" target=""#b25"">Rao et al. (2019)</ref>. Baselines include the classic query likelihood (QL) method, RM3 query expansion <ref type=""bib we used implementations in MatchZoo.<ref type=""foot"" target=""#foot_1"">2</ref> For L2R, we used LambdaMART (Burges, 2010) on the same feature sets as <ref type=""bibr"" target=""#b25"">Rao et al. (2019)</ref> In our experiments, we use trainable 300d word2vec <ref type=""bibr"" target=""#b19"">(Mikolov et a differentiable kernelbased pooling technique to capture matching signals at different strength levels. Sharing similarities with our architecture is <ref type=""bibr"" target=""#b25"">Rao et al. (2019)</ref>, who developed a multi-perspective relevance matching method with a hierarchical convolutional",0
"signals. The contextual encoder performs worse than the other two on TrecQA, but is comparable on all other datasets. This finding is consistent with <ref type=""bibr"" target=""#b23"">Rao et al. (2017a)</ref>, which shows that keyword matching signals are important for TrecQA. Also, we notice that the",0
"p://www.tei-c.org/ns/1.0""><head n=""1"">Introduction</head><p>Neural networks have achieved great success in many NLP tasks, such as question answering <ref type=""bibr"" target=""#b22"">(Rao et al., 2016;</ref><ref type=""bibr"" target=""#b2"">Chen et al., 2017a)</ref>, paraphrase detection <ref type=""bibr""",0
"ccuracy. Tweet Search. This task is to rank candidate tweets by relevance with respect to a short query. We use the TREC Microblog 2013-2014 datasets <ref type=""bibr"" target=""#b17"">(Lin and Efron, 2013;</ref><ref type=""bibr"" target=""#b18"">Lin et al., 2014)</ref>, as prepared by <ref type=""bibr"" targ",0
"reciprocal rank (MRR). Paraphrase Identification. This task is to identify whether two sentences are paraphrases of each other. We use the TwitterURL <ref type=""bibr"" target=""#b15"">(Lan et al., 2017)</ref> dataset with 50k sentence pairs. We report the unweighted average of F1 scores on the positive",0
"dentification, and STS tasks, we compared against the following baselines: InferSent <ref type=""bibr"" target=""#b4"">(Conneau et al., 2017)</ref>, ESIM <ref type=""bibr"" target=""#b3"">(Chen et al., 2017b)</ref>, DecAtt <ref type=""bibr"" target=""#b21"">(Parikh et al., 2016)</ref>, and PWIM <ref type=""bibr"" u et al., 2017;</ref><ref type=""bibr"" target=""#b7"">Gong et al., 2018)</ref> to emphasize salient word pair interactions;</p><p>(3) structure modeling <ref type=""bibr"" target=""#b3"">(Chen et al., 2017b)</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""6"">Conclusion</head><p>In this w",0
"r instance, Kipf et al. trained a semi-supervised GCN model for node classification and showed how performance degrades when using more than 3 layers <ref type=""bibr"" target=""#b17"">[18]</ref>. Pham et al. <ref type=""bibr"" target=""#b25"">[26]</ref> proposed Column Network (CLN) for collective classifi mpute new vertex representations. There are different variants of those two functions. For example, the aggregation function can be a mean aggregator <ref type=""bibr"" target=""#b17"">[18]</ref>, a max-pooling aggregator <ref type=""bibr"" target=""#b26"">[27,</ref><ref type=""bibr"" target=""#b9"">10,</ref><r",1
"ry deep CNN models. In contrast, it is not yet clear how to properly train deep GCN architectures, where several works have studied their limitations <ref type=""bibr"" target=""#b18"">[19,</ref><ref type=""bibr"" target=""#b42"">43,</ref><ref type=""bibr"" target=""#b52"">53]</ref>. Stacking more layers into a ectures <ref type=""bibr"" target=""#b42"">[43,</ref><ref type=""bibr"" target=""#b52"">53]</ref> is an open problem in the graph learning space. Recent work <ref type=""bibr"" target=""#b18"">[19,</ref><ref type=""bibr"" target=""#b42"">43,</ref><ref type=""bibr"" target=""#b52"">53]</ref> suggests that GCNs do not sc eans that back-propagating through these networks causes oversmoothing, eventually leading to features of graph vertices converging to the same value <ref type=""bibr"" target=""#b18"">[19]</ref>. Due to these limitations, most state-of-the-art GCNs are no deeper than 4 layers <ref type=""bibr"" target=""# aph structure. As with other works, their network is limited to a small number of layers <ref type=""bibr"" target=""#b5"">(6)</ref>. Recently, Li et al. <ref type=""bibr"" target=""#b18"">[19]</ref> studied the depth limitations of GCNs and showed that deep GCNs can cause over-smoothing, which results in f",1
"tive classification in relational learning and showed peak performance with 10 layers with the performance degrading for deeper graphs. Rahimi et al. <ref type=""bibr"" target=""#b30"">[31]</ref> developed a Highway GCN for user geo-location in social media graphs, where they add ""highway"" gates between",1
"=""bibr"" target=""#b26"">[27,</ref><ref type=""bibr"" target=""#b28"">29,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b48"">49]</ref>. The recent EdgeConv method by Wang et al. <ref type=""bibr"" target=""#b41"">[42]</ref> applies GCNs to point cl",0
"=""bibr"" target=""#b16"">[17]</ref>. Graphs are also used to model human joints for action recognition in video <ref type=""bibr"" target=""#b46"">[47,</ref><ref type=""bibr"" target=""#b15"">16]</ref>. GCNs are a perfect candidate for 3D point cloud processing, especially since the unstructured nature of poin study the effect of different parameters, e.g. number of k-NN neighbors <ref type=""bibr"" target=""#b3"">(4,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b31"">32)</ref>, number of filters <ref type=""bibr"" target=""#b31"">(32,</ref><ref type",0
"arize our contributions as three fold. <ref type=""bibr"" target=""#b0"">(1)</ref> We adapt residual/dense connections, and dilated convolutions to GCNs. <ref type=""bibr"" target=""#b1"">(2)</ref> We present extensive experiments on point cloud data, showing the effect of each of these new layers to the st elling of user interactions leads to improved product recommendations. Graphs are also popular modes of representation in natural language processing <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b22"">23]</ref>, where they are used to represent complex relations between large text",0
"STM aggregator <ref type=""bibr"" target=""#b24"">[25]</ref>. The update function can be a multi-layer perceptron <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b6"">7]</ref>, a gated network <ref type=""bibr"" target=""#b20"">[21]</ref>, etc. More concretely, the representation of vertice 31"">32)</ref>, number of filters <ref type=""bibr"" target=""#b31"">(32,</ref><ref type=""bibr"">64,</ref><ref type=""bibr"">128)</ref>, and number of layers <ref type=""bibr"" target=""#b6"">(7,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"">56)</ref>. Ove",0
"arget=""#b31"">32,</ref><ref type=""bibr"" target=""#b36"">37]</ref>. More recent work focuses on directly processing unordered point cloud representations <ref type=""bibr"" target=""#b26"">[27,</ref><ref type=""bibr"" target=""#b28"">29,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b13""> ose two functions. For example, the aggregation function can be a mean aggregator <ref type=""bibr"" target=""#b17"">[18]</ref>, a max-pooling aggregator <ref type=""bibr"" target=""#b26"">[27,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b41"">42]</ref>, an attention aggregator <ref mation, and outputs a learned graph representation with 4096 vertices. The fusion and MLP prediction blocks follow a similar architecture as PointNet <ref type=""bibr"" target=""#b26"">[27]</ref> and DGCNN <ref type=""bibr"" target=""#b41"">[42]</ref>. The fusion block is used to fuse the global and multi-s",0
"rget=""#b9"">10,</ref><ref type=""bibr"" target=""#b41"">42]</ref>, an attention aggregator <ref type=""bibr"" target=""#b38"">[39]</ref> or an LSTM aggregator <ref type=""bibr"" target=""#b24"">[25]</ref>. The update function can be a multi-layer perceptron <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr",0
"s for drug discovery <ref type=""bibr"" target=""#b53"">[54,</ref><ref type=""bibr"" target=""#b39"">40]</ref>, enhance predictions of recommendation engines <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b49"">50]</ref>, efficiently segment large point clouds <ref type=""bibr"" target=""#b4 anding the bioactivities of these molecules can have substantial impact on drug discovery. Another popular use of graphs is in recommendation engines <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b49"">50]</ref>, where accurate modelling of user interactions leads to improved pro",0
"target=""#b19"">20]</ref>. Scene graphs also facilitate the inverse process, where an image is reconstructed given a graph representation of the scene <ref type=""bibr"" target=""#b16"">[17]</ref>. Graphs are also used to model human joints for action recognition in video <ref type=""bibr"" target=""#b46"">[",0
"ax(h u l − h v l | u l ∈ N (v l )).</formula><p>We then model the vertex feature updater φ as a multi-layer perceptron (MLP) with batch normalization <ref type=""bibr"" target=""#b14"">[15]</ref> and a ReLU as an activation function. This MLP concatenates h v l with its aggregate features from ρ(.) to f",0
"t yet clear how to properly train deep GCN architectures, where several works have studied their limitations <ref type=""bibr"" target=""#b18"">[19,</ref><ref type=""bibr"" target=""#b42"">43,</ref><ref type=""bibr"" target=""#b52"">53]</ref>. Stacking more layers into a GCN leads to the common vanishing gradie 3,</ref><ref type=""bibr"" target=""#b52"">53]</ref> is an open problem in the graph learning space. Recent work <ref type=""bibr"" target=""#b18"">[19,</ref><ref type=""bibr"" target=""#b42"">43,</ref><ref type=""bibr"" target=""#b52"">53]</ref> suggests that GCNs do not scale well to deep architectures, since sta t deep GCNs can cause over-smoothing, which results in features at vertices within each connected component converging to the same value. Other works <ref type=""bibr"" target=""#b42"">[43,</ref><ref type=""bibr"" target=""#b52"">53]</ref> also show the limitations of stacking multiple GCN layers, which lea d aggregation.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.2."">Residual Learning for GCNs</head><p>Designing deep GCN architectures <ref type=""bibr"" target=""#b42"">[43,</ref><ref type=""bibr"" target=""#b52"">53]</ref> is an open problem in the graph learning space. Recent work <ref typ",0
"ed point cloud representations <ref type=""bibr"" target=""#b26"">[27,</ref><ref type=""bibr"" target=""#b28"">29,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b48"">49]</ref>. The recent EdgeConv method by Wang et al. <ref type=""bibr"" target=""# ype=""bibr"" target=""#b31"">(32,</ref><ref type=""bibr"">64,</ref><ref type=""bibr"">128)</ref>, and number of layers <ref type=""bibr"" target=""#b6"">(7,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"">56)</ref>. Overall, we conduct 20 experiments and sho",0
"between layers to facilitate gradient flow. Even with these gates, the authors demonstrate performance degradation after 6 layers of depth. Xu et al. <ref type=""bibr"" target=""#b45"">[46]</ref> developed a Jump Knowledge Network for representation learning and devised an alternative strategy to select",0
"n designed for rigid objects <ref type=""bibr"" target=""#b30"">[31,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b27"">28]</ref>. Algorithms that do consider object articulations <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" t tegory-level pose estimation aims to infer an object's pose and scale relative to a category-specific canonical representation. Recently, Wang et al. <ref type=""bibr"" target=""#b27"">[28]</ref> extended the object coordinate based approach to perform categorylevel pose estimation. The key idea behind Object Coordinate Space (NOCS), where the sizes are normalized and the orientations are aligned for objects in a given category. Whereas the work by <ref type=""bibr"" target=""#b27"">[28]</ref> focuses on pose and size estimation for rigid objects, the work presented here extends the NOCS concept to a <head n=""4.1."">ANCSH Representation</head><p>Our ANCSH representation is inspired by and closely related to Normalized Object Coordinate Space (NOCS) <ref type=""bibr"" target=""#b27"">[28]</ref>, which we briefly review here. NOCS is defined as a 3D space contained within a unit cube and was introduced type=""bibr"" target=""#b27"">[28]</ref>, which we briefly review here. NOCS is defined as a 3D space contained within a unit cube and was introduced in <ref type=""bibr"" target=""#b27"">[28]</ref> to estimate the category-level 6D pose and size of rigid objects. For a given category, the objects are cons es; we define the rest states of all drawers to be closed. In addition to normalizing the articulations, NAOCS applies the same normalization used in <ref type=""bibr"" target=""#b27"">[28]</ref> to the objects, including zero-centering, aligning orientations, and uniformly scaling.</p><p>As a canonical xmlns=""http://www.tei-c.org/ns/1.0""><head>NPCS.</head><p>For each part, NPCS further zero-centers its position and uniformly scales it as is done in <ref type=""bibr"" target=""#b27"">[28]</ref>, while at the same time keeps its orientation unchanged as in NAOCS. In this respect, NPCS is defined simila ibr"" target=""#b27"">[28]</ref>, while at the same time keeps its orientation unchanged as in NAOCS. In this respect, NPCS is defined similarly to NOCS <ref type=""bibr"" target=""#b27"">[28]</ref> but for individual parts instead of whole objects. NPCS provides a part reference frame and we can define th ) .</p><p>Considering a part S (j) , for the points {p i ∈ S (j) }, we have their corresponding NPCS predictions {c i |p i ∈ S (j) }. We could follow <ref type=""bibr"" target=""#b27"">[28]</ref> to perform pose fitting, where the Umeyama algorithm <ref type=""bibr"" target=""#b25"">[26]</ref> is adopted wi st-squares solver to further optimize {R (j) , t (j) }, as is commonly done for bundle adjustment <ref type=""bibr"" target=""#b1"">[2]</ref>. Similar to <ref type=""bibr"" target=""#b27"">[28]</ref>, we also use RANSAC for outlier removal.</p><p>Finally, for each part S (j) , we use the fitted R (j) , t (j lly, for each part S (j) , we use the fitted R (j) , t (j) , s (j) and the NPCS {c i |p i ∈ S (j) } to compute an amodal bounding box, the same as in <ref type=""bibr"" target=""#b27"">[28]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.4."">Camera-Space Joint Parameters and Joint S",1
"ts. We have evaluated our algorithm using both synthetic and real-word datasets. To generate the synthetic data, we mainly use object CAD models from <ref type=""bibr"" target=""#b28"">[29]</ref> along with drawer models from <ref type=""bibr"" target=""#b29"">[30]</ref>. Following the same rendering pipeli ng PointNet++, our approach significantly improves the joint axis prediction for unseen instances (Table <ref type=""table"" target=""#tab_1"">2</ref>).  <ref type=""bibr"" target=""#b28"">[29]</ref> and SAPIEN dataset <ref type=""bibr"" target=""#b29"">[30]</ref> (for only drawer category). Bottom two rows sho 1.0""><head>B. Data generation and statistics</head><p>We render synthetic depth images using the object 3D model provided in the Shape2Motion dataset <ref type=""bibr"" target=""#b28"">[29]</ref> and SAPIEN dataset <ref type=""bibr"" target=""#b29"">[30]</ref>. Both datasets provide the descriptions of the </head><label>4</label><figDesc>Figure 4. Qualitative Results. Top tow rows show test results on unseen object instances from the Shape2Motion dataset<ref type=""bibr"" target=""#b28"">[29]</ref> and SAPIEN dataset<ref type=""bibr"" target=""#b29"">[30]</ref> (for only drawer category). Bottom two rows show",0
"t=""#b27"">28]</ref>. Algorithms that do consider object articulations <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b15"">16]</ref> often require the exact object CAD model and the associated joint par n active manipulation of an object to infer its articulation pattern <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b31"">32]</ref>. For example, Katz et al. <re",0
"n parameters. However, these approaches only consider known object instances and cannot handle different part and kinematic variations. A recent work <ref type=""bibr"" target=""#b0"">[1]</ref> also tries to handle novel objects within the same category by training a mixed density model <ref type=""bibr""",0
"tes and part-level poses. Such understanding is beyond the scope of typical 6D pose estimation algorithms, which have been designed for rigid objects <ref type=""bibr"" target=""#b30"">[31,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b27",0
"f><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b33"">34]</ref>, with adversarial training <ref type=""bibr"" target=""#b20"">[21]</ref> being one of the most effective methods. It formulates training as a game between adversarial attacks and th n widely adopted to generate adversarial examples. Typically, using more attack iterations (higher value of k) produces stronger adversarial examples <ref type=""bibr"" target=""#b20"">[21]</ref>. However, each attack iteration needs to compute the gradient on the input, which causes a large computation can still achieve comparable robustness with respect to traditional PGD-40.</p><p>We apply our technique on Madry's Adversarial Training method (MAT) <ref type=""bibr"" target=""#b20"">[21]</ref> and TRADES <ref type=""bibr"" target=""#b38"">[39]</ref> and evaluate the performance on both MNIST and CIFAR10 e=""bibr"" target=""#b37"">38,</ref><ref type=""bibr"" target=""#b38"">39]</ref> focuse on analyzing and improving adversarial machine learning. Madry et al. <ref type=""bibr"" target=""#b20"">[21]</ref> first formulate adversarial training as a min-max optimization problem:</p><formula xml:id=""formula_0"">min f to the allowed perturbation space S.</p><p>With a higher value of k (more attack iterations), PGDk can generate adversarial examples with higher loss <ref type=""bibr"" target=""#b20"">[21]</ref> els. This property is named as transferability. This property is usually leveraged to perform a black-box at attack is performed based on the accumulated perturbations from previous epochs. To compare the attack strength of two attacks, we use Madry's method <ref type=""bibr"" target=""#b20"">[21]</ref> to adversarially train two models on MNIST and CIFAR10 and evaluate the loss value L(f n (x * n ), y) of adv s/1.0""><head>Evaluation</head><p>In this section, we integrate ATTA with two popular adversarial training methods: Madry's Adversarial Training (MAT) <ref type=""bibr"" target=""#b20"">[21]</ref> and TRADES <ref type=""bibr"" target=""#b38"">[39]</ref>. By evaluating the training time and robustness, we sho p://www.tei-c.org/ns/1.0""><head n=""5.2.1"">Training efficiency</head><p>We select four state-of-the-art adversarial training methods as baselines: MAT <ref type=""bibr"" target=""#b20"">[21]</ref>, TRADES <ref type=""bibr"" target=""#b38"">[39]</ref>, YOPO <ref type=""bibr"" target=""#b36"">[37]</ref> and Free < work</head><p>Adversarial training is first proposed in <ref type=""bibr"" target=""#b15"">[16]</ref> and is formulated as a min-max optimization problem <ref type=""bibr"" target=""#b20"">[21]</ref>. As one of the most effective defense methods, lots of works <ref type=""bibr"" target=""#b1"">[2,</ref><ref typ attack iterations to generate adversarial examples, are widely adopted in various adversarial training methods <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b31"">32,</ref><ref type=""bibr"" target=""#b38"">39]</ref>. Since adversarial perturbati tness. Recently, lots of works <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b37"">38,</ref><ref type=""bibr"" target=""#b38"" pe=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b31"" and attack perturbations outside S need to be projected back to S, k-step projected gradient descent method <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b20"">21]</ref> (PGDk) has been widely adopted to generate adversarial examples. Typically, using more attack iterations (hig owed perturbation space S, PGD-k (k-step projected gradient descent <ref type=""bibr"" target=""#b15"">[16]</ref>) is adopted to conduct iterative attack <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" target=""#b36"">37,</ref><ref type=""bibr"" target=""#b38 to robustness, we conduct an ablation study.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""5.1"">Setup</head><p>Following the literature <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b36"">37,</ref><ref type=""bibr"" target=""#b38"">39]</ref>, we use both MNIST <ref type A.</p><p>For the MNIST dataset, the model has four convolutional layers followed by three full-connected layers which is same architecture as used in <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b38"">39]</ref>. The adversarial perturbation is bounded by l ∞ ball with size = 0.3 he adversarial perturbation is bounded by l ∞ ball with size = 0.3.</p><p>For the CIFAR10 dataset, we use the wide residual network  which is same as <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b38"">39]</ref>. The perturbation is bounded by l ∞ ball with size = 0.031.</p></div r researchers to enhance adversarial training from a new perspective (e.g., improving transferability between epochs). which is same with other works <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" target=""#b36"">37,</ref><ref type=""bibr"" target=""#b38 al details on the implementation, model architecture, and hyper-parameters used in this work.</p><p>MNIST. We use the same model architecture used in <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b36"">37,</ref><ref type=""bibr"" target=""#b38"">39]</ref>, which has four convolutiona e=""bibr"" target=""#b2"">[3]</ref> attack with a 0.01 step size and set decay factor as 1 for M-PGD (momentum PGD).</p><p>CIFAR10. Following other works <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" target=""#b36"">37,</ref><ref type=""bibr"" target=""#b38",1
"s adversarial training methods <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b31"">32,</ref><ref type=""bibr"" target=""#b38"">39]</ref>. Since adversarial perturbations are usually bounded by a constrained space S and attack perturbations outsid ""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b37"">38,</ref><ref type=""bibr"" target=""#b38"">39]</ref> focuse on analyzing and improving adversarial machine learning. Madry et al. <ref type=""bibr"" target=""#b20"">[ to conduct iterative attack <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" target=""#b36"">37,</ref><ref type=""bibr"" target=""#b38"">39]</ref>. PGD-k adversarial attack is the multi-step projected gradient descent on a negative loss function:</p><formu -c.org/ns/1.0""><head n=""5.1"">Setup</head><p>Following the literature <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b36"">37,</ref><ref type=""bibr"" target=""#b38"">39]</ref>, we use both MNIST <ref type=""bibr"" target=""#b16"">[17]</ref> and CIFAR10 dataset <ref type=""bibr"" target=""#b1 has four convolutional layers followed by three full-connected layers which is same architecture as used in <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b38"">39]</ref>. The adversarial perturbation is bounded by l ∞ ball with size = 0.3.</p><p>For the CIFAR10 dataset, we use t l ∞ ball with size = 0.3.</p><p>For the CIFAR10 dataset, we use the wide residual network  which is same as <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b38"">39]</ref>. The perturbation is bounded by l ∞ ball with size = 0.031.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0 ich is same with other works <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" target=""#b36"">37,</ref><ref type=""bibr"" target=""#b38"">39]</ref>. We set the epoch period to reset perturbation as 10 epochs. Following YOPO <ref type=""bibr"" target=""#b36"">[3 at higher adversarial accuracy can lower natural accuracy. This trade-off has been observed and explained in <ref type=""bibr"" target=""#b32"">[33,</ref><ref type=""bibr"" target=""#b38"">39]</ref>. A recent work <ref type=""bibr"" target=""#b12"">[13]</ref> points out that features used by naturally trained m n this work.</p><p>MNIST. We use the same model architecture used in <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b36"">37,</ref><ref type=""bibr"" target=""#b38"">39]</ref>, which has four convolutional layers followed by three fully-connected layers. The adversarial examples used FAR10. Following other works <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" target=""#b36"">37,</ref><ref type=""bibr"" target=""#b38"">39]</ref>, we use Wide-Resnet- <ref type=""bibr"">34-10 [36]</ref> as the model architecture. The adversarial examples us to traditional PGD-40.</p><p>We apply our technique on Madry's Adversarial Training method (MAT) <ref type=""bibr"" target=""#b20"">[21]</ref> and TRADES <ref type=""bibr"" target=""#b38"">[39]</ref> and evaluate the performance on both MNIST and CIFAR10 dataset. Compared with traditional PGD attack, our me g/ns/1.0""><head n=""4.2"">Attack Loss</head><p>Various attack loss functions are used in different adversarial training algorithms. For example, TRADES <ref type=""bibr"" target=""#b38"">[39]</ref> uses robustness loss (Equation <ref type=""formula"" target=""#formula_5"">2</ref>) as the loss to generate the are natural and adversarial example respectively. This loss represents how much the adversarial example diverges from the natural image. Zhang et al. <ref type=""bibr"" target=""#b38"">[39]</ref> shows that this loss has a better performance in the TRADES algorithm.</p><p>In our method, we use the follo integrate ATTA with two popular adversarial training methods: Madry's Adversarial Training (MAT) <ref type=""bibr"" target=""#b20"">[21]</ref> and TRADES <ref type=""bibr"" target=""#b38"">[39]</ref>. By evaluating the training time and robustness, we show that ATTA can provide a better trade-off than other efficiency</head><p>We select four state-of-the-art adversarial training methods as baselines: MAT <ref type=""bibr"" target=""#b20"">[21]</ref>, TRADES <ref type=""bibr"" target=""#b38"">[39]</ref>, YOPO <ref type=""bibr"" target=""#b36"">[37]</ref> and Free <ref type=""bibr"" target=""#b26"">[27]</ref>. For MNIS .tei-c.org/ns/1.0""><head>Defense</head></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>Attack</head><p>Natural PGD-20 Attack loss. Zhang et al. <ref type=""bibr"" target=""#b38"">[39]</ref> show that, for TRADES, using Equation 2 leads to a better robustness. However, in this attack loss, we notic f> improve the training efficiency by recycling the gradient information computed when updating model parameters to generate adversarial examples. In <ref type=""bibr"" target=""#b38"">[39]</ref>, TRADES improves the robustness of an adversarially trained model by adding a robustness regularizer in the <ref type=""foot"" target=""#foot_3"">4</ref> [21], TRADES <ref type=""foot"" target=""#foot_4"">5</ref> [39], YOPO<ref type=""foot"" target=""#foot_5"">6</ref>  <ref type=""bibr"" target=""#b38"">[39]</ref>, and Free<ref type=""foot"" target=""#foot_6"">7</ref>  <ref type=""bibr"" target=""#b26"">[27]</ref> with the hyper",1
"adversarial examples generated to train the model are, the more robust the model is.</p><p>To generate strong adversarial examples, iterative attacks <ref type=""bibr"" target=""#b15"">[16]</ref>, which use multiple attack iterations to generate adversarial examples, are widely adopted in various advers o have high attack strength. PGD-k attack based adversarial training: Unfortunately, solving the inner maximization problem is hard. Iterative attack <ref type=""bibr"" target=""#b15"">[16]</ref> is commonly used to generate strong adversarial examples as an approximate solution for the inner maximizati formula_0"">1</ref>. Since adversarial perturbations are usually bounded by the allowed perturbation space S, PGD-k (k-step projected gradient descent <ref type=""bibr"" target=""#b15"">[16]</ref>) is adopted to conduct iterative attack <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b "">Defense under other attacks</head><p>To verify the robustness of our method, we evaluate models in the previous section with other attacks: PGD-100 <ref type=""bibr"" target=""#b15"">[16]</ref>, FGSM <ref type=""bibr"" target=""#b9"">[10]</ref>, CW-20 <ref type=""bibr"" target=""#b2"">[3]</ref>.  As shown in AT loss 85.04% 54.50% </p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""6"">Related work</head><p>Adversarial training is first proposed in <ref type=""bibr"" target=""#b15"">[16]</ref> and is formulated as a min-max optimization problem <ref type=""bibr"" target=""#b20"">[21]</ref>. As one of the 1 learning rate after 55 epochs, which is the same as <ref type=""bibr"" target=""#b36"">[37]</ref>. To evaluate the model robustness, we perform the PGD <ref type=""bibr"" target=""#b15"">[16]</ref>, M-PGD <ref type=""bibr"" target=""#b7"">[8]</ref> and CW <ref type=""bibr"" target=""#b2"">[3]</ref> attack with a usually bounded by a constrained space S and attack perturbations outside S need to be projected back to S, k-step projected gradient descent method <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b20"">21]</ref> (PGDk) has been widely adopted to generate adversarial examples. Typ",0
"dversarial attacks as a data augmentation method, a model trained with adversarial examples achieves considerable robustness. Recently, lots of works <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b20""> s formulated as a min-max optimization problem <ref type=""bibr"" target=""#b20"">[21]</ref>. As one of the most effective defense methods, lots of works <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b17"">18",0
"=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b37"">38,</ref><ref type=""bibr"" target=""#b38"">39]</ref> focuse on analyzing and impro",0
"close to 100 times larger than natural training.</p><p>Recent works <ref type=""bibr"" target=""#b19"">[20,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b23"">24]</ref> show that adversarial examples can be transferred between models: adversarial examples generated for one mode o perform a black-box attack <ref type=""bibr"" target=""#b18"">[19,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b23"">24]</ref>. To attack a targeted model f t , the attacker generates transferable adversarial examples from the source mo hod to train a source model f s . Rather than the benchmark label y, f s is trained with f t (x) which is the prediction result of the targeted model <ref type=""bibr"" target=""#b23"">[24]</ref> to achieve a higher black-box attack success rate. While our work does not use black-box attacks, we do rely lack-box attack between models <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b34"">35]</ref>. Lie et al. <ref type=""bibr"" target=""#b19"">[20]</ref> show that adver",0
"ning time of adversarial training can be close to 100 times larger than natural training.</p><p>Recent works <ref type=""bibr"" target=""#b19"">[20,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b23"">24]</ref> show that adversarial examples can be transferred between models: adv ty. This property is usually leveraged to perform a black-box attack <ref type=""bibr"" target=""#b18"">[19,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b23"">24]</ref>. To attack a targeted model f t , the attacker generates transferable source model to the targeted model, we use two metrics. The first metric is error rate transferability used in <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b22"">23]</ref>, which is the ratio of the number of adversarial examples misclassified by source model to that of the target",0
"correctly classifies adversarial examples, is an active research area <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b33"">34]</ref>, with adversarial training <r terature <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b36"">37,</ref><ref type=""bibr"" target=""#b38"">39]</ref>, we use both MNIST <ref type=""bibr"" target=""#b16"">[17]</ref> and CIFAR10 dataset <ref type=""bibr"" target=""#b14"">[15]</ref> to evaluate ATTA.</p><p>For the MNIST dataset,",0
"n=""1"">Introduction</head><p>State-of-the-art deep learning models for computer vision tasks have been found to be vulnerable to adversarial examples <ref type=""bibr"" target=""#b30"">[31]</ref>. Even a small perturbation on the image can fool a well-trained model. Recent works <ref type=""bibr"" target= an adversarially trained model by adding a robustness regularizer in the loss function.</p><p>Transferability of adversarial examples. Szegedy et al. <ref type=""bibr"" target=""#b30"">[31]</ref> first describes the transferability of adversarial examples. This property is usually used to perform black-",0
"on method, a model trained with adversarial examples achieves considerable robustness. Recently, lots of works <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b25"" >[21]</ref>. As one of the most effective defense methods, lots of works <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b26""",0
"urrent hyperbolic embedding techniques only account for the graph structure and do not leverage rich node features. For instance, Poincar? embeddings <ref type=""bibr"" target=""#b28"">[29]</ref> capture the hyperbolic properties of real graphs by learning shallow embeddings with hyperbolic distance met s to as the label for node classification.</p><p>Baselines. For shallow methods, we consider Euclidean embeddings (EUC) and Poincar? embeddings (HYP) <ref type=""bibr"" target=""#b28"">[29]</ref>. We conjecture that HYP will outperform EUC on hierarchical graphs. For a fair comparison with HGCN which le pe=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b30"">31]</ref>. Shallow embedding methods have also been developed in hyperbolic geometry <ref type=""bibr"" target=""#b28"">[29,</ref><ref type=""bibr"" target=""#b29"">30]</ref> for reconstructing trees <ref type=""bibr"" target=""#b34"">[35]</ref> a then be used to predict node attributes or links.</p><p>For link prediction, we use the Fermi-Dirac decoder <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b28"">29]</ref>, a generalization of sigmoid, to compute probability scores for edges:</p><formula xml:id=""formula_21"">p((i,",1
"""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b40"">41,</ref><ref type=""bibr"" target=""#b44"">45,</ref><ref type=""bibr"" target=""#b46"">47]</ref>. While various Graph Neural Network architectures resolve the disadvantages of shallow embeddings, they gener",0
"he same local minimum in training also vary by a factor of ? K. In practice, however, optimization is much more stable when the values are normalized <ref type=""bibr"" target=""#b15"">[16]</ref>. In the context of HGCN, trainable curvature provides a natural way to learn embeddings of the right scale a",0
"operations of GCNs in the hyperboloid model of hyperbolic space to transform input features which lie in Euclidean space into hyperbolic embeddings; <ref type=""bibr"" target=""#b1"">(2)</ref> We introduce a hyperbolic attention-based aggregation scheme that captures hierarchical structure of networks; PUBMED has 19,717 publications in the area of medicine grouped in 3 classes. 2. Disease propagation tree. We simulate the SIR disease spreading model <ref type=""bibr"" target=""#b1"">[2]</ref>, where the label of a node is whether the node was infected or not. Based on the model, we build tree networks",0
"an aggregation in Euclidean GCN computes the weighted average j?N (i) w j x j . An analog of mean aggregation in hyperbolic space is the Fr?chet mean <ref type=""bibr"" target=""#b8"">[9]</ref>, which, however, has no closed form solution. Instead, we propose to perform aggregation in tangent spaces usi",0
"rm a heterogeneous graph into a homogeneous graph defined by the meta-paths. Then conventional GNNs can operate on the transformed homogeneous graphs <ref type=""bibr"" target=""#b36"">[37,</ref><ref type=""bibr"" target=""#b42"">43]</ref>. This is a two-stage approach and requires hand-crafted meta-paths f -negative weights from softmax(W 1 φ ).</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.2"">Meta-Path Generation</head><p>Previous works <ref type=""bibr"" target=""#b36"">[37,</ref><ref type=""bibr"" target=""#b42"">43]</ref> require manually defined meta-paths and perform Graph Neural Network s representation. The metapath2vec <ref type=""bibr"" target=""#b9"">[10]</ref> learns graph representations by using meta-path based random walk and HAN <ref type=""bibr"" target=""#b36"">[37]</ref> learns graph representation learning by transforming a heterogeneous graph into a homogeneous graph construc s a tensor A ∈ R N ×N ×K . We also have a feature matrix X ∈ R N ×D meaning that the D-dimensional input feature given for each node.</p><p>Meta-Path <ref type=""bibr"" target=""#b36"">[37]</ref> denoted by p is a path on the heterogeneous graph G that is connected with heterogeneous edges, i.e., v</p>< 0""><head>GNN-based methods</head><p>We used the GCN <ref type=""bibr"" target=""#b18"">[19]</ref>, GAT <ref type=""bibr"" target=""#b32"">[33]</ref>, and HAN <ref type=""bibr"" target=""#b36"">[37]</ref> as GNN based methods. GCN is a graph convolutional network which utilizes a localized first-order approximat connecting vertices with pre-defined meta-paths. Here, we test HAN on the selected sub-graphs whose nodes are linked with meta-paths as described in <ref type=""bibr"" target=""#b36"">[37]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.2"">Results on Node Classification</head><p>Ef",1
"ogeneous graph datasets have been recently studied for other network analysis tasks, such as link prediction <ref type=""bibr"" target=""#b35"">[36,</ref><ref type=""bibr"" target=""#b40"">41]</ref> and graph classification <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b23"">24]</ref>, a",1
"us graph defined by the meta-paths. Then conventional GNNs can operate on the transformed homogeneous graphs <ref type=""bibr"" target=""#b36"">[37,</ref><ref type=""bibr"" target=""#b42"">43]</ref>. This is a two-stage approach and requires hand-crafted meta-paths for each problem. The accuracy of downstre p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.2"">Meta-Path Generation</head><p>Previous works <ref type=""bibr"" target=""#b36"">[37,</ref><ref type=""bibr"" target=""#b42"">43]</ref> require manually defined meta-paths and perform Graph Neural Networks on the meta-path graphs. Instead, our G",1
"ach task and learn node representation on the graphs in an end-to-end fashion. GTNs can be viewed as a graph analogue of Spatial Transformer Networks <ref type=""bibr"" target=""#b15"">[16]</ref> which explicitly learn spatial transformations of input images or features. The main challenge to transform",0
"ion with GNNs. Node classification has been studied for decades. Conventionally, hand-crafted features have been used such as simple graph statistics <ref type=""bibr"" target=""#b1"">[2]</ref>, graph kernel <ref type=""bibr"" target=""#b33"">[34]</ref>, and engineered features from a local neighbor structu",0
"ution based on spectral convolution <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b25"">26]</ref>, attention mechanism on neighbors <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b32"">33]</ref>, subsampling <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr""",0
"f type=""bibr"" target=""#b1"">[2]</ref>, graph kernel <ref type=""bibr"" target=""#b33"">[34]</ref>, and engineered features from a local neighbor structure <ref type=""bibr"" target=""#b22"">[23]</ref>. These features are not flexible and suffer from poor performance. To overcome the drawback, recently node r",0
"pe=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b37"">38]</ref> and nonspectral methods <ref type=""bibr"" target=""#b6"">[7,</ref><ref t",0
"rk analysis tasks, such as link prediction <ref type=""bibr"" target=""#b35"">[36,</ref><ref type=""bibr"" target=""#b40"">41]</ref> and graph classification <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b23"">24]</ref>, applying our GTNs to the other tasks can be interesting future dire",0
"n the spectral domain using the Fourier basis of a given graph, i.e., eigenfunctions of the Laplacian operator <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b18"">19]</ref>.</p><p>However, one limitation of most GNNs is that they assume the g wide range of tasks. They are categorized into two approaches: spectral <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b37""",0
"</ref><ref type=""bibr"" target=""#b29"">30,</ref><ref type=""bibr"" target=""#b41"">42]</ref> and node classification <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b32"">33]</ref>. The representation learnt by GNNs has been proven to be effective in o be effective in achieving state-ofthe-art performance in a variety of graph datasets such as social networks <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b34"">35]</ref>, citation networks <ref type=""bibr"" target=""#b18"">[19,</ref><ref type </ref><ref type=""bibr"" target=""#b37"">38]</ref> and nonspectral methods <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b28"">29,</ref><ref type=""bibr"" target=""#b32"" lying graph structure is utilized by GNNs to operate convolution directly on graphs by passing node features <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b13"">14]</ref> to neighbors, or perform convolution in the spectral domain using the Fourier basis of a given graph, i.e., e e, Veličković et al. <ref type=""bibr"" target=""#b32"">[33]</ref> applies different weight matrices for nodes with different degrees and Hamilton et al. <ref type=""bibr"" target=""#b13"">[14]</ref> has proposed learnable aggregator functions which summarize neighbors' information for graph representation "">33]</ref>, subsampling <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b6"">7]</ref> and inductive representation for a large graph <ref type=""bibr"" target=""#b13"">[14]</ref> have been studied. Although these methods show outstanding results, all these methods have a common limitati",0
"r decades. Conventionally, hand-crafted features have been used such as simple graph statistics <ref type=""bibr"" target=""#b1"">[2]</ref>, graph kernel <ref type=""bibr"" target=""#b33"">[34]</ref>, and engineered features from a local neighbor structure <ref type=""bibr"" target=""#b22"">[23]</ref>. These fe",0
"to neighbors, or perform convolution in the spectral domain using the Fourier basis of a given graph, i.e., eigenfunctions of the Laplacian operator <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b18"">19]</ref>.</p><p>However, one limitation sses of GNNs have been developed for a wide range of tasks. They are categorized into two approaches: spectral <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b21"">2",0
"18"">[19,</ref><ref type=""bibr"" target=""#b32"">33]</ref>, functional structure of brains <ref type=""bibr"" target=""#b19"">[20]</ref>, recommender systems <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" target=""#b38"">39]</ref>. The underlying graph structur",0
"troduction</head><p>In recent years, Graph Neural Networks (GNNs) have been widely adopted in various tasks over graphs, such as graph classification <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b39"">40]</ref>, link prediction <ref type=""",0
"substantial efforts from both the industrial and academic communities <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b14"">14,</ref><ref type=""bibr"" target=""#b18"">18,</ref><ref type=""bibr"" target=""#b20"">20,</ref><ref type=""bibr"" target=""#b28"">28,</ref><ref type=""bibr"" target=""#b43"" 1.0""><head n=""3.2"">The Need for a GCN Accelerator</head><p>GCNs are showing great potential in various tasks <ref type=""bibr"" target=""#b16"">[16,</ref><ref type=""bibr"" target=""#b18"">18,</ref><ref type=""bibr"" target=""#b19"">19,</ref><ref type=""bibr"" target=""#b38"">38,</ref><ref type=""bibr"" target=""#b43"" , where the feature vector of each vertex is computed by recursively aggregating and transforming the representation vectors of its neighbor vertices <ref type=""bibr"" target=""#b18"">[18,</ref><ref type=""bibr"" target=""#b39"">39,</ref><ref type=""bibr"" target=""#b47"">47]</ref>. Fig. <ref type=""figure"" tar is usually applied before the Aggregate function to sample a subset from the neighbor vertices of each vertex <ref type=""bibr"" target=""#b6"">[6,</ref><ref type=""bibr"" target=""#b18"">18]</ref> as the new neighbors, specifically,</p><formula xml:id=""formula_1"">S(v) = Sample k N(v) .</formula><p>(2) Som ><p>GraphSage further adopts uniform neighbor sampling to alleviate receptive field expansion that effectively trades off accuracy and execution time <ref type=""bibr"" target=""#b18"">[18]</ref>. It is formulated as</p><formula xml:id=""formula_4"">a k v = Mean {h (k−1) v } ∪ {h (k−1) u , ∀u ∈ S(v)} , h ple a subset from neighbors, which can be done during preprocessing <ref type=""bibr"" target=""#b20"">[20]</ref> or with random selection during runtime <ref type=""bibr"" target=""#b18"">[18]</ref>. Aggregation aggregates the features from its 1-hop neighbors. Pooling acts like the pooling layer in CNNs t ibr"" target=""#b15"">[15]</ref> on Intel Xeon CPU. The execution time breakdown of GCN (GCN) <ref type=""bibr"" target=""#b25"">[25]</ref>, GraphSage (GSC) <ref type=""bibr"" target=""#b18"">[18]</ref>, and GINConv (GIN) <ref type=""bibr"" target=""#b39"">[39]</ref> on several datasets <ref type=""bibr"" target=""#b",1
"an effective category of models to represent and process graph data <ref type=""bibr"" target=""#b38"">[38,</ref><ref type=""bibr"" target=""#b45"">45,</ref><ref type=""bibr"" target=""#b46"">46,</ref><ref type=""bibr"" target=""#b47"">47]</ref>. GCNs convert the graph data into a low dimensional space while keepi ""bibr"" target=""#b18"">18,</ref><ref type=""bibr"" target=""#b19"">19,</ref><ref type=""bibr"" target=""#b38"">38,</ref><ref type=""bibr"" target=""#b43"">43,</ref><ref type=""bibr"" target=""#b46"">46]</ref>. Many companies, such as Google <ref type=""bibr"" target=""#b13"">[13]</ref>, Facebook <ref type=""bibr"" target=""",0
""">47]</ref>.</p><p>The convolutional layers occupy the major execution time of GCNs through two primary execution phases: Aggregation and Combination <ref type=""bibr"" target=""#b15"">[15,</ref><ref type=""bibr"" target=""#b40"">40,</ref><ref type=""bibr"" target=""#b47"">47]</ref>. The Aggregation phase maint layer in CNNs to realize graph transformation by reducing the number of vertices and the length of feature vectors. Readout can be a simple summation <ref type=""bibr"" target=""#b15"">[15]</ref> across vertices or further concatenation across iterations <ref type=""bibr"" target=""#b39"">[39]</ref>. Theref d n=""3.1"">Characterization on CPU</head><p>We conduct quantitative characterizations using a state-ofthe-art GCN software framework PyTorch Geometric <ref type=""bibr"" target=""#b15"">[15]</ref> on Intel Xeon CPU. The execution time breakdown of GCN (GCN) <ref type=""bibr"" target=""#b25"">[25]</ref>, Grap mode. Baseline Platform. To compare the performance and energy consumption of HyGCN with state-of-the-art works, we evaluate PyTorch Geometric (PyG) <ref type=""bibr"" target=""#b15"">[15]</ref> on a Linux workstation equipped with two Intel Xeon E5-2680 v3 CPUs and 378 GB DDR4 memory and on an NVIDIA type=""bibr"" target=""#b28"">28,</ref><ref type=""bibr"" target=""#b43"">43,</ref><ref type=""bibr"" target=""#b47"">47]</ref>. For instance, Py-Torch Geometric <ref type=""bibr"" target=""#b15"">[15]</ref> leverages message-passing framework to enhance its expression ability and the hardware-optimized operations tern workloads. Therefore, a large number of software frameworks for hybrid-pattern GCNs are proposed recently <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b15"">15,</ref><ref type=""bibr"" target=""#b28"">28,</ref><ref type=""bibr"" target=""#b43"">43,</ref><ref type=""bibr"" target=""#b47""",0
"them only work well for the single-pattern workloads. Therefore, a large number of software frameworks for hybrid-pattern GCNs are proposed recently <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b15"">15,</ref><ref type=""bibr"" target=""#b28"">28,</ref><ref type=""bibr"" target=""#b43"">",0
"rget=""#b28"">28,</ref><ref type=""bibr"" target=""#b43"">43,</ref><ref type=""bibr"" target=""#b44"">44]</ref> to solve problems including node classification <ref type=""bibr"" target=""#b25"">[25]</ref>, link prediction <ref type=""bibr"" target=""#b14"">[14,</ref><ref type=""bibr"" target=""#b16"">16]</ref>, graph cl e-art GCN software framework PyTorch Geometric <ref type=""bibr"" target=""#b15"">[15]</ref> on Intel Xeon CPU. The execution time breakdown of GCN (GCN) <ref type=""bibr"" target=""#b25"">[25]</ref>, GraphSage (GSC) <ref type=""bibr"" target=""#b18"">[18]</ref>, and GINConv (GIN) <ref type=""bibr"" target=""#b39"" al datasets <ref type=""bibr"" target=""#b23"">[23]</ref> is illustrated in Fig. <ref type=""figure"" target=""#fig_3"">2</ref>. The profiling results of GCN <ref type=""bibr"" target=""#b25"">[25]</ref> on the COLLAB dataset <ref type=""bibr"" target=""#b23"">[23]</ref> are presented in Table <ref type=""table"" tar l GCN models as examples to explain the above operations in detail.</p><p>GCN is one of the most successful convolutional networks for graph learning <ref type=""bibr"" target=""#b25"">[25,</ref><ref type=""bibr"" target=""#b38"">38]</ref>, which bridges the gap between spectral-based convolutions and spati",0
"on practice to share only the gradients in order protect the proprietary data. However, recent work by Zhu et al., ""Deep Leakage from Gradient"" (DLG) <ref type=""bibr"" target=""#b0"">[1]</ref> showed the possibility to steal the private training data from the shared gradients of other participants.</p> twork (NN) trained with cross-entropy loss. This enables us to always extract the ground-truth labels and significantly simplify the objective of DLG <ref type=""bibr"" target=""#b0"">[1]</ref> in order to extract good-quality data. Hence, we name our approach, Improved DLG (iDLG). The main contribution ts with 100% accuracy, which facilitates the data extraction with better fidelity.</p><p>• We empirically demonstrate the advantages of iDLG over DLG <ref type=""bibr"" target=""#b0"">[1]</ref> via comparing the accuracy of extracted labels and the fidelity of extracted data on three datasets.</p><p>The 4 concludes the paper with discussion.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2"">Methodology</head><p>Recent work by Zhu et al. <ref type=""bibr"" target=""#b0"">[1]</ref> presents an approach (DLG) to steal the proprietary data protected by the participants in distributed learning lns=""http://www.tei-c.org/ns/1.0""><head>Experiments</head><p>In this section, we empirically demonstrate the advantages of our (iDLG) method over DLG <ref type=""bibr"" target=""#b0"">[1]</ref>. We perform experiments on the classification task over three datasets: MNIST <ref type=""bibr"" target=""#b7"">[8 r"" target=""#b8"">[9]</ref>, and LFW <ref type=""bibr"" target=""#b9"">[10]</ref> with 10, 100, and 5749 categories respectively. Following the settings in <ref type=""bibr"" target=""#b0"">[1]</ref>, we use the randomly initialized LeNet for all experiments. L-BFGS <ref type=""bibr"" target=""#b10"">[11]</ref> w ibr"" target=""#b10"">[11]</ref> with learning rate 1 is used as the optimizer. For fast training, we resize all images in LFW to 32 × 32.</p><p>For DLG <ref type=""bibr"" target=""#b0"">[1]</ref>, as described by the authors, we start the procedure with the randomly initialized dummy data and outputs (x , Dataset DLG iDLG MNIST 89.9% 100.0% CIFAR-100 83.3% 100.0% LFW 79.1% 100.0% Table <ref type=""table"">1</ref>: Accuracy of the extracted labels for DLG <ref type=""bibr"" target=""#b0"">[1]</ref> and iDLG. Note that iDLG always extracts the correct label as opposed to DLG which extracts wrong labels frequ",1
"es respectively. Following the settings in <ref type=""bibr"" target=""#b0"">[1]</ref>, we use the randomly initialized LeNet for all experiments. L-BFGS <ref type=""bibr"" target=""#b10"">[11]</ref> with learning rate 1 is used as the optimizer. For fast training, we resize all images in LFW to 32 × 32.</p",0
"<div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""1"">Introduction</head><p>In multi-node distributed learning systems such as Collaborative Learning <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b3"">4]</ref> and Federated Learning <ref type=""",0
"e Learning <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b3"">4]</ref> and Federated Learning <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b6"">7]</ref>, it is widely believed that sharin",0
"he classification task over three datasets: MNIST <ref type=""bibr"" target=""#b7"">[8]</ref>, CIFAR-100 <ref type=""bibr"" target=""#b8"">[9]</ref>, and LFW <ref type=""bibr"" target=""#b9"">[10]</ref> with 10, 100, and 5749 categories respectively. Following the settings in <ref type=""bibr"" target=""#b0"">[1]</",0
"introduce a selection of augmentations to known MPNN architectures, which we refer to as Attention MPNN (AMPNN) and Edge Memory Neural Network (EMNN) <ref type=""bibr"" target=""#b36"">[34]</ref>, and evaluate them against published benchmark results with a range of metrics. The EMNN network shares arch",1
"</ref>, Graph Autoencoders <ref type=""bibr"" target=""#b18"">[16]</ref><ref type=""bibr"" target=""#b19"">[17]</ref><ref type=""bibr"" target=""#b20"">[18]</ref><ref type=""bibr"" target=""#b21"">[19]</ref>, and Graph Spatial-Temporal Networks <ref type=""bibr"" target=""#b22"">[20]</ref><ref type=""bibr"" target=""#b23""",0
"senal for doing QSAR.</p><p>Over the past decade, deep learning has become a staple in the machine learning toolbox of many fields and research areas <ref type=""bibr"" target=""#b11"">[9,</ref><ref type=""bibr"" target=""#b12"">10]</ref>. Notably in the pharmaceutical area, in recent years AI has shown inc",0
"assing neural network (SELU-MPNN)</head><p>Our first architecture involved the basic MPNN framework, but with the use of the SELU activation function <ref type=""bibr"" target=""#b41"">[39]</ref> instead of more traditional batch or layer norm functions. The SELU activation function is parameterised to",0
"ural similarities to the D-MPNN model published by Yang et al. <ref type=""bibr"" target=""#b37"">[35]</ref> that was developed concurrently to this work <ref type=""bibr"" target=""#b38"">[36]</ref>, but the D-MPNN includes additional chemical descriptor information. We applied these two types of neural ne",0
"for users.</p><p>Recommendation while countering private-attribute inference attack can be naturally formulated as a problem of adversarial learning <ref type=""bibr"" target=""#b18"">[19]</ref>. In our proposed RAP, there are two components: a Bayesian personalized ranking recommender and a private-at",1
"ifferent aspects such as textual information <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b10"">11]</ref>, web browsing histories <ref type=""bibr"" target=""#b5"">[6]</ref>, private-attributes disclosure <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b23"">24]</re",0
"bibr"" target=""#b29"">[30]</ref> to infer their private attribute information. The third group of works exploits both friend and behavioral information <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b24"">25]</ref>. Gong et al. <ref type=""bibr al information <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b24"">25]</ref>. Gong et al. <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b16"">17]</ref> make a social-behavior-attribute network in which all users' behavio",0
"<p>Explosive growth of the Web has raised numerous challenges for online users including disinformation spread <ref type=""bibr"" target=""#b0"">[1]</ref><ref type=""bibr"" target=""#b1"">[2]</ref><ref type=""bibr"" target=""#b2"">[3]</ref><ref type=""bibr"" target=""#b3"">[4]</ref> and threats to users' privacy <r",0
"and investigate if the target is in the database. They could be categorized into differential privacy based <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" target=""#b44"">45]</ref> and perturbation based <ref t ""bibr"" target=""#b32"">[33]</ref> utilize differential privacy to construct private covariance matrices to be further used by recommender. Another work <ref type=""bibr"" target=""#b25"">[26]</ref> clusters users w.r.t. the social relations and generates differentially private average of users' preference",0
"ef> uses a gated recurrent unit <ref type=""bibr"" target=""#b1"">[2]</ref> based controller to update the memory, while Working Memory Network (W-MemNN) <ref type=""bibr"" target=""#b16"">[17]</ref> uses a multi-head attention <ref type=""bibr"" target=""#b25"">[26]</ref> based controller. All these networks u updating controller based on multi-head attention <ref type=""bibr"" target=""#b25"">[26]</ref>, which is similar to that used in Working Memory Network <ref type=""bibr"" target=""#b16"">[17]</ref>. Multi-head attention allows the model to jointly attend to different representation subspaces using project",1
"e=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b30"">31]</ref>. The recognition process has become a necessary part of many online systems, such as AMiner <ref type=""bibr"" ually, the plain text of an academic homepage is saved first, then the recognition tasks are conducted on text <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b30"">31]</ref>. Given the plain text of an academic homepage, we aim to recognise all the person names and publications from oken is represented as a d e -dimension word embedding, i.e., S ∈ R n×d e . Following state-of-the-art methods <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b30"">31]</ref>, we use GloVe <ref type=""bibr"" target=""#b18"">[19]</ref> to learn word embeddings on an academic homepage data example.</p><p>Recently, deep learning based methods have been developed to address these problems. The state-of-the-art for publication recognition <ref type=""bibr"" target=""#b30"">[31]</ref> uses a Bi-LSTM-CRF based model to learn the page-level and line-level structure. The state-of-the-art for pe om the plain text using deep learning based natural language processing methods. For example, state-of-the-art techniques for publication recognition <ref type=""bibr"" target=""#b30"">[31]</ref> and for person names recognition <ref type=""bibr"" target=""#b0"">[1]</ref> use Bi-LSTM-CRF based models to rec rom two correlated tasks.</p><p>Moreover, we use different methods to capture the position patterns. The state-of-the-art for publication recognition <ref type=""bibr"" target=""#b30"">[31]</ref> trains webpage-level and line-level models together to capture the position information of academic homepage =""4.1"">Experimental Setup</head><p>4.1.1 Dataset and Preprocessing. We use the same datasets used by the state-of-the-art for publication recognition <ref type=""bibr"" target=""#b30"">[31]</ref> and person name recognition <ref type=""bibr"" target=""#b0"">[1]</ref>. Table <ref type=""table"" target=""#tab_0"" n <ref type=""bibr"" target=""#b0"">[1]</ref>. Table <ref type=""table"" target=""#tab_0"">2</ref> summarises the dataset statistics.</p><p>• HomePub dataset <ref type=""bibr"" target=""#b30"">[31]</ref> contains the plain text of 2,087 homepages from different universities and research institutes with 12,796 p rdly handle complex homepages without the extra information about the position patterns and person names. PAM also outperforms the hierarchical PubSE <ref type=""bibr"" target=""#b30"">[31]</ref> model, which can capture the positional diversity, by 3.64% in F1 score. The advantage of our model is more ><head n=""4.5"">Error Analysis</head><p>We perform a manual inspection of recognition results of state-ofthe-art models for the two tasks (i.e., PubSE <ref type=""bibr"" target=""#b30"">[31]</ref> and CogNN <ref type=""bibr"" target=""#b0"">[1]</ref>) and our proposed PAM model on 50 randomly selected homepa ame recognition task. In total, we inspect 1,137 publications and 5,542 person names.</p><p>For publication string recognition, we observe that PubSE <ref type=""bibr"" target=""#b30"">[31]</ref> misrecognises strings about patents, grants, and research projects as publications. PAM avoids these errors",1
"nto the memory updating process. Studies have exploited relative token position and importance in a sentence <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b28"">29]</ref>, whereas our algorithm focuses on relative line position and importance in a page.</p></div> <div xmlns=""http",0
"r"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b26"">27]</ref> are proposed for question answering in recent years. Dynamic Memory Network (DMN) <ref type=""bibr"" target=""#b11"">[12]</ref> uses a gated recurrent unit <ref type=""bibr"" target=""#b1"">[2]</ref> based controller to update the memory, w are concatenated to be the input of the name predictor. • Joint-Gate is resulted from replacing the AM and PM modules with the gating function in DMN <ref type=""bibr"" target=""#b11"">[12]</ref>. This model has a pipeline architecture for two jointly trained tasks. In the N → P direction, the output of",0
"31]</ref> uses a Bi-LSTM-CRF based model to learn the page-level and line-level structure. The state-of-the-art for person name in academic homepages <ref type=""bibr"" target=""#b0"">[1]</ref> uses a co-guided neural network to learn from fine-grained annotation of names. Despite their success, these s hods. For example, state-of-the-art techniques for publication recognition <ref type=""bibr"" target=""#b30"">[31]</ref> and for person names recognition <ref type=""bibr"" target=""#b0"">[1]</ref> use Bi-LSTM-CRF based models to recognise information from the plain text of the homepages. However, they solv use the same datasets used by the state-of-the-art for publication recognition <ref type=""bibr"" target=""#b30"">[31]</ref> and person name recognition <ref type=""bibr"" target=""#b0"">[1]</ref>. Table <ref type=""table"" target=""#tab_0"">2</ref> summarises the dataset statistics.</p><p>• HomePub dataset <r contains the plain text of 2,087 homepages from different universities and research institutes with 12,796 publications annotated. • HomeName dataset <ref type=""bibr"" target=""#b0"">[1]</ref> is constructed from the HomePub dataset by further labeling the person names. All the 70,864 person names are that our model has better capability to cover more person names with the knowledge from the publication recognition task. PAM also outperforms CogNN <ref type=""bibr"" target=""#b0"">[1]</ref> by 1.40% on token level and 2.06% on name level in F1 score. Note that CogNN relies on extra labelling informa manual inspection of recognition results of state-ofthe-art models for the two tasks (i.e., PubSE <ref type=""bibr"" target=""#b30"">[31]</ref> and CogNN <ref type=""bibr"" target=""#b0"">[1]</ref>) and our proposed PAM model on 50 randomly selected homepages. We focus on string level performance for the pu Pedagogy, Virginia Tech (Feb 2016) is a talk given by the page owenr but not a publication.</p><p>For person name recognition, we observe that CogNN <ref type=""bibr"" target=""#b0"">[1]</ref> tends to produce false negative predictions in groups, i.e., a series of person names in a publication string INT LEARNING FOR BOTH TASKS</head><p>Usually, the plain text of an academic homepage is saved first, then the recognition tasks are conducted on text <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b30"">31]</ref>. Given the plain text of an academic homepage, we aim to recognise all get a sequence S of n tokens and each token is represented as a d e -dimension word embedding, i.e., S ∈ R n×d e . Following state-of-the-art methods <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b30"">31]</ref>, we use GloVe <ref type=""bibr"" target=""#b18"">[19]</ref> to learn word",0
"the two tasks, or using simple concatenation procedures when training <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b31"">32]</ref>. However, our experimental study (Section 4.3.2) shows that such a naive way of joint learning does not yield",0
"/p><p>A few other studies recognise person names and publications from research papers and digital libraries <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b24"">25]</ref>. Such a recognition problem i",0
"in the extraction and mining of such information from academic homepages <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b30"" >. The recognition process has become a necessary part of many online systems, such as AMiner <ref type=""bibr"" target=""#b23"">[24]</ref> and CiteSeerX <ref type=""bibr"" target=""#b15"">[16]</ref>, and the extracted person names and publications can bring interesting applications. For example, person nam",0
"<head n=""4.2.2"">Person Name Recognition.</head><p>Our proposed PAM model outperforms the baselines that use standard NER models, such as Stanford NER <ref type=""bibr"" target=""#b4"">[5]</ref> and Bi-LSTM-CRF <ref type=""bibr"" target=""#b8"">[9]</ref>, by at least 5.56% on token level and 7.09% on name le",0
"loss of the two tasks or simply concatenating the representation of publication and person name when training <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b12"">13]</ref>. However, our experimental study shows that such a straightforward approach performs poorly. The issue is tha blem, i.e., minimising the total loss of the two tasks, or using simple concatenation procedures when training <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b31"">32]</ref>. However, our experimental study (Section 4.3.2) shows that such a na tly by minimising the total loss. • Joint-Concat is resulted from replacing the AM and PM modules with a concatenation procedure similar to Ma et al. <ref type=""bibr"" target=""#b12"">[13]</ref> and Hashimoto et al. <ref type=""bibr"" target=""#b5"">[6]</ref>. This model has a pipeline architecture for two",0
"ition by describing general semantic representation or essential characteristics. However, there is no such wide-accepted formal definition. Paulheim <ref type=""bibr"" target=""#b5"">[6]</ref> defined four criteria for knowledge graphs. Ehrlinger and W öß <ref type=""bibr"" target=""#b6"">[7]</ref> analyze survey papers on knowledge graphs mainly focus on statistical relational learning <ref type=""bibr"" target=""#b8"">[9]</ref>, knowledge graph refinement <ref type=""bibr"" target=""#b5"">[6]</ref>, Chinese knowledge graph construction <ref type=""bibr"" target=""#b9"">[10]</ref>, KGE <ref type=""bibr"" target=""#",1
"#b6"">[7]</ref> analyzed several existing definitions and proposed Definition 1 which emphasizes the reasoning engine of knowledge graphs. Wang et al. <ref type=""bibr"" target=""#b7"">[8]</ref> proposed a definition as a multi-relational graph in Definition 2. Following previous literature, we define a f>). A knowledge graph acquires and integrates information into an ontology and applies a reasoner to derive new knowledge. Definition 2 (Wang et al. <ref type=""bibr"" target=""#b7"">[8]</ref>). A knowledge graph is a multirelational graph composed of entities and relations which are regarded as nodes nowledge graph refinement <ref type=""bibr"" target=""#b5"">[6]</ref>, Chinese knowledge graph construction <ref type=""bibr"" target=""#b9"">[10]</ref>, KGE <ref type=""bibr"" target=""#b7"">[8]</ref> or KRL <ref type=""bibr"" target=""#b10"">[11]</ref>. The latter two surveys are more related to our work. Lin et rk. Lin et al. <ref type=""bibr"" target=""#b10"">[11]</ref> presented KRL in a linear manner, with a concentration on quantitative analysis. Wang et al. <ref type=""bibr"" target=""#b7"">[8]</ref> categorized KRL according to scoring functions, and specifically focused on the type of information utilized i epresentation space. There still remains many kinds of auxiliary information for KRL such as attributes, relation path and logical rules. Wang et al. <ref type=""bibr"" target=""#b7"">[8]</ref> gave a detailed review on these information. This paper discusses relation path and logical rules under the um",1
"w.tei-c.org/ns/1.0""><head n=""2.4"">Related Surveys</head><p>Previous survey papers on knowledge graphs mainly focus on statistical relational learning <ref type=""bibr"" target=""#b8"">[9]</ref>, knowledge graph refinement <ref type=""bibr"" target=""#b5"">[6]</ref>, Chinese knowledge graph construction <ref",1
"type encoder model for projection matrix of entities to capture type hierarchy. Noticing that some relations indicate attributes of entities, KR-EAR <ref type=""bibr"" target=""#b60"">[61]</ref> categorizes relation types into attributes and relations and modeled the correlations between entity descrip",0
"dependency tree, but utilizes multi-head attention for edge selection in a soft weighting manner. Unlike previous two GCN-based models, Zhang et al., <ref type=""bibr"" target=""#b123"">[124]</ref> applied GCN for relation embedding in knowledge graph for sentence-based relation extraction. The authors position embedding, relation hierarchy GCNs C-GCN <ref type=""bibr"" target=""#b121"">[122]</ref> LSTM + GCN + path-centric pruning dependency tree KATT <ref type=""bibr"" target=""#b123"">[124]</ref> Pre-training + GCN + CNN + attention position embedding, relation hierarchy AGGCN <ref type=""bibr"" target=",0
"e=""bibr"" target=""#b102"">[103]</ref> learning multiple views of entity name, relation and attributes, and alignment with character attribute embedding <ref type=""bibr"" target=""#b103"">[104]</ref>. </p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.3"">Relation Extraction</head><p>Relation e",0
"the structural entity embedding for multi-step matching guided by long short-term memory (LSTM) networks to calculate the similarity scores. Meta-KGR <ref type=""bibr"" target=""#b86"">[87]</ref>, an optimization-based meta learning approach, adopts model agnostic meta learning for fast adaption and rei",0
"160"">[161]</ref> conducts multi-hop logic reasoning with reasoning-graph embedding, while handles the uncertainty in topic entity recognition. KagNet <ref type=""bibr"" target=""#b161"">[162]</ref> performs concept recognition to build a schema graph from ConceptNet and learns path-based relational repr",0
"nal intellectual source, simple factoid QA or single-fact QA is to answer simple question involving with a single knowledge graph fact. Bordes et al. <ref type=""bibr"" target=""#b154"">[155]</ref> adapted memory network for simple question answering, taking knowledge base as external memory. Dai et al. lications also come up with many datasets, for example, Wiki-Facts <ref type=""bibr"" target=""#b202"">[203]</ref> for language modeling; SimpleQuestions <ref type=""bibr"" target=""#b154"">[155]</ref> and LC-QuAD <ref type=""bibr"" target=""#b203"">[204]</ref> for question answering; and Freebase Semantic Scho",0
"sH <ref type=""bibr"" target=""#b14"">[15]</ref> also use similar representation space, while semantic matching models use plain vector space (e.g., HolE <ref type=""bibr"" target=""#b15"">[16]</ref>) and relational projection matrix (e.g., ANALOGY <ref type=""bibr"" target=""#b16"">[17]</ref>). Principles of t :id=""formula_21"">fr(h, t) = h diag(Mr)t.<label>(15)</label></formula><p>To capture rich interactions in relational data and compute efficiently, HolE <ref type=""bibr"" target=""#b15"">[16]</ref> introduces circular correlation of embedding, which can be interpreted as compressed tensor product, to lear E <ref type=""bibr"" target=""#b11"">[12]</ref>, TransH <ref type=""bibr"" target=""#b14"">[15]</ref>, TransR <ref type=""bibr"" target=""#b12"">[13]</ref>, HolE <ref type=""bibr"" target=""#b15"">[16]</ref>, and R-GCN <ref type=""bibr"" target=""#b52"">[53]</ref>) and joint learning methods like DKRL <ref type=""bibr"" and TransR <ref type=""bibr"" target=""#b12"">[13]</ref> and semantic matching-based methods such as NTN <ref type=""bibr"" target=""#b13"">[14]</ref>, HolE <ref type=""bibr"" target=""#b15"">[16]</ref> and ANALOGY <ref type=""bibr"" target=""#b16"">[17]</ref>.</p><p>Vanilla vector-based embedding methods failed t eral embedding methods use simplification to reduce the computation cost, for example, simplifying tensor product with circular correlation operation <ref type=""bibr"" target=""#b15"">[16]</ref>. However, these methods still struggle with scaling to millions of entities and relations.</p><p>Probabilist methods to some extent. By expanding point-wise Euclidean space <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" target=""#b13"">[14]</ref>, <ref type=""bibr"" target=""#b15"">[16]</ref>, manifold space <ref type=""bibr"" target=""#b22"">[23]</ref>, complex space <ref type=""bibr"" target=""#b17"">[18] in DistMult <ref type=""bibr"" target=""#b25"">[26]</ref>, tensor product in NTN <ref type=""bibr"" target=""#b13"">[14]</ref>, circular correlation in HolE <ref type=""bibr"" target=""#b15"">[16]</ref> and ANALOGY <ref type=""bibr"" target=""#b16"">[17]</ref>, Hadamard product in CrossE <ref type=""bibr"" target=""#",0
"aces for entities and relations. The authors projected entities (h, t ∈ R k ) into relation (r ∈ R d ) space by a projection matrix M r ∈ R k×d . NTN <ref type=""bibr"" target=""#b13"">[14]</ref> models entities across multiple dimensions by a bilinear tensor neural layer. The relational interaction bet ]</ref>.</p><p>Representative neural models include multi-layer perceptron (MLP) <ref type=""bibr"" target=""#b4"">[5]</ref>, neural tensor network (NTN) <ref type=""bibr"" target=""#b13"">[14]</ref>, and neural association model (NAM) <ref type=""bibr"" target=""#b45"">[46]</ref>. Generally, they take entities t) = σ(w σ(W[h, r, t])),<label>(27)</label></formula><p>where W ∈ R n×3d is the weight matrix and [h, r, t] is a concatenation of three vectors. NTN <ref type=""bibr"" target=""#b13"">[14]</ref> takes entity embeddings as input associated with a relational tensor and outputs predictive score in as</p>< TransH <ref type=""bibr"" target=""#b14"">[15]</ref> and TransR <ref type=""bibr"" target=""#b12"">[13]</ref> and semantic matching-based methods such as NTN <ref type=""bibr"" target=""#b13"">[14]</ref>, HolE <ref type=""bibr"" target=""#b15"">[16]</ref> and ANALOGY <ref type=""bibr"" target=""#b16"">[17]</ref>.</p><p has an impact on the expressiveness of KRL methods to some extent. By expanding point-wise Euclidean space <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" target=""#b13"">[14]</ref>, <ref type=""bibr"" target=""#b15"">[16]</ref>, manifold space <ref type=""bibr"" target=""#b22"">[23]</ref>, comple ching in SME <ref type=""bibr"" target=""#b33"">[34]</ref>, bilinear mapping in DistMult <ref type=""bibr"" target=""#b25"">[26]</ref>, tensor product in NTN <ref type=""bibr"" target=""#b13"">[14]</ref>, circular correlation in HolE <ref type=""bibr"" target=""#b15"">[16]</ref> and ANALOGY <ref type=""bibr"" target= encoding models start from distributed representation of entities and relations, and some utilizes complex neural structures such as tensor networks <ref type=""bibr"" target=""#b13"">[14]</ref>, graph convolution networks <ref type=""bibr"" target=""#b37"">[38]</ref>, <ref type=""bibr"" target=""#b52"">[53]</",0
"N CPL [76] Reasoner (es, rq, ht) {e G } I {et = eq} ht = LST M (ht−1, [rt, et]) CPL</formula><p>Targeting at the previous two observations, GMatching <ref type=""bibr"" target=""#b85"">[86]</ref> develops a metric based few-shot learning method with entity embeddings and local graph structures. It encod",0
"/head><p>GCNs are utilized for encoding dependency tree over sentences or learning KGEs to leverage relational knowledge for sentence encoding. C-GCN <ref type=""bibr"" target=""#b121"">[122]</ref> is a contextualized GCN model over pruned dependency tree of sentences after path-centric pruning. AGGCN < ity descriptions HATT <ref type=""bibr"" target=""#b119"">[120]</ref> CNN/PCNN + hierarchical attention position embedding, relation hierarchy GCNs C-GCN <ref type=""bibr"" target=""#b121"">[122]</ref> LSTM + GCN + path-centric pruning dependency tree KATT <ref type=""bibr"" target=""#b123"">[124]</ref> Pre-tra",0
"ref>, a neural attention mechanism to enable multiple reasons, represents logical composition across all relations, entities and text. Recently, DIVA <ref type=""bibr"" target=""#b70"">[71]</ref> proposes a unified variational inference framework that takes multi-hop reasoning as two sub-steps of path-f",0
"resentation, (c) GCN <ref type=""bibr"" target=""#b37"">[38]</ref> acts as encoder of knowledge graphs to produce entity and relation embeddings. (d) RSN <ref type=""bibr"" target=""#b38"">[39]</ref> encodes entity-relation sequences and skips relations discriminatively.</p><formula xml:id=""formula_36"">fr ( get=""#b49"">[50]</ref> propose RNNbased model over relation path to learn vector representation without and with entity information, respectively. RSN <ref type=""bibr"" target=""#b38"">[39]</ref> (Fig. <ref type=""figure"" target=""#fig_1"">5d</ref>) designs a recurrent skip mechanism to enhance semantic re s <ref type=""bibr"" target=""#b37"">[38]</ref>, <ref type=""bibr"" target=""#b52"">[53]</ref>, <ref type=""bibr"" target=""#b54"">[55]</ref>, recurrent networks <ref type=""bibr"" target=""#b38"">[39]</ref> and transformers <ref type=""bibr"" target=""#b50"">[51]</ref>, <ref type=""bibr"" target=""#b51"">[52]</ref> to lea epresentation, (c) GCN<ref type=""bibr"" target=""#b37"">[38]</ref> acts as encoder of knowledge graphs to produce entity and relation embeddings. (d) RSN<ref type=""bibr"" target=""#b38"">[39]</ref> encodes entity-relation sequences and skips relations discriminatively.</figDesc></figure> <figure xmlns=""ht",0
"is to develop a recommender model that achieves a balance between effectiveness and efficiency. In this paper, we employ knowledge distillation (KD) <ref type=""bibr"" target=""#b9"">[10]</ref> which is a network compression technique by transferring the distilled knowledge of a large model (a.k.a., a n, we first introduce the basic notations and formulate the top-N recommendation problem. Then, we explain the concept of knowledge distillation (KD) <ref type=""bibr"" target=""#b9"">[10]</ref> and present rank distillation (RD) <ref type=""bibr"" target=""#b10"">[11]</ref> that applies knowledge distillat pling (line 4). This sampling method is used in our proposed training strategies.</p><p>Temperature in the KD loss. One key factor of the original KD <ref type=""bibr"" target=""#b9"">[10]</ref> is to find a proper balance between the soft targets and hard labels. To tackle this issue, <ref type=""bibr"" the original KD <ref type=""bibr"" target=""#b9"">[10]</ref> is to find a proper balance between the soft targets and hard labels. To tackle this issue, <ref type=""bibr"" target=""#b9"">[10]</ref> introduces the notion of a temperature T . Although the soft target is a useful resource for educating the st",1
"are of interest to top-N recommendation, we should consider the degrees of importance of items based on their rankings.</p><p>Recently, Tang and Wang <ref type=""bibr"" target=""#b10"">[11]</ref> proposed a KD model to address the ranking problem, called rank distillation (RD). RD uses only a few items tion problem. Then, we explain the concept of knowledge distillation (KD) <ref type=""bibr"" target=""#b9"">[10]</ref> and present rank distillation (RD) <ref type=""bibr"" target=""#b10"">[11]</ref> that applies knowledge distillation to recommender models. Problem statement. For a set of users U = {u 1 , blem. They no longer remain valid in the top-N recommendation problem because Fig. <ref type=""figure"">1</ref>. Illustration of rank distillation (RD) <ref type=""bibr"" target=""#b10"">[11]</ref>. The teacher model transfers manipulated top-k items as the distilled knowledge to the student model. of two nsequently, the student model using the soft target may have worse performance than the original student model. Rank distillation (RD). Tang and Wang <ref type=""bibr"" target=""#b10"">[11]</ref> proposed ranking distillation (RD) that applies KD for ranking models. As depicted in Fig. <ref type=""figure ho had less than 10 ratings and the items that were rated by less than 5 users. Table I reports the detailed statistics of these datasets.</p><p>• RD <ref type=""bibr"" target=""#b10"">[11]</ref>: To define the KD loss in equation ( <ref type=""formula"" target=""#formula_3"">4</ref>), this utilizes only th plementation. Note that there is a difference between λ that appears in RD and CD. Specifically, we used the following parameter settings.</p><p>• RD <ref type=""bibr"" target=""#b10"">[11]</ref> and RD-Rank: We set ρ to be 0.5. For CDAE, the number of items in the soft target was 15. For Caser, the num ow 1.9.0 (CDAE) and PyTorch 1.0.0 (Caser). For Caser, we used the public PyTorch implementation <ref type=""foot"" target=""#foot_5"">6</ref> provided in <ref type=""bibr"" target=""#b10"">[11]</ref>. All experiments were conducted on a desktop with 128 GB memory and 2 Intel Xeon Processor E5-2630 v4 (2.20 seline CF model with different parameters without KD. Also, the gain indicates how additional accuracy achieved by the proposed model over that of RD <ref type=""bibr"" target=""#b10"">[11]</ref>.</p><p>Based on this evaluation, we found several interesting observations. Firstly, both CD-TG and CD-SG si Firstly, both CD-TG and CD-SG significantly outperform RD over all datasets. Note that the improvement gap for RD is somewhat different from that in <ref type=""bibr"" target=""#b10"">[11]</ref>. It is because we used leave-one-out evaluation while <ref type=""bibr"" target=""#b10"">[11]</ref> used cross-v ovement gap for RD is somewhat different from that in <ref type=""bibr"" target=""#b10"">[11]</ref>. It is because we used leave-one-out evaluation while <ref type=""bibr"" target=""#b10"">[11]</ref> used cross-validation evaluation. Our models are consistently better than RD by 2.7-33.2% and 2.7-29.1% in H ig_5"">4</ref> depicts the relationship between model size and efficiency. The model size is proportional to the accuracy of our model, as observed in <ref type=""bibr"" target=""#b10"">[11]</ref> as well. The same tendency consistently holds in different CF models. In both CF models, ones of the small s <note xmlns=""http://www.tei-c.org/ns/1.0"" place=""foot"" n=""4"" xml:id=""foot_3"">http://dawenl.github.io/data/gowalla pro.zip Competitive models. Since RD<ref type=""bibr"" target=""#b10"">[11]</ref> is the state-of-the-art KD model for top-N recommendation, we compare the proposed model with the original R",1
"tive preferences. Such ambiguity has been explicitly discussed in one-class collaborative filtering (OCCF) <ref type=""bibr"" target=""#b11"">[12]</ref>- <ref type=""bibr"" target=""#b18"">[19]</ref>. Given user u, I + u = {i ∈ I|r ui = 1} is the set of items with known positive feedback, and I − u = I\I + bibr"" target=""#b17"">[18]</ref> employed multiple similarity matrices between users and items to predict drug-target interaction. Moreover, Yao et al. <ref type=""bibr"" target=""#b18"">[19]</ref> proposed dual regularization by combining the weighted-and imputation-based methods.</p><p>The proposed mode",1
"odel, respectively. Note that this setting is consistent with existing KD studies.</p><p>Evaluation protocol. We adopted the leave-one-out evaluation <ref type=""bibr"" target=""#b3"">[4]</ref>- <ref type=""bibr"" target=""#b5"">[6]</ref>. Specifically, we held-out the last timestamp useritem interaction as ritem interaction as the test data for each user, and the rest of user-item interactions are used for training data. Unlike sampling-based evaluation <ref type=""bibr"" target=""#b3"">[4]</ref>- <ref type=""bibr"" target=""#b5"">[6]</ref> that randomly chose 100 items from the set of unrated items, we chose red the accuracy of top-N recommendation for two metrics, hit rate (HR) and normalized discounted cumulative gain (NDCG), as done in existing studies <ref type=""bibr"" target=""#b3"">[4]</ref>- <ref type=""bibr"" target=""#b5"">[6]</ref>. The size N of the ranked list was chosen to be 50 for HR@N and NDCG@",0
"rget=""#b14"">[15]</ref> proposed a sampling-based method by considering the degree distributions of users/items in the graph. Lastly, Sindhwani et al. <ref type=""bibr"" target=""#b12"">[13]</ref> regarded unobserved feedback as optimization variables and imputed missing feedback via optimization. Beside",0
"eDesc> 	</teiHeader> 	<text xml:lang=""en""> 		<body> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>I. INTRODUCTION</head><p>Neural recommender models <ref type=""bibr"" target=""#b0"">[1]</ref>- <ref type=""bibr"" target=""#b8"">[9]</ref> have achieved better performance than conventional latent factor mode",0
"it can reduce the memory size and enhance efficiency. Second, the pruning and sharing method presented in <ref type=""bibr"" target=""#b31"">[32]</ref>, <ref type=""bibr"" target=""#b32"">[33]</ref> removes or binds model parameters which are redundant or have minimal impacts in loss functions. In principl",0
"nowledge transfer framework designed to deliver the knowledge extracted from a complex teacher model to a simple student model. Many existing studies <ref type=""bibr"" target=""#b19"">[20]</ref>- <ref type=""bibr"" target=""#b27"">[28]</ref> have utilized KD to compress deep neural networks as well as to a <ref type=""bibr"" target=""#b33"">[34]</ref> employed model parameters of the teacher model directly to initialize those of the student model. Recently, <ref type=""bibr"" target=""#b19"">[20]</ref> used the attention map as an additional matching constraint. That is, in addition to the original loss term",0
"mization. Besides, Li et al. <ref type=""bibr"" target=""#b16"">[17]</ref> leveraged side information to construct user-item similarity, and Zheng et al. <ref type=""bibr"" target=""#b17"">[18]</ref> employed multiple similarity matrices between users and items to predict drug-target interaction. Moreover,",0
""">[26]</ref> utilized the gram matrix of the channel responses from the teacher model as additional information to educate the student model. Net2Net <ref type=""bibr"" target=""#b33"">[34]</ref> employed model parameters of the teacher model directly to initialize those of the student model. Recently,",0
"as the confidence of negative values with various schemes, such as uniform, user-oriented, and item-oriented methods. Second, Paquet and Koenigstein <ref type=""bibr"" target=""#b14"">[15]</ref> proposed a sampling-based method by considering the degree distributions of users/items in the graph. Lastly",0
"This is our proposed model using student-guided model training. To validate the proposed model, we chose two state-ofthe-art recommender models-CDAE <ref type=""bibr"" target=""#b7"">[8]</ref> and Caser <ref type=""bibr"" target=""#b6"">[7]</ref>. (This paper focuses on top-N recommender models with point- itialized model parameters using Gaussian distribution N (0, 1). Specifically, each baseline CF model had the following hyperparameters.</p><p>• CDAE <ref type=""bibr"" target=""#b7"">[8]</ref>: The latent dimensions for the teacher and the student model were 100 and 10, respectively. We set the number",0
"iver the knowledge extracted from a complex teacher model to a simple student model. Many existing studies <ref type=""bibr"" target=""#b19"">[20]</ref>- <ref type=""bibr"" target=""#b27"">[28]</ref> have utilized KD to compress deep neural networks as well as to achieve stable performances by distilling th rk. Recently, KDGAN <ref type=""bibr"" target=""#b20"">[21]</ref> bypassed the convergence step of adversarial learning by employing a triple-player game <ref type=""bibr"" target=""#b27"">[28]</ref>. In this study, we develop an improved loss function for KD. Unlike existing models, we mainly focus on modi",0
"plying a mixture of unlabeled positive/negative preferences. Such ambiguity has been explicitly discussed in one-class collaborative filtering (OCCF) <ref type=""bibr"" target=""#b11"">[12]</ref>- <ref type=""bibr"" target=""#b18"">[19]</ref>. Given user u, I + u = {i ∈ I|r ui = 1} is the set of items with ss this challenge, existing studies can be categorized into weight-based, sampling-based, and imputationbased methods. First, the weight-based method <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" target=""#b15"">[16]</ref> regards all missing feedback as negative ones. For instance, Hu e ype=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" target=""#b15"">[16]</ref> regards all missing feedback as negative ones. For instance, Hu et al. <ref type=""bibr"" target=""#b11"">[12]</ref> and Pan et al. <ref type=""bibr"" target=""#b15"">[16]</ref> controlled weights as the confidence of negative va",0
"effectiveness and efficiency of computational models is an fundamental issue for real-world applications. To address this problem, various techniques <ref type=""bibr"" target=""#b28"">[29]</ref> have been widely developed to compress cumbersome models into smaller ones. In general, existing work falls",0
"to the original loss term for matching the soft target, the attention map of the student model should match that of the teacher model. Most recently, <ref type=""bibr"" target=""#b23"">[24]</ref> further improved the attentionbased method by matching the gradients (i.e., Jacobians) of output activations",0
"o recommender models. More concretely, applying KD to recommender models involves several challenges: (1) Implicit user feedback is extremely sparse. <ref type=""bibr"" target=""#b1"">(2)</ref> As users only provide positive feedback in implicit datasets, there is inherent ambiguity regarding unknown (o",0
"odel training. To validate the proposed model, we chose two state-ofthe-art recommender models-CDAE <ref type=""bibr"" target=""#b7"">[8]</ref> and Caser <ref type=""bibr"" target=""#b6"">[7]</ref>. (This paper focuses on top-N recommender models with point-wise preferences. We leave the evaluation for othe *|I u | and the denoising ratio as 0.1. We used the Adagrad optimizer with learning rate = 0.2, l2-regularizer = 0.001, and batch size = 256. • Caser <ref type=""bibr"" target=""#b6"">[7]</ref>: The latent dimensions for the teacher and the student model were 50 and 5, respectively. We set sequence leng",0
"output activations for the input.</p><p>Along an alternative direction, several algorithms focused on analyzing the choice of loss functions for KD. <ref type=""bibr"" target=""#b26"">[27]</ref> observed that the distance-based loss is inappropriate for transferring activation boundaries, and thus sugg",0
"ef type=""bibr"" target=""#b35"">[36,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b14"">15]</ref>, large Transformer based models <ref type=""bibr"" target=""#b31"">[32]</ref>, such as BERT <ref type=""bibr"" target=""#b5"">[6]</ref>, show substantially better effectiveness at the cost o s.hofstaetter@tuwien.ac.at 2 TU Wien, Austria, email: markus.zlabinger@tuwien.ac.at 3 TU Wien, Austria, email: hanbury@ifs.tuwien.ac.at former layers <ref type=""bibr"" target=""#b31"">[32]</ref> (we evaluate up to three) can effectively contextualize query and document word embeddings. TK's second cont eddings, enabling word-level n-gram representation learning -a local contextualization, fixed by the n-gram size hyperparameter.</p><p>Vaswani et al. <ref type=""bibr"" target=""#b31"">[32]</ref> proposed the Transformer architecture in the context of language translation. Their encoder-decoder is built α parameter. This allows the model to decide the intensity of the contextualization. We calculate the context(t1:n) with a set of Transformer layers <ref type=""bibr"" target=""#b31"">[32]</ref>. First, the input sequence is fused with a positional encoding to form p1:n, followed by a set of l Transfor",1
"and a distinct trade-off emerged between a neural re-ranking model's effectiveness and its efficiency. While IR-specific networks are reasonably fast <ref type=""bibr"" target=""#b35"">[36,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b14"">15]</ref>, large Transformer based model each other and distill the interactions between terms in a single interaction match matrix, followed by softhistogram scoring based on kernel-pooling <ref type=""bibr"" target=""#b35"">[36]</ref>. This allows us to explain scoring reasons by probing the model at the point of the information bottleneck t However, it suffered from the non-differentability of a hard histogram method and the resulting lack of fine-tuned word representations. Xiong et al. <ref type=""bibr"" target=""#b35"">[36]</ref> improve on the idea and propose the kernel-pooling technique as part of the KNRM model. Conceptually, it app ractor:</p><formula xml:id=""formula_3"">Mi,j = cos( qi, dj)<label>(4)</label></formula><p>Then, we transform each entry in M with a set of RBF-kernels <ref type=""bibr"" target=""#b35"">[36]</ref>. Each kernel focuses on a specific similarity range with center µ k . The size of all ranges is set by σ. In b35"">[36]</ref>. Each kernel focuses on a specific similarity range with center µ k . The size of all ranges is set by σ. In contrast to Xiong et al. <ref type=""bibr"" target=""#b35"">[36]</ref> we do not employ an exact match kernel -as contextualized representations do not produce exact matches. Each layer and each attention head, making a focused analysis unfeasible.</p><p>The differences of TK to previous kernel-pooling methods are:</p><p>• KNRM <ref type=""bibr"" target=""#b35"">[36]</ref> uses only word embeddings, therefore a match does not have context or positional information. • CONV-KNRM <r s (via fixed window neighborhood mean vectors) and improves the robustness of PACRR's pooling strategy with randomization during training.</p><p>KNRM <ref type=""bibr"" target=""#b35"">[36]</ref> uses a soft-histogram (differentiable Gaussian kernel functions) on top of the interaction matrix of query a",1
"#b8"">[9]</ref>.</p><p>Contextualization allows neural IR models to vary the importance of otherwise identical term matches. The neural CO-PACRR model <ref type=""bibr"" target=""#b15"">[16]</ref> provides a lightweight contextualization. It averages word vectors with a sliding window and appends their s This makes TK more powerful than previous local-only contextualization methods used in CONV-KNRM <ref type=""bibr"" target=""#b4"">[5]</ref> and CO-PACRR <ref type=""bibr"" target=""#b15"">[16]</ref>.  </p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.2"">Interaction Scoring</head><p>After the c Ns on the match matrix followed by a max pooling. In contrast to MatchPyramid, the single CNN layer focuses on different n-gram sizes.</p><p>CO-PACRR <ref type=""bibr"" target=""#b15"">[16]</ref> extends the PACRR model with additional contextualized similarities (via fixed window neighborhood mean vect",0
"lyze contextualized term representations and interaction patterns.</p><p>We conduct experiments on three large retrieval collections: MSMARCO-Passage <ref type=""bibr"" target=""#b1"">[2]</ref>, MSMARCO-Document <ref type=""bibr"" target=""#b1"">[2]</ref>, and TREC CAR 2017 <ref type=""bibr"" target=""#b6"">[7] atterns.</p><p>We conduct experiments on three large retrieval collections: MSMARCO-Passage <ref type=""bibr"" target=""#b1"">[2]</ref>, MSMARCO-Document <ref type=""bibr"" target=""#b1"">[2]</ref>, and TREC CAR 2017 <ref type=""bibr"" target=""#b6"">[7]</ref>. We evaluate a broad range of traditional and neura tatistics are shown in Table <ref type=""table"" target=""#tab_0"">1</ref> and in the following we describe the collections in more detail:</p><p>MSMARCO <ref type=""bibr"" target=""#b1"">[2]</ref> collections are based on real Bing queries and results. We use both the Passage and the Document version with",0
"fectiveness at the cost of orders of magnitude longer inference time <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" target=""#b24"">25]</ref>. Given the same amount of limited time, a faster reranking model can incorporate more documents than a less e models can then be fine-tuned for various tasks, including pairwise sequence classification. Nogueira et al. <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b24"">25]</ref> first showed the applicability of BERT for re-ranking and the resulting substantial effectiveness gains. MacA",0
"ds. One way to make neural IR models faster at query time is to offload computation to the indexing phase, either by assuming query term independence <ref type=""bibr"" target=""#b22"">[23]</ref> or by approximating interaction similarities <ref type=""bibr"" target=""#b16"">[17]</ref>. When a large number",0
"e match-matrix of term similarities to the matching score: using stacked Convolutional Neural Networks (CNN) <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b21"">22]</ref>, parallel single-layered CNNs for n-gram interaction modelling <ref type=""bibr"" target=""#b14"">[15]</ref>, rec CNN layers with max-pooling on top of a term-by-term interaction matrix. The pooling sizes become smaller with each layer -like a pyramid.</p><p>DUET <ref type=""bibr"" target=""#b21"">[22]</ref> is a hybrid model which applies CNNs to local termby-term interactions and it learns a single representation",0
"the indexing phase, either by assuming query term independence <ref type=""bibr"" target=""#b22"">[23]</ref> or by approximating interaction similarities <ref type=""bibr"" target=""#b16"">[17]</ref>. When a large number of pre-trained Transformer layers is involved, inference can be sped up by removing lat",0
"has been thoroughly studied <ref type=""bibr"" target=""#b33"">[34,</ref><ref type=""bibr"" target=""#b34"">35,</ref><ref type=""bibr"" target=""#b36"">37,</ref><ref type=""bibr"" target=""#b3"">4]</ref>. This includes applying a temporal constraint on the number of features that are selected for a re-ranking mode ng of linear rankers <ref type=""bibr"" target=""#b33"">[34]</ref>, and comparing the effectiveness and efficiency of various learning-to-rank algorithms <ref type=""bibr"" target=""#b3"">[4]</ref>. In web search the speed of a response is crucial as determined by Kohavi et al. <ref type=""bibr"" target=""#b18",0
"uch as BERT <ref type=""bibr"" target=""#b5"">[6]</ref>, show substantially better effectiveness at the cost of orders of magnitude longer inference time <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" target=""#b24"">25]</ref>. Given the same amount of li dent <ref type=""bibr"" target=""#b30"">[31]</ref>.</p><p>Recently, the issue of efficiency gained traction in the neural IR community. Hofstätter et al. <ref type=""bibr"" target=""#b11"">[12]</ref> establish efficiency baselines for common neural IR models (including BERT) and propose to incorporate speed",0
"ions to achieve the prediction, remains to be a fundamental task towards effective personalized recommendation <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b38"">39]</ref>.</p><p>The most common paradi paradigm for CF is to learn latent features (a.k.a. embedding) to represent a user and an item, and perform prediction based on the embedding vectors <ref type=""bibr"" target=""#b17"">[18]</ref>. Matrix factorization is an early such model, which directly projects the single ID of a user to her embeddi based models GC-MC <ref type=""bibr"" target=""#b32"">[33]</ref> and PinSage <ref type=""bibr"" target=""#b45"">[46]</ref>, neural network-based models NeuMF <ref type=""bibr"" target=""#b17"">[18]</ref> and CMN <ref type=""bibr"" target=""#b7"">[8]</ref>, and factorization-based models MF <ref type=""bibr"" target="" ref type=""bibr"" target=""#b29"">30]</ref> project the ID of a user (or an item) into an embedding vector. The recent neural recommender models like NCF <ref type=""bibr"" target=""#b17"">[18]</ref> and LRML <ref type=""bibr"" target=""#b31"">[32]</ref> use the same embedding component, while enhance the inter",1
"ve personalized recommendation <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b38"">39]</ref>.</p><p>The most common paradigm for CF is to learn latent features (a.k.a. embedding) to represent a user and graph convolution operations <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b33"">34,</ref><ref type=""bibr"" target=""#b38"">39]</ref> that typically aggregate extended neighbors and need to handle the self-connection specially. The layer combi fically, her one-hop neighbors -to improve the embedding learning.</p><p>To deepen the use of subgraph structure with high-hop neighbors, Wang et al. <ref type=""bibr"" target=""#b38"">[39]</ref> recently proposes NGCF and achieves state-of-the-art performance for CF. It takes inspiration from the Graph ectiveness of collaborative filtering. </p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2"">PRELIMINARIES</head><p>We first introduce NGCF <ref type=""bibr"" target=""#b38"">[39]</ref>, a representative and state-of-the-art GCN model for recommendation. We then perform ablation studies on NGC e <ref type=""table"" target=""#tab_1"">1</ref>, where the scores of NGCF are directly copied from the Table <ref type=""table"" target=""#tab_4"">3</ref> of <ref type=""bibr"" target=""#b38"">[39]</ref>. As can be seen, removing feature transformation (i.e., NGCF-f) leads to consistent improvements over NGCF o h layer to obtain the final representations.</p><p>transformation and nonlinear activation. The graph convolution operation (a.k.a., propagation rule <ref type=""bibr"" target=""#b38"">[39]</ref>) in LightGCN is defined as:</p><formula xml:id=""formula_5"">e (k +1) u = i ∈N u 1 |N u | |N i | e (k ) i , e interactions, the second layer smooths users (items) that have overlap on interacted items (users), and higher-layers capture higher-order proximity <ref type=""bibr"" target=""#b38"">[39]</ref>. Thus combining them will make the representation more comprehensive. ( <ref type=""formula"" target=""#formula http://www.tei-c.org/ns/1.0""><head n=""4"">EXPERIMENTS</head><p>We first describe experimental settings, and then conduct detailed comparison with NGCF <ref type=""bibr"" target=""#b38"">[39]</ref>, the method that is most relevant with LightGCN but more complicated (Section 4.2). We next compare with oth =""4.1"">Experimental Settings</head><p>To reduce the experiment workload and keep the comparison fair, we closely follow the settings of the NGCF work <ref type=""bibr"" target=""#b38"">[39]</ref>. We request the experimented datasets (including train/test splits) from the authors, for which the statisti nce, but the benefits diminish. The general observation is that increasing the layer number from 0 (i.e., the matrix factorization model, results see <ref type=""bibr"" target=""#b38"">[39]</ref>) to 1 leads to the largest performance gain, and  using a layer number of 3 leads to satisfactory performanc type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b26"">27]</ref>. Motivated by the strength of graph convolution, recent efforts like NGCF <ref type=""bibr"" target=""#b38"">[39]</ref>, GC-MC <ref type=""bibr"" target=""#b32"">[33]</ref>, and PinSage <ref type=""bibr"" target=""#b45"">[46]</ref> adap formance comparison between NGCF and LightGCN at different layers. The scores of NGCF on Gowalla and Amazon-Book are directly copied from the Table3of<ref type=""bibr"" target=""#b38"">[39]</ref>; the scores of NGCF on Yelp2018 are re-run by us.</figDesc><table><row><cell cols=""2"">Dataset</cell><cell></",1
"gies which might improve the LightGCN training, such as the hard negative sampling <ref type=""bibr"" target=""#b28"">[29]</ref> and adversarial sampling <ref type=""bibr"" target=""#b6"">[7]</ref>. We leave this extension in the future since it is not the focus of this work. Note that we do not introduce d",0
"is exploiting the user-item graph structure for recommendation. Prior efforts, such as ItemRank <ref type=""bibr"" target=""#b10"">[11]</ref> and BiRank <ref type=""bibr"" target=""#b15"">[16]</ref>, use the label propagation mechanism to directly propagate user preference scores over the graph, i.e., enco",0
"larger; and 3) the activity of v, the less active the larger. Such interpretability well caters for the assumption of CF in measuring user similarity <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b35"">36]</ref> and evidences the reasonability of LightGCN. Due to the symmetric form",0
"to be a fundamental task towards effective personalized recommendation <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b38"">39]</ref>.</p><p>The most common paradigm for CF is to learn latent features (a elegant, which not only is easier to train, but also achieves better empirical performance than NGCF and other state-of-the-art methods like Mult-VAE <ref type=""bibr"" target=""#b25"">[26]</ref>.</p><p>To summarize, this work makes the following main contributions:</p><p>• We empirically show that two tocol, we do not further compare with these methods. In addition to NGCF, we further compare with two relevant and competitive CF methods: • Mult-VAE <ref type=""bibr"" target=""#b25"">[26]</ref>. This is an item-based CF method based on the variational autoencoder (VAE). It assumes the data is generate",0
"f type=""bibr"" target=""#b17"">[18]</ref>. Matrix factorization is an early such model, which directly projects the single ID of a user to her embedding <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b29"">30]</ref>. Later on, several research find that augmenting user ID with the he and learn the embedding parameters by reconstructing historical useritem interactions. For example, earlier CF models like matrix factorization (MF) <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b29"">30]</ref> project the ID of a user (or an item) into an embedding vector. The",0
"several research find that augmenting user ID with the her interaction history as the input can improve the quality of embedding. For example, SVD++ <ref type=""bibr"" target=""#b22"">[23]</ref> demonstrates the benefits of user interaction history in predicting user numerical ratings, and Neural Atten ms as the pre-existing features of a user, towards better user representations. For example, FISM <ref type=""bibr"" target=""#b18"">[19]</ref> and SVD++ <ref type=""bibr"" target=""#b22"">[23]</ref> use the weighted average of the ID embeddings of historical items as the target user's embedding. Recently,",0
"2"">[23]</ref> demonstrates the benefits of user interaction history in predicting user numerical ratings, and Neural Attentive Item Similarity (NAIS) <ref type=""bibr"" target=""#b16"">[17]</ref> differentiates the importance of items in the interaction history and shows improvements in predicting item Towards this end, attention mechanisms are introduced to capture the varying contributions, such as ACF <ref type=""bibr"" target=""#b2"">[3]</ref>, NAIS <ref type=""bibr"" target=""#b16"">[17]</ref>, and DeepICF <ref type=""bibr"" target=""#b42"">[43]</ref>, to automatically learn the importance of each histor",0
"ing the relations among entities in the predictive model, they are advantageous to traditional supervised learning scheme like factorization machines <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b30"">31]</ref> that model the relations implicitly. For example, a recent trend is",0
"ed propensity.</p><p>While constructing the propensity model typically involves perturbing the search results to collect examples of counterfactuals, <ref type=""bibr"" target=""#b0"">[1]</ref> describes methods to construct the propensity model without additional interventions.</p></div> <div xmlns=""ht",1
"urt the relevance prediction.</p><p>To reduce the dependence of the relevance prediction on the position feature, we regularize it down using dropout <ref type=""bibr"" target=""#b12"">[13]</ref>. During training, we probabilistically set the position for a listing to 0, controlled by the dropout rate.<",0
"point are new or are using the product a er a long gap of time. For all practical purposes users are in a state of continuous cold start as noted in <ref type=""bibr"" target=""#b1"">[2]</ref>. Handling user level cold start is part of the core ranking formulation itself. So when referring to the cold",0
"son of two listing scores independent of positional bias and dependent only on listing relevance. In essence, we added position as a control variable <ref type=""bibr"" target=""#b17"">[18]</ref> in the ranking model. </p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.3"">Position Dropout</he",0
"nstead of discriminating purely based o listing features. is inspired the next revision of the architecture which consisted of two towers, similar to <ref type=""bibr"" target=""#b9"">[10]</ref>.</p><p>e rst tower, fed by the query and user features, generated a 100-d vector which conceptually represent",0
"b19"">(Hadsell et al., 2006;</ref><ref type=""bibr"" target=""#b14"">Dosovitskiy et al., 2014;</ref><ref type=""bibr"" target=""#b41"">Oord et al., 2018;</ref><ref type=""bibr"" target=""#b1"">Bachman et al., 2019)</ref>.</p><p>In this work, we introduce a simple framework for contrastive learning of visual repr rvised representation learning <ref type=""bibr"" target=""#b33"">(Krizhevsky et al., 2012;</ref><ref type=""bibr"" target=""#b22"">Hénaff et al., 2019;</ref><ref type=""bibr"" target=""#b1"">Bachman et al., 2019)</ref>, it has not been considered as a systematic way to define the contrastive prediction task. M rget=""#b41"">(Oord et al., 2018;</ref><ref type=""bibr"" target=""#b22"">Hénaff et al., 2019;</ref><ref type=""bibr"" target=""#b24"">Hjelm et al., 2018;</ref><ref type=""bibr"" target=""#b1"">Bachman et al., 2019)</ref>. However, it is not clear if the success of contrastive approaches is determined by the mutu thod to the recently proposed contrastive representation learning methods:</p><p>• DIM/AMDIM <ref type=""bibr"" target=""#b24"">(Hjelm et al., 2018;</ref><ref type=""bibr"" target=""#b1"">Bachman et al., 2019)</ref> achieve global-to-local/local-to-neighbor prediction by predicting the middle layer of ConvN CLR outperform previous work (Figure <ref type=""figure"" target=""#fig_0"">1</ref>), but it is also simpler, requiring neither specialized architectures <ref type=""bibr"" target=""#b1"">(Bachman et al., 2019;</ref><ref type=""bibr"" target=""#b22"">Hénaff et al., 2019)</ref> nor a memory bank <ref type=""bibr"" idely used linear evaluation protocol <ref type=""bibr"" target=""#b20"">(Zhang et al., 2016;</ref><ref type=""bibr"" target=""#b41"">Oord et al., 2018;</ref><ref type=""bibr"" target=""#b1"">Bachman et al., 2019;</ref><ref type=""bibr"" target=""#b29"">Kolesnikov et al., 2019)</ref>, where a linear classifier is t h previous work has reported that data augmentation is useful for self-supervised learning <ref type=""bibr"" target=""#b11"">(Doersch et al., 2015;</ref><ref type=""bibr"" target=""#b1"">Bachman et al., 2019;</ref><ref type=""bibr"" target=""#b22"">Hénaff et al., 2019;</ref><ref type=""bibr"" target=""#b0"">Asano r"" target=""#b51"">(Wu et al., 2018)</ref>; and (3) the default nonlinear projection with one additional hidden layer (and ReLU activation), similar to <ref type=""bibr"" target=""#b1"">Bachman et al. (2019)</ref>. We observe that a nonlinear projection is better than a linear projection (+3%), and much b rvised baseline using the same architecture and batch size. The best self-supervised model that reports linear evaluation result on CIFAR-10 is AMDIM <ref type=""bibr"" target=""#b1"">(Bachman et al., 2019)</ref>, which achieves 91.2% with a model 25× larger than ours. We note that our model can be impr",1
"ving state-of-theart results <ref type=""bibr"" target=""#b19"">(Hadsell et al., 2006;</ref><ref type=""bibr"" target=""#b14"">Dosovitskiy et al., 2014;</ref><ref type=""bibr"" target=""#b41"">Oord et al., 2018;</ref><ref type=""bibr"" target=""#b1"">Bachman et al., 2019)</ref>.</p><p>In this work, we introduce a s o evaluate the learned representations, we follow the widely used linear evaluation protocol <ref type=""bibr"" target=""#b20"">(Zhang et al., 2016;</ref><ref type=""bibr"" target=""#b41"">Oord et al., 2018;</ref><ref type=""bibr"" target=""#b1"">Bachman et al., 2019;</ref><ref type=""bibr"" target=""#b29"">Kolesni batch. This loss has been used in previous work <ref type=""bibr"" target=""#b46"">(Sohn, 2016;</ref><ref type=""bibr"" target=""#b51"">Wu et al., 2018;</ref><ref type=""bibr"" target=""#b41"">Oord et al., 2018)</ref>; for convenience, we term it NT-Xent (the normalized temperature-scaled cross entropy loss).</ f>.</p><p>Recent literature has attempted to relate the success of their methods to maximization of mutual information between latent representations <ref type=""bibr"" target=""#b41"">(Oord et al., 2018;</ref><ref type=""bibr"" target=""#b22"">Hénaff et al., 2019;</ref><ref type=""bibr"" target=""#b24"">Hjelm function with regularization. We use a simpler data augmentation policy, while they use FastAutoAugment for their best result.</p><p>• CPC v1 and v2 <ref type=""bibr"" target=""#b41"">(Oord et al., 2018;</ref><ref type=""bibr"" target=""#b22"">Hénaff et al., 2019)</ref> define the context prediction task u",1
""">(Doersch et al., 2015;</ref><ref type=""bibr"" target=""#b20"">Zhang et al., 2016;</ref><ref type=""bibr"" target=""#b40"">Noroozi &amp; Favaro, 2016;</ref><ref type=""bibr"" target=""#b16"">Gidaris et al., 2018)</ref>, which could limit the generality of the learned representations. Discriminative approaches here. One type of augmentation involves spatial/geometric transformation of data, such as cropping and resizing (with horizontal flipping), rotation <ref type=""bibr"" target=""#b16"">(Gidaris et al., 2018)</ref> and cutout <ref type=""bibr"">(De-Vries &amp; Taylor, 2017)</ref>. The other type of augment s <ref type=""bibr"" target=""#b40"">(Noroozi &amp; Favaro, 2016</ref><ref type=""bibr"">), colorization (Zhang et al., 2016)</ref> and rotation prediction <ref type=""bibr"" target=""#b16"">(Gidaris et al., 2018)</ref>. Although good results can be obtained with bigger networks and longer training <ref type=",0
"ntation, network architecture and contrastive losses. A similar consistency idea has been explored in other contexts such as semi-supervised learning <ref type=""bibr"" target=""#b53"">(Xie et al., 2019;</ref><ref type=""bibr"" target=""#b4"">Berthelot et al., 2019)</ref>.</p><p>Handcrafted pretext tasks. T",0
"r type of augmentation involves appearance transformation, such as color distortion (including color dropping, brightness, contrast, saturation, hue) <ref type=""bibr"" target=""#b25"">(Howard, 2013;</ref><ref type=""bibr"" target=""#b48"">Szegedy et al., 2015)</ref>, Gaussian blur, and Sobel filtering. Fig",0
"ches fall into one of two classes: generative or discriminative. Generative approaches learn to generate or otherwise model pixels in the input space <ref type=""bibr"" target=""#b23"">(Hinton et al., 2006;</ref><ref type=""bibr"" target=""#b28"">Kingma &amp; Welling, 2013;</ref><ref type=""bibr"" target=""#b1",0
"rget=""#fig_0"">1</ref>), but it is also simpler, requiring neither specialized architectures <ref type=""bibr"" target=""#b1"">(Bachman et al., 2019;</ref><ref type=""bibr"" target=""#b22"">Hénaff et al., 2019)</ref> nor a memory bank <ref type=""bibr"" target=""#b51"">(Wu et al., 2018;</ref><ref type=""bibr"" tar mentation policy, while they use FastAutoAugment for their best result.</p><p>• CPC v1 and v2 <ref type=""bibr"" target=""#b41"">(Oord et al., 2018;</ref><ref type=""bibr"" target=""#b22"">Hénaff et al., 2019)</ref> define the context prediction task using a deterministic strategy to split examples into pat </ref>. Under the linear evaluation protocol, SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art <ref type=""bibr"" target=""#b22"">(Hénaff et al., 2019)</ref>. When fine-tuned with only 1% of the ImageNet labels, SimCLR achieves 85.8% top-5 accuracy, (Hénaff et al., 2019)</ref>. When fine-tuned with only 1% of the ImageNet labels, SimCLR achieves 85.8% top-5 accuracy, a relative improvement of 10% <ref type=""bibr"" target=""#b22"">(Hénaff et al., 2019)</ref>. When fine-tuned on other natural image classification datasets, SimCLR performs on par wit he training. Other approaches include shuffling data examples <ref type=""bibr"" target=""#b21"">(He et al., 2019)</ref>, or replacing BN with layer norm <ref type=""bibr"" target=""#b22"">(Hénaff et al., 2019)</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2.3."">Evaluation Protocol</hea tation has been widely used in both supervised and unsupervised representation learning <ref type=""bibr"" target=""#b33"">(Krizhevsky et al., 2012;</ref><ref type=""bibr"" target=""#b22"">Hénaff et al., 2019;</ref><ref type=""bibr"" target=""#b1"">Bachman et al., 2019)</ref>, it has not been considered as a sy eful for self-supervised learning <ref type=""bibr"" target=""#b11"">(Doersch et al., 2015;</ref><ref type=""bibr"" target=""#b1"">Bachman et al., 2019;</ref><ref type=""bibr"" target=""#b22"">Hénaff et al., 2019;</ref><ref type=""bibr"" target=""#b0"">Asano et al., 2019)</ref>, we show that data augmentation that success of their methods to maximization of mutual information between latent representations <ref type=""bibr"" target=""#b41"">(Oord et al., 2018;</ref><ref type=""bibr"" target=""#b22"">Hénaff et al., 2019;</ref><ref type=""bibr"" target=""#b24"">Hjelm et al., 2018;</ref><ref type=""bibr"" target=""#b1"">Bachman",0
"l pixels in the input space <ref type=""bibr"" target=""#b23"">(Hinton et al., 2006;</ref><ref type=""bibr"" target=""#b28"">Kingma &amp; Welling, 2013;</ref><ref type=""bibr"" target=""#b17"">Goodfellow et al., 2014)</ref>. However, pixel-level generation is computationally expensive and may not be necessary f",0
"queous solution. However, emerging models in retrosynthesis and physicochemical property prediction may overcome these limitations in the near future <ref type=""bibr"" target=""#b5"">(Coley et al., 2018;</ref><ref type=""bibr"" target=""#b16"">Gao et al., 2018)</ref>.</p><p>Where our deep neural network mo",1
"called unweighted pair group method with arithmetic mean (UPGMA), which is the most popular and preferred algorithm for hierarchical data clustering <ref type=""bibr"" target=""#b21"">(Jaskowiak et al., 2014;</ref><ref type=""bibr"" target=""#b30"">Loewenstein et al., 2008)</ref>. UPGMA uses the mean simil",0
"hts are trained and their predictions are averaged. We used an ensemble of 20 models, with each model trained on a different random split of the data <ref type=""bibr"" target=""#b11"">(Dietterich, 2000)</ref>.</p><p>Our initial training dataset consisted of 2,335 molecules, with 120 compounds (5.14%) s",0
"i as one of the highest priority pathogens against which new antibiotics are urgently required <ref type=""bibr"" target=""#b27"">(Lee et al., 2017;</ref><ref type=""bibr"" target=""#b40"">Perez et al., 2007)</ref>. In addition to halicin, from a distinct set of 23 empirically tested predictions from &gt;10",0
"tion to discover and develop new antibiotics, it is projected that deaths attributable to resistant infections will reach 10 million per year by 2050 <ref type=""bibr"" target=""#b37"">(O'Neill, 2014)</ref>. Historically, antibiotics were discovered largely through screening soil-dwelling microbes for s",0
"r time scales than those on which it was trained. While previous learning simulation approaches <ref type=""bibr"" target=""#b17"">(Li et al., 2018;</ref><ref type=""bibr"" target=""#b33"">Ummenhofer et al., 2020)</ref> have been highly specialized for particular tasks, we found our single GNS model perform /ref>.</p><p>We also created WATER-3D, a high-resolution 3D water scenario with randomized water position, initial velocity and volume, comparable to <ref type=""bibr"" target=""#b33"">Ummenhofer et al. (2020)</ref>'s containers of water. We used SPlisHSPlasH <ref type=""bibr"" target=""#b3"">(Bender &amp; local kernel, different sub-networks for fluid and boundary particles, a loss function that weights slow particles with few neighbors more heavily). <ref type=""bibr"" target=""#b33"">Ummenhofer et al. (2020)</ref> reported CConv outperformed DPI, so we quantitatively compared our GNS model to CConv. W fferences in training and test input distributions is to, during training, provide the model with its own predictions by rolling out short sequences. <ref type=""bibr"" target=""#b33"">Ummenhofer et al. (2020)</ref>, for example, train with two-step predictions. However computing additional model predic to how graph convolutional networks (GCN) <ref type=""bibr"" target=""#b14"">(Kipf &amp; Welling, 2016)</ref> work. The full CConv update as described in <ref type=""bibr"" target=""#b33"">Ummenhofer et al. (2020)</ref> is,</p><formula xml:id=""formula_19"">f i = 1 ψ(xi)</formula><p>j∈N (xi,R) a (x j , x i ) lns=""http://www.tei-c.org/ns/1.0""><head>Performance comparisons.</head><p>We implemented the CConv model, loss and training procedure as described by <ref type=""bibr"" target=""#b33"">Ummenhofer et al. (2020)</ref>. For simplicity, we only tested the CConv model on datasets with flat walls, rather than Also, for environments with multiple materials, we appended a particle type learned embedding to the input node features.</p><p>To be consistent with <ref type=""bibr"" target=""#b33"">Ummenhofer et al. (2020)</ref>, we used their batch size of 16, learning rate decay of 10 −3 to 10 −5 for 50k iteration </p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>D. Supplementary baseline comparisons D.1. Continuous convolution (CConv)</head><p>Recently <ref type=""bibr"" target=""#b33"">Ummenhofer et al. (2020)</ref> presented Continuous Convolution (CConv) as a method for particle-based fluid simulation ramework, and compare CConv to our approach on several tasks.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>Interpretation.</head><p>While <ref type=""bibr"" target=""#b33"">Ummenhofer et al. (2020)</ref> state that ""Unlike previous approaches, we do not build an explicit graph structure to c",1
"through fast batch parallelism (each element of a batch on a separate TPU) and model parallelism (a single training distributed over multiple chips) <ref type=""bibr"" target=""#b15"">(Kumar et al., 2019)</ref>. Furthermore, since TPUs require fixed size tensors, instead of just padding each device's t",0
"a the edges. GNs and their variants, e.g., ""interaction networks"", can learn to simulate rigid body, mass-spring, n-body, and robotic control systems <ref type=""bibr"" target=""#b1"">(Battaglia et al., 2016;</ref><ref type=""bibr"" target=""#b4"">Chang et al., 2016;</ref><ref type=""bibr"" target=""#b24"">Sanc",0
"-body, and robotic control systems <ref type=""bibr"" target=""#b1"">(Battaglia et al., 2016;</ref><ref type=""bibr"" target=""#b4"">Chang et al., 2016;</ref><ref type=""bibr"" target=""#b24"">Sanchez-Gonzalez et al., 2018;</ref><ref type=""bibr"" target=""#b22"">Mrowca et al., 2018;</ref><ref type=""bibr"" target=""# inputs. This it implicitly what happens when the loss is defined directly on the positions, regardless of whether the inputs are perturbed with noise <ref type=""bibr"" target=""#b24"">(Sanchez-Gonzalez et al., 2018)</ref> or the inputs have noise due to model error <ref type=""bibr"">(Ummenhofer et al., erformance. We speculate that this is because physically valid distributions are very complex and smooth in these datasets, and unlike in the work by <ref type=""bibr"" target=""#b24"">Sanchez-Gonzalez et al. (2018)</ref>, once noise is applied, it is not clear which is the route the model should take t",0
"attaglia et al., 2016;</ref><ref type=""bibr"" target=""#b4"">Chang et al., 2016;</ref><ref type=""bibr"" target=""#b24"">Sanchez-Gonzalez et al., 2018;</ref><ref type=""bibr"" target=""#b22"">Mrowca et al., 2018;</ref><ref type=""bibr"" target=""#b18"">Li et al., 2019;</ref><ref type=""bibr"" target=""#b25"">Sanchez-G",0
"ng on the content information <ref type=""bibr"" target=""#b3"">(Han et al. 2004;</ref><ref type=""bibr"" target=""#b4"">Huang, Ertekin, and Giles 2006;</ref><ref type=""bibr"" target=""#b12"">Yoshida et al. 2010</ref>) usually leverage supervised learning algorithms to learn the pairwise similarity functions. #b3"">(Han et al. 2004;</ref><ref type=""bibr"" target=""#b4"">Huang, Ertekin, and Giles 2006;</ref><ref type=""bibr"" target=""#b8"">Louppe et al. 2016;</ref><ref type=""bibr"" target=""#b12"">Yoshida et al. 2010)</ref>, which usually solve the problem in a discriminative way. These methods calculate the conten to support the researches which study the author name disambiguation task using content information, we construct a new dataset collected from AceKG <ref type=""bibr"" target=""#b12"">(Wang et al. 2018b</ref>). The benchmark dataset consists of 130,655 papers from 17,816 distinguished authors. Each sam",1
"he network are likely to be written by the same author. Thus constructing the network becomes the critical part of these methods, e.g., paper network <ref type=""bibr"" target=""#b13"">(Zhang et al. 2018)</ref>, paper-author network <ref type=""bibr"" target=""#b13"">(Zhang and Al Hasan 2017)</ref>. However l-world author name disambiguation datasets for experiments:</p><p>• AMiner-AND<ref type=""foot"" target=""#foot_0"">1</ref> . The dataset is released by <ref type=""bibr"" target=""#b13"">(Zhang et al. 2018)</ref>, which contains 500 author names for training and 100 author names for testing. We construct etwork becomes the critical part of these methods, e.g., paper network <ref type=""bibr"" target=""#b13"">(Zhang et al. 2018)</ref>, paper-author network <ref type=""bibr"" target=""#b13"">(Zhang and Al Hasan 2017)</ref>. However, either complicated feature engineering or the supervision <ref type=""bibr"" ta per-author network <ref type=""bibr"" target=""#b13"">(Zhang and Al Hasan 2017)</ref>. However, either complicated feature engineering or the supervision <ref type=""bibr"" target=""#b13"">(Zhang et al. 2018</ref>) is required.</p><p>The two categories of methods are like the two sides of the same coin. The ilarity between each pair of papers using the carefully designed pairwise features, including author names, titles, institute names etc.</p><p>AMiner <ref type=""bibr"" target=""#b13"">(Zhang et al. 2018</ref>): This model designs a supervised global stage to fine-tune the word2vec result, and designs a -AND and AceKG-AND. In the experiment on AMiner-AND, we use 100 names for testing and compare the result with the results of other models reported in <ref type=""bibr"" target=""#b13"">(Zhang et al. 2018</ref>). In the experiment on AceKG-AND, we sample 85 names for testing. Since Louppe et al. and AMin",1
"ed on the content information <ref type=""bibr"" target=""#b3"">(Han et al. 2004;</ref><ref type=""bibr"" target=""#b4"">Huang, Ertekin, and Giles 2006;</ref><ref type=""bibr"" target=""#b8"">Louppe et al. 2016;</ref><ref type=""bibr"" target=""#b12"">Yoshida et al. 2010)</ref>, which usually solve the problem in a s represent the connections among them. A designed network embedding is learned with an aim to preserve the connectivity of the constructed networks. <ref type=""bibr"" target=""#b8"">Louppe et al. (2016)</ref>: This model trains a function to measure the similarity between each pair of papers using the",0
"hich can eliminate the requirement of labeled samples and complicated feature engineering to some extent. Inspired by generative adversarial networks <ref type=""bibr"" target=""#b1"">(Goodfellow et al. 2014)</ref>, we may combine the two categories in an adversarial way. In this paper, we propose a uni",0
"the high-order connections among papers. Methods focusing on relation information <ref type=""bibr"" target=""#b5"">(Kanani, McCallum, and Pal 2007;</ref><ref type=""bibr"" target=""#b0"">Bekkerman and McCallum 2005)</ref> usually solve the problem on the bibliographic network, where the relation informatio",0
"are connected in the HIN. Consequently, We can represent the relation features by preserving the connectivity information of the HIN. We use node2vec <ref type=""bibr"" target=""#b2"">(Grover and Leskovec 2016)</ref> to represent these features by v i ∈ R k , where papers are close in the feature space",0
"ractical scenarios, e.g., scholar searching, influence evaluating and mentor recommendation, which raises the necessity of author name disambiguation <ref type=""bibr"" target=""#b9"">(Smalheiser and Torvik 2009)</ref>.</p><p>Author name disambiguation is to split the papers under the same name into sev ether the papers are selected by the generator, the generator is guided to find the rules to select the homogeneous papers. To update θ G , we follow <ref type=""bibr"" target=""#b9"">(Schulman et al. 2015)</ref> to compute the gradient of V(G, D) by policy gradient:</p><formula xml:id=""formula_6"">∇ θ G",0
"udes title, abstract, introduction and keywords etc. Methods focusing on the content information <ref type=""bibr"" target=""#b3"">(Han et al. 2004;</ref><ref type=""bibr"" target=""#b4"">Huang, Ertekin, and Giles 2006;</ref><ref type=""bibr"" target=""#b12"">Yoshida et al. 2010</ref>) usually leverage supervis ories according to the information they focus on. The first are based on the content information <ref type=""bibr"" target=""#b3"">(Han et al. 2004;</ref><ref type=""bibr"" target=""#b4"">Huang, Ertekin, and Giles 2006;</ref><ref type=""bibr"" target=""#b8"">Louppe et al. 2016;</ref><ref type=""bibr"" target=""#b1",0
"target=""#b20"">20]</ref> are used. However, the roofline model lack fine-grained estimation and customized models are not general as desired. Timeloop <ref type=""bibr"" target=""#b21"">[21]</ref> and Eyeriss <ref type=""bibr"" target=""#b22"">[22]</ref> use for and parallel-for to describe the temporal and",1
">13,</ref><ref type=""bibr"" target=""#b13"">14]</ref>.</p><p>While DNN accelerators can be 1000× more efficient than general purpose computing platforms <ref type=""bibr"" target=""#b15"">[15]</ref>, developing DNN accelerators presents significant challenges, because: <ref type=""bibr"" target=""#b0"">(1)</re",0
"etworks (DNNs) have achieved record-breaking performance in various applications, such as image classification <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref> and natural language processing <ref type=""bibr"" target=""#b2"">[3]</ref>. However, their powerful performance of ents significant challenges, because: <ref type=""bibr"" target=""#b0"">(1)</ref> mainstream DNNs have millions of parameters and billions of operations; <ref type=""bibr"" target=""#b1"">(2)</ref> the design space of DNN accelerator is large due to numerous design choices of architectures, hardware IPs, DN",0
"imeloop, to calculate the energy as in <ref type=""bibr"" target=""#b13"">[14]</ref>. The work in <ref type=""bibr"" target=""#b24"">[24]</ref> adopts Halide <ref type=""bibr"" target=""#b25"">[25]</ref>, a domain-specific language for image processing applications, and proposes a modeling framework which is si",0
"d estimates the latency by calculating the maximum isolated execution cycle across all hardware IPs based on a double-buffering assumption. Accelergy <ref type=""bibr"" target=""#b23"">[23]</ref> proposes a configuration language to describe hardware architectures and depends on plug-ins, e.g., Timeloop",0
"rk (FFNN) as our baseline model. We extended this model with feature selection techniques to enhance the performance. The magnitude measure technique <ref type=""bibr"" target=""#b2"">[3]</ref> uses the absolute value of weights from a fully trained network to measure the contribution of input features work by reducing the number of input features. We presented two techniques to reduce the number of features, namely filter method (magnitude measure) <ref type=""bibr"" target=""#b2"">[3]</ref> and embedded method ( -1 norm regularisation) <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target="" s into account contribution of absolute weight values that connects a hidden neuron j in the hidden layer into an output neuron k in the output layer <ref type=""bibr"" target=""#b2"">[3]</ref>. The following equation measures the contribution from an input feature neuron i to a hidden neuron j with inp ures in fully-trained neural network based for the data set described in Sect. 2.1, and removed two features with the lowest Q-values as suggested by <ref type=""bibr"" target=""#b2"">[3]</ref> to produce more consistent results. The network was re-trained using reduced features. We trained the reduced",1
"t> <div xmlns=""http://www.tei-c.org/ns/1.0""><p>We examined neural network models to perform video-based stress recognition using ANUStressDB data set <ref type=""bibr"" target=""#b5"">[6]</ref>. Recent works on video-based stress recognition <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target od used to measure the model performance.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2.1"">Data Set</head><p>The ANUStressDB data set <ref type=""bibr"" target=""#b5"">[6]</ref> consists of video samples from 24 subjects. Each video has a duration of 32 min 17 s, divided into 58110 sampl has much less parameters with the expense of around 6-7% accuracy loss</p><p>The methods described in this paper are neural-based model. The paper in <ref type=""bibr"" target=""#b5"">[6]</ref> used Support Vector Machine (SVM) based model to perform the task and obtain reported accuracy of 89%. The SVM orm video-based stress recognition using ANUStressDB data set <ref type=""bibr"" target=""#b5"">[6]</ref>. Recent works on video-based stress recognition <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b10"">11]</ref> requires feature engineering process, which is time consuming and expe rt-Term Memory (LSTM). Recent researches on video-based stress recognition system incorporates feature engineering process before making a prediction <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b10"">11]</ref>. However, this process is expensive and timeconsuming. In our work, we",1
"abel>(2)</label></formula><p>The error function used in the model is cross-entropy error function. The model was trained using error back-propagation <ref type=""bibr"" target=""#b1"">[2]</ref> and optimised using Stochastic Gradient Descent (SGD) with momentum <ref type=""bibr"" target=""#b9"">[10]</ref> w",0
"lue of weights from a fully trained network to measure the contribution of input features towards output values. The -1 norm regularisation technique <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b7"">8]</ref> is an embedded feature selection technique used to bring weight of irrel he number of features, namely filter method (magnitude measure) <ref type=""bibr"" target=""#b2"">[3]</ref> and embedded method ( -1 norm regularisation) <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b7"">8]</ref>.</p><p>Magnitude Measure Technique. The magnitude measure technique take",0
"atures, denotes as x ? R 34 . A weight matrix W ih ? R 7?34 transforms input vector into a vector with 7-dimension. Finally, ReLU activation function <ref type=""bibr"" target=""#b3"">[4]</ref> is applied to this 7-dimensional vector producing hidden vector, denotes as h ? R 7 . The mapping between an i",0
"was trained using error back-propagation <ref type=""bibr"" target=""#b1"">[2]</ref> and optimised using Stochastic Gradient Descent (SGD) with momentum <ref type=""bibr"" target=""#b9"">[10]</ref> with learning rate 0.1, and momentum term 0.9. The model hyper-parameters were cross-validated on the trainin",0
"performance, the model uses several modification from original fully-connected layers. Instead of ReLU activation function, the model uses LeakyReLU <ref type=""bibr"" target=""#b6"">[7]</ref> with negative slope 0.1 to avoid dying ReLU problem due to zero gradient during back-propagation. To address o",0
"odel during training <ref type=""bibr"" target=""#b8"">[9]</ref>. Finally, we compared improved NN models with a recurrent neural network model with LSTM <ref type=""bibr"" target=""#b4"">[5]</ref> to perform video-based stress recognition task. The LSTM model worked well with time-series data set with addi",0
"-Forward Neural Network (FFNN) and extended the model using Feature Selection Technique, namely magnitude measure technique [3] and -1 regularisation <ref type=""bibr"" target=""#b8"">[9]</ref>. Subsequently, we performed extensive evaluation between those models with the Long Short-Term Memory (LSTM) [ >8]</ref> is an embedded feature selection technique used to bring weight of irrelevant inputs to 0, hence remove them from the model during training <ref type=""bibr"" target=""#b8"">[9]</ref>. Finally, we compared improved NN models with a recurrent neural network model with LSTM <ref type=""bibr"" targ hence discouraging the model from over-fitting regularisation using -1 norm discourages parameters with high sum of absolute values of the parameters <ref type=""bibr"" target=""#b8"">[9]</ref>, thus creating a sparse weight (parameter) matrix solution. The capabilities of -1 norm to create this sparse pabilities of -1 norm to create this sparse solution hence bring some parameters to zero makes this technique a good candidate for feature selection. <ref type=""bibr"" target=""#b8"">[9]</ref>. We applied -1 penalty term using following equation.</p><formula xml:id=""formula_5"">error 1 = ? dhid i=0 din",0
"g ANUStressDB data set <ref type=""bibr"" target=""#b5"">[6]</ref>. Recent works on video-based stress recognition <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b10"">11]</ref> requires feature engineering process, which is time consuming and expensive. The neural network (NN) model ai s on video-based stress recognition system incorporates feature engineering process before making a prediction <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b10"">11]</ref>. However, this process is expensive and timeconsuming. In our work, we compared methods to improve neural net",0
"ory.</p><p>We introduce the Reformer model which solves these problems using the following techniques:</p><p>• Reversible layers, first introduced in <ref type=""bibr"" target=""#b7"">Gomez et al. (2017)</ref>, enable storing only a single copy of activations in the whole model, so the N factor disappea >3</ref>.</p><formula xml:id=""formula_9"">• n h • l term: the b • n h • l • d k ,</formula><p>RevNets. Reversible residual networks were introduced by <ref type=""bibr"" target=""#b7"">Gomez et al. (2017)</ref> where it was shown that they can replace ResNets for image classification. The main idea is to 19)</ref>, more efficient versions of the Transformer model's self-attention mechanism <ref type=""bibr"" target=""#b21"">(Sukhbaatar et al., 2019a;</ref><ref type=""bibr"" target=""#b7"">b)</ref> have also recently been explored.</p><p>In particular, leveraging sparsity in the attention layers has proved f",1
"</teiHeader> 	<text xml:lang=""en""> 		<body> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""1"">INTRODUCTION</head><p>The Transformer architecture <ref type=""bibr"" target=""#b23"">(Vaswani et al., 2017)</ref> is widely used in natural language processing and yields state-of-the-art results on a num CALITY-SENSITIVE HASHING ATTENTION</head><p>Dot-product attention. The standard attention used in the Transformer is the scaled dot-product attention <ref type=""bibr"" target=""#b23"">(Vaswani et al., 2017)</ref>. The input consists of queries and keys of dimension d k , and values of dimension d v . T f f + bn h n r lc)n l</formula></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4"">RELATED WORK</head><p>The Transformer model introduced in <ref type=""bibr"" target=""#b23"">(Vaswani et al., 2017)</ref> has been used widely in natural language tasks and further extended to model diverse data ; Stern, 2018)</ref> for training these models. We also evaluate on the WMT 2014 English-to-German translation task, following the hyperparameters of <ref type=""bibr"" target=""#b23"">Vaswani et al. (2017)</ref>. Training for all experiments  Effect of sharing QK. We first consider the effect of shared Effect of reversible layers. In the two plots on the right in Figure <ref type=""figure"" target=""#fig_2"">3</ref>, we compare a regular Transformer per <ref type=""bibr"" target=""#b23"">Vaswani et al. (2017)</ref> with the reversible one describe in Section 3. The two models have identical parameter coun king both the encoder and the decoder fully reversible in the Transformer-base architecture, and  see that the resulting model performs comparably to <ref type=""bibr"" target=""#b23"">Vaswani et al. (2017)</ref> when trained for 100K steps. We also evaluate training for a greater number of steps and wi",1
"et al., 2019)</ref>. Most notably, this model class has been applied successfully in the self-supervised training of extremely large language models <ref type=""bibr"" target=""#b6"">(Devlin et al., 2018;</ref><ref type=""bibr"" target=""#b14"">Radford et al., 2019)</ref>.</p><p>Given the enormous computat",0
"2018)</ref>. Transformer models are also used on increasingly long sequences. Up to 11 thousand tokens of text in a single example were processed in <ref type=""bibr"" target=""#b11"">(Liu et al., 2018)</ref> and when processing other modalities, like music <ref type=""bibr"" target=""#b9"">(Huang et al.,",0
"eds 0.5B per layer in the largest configuration reported in <ref type=""bibr"">(Shazeer et al., 2018)</ref> while the number of layers goes up to 64 in <ref type=""bibr"" target=""#b0"">(Al-Rfou et al., 2018)</ref>. Transformer models are also used on increasingly long sequences. Up to 11 thousand tokens",0
"a such as music scores <ref type=""bibr"" target=""#b9"">(Huang et al., 2018)</ref>, and images <ref type=""bibr"" target=""#b13"">(Parmar et al., 2018;</ref><ref type=""bibr"" target=""#b16"">Ramachandran et al., 2019)</ref>. Most notably, this model class has been applied successfully in the self-supervised t",0
"ory be fixed before has been removed in <ref type=""bibr"" target=""#b17"">Santoro et al. (2016)</ref> at the cost of memory size and later alleviated by <ref type=""bibr"" target=""#b15"">Rae et al. (2016)</ref>. The last paper considered memory lookups with approximate nearest neighbors including both LSH",0
"orks <ref type=""bibr"" target=""#b24"">(Weston et al., 2014)</ref> and later work on scaling it <ref type=""bibr"" target=""#b3"">(Bordes et al., 2015;</ref><ref type=""bibr"" target=""#b4"">Chandar et al., 2016)</ref> used memory with size in the millions. The cost of doing so is that the memory must be fixed",0
"ry memory locations that are useful. These hints are either given as additional supervising information by the task or determined heuristically as in <ref type=""bibr"" target=""#b8"">Hill et al. (2015)</ref>. The requirement that the memory be fixed before has been removed in <ref type=""bibr"" target=""#",0
"table. Therefore, we use the first-order approximation to calculate the rectification term. Specifically, by approximating ψ 2 (.) to the first order <ref type=""bibr"" target=""#b29"">(Wolter, 2007)</ref>,</p><formula xml:id=""formula_20"">ψ 2 (.) ≈ E[ψ 2 (.)] + 1 2 E[ψ 2 (.)] (ψ 2 (.) − E[ψ 2 (.)]) and",0
"ite its simplicity. Recently, many efforts have been made to accelerate optimization by applying adaptive learning rate.</p><p>In particular, Adagrad <ref type=""bibr"" target=""#b11"">(Duchi et al., 2010)</ref> and its variants, e.g., RMSprop <ref type=""bibr"" target=""#b16"">(Hinton et al., 2012)</ref>,",0
"initialization <ref type=""bibr"" target=""#b1"">(Balduzzi et al., 2017;</ref><ref type=""bibr"" target=""#b32"">Zhang et al., 2019)</ref> and normalization <ref type=""bibr"" target=""#b0"">(Ba et al., 2016;</ref><ref type=""bibr"" target=""#b17"">Ioffe &amp; Szegedy, 2015)</ref>. Indeed, these techniques can be",0
"e.g., RMSprop <ref type=""bibr"" target=""#b16"">(Hinton et al., 2012)</ref>, Adam <ref type=""bibr"" target=""#b18"">(Kingma &amp; Ba, 2014)</ref>, Adadelta <ref type=""bibr"" target=""#b31"">(Zeiler, 2012)</ref> and Nadam <ref type=""bibr"" target=""#b10"">(Dozat, 2016)</ref>, stand out due to their fast converge",0
"plying adaptive learning rate.</p><p>In particular, Adagrad <ref type=""bibr"" target=""#b11"">(Duchi et al., 2010)</ref> and its variants, e.g., RMSprop <ref type=""bibr"" target=""#b16"">(Hinton et al., 2012)</ref>, Adam <ref type=""bibr"" target=""#b18"">(Kingma &amp; Ba, 2014)</ref>, Adadelta <ref type=""bib rate warmup for the Adam algorithm, while it can be applied to other algorithms that use similar adaptive learning rate (ψ(.)) designs, e.g., RMSprop <ref type=""bibr"" target=""#b16"">(Hinton et al., 2012)</ref> and Nadam <ref type=""bibr"" target=""#b10"">(Dozat, 2016)</ref>.</p></div> <div xmlns=""http://",0
"Tesla V100 GPU.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>B.2 IMAGEINE CLASSIFICATION</head><p>We use the default ResNet architectures <ref type=""bibr"" target=""#b15"">(He et al., 2016)</ref> in a public pytorch re-implementation<ref type=""foot"" target=""#foot_2"">4</ref> . Specifically,",0
"rmup Fast and stable optimization algorithms are what generations of researchers have been pursuing <ref type=""bibr"" target=""#b12"">(Gauss, 1823;</ref><ref type=""bibr"" target=""#b4"">Cauchy, 1847)</ref>. Remarkably, stochastic gradient-based optimization, such as stochastic gradient descent (SGD), has",0
"am-eps</head><p>Adam-2k Adam-vanilla RAdam Adam-warmup Fast and stable optimization algorithms are what generations of researchers have been pursuing <ref type=""bibr"" target=""#b12"">(Gauss, 1823;</ref><ref type=""bibr"" target=""#b4"">Cauchy, 1847)</ref>. Remarkably, stochastic gradient-based optimizatio",0
"target=""#b2"">Cai et al., 2017)</ref>. Current works on Graph Convolutional Networks (GCNs) <ref type=""bibr"" target=""#b9"">(Hamilton et al., 2017;</ref><ref type=""bibr"" target=""#b4"">Chen et al., 2018b;</ref><ref type=""bibr"" target=""#b8"">Gao et al., 2018;</ref><ref type=""bibr"" target=""#b14"">Huang et al proceeds in full batch. In order to scale GCNs to large graphs, layer sampling techniques <ref type=""bibr"" target=""#b9"">(Hamilton et al., 2017;</ref><ref type=""bibr"" target=""#b4"">Chen et al., 2018b;</ref><ref type=""bibr"" target=""#b26"">Ying et al., 2018a;</ref><ref type=""bibr"" target=""#b3"">Chen et a get=""#b3"">Chen et al. (2018a)</ref> ensure that only a small number of neighbors (typically from 2 to 50) are selected by one node in the next layer. <ref type=""bibr"" target=""#b4"">Chen et al. (2018b)</ref> and <ref type=""bibr"" target=""#b14"">Huang et al. (2018)</ref> further propose samplers to restr d in Section 3.4.</p><p>Remark We can also apply the above edge sampler to perform layer sampling. Under the independent layer sampling assumption of <ref type=""bibr"" target=""#b4"">Chen et al. (2018b)</ref>, one would sample a connection u ( ) , v ( +1)  with probability p</p><formula xml:id=""formula g et al. (2018a)</ref>; <ref type=""bibr"" target=""#b3"">Chen et al. (2018a)</ref>. Point (2) is due to the better interlayer connectivity compared with <ref type=""bibr"" target=""#b4"">Chen et al. (2018b)</ref>, and unbiased minibatch estimator compared with <ref type=""bibr"" target=""#b5"">Chiang et al. (2 wo support nodes in the previous layer. The idea is to use the historical activations in the previous layer to avoid redundant re-evaluation. FastGCN <ref type=""bibr"" target=""#b4"">(Chen et al., 2018b)</ref> performs sampling from another perspective. Instead of tracking down the inter-layer connecti <ref type=""bibr"" target=""#b16"">(Kipf &amp; Welling, 2016)</ref>, 2. GraphSAGE <ref type=""bibr"" target=""#b9"">(Hamilton et al., 2017)</ref>, 3. FastGCN <ref type=""bibr"" target=""#b4"">(Chen et al., 2018b)</ref>, 4. S-GCN <ref type=""bibr"" target=""#b3"">(Chen et al., 2018a)</ref>, 5. AS-GCN <ref type=""bibr inear activations. Thus, we analyze the embedding of each layer independently. This is similar to the treatment of layers independently by prior work <ref type=""bibr"" target=""#b4"">(Chen et al., 2018b;</ref><ref type=""bibr"" target=""#b14"">Huang et al., 2018)</ref>. Consider a layer-( + 1) node v and a structed during training are complete, applying skip connections to GraphSAINT is straightforward. On the other hand, for some layer sampling methods <ref type=""bibr"" target=""#b4"">(Chen et al., 2018b;</ref><ref type=""bibr"" target=""#b14"">Huang et al., 2018)</ref>, extra modification to their samplers",1
"al., 2018;</ref><ref type=""bibr"" target=""#b0"">Abu-El-Haija et al., 2019)</ref> or even more complicated networks for the task of graph classification <ref type=""bibr"" target=""#b27"">(Ying et al., 2018b)</ref>, we replace the full adjacency matrix A with the (normalized) one for the subgraph A s to pe",0
"ddition, high order graph convolutional layers <ref type=""bibr"" target=""#b31"">(Zhou, 2017;</ref><ref type=""bibr"" target=""#b18"">Lee et al., 2018;</ref><ref type=""bibr"" target=""#b0"">Abu-El-Haija et al., 2019)</ref> also help propagate long-distance features. With the numerous architectural variants de le † . 3. Others: To support high order layers <ref type=""bibr"" target=""#b31"">(Zhou, 2017;</ref><ref type=""bibr"" target=""#b18"">Lee et al., 2018;</ref><ref type=""bibr"" target=""#b0"">Abu-El-Haija et al., 2019)</ref> or even more complicated networks for the task of graph classification <ref type=""bibr""",0
"r"" target=""#b4"">Chen et al., 2018b;</ref><ref type=""bibr"" target=""#b8"">Gao et al., 2018;</ref><ref type=""bibr"" target=""#b14"">Huang et al., 2018;</ref><ref type=""bibr"" target=""#b3"">Chen et al., 2018a)</ref> mostly focus on shallow models (2 layers) on relatively small graphs. Scaling GCNs to larger d mpling techniques. The works by <ref type=""bibr"" target=""#b9"">Hamilton et al. (2017)</ref>; <ref type=""bibr"" target=""#b26"">Ying et al. (2018a)</ref>; <ref type=""bibr"" target=""#b3"">Chen et al. (2018a)</ref> ensure that only a small number of neighbors (typically from 2 to 50) are selected by one node neighborhood size compared with <ref type=""bibr"" target=""#b9"">Hamilton et al. (2017)</ref>; <ref type=""bibr"" target=""#b26"">Ying et al. (2018a)</ref>; <ref type=""bibr"" target=""#b3"">Chen et al. (2018a)</ref>. Point (2) is due to the better interlayer connectivity compared with <ref type=""bibr"" target= tp://www.tei-c.org/ns/1.0"" xml:id=""fig_2""><head></head><label></label><figDesc>2), since<ref type=""bibr"" target=""#b9"">Hamilton et al. (2017)</ref> and<ref type=""bibr"" target=""#b3"">Chen et al. (2018a)</ref> report accuracy for this version in their original papers. We use the large version for additi get=""#b9"">(Hamilton et al., 2017;</ref><ref type=""bibr"" target=""#b4"">Chen et al., 2018b;</ref><ref type=""bibr"" target=""#b26"">Ying et al., 2018a;</ref><ref type=""bibr"" target=""#b3"">Chen et al., 2018a;</ref><ref type=""bibr"" target=""#b8"">Gao et al., 2018;</ref><ref type=""bibr"" target=""#b14"">Huang et al /ref> by introducing an importance score to each neighbor. The algorithm presumably leads to less information loss due to weighted aggregation. S-GCN <ref type=""bibr"" target=""#b3"">(Chen et al., 2018a)</ref> further restricts neighborhood size by requiring only two support nodes in the previous layer GraphSAGE <ref type=""bibr"" target=""#b9"">(Hamilton et al., 2017)</ref>, 3. FastGCN <ref type=""bibr"" target=""#b4"">(Chen et al., 2018b)</ref>, 4. S-GCN <ref type=""bibr"" target=""#b3"">(Chen et al., 2018a)</ref>, 5. AS-GCN <ref type=""bibr"" target=""#b14"">(Huang et al., 2018)</ref>, and 6. ClusterGCN <ref cv -cvd (which stand for ""control variate"" and ""control variate dropout"") with pre-computation of the first layer aggregation. According to the paper <ref type=""bibr"" target=""#b3"">(Chen et al., 2018a)</ref>, such pre-computation significantly reduces training time without affecting accuracy. For S-G",0
"et al. (2017)</ref> develop a multi-graph CNN (MGCNN) model to extract user and item latent features from their respective nearest-neighbor networks. <ref type=""bibr"" target=""#b1"">Berg et al. (2017)</ref> propose graph convolutional matrix completion (GC-MC) which directly applies a GNN to the user- r, this method fails to model the interactions between content and graph structures in the early graph convolution stage.</p><p>On the other hand, as <ref type=""bibr"" target=""#b1"">Berg et al. (2017)</ref> and <ref type=""bibr"">Zhang &amp; Chen (2018)</ref> found, directly concatenating content with i out leveraging any global information specific to the bipartite graph. Our node labeling is also different from using the global node IDs as in GC-MC <ref type=""bibr"" target=""#b1"">(Berg et al., 2017)</ref>. Using one-hot encoding of global IDs is essentially transforming the first message passing la our IGMC with GRALS <ref type=""bibr"" target=""#b31"">(Rao et al., 2015)</ref>, sRGCNN <ref type=""bibr"" target=""#b29"">(Monti et al., 2017)</ref>, GC-MC <ref type=""bibr"" target=""#b1"">(Berg et al., 2017)</ref>, F-EAE <ref type=""bibr"" target=""#b13"">(Hartford et al., 2018)</ref>, and PinSage <ref type=""bi p>In our experiments we only concatenate the target user and item's content vectors with the final graph representation output by the GNN, similar to <ref type=""bibr"" target=""#b1"">(Berg et al., 2017)</ref>. However, this method fails to model the interactions between content and graph structures in nt by concatenating target user and item's content feature vectors to the graph representation g before feeding into the MLP (4), which is similar to <ref type=""bibr"" target=""#b1"">(Berg et al., 2017)</ref>. The results are shown in Table <ref type=""table"">5</ref>. From Table <ref type=""table"">5</ref br"" target=""#b21"">(Li et al., 2018)</ref>. This is reflected in that previous node-based approaches mainly use only one or two message passing layers <ref type=""bibr"" target=""#b1"">(Berg et al., 2017;</ref><ref type=""bibr"" target=""#b39"">Ying et al., 2018a)</ref>.</p></div> <div xmlns=""http://www.tei-",1
"(IMC) has been proposed, which leverages content (side information) of users and items <ref type=""bibr"" target=""#b14"">(Jain &amp; Dhillon, 2013;</ref><ref type=""bibr"" target=""#b38"">Xu et al., 2013)</ref>. In IMC, a rating is decomposed by r ij = x i Qy j , where x i and y j are content feature vecto",1
"=""#b23"">Li et al., 2015;</ref><ref type=""bibr"" target=""#b19"">Kipf &amp; Welling, 2016;</ref><ref type=""bibr"" target=""#b30"">Niepert et al., 2016;</ref><ref type=""bibr"" target=""#b6"">Dai et al., 2016)</ref>. There are two types of GNNs: Nodelevel GNNs use message passing layers to iteratively pass mess",0
"ome heuristic scores. For example, the common neighbors heuristic count the common neighbors between two nodes to predict links, while the Katz index <ref type=""bibr"" target=""#b17"">(Katz, 1953)</ref> uses a weighted sum of all the walks between two nodes. See <ref type=""bibr"" target=""#b24"">(Liben-No",0
"<p>We conduct experiments on five common matrix completion datasets: Flixster <ref type=""bibr"" target=""#b15"">(Jamali &amp; Ester, 2010)</ref>, Douban <ref type=""bibr"" target=""#b26"">(Ma et al., 2011)</ref>, YahooMusic <ref type=""bibr"" target=""#b8"">(Dror et al., 2011)</ref>, MovieLens-100K and MovieLe",0
"""#b33"">(Scarselli et al., 2009;</ref><ref type=""bibr"" target=""#b3"">Bruna et al., 2013;</ref><ref type=""bibr"" target=""#b9"">Duvenaud et al., 2015;</ref><ref type=""bibr"" target=""#b23"">Li et al., 2015;</ref><ref type=""bibr"" target=""#b19"">Kipf &amp; Welling, 2016;</ref><ref type=""bibr"" target=""#b30"">Niep",0
"ht, 2009)</ref>, inductive matrix completion (IMC) <ref type=""bibr"" target=""#b14"">(Jain &amp; Dhillon, 2013)</ref>, geometric matrix completion (GMC) <ref type=""bibr"" target=""#b16"">(Kalofolias et al., 2014)</ref>, as well as GRALS, sRGCNN, GC-MC, F-EAE and PinSage. We train IGMC for 80 epochs and re",0
"features.</p><p>To learn the rich graph patterns introduced by the different edge types, we adopt the relational graph convolutional operator (R-GCN) <ref type=""bibr"" target=""#b35"">(Schlichtkrull et al., 2018)</ref> as our GNN's message passing layers, which has the following form:</p><formula xml:i tecture uses 4 R-GCN layers with 32, 32, 32, 32 hidden dimensions. Basis decomposition with 4 bases is used to reduce the number of parameters in W r <ref type=""bibr"" target=""#b35"">(Schlichtkrull et al., 2018)</ref>. The final MLP has 128 hidden units and a dropout rate of 0.5. We use 1-hop enclosin",0
"ract> <div xmlns=""http://www.tei-c.org/ns/1.0""><p>The use of deep pre-trained transformers has led to remarkable progress in a number of applications <ref type=""bibr"" target=""#b5"">(Devlin et al., 2019)</ref>. For tasks that make pairwise comparisons between sequences, matching a given input with a c enchmarks on a variety of language understanding tasks have been achieved through the use of deep pre-trained language models followed by fine-tuning <ref type=""bibr"" target=""#b5"">(Devlin et al., 2019)</ref>. In this work we explore improvements to this approach for the class of tasks that require m diction speed, as scoring many candidates can be prohibitively slow.</p><p>The current state-of-the-art focuses on using BERT models for pre-training <ref type=""bibr"" target=""#b5"">(Devlin et al., 2019)</ref>, which employ large text corpora on general subjects: Wikipedia and the Toronto Books Corpus ed in sections 4.2, 4.3 and 4.4 respectively, are based on large pre-trained transformer models with the same architecture and dimension as BERT-base <ref type=""bibr"" target=""#b5"">(Devlin et al., 2019)</ref>, which has 12 layers, 12 attention heads, and a hidden size of 768. As well as considering t he simpler architecture (results not shown).</p><p>We tried two optimizers: Adam (Kingma &amp; Ba, 2015) with weight decay of 0.01 (as recommended by <ref type=""bibr"" target=""#b5"">(Devlin et al., 2019)</ref>) and Adamax (Kingma &amp; Ba, 2015) without weight decay; based on validation set performanc f epoch. In Table <ref type=""table"" target=""#tab_3"">3</ref> we show validation performance when fine-tuning various layers of the weights provided by <ref type=""bibr"" target=""#b5"">(Devlin et al., 2019)</ref>, using Adam with decay optimizer. Fine-tuning the entire network is important, with the exce pre-training on Reddit, the input is the context, and the label is the next utterance.</p><p>When pre-training on Wikipedia and Toronto Books, as in <ref type=""bibr"" target=""#b5"">Devlin et al. (2019)</ref>, the input is one sentence and the label the next sentence in the text. Each input token is r okens are 1.</p><p>Pre-training Procedure Our pre-training strategy involves training with a masked language model (MLM) task identical to the one in <ref type=""bibr"" target=""#b5"">Devlin et al. (2019)</ref>. In the pre-training on Wikipedia and Toronto Books we add a next-sentence prediction task id encoders and Cross-encoders</head><p>We first investigate fine-tuning the Bi-and Cross-encoder architectures initialized with the weights provided by <ref type=""bibr"" target=""#b5"">Devlin et al. (2019)</ref>, studying the choice of other hyperparameters (we explore our own pre-training schemes in sec",1
"networks <ref type=""bibr"" target=""#b6"">(Dinan et al., 2019)</ref> as well as LSTMs <ref type=""bibr"" target=""#b15"">(Lowe et al., 2015)</ref> and CNNs <ref type=""bibr"" target=""#b11"">(Kadlec et al., 2015)</ref> which encode input and candidate label separately. A major advantage of Bi-encoder methods",0
"<ref type=""bibr"" target=""#b17"">(Salton et al., 1975)</ref>, LSI <ref type=""bibr"" target=""#b4"">(Deerwester et al., 1990)</ref>, supervised embeddings <ref type=""bibr"" target=""#b0"">(Bai et al., 2009;</ref><ref type=""bibr"" target=""#b21"">Wu et al., 2018)</ref> and classical siamese networks <ref type=""",0
"2012 , he was named to the 2012 All -NBA First Team . On March 7 , 2012 , he was named one of five finalists for the Naismith Award , which is 0.064  <ref type=""bibr"" target=""#b0"">(Baevski and Auli, 2019;</ref><ref type=""bibr"" target=""#b19"">Radford et al., 2019)</ref>. We perform experiments at the",1
"it is well known there are major problems at its core. In particular, standard likelihood training and decoding leads to dull and repetitive outputs <ref type=""bibr"" target=""#b10"">(Holtzman et al., 2019)</ref>. While some post-hoc fixes have been proposed, in particular top-k and nucleus sampling, type=""bibr"" target=""#b10"">(Holtzman et al., 2019;</ref><ref type=""bibr"" target=""#b25"">Vig, 2018)</ref>, (ii) an intrinsic property of human language <ref type=""bibr"" target=""#b10"">(Holtzman et al., 2019)</ref> rather than a modeling deficiency, or that (iii) a training objective relying on fixed co ach is that of sampling from the model at generation time. Top k-sampling <ref type=""bibr"" target=""#b7"">(Fan et al., 2018)</ref> and nucleus sampling <ref type=""bibr"" target=""#b10"">(Holtzman et al., 2019)</ref> are two methods that sample sequences based on a function of the predicted next token pro However, as the underlying model is unchanged, it often prefers semantically similar phrasing, depending on the temperature parameter of the sampling <ref type=""bibr"" target=""#b10"">(Holtzman et al., 2019)</ref>. Furthermore, this solution is less relevant in less open-ended tasks such as machine tra sampling to the smallest set of tokens with total mass above a threshold p ∈ [0, 1]; i.e. U is the smallest subset with x∈U p θ (x|x &lt;t ) &gt;= p <ref type=""bibr"" target=""#b10"">(Holtzman et al., 2019)</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4"">NEURAL TEXT DEGENERATION< ans (0.5%), computed over prefixes drawn from a validation corpus.</p><p>Unlike previous work which only focused on degenerate sequence-level repeats <ref type=""bibr"" target=""#b10"">(Holtzman et al., 2019)</ref>, we additionally observe that neural language models exhibit substantially more repetitio ab_7"">6</ref> shows metrics for completions generated with top-k sampling <ref type=""bibr"" target=""#b7"">(Fan et al., 2018)</ref> and nucleus sampling <ref type=""bibr"" target=""#b10"">(Holtzman et al., 2019)</ref>. Models trained with unlikelihood objectives maintain language modeling quality compared s language modeling or dialogue has been observed to be dull, with high frequency tokens used too often and interesting content words used too rarely <ref type=""bibr"" target=""#b10"">(Holtzman et al., 2019;</ref><ref type=""bibr"" target=""#b5"">Dinan et al., 2019)</ref>. Moreover, the models repeat thems known. Possible candidates include the problem being (i) a by-product of the model architecture, e.g. the Transformer architecture preferring repeats <ref type=""bibr"" target=""#b10"">(Holtzman et al., 2019;</ref><ref type=""bibr"" target=""#b25"">Vig, 2018)</ref>, (ii) an intrinsic property of human langu models submitted to the ConvAI2 NeurIPS 2018 competition <ref type=""bibr"" target=""#b5"">(Dinan et al., 2019)</ref>. In language modeling, the work of <ref type=""bibr"" target=""#b10"">Holtzman et al. (2019)</ref> highlighted problems with the word frequency distribution and level of repetition in model tinuations and next-token predictions from conventional neural text generators have different token distributions from human text. As demonstrated by <ref type=""bibr"" target=""#b10"">Holtzman et al. (2019)</ref>, such models with greedy or beam search tend to produce high frequency tokens too often an",1
")</ref>, contextual text completion <ref type=""bibr"" target=""#b19"">(Radford et al., 2019)</ref>, language modeling (for k = 0), and dialogue modeling <ref type=""bibr"" target=""#b29"">(Zhang et al., 2018)</ref> where x 1:k is a dialogue history and a continuation is a next utterance.</p><p>Given p θ an",0
"e model itself. Some representative algorithms include structured perceptron <ref type=""bibr"" target=""#b2"">(Collins, 2002)</ref>, energy-based models <ref type=""bibr"" target=""#b13"">(LeCun et al., 2006)</ref> and more recently reflective likelihood <ref type=""bibr"" target=""#b4"">(Dieng et al., 2018)</",0
"than a modeling deficiency, or that (iii) a training objective relying on fixed corpora cannot take into account the real goal of using the language <ref type=""bibr"" target=""#b1"">(Choi, 2018)</ref>. Our work shows that, while the above may be factors, a primary factor is the use of the likelihood o",0
"el's level of repetition by selecting candidates that are unlike previously chosen ones. Separately, hard or soft beam blocking has been investigated <ref type=""bibr"" target=""#b18"">(Paulus et al., 2017;</ref><ref type=""bibr"" target=""#b11"">Klein et al., 2017)</ref>, whereby previously generated n-gra",0
"t"" n=""2"" xml:id=""foot_1"">Code and trained models are available at https://github.com/facebookresearch/ unlikelihood_training; implemented with Fairseq<ref type=""bibr"" target=""#b17"">(Ott et al., 2019)</ref>.</note> 		</body> 		<back> 			<div type=""references"">  				<listBibl>  <biblStruct xml:id=""b0""",0
"et=""#b2"">(Collins, 2002)</ref>, energy-based models <ref type=""bibr"" target=""#b13"">(LeCun et al., 2006)</ref> and more recently reflective likelihood <ref type=""bibr"" target=""#b4"">(Dieng et al., 2018)</ref>. A particular variant in this family of algorithms, called negative training, was recently us",0
"decoding method to a sophisticated beam search variant or to stochastic decoding, e.g. sampling. Different variants of beam search have been explored <ref type=""bibr"" target=""#b14"">(Li et al., 2016;</ref><ref type=""bibr"" target=""#b26"">Vijayakumar et al., 2018;</ref><ref type=""bibr"" target=""#b12"">Kul",0
"Citation networks. Cora, Citeseer, and Pubmed are standard citation network benchmark datasets <ref type=""bibr"" target=""#b24"">(Sen et al., 2008;</ref><ref type=""bibr"" target=""#b15"">Namata et al., 2012)</ref>. In these networks, nodes represent papers, and edges denote citations of one paper by anoth",0
"hyperbolic space <ref type=""bibr"" target=""#b18"">(Nickel &amp; Kiela, 2017)</ref>. One can employ various embedding methods to infer the latent space <ref type=""bibr"" target=""#b1"">(Cai et al., 2018;</ref><ref type=""bibr"" target=""#b28"">Wang et al., 2018)</ref>.</p><p>B. Structural neighborhood. Based",0
"en in Table <ref type=""table"" target=""#tab_1"">2</ref>. Citation networks. Cora, Citeseer, and Pubmed are standard citation network benchmark datasets <ref type=""bibr"" target=""#b24"">(Sen et al., 2008;</ref><ref type=""bibr"" target=""#b15"">Namata et al., 2012)</ref>. In these networks, nodes represent p",0
"ies in term of words of actor's Wikipedia.</p><p>Wikipedia network. Chameleon and squirrel are two page-page networks on specific topics in Wikipedia <ref type=""bibr"" target=""#b22"">(Rozemberczki et al., 2019)</ref>. In those datasets, nodes represent web pages and edges are mutual links between page",0
"round truth would be efficient but hardly useful for generation purposes, when at test time the model needs to generate from scratch.</p><p>Recently, <ref type=""bibr"" target=""#b1"">Bakhtin et al. (2019)</ref> carefully studied the problem of training a discriminator to distinguish human written text ing/test time corpora and concluded that the discriminator can generalize rather well to weaker language models when the training/test corpora match. <ref type=""bibr"" target=""#b1"">Bakhtin et al. (2019)</ref> found that the learned discriminator is not robust to random perturbations, and argued that r and using its output for importance sampling.</p><p>In this work, we build upon these two works. First, we formalize the residual interpretation by <ref type=""bibr"" target=""#b1"">Bakhtin et al. (2019)</ref> and use a generative model of the form:</p><formula xml:id=""formula_0"">P θ (x) ∝ P LM (x) ex les are produced by the discriminator itself. In our work, the pretrained locally normalized language model can be seen as a fixed generator, like in <ref type=""bibr"" target=""#b1"">Bakhtin et al. (2019)</ref>. <ref type=""bibr"" target=""#b0"">Azadi et al. (2018)</ref> also share our same goal but their everage recent advancements in locally normalized language modeling. Second, the language model provides a natural proposal distribution for training <ref type=""bibr"" target=""#b1"">(Bakhtin et al., 2019)</ref>, and training can be made efficient by using the conditional noise contrastive estimation o : the Toronto Book Corpus <ref type=""bibr"" target=""#b16"">(Zhu et al., 2015;</ref><ref type=""bibr"" target=""#b16"">Kiros et al., 2015)</ref> and CC-News <ref type=""bibr"" target=""#b1"">(Bakhtin et al., 2019)</ref>. The former dataset consists of fiction books in 16 different genres, totaling about half a",1
"inator is not robust to random perturbations, and argued that the discriminator operates in the ""residual"" space of the language model. Concurrently, <ref type=""bibr"" target=""#b9"">Grover et al. (2019)</ref> proposed a general approach to ""de-bias"" a generator, by simply training a discriminator and ocally normalized and they propose to improve the sampling from the generator by using the discriminator for rejection sampling. Similar to our work, <ref type=""bibr"" target=""#b9"">Grover et al. (2019)</ref> propose to use the discriminator to de-bias the pretrained generator using importance samplin is formulation enables efficient evaluation and generation via importance sampling <ref type=""bibr"" target=""#b15"">(Horvitz &amp; Thompson, 1952;</ref><ref type=""bibr"" target=""#b9"">Grover et al., 2019)</ref>.</p><p>In some sense, this last point is perhaps the central contribution of the paper, as it e joint model in Appendix A.1.</p><p>In order to generate efficiently, we use self-normalizing importance sampling <ref type=""bibr"">(Owen, 2013;</ref><ref type=""bibr"" target=""#b9"">Grover et al., 2019)</ref>. Under the assumptions that the model from which we wish to draw samples is the joint model,",1
"o a single scalar value. We consider three variants, a BIT-BASE following the architecture of RoBERTa-Base, and a BIT-LARGE * following RoBERTa-Large <ref type=""bibr"" target=""#b18"">(Liu et al., 2019)</ref>, and a BIT-MED with the same number of parameters as UNIT (such that JOINT BIT-MED has roughly MED has roughly the same number of parameters as BALM). We initialize the parameters with a trained BERT, and we use * to mark usage of external data <ref type=""bibr"" target=""#b18"">(Liu et al., 2019)</ref>, otherwise it means that BERT was trained on our training set. Notice how our model can be int",0
"lacks long-range coherency because it is produced by the greedy selection of one token at a time without lookahead.</p><p>Energy-based models (EBMs) <ref type=""bibr"" target=""#b11"">(Hinton, 2002;</ref><ref type=""bibr"" target=""#b17"">LeCun et al., 2006;</ref><ref type=""bibr"" target=""#b25"">Ranzato et a 0""><head n=""2"">RELATED WORK</head><p>Energy-based models have a long history in machine learning <ref type=""bibr"" target=""#b14"">(Hopfield, 1982;</ref><ref type=""bibr"" target=""#b11"">Hinton, 2002;</ref><ref type=""bibr"" target=""#b17"">LeCun et al., 2006;</ref><ref type=""bibr"" target=""#b25"">Ranzato et al rget=""#b21"">Nijkamp et al., 2019)</ref> are not applicable when the input is discrete like in text applications. Other approaches like Gibbs sampling <ref type=""bibr"" target=""#b11"">(Hinton, 2002)</ref> were applied to binary inputs but do not scale well to large dictionaries once the energy function",0
"rameterized by θ. The resulting model P θ (x) is globally normalized due to the energy term. Note that the same residual formulation was also used in <ref type=""bibr"" target=""#b27"">Rosenfeld et al. (2001)</ref>; <ref type=""bibr"" target=""#b34"">Wang &amp; Ou (2018b)</ref>; <ref type=""bibr"" target=""#b2 raining aims at improving the baseline model, but generation at test time is still greedy.</p><p>Energy Networks have been used for sequence modeling <ref type=""bibr"" target=""#b27"">(Rosenfeld et al., 2001;</ref><ref type=""bibr"" target=""#b35"">Wang et al., 2015;</ref><ref type=""bibr"">2017;</ref><ref t",0
"ookahead.</p><p>Energy-based models (EBMs) <ref type=""bibr"" target=""#b11"">(Hinton, 2002;</ref><ref type=""bibr"" target=""#b17"">LeCun et al., 2006;</ref><ref type=""bibr"" target=""#b25"">Ranzato et al., 2007</ref>) are a more general framework which potentially address all these issues, as they do not req =""bibr"" target=""#b14"">(Hopfield, 1982;</ref><ref type=""bibr"" target=""#b11"">Hinton, 2002;</ref><ref type=""bibr"" target=""#b17"">LeCun et al., 2006;</ref><ref type=""bibr"" target=""#b25"">Ranzato et al., 2007)</ref>. The key challenge of training is mining for good negatives. This can be accomplished expli This can be accomplished explicitly by fantasizing inputs where the energy should be increased or implicitly via global constraints such as sparsity <ref type=""bibr"" target=""#b25"">(Ranzato et al., 2007)</ref>. Methods attempting at maximizing the likelihood of the data require to sample from the di . Unfortunately, gradient-based MCMC approaches like Hybrid Monte Carlo <ref type=""bibr"" target=""#b30"">(Teh et al., 2003)</ref> and Langevyn dynamics <ref type=""bibr"" target=""#b25"">(Ranzato et al., 2007;</ref><ref type=""bibr"" target=""#b5"">Du &amp; Mordatch, 2019;</ref><ref type=""bibr"" target=""#b38"">",0
"i-c.org/ns/1.0""><head n=""1"">INTRODUCTION</head><p>The dominant approach to parametric text generation is based on large neural auto-regressive models <ref type=""bibr"" target=""#b24"">(Radford et al., 2019)</ref>. These models can be trained efficiently via maximum likelihood and they can efficiently g sumption is met quite well in practice as large auto-regressive language models trained on large datasets have improved significantly in recent years <ref type=""bibr"" target=""#b24"">(Radford et al., 2019)</ref>. In general however, residual learning always carries liability to its base model.</p></di",0
"by the greedy selection of one token at a time without lookahead.</p><p>Energy-based models (EBMs) <ref type=""bibr"" target=""#b11"">(Hinton, 2002;</ref><ref type=""bibr"" target=""#b17"">LeCun et al., 2006;</ref><ref type=""bibr"" target=""#b25"">Ranzato et al., 2007</ref>) are a more general framework which models have a long history in machine learning <ref type=""bibr"" target=""#b14"">(Hopfield, 1982;</ref><ref type=""bibr"" target=""#b11"">Hinton, 2002;</ref><ref type=""bibr"" target=""#b17"">LeCun et al., 2006;</ref><ref type=""bibr"" target=""#b25"">Ranzato et al., 2007)</ref>. The key challenge of training is m odel distribution, which is usually approximated with Monte Carlo sampling or mean field inference <ref type=""bibr"" target=""#b12"">(Hinton, 2012;</ref><ref type=""bibr"" target=""#b17"">LeCun et al., 2006)</ref> for globally normalized models. Unfortunately, both approaches are too expensive for text app",0
"e examples). In maximum likelihood training negatives are generated from the model, but in text application we cannot use gradient-based MCMC methods <ref type=""bibr"" target=""#b30"">(Teh et al., 2003;</ref><ref type=""bibr"" target=""#b5"">Du &amp; Mordatch, 2019)</ref> and Gibbs sampling <ref type=""bibr ihood of the data require to sample from the distribution induced by the model. Unfortunately, gradient-based MCMC approaches like Hybrid Monte Carlo <ref type=""bibr"" target=""#b30"">(Teh et al., 2003)</ref> and Langevyn dynamics <ref type=""bibr"" target=""#b25"">(Ranzato et al., 2007;</ref><ref type=""bi",0
"d MCMC methods <ref type=""bibr"" target=""#b30"">(Teh et al., 2003;</ref><ref type=""bibr"" target=""#b5"">Du &amp; Mordatch, 2019)</ref> and Gibbs sampling <ref type=""bibr"" target=""#b37"">(Welling et al., 2005)</ref> is too slow to be practical. Generating negatives by local perturbations of the ground tru",0
"and the data distribution, rather than relying on surrogate ranking losses. This approach is also related to other sequence level training objectives <ref type=""bibr"" target=""#b6"">(Edunov et al., 2018)</ref>, with the major differ-ence that in those works training aims at improving the baseline mode",0
"lihood Estimation (MLE) requires samples from the model distribution, which is usually approximated with Monte Carlo sampling or mean field inference <ref type=""bibr"" target=""#b12"">(Hinton, 2012;</ref><ref type=""bibr"" target=""#b17"">LeCun et al., 2006)</ref> for globally normalized models. Unfortunat",0
"sequence. Such distributions are relatively cheap to compute with modern hardware given the limited vocabulary size of common sub-word units like BPE <ref type=""bibr"" target=""#b28"">(Sennrich et al., 2015)</ref>.</p><p>Unfortunately, local normalization also brings some drawbacks. First, the designer",0
"ormulation was also used in <ref type=""bibr"" target=""#b27"">Rosenfeld et al. (2001)</ref>; <ref type=""bibr"" target=""#b34"">Wang &amp; Ou (2018b)</ref>; <ref type=""bibr"" target=""#b23"">Parshakova et al. (2019)</ref>.</p><p>This formulation has multi-fold benefits. First, by incorporating a locally norma rget=""#b35"">Wang et al., 2015;</ref><ref type=""bibr"">2017;</ref><ref type=""bibr"">Wang &amp; Ou, 2017;</ref><ref type=""bibr"" target=""#b33"">2018a;</ref><ref type=""bibr"" target=""#b23"">Parshakova et al., 2019)</ref>. In particular, our residual modeling form and the training algorithm is the same as in using importance sampling. We want to note that the same formulation has been proposed in <ref type=""bibr"" target=""#b34"">(Wang &amp; Ou, 2018b;</ref><ref type=""bibr"" target=""#b23"">Parshakova et al., 2019)</ref>. While <ref type=""bibr"" target=""#b19"">Ma &amp; Collins (2018)</ref> used conditional NCE investigated an EBM trained on the residual of a pretrained autoregressive language model <ref type=""bibr"" target=""#b34"">(Wang &amp; Ou, 2018b;</ref><ref type=""bibr"" target=""#b23"">Parshakova et al., 2019)</ref>. The resulting joint model scores sequences holistically, thanks to the energy function.",0
"ef type=""bibr"" target=""#b38"">Xie et al., 2016;</ref><ref type=""bibr"">2017;</ref><ref type=""bibr"">2019;</ref><ref type=""bibr"" target=""#b22"">2018;</ref><ref type=""bibr"" target=""#b7"">Gao et al., 2018;</ref><ref type=""bibr"" target=""#b21"">Nijkamp et al., 2019)</ref> are not applicable when the input is d",0
"the pretrained locally normalized language model can be seen as a fixed generator, like in <ref type=""bibr"" target=""#b1"">Bakhtin et al. (2019)</ref>. <ref type=""bibr"" target=""#b0"">Azadi et al. (2018)</ref> also share our same goal but their generator is not locally normalized and they propose to imp",0
">(Devlin et al., 2018;</ref><ref type=""bibr"" target=""#b16"">Phang et al., 2018)</ref>.</p><p>When finetuning a big, pretrained language model, dropout <ref type=""bibr"" target=""#b20"">(Srivastava et al., 2014)</ref> has been used as a regularization technique to prevent co-adaptation of neurons <ref ty p during training. If we denote the weight parameter of that neuron as w during training, then we use (1 − p)w for that weight parameter at test time <ref type=""bibr"" target=""#b20"">(Srivastava et al., 2014)</ref>. This ensures that the expected output of a neuron is the same as the actual output at where λ is a regularization coefficient. Usual weight decay of λ is equivalent to wdecay(0, λ).</p><p>Probability for Dropout and Dropconnect Dropout <ref type=""bibr"" target=""#b20"">(Srivastava et al., 2014</ref>) is a regularization technique selecting a neuron to drop with a probability of p. Dropc",1
"t(w pre ) where w pre denotes a pretrained model parameter. To validate our theoretical findings, we train a fully connected network on EMNIST Digits <ref type=""bibr"" target=""#b3"">(Cohen et al., 2017)</ref> and finetune it on MNIST. We observe that a finetuning solution of mixout(w pre ) deviates le viating the issue of catastrophic forgetting. To empirically verify this claim, we pretrain a 784-300-100-10 fully-connected network on EMNIST Digits <ref type=""bibr"" target=""#b3"">(Cohen et al., 2017)</ref>, and finetune it on MNIST. For more detailed description of the model architecture and datase",0
"ith a drop probability of p for all i.</p><p>Gaussian dropout <ref type=""bibr"" target=""#b24"">(Wang &amp; Manning, 2013)</ref> and variational dropout <ref type=""bibr"" target=""#b11"">(Kingma et al., 2015)</ref> use other random masks to improve dropout rather than Bernoulli random masks. To explain th",0
"architecture in Section 4 is a 784-300-100-10 fully connected network with a softmax output layer. For each hidden layer, we add layer normalization <ref type=""bibr"" target=""#b0"">(Ba et al., 2016)</ref> right after the ReLU <ref type=""bibr"" target=""#b15"">(Nair &amp; Hinton, 2010)</ref> nonlinearity",0
"Transfer learning has been widely used for the tasks in natural language processing (NLP) <ref type=""bibr"" target=""#b4"">(Collobert et al., 2011;</ref><ref type=""bibr"" target=""#b6"">Devlin et al., 2018;</ref><ref type=""bibr"" target=""#b27"">Yang et al., 2019;</ref><ref type=""bibr"" target=""#b13"">Liu et a 014)</ref> has been used as a regularization technique to prevent co-adaptation of neurons <ref type=""bibr"" target=""#b21"">(Vaswani et al., 2017;</ref><ref type=""bibr"" target=""#b6"">Devlin et al., 2018;</ref><ref type=""bibr"" target=""#b27"">Yang et al., 2019)</ref>. We provide a theoretical understandin org/ns/1.0""><head n=""1.1"">RELATED WORK</head><p>For large-scale pretrained language models <ref type=""bibr"" target=""#b21"">(Vaswani et al., 2017;</ref><ref type=""bibr"" target=""#b6"">Devlin et al., 2018;</ref><ref type=""bibr"" target=""#b27"">Yang et al., 2019)</ref>, dropout has been used as one of sever ang et al., 2019;</ref><ref type=""bibr"" target=""#b13"">Liu et al., 2019;</ref><ref type=""bibr"" target=""#b16"">Phang et al., 2018)</ref>. In particular, <ref type=""bibr"" target=""#b6"">Devlin et al. (2018)</ref> recently demonstrated the effectiveness of finetuning a large-scale language model pretrained 2018)</ref> have observed that it was unstable to finetune BERT LARGE on these four tasks. We use the publicly available pretrained model released by <ref type=""bibr"" target=""#b6"">Devlin et al. (2018)</ref>, ported into PyTorch by HuggingFace. <ref type=""foot"" target=""#foot_2"">3</ref> We use the lea 18)</ref>, ported into PyTorch by HuggingFace. <ref type=""foot"" target=""#foot_2"">3</ref> We use the learning setup and hyperparameters recommended by <ref type=""bibr"" target=""#b6"">Devlin et al. (2018)</ref>. We use Adam with a learning rate of 2 × 10 −5 , β 1 = 0.9, β 2 = 0.999, learning rate warmup parameter of it with N (0, 0.02 2 ). We describe our experimental setup further in Supplement C.2.</p><p>The original regularization strategy used in <ref type=""bibr"" target=""#b6"">Devlin et al. (2018)</ref> for finetuning BERT LARGE is using both dropout(0.1) and wdecay(0, 0.01) for all layers excep es unstable as p increases. In Supplement F, we demonstrate that dropout(0.1) is almost optimal for all the tasks in terms of mean dev score although <ref type=""bibr"" target=""#b6"">Devlin et al. (2018)</ref> selected it to improve the maximum dev score.</p><p>In Figure <ref type=""figure"" target=""#fig estarts when finetuning BERT LARGE with various regularization strategies on each task. For conciseness, we only show four regularization strategies; <ref type=""bibr"" target=""#b6"">Devlin et al. (2018)</ref>'s: both dropout(0.1) and wdecay(0, 0.01), <ref type=""bibr"" target=""#b26"">Wiese et al. (2017)< ARGE has 24 layers, 1024 hidden size, and 16 self-attention heads (total 340M parameters). We use the publicly available pretrained model released by <ref type=""bibr"" target=""#b6"">Devlin et al. (2018)</ref>, ported into PyTorch by HuggingFace. <ref type=""foot"" target=""#foot_6"">7</ref> We initialize each weight parameter and bias for an additional output layer with N (0, 0.02 2 ) and 0, respectively.</p><p>Regularization In the finetuning stage, <ref type=""bibr"" target=""#b6"">Devlin et al. (2018)</ref> used wdecay(0, 0.01) for all parameters except bias and layer normalization. They apply dropo each task. The first row shows the test scores obtained by using both dropout(p) and wdecay(0, 0.01). These results in the first row are reported by <ref type=""bibr"" target=""#b6"">Devlin et al. (2018)</ref>. They used the learning rate of {2 × 10 −5 , 3 × 10 −5 , 4 × 10 −5 , 5 × 10 −5 } and a batch d the best model on each dev set. For all the tasks except MRPC, the test scores obtained by the proposed mixout 10 are better than those reported by <ref type=""bibr"" target=""#b6"">Devlin et al. (2018)</ref>. We explored the behaviour of finetuning BERT LARGE with mixout by using the learning rate of target=""#b6"">Devlin et al. (2018)</ref>. We explored the behaviour of finetuning BERT LARGE with mixout by using the learning rate of 2 × 10 −5 while <ref type=""bibr"" target=""#b6"">Devlin et al. (2018)</ref> obtained their results by using the learning rate of {2 × 10 −5 , 3 × 10 −5 , 4 × 10 −5 , 5 × ing the learning rate of {2 × 10 −5 , 3 × 10 −5 , 4 × 10 −5 , 5 × 10 −5 }. We thus present the test scores obtained by the regularization strategy of <ref type=""bibr"" target=""#b6"">Devlin et al. (2018)</ref> when the learning rate is 2 × 10 −5 . The results in this section show that the best model on netune with the proposed mixout although this requires additional time usage compared to dropout.</p><p>F EXTENSIVE HYPERPARAMETER SEARCH FOR DROPOUT <ref type=""bibr"" target=""#b6"">Devlin et al. (2018)</ref> finetuned BERT LARGE with dropout(0.1) on all GLUE <ref type=""bibr"" target=""#b23"">(Wang et al rks the best within each column. The proposed mixout improves the test scores except MRPC compared to the original regularization strategy proposed by<ref type=""bibr"" target=""#b6"">Devlin et al. (2018)</ref>.</figDesc><table><row><cell>STRATEGY</cell><cell cols=""4"">RTE MRPC CoLA STS-B</cell></row><ro LARGE outperforms BERT BASE generally, it was observed that finetuning sometimes fails when a target dataset has fewer than 10,000 training instances <ref type=""bibr"" target=""#b6"">(Devlin et al., 2018;</ref><ref type=""bibr"" target=""#b16"">Phang et al., 2018)</ref>.</p><p>When finetuning a big, pretra guage model such as BERT LARGE on a small training set frequently fails, the final model performance has often been reported as the maximum dev score <ref type=""bibr"" target=""#b6"">(Devlin et al., 2018;</ref><ref type=""bibr"" target=""#b16"">Phang et al., 2018)</ref> among a few random restarts. We thus e proposed mixout in the case of a larger finetuning set. Since it has been stable to finetune BERT LARGE on a sufficient number of training examples <ref type=""bibr"" target=""#b6"">(Devlin et al., 2018;</ref><ref type=""bibr"" target=""#b16"">Phang et al., 2018)</ref>, we expect to see the change in the dataset has 280,000 characters into 10 balanced classes. Model Architecture Because the model architecture of BERT LARGE is identical to the original <ref type=""bibr"" target=""#b6"">(Devlin et al., 2018)</ref>, we omit its exhaustive description. Briefly, BERT LARGE has 24 layers, 1024 hidden size, an",0
"=""#b8"">(Hamilton et al., 2017)</ref>, the layer-wise approach <ref type=""bibr"" target=""#b2"">(Chen et al., 2018)</ref> and its layer-dependent variant <ref type=""bibr"" target=""#b13"">(Huang et al., 2018)</ref>. Specifically, GAT <ref type=""bibr"" target=""#b29"">(Velickovic et al., 2018)</ref> has discus ng GraphSAGE <ref type=""bibr"" target=""#b8"">(Hamilton et al., 2017)</ref>, FastGCN <ref type=""bibr"" target=""#b2"">(Chen et al., 2018)</ref>, and AS-GCN <ref type=""bibr"" target=""#b13"">(Huang et al., 2018)</ref>. We name this category of approaches as DropNode. For its original motivation, DropNode samp cluding GCN, FastGCN, AS-GCN and GraphSAGE in Table <ref type=""table"" target=""#tab_1"">2</ref>; for the SOTA methods, we reuse the results reported in <ref type=""bibr"" target=""#b13"">Huang et al. (2018)</ref>.</p><p>We have these findings: (1) Clearly, our DropEdge obtains significant enhancement agai aining, while the task in Reddit is inductive meaning the testing nodes are unseen for training. We apply the full-supervised training fashion used in<ref type=""bibr"" target=""#b13"">Huang et al. (2018)</ref> and<ref type=""bibr"" target=""#b2"">Chen et al. (2018)</ref> on all datasets in our experiments.",1
"3"">Defferrard et al. (2016)</ref>; <ref type=""bibr"" target=""#b10"">Henaff et al. (2015)</ref>; <ref type=""bibr"" target=""#b19"">Li et al. (2018b)</ref>; <ref type=""bibr"" target=""#b16"">Levie et al. (2017)</ref> apply improvements, extensions, and approximations on spectralbased GCNs. To address the scal",0
"umber of methods come redefining the notion of convolution on graphs under the umbrella of GCNs. The first prominent research on GCNs is presented in <ref type=""bibr"" target=""#b1"">Bruna et al. (2013)</ref>, which develops graph convolution based on spectral graph theory. Later, Kipf &amp; Welling (2",0
"eme, makes training a very deep GCN difficult. As first introduced by <ref type=""bibr"" target=""#b18"">Li et al. (2018a)</ref> and further explained in <ref type=""bibr"" target=""#b31"">Wu et al. (2019)</ref>; <ref type=""bibr"" target=""#b32"">Xu et al. (2018a)</ref>; <ref type=""bibr"" target=""#b15"">Klicpera",0
"et=""#b14"">(Kipf &amp; Welling, 2017;</ref><ref type=""bibr"" target=""#b18"">Li et al., 2018a;</ref><ref type=""bibr"" target=""#b32"">Xu et al., 2018a;</ref><ref type=""bibr"" target=""#b17"">Li et al., 2019)</ref>; nevertheless, none of them delivers sufficiently expressive architecture. The motivation of thi gure"">2</ref> Implementations We consider five backbones: GCN (Kipf &amp; Welling, 2017), ResGCN <ref type=""bibr"" target=""#b9"">(He et al., 2016;</ref><ref type=""bibr"" target=""#b17"">Li et al., 2019)</ref>, JKNet <ref type=""bibr"" target=""#b32"">(Xu et al., 2018a)</ref>, IncepGCN<ref type=""foot"" target= t three popular backbones recasted from image classification. They are residual network (ResGCN) <ref type=""bibr"" target=""#b9"">(He et al., 2016;</ref><ref type=""bibr"" target=""#b17"">Li et al., 2019)</ref>, inception network (IncepGCN) <ref type=""bibr"" target=""#b28"">(Szegedy et al., 2016)</ref> and de e the performance of various popular backbone networks on several benchmarks, including <ref type=""bibr"">GCN (Kipf &amp; Welling, 2017)</ref>, ResGCN <ref type=""bibr"" target=""#b17"">(Li et al., 2019)</ref>, JKNet <ref type=""bibr"" target=""#b32"">(Xu et al., 2018a)</ref>, and GraphSAGE <ref type=""bibr"" ropEdge can impede over-smoothing is based on the concepts proposed by <ref type=""bibr"" target=""#b24"">Oono &amp; Suzuki (2019)</ref>. A recent method <ref type=""bibr"" target=""#b17"">(Li et al., 2019)</ref> has incorporated residual layers, dense connections and dilated convolutions into GCNs to facil",0
"n graph size and feature type:   Figure <ref type=""figure"">2</ref> Implementations We consider five backbones: GCN (Kipf &amp; Welling, 2017), ResGCN <ref type=""bibr"" target=""#b9"">(He et al., 2016;</ref><ref type=""bibr"" target=""#b17"">Li et al., 2019)</ref>, JKNet <ref type=""bibr"" target=""#b32"">(Xu e he CNN layer with graph convolution layer to implement three popular backbones recasted from image classification. They are residual network (ResGCN) <ref type=""bibr"" target=""#b9"">(He et al., 2016;</ref><ref type=""bibr"" target=""#b17"">Li et al., 2019)</ref>, inception network (IncepGCN) <ref type=""bi",0
"ibr"" target=""#b30"">(Wang et al., 2019)</ref>. Self Feature Modeling We also implement a variant of graph convolution layer with self feature modeling <ref type=""bibr"" target=""#b5"">(Fout et al., 2017)</ref>:</p><formula xml:id=""formula_28"">H (l+1) = σ ÂH (l) W (l) + H (l) W (l) self ,<label>(9)</labe",0
"get=""#b17"">Li et al., 2019)</ref>, inception network (IncepGCN) <ref type=""bibr"" target=""#b28"">(Szegedy et al., 2016)</ref> and dense network (JKNet) <ref type=""bibr"" target=""#b12"">(Huang et al., 2017;</ref><ref type=""bibr"" target=""#b33"">Xu et al., 2018b)</ref>. Figure <ref type=""figure"" target=""#fi",0
"rties of effective resistance, the effective resistance can only increase if one edge that not connected to either s or t is removed from the circuit <ref type=""bibr"" target=""#b7"">Ghosh et al. (2008)</ref>. It means that according to Inequality 8, the conductance of the graph can only decrease if on",0
"2"">CROSS-LAYER PARAMETER SHARING</head><p>The idea of sharing parameters across layers has been previously explored with the Transformer architecture <ref type=""bibr"" target=""#b53"">(Vaswani et al., 2017)</ref>, but this prior work has focused on training for standard encoderdecoder tasks rather than 0""><head n=""3.1"">MODEL ARCHITECTURE CHOICES</head><p>The backbone of the ALBERT architecture is similar to BERT in that it uses a transformer encoder <ref type=""bibr"" target=""#b53"">(Vaswani et al., 2017)</ref> with GELU nonlinearities <ref type=""bibr"" target=""#b22"">(Hendrycks &amp; Gimpel, 2016)</re",1
"mbedding and the output embedding of a certain layer stay the same. Our observations show that our embeddings are oscillating rather than converging. <ref type=""bibr"" target=""#b21"">Hao et al. (2019)</ref> combine a parameter-sharing transformer with the standard one, which further increases the numb",0
"prior work has focused on training for standard encoderdecoder tasks rather than the pretraining/finetuning setting. Different from our observations, <ref type=""bibr"" target=""#b11"">Dehghani et al. (2018)</ref> show that networks with cross-layer parameter sharing (Universal Transformer, UT) get bett o the best-performing ALBERT configurations.</p></div><figure xmlns=""http://www.tei-c.org/ns/1.0"" xml:id=""fig_0""><head></head><label></label><figDesc><ref type=""bibr"" target=""#b11"">Dehghani et al. (2018)</ref> (Universal Transformer, UT) and Bai et al. (2019) (Deep Equilibrium Models, DQE) for Trans",0
"t=""#b47"">(Shen et al., 2018</ref>). An orthogonal line of research, which could provide additional representation power, includes hard example mining <ref type=""bibr"" target=""#b37"">(Mikolov et al., 2013)</ref> and more efficient language modeling training <ref type=""bibr"" target=""#b59"">(Yang et al.,",0
"g</head><p>We evaluated models on two datasets: New York Times (NYT) <ref type=""bibr"" target=""#b13"">(Riedel, Yao, and McCallum 2010)</ref> and WebNLG <ref type=""bibr"" target=""#b6"">(Gardent et al. 2017)</ref>. NYT comes from the distant supervised relation extraction task (DSRE), which aims to levera on problem, which extracts triplets by a Seq2Seq framework <ref type=""bibr"" target=""#b15"">(Sutskever, Vinyals, and Le 2014)</ref> with copy mechanism <ref type=""bibr"" target=""#b6"">(Gu et al. 2016)</ref>. But it cannot predict the entire entities. In addition, the weak performance hinders it from rea",1
"to automatically learn triplets (relation, head, tail) from the unstructured text without human intervention.</p><p>Early studies use pipeline models <ref type=""bibr"" target=""#b11"">(Nadeau and Sekine 2007;</ref><ref type=""bibr"" target=""#b2"">Chan and Roth 2011)</ref>, where they cast the relation ext cance to many NLP tasks. In recent years, there have been four mainstream methods.</p><p>Pipeline methods: Previous works mainly use pipeline methods <ref type=""bibr"" target=""#b11"">(Nadeau and Sekine 2007)</ref>, a.k.a extract entities first then classify the relations. Most of the recent neural mod",0
"quence-to-Sequence (Seq2Seq). Among these approaches, the table filling method <ref type=""bibr"" target=""#b7"">(Gupta, Schütze, and Andrassy 2016;</ref><ref type=""bibr"" target=""#b0"">Adel and Schütze 2017)</ref> requires the model to enumerate over all possible entity pairs, which leads to a heavy comp",0
"suffers from the overlapping relation problem that the model cannot assign different relation tags to one token. To solve this problem, the followers <ref type=""bibr"" target=""#b16"">(Takanobu et al. 2018;</ref><ref type=""bibr"" target=""#b4"">Dai et al. 2019</ref>) run tagging on a sentence for multiple not assign one token with multiple labels. To solve it, multi-pass tagging training, HRL<ref type=""foot"" target=""#foot_4"">5</ref> , has been purposed <ref type=""bibr"" target=""#b16"">(Takanobu et al. 2018)</ref>, based on the reinforcement learning framework and <ref type=""bibr"" target=""#b4"">(Dai et a",0
"target=""#b19"">(Zeng et al. 2018</ref>) is another method for solving the overlapping relation problem, which extracts triplets by a Seq2Seq framework <ref type=""bibr"" target=""#b15"">(Sutskever, Vinyals, and Le 2014)</ref> with copy mechanism <ref type=""bibr"" target=""#b6"">(Gu et al. 2016)</ref>. But i",0
"type=""bibr"" target=""#b11"">(Li et al. 2017)</ref>. Some following literature modifies the framework for the purpose of the adversarial training. IRGAN <ref type=""bibr"" target=""#b18"">(Wang et al. 2017)</ref>  </p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>Preliminaries Problem Formulation</ ive model, which uses top-relevant papers as positive samples iteratively. And to make the generative module aware of relation information, following <ref type=""bibr"" target=""#b18"">(Wang et al. 2018a)</ref>, we design a random walk based generating strategy. Given a paper p k , we design two modules to support the researches which study the author name disambiguation task using content information, we construct a new dataset collected from AceKG <ref type=""bibr"" target=""#b18"">(Wang et al. 2018b</ref>). The benchmark dataset consists of 130,655 papers from 17,816 distinguished authors. Each sam",1
"etwork becomes the critical part of these methods, e.g., paper network <ref type=""bibr"" target=""#b21"">(Zhang et al. 2018)</ref>, paper-author network <ref type=""bibr"" target=""#b20"">(Zhang and Al Hasan 2017)</ref>. However, either complicated feature engineering or the supervision <ref type=""bibr"" ta",1
"he network are likely to be written by the same author. Thus constructing the network becomes the critical part of these methods, e.g., paper network <ref type=""bibr"" target=""#b21"">(Zhang et al. 2018)</ref>, paper-author network <ref type=""bibr"" target=""#b20"">(Zhang and Al Hasan 2017)</ref>. However wo real-world author name disambiguation datasets for experiments: • AMiner-AND<ref type=""foot"" target=""#foot_0"">1</ref> . The dataset is released by <ref type=""bibr"" target=""#b21"">(Zhang et al. 2018)</ref>, which contains 500 author names for training and 100 author names for testing. We construct per-author network <ref type=""bibr"" target=""#b20"">(Zhang and Al Hasan 2017)</ref>. However, either complicated feature engineering or the supervision <ref type=""bibr"" target=""#b21"">(Zhang et al. 2018</ref>) is required.</p><p>The two categories of methods are like the two sides of the same coin. The ilarity between each pair of papers using the carefully designed pairwise features, including author names, titles, institute names etc.</p><p>AMiner <ref type=""bibr"" target=""#b21"">(Zhang et al. 2018</ref>): This model designs a supervised global stage to fine-tune the word2vec result, and designs a -AND and AceKG-AND. In the experiment on AMiner-AND, we use 100 names for testing and compare the result with the results of other models reported in <ref type=""bibr"" target=""#b21"">(Zhang et al. 2018</ref>). In the experiment on AceKG-AND, we sample 85 names for testing. Since Louppe et al. and AMin",1
"ution, e.g., image <ref type=""bibr"" target=""#b2"">(Denton et al. 2015)</ref>, sequence <ref type=""bibr"" target=""#b19"">(Yu et al. 2017)</ref>, dialogue <ref type=""bibr"" target=""#b11"">(Li et al. 2017)</ref>. Some following literature modifies the framework for the purpose of the adversarial training. I",0
"the high-order connections among papers. Methods focusing on relation information <ref type=""bibr"" target=""#b9"">(Kanani, McCallum, and Pal 2007;</ref><ref type=""bibr"" target=""#b0"">Bekkerman and McCallum 2005)</ref> usually solve the problem on the bibliographic network, where the relation informatio",0
"ve the problem in a local way, which means that they cannot measure the high-order connections among papers. Methods focusing on relation information <ref type=""bibr"" target=""#b9"">(Kanani, McCallum, and Pal 2007;</ref><ref type=""bibr"" target=""#b0"">Bekkerman and McCallum 2005)</ref> usually solve the",0
". 2014</ref>) has attracted a great deal of attention. Original purpose of GAN is to generate data from the underlying true distribution, e.g., image <ref type=""bibr"" target=""#b2"">(Denton et al. 2015)</ref>, sequence <ref type=""bibr"" target=""#b19"">(Yu et al. 2017)</ref>, dialogue <ref type=""bibr"" ta",0
"pose of GAN is to generate data from the underlying true distribution, e.g., image <ref type=""bibr"" target=""#b2"">(Denton et al. 2015)</ref>, sequence <ref type=""bibr"" target=""#b19"">(Yu et al. 2017)</ref>, dialogue <ref type=""bibr"" target=""#b11"">(Li et al. 2017)</ref>. Some following literature modif",0
"irst-order and second-order network structures. Some literature explores NRL on heterogeneous networks <ref type=""bibr"">(Tang, Qu, and Mei 2015;</ref><ref type=""bibr"" target=""#b3"">Dong, Chawla, and Swami 2017)</ref>. However, existing algorithms are designed to preserve the topology information of t",0
"ractical scenarios, e.g., scholar searching, influence evaluating and mentor recommendation, which raises the necessity of author name disambiguation <ref type=""bibr"" target=""#b14"">(Smalheiser and Torvik 2009)</ref>.</p><p>Author name disambiguation is to split the papers under the same name into se ether the papers are selected by the generator, the generator is guided to find the rules to select the homogeneous papers. To update θ G , we follow <ref type=""bibr"" target=""#b14"">(Schulman et al. 2015)</ref> to compute the gradient of V (G, D) by policy gradient:</p><formula xml:id=""formula_10"">∇",0
"ectures, for this work we opted for a simple but still very effective Bag of Embeddings model <ref type=""bibr"" target=""#b45"">(White et al. 2015;</ref><ref type=""bibr"" target=""#b0"">Arora, Liang, and Ma 2017)</ref> showing that, even in this case, A related field is differentiable interpreters-program",1
"combining neural models and symbolic reasoning, given their complementary strengths and weaknesses <ref type=""bibr"">(d'Avila Garcez et al. 2015;</ref><ref type=""bibr"" target=""#b39"">Rocktäschel and Riedel 2017;</ref><ref type=""bibr"" target=""#b47"">Yang, Yang, and Cohen 2017;</ref><ref type=""bibr"">Evan ram interpreters where declarative or procedural knowledge is compiled into a neural network architecture <ref type=""bibr"">(Bošnjak et al. 2017;</ref><ref type=""bibr"" target=""#b39"">Rocktäschel and Riedel 2017;</ref><ref type=""bibr"">Evans and Grefenstette 2018)</ref>. This family of models allows imp ained via backpropagation while maintaining interpretability and generalisation, thereby inheriting the best of both worlds. Among such systems, NTPs <ref type=""bibr"" target=""#b39"">(Rocktäschel and Riedel 2017;</ref><ref type=""bibr"" target=""#b32"">Minervini et al. 2018</ref>) are end-to-end different end-to-end differentiable reading component.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>End-to-end Differentiable Proving</head><p>NTPs <ref type=""bibr"" target=""#b39"">(Rocktäschel and Riedel 2017)</ref> recursively build a neural network enumerating all the possible proof paths for pro ncrease of the sub-goals to prove, both because all atoms in the body need to be proven, and because Z is a free variable with many possible bindings <ref type=""bibr"" target=""#b39"">(Rocktäschel and Riedel 2017)</ref>. We consider two problems -given a sub-goal G such as [p, A, B], we need to efficie (θ) = − F :-[]∈K log ntp K\F θ (F, d) − F∼corrupt(F) log[1 − ntp K θ ( F, d)]<label>(4)</label></formula><p>NTPs can also learn interpretable rules. <ref type=""bibr"" target=""#b39"">Rocktäschel and Riedel (2017)</ref> show that it is possible to learn rules from data by specifying rule templates, suc P (G) of G.</p><p>Learning to Attend Over Predicates. Although NTPs can be used for learning interpretable rules from data, the solution proposed by <ref type=""bibr"" target=""#b39"">Rocktäschel and Riedel (2017)</ref> can be quite inefficient, as the number of parameters associated to rules can be qu Trouillon 2015)</ref>, Nations, UMLS, and Kinship <ref type=""bibr"" target=""#b21"">(Kemp et al. 2006</ref>) -following the same evaluation protocols as <ref type=""bibr"" target=""#b39"">Rocktäschel and Riedel (2017)</ref>. Furthermore, since GNTPs allows to experiment on significantly larger datasets, we vements by several orders of magnitude.</p><p>Link Prediction Results. We compare GNTPs and NTPs on a set of link prediction benchmarks, also used in <ref type=""bibr"" target=""#b39"">Rocktäschel and Riedel (2017)</ref>. Results, presented in Table 1, show that GNTPs achieves better or on-par results i 6529 true facts, while Nations contains 56 binary predicates, 111 unary predicates, 14 constants and 2565 true facts. We follow the protocol used by <ref type=""bibr"" target=""#b39"">Rocktäschel and Riedel (2017)</ref> and split every dataset into training, development, and test facts, with a 80%/10%/ l></row></table></figure> 			<note xmlns=""http://www.tei-c.org/ns/1.0"" place=""foot"" n=""2"" xml:id=""foot_0"">For consistency, we use the same notation as<ref type=""bibr"" target=""#b39"">Rocktäschel and Riedel (2017)</ref>.</note> 			<note xmlns=""http://www.tei-c.org/ns/1.0"" place=""foot"" n=""3"" xml:id=""foo rules with the same structure together makes allows parallel inference to be implemented very efficiently on GPU. This optimisation is also present in<ref type=""bibr"" target=""#b39"">Rocktäschel and Riedel (2017)</ref>.</note> 			<note xmlns=""http://www.tei-c.org/ns/1.0"" place=""foot"" n=""6"" xml:id=""foo be found at https://github.com/uclnlp/gntp</note> 			<note xmlns=""http://www.tei-c.org/ns/1.0"" place=""foot"" n=""7"" xml:id=""foot_5"">Results reported in<ref type=""bibr"" target=""#b39"">Rocktäschel and Riedel (2017)</ref> were calculated with an incorrect evaluation function, causing artificially better gions (e.g. WESTERN EU-ROPE, NORTH AMERICA), and 1158 facts about the neighbourhood of countries, and the location of countries and subregions. As in <ref type=""bibr"" target=""#b39"">Rocktäschel and Riedel (2017)</ref>, we randomly split countries into a training set of 204 countries (train), a develo",1
"ementary strengths and weaknesses <ref type=""bibr"">(d'Avila Garcez et al. 2015;</ref><ref type=""bibr"" target=""#b39"">Rocktäschel and Riedel 2017;</ref><ref type=""bibr"" target=""#b47"">Yang, Yang, and Cohen 2017;</ref><ref type=""bibr"">Evans and Grefenstette 2018;</ref><ref type=""bibr"" target=""#b43"">Webe ""bibr"" target=""#b34"">(Nickel, Jiang, and Tresp 2014;</ref><ref type=""bibr"" target=""#b30"">Minervini et al. 2016)</ref>. Lastly, our work is related to <ref type=""bibr"" target=""#b47"">Yang, Yang, and Cohen (2017)</ref>, a scalable rule induction approach for KB completion, but has not been applied to t ibr"" target=""#b8"">(Das et al. 2018)</ref>, which employs a reinforcement learning algorithm to reach answers by traversing the KB graph, and NeuralLP <ref type=""bibr"" target=""#b47"">(Yang, Yang, and Cohen 2017)</ref>, which compiles inference tasks in a sequence of differentiable operations. In addit -art models, such as ComplEx and KBLR. In García-Durán and Niepert (2018) authors report a 94.2 MRR for ComplEx and 93.6 MRR for KBLR, while NeuralLP <ref type=""bibr"" target=""#b47"">(Yang, Yang, and Cohen 2017)</ref> achieves 94.0, with hits@10 equal to 94.5. GNTP achieves <ref type=""bibr"">94.2 MRR a ure xmlns=""http://www.tei-c.org/ns/1.0"" type=""table"" xml:id=""tab_5""><head>Table 1 :</head><label>1</label><figDesc>Comparison of GNTPs, NTPs, NeuralLP<ref type=""bibr"" target=""#b47"">(Yang, Yang, and Cohen 2017)</ref>, and MINERVA<ref type=""bibr"" target=""#b8"">(Das et al. 2018</ref>) (from<ref type=""bi",0
"nd systems with the ability to read text, extract meaningful knowledge, and reason with it <ref type=""bibr"">(Etzioni, Banko, and Cafarella 2006;</ref><ref type=""bibr"" target=""#b16"">Hermann et al. 2015;</ref><ref type=""bibr"" target=""#b44"">Weston et al. 2015;</ref><ref type=""bibr"" target=""#b7"">Das et",0
"ney and Manning 2007), Semantic Parsing <ref type=""bibr"" target=""#b2"">(Bos 2008)</ref>, Natural Language Inference and Recognising Textual Entailment <ref type=""bibr"" target=""#b10"">(Fyodorov, Winter, and Francez 2000;</ref><ref type=""bibr"" target=""#b6"">Bowman et al. 2015)</ref>, and Question Answeri",0
"://www.tei-c.org/ns/1.0""><head>Introduction</head><p>The main focus of Artificial Intelligence is building systems that exhibit intelligent behaviour <ref type=""bibr"" target=""#b25"">(Levesque 2014)</ref>. Notably, Natural Language Understanding (NLU) and Machine Reading (MR) aim at building models an",0
"of b, a was a neighboring state to b   Nations and UMLS Furthermore, we consider the Nations, and the Unified Medical Language System (UMLS) datasets <ref type=""bibr"" target=""#b23"">(Kok and Domingos 2007)</ref>. UMLS contains 49 predicates, 135 constants and 6529 true facts, while Nations contains 5",0
"//www.tei-c.org/ns/1.0""><head>Hyper-parameters</head><p>For each experiment, the best hyperparameters were selected via cross-validation. We use Adam <ref type=""bibr"" target=""#b22"">(Kingma and Ba 2015)</ref> for minimising the loss function in Eq. 4. We searched for the best learning rates in {0.001",0
"et al. 2018)</ref>. However, the compiled KBs tend to be incomplete, ambiguous, and noisy, impairing the application of standard deductive reasoners <ref type=""bibr"" target=""#b17"">(Huang, van Harmelen, and ten Teije 2005)</ref>.</p><p>A rich and broad literature in MR has approached this problem wi",0
"dels are robust to noise and ambiguity but not easily interpretable, making them unable to provide explanations or incorporating background knowledge <ref type=""bibr"" target=""#b15"">(Guidotti et al. 2018)</ref>.</p><p>Recent work in neuro-symbolic systems has made progress towards end-to-end differen Link prediction results on the Test-I, Test-II and Test-ALL on FB122. Note that KALE, ASR methods, and KBLR have access to a set of rules provided by <ref type=""bibr"" target=""#b15"">Guo et al. (2016)</ref>, while neural link predictors and GNTPs do not. Test-II (6,186 triples) denotes a subset of FB1 d>WordNet and Freebase</head><p>We also evaluate the proposed method on WordNet (WN18) and Freebase (FB122) jointly with the set of rules released by <ref type=""bibr"" target=""#b15"">Guo et al. (2016)</ref>. WordNet <ref type=""bibr"" target=""#b29"">(Miller 1995</ref>) is a lexical knowledge base for the entities, 122 relation types, and 112,476 triples.</p><p>For both data sets, we used the fixed training, validation, test sets and rules provided by <ref type=""bibr"" target=""#b15"">Guo et al. (2016)</ref>; a subset of the rules is shown in Table <ref type=""table"" target=""#tab_9"">5</ref>. Note that a ble"" target=""#tab_9"">5</ref>. Note that a subset of the test triples can be inferred by deductive logic inference.</p><p>For such a reason, following <ref type=""bibr"" target=""#b15"">Guo et al. (2016)</ref>, we also partition the test set in two subsets, namely Test-I and Test-II: Test-I contains trip",0
"tion in theory.</p><p>Since it is hard to obtain a large-scale ST dataset, multitask learning <ref type=""bibr"" target=""#b32"">(Weiss et al. 2017;</ref><ref type=""bibr"" target=""#b6"">Bérard et al. 2018</ref>) and pretraining techniques <ref type=""bibr"" target=""#b3"">(Bansal et al. 2019)</ref> have been features additionally in fine-tuning, which significantly increases the learning difficulty.</p><p>• Non-pre-trained Attention Module: Previous work <ref type=""bibr"" target=""#b6"">(Bérard et al. 2018)</ref> trains attention modules for ASR, MT and ST respectively, hence, the attention module of ST d",1
"atusov, Kanthak, and <ref type=""bibr"" target=""#b22"">Ney 2005)</ref>, where the ASR output are fed into an MT system to generate target sentences. HMM <ref type=""bibr"" target=""#b15"">(Juang and Rabiner 1991)</ref> </p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""6"">Conclusion</head><p>This",0
"(ST) is essential for a wide range of scenarios: for example in emergency calls, where agents have to respond emergent requests in a foreign language <ref type=""bibr"" target=""#b24"">(Munro 2010)</ref>; or in online courses, where audiences and speakers use different languages <ref type=""bibr"" target=",0
"iv xmlns=""http://www.tei-c.org/ns/1.0""><head n=""5"">Related Work</head><p>Early works conduct ST in a pipeline manner (Ney 1999; Matusov, Kanthak, and <ref type=""bibr"" target=""#b22"">Ney 2005)</ref>, where the ASR output are fed into an MT system to generate target sentences. HMM <ref type=""bibr"" targ",0
"> 	<text xml:lang=""en""> 		<body> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>Introduction</head><p>Large-scale knowledge graphs (KGs) such as YAGO <ref type=""bibr"" target=""#b10"">(Suchanek, Kasneci, and Weikum 2007)</ref>, NELL <ref type=""bibr"" target=""#b0"">(Carlson et al. 2010), and</ref><ref typ ek, Kasneci, and Weikum 2007)</ref>, NELL <ref type=""bibr"" target=""#b0"">(Carlson et al. 2010), and</ref><ref type=""bibr"">Wikidata (Vrandečić and</ref><ref type=""bibr"" target=""#b10"">Krötzsch 2014)</ref> usually represent facts in the form of relations (edges) between (head-tail) entity pairs (nodes). r"" target=""#b0"">Bordes et al. 2013;</ref><ref type=""bibr"" target=""#b9"">Socher et al. 2013;</ref><ref type=""bibr"" target=""#b11"">Yang et al. 2015;</ref><ref type=""bibr"" target=""#b10"">Trouillon et al. 2016;</ref><ref type=""bibr"" target=""#b8"">Schlichtkrull et al. 2018;</ref><ref type=""bibr"" target=""#b0"" curs the infeasibility of previous models which assume available, sufficient training instances for all relations.</p><p>In light of the above issue, <ref type=""bibr"" target=""#b10"">Xiong et al. (2018)</ref> proposed GMatching which introduces a local neighbor encoder to learn entity embeddings. It a ef><ref type=""bibr"" target=""#b11"">Yang et al. 2015)</ref> have been proposed to learn entity embeddings by using relational information, Xiong et al. <ref type=""bibr"" target=""#b10"">(Xiong et al. 2018</ref>) demonstrated that explicitly encoding graph local structure (i.e., one-hop neighbors) can ben ,t l = [f θ (h l ) ⊕ f θ (t l )] and f (R r ), respectively. In order to measure the similarity between two vectors, we employ a recurrent processor <ref type=""bibr"" target=""#b10"">(Vinyals et al. 2016</ref>) f µ to perform multiple steps matching. The t-th process step is formulated as:</p><formula",1
"Introduction</head><p>Large-scale knowledge graphs (KGs) such as YAGO <ref type=""bibr"" target=""#b10"">(Suchanek, Kasneci, and Weikum 2007)</ref>, NELL <ref type=""bibr"" target=""#b0"">(Carlson et al. 2010), and</ref><ref type=""bibr"">Wikidata (Vrandečić and</ref><ref type=""bibr"" target=""#b10"">Krötzsch 20 their incompleteness. In order to automate the KG completion process, many work <ref type=""bibr"" target=""#b8"">(Nickel, Tresp, and Kriegel 2011;</ref><ref type=""bibr"" target=""#b0"">Bordes et al. 2013;</ref><ref type=""bibr"" target=""#b9"">Socher et al. 2013;</ref><ref type=""bibr"" target=""#b11"">Yang et a c.org/ns/1.0""><head>Encoding Heterogeneous Neighbors</head><p>Although many work <ref type=""bibr"" target=""#b8"">(Nickel, Tresp, and Kriegel 2011;</ref><ref type=""bibr"" target=""#b0"">Bordes et al. 2013;</ref><ref type=""bibr"" target=""#b11"">Yang et al. 2015)</ref> have been proposed to learn entity embed #b11"">Yang et al. 2015;</ref><ref type=""bibr"" target=""#b10"">Trouillon et al. 2016;</ref><ref type=""bibr"" target=""#b8"">Schlichtkrull et al. 2018;</ref><ref type=""bibr"" target=""#b0"">Dettmers et al. 2018</ref>) have been proposed to infer missing relations by learning existing ones. For example, RESCAL rget=""#b8"">(Nickel, Tresp, and Kriegel 2011)</ref> employs tensor factorization to capture inherent structure of multi-relational data in KGs. TransE <ref type=""bibr"" target=""#b0"">(Bordes et al. 2013)</ref> interprets relations as translation operation on the low-dimensional embeddings of entities. eractions among different entity pairs and accumulating their expression capability. Inspired by the common practices in learning sentence embeddings <ref type=""bibr"" target=""#b0"">(Conneau et al. 2017</ref>) in natural language processing and aggregating node embeddings <ref type=""bibr"" target=""#b4""",0
"dialogue state tracking and policy learning. This line of work has been facilitated by the release of multi-domain dialogue corpora such as MultiWOZ <ref type=""bibr"" target=""#b1"">(Budzianowski et al. 2018)</ref>, M2M <ref type=""bibr"" target=""#b15"">(Shah et al. 2018</ref>) and FRAMES <ref type=""bibr en et al. 2017</ref><ref type=""bibr"">), FRAMES (El Asri et al. 2017)</ref>, M2M <ref type=""bibr"" target=""#b15"">(Shah et al. 2018</ref>) and Multi-WOZ <ref type=""bibr"" target=""#b1"">(Budzianowski et al. 2018</ref>). These datasets have utilized a variety of data collection techniques, falling within t",1
"ery large and dy-namic set of possible values. Addressing these concerns, approaches utilizing a dynamic vocabulary of slot values have been proposed <ref type=""bibr"" target=""#b13"">(Rastogi, Gupta, and Hakkani-Tur 2018;</ref><ref type=""bibr"" target=""#b6"">Goel, Paul, and Hakkani-Tür 2019;</ref><ref t slots. These models, however, also need access to representations for potentially unseen inputs from new services. Recent pretrained models like ELMo <ref type=""bibr"" target=""#b13"">(Peters et al. 2018)</ref> and BERT <ref type=""bibr"" target=""#b3"">(Devlin et al. 2019</ref>) can help, since they are t",1
"b9"">(Henderson, Thomson, and Young 2014;</ref><ref type=""bibr"" target=""#b17"">Wen et al. 2017)</ref> or individually score all slot-value combinations <ref type=""bibr"" target=""#b12"">(Mrkšić et al. 2017;</ref><ref type=""bibr"" target=""#b21"">Zhong, Xiong, and Socher 2018)</ref>. Such approaches are not",0
"mantic annotations are obtained automatically.</p><p>As virtual assistants incorporate diverse domains, recent work has focused on zero-shot modeling <ref type=""bibr"" target=""#b0"">(Bapna et al. 2017;</ref><ref type=""bibr"" target=""#b20"">Xia et al. 2018;</ref><ref type=""bibr"" target=""#b16"">Shah et al.",0
"br"" target=""#b7"">(Hemphill, Godfrey, and Doddington 1990)</ref> for spoken language understanding for flights. The Dialogue State Tracking Challenges <ref type=""bibr"" target=""#b18"">(Williams et al. 2013;</ref><ref type=""bibr"">Henderson, Thomson, and Williams 2014a;</ref><ref type=""bibr"" target=""#b8""",0
"10"">Kim et al. 2017</ref>) contributed to the creation of dialogue datasets with increasing complexity. Other notable related datasets include WOZ2.0 <ref type=""bibr"" target=""#b17"">(Wen et al. 2017</ref><ref type=""bibr"">), FRAMES (El Asri et al. 2017)</ref>, M2M <ref type=""bibr"" target=""#b15"">(Shah s estimate the dialogue state as a distribution over all possible slot-values <ref type=""bibr"" target=""#b9"">(Henderson, Thomson, and Young 2014;</ref><ref type=""bibr"" target=""#b17"">Wen et al. 2017)</ref> or individually score all slot-value combinations <ref type=""bibr"" target=""#b12"">(Mrkšić et al.",0
"pproaches utilizing a dynamic vocabulary of slot values have been proposed <ref type=""bibr"" target=""#b13"">(Rastogi, Gupta, and Hakkani-Tur 2018;</ref><ref type=""bibr"" target=""#b6"">Goel, Paul, and Hakkani-Tür 2019;</ref><ref type=""bibr"" target=""#b19"">Wu et al. 2019</ref>).</p></div> <div xmlns=""http:",0
"do not sufficiently capture a number of challenges that arise with scaling virtual assistants in production. These assistants need to support a large <ref type=""bibr"" target=""#b11"">(Kim et al. 2018)</ref>, constantly increasing number of services over a large number of domains. In comparison, existi",0
"virtual assistants incorporate diverse domains, recent work has focused on zero-shot modeling <ref type=""bibr"" target=""#b0"">(Bapna et al. 2017;</ref><ref type=""bibr"" target=""#b20"">Xia et al. 2018;</ref><ref type=""bibr"" target=""#b16"">Shah et al. 2019)</ref>, domain adaptation and transfer learning t",0
". 2013;</ref><ref type=""bibr"">Henderson, Thomson, and Williams 2014a;</ref><ref type=""bibr"" target=""#b8"">Henderson, Thomson, and Williams 2014b;</ref><ref type=""bibr"" target=""#b10"">Kim et al. 2017</ref>) contributed to the creation of dialogue datasets with increasing complexity. Other notable relat",0
"ns/1.0""><head n=""1."">Introduction</head><p>Human identification by the uniqueness of every individual is an indispensable part of human life nowadays <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref>. However, because of the lack of liveness check, traditional biometric r",1
"ted the feasibility of human identification based on ECG signals. Furthermore, Kyoso et al. <ref type=""bibr"" target=""#b21"">[22]</ref> and Page et al. <ref type=""bibr"" target=""#b22"">[23]</ref> have shown the availability of human identification by one-lead ECG signals. Although the less information v res and improve the generalization ability of learners. However, there are few literatures for ECG biometrics using the learned features. Page et al. <ref type=""bibr"" target=""#b22"">[23]</ref> proposed to utilize neural networks (NN) for embedded ECG-based biometric authentication system. Neural netw sed biometric authentication system. Neural networks were used for QRS detection and user identification.</p><p>However, the shallow networks used in <ref type=""bibr"" target=""#b22"">[23]</ref> cannot provide abundant features. Convolutional neural networks (CNN) have drawn much attention on various v",0
"ype=""bibr"" target=""#b38"">[39,</ref><ref type=""bibr"" target=""#b39"">40]</ref>are favored in the literatures. Based on the fiducial points, Safie et al. <ref type=""bibr"" target=""#b12"">[13]</ref> adopted pulse active ratio as the features for one-to-one verification. Louis et al. <ref type=""bibr"" target",0
"acy of the detection, especially for one-lead ECG signals with poor quality. For nonfiducial features, they are subdivided into auto-correction based <ref type=""bibr"" target=""#b41"">[42,</ref><ref type=""bibr"" target=""#b32"">33]</ref>, phase space based <ref type=""bibr"" target=""#b42"">[43,</ref><ref typ",0
"ef><ref type=""bibr"" target=""#b25"">26]</ref>, amplitude features of various peaks <ref type=""bibr"" target=""#b23"">[24]</ref> and morphological features <ref type=""bibr"" target=""#b38"">[39,</ref><ref type=""bibr"" target=""#b39"">40]</ref>are favored in the literatures. Based on the fiducial points, Safie e",0
"ted an acute need for highly efficient image processing pipeline implementations.</p><p>In recent years, the Halide image processing language [Ragan- <ref type=""bibr"" target=""#b12"">Kelley et al. 2012;</ref><ref type=""bibr"" target=""#b12"">Ragan-Kelley et al. 2013</ref>] has proven to be an effective s pipeline implementations.</p><p>In recent years, the Halide image processing language [Ragan- <ref type=""bibr"" target=""#b12"">Kelley et al. 2012;</ref><ref type=""bibr"" target=""#b12"">Ragan-Kelley et al. 2013</ref>] has proven to be an effective system for authoring high-performance image processing co ior Work</head><p>There have been a number of recent efforts to automatically generate efficient image processing pipelines from high-level programs. <ref type=""bibr"" target=""#b12"">Ragan-Kelley et al. [2013]</ref> employed auto-tuning guided by genetic search to automatically generate Halide schedul cturing decisions made by Halide developers when authoring efficient schedules. We assume familiarity with the Halide system, and refer the reader to <ref type=""bibr"" target=""#b12"">[Ragan-Kelley et al. 2012;</ref><ref type=""bibr"" target=""#b12"">Ragan-Kelley et al. 2013]</ref> for a comprehensive desc span a range of computational photography, image processing, and computer vision workloads. Eight of the benchmarks are drawn from public literature <ref type=""bibr"" target=""#b12"">[Ragan-Kelley et al. 2012;</ref><ref type=""bibr"" target=""#b12"">Ragan-Kelley et al. 2013;</ref><ref type=""bibr"" target="" cient schedules. We assume familiarity with the Halide system, and refer the reader to <ref type=""bibr"" target=""#b12"">[Ragan-Kelley et al. 2012;</ref><ref type=""bibr"" target=""#b12"">Ragan-Kelley et al. 2013]</ref> for a comprehensive description of the language and its features.</p><p>A Halide progra nd computer vision workloads. Eight of the benchmarks are drawn from public literature <ref type=""bibr"" target=""#b12"">[Ragan-Kelley et al. 2012;</ref><ref type=""bibr"" target=""#b12"">Ragan-Kelley et al. 2013;</ref><ref type=""bibr"" target=""#b13"">Ragan-Kelley et al. 2015]</ref> and the Halide open sourc",1
"e=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b21"">22]</ref>, there are several attempts to adopt GNNs to learn with heterogeneous networks <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b26 .0""><head n=""2.3"">Heterogeneous GNNs</head><p>Recently, studies have attempted to extend GNNs for modeling heterogeneous graphs. Schlichtkrull et al. <ref type=""bibr"" target=""#b13"">[14]</ref> propose the relational graph convolutional networks (RGCN) to model knowledge graphs. RGCN keeps a distinct p>The second class considered is several dedicated heterogeneous GNNs as baselines, including:</p><p>• Relational Graph Convolutional Networks (RGCN) <ref type=""bibr"" target=""#b13"">[14]</ref>, which keeps a different weight for each relationship, i.e., a relation triplet. We use the implementation p ach network are exactly ‡ Unless other stated, HGT refers to HGT +RT E +H e t e r .</p><p>GNN Models GCN <ref type=""bibr"" target=""#b8"">[9]</ref> RGCN <ref type=""bibr"" target=""#b13"">[14]</ref> GAT <ref type=""bibr"" target=""#b21"">[22]</ref> HetGNN <ref type=""bibr"" target=""#b26"">[27]</ref> HAN <ref type",1
"mutual attention grounded by their meta relations, i.e., the ⟨τ (s), ϕ(e), τ (t)⟩ triplets.</p><p>Inspired by the architecture design of Transformer <ref type=""bibr"" target=""#b20"">[21]</ref>, we map target node t into a Query vector, and source node s into a Key vector, and calculate their dot prod he dynamic dependencies in heterogeneous graphs. RTE is inspired by Transformer's positional encoding method <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b20"">21]</ref>, which has been shown successful to capture the sequential dependencies of words in long texts.</p><p>Specifi",1
"horships obviously differ from citation links; Second, OAG has been consistently evolving, e.g., 1) the volume of publications doubles every 12 years <ref type=""bibr"" target=""#b3"">[4]</ref>, and 2) the KDD conference was more related to database in the 1990s whereas more to machine learning in recen",0
"same. All baselines are optimized via the AdamW optimizer <ref type=""bibr"" target=""#b12"">[13]</ref> with the Cosine Annealing Learning Rate Scheduler <ref type=""bibr"" target=""#b11"">[12]</ref>. For each model, we train it for 200 epochs and select the one with the lowest validation loss as the report",0
"roportions, since the direct usage of existing (homogeneous) GNN sampling methods, such as GraphSage <ref type=""bibr"" target=""#b6"">[7]</ref>, FastGCN <ref type=""bibr"" target=""#b0"">[1]</ref>, and LADIES <ref type=""bibr"" target=""#b28"">[29]</ref>, results in highly imbalanced ones regarding to both nod lculation of all node representations per layer, making it not scalable for Web-scale graphs. To address this issue, different sampling-based methods <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b28"">29]<",0
"=""bibr"" target=""#b17"">[18]</ref> and metapath2vec <ref type=""bibr"" target=""#b2"">[3]</ref>. Recently, in view of graph neural networks' (GNNs) success <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b21"">22]</ref>, there are several attempts to a w.tei-c.org/ns/1.0""><head n=""2.2"">Graph Neural Networks</head><p>Recent years have witnessed the success of graph neural networks for relational data <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b21"">22]</ref>. Generally, a GNN can be regarde which different types of nodes are with similar proportions, since the direct usage of existing (homogeneous) GNN sampling methods, such as GraphSage <ref type=""bibr"" target=""#b6"">[7]</ref>, FastGCN <ref type=""bibr"" target=""#b0"">[1]</ref>, and LADIES <ref type=""bibr"" target=""#b28"">[29]</ref>, result eb-scale graphs. To address this issue, different sampling-based methods <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b28"">29]</ref> have been proposed to train GNNs on a subset of nodes. However, directl",0
"pe=""bibr"" target=""#b16"">[17]</ref>. One of the classical paradigms is to define and use meta paths to model heterogeneous structures, such as PathSim <ref type=""bibr"" target=""#b17"">[18]</ref> and metapath2vec <ref type=""bibr"" target=""#b2"">[3]</ref>. Recently, in view of graph neural networks' (GNNs) as ⟨τ (s), ϕ(e), τ (t)⟩. Naturally, ϕ(e) −1 represents the inverse of ϕ(e). The classical meta path paradigm <ref type=""bibr"" target=""#b16"">[17]</ref><ref type=""bibr"" target=""#b17"">[18]</ref><ref type=""bibr"" target=""#b18"">[19]</ref> is defined as a sequence of such meta relation.</p><p>Notice that, node classification, clustering, ranking and representation learning <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b16"">[17]</ref><ref type=""bibr"" target=""#b17"">[18]</ref><ref type=""bibr"" target=""#b18"">[19]</ref>, while the dynamic perspective of HGs has not been extensively expl",0
"ent types of relationships between them.</p><p>Over the past decade, a significant line of research has been explored for mining heterogeneous graphs <ref type=""bibr"" target=""#b16"">[17]</ref>. One of the classical paradigms is to define and use meta paths to model heterogeneous structures, such as P eous graph neural networks.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2.1"">Heterogeneous Graph Mining</head><p>Heterogeneous graphs <ref type=""bibr"" target=""#b16"">[17]</ref> (a.k.a., heterogeneous information networks) are an important abstraction for modeling relational data for m get node t, its meta relation is denoted as ⟨τ (s), ϕ(e), τ (t)⟩. Naturally, ϕ(e) −1 represents the inverse of ϕ(e). The classical meta path paradigm <ref type=""bibr"" target=""#b16"">[17]</ref><ref type=""bibr"" target=""#b17"">[18]</ref><ref type=""bibr"" target=""#b18"">[19]</ref> is defined as a sequence o ch on mining heterogenous graphs, such as node classification, clustering, ranking and representation learning <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b16"">[17]</ref><ref type=""bibr"" target=""#b17"">[18]</ref><ref type=""bibr"" target=""#b18"">[19]</ref>, while the dynamic perspec",0
"th two classes of state-of-art graph neural networks. All baselines as well as our own model, are implemented via the PyTorch Geometric (PyG) package <ref type=""bibr"" target=""#b4"">[5]</ref>.</p><p>The first class of GNN baselines is designed for homogeneous graphs, including:</p><p>• Graph Convoluti",0
"type=""bibr"" target=""#b26"">[27]</ref> HAN <ref type=""bibr"" target=""#b22"">[23]</ref> HGT the same. All baselines are optimized via the AdamW optimizer <ref type=""bibr"" target=""#b12"">[13]</ref> with the Cosine Annealing Learning Rate Scheduler <ref type=""bibr"" target=""#b11"">[12]</ref>. For each model,",0
"h correctly labeled data and mislabeled data. Co-learning is motivated by the self-training algorithm <ref type=""bibr"" target=""#b52"">(Zhu, 2006;</ref><ref type=""bibr"" target=""#b22"">Li et al., 2008;</ref><ref type=""bibr"" target=""#b34"">Rosenberg et al., 2005;</ref><ref type=""bibr"" target=""#b33"">Riloff efines the labels of training samples and updates model parameters, in a way similar to self-training <ref type=""bibr"" target=""#b52"">(Zhu, 2006;</ref><ref type=""bibr"" target=""#b22"">Li et al., 2008;</ref><ref type=""bibr"" target=""#b34"">Rosenberg et al., 2005;</ref><ref type=""bibr"" target=""#b33"">Riloff",1
"bibr"" target=""#b52"">(Zhu, 2006;</ref><ref type=""bibr"" target=""#b22"">Li et al., 2008;</ref><ref type=""bibr"" target=""#b34"">Rosenberg et al., 2005;</ref><ref type=""bibr"" target=""#b33"">Riloff and Jones, 1999)</ref>, which uses the prediction of models with high confidence to produce pseudo labels for un bibr"" target=""#b52"">(Zhu, 2006;</ref><ref type=""bibr"" target=""#b22"">Li et al., 2008;</ref><ref type=""bibr"" target=""#b34"">Rosenberg et al., 2005;</ref><ref type=""bibr"" target=""#b33"">Riloff and Jones, 1999)</ref>. However, in order to prevent the deep model from overfitting to the noise easily and amp",1
"ning is motivated by the self-training algorithm <ref type=""bibr"" target=""#b52"">(Zhu, 2006;</ref><ref type=""bibr"" target=""#b22"">Li et al., 2008;</ref><ref type=""bibr"" target=""#b34"">Rosenberg et al., 2005;</ref><ref type=""bibr"" target=""#b33"">Riloff and Jones, 1999)</ref>, which uses the prediction of el parameters, in a way similar to self-training <ref type=""bibr"" target=""#b52"">(Zhu, 2006;</ref><ref type=""bibr"" target=""#b22"">Li et al., 2008;</ref><ref type=""bibr"" target=""#b34"">Rosenberg et al., 2005;</ref><ref type=""bibr"" target=""#b33"">Riloff and Jones, 1999)</ref>. However, in order to prevent",1
"d labels, such that the model could leverage both correctly labeled data and mislabeled data. Co-learning is motivated by the self-training algorithm <ref type=""bibr"" target=""#b52"">(Zhu, 2006;</ref><ref type=""bibr"" target=""#b22"">Li et al., 2008;</ref><ref type=""bibr"" target=""#b34"">Rosenberg et al., rget=""#fig_0"">1</ref>. Co-learning iteratively refines the labels of training samples and updates model parameters, in a way similar to self-training <ref type=""bibr"" target=""#b52"">(Zhu, 2006;</ref><ref type=""bibr"" target=""#b22"">Li et al., 2008;</ref><ref type=""bibr"" target=""#b34"">Rosenberg et al.,",1
"the network is modified from classical convolutional networks such as ResNets <ref type=""bibr"" target=""#b9"">(He et al., 2016)</ref> and Wide ResNets <ref type=""bibr"" target=""#b48"">(Zagoruyko and Komodakis, 2016)</ref>. We simply duplicate the final convolutional group (conv4) to build the two branc al., 2014)</ref>. For b6), we use hyper-parameters recommended by <ref type=""bibr"">Han et al. (2018)</ref>.</p><p>Training details. Wide-ResNet (WRN) <ref type=""bibr"" target=""#b48"">(Zagoruyko and Komodakis, 2016)</ref> and ResNet-32 <ref type=""bibr"" target=""#b9"">(He et al., 2016)</ref> are implement",0
"algorithms are also proposed to handle the label noise with probabilistic models <ref type=""bibr"" target=""#b6"">(Goldberger and Ben-Reuven, 2017;</ref><ref type=""bibr"" target=""#b15"">Khetan et al., 2018;</ref><ref type=""bibr"" target=""#b47"">Xiao et al., 2015)</ref>. In addition, recent work also models",0
"samples is also proved to be effective <ref type=""bibr"" target=""#b25"">(Lin et al., 2017;</ref><ref type=""bibr"" target=""#b13"">Jiang et al., 2018;</ref><ref type=""bibr"" target=""#b1"">Bengio et al., 2009)</ref>. From the perspective of loss functions, many robust forms are designed to reduce the effect",0
""">Chang et al., 2017)</ref>. Adding pre-defined curriculums in loss functions to dynamically adjust weights of samples is also proved to be effective <ref type=""bibr"" target=""#b25"">(Lin et al., 2017;</ref><ref type=""bibr"" target=""#b13"">Jiang et al., 2018;</ref><ref type=""bibr"" target=""#b1"">Bengio et",0
"tions when fueled by large scale datasets with precise annotations, such as ImageNet <ref type=""bibr"" target=""#b3"">(Deng et al., 2009)</ref> and COCO <ref type=""bibr"" target=""#b23"">(Lin et al., 2014)</ref>. However, collecting high-quality labeled data is usually very time-consuming and costly <ref",0
"f><ref type=""bibr"" target=""#b44"">Tavanaei et al., 2019)</ref>. In contrast, obtaining coarsely labeled data, e.g., images retrieved by search engines <ref type=""bibr"" target=""#b21"">(Li et al., 2017)</ref>, web social data, Flickr tags <ref type=""bibr"" target=""#b45"">(Vahdat, 2017)</ref>, is a relativ",0
"d label noise.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.1."">Setup Details and Baselines</head><p>Dataset. The two CIFAR datasets <ref type=""bibr"" target=""#b16"">(Krizhevsky and Hinton, 2009)</ref> consist of 32x32 colored natural images with 10 classes (CIFAR-10) and 100 classes",0
"icated to estimate this matrix <ref type=""bibr"" target=""#b29"">(Patrini et al., 2017;</ref><ref type=""bibr"" target=""#b10"">Hendrycks et al., 2018;</ref><ref type=""bibr"" target=""#b14"">Jindal et al., 2016)</ref>. As true labels can be seen as latent variables, EM-based algorithms are also proposed to ha",0
"b13"">(Jiang et al., 2018;</ref><ref type=""bibr"" target=""#b50"">Zhang and Sabuncu, 2018;</ref><ref type=""bibr"" target=""#b29"">Patrini et al., 2017;</ref><ref type=""bibr"" target=""#b42"">Tanaka et al., 2018)</ref>. The MNIST dataset consists of 32x32 images of handwritten digits with 10 classes, 60000 for",0
"br"" target=""#b31"">Ren et al., 2018;</ref><ref type=""bibr"" target=""#b18"">Kumar et al., 2010;</ref><ref type=""bibr"" target=""#b20"">Li et al., 2018;</ref><ref type=""bibr"" target=""#b13"">Jiang et al., 2018;</ref><ref type=""bibr"" target=""#b51"">Zhao et al., 2019)</ref>. Popular approaches include designing lums in loss functions to dynamically adjust weights of samples is also proved to be effective <ref type=""bibr"" target=""#b25"">(Lin et al., 2017;</ref><ref type=""bibr"" target=""#b13"">Jiang et al., 2018;</ref><ref type=""bibr"" target=""#b1"">Bengio et al., 2009)</ref>. From the perspective of loss functio amples <ref type=""bibr"" target=""#b31"">(Ren et al., 2018)</ref>, and learning with meta-learning <ref type=""bibr"" target=""#b20"">(Li et al., 2018;</ref><ref type=""bibr"" target=""#b13"">Jiang et al., 2018)</ref>. However, existing approaches mainly focus on identifying and leveraging only correctly annot ref type=""bibr"" target=""#b31"">Ren et al. (2018)</ref> propose a meta-learning method to reweight training samples based on their gradient directions. <ref type=""bibr"" target=""#b13"">Jiang et al. (2018)</ref> train an auxiliary LSTM-based MentorNet to reweight samples. Both of them need an additional viations. We adopt a standard data augmentation scheme widely-used for them: random 32x32 cropping with 4-pixel padded and random horizontal flipping <ref type=""bibr"" target=""#b13"">(Jiang et al., 2018;</ref><ref type=""bibr"" target=""#b50"">Zhang and Sabuncu, 2018;</ref><ref type=""bibr"" target=""#b29"">P al., 2018)</ref>.</p><p>Baselines. Our method is compared with several state-of-the-art methods to make DNN robust against label noise. b1) MentorNet <ref type=""bibr"" target=""#b13"">(Jiang et al., 2018)</ref>  </p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>J o u r n a l P r e -p r o o f</h the result of the last epoch. The noise rate ρ denotes the ratio of wrong labels. For self-paced learning and MentorNet, we cite results reported in <ref type=""bibr"" target=""#b13"">(Jiang et al., 2018)</ref>. For other methods, we report results of our implementations. Following <ref type=""bibr"" tar results reported in <ref type=""bibr"" target=""#b13"">(Jiang et al., 2018)</ref>. For other methods, we report results of our implementations. Following <ref type=""bibr"" target=""#b13"">(Jiang et al., 2018)</ref> For b1) and b2), we directly cite the results reported in <ref type=""bibr"" target=""#b13"">(Ji of our implementations. Following <ref type=""bibr"" target=""#b13"">(Jiang et al., 2018)</ref> For b1) and b2), we directly cite the results reported in <ref type=""bibr"" target=""#b13"">(Jiang et al., 2018)</ref>. For a fair comparison, we also use ResNets with wide filters mentioned in their paper and a l., 2018)</ref>. For a fair comparison, we also use ResNets with wide filters mentioned in their paper and apply exactly the same training details as <ref type=""bibr"" target=""#b13"">(Jiang et al., 2018)</ref>. The conv4 group of the network is duplicated as B1 and B2. For other methods, we report the he learning rate is set to 0.1 initially, and is divided by 10 after the 80 th and 120 th epoch, respectively.</p><p>Hyper-parameter. As MentorNet-DD <ref type=""bibr"" target=""#b13"">(Jiang et al., 2018)</ref> uses additional samples with accurate labels to learn data-driven curriculums, following the",0
"coarsely labeled data, e.g., images retrieved by search engines <ref type=""bibr"" target=""#b21"">(Li et al., 2017)</ref>, web social data, Flickr tags <ref type=""bibr"" target=""#b45"">(Vahdat, 2017)</ref>, is a relatively easier task. Unfortunately, DNNs are easy to overfit the label noise in the datas >. In addition, recent work also models the label noise via more complicated distributions, which takes the instances distribution into consideration <ref type=""bibr"" target=""#b45"">(Vahdat, 2017)</ref>. However, estimating the label noise distribution is usually challenging in practice, especially w",0
"37"">Simonyan and Zisserman, 2015;</ref><ref type=""bibr"" target=""#b32"">Ren et al., 2015;</ref><ref type=""bibr"" target=""#b37"">Silver et al., 2016;</ref><ref type=""bibr"" target=""#b36"">Schmidhuber, 2015)</ref>. They can learn highly generalizable representations when fueled by large scale datasets with",0
"ukhbaatar et al., 2018)</ref>, and many existing work is dedicated to estimate this matrix <ref type=""bibr"" target=""#b29"">(Patrini et al., 2017;</ref><ref type=""bibr"" target=""#b10"">Hendrycks et al., 2018;</ref><ref type=""bibr"" target=""#b14"">Jindal et al., 2016)</ref>. As true labels can be seen as l",0
"th probabilistic models <ref type=""bibr"" target=""#b6"">(Goldberger and Ben-Reuven, 2017;</ref><ref type=""bibr"" target=""#b15"">Khetan et al., 2018;</ref><ref type=""bibr"" target=""#b47"">Xiao et al., 2015)</ref>. In addition, recent work also models the label noise via more complicated distributions, whic",0
""" target=""#b5"">Ghosh et al., 2017;</ref><ref type=""bibr"" target=""#b31"">Ren et al., 2018;</ref><ref type=""bibr"" target=""#b18"">Kumar et al., 2010;</ref><ref type=""bibr"" target=""#b20"">Li et al., 2018;</ref><ref type=""bibr"" target=""#b13"">Jiang et al., 2018;</ref><ref type=""bibr"" target=""#b51"">Zhao et al target=""#b5"">(Ghosh et al., 2017)</ref>, reweighting samples <ref type=""bibr"" target=""#b31"">(Ren et al., 2018)</ref>, and learning with meta-learning <ref type=""bibr"" target=""#b20"">(Li et al., 2018;</ref><ref type=""bibr"" target=""#b13"">Jiang et al., 2018)</ref>. However, existing approaches mainly fo",0
"der> 	<text xml:lang=""en""> 		<body> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""1"">Introduction</head><p>Batch Normalization (Batch Norm or BN) <ref type=""bibr"" target=""#b0"">[1]</ref> has been established as a very effective component in deep learning, largely helping push the frontier in comp b29"">30,</ref><ref type=""bibr"" target=""#b30"">31]</ref>. LRN computes the statistics in a small neighborhood for each pixel.</p><p>Batch Normalization <ref type=""bibr"" target=""#b0"">[1]</ref> performs more global normalization along the batch dimension (and as importantly, it suggests to do this for a e to time. For example, batch-wise normalization is not legitimate at inference time, so the mean and variance are pre-computed from the training set <ref type=""bibr"" target=""#b0"">[1]</ref>, often by running average; consequently, there is no normalization performed when testing. The pre-computed st on methods mainly differ in how the set S i is defined (Figure <ref type=""figure"" target=""#fig_1"">2</ref>), discussed as follows.</p><p>In Batch Norm <ref type=""bibr"" target=""#b0"">[1]</ref>, the set S i is defined as:</p><formula xml:id=""formula_4"">S i = {k | k C = i C },<label>(3)</label></formula> , W ) axes for each sample and each channel. The relations among BN, LN, and IN are in Figure <ref type=""figure"" target=""#fig_1"">2</ref>.</p><p>As in <ref type=""bibr"" target=""#b0"">[1]</ref>, all methods of BN, LN, and IN learn a per-channel linear transform to compensate for the possible lost of rep understandable, because BN's mean and variance computation introduces uncertainty caused by the stochastic batch sampling, which helps regularization <ref type=""bibr"" target=""#b0"">[1]</ref>. This uncertainty is missing in GN (and LN/IN). But it is possible that GN combined with a suitable regularize or of normalizing along the batch dimension. In particular, it is required for BN to work with a sufficiently large batch size (e.g., 32 per worker 1 <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b2"">3]</ref>). A small batch leads to inaccurat ure"" target=""#fig_0"">1</ref>.</p><p>Comparison of feature normalization methods. We first experiment with a regular batch size of 32 images (per GPU) <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b2"">3]</ref>. BN works successfully in this regime, so this is a strong baseline to c",1
"n and batch sizes.</p><p>This paper presents Group Normalization (GN) as a simple alternative to BN. We notice that many classical features like SIFT <ref type=""bibr"" target=""#b13"">[14]</ref> and HOG <ref type=""bibr"" target=""#b14"">[15]</ref> are group-wise features and involve group-wise normalizati .org/ns/1.0""><head n=""3"">Group Normalization</head><p>The channels of visual representations are not entirely independent. Classical features of SIFT <ref type=""bibr"" target=""#b13"">[14]</ref>, HOG <ref type=""bibr"" target=""#b14"">[15]</ref>, and GIST <ref type=""bibr"" target=""#b40"">[41]</ref> are group alized together.</p><p>The higher-level layers are more abstract and their behaviors are not as intuitive. However, in addition to orientations (SIFT <ref type=""bibr"" target=""#b13"">[14]</ref>, HOG <ref type=""bibr"" target=""#b14"">[15]</ref>, or <ref type=""bibr"" target=""#b43"">[44,</ref><ref type=""bibr""",1
"Normalization (GN) as a simple alternative to BN. We notice that many classical features like SIFT <ref type=""bibr"" target=""#b13"">[14]</ref> and HOG <ref type=""bibr"" target=""#b14"">[15]</ref> are group-wise features and involve group-wise normalization. For example, a HOG vector is the outcome of se ead><p>The channels of visual representations are not entirely independent. Classical features of SIFT <ref type=""bibr"" target=""#b13"">[14]</ref>, HOG <ref type=""bibr"" target=""#b14"">[15]</ref>, and GIST <ref type=""bibr"" target=""#b40"">[41]</ref> are group-wise representations by design, where each gro are more abstract and their behaviors are not as intuitive. However, in addition to orientations (SIFT <ref type=""bibr"" target=""#b13"">[14]</ref>, HOG <ref type=""bibr"" target=""#b14"">[15]</ref>, or <ref type=""bibr"" target=""#b43"">[44,</ref><ref type=""bibr"" target=""#b44"">45]</ref>), there are many facto",1
"ze across the cell responses <ref type=""bibr"" target=""#b45"">[46,</ref><ref type=""bibr"" target=""#b46"">47,</ref><ref type=""bibr"" target=""#b47"">48,</ref><ref type=""bibr"" target=""#b48"">49]</ref>, ""with various receptive-field centers (covering the visual field) and with various spatiotemporal frequency gs"" (p183, <ref type=""bibr"" target=""#b45"">[46]</ref>); this can happen not only in the primary visual cortex, but also ""throughout the visual system"" <ref type=""bibr"" target=""#b48"">[49]</ref>. Motivated by these works, we propose new generic group-wise normalization for deep neural networks.</p><p>F",0
"N (with a gap of ∼0.5%) and outperforms other normalization variants <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b18"">19]</ref>. Moreover, although the batch size may change, GN can naturally transfer from pre-training to fine-tuning. GN the estimated batch statistics.</p><p>Several normalization methods <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" target=""#b33"">34]</ref> have been proposed to avoid e computation but only for each sample (Figure <ref type=""figure"" target=""#fig_1"">2</ref>). Instead of operating on features, Weight Normalization (WN) <ref type=""bibr"" target=""#b18"">[19]</ref> proposes to normalize the filter weights. These methods do not suffer from the issues caused by the batch di",0
"ncluding detection <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b9"">10]</ref>, segmentation <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b9"">10]</ref>, video recognition <ref type=""bibr"" target=""#b11"">[12,</ref><ref type",0
"d within each GPU and not synchronized.  All models are trained in 8 GPUs. In this set of experiments, we adopt the linear learning rate scaling rule <ref type=""bibr"" target=""#b54"">[55,</ref><ref type=""bibr"" target=""#b55"">56,</ref><ref type=""bibr"" target=""#b53"">54]</ref> to adapt to batch size chang",0
"sed before the development of BN. Local Response Normalization (LRN) <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" target=""#b27"">28]</ref> was a component in AlexNet <ref type=""bibr"" target=""#b27"">[28]</ref> and following models <ref type=""bibr"" ta <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" target=""#b27"">28]</ref> was a component in AlexNet <ref type=""bibr"" target=""#b27"">[28]</ref> and following models <ref type=""bibr"" target=""#b28"">[29,</ref><ref type=""bibr"" target=""#b29"">30,</ref><ref t 5"">36]</ref>), our normalization method inherently avoids this computation. Group-wise computation. Group convolutions have been presented by AlexNet <ref type=""bibr"" target=""#b27"">[28]</ref> for distributing a model into two GPUs. The concept of groups as a dimension for model design has been more",0
"k</head><p>Normalization. Normalization layers in deep networks had been widely used before the development of BN. Local Response Normalization (LRN) <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" target=""#b27"">28]</ref> was a component in AlexNet <",0
"ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b22"">23]</ref>) or generative models (GANs <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b24"">25]</ref>). But as we will show by experiments, both LN and IN have limited success in visual recognition, for which GN",0
"transfer from pre-training to fine-tuning. GN shows improved results vs. its BN counterpart on Mask R-CNN for COCO object detection and segmentation <ref type=""bibr"" target=""#b19"">[20]</ref>, and on 3D convolutional networks for Kinetics video classification <ref type=""bibr"" target=""#b20"">[21]</ref",0
"experiment with Inflated 3D (I3D) convolutional networks <ref type=""bibr"" target=""#b12"">[13]</ref>. We use the ResNet-50 I3D baseline as described in <ref type=""bibr"" target=""#b62"">[63]</ref>. The models are pre-trained from ImageNet. For both BN and GN, we extend the normalization from over (H, W )",0
"r batch size, GN is comparably good as BN (with a gap of ∼0.5%) and outperforms other normalization variants <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b18"">19]</ref>. Moreover, although the batch size may change, GN can naturally trans e batch size can have dramatic impact on the estimated batch statistics.</p><p>Several normalization methods <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" target=""#b33"" s.</p><p>There have been existing methods, such as Layer Normalization (LN) <ref type=""bibr"" target=""#b16"">[17]</ref> and Instance Normalization (IN) <ref type=""bibr"" target=""#b17"">[18]</ref> (Figure <ref type=""figure"" target=""#fig_1"">2</ref>), that also avoid normalizing along the batch dimension. h dimension. Layer Normalization (LN) <ref type=""bibr"" target=""#b16"">[17]</ref> operates along the channel dimension, and Instance Normalization (IN) <ref type=""bibr"" target=""#b17"">[18]</ref> performs BN-like computation but only for each sample (Figure <ref type=""figure"" target=""#fig_1"">2</ref>). I 5"">S i = {k | k N = i N },<label>(4)</label></formula><p>meaning that LN computes µ and σ along the (C, H, W ) axes for each sample. In Instance Norm <ref type=""bibr"" target=""#b17"">[18]</ref>, the set is:</p><formula xml:id=""formula_6"">S i = {k | k N = i N , k C = i C }.<label>(5)</label></formula>< of GN over LN, as shown by the lower training and validation error in experiments (Figure <ref type=""figure"" target=""#fig_3"">4</ref>). GN becomes IN <ref type=""bibr"" target=""#b17"">[18]</ref> if we set the group number as G = C (i.e., one channel per group). But IN can only rely on the spatial dimen",0
"dependence.  Implementation. GN can be easily implemented by a few lines of code in PyTorch <ref type=""bibr"" target=""#b49"">[50]</ref> and TensorFlow <ref type=""bibr"" target=""#b50"">[51]</ref> where automatic differentiation is supported. Figure <ref type=""figure"" target=""#fig_2"">3</ref> shows the co",0
"/p><p>We experiment on the Mask R-CNN baselines <ref type=""bibr"" target=""#b9"">[10]</ref>, implemented in the publicly available codebase of Detectron <ref type=""bibr"" target=""#b58"">[59]</ref>. We use the end-to-end variant with the same hyper-parameters as in <ref type=""bibr"" target=""#b58"">[59]</ref ublicly available codebase of Detectron <ref type=""bibr"" target=""#b58"">[59]</ref>. We use the end-to-end variant with the same hyper-parameters as in <ref type=""bibr"" target=""#b58"">[59]</ref>. We replace BN * with GN during fine-tuning, using the corresponding models pre-trained from ImageNet. <ref ""table"" target=""#tab_6"">6</ref> shows the full results of GN (applied to the backbone, box head, and mask head), compared with the Detectron baseline <ref type=""bibr"" target=""#b58"">[59]</ref> based on BN * . Using the same hyper-parameters as <ref type=""bibr"" target=""#b58"">[59]</ref>, GN increases o ad, and mask head), compared with the Detectron baseline <ref type=""bibr"" target=""#b58"">[59]</ref> based on BN * . Using the same hyper-parameters as <ref type=""bibr"" target=""#b58"">[59]</ref>, GN increases over BN * by a healthy margin. Moreover, we found that GN is not fully trained with the defaul r"" target=""#b58"">[59]</ref>, GN increases over BN * by a healthy margin. Moreover, we found that GN is not fully trained with the default schedule in <ref type=""bibr"" target=""#b58"">[59]</ref>, so we also tried increasing the iterations from 180k to 270k (BN * does not benefit from longer training). le 6 .</head><label>6</label><figDesc>Detection and segmentation results in COCO using Mask R-CNN and FPN. Here BN * is the default Detectron baseline<ref type=""bibr"" target=""#b58"">[59]</ref>, and GN is applied to the backbone, box head, and mask head. ""long"" means training with more iterations.</fi / 90.6</cell><cell>74.5 / 91.7</cell></row></table></figure> 			<note xmlns=""http://www.tei-c.org/ns/1.0"" place=""foot"" n=""2"" xml:id=""foot_0"">Detectron<ref type=""bibr"" target=""#b58"">[59]</ref> uses pre-trained models provided by the authors of<ref type=""bibr"" target=""#b2"">[3]</ref>. For fair comparis",0
"er control is shown in Table <ref type=""table"" target=""#tab_0"">I</ref>.</p><p>Graph neural networks (GNNs) can effectively exploit non-Euclidean data <ref type=""bibr"" target=""#b12"">[13]</ref>, e.g., CSI. In this paper, to overcome the limitations mentioned above, we propose to employ GNNs for wirele =""http://www.tei-c.org/ns/1.0""><head>B. Graph Neural Networks</head><p>In this subsection, we give a brief introduction to GNNs, and one can refer to <ref type=""bibr"" target=""#b12"">[13]</ref>, <ref type=""bibr"" target=""#b16"">[17]</ref> for a more detailed information. GNNs deal with learning problems",1
"st requirement, the idea is to use a symmetric function on transformed elements in the point set to approximate a general function defined on the set <ref type=""bibr"" target=""#b20"">[21]</ref>:</p><formula xml:id=""formula_12"">AGGREGATE({x 1 , • • • , x n }) ≈ g(o(x 1 ), • • • , o(x n )), where f : 2",1
"ibr"" target=""#b1"">[2]</ref>, which applied MLP and CNN, respectively, to approximate the classic weighted minimum mean square error (WMMSE) algorithm <ref type=""bibr"" target=""#b11"">[12]</ref> and accelerate the computation. Unsupervised learning and an ensembling mechanism were employed in <ref type ula><p>This problem is known to be NP-hard <ref type=""bibr"" target=""#b6"">[7]</ref>. Although several optimization-based methods have been proposed in <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" target=""#b13"">[14]</ref>, they are computationally demanding, and thus cannot be applied f iment, three network setups K ∈ {10, 20, 30} are considered. We mainly compare the proposed IGCNet with the following five benchmarks:</p><p>1) WMMSE <ref type=""bibr"" target=""#b11"">[12]</ref>: This is the most popular optimizationbased algorithm for the K-user interference channel power control. It",0
"ccesses of deep learning, researchers have attempted to apply deep learning based methods to solve NP-hard optimization problems in wireless networks <ref type=""bibr"" target=""#b0"">[1]</ref>- <ref type=""bibr"" target=""#b2"">[3]</ref>, <ref type=""bibr"" target=""#b8"">[9]</ref>- <ref type=""bibr"" target=""#b "">[11]</ref>. As a classic wireless resource allocation problem, power control in the K-user interference channel has attracted most of the attention <ref type=""bibr"" target=""#b0"">[1]</ref>- <ref type=""bibr"" target=""#b4"">[5]</ref>. The first attempts came from <ref type=""bibr"" target=""#b0"">[1]</ref> el has attracted most of the attention <ref type=""bibr"" target=""#b0"">[1]</ref>- <ref type=""bibr"" target=""#b4"">[5]</ref>. The first attempts came from <ref type=""bibr"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b1"">[2]</ref>, which applied MLP and CNN, respectively, to approximate the classic 1"">[12]</ref>, <ref type=""bibr"" target=""#b13"">[14]</ref>, they are computationally demanding, and thus cannot be applied for real-time implementation <ref type=""bibr"" target=""#b0"">[1]</ref>. To alleviate the computation burden while achieving near-optimal performance, machine learning based methods >. To alleviate the computation burden while achieving near-optimal performance, machine learning based methods have been proposed. Specifically, MLP <ref type=""bibr"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b2"">[3]</ref> and CNN <ref type=""bibr"" target=""#b1"">[2]</ref> have been used to app isting Approaches' Limitations</head><p>In this subsection, we identify the performance deterioration phenomenon of existing methods using MLP or CNN <ref type=""bibr"" target=""#b0"">[1]</ref>- <ref type=""bibr"" target=""#b2"">[3]</ref>.</p><p>Fig. <ref type=""figure"">II</ref> illustrates MLP and CNN based ""#b2"">[3]</ref>.</p><p>Fig. <ref type=""figure"">II</ref> illustrates MLP and CNN based approaches for power control. From the numerical experiments in <ref type=""bibr"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b2"">[3]</ref>, we observe a performance loss when K gets larger. For example, in <r n <ref type=""bibr"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b2"">[3]</ref>, we observe a performance loss when K gets larger. For example, in <ref type=""bibr"" target=""#b0"">[1]</ref>, the performance gap to the WMMSE algorithm is 3% when K = 10 and becomes 12% when K = 30. From the perspectiv Interference Channel Power Control</head><p>In this subsection, in order to demonstrate the effectiveness of IGCNet, we follow the system setting of <ref type=""bibr"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b2"">[3]</ref> to set up simulations. Under this system setting, all the weights are [12]</ref>: This is the most popular optimizationbased algorithm for the K-user interference channel power control. It is also used as a benchmark in <ref type=""bibr"" target=""#b0"">[1]</ref>- <ref type=""bibr"" target=""#b2"">[3]</ref>. 2) MLP <ref type=""bibr"" target=""#b0"">[1]</ref>: It leverages MLP to nce channel power control. It is also used as a benchmark in <ref type=""bibr"" target=""#b0"">[1]</ref>- <ref type=""bibr"" target=""#b2"">[3]</ref>. 2) MLP <ref type=""bibr"" target=""#b0"">[1]</ref>: It leverages MLP to learn the input-output mapping of WMMSE. 3) PCNet <ref type=""bibr"" target=""#b2"">[3]</ref> they can not incorporate instantaneous CSI.</p><p>We generate 20000 training samples, i.e., network realizations, to train MLP, PCNet, and DPC as in <ref type=""bibr"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b1"">[2]</ref> while the number of training samples used for IGCNet is 2000. The tes suggests IGCNet is robust to CSI inaccuracy.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>D. Time Comparison</head><p>It was reported in <ref type=""bibr"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b2"">[3]</ref> that learning-based methods have less computation time than the optim",0
"GNNs deal with learning problems with graph data or non-Euclidean data. There are many sucessful applications of GNNs such as recommendation systems <ref type=""bibr"" target=""#b17"">[18]</ref> and solving combinatorial problems <ref type=""bibr"" target=""#b18"">[19]</ref>. GNNs utilize the graph structu",0
"ph Neural Networks</head><p>In this subsection, we give a brief introduction to GNNs, and one can refer to <ref type=""bibr"" target=""#b12"">[13]</ref>, <ref type=""bibr"" target=""#b16"">[17]</ref> for a more detailed information. GNNs deal with learning problems with graph data or non-Euclidean data. The rmula_8"">(k) v</formula><p>is an intermediate variable.</p><p>The design of the two functions in GNNs is crucial and leads to different kinds of GNNs <ref type=""bibr"" target=""#b16"">[17]</ref>. The most popular GNNs are listed as follows.</p><p>1) Graph Convolutional Network <ref type=""bibr"" target="" LU W 1 β (k−1) v + W 2 u β (k−1) u ,</formula><p>where u ∈ N (v), and W 1 and W 2 are the weight matrices to be learned. 3) Graph Isomorphism Network <ref type=""bibr"" target=""#b16"">[17]</ref>: It uses the MLP and sum pooling as the aggregation and combination functions,</p><formula xml:id=""formula_1",0
"location plays a crucial role for performance optimization in wireless networks. However, typical resource allocation problems, such as power control <ref type=""bibr"" target=""#b6"">[7]</ref>, <ref type=""bibr"" target=""#b7"">[8]</ref>, are non-convex and computationally challenging. Moreover, they need xml:id=""formula_4"">H = [h 1 , • • • , h K ] T and h i = [h 1i , • • • , h Ki ] T , i = 1, • • • , K.</formula><p>This problem is known to be NP-hard <ref type=""bibr"" target=""#b6"">[7]</ref>. Although several optimization-based methods have been proposed in <ref type=""bibr"" target=""#b11"">[12]</ref>,",0
"blems in wireless networks <ref type=""bibr"" target=""#b0"">[1]</ref>- <ref type=""bibr"" target=""#b2"">[3]</ref>, <ref type=""bibr"" target=""#b8"">[9]</ref>- <ref type=""bibr"" target=""#b10"">[11]</ref>. As a classic wireless resource allocation problem, power control in the K-user interference channel has att",0
"e attempted to apply deep learning based methods to solve NP-hard optimization problems in wireless networks <ref type=""bibr"" target=""#b0"">[1]</ref>- <ref type=""bibr"" target=""#b2"">[3]</ref>, <ref type=""bibr"" target=""#b8"">[9]</ref>- <ref type=""bibr"" target=""#b10"">[11]</ref>. As a classic wireless res lgorithm <ref type=""bibr"" target=""#b11"">[12]</ref> and accelerate the computation. Unsupervised learning and an ensembling mechanism were employed in <ref type=""bibr"" target=""#b2"">[3]</ref> to achieve better performance than the suboptimal WMMSE algorithm.</p><p>However, MLP and CNN, which are desig ile achieving near-optimal performance, machine learning based methods have been proposed. Specifically, MLP <ref type=""bibr"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b2"">[3]</ref> and CNN <ref type=""bibr"" target=""#b1"">[2]</ref> have been used to approximate the input-output mapping of this n this subsection, we identify the performance deterioration phenomenon of existing methods using MLP or CNN <ref type=""bibr"" target=""#b0"">[1]</ref>- <ref type=""bibr"" target=""#b2"">[3]</ref>.</p><p>Fig. <ref type=""figure"">II</ref> illustrates MLP and CNN based approaches for power control. From the n gure"">II</ref> illustrates MLP and CNN based approaches for power control. From the numerical experiments in <ref type=""bibr"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b2"">[3]</ref>, we observe a performance loss when K gets larger. For example, in <ref type=""bibr"" target=""#b0"">[1]</ref>, th <ref type=""figure"" target=""#fig_3"">3</ref>.</p><p>The loss function adopted is the negative sum rate, as in <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b2"">[3]</ref>,</p><formula xml:id=""formula_15"">= −E H K k=1 w k log 2 1 + |h kk | 2 p k (θ) i =k |h ki | 2 p i (θ) + σ 2 k , ><p>In this subsection, in order to demonstrate the effectiveness of IGCNet, we follow the system setting of <ref type=""bibr"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b2"">[3]</ref> to set up simulations. Under this system setting, all the weights are the same and the channel coefficients ar mizationbased algorithm for the K-user interference channel power control. It is also used as a benchmark in <ref type=""bibr"" target=""#b0"">[1]</ref>- <ref type=""bibr"" target=""#b2"">[3]</ref>. 2) MLP <ref type=""bibr"" target=""#b0"">[1]</ref>: It leverages MLP to learn the input-output mapping of WMMSE. =""bibr"" target=""#b2"">[3]</ref>. 2) MLP <ref type=""bibr"" target=""#b0"">[1]</ref>: It leverages MLP to learn the input-output mapping of WMMSE. 3) PCNet <ref type=""bibr"" target=""#b2"">[3]</ref>: It employs MLP and an unsupervised loss function to learn near-optimal power allocation. 4) DPC <ref type=""bi acy.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>D. Time Comparison</head><p>It was reported in <ref type=""bibr"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b2"">[3]</ref> that learning-based methods have less computation time than the optimization-based methods. We also compare th",0
"ef type=""bibr"" target=""#b0"">[1]</ref>- <ref type=""bibr"" target=""#b4"">[5]</ref>. The first attempts came from <ref type=""bibr"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b1"">[2]</ref>, which applied MLP and CNN, respectively, to approximate the classic weighted minimum mean square error (WMMSE learning based methods have been proposed. Specifically, MLP <ref type=""bibr"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b2"">[3]</ref> and CNN <ref type=""bibr"" target=""#b1"">[2]</ref> have been used to approximate the input-output mapping of this problem. The optimization-based methods involve re and parameter setting is shown in Fig. <ref type=""figure"" target=""#fig_3"">3</ref>.</p><p>The loss function adopted is the negative sum rate, as in <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b2"">[3]</ref>,</p><formula xml:id=""formula_15"">= −E H K k=1 w k log 2 1 + |h kk | 2 SE. 3) PCNet <ref type=""bibr"" target=""#b2"">[3]</ref>: It employs MLP and an unsupervised loss function to learn near-optimal power allocation. 4) DPC <ref type=""bibr"" target=""#b1"">[2]</ref>: CNN and the unsupervised loss function are used in this method to learn a near-optimal power control. 5) Base SI.</p><p>We generate 20000 training samples, i.e., network realizations, to train MLP, PCNet, and DPC as in <ref type=""bibr"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b1"">[2]</ref> while the number of training samples used for IGCNet is 2000. The test dataset contains 500 network realizatio mple vary. The transmitters are uniformly distributed in the square region [0, 100] × [0, 100] meters. The receivers are uniformly distributed within <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b9"">10]</ref> meters away from the transmitter. The adopted channel model is</p><form from that in the training. We follow <ref type=""bibr"" target=""#b3"">[4]</ref> to set up the simulation. The link distance is uniformly distributed in <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b9"">10]</ref> meters during training. In the test, the link distance is uniformly dis arget=""#b9"">10]</ref> meters during training. In the test, the link distance is uniformly distributed in [l r , u r ] meters, where l r is uniform in <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b19"">20]</ref> meters and u r is uniform in [l r , 20] meters. The performance of IGC",0
"an naive software execution <ref type=""bibr"" target=""#b54"">[53]</ref>. (iii) Designing a hardware for a fixed modest-sized parameter, e.g., n = 2 12  <ref type=""bibr"" target=""#b55"">[54]</ref>. However, encryption parameters determine the security-level and the maximum number of consecutive multiplic ions. We measure the performance of SEAL on a single-threaded Intel Xeon(R) Silver 4108 running at 1.80 GHz; which is a similar CPU used in prior art <ref type=""bibr"" target=""#b55"">[54]</ref>. The single-thread baseline is used by prior art for measuring the performance (non-CKKS schemes) <ref type= prior art <ref type=""bibr"" target=""#b55"">[54]</ref>. The single-thread baseline is used by prior art for measuring the performance (non-CKKS schemes) <ref type=""bibr"" target=""#b55"">[54]</ref>. In addition, SEAL is thread-safe but not multithreaded due to the complex data dependencies, hence, we cann p data transfer, their design does not yield superior performance compared to CPU execution.</p><p>Perhaps, the closest work to ours is by Roy et al. <ref type=""bibr"" target=""#b55"">[54]</ref> in which authors propose an architecture for BFV scheme and implement their design on Xilinx Zynq UltraScale de performance improvement compared to Microsoft SEAL running on Intel Xeon Silver 4108 at 1.8 GHz (note that similar processor is used compared with <ref type=""bibr"" target=""#b55"">[54]</ref> running at identical frequency).</p><p>FPGA-based Co-Processors. Designing co-processors has also been studi w megabits of capacity but very fast response time and high throughput.</p><p>As has been shown by prior art <ref type=""bibr"" target=""#b54"">[53,</ref><ref type=""bibr"" target=""#b55"">54]</ref>, leveraging off-chip memory to store intermediate results significantly reduces the overall performance due t",1
"NTT form to reduce the number of NTT/INTT conversions. Fast NTT algorithms are well studied in lattice-based cryptography. We adapt the algorithms in <ref type=""bibr"" target=""#b45"">[44]</ref> which analyzes fast NTT algorithms and introduces specific optimizations for negacyclic convolution. For a r",0
"ition contributes to a fast key switching and high noise growth. With the special modulus p and a rescaling at the end of key switching, explained in <ref type=""bibr"" target=""#b15"">[15]</ref>, key switching is almost noise-free.</p><p>? KeySwitch(ct, ksk): Given a ciphertext ct = (c 0 , c 1 ) ? R 2",0
"""bibr"" target=""#b20"">20,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" target=""#b50"">49,</ref><ref type=""bibr"" target=""#b51"">50]</ref> focus on improving the performance of YASHE <ref type=""bibr"" target=""#b10"">[10]</ref> and LTV <ref type=""bibr",0
"seful prefetch candidates, but do not leverage the capabilities of trace-based dataflow analysis to explore timeliness and load classification. Zhang <ref type=""bibr"" target=""#b52"">[53]</ref> performs dynamic prefetch optimization based on profiling, however, this work requires an extra thread while",1
"=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" target=""#b34"">35,</ref><ref type=""bibr"" target=""#b45"">46,</ref><ref type=""bibr"" target=""#b51"">52]</ref> struggle to address these questions. In particular, they either requi work. Compiler-based techniques <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b45"">46]</ref> perform static code analysis to generate prefetch targets. The performance benefits provided by static approa",0
"ile can be generated by feeding the traces through a cache simulator, or via performance counters such as Intel's Precise Event-Based Sampling (PEBS) <ref type=""bibr"" target=""#b15"">[16]</ref>. It is important to note that a miss profile is specific not only to individual workloads, but also to the m",0
"nevertheless a 9% IPC gain can save millions of dollars for large WSC providers. Hardware Techniques: Several hardware mechanisms have been proposed <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b20"">",0
"""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" target=""#b34"">35,</ref><ref type=""bibr"" target=""#b45"">46,</ref><ref type=""bibr"" target=""#b51"">52]</ref> struggle to address these questions. In particular, they either require manual annotation or are significantl (SNLNs) <ref type=""bibr"" target=""#b49"">[50]</ref> or regular strides <ref type=""bibr"" target=""#b28"">[29,</ref><ref type=""bibr"" target=""#b34"">35,</ref><ref type=""bibr"" target=""#b51"">52</ref>] can be learned. In contrast, our approach can handle complex dataflows of generic software algorithms and dat",0
"to reconstruct high-resolution (HR) images from low-resolution (LR) images. Based on DNNs, many methods have been proposed to improve SR performance <ref type=""bibr"" target=""#b53"">[51,</ref><ref type=""bibr"" target=""#b28"">26,</ref><ref type=""bibr"" target=""#b12"">10,</ref><ref type=""bibr"" target=""#b14 dels by increasing the model capacity, e.g., EDSR <ref type=""bibr"" target=""#b28"">[26]</ref>, DBPN <ref type=""bibr"" target=""#b18"">[16]</ref>, and RCAN <ref type=""bibr"" target=""#b53"">[51]</ref>. However, these methods still suffer from the large space issue of possible mapping functions, resulting in f> propose a backprojection network (DBPN) that consists of several up-and down-sampling layers to iteratively produce LR and HR images. Zhang et al. <ref type=""bibr"" target=""#b53"">[51]</ref> propose the channel attention mechanism to build a deep model called RCAN to further improve the performance ""#fig_2"">3</ref>) and 3 blocks for 8× upscaling. Unlike the baseline U-Net, we build each basic block using B residual channel attention block (RCAB) <ref type=""bibr"" target=""#b53"">[51]</ref> to improve the model capacity. Following <ref type=""bibr"" target=""#b41"">[39,</ref><ref type=""bibr"" target=""# he architecture design of the primal model, there are 2 dual models for 4× SR and 3 dual models for 8× SR, respectively. Let B be the number of RCABs <ref type=""bibr"" target=""#b53"">[51]</ref> and F be the number of base feature channels. For 4× SR, we set B = 30 and F = 16 for DRN-S and B = 40 and F type=""bibr"" target=""#b21"">[19]</ref> and reconstructionbased methods <ref type=""bibr"" target=""#b18"">[16,</ref><ref type=""bibr"" target=""#b27"">25,</ref><ref type=""bibr"" target=""#b53"">51]</ref>. Haris et al. <ref type=""bibr"" target=""#b18"">[16]</ref> propose a backprojection network (DBPN) that consists corresponding HR patches as the paired training data, and augment the training data following the method in <ref type=""bibr"" target=""#b28"">[26,</ref><ref type=""bibr"" target=""#b53"">51]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>As shown in Table</head><p>Test data. For quantitat",1
"without paired data in the unsupervised setting <ref type=""bibr"" target=""#b45"">[43,</ref><ref type=""bibr"" target=""#b56"">54]</ref>. Based on Cycle-GAN <ref type=""bibr"" target=""#b58"">[56]</ref>, Yuan et al. <ref type=""bibr"" target=""#b45"">[43]</ref> propose a CinCGAN model to generate HR images without rformance of language translation. Recently, this scheme has also been used to perform image translation without paired training data, e.g., CycleGAN <ref type=""bibr"" target=""#b58"">[56]</ref> and DualGAN <ref type=""bibr"" target=""#b44"">[42]</ref>. Specifically, a cycle consistency loss is proposed to =""#b58"">56]</ref> use a cycle consistency loss to avoid the possible mode collapse issue when solving the under-constrained image translation problem <ref type=""bibr"" target=""#b58"">[56]</ref>. Unlike these methods, we seek to improve the performance of our SR model by adding an extra constraint, whi data, i.e., Nearest data, BD data, and video frames collected from YouTube. Thus, there are 3 DRN-adapt models in total. And We also train a CinCGAN <ref type=""bibr"" target=""#b58"">[56]</ref> model for each kind of unpaired data for comparison. Based on pretrained DRN-S, We train our DRN-Adapt model DualGAN <ref type=""bibr"" target=""#b44"">[42]</ref>. Specifically, a cycle consistency loss is proposed to avoid the mode collapse issue of GAN methods <ref type=""bibr"" target=""#b58"">[56,</ref><ref type=""bibr"" target=""#b6"">4,</ref><ref type=""bibr"" target=""#b7"">5]</ref> and help minimize the distributi eral differences and advantages of DRN compared to CycleGAN based SR methods. First, Cycle-GAN based methods <ref type=""bibr"" target=""#b45"">[43,</ref><ref type=""bibr"" target=""#b58"">56]</ref> use a cycle consistency loss to avoid the possible mode collapse issue when solving the under-constrained ima",1
"the training of SR models becomes an important problem.</p><p>Second, it is hard to obtain a promising SR model when the paired data are unavailable <ref type=""bibr"" target=""#b45"">[43,</ref><ref type=""bibr"" target=""#b56"">54]</ref>. Note that most SR methods rely on the paired training data, i.e., H More critically, if we directly apply existing SR models to real-world data, they often incur a severe adaptation problem and yield poor performance <ref type=""bibr"" target=""#b45"">[43,</ref><ref type=""bibr"" target=""#b56"">54]</ref>. Therefore, how to effectively exploit the unpaired data to adapt SR rvised super-resolution.</head><p>There is an increasing interest in learning super-resolution models without paired data in the unsupervised setting <ref type=""bibr"" target=""#b45"">[43,</ref><ref type=""bibr"" target=""#b56"">54]</ref>. Based on Cycle-GAN <ref type=""bibr"" target=""#b58"">[56]</ref>, Yuan hods of LR images are often unknown, making this problem very challenging. In this case, existing SR models often incur the severe adaptation problem <ref type=""bibr"" target=""#b45"">[43,</ref><ref type=""bibr"" target=""#b56"">54]</ref>. To alleviate this issue, we propose an efficient algorithm to adapt N based SR Methods</head><p>There are several differences and advantages of DRN compared to CycleGAN based SR methods. First, Cycle-GAN based methods <ref type=""bibr"" target=""#b45"">[43,</ref><ref type=""bibr"" target=""#b58"">56]</ref> use a cycle consistency loss to avoid the possible mode collapse iss pe=""bibr"" target=""#b45"">[43,</ref><ref type=""bibr"" target=""#b56"">54]</ref>. Based on Cycle-GAN <ref type=""bibr"" target=""#b58"">[56]</ref>, Yuan et al. <ref type=""bibr"" target=""#b45"">[43]</ref> propose a CinCGAN model to generate HR images without paired data. Recently, some blind SR methods <ref type",0
"=""bibr"" target=""#b16"">14,</ref><ref type=""bibr"" target=""#b11"">9,</ref><ref type=""bibr"" target=""#b17"">15,</ref><ref type=""bibr"" target=""#b29"">27,</ref><ref type=""bibr"" target=""#b15"">13]</ref>, video understanding <ref type=""bibr"">[46, 45,</ref> Figure <ref type=""figure"">1</ref>. Performance compariso l networks (DNNs) have achieved great success in image classification <ref type=""bibr"" target=""#b16"">[14,</ref><ref type=""bibr"" target=""#b11"">9,</ref><ref type=""bibr"" target=""#b15"">13,</ref><ref type=""bibr"" target=""#b17"">15]</ref>, image generation <ref type=""bibr"" target=""#b13"">[11,</ref><ref type=",0
"bibr"" target=""#b53"">[51,</ref><ref type=""bibr"" target=""#b28"">26,</ref><ref type=""bibr"" target=""#b12"">10,</ref><ref type=""bibr"" target=""#b14"">12,</ref><ref type=""bibr"" target=""#b51"">49]</ref>. However, these methods may suffer from two limitations.</p><p>First, learning the mapping from LR to HR imag",0
"xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.1."">Architecture Design of DRN</head><p>We build our DRN upon the design of U-Net for superresolution <ref type=""bibr"" target=""#b24"">[22,</ref><ref type=""bibr"" target=""#b33"">31]</ref> (See Figure <ref type=""figure"" target=""#fig_2"">3</ref>). Our DRN mod",0
"=""bibr"" target=""#b9"">[7,</ref><ref type=""bibr"" target=""#b52"">50,</ref><ref type=""bibr"" target=""#b54"">52,</ref><ref type=""bibr"" target=""#b13"">11,</ref><ref type=""bibr"" target=""#b22"">20]</ref>. Recently, image super-resolution (SR) has become an important task that aims at learning a nonlinear mapping",0
"Work</head><p>Supervised super-resolution. Many efforts have been made to improve the performance of SR, including the interpolation-based approaches <ref type=""bibr"" target=""#b21"">[19]</ref> and reconstructionbased methods <ref type=""bibr"" target=""#b18"">[16,</ref><ref type=""bibr"" target=""#b27"">25,<",0
"ural networks (DNNs) have been the workhorse of many real-world applications, including image classification <ref type=""bibr"" target=""#b20"">[18,</ref><ref type=""bibr"" target=""#b16"">14,</ref><ref type=""bibr"" target=""#b11"">9,</ref><ref type=""bibr"" target=""#b17"">15,</ref><ref type=""bibr"" target=""#b29""> rg/ns/1.0""><head>B. Model Details of Dual Regression Network</head><p>Deep neural networks (DNNs) have achieved great success in image classification <ref type=""bibr"" target=""#b16"">[14,</ref><ref type=""bibr"" target=""#b11"">9,</ref><ref type=""bibr"" target=""#b15"">13,</ref><ref type=""bibr"" target=""#b17""",0
"he influence of noises or outliers on the classification hyperplane <ref type=""bibr"" target=""#b60"">[61]</ref><ref type=""bibr"" target=""#b61"">[62]</ref><ref type=""bibr"" target=""#b62"">[63]</ref><ref type=""bibr"" target=""#b63"">[64]</ref><ref type=""bibr"" target=""#b64"">[65]</ref><ref type=""bibr"" target=""#b",1
"of the normal database for each individual program <ref type=""bibr"" target=""#b19"">[20,</ref><ref type=""bibr"" target=""#b20"">21]</ref>. The Murmurhash <ref type=""bibr"" target=""#b21"">[22]</ref><ref type=""bibr"" target=""#b22"">[23]</ref><ref type=""bibr"" target=""#b23"">[24]</ref> is utilized with the Bloom",0
"e lengths <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b9"">10]</ref>. Aron Laszka et al. <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b11"">12]</ref> investigated and claimed that the optimal n-gram is 6-gram in UNM dataset and 7-gram in ADFA-LD dataset. Suaa type=""bibr"" target=""#b10"">(11)</ref>. </p><p>Therefore, membership functions of both positive and negative sample points are constructed according to <ref type=""bibr"" target=""#b11"">(12)</ref> and <ref type=""bibr"" target=""#b12"">(13)</ref>. </p><formula xml:id=""formula_10"">? ? + = { 0.</formula></div>",0
"ype=""bibr"" target=""#b37"">[38]</ref> and natural language processing <ref type=""bibr"" target=""#b38"">[39]</ref><ref type=""bibr"" target=""#b39"">[40]</ref><ref type=""bibr"" target=""#b40"">[41]</ref>. Researchers have also tried to use neural networks to process system call sequences <ref type=""bibr"" target",0
"eural networks have made remarkable achievements in computer vision <ref type=""bibr"" target=""#b35"">[36]</ref><ref type=""bibr"" target=""#b36"">[37]</ref><ref type=""bibr"" target=""#b37"">[38]</ref> and natural language processing <ref type=""bibr"" target=""#b38"">[39]</ref><ref type=""bibr"" target=""#b39"">[40]",0
"essing.</p><p>In this work, we propose to combine acoustics and first-pass text hypotheses for second-pass decoding based on the deliberation network <ref type=""bibr"" target=""#b15"">[16]</ref>. The deliberation model has been used in state-of-the-art machine translation <ref type=""bibr"" target=""#b16"" ntermediate representation in speech-to-text translation <ref type=""bibr"" target=""#b17"">[18]</ref>. Our deliberation model has a similar structure as <ref type=""bibr"" target=""#b15"">[16]</ref>: An RNN-T model generates the first-pass hypotheses, and deliberation attends to both acoustics and first-pa ://www.tei-c.org/ns/1.0""><head n=""2.2."">Training</head><p>A deliberation model is typically trained from scratch by jointly optimizing all components <ref type=""bibr"" target=""#b15"">[16]</ref>. However, we find training a two-pass model from scratch tends to be unstable in practice <ref type=""bibr"" t red encoder, an RNN-T decoder <ref type=""bibr"" target=""#b0"">[1]</ref>, and a deliberation decoder, similar to <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b15"">16]</ref>. The shared encoder takes log-mel filterbank energies, x = (x1, ..., xT ), where T denotes the number of fram",1
"d on the deliberation network <ref type=""bibr"" target=""#b15"">[16]</ref>. The deliberation model has been used in state-of-the-art machine translation <ref type=""bibr"" target=""#b16"">[17]</ref>, or generating intermediate representation in speech-to-text translation <ref type=""bibr"" target=""#b17"">[18]",1
"ate-of-the-art machine translation <ref type=""bibr"" target=""#b16"">[17]</ref>, or generating intermediate representation in speech-to-text translation <ref type=""bibr"" target=""#b17"">[18]</ref>. Our deliberation model has a similar structure as <ref type=""bibr"" target=""#b15"">[16]</ref>: An RNN-T model",1
"f type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b4"">5]</ref>. In large scale training, E2E models perform competitively compared to more sophisticated conventional systems",0
"models post-process hypotheses using only the text information, and can be considered as second-pass models <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b12"">13]</ref>. The models typically use beam search to generate new hypotheses, com nces to improve numeric utterance recognition <ref type=""bibr"" target=""#b14"">[15]</ref>. A transformer-based spelling correction model is proposed in <ref type=""bibr"" target=""#b11"">[12]</ref> to correct the outputs of a connectionist temporal classification model in Mandarin ASR. In addition, <ref t",0
"idually train components of a conventional model (i.e., acoustic, pronunciation, and language models), and directly outputs subword (or word) symbols <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b3"">4,</r n in Fig. <ref type=""figure"" target=""#fig_0"">1</ref>, our deliberation network consists of three major components: A shared encoder, an RNN-T decoder <ref type=""bibr"" target=""#b0"">[1]</ref>, and a deliberation decoder, similar to <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b15""",0
"ally short in VS, and thus the encoding should have limited impact on latency.</p><p>Our experiments are conducted using the same training data as in <ref type=""bibr"" target=""#b19"">[20,</ref><ref type=""bibr"" target=""#b20"">21]</ref>, which is from multiple domains such as Voice Search, YouTube, Farfi SETUP</head></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.1."">Datasets</head><p>For training, we use the same multidomain datasets as in <ref type=""bibr"" target=""#b19"">[20,</ref><ref type=""bibr"" target=""#b20"">21]</ref> which include anonymized and hand-transcribed English utterances fro",0
"e first-pass hypotheses bidirectionally to leverage context information for decoding. Note that the first-pass hypotheses are sequences of wordpieces <ref type=""bibr"" target=""#b18"">[19]</ref> and are usually short in VS, and thus the encoding should have limited impact on latency.</p><p>Our experime ts followed by 640-dimensional projection per layer). The LAS decoder has a 4,096-dimensional softmax layer to predict the same mixed-case wordpieces <ref type=""bibr"" target=""#b18"">[19]</ref> as the RNN-T.</p><p>For feature extraction, we use 128-dimensional log-Mel features from 32-ms windows at a",0
"arget=""#b4"">5]</ref>. In large scale training, E2E models perform competitively compared to more sophisticated conventional systems on Google traffic <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b6"">7]</ref>. Given its all-neural nature, an E2E model can be reasonably downsized t ""#b5"">[6,</ref><ref type=""bibr"" target=""#b6"">7]</ref>. Given its all-neural nature, an E2E model can be reasonably downsized to fit on mobile devices <ref type=""bibr"" target=""#b5"">[6]</ref>.</p><p>Despite the rapid progress made by E2E models, they still face challenges compared to state-of-the-art rget=""#b7"">[8,</ref><ref type=""bibr"" target=""#b8"">9]</ref>. To bridge the quality gap between a streaming recurrent neural network transducer (RNN-T) <ref type=""bibr"" target=""#b5"">[6]</ref> and a large conventional model <ref type=""bibr"" target=""#b7"">[8]</ref>, a two-pass framework has been proposed computes sequence-level log-likelihoods of first-pass hypotheses. The two-pass model achieves 17%-22% relative WER reduction (WERR) compared to RNN-T <ref type=""bibr"" target=""#b5"">[6]</ref> and has a similar WER to a large conventional model <ref type=""bibr"" target=""#b7"">[8]</ref>.</p><p>A class of om scratch tends to be unstable in practice <ref type=""bibr"" target=""#b9"">[10]</ref>, and thus use a two-step training process: Train the RNN-T as in <ref type=""bibr"" target=""#b5"">[6]</ref>, and then fix the RNN-T parameters and only train the deliberation decoder and additional encoder layers as in raffic. To evaluate the performance of proper noun recognition, we report performance on a side-by-side (SxS) test set, and 4 voice command test sets <ref type=""bibr"" target=""#b5"">[6]</ref>. The SxS set contains utterances where the LAS rescoring model <ref type=""bibr"" target=""#b9"">[10]</ref> perfor xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.2."">Architecture Details and Training</head><p>Our first-pass RNN-T model has the same architecture as <ref type=""bibr"" target=""#b5"">[6]</ref>. The encoder of the RNN-T consists of an 8-layer Long Short-Term Memory (LSTM) <ref type=""bibr"" target=""#b25"">",0
"ly the text information, and can be considered as second-pass models <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b12"">13]</ref>. The models typically use beam search to generate new hypotheses, compared to rescoring where one leverages e ed in <ref type=""bibr"" target=""#b11"">[12]</ref> to correct the outputs of a connectionist temporal classification model in Mandarin ASR. In addition, <ref type=""bibr"" target=""#b12"">[13]</ref> leverages text-to-speech (TTS) audio to train an attention-based neural spelling corrector to improve LAS de",0
"rapid progress made by E2E models, they still face challenges compared to state-of-the-art conventional models <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b8"">9]</ref>. To bridge the quality gap between a streaming recurrent neural network transducer (RNN-T) <ref type=""bibr"" tar",0
"stacked with three previous frames to form a 512-dimensional vector, and then downsampled to a 30-ms frame rate. Our models are trained in Tensorflow <ref type=""bibr"" target=""#b27"">[28]</ref> using the Lingvo framework <ref type=""bibr"" target=""#b28"">[29]</ref> on 8×8 Tensor Processing Units (TPU) sl",0
"/ref>.</p><p>A class of neural correction models post-process hypotheses using only the text information, and can be considered as second-pass models <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b12"">13]</ref>. The models typically use be verages external language models trained with large text corpora <ref type=""bibr"" target=""#b13"">[14]</ref>. For example, a neural correction model in <ref type=""bibr"" target=""#b10"">[11]</ref> takes first-pass text hypotheses and generates new sequences to improve numeric utterance recognition <ref t",0
"ratio (SNR) is between 0dB and 30dB <ref type=""bibr"" target=""#b22"">[23]</ref>. We also use mixed-bandwidth utterances at 8kHz or 16 kHz for training <ref type=""bibr"" target=""#b23"">[24]</ref>.</p><p>Our main test set includes ~14K anonymized hand-transcribed VS utterances sampled from Google traffic",0
"get=""#b5"">[6]</ref>.</p><p>Despite the rapid progress made by E2E models, they still face challenges compared to state-of-the-art conventional models <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b8"">9]</ref>. To bridge the quality gap between a streaming recurrent neural network he quality gap between a streaming recurrent neural network transducer (RNN-T) <ref type=""bibr"" target=""#b5"">[6]</ref> and a large conventional model <ref type=""bibr"" target=""#b7"">[8]</ref>, a two-pass framework has been proposed in <ref type=""bibr"" target=""#b9"">[10]</ref>, which uses a non-streamin s 17%-22% relative WER reduction (WERR) compared to RNN-T <ref type=""bibr"" target=""#b5"">[6]</ref> and has a similar WER to a large conventional model <ref type=""bibr"" target=""#b7"">[8]</ref>.</p><p>A class of neural correction models post-process hypotheses using only the text information, and can be test set: 9%. As a result, our best deliberation model achieves a WER of 5.0% on VS, which is 21% relatively better than the large conventional model <ref type=""bibr"" target=""#b7"">[8]</ref> (6.3% VS WER). Lastly, we analyze the computational complexity of the deliberation model, and show some decodi contains utterances where the LAS rescoring model <ref type=""bibr"" target=""#b9"">[10]</ref> performs inferior to a state-of-the-art conventional model <ref type=""bibr"" target=""#b7"">[8]</ref>, and one reason is due to proper nouns. The voice command test sets include 3 TTS test sets created using para",0
"=""bibr"" target=""#b22"">[23]</ref> (as Doc2vec implementation <ref type=""bibr"" target=""#b32"">[33]</ref>), and deep contextual language models from BERT <ref type=""bibr"" target=""#b14"">[15]</ref> and XLNet <ref type=""bibr"" target=""#b40"">[41]</ref> in a vanilla and Siamese architecture <ref type=""bibr"" t ed a shift from context-free word embeddings, like GloVe <ref type=""bibr"" target=""#b30"">[31]</ref>, to contextual embeddings as the ones used in BERT <ref type=""bibr"" target=""#b14"">[15]</ref> and XL-Net <ref type=""bibr"" target=""#b40"">[41]</ref>. The Transformer architecture allowed the efficient uns language models for deep contextual text representations based on the Transformer architecture <ref type=""bibr"" target=""#b36"">[37]</ref>, named BERT <ref type=""bibr"" target=""#b14"">[15]</ref> and XLNet <ref type=""bibr"" target=""#b40"">[41]</ref>. The two Transformer models are originally designed to s mplest Transformer architecture), to 55 minutes for Siamese</figDesc><table /><note>XLNet-512 (most complex Transformer architecture). As suggested in<ref type=""bibr"" target=""#b14"">[15]</ref>, the Transformer training is performed with batch size b = 4, dropout probability d = 0.1, learning rate η =",1
"nd, we implement six different models using word-based document embeddings from GloVe <ref type=""bibr"" target=""#b30"">[31]</ref> and Paragraph Vectors <ref type=""bibr"" target=""#b22"">[23]</ref> (as Doc2vec implementation <ref type=""bibr"" target=""#b32"">[33]</ref>), and deep contextual language models f ckground, purpose, mechanism, and findings. Next, they encode the segments with GloVe <ref type=""bibr"" target=""#b30"">[31]</ref> and Paragraph Vectors <ref type=""bibr"" target=""#b22"">[23]</ref> and compute their similarity to determine whether papers are similar with respect to those segments. However P tasks <ref type=""bibr"" target=""#b18"">[19,</ref><ref type=""bibr"" target=""#b34"">35]</ref> but unable to represent entire documents. Paragraph Vectors <ref type=""bibr"" target=""#b22"">[23]</ref> (also known as Doc2vec), extends word2vec to learn embeddings for word sequences of arbitrary length. In the",1
"""bibr"" target=""#b8"">[9]</ref> for semantic representations of sentences and their similarity <ref type=""bibr"" target=""#b25"">[26]</ref>. In prior work <ref type=""bibr"" target=""#b31"">[32]</ref>, we also utilized a Siamese BERT model to determine the discourse relations between text segments to generat",1
"ntation <ref type=""bibr"" target=""#b32"">[33]</ref>), and deep contextual language models from BERT <ref type=""bibr"" target=""#b14"">[15]</ref> and XLNet <ref type=""bibr"" target=""#b40"">[41]</ref> in a vanilla and Siamese architecture <ref type=""bibr"" target=""#b8"">[9]</ref>. Each system is evaluated unde oVe <ref type=""bibr"" target=""#b30"">[31]</ref>, to contextual embeddings as the ones used in BERT <ref type=""bibr"" target=""#b14"">[15]</ref> and XL-Net <ref type=""bibr"" target=""#b40"">[41]</ref>. The Transformer architecture allowed the efficient unsupervised pretraining of language models and led to s tions based on the Transformer architecture <ref type=""bibr"" target=""#b36"">[37]</ref>, named BERT <ref type=""bibr"" target=""#b14"">[15]</ref> and XLNet <ref type=""bibr"" target=""#b40"">[41]</ref>. The two Transformer models are originally designed to solve sequence pair classification. The base training s trained on English Wikipedia and the BooksCorpus <ref type=""bibr"" target=""#b42"">[43]</ref> alone, XLNet uses additional Web corpora for pretraining <ref type=""bibr"" target=""#b40"">[41]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.6"">Hyperparameters</head><p>3.6.1 Sequence le sformers outperform all other methods. Rather unexpected is that BERT generally achieves slightly better results than XLNet. According to Yang et al. <ref type=""bibr"" target=""#b40"">[41]</ref>, XLNet surpasses BERT on the related GLUE benchmark <ref type=""bibr"" target=""#b38"">[39]</ref>, so we were ex milar outcome. We hypothesize that this difference may be attributed to two reasons, pretraining on different corpora, and smaller models compared to <ref type=""bibr"" target=""#b40"">[41]</ref>. We use the BASE, not the LARGE versions of the pretrained models used by Yang et al. <ref type=""bibr"" targe er models compared to <ref type=""bibr"" target=""#b40"">[41]</ref>. We use the BASE, not the LARGE versions of the pretrained models used by Yang et al. <ref type=""bibr"" target=""#b40"">[41]</ref>. Furthermore, the published XLNet BASE model we considered is pretrained on different data than the one in Y bibr"" target=""#b40"">[41]</ref>. Furthermore, the published XLNet BASE model we considered is pretrained on different data than the one in Yang et al. <ref type=""bibr"" target=""#b40"">[41]</ref>  <ref type=""foot"" target=""#foot_13"">15</ref> . In contrast to BERT, XLNet is pretrained on Web corpora in ad",1
"(e.g., find a document with one specific relation to A, but a different relation to B). These queries are generally referred to as analogical queries <ref type=""bibr"" target=""#b16"">[17]</ref>. Especially for complex information needs, the formulation of analogical queries is more intuitive <ref type o illustrate an explanation. Moreover, analogical query solving in the form of ""A is to B as C is to ?"" is a fundamental aspect of human intelligence <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b23"">24]</ref>. Chan et al. <ref type=""bibr"" target=""#b9"">[10]</ref> emphasize the",0
"olved various document classification tasks <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b28"">29]</ref>. Akkalyoncu Yilmaz et al. <ref type=""bibr"" target=""#b1"">[2]</ref> apply BERT to an information retrieval system for an end-to-end search over large document collections. Despit",0
"m community so far and are not even mentioned in a recently published survey <ref type=""bibr"" target=""#b4"">[5]</ref>. To our knowledge, Hassan et al. <ref type=""bibr"" target=""#b26"">[27]</ref> are one of the first to use BERT to recommend research papers. As opposed to our work, Hassan et al. use BER",0
"t.</p><p>In the context of word embeddings, analogies are often illustrated using vector arithmetic, e.g., ì w King − ì w Queen = ì w Man − ì w Woman <ref type=""bibr"" target=""#b24"">[25]</ref>. Allen and Hospedales <ref type=""bibr"" target=""#b2"">[3]</ref> give a mathematical description of analogies a b28"">[29]</ref>), relations between sentences or entities (e.g., natural language inference <ref type=""bibr"" target=""#b38"">[39]</ref>, word analogies <ref type=""bibr"" target=""#b24"">[25]</ref>, entity relation extraction <ref type=""bibr"" target=""#b41"">[42]</ref>), or similarity between text pairs (i. , d t ) as its output. The hyperparameters for the considered systems are detailed in Section 3.6.</p><p>3.5.1 Doc2vec. With word2vec, Mikolov et al. <ref type=""bibr"" target=""#b24"">[25]</ref> introduced an algorithm to learn dense vector representations of words such that semantically similar words",0
"queries <ref type=""bibr"" target=""#b16"">[17]</ref>. Especially for complex information needs, the formulation of analogical queries is more intuitive <ref type=""bibr"" target=""#b23"">[24]</ref>. A system that supports analogical queries would be particularly beneficial for scientific literature since alogical query solving in the form of ""A is to B as C is to ?"" is a fundamental aspect of human intelligence <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b23"">24]</ref>. Chan et al. <ref type=""bibr"" target=""#b9"">[10]</ref> emphasize the importance of analogical query solving fo",0
"cts the many facets of extensive documents in digital libraries. It remains unclear to which of the many facets the similarity relates. In philosophy <ref type=""bibr"" target=""#b17"">[18]</ref>, but also in natural language processing (NLP) <ref type=""bibr"" target=""#b5"">[6]</ref>, the similarity of A",0
"b24"">[25]</ref>, entity relation extraction <ref type=""bibr"" target=""#b41"">[42]</ref>), or similarity between text pairs (i.e., binary classification <ref type=""bibr"" target=""#b15"">[16]</ref>). Our task is defined as as multi-class classification of document pairs consisting of multiple sentences. M",0
"length of 512 tokens due to absolute positional embeddings. However, XLNet integrates the relative positional encoding, as proposed in Transformer-XL <ref type=""bibr"" target=""#b13"">[14]</ref>. Therefore, XLNet's architecture is, in theory, not bound to a maximum sequence length. However, a custom pr",0
"on the content.</p><p>Giving its diversity and reach, Wikipedia had been used as a laboratory in which recommender system methodologies can be tested <ref type=""bibr"" target=""#b27"">[28,</ref><ref type=""bibr"" target=""#b35"">36]</ref>. In <ref type=""bibr"" target=""#b35"">[36]</ref>, we compared text-and data. One example of a digital library that employs an LRS is Wikipedia. Recommendations for Wikipedia articles have been addressed in the literature <ref type=""bibr"" target=""#b27"">[28,</ref><ref type=""bibr"" target=""#b35"">36]</ref>  <ref type=""foot"" target=""#foot_2"">3</ref> is connected to the artic",0
"GloVe word vectors (Section 3.5.2). The choice of dbow over the distributed memory training model is due to its results in semantic similarity tasks <ref type=""bibr"" target=""#b21"">[22]</ref>. It is important to mention that even though the embeddings model used both training and test sets, the latt",0
"models from BERT <ref type=""bibr"" target=""#b14"">[15]</ref> and XLNet <ref type=""bibr"" target=""#b40"">[41]</ref> in a vanilla and Siamese architecture <ref type=""bibr"" target=""#b8"">[9]</ref>. Each system is evaluated under specific configurations regarding its concatenation method and sequence length e=""bibr"" target=""#b42"">43]</ref>. Reimers and Gurevych <ref type=""bibr"" target=""#b33"">[34]</ref> proposed to combine BERT with a Siamese architecture <ref type=""bibr"" target=""#b8"">[9]</ref> for semantic representations of sentences and their similarity <ref type=""bibr"" target=""#b25"">[26]</ref>. In p ir original architecture is unchanged.  3.5.4 Siamese Transformer. We combine the two Transformers (BERT and XLNet) in a Siamese network architecture <ref type=""bibr"" target=""#b8"">[9]</ref>. In Siamese networks, two inputs are fed through identical sub-networks with shared weights (in this case, the",0
"se relations between text segments to generate a story for the segments. Moreover, BERT has successfully solved various document classification tasks <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b28"">29]</ref>. Akkalyoncu Yilmaz et al. <ref type=""bibr"" target=""#b1"">[2]</ref> appl",0
"ectly identified, while some relations are missing even if they are explicitly mentioned in the text. An analysis of the inner Transformer components <ref type=""bibr"" target=""#b10"">[11]</ref> is a subject for future work.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""5"">DISCUSSION</hea",0
"rity measures do not take into account the semantic relations that would underpin such a system. While other NLP tasks, like relation extraction (RE) <ref type=""bibr"" target=""#b41"">[42]</ref>, deal with relations, they are not concerned with semantic relations between documents. For instance, RE is al language inference <ref type=""bibr"" target=""#b38"">[39]</ref>, word analogies <ref type=""bibr"" target=""#b24"">[25]</ref>, entity relation extraction <ref type=""bibr"" target=""#b41"">[42]</ref>), or similarity between text pairs (i.e., binary classification <ref type=""bibr"" target=""#b15"">[16]</ref>).",0
"analogical queries would be particularly beneficial for scientific literature since the discovery of the analogies is crucial for scientific progress <ref type=""bibr"" target=""#b9"">[10]</ref>.</p><p>Nonetheless, document similarity measures do not take into account the semantic relations that would u is to ?"" is a fundamental aspect of human intelligence <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b23"">24]</ref>. Chan et al. <ref type=""bibr"" target=""#b9"">[10]</ref> emphasize the importance of analogical query solving for scientific progress. They propose a semi-automated a ation of document pairs consisting of multiple sentences. Moreover, the learning characteristic in our task requires considerably larger dataset than <ref type=""bibr"" target=""#b9"">[10]</ref> or <ref type=""bibr"" target=""#b19"">[20]</ref>. To the best of our knowledge, no established dataset fulfills t pora is one of the most challenging tasks. After all, even annotations can be solved efficiently as Chan et al.'s crowdsourcing approach demonstrates <ref type=""bibr"" target=""#b9"">[10]</ref>. We are confident that our results are transferable to other domains.</p></div> <div xmlns=""http://www.tei-c.",0
"Transformer architecture allowed the efficient unsupervised pretraining of language models and led to significant improvements in many NLP benchmarks <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b38"">39,</ref><ref type=""bibr"" target=""#b42"">43]</ref>. Reimers and Gurevych <ref t ed to combine BERT with a Siamese architecture <ref type=""bibr"" target=""#b8"">[9]</ref> for semantic representations of sentences and their similarity <ref type=""bibr"" target=""#b25"">[26]</ref>. In prior work <ref type=""bibr"" target=""#b31"">[32]</ref>, we also utilized a Siamese BERT model to determine",0
"an RNN-based system (similar to Hellendoorn et al. 2017b) by 37.0%, the Deep3 system (Raychev et al., 2016a) by 29.7%, and an adaptation of Code2Seq <ref type=""bibr"" target=""#b7"">(Alon et al., 2019a)</ref> for code prediction by 30.0%. These are significant margins.</p><p>We present in the paper se =""#b23"">, Hellendoorn et al., 2020</ref><ref type=""bibr"" target=""#b45"">, Yang and Xiang, 2019)</ref>. We include an adaptation of path-based Code2Seq <ref type=""bibr"" target=""#b7"">(Alon et al., 2019a)</ref> in our evaluations and show that our models significantly outperforms Code2Seq .</p></div> <d N <ref type=""bibr"" target=""#b22"">(Hellendoorn and Devanbu, 2017b)</ref>, Deep3 <ref type=""bibr"" target=""#b35"">(Raychev et al., 2016a)</ref>, Code2Seq <ref type=""bibr"" target=""#b7"">(Alon et al., 2019a)</ref>).</p><p>Fig 3 puts these models in perspective. Along the x-axis is an indication whether the tree based model (Deep3 <ref type=""bibr"" target=""#b35"">(Raychev et al., 2016a</ref>)) vs. TravTrans+ ; • from 43.6% to 73.6% when comparing Code2Seq <ref type=""bibr"" target=""#b7"">(Alon et al., 2019a)</ref>  Thus, we argue that our proposal of using Transformer+ with tree inputs for code prediction ev et al., 2014</ref><ref type=""bibr"" target=""#b39"">, Svyatkovskiy et al., 2019)</ref>; this model works on token sequences. We also include Code2Seq <ref type=""bibr"" target=""#b7"">(Alon et al., 2019a)</ref> to compare our efforts against a popular code embedding technique that works on ASTs, albeit inal task of Code2Seq was method summarization: given a method body, how well can Code2Seq generate the correct method name? The training proposed in <ref type=""bibr"" target=""#b7"">(Alon et al., 2019a)</ref> is not well suited for next token prediction. In code summarization, a set of leaf-to-leaf pa 017a</ref><ref type=""bibr"" target=""#b27"">, Karampatsis et al., 2020</ref><ref type=""bibr"" target=""#b28"">, Li et al., 2018)</ref>), to paths in an AST <ref type=""bibr"" target=""#b7"">(Alon et al., 2019a</ref><ref type=""bibr"">(Alon et al., ,b, 2020))</ref>, and sometimes even ways to convey static analy explore this similarity further in Sec 6.2.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.3"">Code2Seq</head><p>Code2Seq is a model by <ref type=""bibr"" target=""#b7"">Alon et al. 2019a</ref> that embeds code snippets by embedding AST paths in a neural network.</p><p>At a high-level, giv",1
"nguage processing (NLP) community have recently developed Transformers, a new neural architecture for even more effective natural language processing <ref type=""bibr"" target=""#b42"">(Vaswani et al., 2017)</ref>. Transformers overcome a major drawback of RNNs' ineffectiveness in capturing long-term de plained in <ref type=""bibr"" target=""#b15"">(Dai et al., 2019)</ref>.</p><p>A.3 Why not Positional Encoding? Some Transformers uses positional encoding <ref type=""bibr"" target=""#b42"">(Vaswani et al., 2017)</ref> or positional embedding <ref type=""bibr"" target=""#b33"">(Radford et al., 2019)</ref> to pro ample of self-attention for our SeqTrans model is presented in Sec 2.3.1. For other details (especially on the multi-head attention), please refer to <ref type=""bibr"" target=""#b42"">Vaswani et al. (2017)</ref> and in particular, <ref type=""bibr"">GPT-2 (Radford et al., 2019)</ref>, for a more thorough",1
"N</head><p>Last several years have witnessed exciting progress in the application of machine learning (ML) techniques to developer productivity tools <ref type=""bibr"" target=""#b5"">(Allamanis et al., 2018a)</ref>, and in particular, to code prediction <ref type=""bibr"" target=""#b13"">(Brockschmidt et a mise to offer a better ranking of code completion suggestions provided to a user by learning the statistical property of code, exploiting naturalness <ref type=""bibr"" target=""#b5"">(Allamanis et al., 2018a)</ref>.</p><p>Traditional ML-based techniques for Code Completion. Some of the early ML models",0
"ement to this sliding window technique would be to maintain the hidden states at each segment to pass along more context information, as explained in <ref type=""bibr"" target=""#b15"">(Dai et al., 2019)</ref>.</p><p>A.3 Why not Positional Encoding? Some Transformers uses positional encoding <ref type=""",0
"ntence entailment.</p><p>There has been a surge of interest since 2019 in extending Transformer models to handle beyond sequential structures for NLP <ref type=""bibr"" target=""#b1"">(Ahmed et al., 2019</ref><ref type=""bibr"" target=""#b32"">, Nguyen et al., 2020</ref><ref type=""bibr"">, Wang et al., 2019)",0
"r"" target=""#b5"">(Allamanis et al., 2018a)</ref>, and in particular, to code prediction <ref type=""bibr"" target=""#b13"">(Brockschmidt et al., 2019</ref><ref type=""bibr"" target=""#b24"">, Hindle et al., 2016</ref><ref type=""bibr"" target=""#b28"">, Li et al., 2018</ref><ref type=""bibr"">, Raychev et al., 201 a)</ref>.</p><p>Traditional ML-based techniques for Code Completion. Some of the early ML models for code prediction relied on n-gram language models <ref type=""bibr"" target=""#b24"">(Hindle et al., 2016</ref><ref type=""bibr"" target=""#b31"">, Nguyen et al., 2013</ref>). An n-gram language model compute",0
"luation shows that this significantly improves the results, when compared against the Transformer models as well as models from previous work (SeqRNN <ref type=""bibr"" target=""#b22"">(Hellendoorn and Devanbu, 2017b)</ref>, Deep3 <ref type=""bibr"" target=""#b35"">(Raychev et al., 2016a)</ref>, Code2Seq <r",0
"h they represent a program as an input to a neural architecture. These representations have ranged from linear token sequence (as for code prediction <ref type=""bibr"" target=""#b21"">(Hellendoorn and Devanbu, 2017a</ref><ref type=""bibr"" target=""#b27"">, Karampatsis et al., 2020</ref><ref type=""bibr"" ta",0
"way, any token from the sequence can more directly affect the next token prediction. Transformers have achieved or exceeded state-of-the-art results <ref type=""bibr"" target=""#b16"">(Devlin et al., 2018</ref><ref type=""bibr"" target=""#b17"">, Dong et al., 2019</ref><ref type=""bibr"" target=""#b33"">, Radf",0
"2016</ref><ref type=""bibr"">, Brockschmidt et al., 2019</ref><ref type=""bibr"" target=""#b19"">, Fernandes et al., 2019)</ref> and open-vocabulary models <ref type=""bibr"" target=""#b14"">(Cvitkovic et al., 2019</ref><ref type=""bibr"" target=""#b27"">, Karampatsis et al., 2020)</ref>.</p><p>Exposing Tree Stru",0
"machine learning-based methods apply various features <ref type=""bibr"" target=""#b31"">[32,</ref><ref type=""bibr"" target=""#b7"">8]</ref> and descriptors <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b5"">6]</ref>, and simply depend on the simila",1
"previous works focused on simulationbased methods (i.e., molecule docking and descriptors) or machine  learning-based models. For example, Li et al. <ref type=""bibr"" target=""#b15"">[16]</ref> proposed a docking method based on random forrest (RF). The RF model was also adopted in KronRLS <ref type=""",0
"ure"" target=""#fig_0"">1</ref>). Similar to the SMILES string, we propose to first encode the amino acids into a ddimensional vector following Prot2Vec <ref type=""bibr"" target=""#b1"">[2]</ref>, which allows us to capture local chemical information in targets/proteins. As a single amino acid often makes l|-1 ; x |l| ].</formula><p>For each biological word, we map it to an embedding vector by looking up a pretrained embedding dictionary for 9048 words <ref type=""bibr"" target=""#b1"">[2]</ref>, which is obtained from Swiss-Prot (https://www.uniprot.org/) with 560,118 manually annotated sequences. As a",0
"he target/protein and benefits many other bioinformatic applications <ref type=""bibr"" target=""#b27"">[28,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b24"">25]</ref>. As a result, DTA prediction has received much attention in recent years <ref type=""bibr"" target=""#b2"">[3,</r",0
"s machine learning tasks (e.g., image recognition and natural language processing), deep learning-based methods are also exploited for DTA prediction <ref type=""bibr"" target=""#b22"">[23]</ref>. These methods consider either label/one-hot encodings or the topological structure of molecules, they, howe o a single bioactivity score for drug-target interaction. The dataset contains 2,111 drugs and 229 targets with 118,254 interactions after processing <ref type=""bibr"" target=""#b22"">[23]</ref>.</p><p>We randomly split the datasets into 6 subsets with the equal size, and used five of them for training e Index (CI), r 2 m , and Area Under Precision Recall (AUPR) score.</p><p>MSE has been defined in the previous section as the objective of DeepGS. CI <ref type=""bibr"" target=""#b22"">[23]</ref> measures whether the predicted binding affinity values rank the corresponding drug-target interactions in th f type=""bibr"" target=""#b28"">[29]</ref>. Here, we transformed the datasets into binary datasets with predefined thresholds. We followed the prior work <ref type=""bibr"" target=""#b22"">[23]</ref> to select pK d value of 7 and 12.1 as threshold for the Davis and KIBA dataset, respectively.</p></div> <div loss function of cross entropy with MSE, and set the dimension of output layer to 1. The rest is consistent with the original paper.</p><p>? DeepDTA <ref type=""bibr"" target=""#b22"">[23]</ref>: DeepDTA trains two 3-layer CNNs with label/one-hot encodings of compound and protein sequences to predict D to 6 in terms of x-axis. This is because the pK d value of 5 constitutes more than half of the dataset (i.e., 20,931 out of 30,056, as reported from <ref type=""bibr"" target=""#b22"">[23]</ref>). In addition, we observe that the dense area of the KIBA score is in the range of 10 to 14 in terms of x-ax tein interactions, drugtarget interactions) and the polypharmacy side effects.</p><p>Among the research on deep learning for drug discovery, Deep-DTA <ref type=""bibr"" target=""#b22"">[23]</ref> and DeepCPI <ref type=""bibr"" target=""#b30"">[31]</ref> are the most relevant to our work. Both of them addres abel>2</label><figDesc>The average CI, MSE, r 2 m and AUPR scores on the Davis dataset. The results of KronRLS, SimBoost and DeepDTA are reported from<ref type=""bibr"" target=""#b22"">[23]</ref>.</figDesc><table><row><cell>Method</cell><cell>Drugs</cell><cell>Targets</cell><cell>CI</cell><cell>MSE</cel label>3</label><figDesc>The average CI, MSE, r 2 m and AUPR scores on the KIBA dataset. The results of KronRLS, SimBoost and DeepDTA are reported from<ref type=""bibr"" target=""#b22"">[23]</ref>.</figDesc><table><row><cell>Method</cell><cell>Drugs</cell><cell>Targets</cell><cell>CI</cell><cell>MSE</cel s/1.0""><head n=""3.1"">Experimental Setup</head></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.1.1"">Datasets</head><p>Following prior works <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b11"">12]</ref>, we employed widely-used datasets that are specialized for DTA predi",0
"social network structure. Recently, we propose a preliminary work of a neural influence Diff usion Network (i.e., DiffNet) for social recommendation <ref type=""bibr"" target=""#b42"">[43]</ref>. DiffNet models the recursive social diffusion process for each user, such that the influence diffusion hidd endation performance <ref type=""bibr"" target=""#b18"">[19]</ref>, <ref type=""bibr"" target=""#b19"">[20]</ref>, <ref type=""bibr"" target=""#b13"">[14]</ref>, <ref type=""bibr"" target=""#b42"">[43]</ref>.</p><p>In fact, as users play a central role in social platforms with user-user social behavior and user-ite two kinds of graphs <ref type=""bibr"" target=""#b50"">[51]</ref>, <ref type=""bibr"" target=""#b40"">[41]</ref>, <ref type=""bibr"" target=""#b47"">[48]</ref>, <ref type=""bibr"" target=""#b42"">[43]</ref>. On one hand, given the useritem interest graph, NGCF is proposed to directly encode the collaborative infor cial diffusion process in the social network, such that the higher-order social structure is directly modeled in the recursive user embedding process <ref type=""bibr"" target=""#b42"">[43]</ref>. These graph based models showed superior performance compared to the previous non-graph based recommendatio n a graph, and then from different graphs. In summary, our main contributions are listed as follows:</p><p>? Compared to our previous work of DiffNet <ref type=""bibr"" target=""#b42"">[43]</ref>, we revisit the social recommendation problem as predicting the missing edges in the user-item interest grap raph, recently we propose a preliminary graph based social recommendation model, DiffNet, for modeling the social diffusion process in recommendation <ref type=""bibr"" target=""#b42"">[43]</ref>. DiffNet advances classical embedding based models with carefully designed influence diffusion layers, such ess for iterative user embedding learning, such that the up to K-th order social network structure is injected into the social recommendation process <ref type=""bibr"" target=""#b42"">[43]</ref>. In this part, we propose DiffNet++, an enhanced model of DiffNet that fuses both influence diffusion in the e four datasets, Yelp and Flickr are two datasets with user and item attributes, and are adopted as datasets of our previously proposed DiffNet model <ref type=""bibr"" target=""#b42"">[43]</ref>. The remaining two datasets of Epinions and Dianping do not contain user and item attributes. We use the sam tem graph with both user and item features as input, in order to transform this model for the recommendation task. For our proposed models of DiffNet <ref type=""bibr"" target=""#b42"">[43]</ref> and DiffNet++, since both models are flexible and could be reduced to simpler versions without user and item cs, Hit Ratio (HR) <ref type=""bibr"" target=""#b8"">[9]</ref> and Normalized Discounted Cummulative Gain (NDCG) <ref type=""bibr"" target=""#b8"">[9]</ref>, <ref type=""bibr"" target=""#b42"">[43]</ref>. Specifically, HR measures the percentage of hit items in the top-N list, and NDCG puts more emphasis on the ranked items. As we focus on the top-N ranking performance with large itemset, similar as many other works <ref type=""bibr"" target=""#b16"">[17]</ref>, <ref type=""bibr"" target=""#b42"">[43]</ref>, to evaluate the performance, for each user, we randomly select 1000 unrated items that a user has not inter",1
"tures by iteratively convolutional aggregations from neighborhood nodes, such that the up to K-th order graph structure is captured with K iterations <ref type=""bibr"" target=""#b46"">[47]</ref>. By treating user-item interactions as a bipartite interest graph and user-user social network as a social g",0
". Given a user's rated item history, NAIS is proposed to learn the neural attentive weights for item similarity in item based collaborative filtering <ref type=""bibr"" target=""#b15"">[16]</ref>. For graph structure data, researchers proposed graph attention networks to attentively learn weights of eac we first group users into different interest groups based on the number of observed ratings of each user. E.g, <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b15"">16)</ref> means each user has at least 8 rating records and less than 16    <ref type=""figure"" target=""#fig_4"">3</ref>(",0
"ial influence theory, users in a social network would influence each other, leading to similar preferences <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" target=""#b1"">[2]</ref>. Therefore, social recommendation has emerged, which focuses on exploiting social relations among users to all",0
"v 0 i ||v 1 i ||...||v K i ].</formula><p>After that, the predicted rating is modeled as the inner product between the final user and item embeddings <ref type=""bibr"" target=""#b6"">[7]</ref>:</p><formula xml:id=""formula_27"">rai = [u 0 a ||u 1 a ||...||u K a ] T [v 0 i ||v 1 i ||...||v K i ].<label>(1 ing problem, we adopt the prediction layer as the LR-GCCF model, which receives state-of-the-art performance with user-item bipartite graph structure <ref type=""bibr"" target=""#b6"">[7]</ref>. In LR-GCCF, Chen et al. carefully analyzed the simple concatenation of entity embedding at each layer is equi entity embedding at each layer is equivalent to residual preference learning, and why this simple operation could alleviate the over-smoothing issue <ref type=""bibr"" target=""#b6"">[7]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.2"">Model Training</head><p>We use a pair-wise r",0
"of the distinguishing power of some GNN variants has been initiated. In two independent studies <ref type=""bibr"" target=""#b15"">[Xu et al., 2019</ref><ref type=""bibr"" target=""#b11"">, Morris et al., 2019]</ref> the distinguishing power of GNNs is linked to the distinguishing power of the classical We aware MPNNs that do use degree information. The former class of MPNNs covers the GNNs studied in <ref type=""bibr"" target=""#b15"">[Xu et al., 2019</ref><ref type=""bibr"" target=""#b11"">, Morris et al., 2019]</ref>, the latter class covers the GCNs <ref type=""bibr"" target=""#b8"">[Kipf and Welling, 2017]</ d degree-aware MPNNs matches that of the WL algorithm.</p><p>For anonymous MPNNs related to GNNs <ref type=""bibr"" target=""#b15"">[Xu et al., 2019</ref><ref type=""bibr"" target=""#b11"">, Morris et al., 2019]</ref> and degree-aware MPNNs related to GCNs <ref type=""bibr"" target=""#b8"">[Kipf and Welling, 20 p>Example 3.1 (GNN architectures). We first consider the graph neural network architectures <ref type=""bibr"" target=""#b4"">[Hamilton et al., 2017</ref><ref type=""bibr"" target=""#b11"">, Morris et al., 2019]</ref> defined by:</p><formula xml:id=""formula_8"">L (t) := σ L (t−1) W (t) 1 + AL (t−1) W (t) 2 + short) is well understood. Indeed, as we will shortly see, it follows from two independent works <ref type=""bibr"" target=""#b15"">[Xu et al., 2019</ref><ref type=""bibr"" target=""#b11"">, Morris et al., 2019]</ref> that the distinguishing power of aMPNNs can be linked to the distinguishing power of the W as is indicated in Figure <ref type=""figure"" target=""#fig_1"">1</ref>. Proposition 5.2 (Based on <ref type=""bibr"" target=""#b15"">[Xu et al., 2019</ref><ref type=""bibr"" target=""#b11"">, Morris et al., 2019]</ref>). The classes M anon and M WL are equally strong.</p><p>Proof. First, we prove that M WL i ures of graph neural networks. We again distinguish between anonymous graph neural networks <ref type=""bibr"" target=""#b4"">[Hamilton et al., 2017</ref><ref type=""bibr"" target=""#b11"">, Morris et al., 2019]</ref> and degree-aware graph neural networks <ref type=""bibr"" target=""#b8"">[Kipf and</ref><ref t ounded by the WL algorithm. This result can be seen as a slight generalisation of the results in <ref type=""bibr"" target=""#b15"">[Xu et al., 2019</ref><ref type=""bibr"" target=""#b11"">, Morris et al., 2019</ref>]. (ii) The distinguishing power of degree-aware MPNNs is bounded by the WL algorithm, but t input graph, the WL algorithm can be simulated, step-by-step, by GNNs that use ReLU or sign as activation function. This result refines the result in <ref type=""bibr"" target=""#b11"">[Morris et al., 2019]</ref> in that their simulation using the ReLU function requires two GNN ""layers"" for each step of enoted M anon . Finally, we introduce two classes of aMPNNs which are of special interest: those arising from the graph neural networks considered in <ref type=""bibr"" target=""#b11"">[Morris et al., 2019]</ref>. In Example 3.1 we established that such graph neural networks correspond to aMPNNs. Let us ker than M WL . The proof is a trivial adaptation of the proofs of Lemma 2 in <ref type=""bibr"" target=""#b15"">[Xu et al., 2019]</ref> and Theorem 5 in <ref type=""bibr"" target=""#b11"">[Morris et al., 2019]</ref>. We show, by induction on the number of rounds of computation, that ℓ ℓ ℓ</p><formula xml:i ℓ ℓ (t) M ) w ,</formula><p>as desired.</p><p>We remark that we cannot use the results in <ref type=""bibr"" target=""#b15"">[Xu et al., 2019]</ref> and <ref type=""bibr"" target=""#b11"">[Morris et al., 2019]</ref> as a black box because the class M anon is more general than the class considered in those the class M anon is more general than the class considered in those papers. The proofs in <ref type=""bibr"" target=""#b15"">[Xu et al., 2019]</ref> and <ref type=""bibr"" target=""#b11"">[Morris et al., 2019]</ref> relate to graph neural networks which, in round t ≥ 1, compute for each vertex v a label ℓ llenging is to show that M sign GNN , M ReLU GNN and M WL , and thus also M anon , are equally strong. The following results are known. Theorem 5.5 ( <ref type=""bibr"" target=""#b11"">[Morris et al., 2019]</ref>). (i) The classes M sign GNN and M WL are equally strong. (ii) The class M ReLU GNN is weak show that this factor of two can be avoided. As a side effect, we obtain a simpler aMPNN M in M GNN , satisfying M WL M , than the one constructed in <ref type=""bibr"" target=""#b11"">[Morris et al., 2019]</ref>. The proof strategy is inspired by that of <ref type=""bibr"" target=""#b11"">[Morris et al., 2 satisfying M WL M , than the one constructed in <ref type=""bibr"" target=""#b11"">[Morris et al., 2019]</ref>. The proof strategy is inspired by that of <ref type=""bibr"" target=""#b11"">[Morris et al., 2019]</ref>. Crucial in the proof is the notion of row-independence modulo equality, which we define ne we only have one weight matrix per round, instead of two, at the cost of introducing an extra parameter p ∈ A. Furthermore, the aMPNN constructed in <ref type=""bibr"" target=""#b11"">[Morris et al., 2019]</ref> uses two distinct weight matrices in A (st−1+s0)×(st+s0) (we come back to this at the end o in uniq(L (t−1) ) by a 1 , . . . , a m ∈ A st−1 . By the induction hypothesis, these rows are linearly independent. Following the same argument as in <ref type=""bibr"" target=""#b11"">[Morris et al., 2019]</ref>, this implies that there exists an (s t−1 × m)-matrix U (t)  such that uniq(L (t−1) )U (t) ensure row-independence modulo equality and make sure the labelling ""refines"" ℓ ℓ ℓ (t) MWL . To do so, we again follow closely the proof strategy of <ref type=""bibr"" target=""#b11"">[Morris et al., 2019]</ref>. More specifically, we will need an analogue of the following result. In the sequel we deno lt. In the sequel we denote by J a matrix with all entries having value 1 and whose size will be determined from the context. Lemma 5.9 (Lemma 9 from <ref type=""bibr"" target=""#b11"">[Morris et al., 2019]</ref>). Let C ∈ A m×w be a matrix in which all entries are non-negative and all rows are pairwise om 0. Also, λ λ λ</p><p>w implies that the positions of the non-zero entries in µ µ µ Regarding future work, we point out that, following the work of <ref type=""bibr"" target=""#b11"">[Morris et al., 2019]</ref>, we fix the input graph in our analysis. We use this particularly when we prove that certai",1
"eature-based task.</p><p>Only recently a formal study of the distinguishing power of some GNN variants has been initiated. In two independent studies <ref type=""bibr"" target=""#b15"">[Xu et al., 2019</ref><ref type=""bibr"" target=""#b11"">, Morris et al., 2019]</ref> the distinguishing power of GNNs is l MPNNs that do not use degree information, and degree-aware MPNNs that do use degree information. The former class of MPNNs covers the GNNs studied in <ref type=""bibr"" target=""#b15"">[Xu et al., 2019</ref><ref type=""bibr"" target=""#b11"">, Morris et al., 2019]</ref>, the latter class covers the GCNs <re >(i) The distinguishing power of anonymous MPNNs is bounded by the WL algorithm. This result can be seen as a slight generalisation of the results in <ref type=""bibr"" target=""#b15"">[Xu et al., 2019</ref><ref type=""bibr"" target=""#b11"">, Morris et al., 2019</ref>]. (ii) The distinguishing power of deg e distinguishing power of the classes of anonymous and degree-aware MPNNs matches that of the WL algorithm.</p><p>For anonymous MPNNs related to GNNs <ref type=""bibr"" target=""#b15"">[Xu et al., 2019</ref><ref type=""bibr"" target=""#b11"">, Morris et al., 2019]</ref> and degree-aware MPNNs related to GCN tinguishing power of anonymous MPNNs (or aMPNNs, for short) is well understood. Indeed, as we will shortly see, it follows from two independent works <ref type=""bibr"" target=""#b15"">[Xu et al., 2019</ref><ref type=""bibr"" target=""#b11"">, Morris et al., 2019]</ref> that the distinguishing power of aMPN em 5.1 we only need that M anon is weaker than M WL , as is indicated in Figure <ref type=""figure"" target=""#fig_1"">1</ref>. Proposition 5.2 (Based on <ref type=""bibr"" target=""#b15"">[Xu et al., 2019</ref><ref type=""bibr"" target=""#b11"">, Morris et al., 2019]</ref>). The classes M anon and M WL are equ ixed s ∈ N + . We cast the WL algorithm as an anonymous MPNN by using an injection h : A s → Q. What follows is in fact an adaptation of Lemma 5 from <ref type=""bibr"" target=""#b15"">[Xu et al., 2019]</ref> itself based on <ref type=""bibr"">[Zaheer et al., 2017, Theorem 2]</ref>. We crucially rely on t o note that M WL ⊆ M anon .</p><p>It remains to argue that M anon is weaker than M WL . The proof is a trivial adaptation of the proofs of Lemma 2 in <ref type=""bibr"" target=""#b15"">[Xu et al., 2019]</ref> and Theorem 5 in <ref type=""bibr"" target=""#b11"">[Morris et al., 2019]</ref>. We show, by induct M ) v , m (t) v = UPD (t) (ℓ ℓ ℓ (t−1) M ) w , m (t) w = (ℓ ℓ ℓ (t) M ) w ,</formula><p>as desired.</p><p>We remark that we cannot use the results in <ref type=""bibr"" target=""#b15"">[Xu et al., 2019]</ref> and <ref type=""bibr"" target=""#b11"">[Morris et al., 2019]</ref> as a black box because the class t=""#b11"">[Morris et al., 2019]</ref> as a black box because the class M anon is more general than the class considered in those papers. The proofs in <ref type=""bibr"" target=""#b15"">[Xu et al., 2019]</ref> and <ref type=""bibr"" target=""#b11"">[Morris et al., 2019]</ref> relate to graph neural networks la_42"">(t) aggr { {ℓ ℓ ℓ (t−1) u | u ∈ N G (v)} } can be written in the form g (t) u∈NG(v) h (t) (ℓ ℓ ℓ (t−1) u</formula><p>) , based on Lemma 5 from <ref type=""bibr"" target=""#b15"">[Xu et al., 2019]</ref>.</p><p>Suppose that ν ν ν : V → A s0 . It now suffices to define for every t ≥ 1, every x and y",1
"onveniently, it is known that the operations we will need are indeed computable for algebraic numbers encoded using such a representation (see, e.g., <ref type=""bibr"" target=""#b12"">[Ouaknine and Worrell, 2014]</ref>).</p><p>Labelled graphs. Let G = (V, E) be an undirected graph consisting of n ∈ N v",0
"kas, 2019]</ref> shows Turing universality of position-aware MPNNs using close connections with the LOCAL model for distributed graph computations of <ref type=""bibr"" target=""#b0"">[Angluin, 1980]</ref>. As such, MPNNs from <ref type=""bibr"" target=""#b9"">[Loukas, 2019]</ref> can simulate our MPNNs as",0
"hen the number of distinct labels in ℓ ℓ ℓ (t) and ℓ ℓ ℓ (t−1) is the same, the WL algorithm terminates. Termination is guaranteed in at most n steps <ref type=""bibr"" target=""#b5"">[Immerman and Lander, 1990]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3"">Message Passing Neural",0
"e <ref type=""table"">1</ref>: Various graph neural network formalisms, as reported in e.g., <ref type=""bibr"" target=""#b8"">[Kipf and Welling, 2017</ref><ref type=""bibr"" target=""#b13"">, Wu et al., 2019a</ref><ref type=""bibr"" target=""#b10"">, Meltzer et al., 2019]</ref>, which correspond to degree-aware",0
"graph neural networks in <ref type=""bibr"" target=""#b6"">[Jaume et al., 2019]</ref>.</p><p>We also want to compare our formalisation to the MPNNs from <ref type=""bibr"" target=""#b9"">[Loukas, 2019]</ref>. In that paper, the message functions can depend on identifiers of the vertices involved. Such posi vertices involved. Such position-aware MPNNs correspond to MPNNs in our setting in which f assigns to each vertex a unique identifier. We remark that <ref type=""bibr"" target=""#b9"">[Loukas, 2019]</ref> shows Turing universality of position-aware MPNNs using close connections with the LOCAL model for close connections with the LOCAL model for distributed graph computations of <ref type=""bibr"" target=""#b0"">[Angluin, 1980]</ref>. As such, MPNNs from <ref type=""bibr"" target=""#b9"">[Loukas, 2019]</ref> can simulate our MPNNs as one could add a few initialisation rounds to compute f (v) and f (u). We as, 2019]</ref> can simulate our MPNNs as one could add a few initialisation rounds to compute f (v) and f (u). We also remark that in the MPNNs from <ref type=""bibr"" target=""#b9"">[Loukas, 2019]</ref> every vertex can also send itself a message. We provide this functionality by parameterising the up",0
"rk formalisms, as reported in e.g., <ref type=""bibr"" target=""#b8"">[Kipf and Welling, 2017</ref><ref type=""bibr"" target=""#b13"">, Wu et al., 2019a</ref><ref type=""bibr"" target=""#b10"">, Meltzer et al., 2019]</ref>, which correspond to degree-aware MPNNs. We implicitly assume the presence of a bias matr",0
"DA <ref type=""bibr"" target=""#b24"">(Srivastava and Sutton, 2017)</ref>, a state-of-the-art topic model that implements black-box variational inference <ref type=""bibr"" target=""#b20"">(Ranganath et al., 2014)</ref>, to include BERT representations. Our approach leads to consistent significant improveme",1
"his effect is even more remarkable given that we cannot embed long documents, due to the sentence length limit in BERT. Concretely, we extend ProdLDA <ref type=""bibr"" target=""#b24"">(Srivastava and Sutton, 2017)</ref>, a state-of-the-art topic model that implements black-box variational inference <re lowing the state-of-the-art, we consider k = 10.</p><p>Baselines We compare our approach with the following baselines: (i) Neural-ProdLDA (N-ProdLDA) <ref type=""bibr"" target=""#b24"">(Srivastava and Sutton, 2017)</ref>  <ref type=""foot"" target=""#foot_5"">5</ref> (the model we extended);<ref type=""foot"" onverts the BoW into embeddings. This final representation is again passed through a hidden layer before the variational inference process. We follow <ref type=""bibr"" target=""#b24"">(Srivastava and Sutton, 2017)</ref> for the choice of the parameters. The priors over the topic and document distributi igure1: High-level sketch of our Neural Topic Modeling with BERT embeddings. More details of the architecture we extend are found in the original work<ref type=""bibr"" target=""#b24"">(Srivastava and Sutton, 2017)</ref>.</figDesc></figure> <figure xmlns=""http://www.tei-c.org/ns/1.0"" xml:id=""fig_1""><hea number of topics, including statistical significance. In general, our model provides the most coherent topics across all corpora and topic settings. <ref type=""bibr"" target=""#b24"">Srivastava and Sutton (2017)</ref> reported that NVDM obtains low coherence. We observe that our model suffers the  mos f> or neural variational inference <ref type=""bibr"" target=""#b13"">(Miao et al., 2016;</ref><ref type=""bibr"" target=""#b15"">Mnih and Gregor, 2014;</ref><ref type=""bibr"" target=""#b24"">Srivastava and Sutton, 2017;</ref><ref type=""bibr"" target=""#b12"">Miao et al., 2017;</ref><ref type=""bibr"" target=""#b7""> datasets in the literature are already pre-processed.</note> 			<note xmlns=""http://www.tei-c.org/ns/1.0"" place=""foot"" n=""5"" xml:id=""foot_5"">Note that<ref type=""bibr"" target=""#b24"">(Srivastava and Sutton, 2017</ref>) also propose Neural-LDA, which has been found to be scarcely effective in topic mod tava and Sutton, 2017</ref>) also propose Neural-LDA, which has been found to be scarcely effective in topic modeling by different researchers, though<ref type=""bibr"" target=""#b24"">(Srivastava and Sutton, 2017;</ref><ref type=""bibr"" target=""#b25"">Wang et al., 2020)</ref>. Our first results with this",1
">Nguyen et al., 2015;</ref><ref type=""bibr"" target=""#b18"">Petterson et al., 2010)</ref>, use word relationships derived from external knowledge bases <ref type=""bibr"" target=""#b3"">(Chen et al., 2013;</ref><ref type=""bibr"" target=""#b28"">Yang et al., 2015)</ref>, or pre-trained word embeddings <ref ty",0
"sured in a variety of ways, from human evaluation via intrusion tests <ref type=""bibr"" target=""#b2"">(Chang et al., 2009)</ref> to approximated scores <ref type=""bibr"" target=""#b11"">(Lau et al., 2014;</ref><ref type=""bibr"" target=""#b21"">Röder et al., 2015)</ref>.</p><p>Topic models have inspired many",0
"""foot"" target=""#foot_6"">6</ref> (ii) Neural Variational Document Model (NVDM) <ref type=""bibr"" target=""#b13"">(Miao et al., 2016)</ref>; and (iii) LDA <ref type=""bibr"" target=""#b0"">(Blei et al., 2003)</ref>.</p><p>Configurations We train all models with the same hyper-parameter configurations to maxi",0
"3;</ref><ref type=""bibr"" target=""#b28"">Yang et al., 2015)</ref>, or pre-trained word embeddings <ref type=""bibr"" target=""#b4"">(Das et al., 2015;</ref><ref type=""bibr"" target=""#b6"">Dieng et al., 2019;</ref><ref type=""bibr"" target=""#b16"">Nguyen et al., 2015)</ref>. Even for neural topic models, there c models, there exists little work on incorporating external knowledge, e.g., word embeddings <ref type=""bibr"" target=""#b8"">(Gupta et al., 2019;</ref><ref type=""bibr"" target=""#b6"">Dieng et al., 2019)</ref>. However, most still use Bag of Words (BoW) document representations as model input, rather th",0
"incorporate several types of information <ref type=""bibr"" target=""#b27"">(Xun et al., 2017;</ref><ref type=""bibr"" target=""#b4"">Das et al., 2015;</ref><ref type=""bibr"" target=""#b16"">Nguyen et al., 2015;</ref><ref type=""bibr"" target=""#b18"">Petterson et al., 2010)</ref>, use word relationships derived 5)</ref>, or pre-trained word embeddings <ref type=""bibr"" target=""#b4"">(Das et al., 2015;</ref><ref type=""bibr"" target=""#b6"">Dieng et al., 2019;</ref><ref type=""bibr"" target=""#b16"">Nguyen et al., 2015)</ref>. Even for neural topic models, there exists little work on incorporating external knowledge,",0
"pe=""bibr"" target=""#b23"">Salakhutdinov and Hinton, 2009)</ref> or neural variational inference <ref type=""bibr"" target=""#b13"">(Miao et al., 2016;</ref><ref type=""bibr"" target=""#b15"">Mnih and Gregor, 2014;</ref><ref type=""bibr"" target=""#b24"">Srivastava and Sutton, 2017;</ref><ref type=""bibr"" target=""#",0
"/p><p>Topic models have inspired many extensions that incorporate several types of information <ref type=""bibr"" target=""#b27"">(Xun et al., 2017;</ref><ref type=""bibr"" target=""#b4"">Das et al., 2015;</ref><ref type=""bibr"" target=""#b16"">Nguyen et al., 2015;</ref><ref type=""bibr"" target=""#b18"">Petterson bases <ref type=""bibr"" target=""#b3"">(Chen et al., 2013;</ref><ref type=""bibr"" target=""#b28"">Yang et al., 2015)</ref>, or pre-trained word embeddings <ref type=""bibr"" target=""#b4"">(Das et al., 2015;</ref><ref type=""bibr"" target=""#b6"">Dieng et al., 2019;</ref><ref type=""bibr"" target=""#b16"">Nguyen et",0
">(Miao et al., 2016;</ref><ref type=""bibr"" target=""#b15"">Mnih and Gregor, 2014;</ref><ref type=""bibr"" target=""#b24"">Srivastava and Sutton, 2017;</ref><ref type=""bibr"" target=""#b12"">Miao et al., 2017;</ref><ref type=""bibr"" target=""#b7"">Ding et al., 2018)</ref>. <ref type=""bibr"" target=""#b13"">Miao et plicitly approximates the Dirichlet prior, using a Gaussian distribution to obtain more interpretable and coherent topics. Similar to their approach, <ref type=""bibr"" target=""#b12"">Miao et al. (2017)</ref> parameterize the multinomial distributions of each document, proposing three variants of neura",0
"l structured information, such as knowledge graph and Wikipedia hyperlinks, has also been explored recently <ref type=""bibr"">(Min et al., 2019b;</ref><ref type=""bibr"" target=""#b0"">Asai et al., 2020)</ref>. The use of dense vector representations for retrieval has a long history since Latent Semantic",1
""">(Berant et al., 2013</ref>) consists of questions selected using Google Suggest API, where the answers are entities in Freebase. CuratedTREC (TREC) <ref type=""bibr"" target=""#b1"">(Baudi? and ?ediv?, 2015)</ref>  </p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>Selection of positive passage",1
"nces in each batch, where there are B -1 negative passages for each question. The trick of in-batch negatives has been used in the full batch setting <ref type=""bibr"" target=""#b47"">(Yih et al., 2011)</ref> and more recently for mini-batch <ref type=""bibr"" target=""#b13"">(Henderson et al., 2017;</ref> "">(Deerwester et al., 1990)</ref>. Using labeled pairs of queries and documents, discriminatively trained dense encoders have become popular recently <ref type=""bibr"" target=""#b47"">(Yih et al., 2011;</ref><ref type=""bibr"" target=""#b14"">Huang et al., 2013;</ref><ref type=""bibr"" target=""#b10"">Gillick",0
"ken as the output, so d = 768.</p><p>Inference During inference time, we apply the passage encoder E P to all the passages and index them using FAISS <ref type=""bibr"" target=""#b17"">(Johnson et al., 2017)</ref> offline. FAISS is an extremely efficient, open-source library for similarity search and cl",0
"l can be done efficiently using maximum inner product search (MIPS) algorithms (e.g., <ref type=""bibr"" target=""#b38"">Shrivastava and Li (2014)</ref>; <ref type=""bibr"" target=""#b11"">Guo et al. (2016)</ref>).</p><p>However, it is generally believed that learning a good dense vector representation need",0
"swering. The questions were mined from real Google search queries and the answers were spans in Wikipedia articles identified by annotators. TriviaQA <ref type=""bibr"" target=""#b18"">(Joshi et al., 2017)</ref> contains a set of trivia questions with answers that were originally scraped from the Web. W",0
"amples in each training iteration. Starting from our trained DPR model, they show that the retrieval performance can be further improved. Recent work <ref type=""bibr"" target=""#b16"">(Izacard and Grave, 2020;</ref><ref type=""bibr"">Lewis et al., 2020b)</ref> have also shown that DPR can be combined wit",0
"s on VQA and NLVR2.  VQA One major vision-and-language understanding tasks for the existing VLP models is VQA. The SoTA result for VQA is from UNITER <ref type=""bibr"" target=""#b6"">[6]</ref> large model. Table <ref type=""table"" target=""#tab_3"">6</ref> summarized the evaluation results with the recent the recent VLP work on VQA task, we can see that Oscar B is the best among the models with equivalent size, even slightly better (0.04%) than UNITER <ref type=""bibr"" target=""#b6"">[6]</ref> large. And the Oscar L improves the SoTA overall accuracy with 0.42% on the test-std split.</p><p>NLVR2 Anothe y with 0.42% on the test-std split.</p><p>NLVR2 Another major task for the existing VLP models is NLVR2. Similarly, the SoTA model on NLVR2 is UNITER <ref type=""bibr"" target=""#b6"">[6]</ref> large. As reported in Table <ref type=""table"" target=""#tab_4"">7</ref>, with the equivalent model sizes, Oscar ><ref type=""bibr"" target=""#b36"">36,</ref><ref type=""bibr"" target=""#b19"">19,</ref><ref type=""bibr"" target=""#b10"">10]</ref> employ BERT-like objectives <ref type=""bibr"" target=""#b6"">[6]</ref> to learn crossmodal representations from a concatenated-sequence of visual region features and language token",1
"1"">[21]</ref>, Conceptual Captions (CC) <ref type=""bibr"" target=""#b32"">[32]</ref>, SBU captions <ref type=""bibr"" target=""#b27"">[27]</ref>, flicker30k <ref type=""bibr"" target=""#b45"">[45]</ref>, GQA <ref type=""bibr"" target=""#b13"">[13]</ref> etc. As shown in Table <ref type=""table"" target=""#tab_0"">1</r",0
"> pr regions into a common space using kernelized canonical co achieved excellent results for annotation and segmentati employed for image captioning <ref type=""bibr"" target=""#b15"">[15]</ref> and text-based image ular, the seminal work DeViSE <ref type=""bibr"" target=""#b9"">[9]</ref> was proposed to i cross thousands of novel labels that have never been seen by the vision model. The idea has been extended in <ref type=""bibr"" target=""#b35"">[35,</ref><ref type=""bibr"" target=""#b15"">15,</ref><ref type=""bibr"" target=""#b26"">26]</ref>, showing that leveraging pre-trained linguistic knowledge is highly e",0
"wn in Table <ref type=""table"" target=""#tab_8"">9</ref>, compared with existing VLP works (LXMERT <ref type=""bibr"" target=""#b37"">[37]</ref> and 12-in-1 <ref type=""bibr"" target=""#b24"">[24]</ref>), our Oscar B results on GQA gain 0.6% accuracy, which still demonstrates the superiority of Oscar pretraini concatenate the image regions and then feed into the UNITER model. An MLP transform is applied  <ref type=""bibr"" target=""#b4"">[4]</ref> 60.83 12-in-1 <ref type=""bibr"" target=""#b24"">[24]</ref> 60.65 NSM <ref type=""bibr"" target=""#b12"">[12]</ref> 63.  Image Retrieval R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R type=""bibr"" target=""#b33"">[33]</ref>, PFAN <ref type=""bibr"" target=""#b41"">[41]</ref>, Unicoder-VL <ref type=""bibr"" target=""#b19"">[19]</ref>, 12-in-1 <ref type=""bibr"" target=""#b24"">[24]</ref>, UNITER <ref type=""bibr"" target=""#b5"">[5]</ref>.</p><p>Image Captioning Though the training objective (i.e., compare with four existing methods, including LXMERT <ref type=""bibr"" target=""#b39"">[39]</ref>, MMN <ref type=""bibr"" target=""#b4"">[4]</ref>, 12-in-1 <ref type=""bibr"" target=""#b24"">[24]</ref>, NSM <ref type=""bibr"" target=""#b12"">[12]</ref>.</p><p>NLVR2 For the Oscar B model, we fine-tune for 20 epoch",0
"framework to extract useful features from the heterogeneous textrich network. Meta-paths <ref type=""bibr"" target=""#b32"">[31]</ref> and motif patterns <ref type=""bibr"" target=""#b6"">[5,</ref><ref type=""bibr"" target=""#b19"">18]</ref> have been widely adopted to extract useful structural information from s, such as neuroscience <ref type=""bibr"" target=""#b31"">[30]</ref>, bioinformatics <ref type=""bibr"" target=""#b19"">[18]</ref>, and information networks <ref type=""bibr"" target=""#b6"">[5]</ref>. In the context of heterogeneous information networks, network motifs, sometimes also referred to as meta-grap",1
"om the heterogeneous textrich network. Meta-paths <ref type=""bibr"" target=""#b32"">[31]</ref> and motif patterns <ref type=""bibr"" target=""#b6"">[5,</ref><ref type=""bibr"" target=""#b19"">18]</ref> have been widely adopted to extract useful structural information from networks. As illustrated in Figure <re ructures that are critical in complex networks across various domains, such as neuroscience <ref type=""bibr"" target=""#b31"">[30]</ref>, bioinformatics <ref type=""bibr"" target=""#b19"">[18]</ref>, and information networks <ref type=""bibr"" target=""#b6"">[5]</ref>. In the context of heterogeneous informati",1
"remains a major challenge. We leverage motif patterns in our framework to extract useful features from the heterogeneous textrich network. Meta-paths <ref type=""bibr"" target=""#b32"">[31]</ref> and motif patterns <ref type=""bibr"" target=""#b6"">[5,</ref><ref type=""bibr"" target=""#b19"">18]</ref> have been ork motifs, sometimes also referred to as meta-graphs, can offer more flexibility and capture richer network semantics than the widely used meta-path <ref type=""bibr"" target=""#b32"">[31]</ref> patterns. Recent studies have shown that incorporating motifs for node embedding leads to superior performan f instances can be represented by the combination of two authors (i.e., ""Jure Leskovec"" and ""Jon Kleinberg"").</p><p>It is worth noting that meta-path <ref type=""bibr"" target=""#b32"">[31]</ref> can be viewed as a special case of motif patterns when they degenerate to lines. For example, the meta-path",1
"benefits various downstream applications, such as search and indexing <ref type=""bibr"" target=""#b44"">[43]</ref>, personalized content recommendation <ref type=""bibr"" target=""#b47"">[46]</ref>, and question answering <ref type=""bibr"" target=""#b43"">[42]</ref>. For example, organizing copious scientifi",0
"hen organize the extracted pairs into a taxonomy by applying algorithms like maximum spanning tree. The lexical patterns are either manually designed <ref type=""bibr"" target=""#b15"">[14,</ref><ref type=""bibr"" target=""#b22"">21,</ref><ref type=""bibr"" target=""#b24"">23,</ref><ref type=""bibr"" target=""#b26",0
"f type=""bibr"" target=""#b32"">[31]</ref> patterns. Recent studies have shown that incorporating motifs for node embedding leads to superior performance <ref type=""bibr"" target=""#b25"">[24,</ref><ref type=""bibr"" target=""#b42"">41,</ref><ref type=""bibr"" target=""#b46"">45]</ref> compared to conventional pat",0
"every taxonomy node.</p><p>Term embedding learning is typically conducted on the entire document collection <ref type=""bibr"" target=""#b18"">[17,</ref><ref type=""bibr"" target=""#b23"">22]</ref>. However, such learning paradigm faces a major drawback in topic taxonomy construction: the discriminative po",0
"axonomy by applying algorithms like maximum spanning tree. The lexical patterns are either manually designed <ref type=""bibr"" target=""#b15"">[14,</ref><ref type=""bibr"" target=""#b22"">21,</ref><ref type=""bibr"" target=""#b24"">23,</ref><ref type=""bibr"" target=""#b26"">25]</ref> or derived from the corpus us",0
"26"">25]</ref> or derived from the corpus using some supervision or seeds <ref type=""bibr"" target=""#b2"">[1,</ref><ref type=""bibr"" target=""#b7"">6,</ref><ref type=""bibr"" target=""#b16"">15,</ref><ref type=""bibr"" target=""#b21"">20,</ref><ref type=""bibr"" target=""#b29"">28,</ref><ref type=""bibr"" target=""#b48""",0
"the current taxonomy node.</p><p>To this end, we follow previous work <ref type=""bibr"" target=""#b45"">[44]</ref> and adopt the idea of local embedding <ref type=""bibr"" target=""#b14"">[13]</ref> to learn term embedding from text data. The basic idea of local embedding is to fine-tune term embedding at s function slightly differs from the ones in the previous work <ref type=""bibr"" target=""#b45"">[44]</ref> as well as the original local embedding work <ref type=""bibr"" target=""#b14"">[13]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.3"">Motif Instances as Term Contexts</head><p>",0
"pe=""bibr"" target=""#b39"">[38,</ref><ref type=""bibr"" target=""#b45"">44]</ref> and recent work on topic modeling <ref type=""bibr"" target=""#b40"">[39,</ref><ref type=""bibr"" target=""#b41"">40]</ref>, we design a set of tasks for human evaluation.</p><p>For each dataset, we recruited 10 in-domain human exper the terms should be able to form a semantically coherent topic. Similar to previous topic model evaluations <ref type=""bibr"" target=""#b40"">[39,</ref><ref type=""bibr"" target=""#b41"">40]</ref>, we present the top-5 terms to human annotators from the same taxonomy node. Annotators are asked to first ju",0
"rds automatic topic taxonomy construction from text corpora. In pioneer studies, hierarchical topic modeling <ref type=""bibr"" target=""#b11"">[10,</ref><ref type=""bibr"" target=""#b13"">12,</ref><ref type=""bibr"" target=""#b20"">19,</ref><ref type=""bibr"" target=""#b38"">37,</ref><ref type=""bibr"" target=""#b39"" g text data. As demonstrated in its paper, it beats many strong baselines, such as hierarchical topic models <ref type=""bibr"" target=""#b11"">[10,</ref><ref type=""bibr"" target=""#b13"">12,</ref><ref type=""bibr"" target=""#b20"">19,</ref><ref type=""bibr"" target=""#b38"">37,</ref><ref type=""bibr"" target=""#b39"" different levels as the same as the numbers of clusters in NetTaxo. We have tested our enhanced Hierarchical Latent Dirichlet Allocation (HLDA) model <ref type=""bibr"" target=""#b13"">[12]</ref> and its performance is quite similar to HPAM++. Therefore, we only present the results of HPAM++ here. • Tax",0
"ucture. Specifically, we first learn term embedding vectors from text using word2vec <ref type=""bibr"" target=""#b18"">[17]</ref> and network using LINE <ref type=""bibr"" target=""#b35"">[34]</ref> separately, where every embedding vector has a dimension of 300. And then, we concatenate the two vectors fo",0
"target=""#b25"">[24,</ref><ref type=""bibr"" target=""#b42"">41,</ref><ref type=""bibr"" target=""#b46"">45]</ref> compared to conventional path-based methods <ref type=""bibr"" target=""#b10"">[9,</ref><ref type=""bibr"" target=""#b28"">27]</ref>. In this work, the quality of term embedding is the key to the overal",0
".org/ns/1.0""><head n=""2.2"">NIC Offload of TCP Features</head><p>There have been a large number of works and debates on NIC offloading of TCP features <ref type=""bibr"" target=""#b33"">[35,</ref><ref type=""bibr"" target=""#b45"">47,</ref><ref type=""bibr"" target=""#b48"">50,</ref><ref type=""bibr"" target=""#b55",1
"(e.g., timer operations), it incurs a high context switching overhead. To address the problem, we modify mTCP to use cooperative userlevel threading <ref type=""bibr"" target=""#b11"">[13]</ref>. We find that this not only reduces the context switching overhead, but it also allows other optimizations l",0
"utation cost is as large as 60% of the entire CPU cycles (Section §2). An alternative might be to adopt RDMA <ref type=""bibr"" target=""#b35"">[37,</ref><ref type=""bibr"" target=""#b41"">43]</ref> or a custom RPC protocol <ref type=""bibr"" target=""#b42"">[44]</ref>, but the former requires an extra in-netwo",0
"-L4 NFs that should run prior to TCP stack (e.g., firewalling or host networking) must be offloaded to NIC accordingly. Such NFs can be written in P4 <ref type=""bibr"" target=""#b38"">[40,</ref><ref type=""bibr"" target=""#b43"">45,</ref><ref type=""bibr"" target=""#b54"">56]</ref> and easily integrated with A",0
"system and exhibits all the pixels of the detected image as points in it. Pixels representing the same object naturally cluster in the spectral space <ref type=""bibr"" target=""#b3"">[4]</ref> . This property provides us an opportunity to segment pixels belonging to different objects according to their",1
"se and outliers. Prior probability defined on the label field is one of the most effective way to construct the connection between neighboring pixels <ref type=""bibr"" target=""#b4"">[5]</ref> . It decreases with the number of pixels having the same label with the central one in the neighborhood system",1
".1."">Spectral space-based algorithms</head><p>Mean shift is a typical clustering algorithm which shifts the mean of the cluster to its center of mass <ref type=""bibr"" target=""#b5"">[6]</ref> . Recently, Yamasaki and Tanaka have studied the properties of it via regarding mean shift-based algorithms as",0
"Weighted kernel based FCM combines the advantages of kernel function and the weighted FCM, thus it is able to obtain much better segmentation results <ref type=""bibr"" target=""#b11"">[12]</ref> .</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2.2."">Neighborhood system-based algorithms</he",0
"Introduction</head><p>Image segmentation, which decomposes an image into homogeneous regions, is an important task in remote sensing image processing <ref type=""bibr"" target=""#b0"">[1]</ref> . The accuracy of image segmentation has an essential influence on the subsequent image analysis and interpret",0
". performed FCM on the smoothed images and proposed a noisy image segmentation algorithm by integrating guided filter into fuzzy clustering algorithm <ref type=""bibr"" target=""#b16"">[17]</ref> .</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2.3."">Riemannian manifold space</head><p>Image ernel function based on multi-feature spaces (MPGK_mFS) is defined as</p><formula xml:id=""formula_22"">u pq = s pq exp − γ D (p : q )</formula><p>r pq <ref type=""bibr"" target=""#b16"">(17)</ref> In this paper, we propose four geodesic-kernel function-based manifold projection criteria to measure the di",0
"solved by maximum a posteriori. Chatzis and Varvarigou used the prior distribution as a cluster-size controller in the introduced KL-based constraint <ref type=""bibr"" target=""#b15"">[16]</ref> . Due to the smoothing effect of the prior probability defined on the label field, the proposed algorithm ou M) employed the posterior probability as the dissimilarity and used the prior probability to control the scale of clusters in the KL-based constraint <ref type=""bibr"" target=""#b15"">[16]</ref> . Kernel function-based FCM defined the dissimilarity between pixels by RBF kernel function <ref type=""bibr""",0
"suppress the effect of noise, Mirghasemi et al. used an adaptive wavelet shrinkage to restrain noise and outliers and then segment the image with FCM <ref type=""bibr"" target=""#b9"">[10]</ref> . The reason caused poor performance on noise and outliers of the traditional spectral space-based algorithms",0
"entation algorithms to noise and outliers. Markov Random Field (MRF) model is an efficient way to construct the connection between neighboring pixels <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b13"">14]</ref> . It defines the prior probability according to the labels of pixels",0
"n the centers of the clusters are appropriate <ref type=""bibr"" target=""#b7"">[8]</ref> . It is widely used in image segmentation due to its simplicity <ref type=""bibr"" target=""#b8"">[9]</ref> . However, pixels far away from cluster centers are easily to be wrongly assigned. To suppress the effect of n",0
"hat can use a large amount of unlabeled data to train model, which can help to improve the accuracy of the classifier when there are few labeled data <ref type=""bibr"" target=""#b24"">[25]</ref>. Co-training needs to analyze data from two different ""perspectives"". It requires that the data set has two",1
"order to enrich user experience, more and more sensors (such as acceleration sensors, acoustic sensors, etc.) have been integrated into mobile phones <ref type=""bibr"" target=""#b4"">[5]</ref>, <ref type=""bibr"" target=""#b5"">[6]</ref>. In daily life, mobile phones can continuously record a lot of sensin",0
"e=""bibr"" target=""#b10"">[11]</ref> use a deep sparse neural network to assess people's psychological pressure based on people's Weibo data; Lin et al. <ref type=""bibr"" target=""#b11"">[12]</ref> use a convolutional neural network to detect people's pressure. In terms of adolescents, Xue et al. <ref typ",0
"head><p>In a fast-paced life, the health status of massive crowds is frequently invaded by pressure. The American College Health Association's report <ref type=""bibr"" target=""#b0"">[1]</ref> in 2015 showed that 57.7% of students felt very anxious at least once in the past 12 months. At the same time,",0
"the most widely used one. People's psychological pressure can also be monitored by professional instruments <ref type=""bibr"" target=""#b3"">[4]</ref>, <ref type=""bibr"" target=""#b7"">[8]</ref>- <ref type=""bibr"" target=""#b9"">[10]</ref>. For instance, the electric resistance of human skin relates to cert d to changes in certain physiological indicators, many studies are devoted to using wearable devices to monitor people's daily psychological pressure <ref type=""bibr"" target=""#b7"">[8]</ref>- <ref type=""bibr"" target=""#b9"">[10]</ref>. Typically, these devices integrate specialized sensors that can sen",0
"2"">23]</ref>), in this direction Adversarial Training (AT) procedure <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b34"">35,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b39"">40]</ref> shows promising results.</p><p>In adversarial training regime, models rsarial attacks, various schemes such as adversarial training (e.g., <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b39"">40,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b3"">4] ""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b37"">38,</ref><ref type=""bibr"" target=""#b31"">32,</ref><ref type=""bibr"" target=""#b30"">31,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b8"">9]</ref> accepted to ICLR 2018. In this s linear approximation of loss function to become unreliable for generating adversarial samples during single-step adversarial training. Madry et al. <ref type=""bibr"" target=""#b21"">[22]</ref> demonstrated that models trained using adversarial samples that maximize the training loss are robust agains ><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b8"">9]</ref> accepted to ICLR 2018. In this direction, adversarial training method <ref type=""bibr"" target=""#b21"">[22]</ref>, shows promising results for learning robust deep learning models. Kurakin et al. <ref type=""bibr"" target=""# e Adversarial Training (EAT) method. However, models trained using EAT are still susceptible to multi-step attacks in white-box setting. Madry et al. <ref type=""bibr"" target=""#b21"">[22]</ref> demonstrated that adversarially trained model can be made robust against white-box attacks, if perturbation adversarial training method. Most importantly, we show that over-fitting effect is the reason for failure to satisfy the criteria.</p><p>Madry et al. <ref type=""bibr"" target=""#b21"">[22]</ref> demonstrated that it is possible to learn robust models using adversarial training method, if adversarial pe loss should be generated. Further, the model's parameters (θ) should be updated so as to decrease the loss on such adversarial samples. Madry et al. <ref type=""bibr"" target=""#b21"">[22]</ref> solves the maximization step by generating adversarial samples using an iterative method named Projected Gra for models trained using single-step adversarial training method <ref type=""bibr"" target=""#b12"">[13]</ref> and multi-step adversarial training method <ref type=""bibr"" target=""#b21"">[22]</ref>. Column-1 of Fig. <ref type=""figure"" target=""#fig_0"">1</ref> and Fig. <ref type=""figure"" target=""#fig_1"">2</ .   <ref type=""table"">8</ref> shows the setup used for EAT method. PGD Adversarial Training (PAT): Multi-step adversarial training method proposed by <ref type=""bibr"" target=""#b21"">[22]</ref>. At each iteration all the clean samples in the mini-batch are replaced with their corresponding adversarial t of pre-trained models. Table8shows the setup used for EAT method. PGD Adversarial Training (PAT): Multi-step adversarial training method proposed by<ref type=""bibr"" target=""#b21"">[22]</ref>. At each iteration all the clean samples in the mini-batch are replaced with their corresponding adversarial adversarial perturbation of small step size (α) is added to the image. In our experiments, we set α = /steps.</p><p>Projected Gradient Descent (PGD) <ref type=""bibr"" target=""#b21"">[22]</ref>: Initially, a small random noise sampled from Uniform distribution (U ) is added to the image. Then at each",1
"al attacks). We show results on MNIST <ref type=""bibr"" target=""#b18"">[19]</ref>, Fashion-MNIST <ref type=""bibr"" target=""#b36"">[37]</ref> and CIFAR-10 <ref type=""bibr"" target=""#b15"">[16]</ref> datasets. We use LeNet+ (please refer to supplementary document for details on network architecture) for bot",0
"e to adversarial samples: samples with imperceptible, engineered noise designed to manipulate model's output <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b33"">34,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b12"">13,",0
"</ref>, various attacks (e.g., <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b11"">",0
"ng trained. Adversarial sample generation methods range from simple methods <ref type=""bibr"" target=""#b12"">[13]</ref> to complex optimization methods <ref type=""bibr"" target=""#b23"">[24]</ref>. In order to reduce computational complexity, non-iterative methods such as Fast Gradient Sign Method (FGSM) f source models.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""5.3."">Performance against DeepFool and C&amp;W attacks</head><p>DeepFool <ref type=""bibr"" target=""#b23"">[24]</ref> and C&amp;W <ref type=""bibr"" target=""#b7"">[8]</ref> attacks generate adversarial perturbations with minimum p>Following the findings of Szegedy et al. <ref type=""bibr"" target=""#b33"">[34]</ref>, various attacks (e.g., <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b24"">2",0
".0""><p>We design and implement a ready-to-use library in PyTorch for performing micro-batch pipeline parallelism with checkpointing proposed by GPipe <ref type=""bibr"" target=""#b10"">[11]</ref>. In particular, we develop a set of design components to enable pipeline-parallel gradient computation in Py reased capacity of DNN effectively has improved the performance. For example, AmoebaNet-B <ref type=""bibr"" target=""#b22"">[23]</ref> scaled with GPipe <ref type=""bibr"" target=""#b10"">[11]</ref> has 557 million parameters and has achieved top-1 accuracy 84.4% which was state-of-the-arts result at the t Transformer-based <ref type=""bibr"" target=""#b27"">[28]</ref> language model which has 1.5 billion parameters (see Figure <ref type=""figure"">1</ref> of <ref type=""bibr"" target=""#b10"">[11]</ref> for the effect of model scaling). However, training such a massive model is very resource intensive. One can ne parallelism a way to accelerate neural network training by combining model parallelism with data pipelining, either in synchronous way as in GPipe <ref type=""bibr"" target=""#b10"">[11]</ref> or in asynchronous way as in <ref type=""bibr"" target=""#b11"">[12]</ref>, PipeDream <ref type=""bibr"" target=""# t be completed before executing F i+1,j and B i,j must be completed before executing B i−1,j .</p><p>In addition to the micro-batch pipelining, GPipe <ref type=""bibr"" target=""#b10"">[11]</ref> further reduces the memory requirement by utilizing gradient checkpointing for each B i,j . Since jth device .2."">Performance Benchmarks</head><p>To demonstrate the efficiency of torchgpipe, we report performance benchmarks similar to that conducted by GPipe <ref type=""bibr"" target=""#b10"">[11]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.2.1"">AmoebaNet-D Speed Benchmark</head><p>We = 1, we used checkpointing to all micro-batches<ref type=""foot"" target=""#foot_2"">5</ref> to make a fair comparison of loss due to checkpointing with <ref type=""bibr"" target=""#b10"">[11]</ref>. The model we used is our implementation of a sequential version of AmoebaNet-D in PyTorch <ref type=""foot"" e-1, - Table <ref type=""table"">2</ref>: Speed benchmark on AmoebaNet-D <ref type=""bibr"" target=""#b17"">(18,</ref><ref type=""bibr"">256)</ref>.</p><p>In <ref type=""bibr"" target=""#b10"">[11]</ref>, Cloud TPUv3s were used while we used NVIDIA Tesla P40 GPUs in our experiments. with the corresponding numbe p>In this paper, we introduced torchgpipe, a ready-touse library in PyTorch for micro-batch pipeline parallelism with checkpointing proposed by GPipe <ref type=""bibr"" target=""#b10"">[11]</ref>. This library is designed and implemented in PyTorch's define-by-run and eager execution environment. Ablati e=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b6"">7]</ref>, and recent lines of research questions how to find an optimal strategy",1
"to find an optimal strategy <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b28"">29]</ref>. Among them, pipeline parallelism a way to accelerate neural network training by combining model parallelism",0
"j = 1, • • • , n</formula><p>, assuming that f does not involve any intra-batch computation. One prominent exception for this is batch normalization <ref type=""bibr"" target=""#b12"">[13]</ref> 1 . The loss is obtained by aggregating x n i = f (x i ) and evaluating the loss function on them.</p><p>In",0
"can mitigate this issue by reducing the size of the model without losing the performance by pruning the model <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b0"">1]</ref>, designing more efficient architectures <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b26"">",0
"eter tuning, this effectively reduce the training time up to a certain size of mini-batch which may depend on model, optimization algorithm, and data <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b24"">25]</ref>. One drawback of data-parallel training is that devices hold their own",0
"calable graph neural network architecture generalizing GCN, S-GCN, ChebNet and related methods. Our architecture is analogous to the inception module <ref type=""bibr"" target=""#b43"">Szegedy et al. (2015)</ref>; <ref type=""bibr"" target=""#b22"">Kazi et al. (2019)</ref> and combines graph convolutional f on the task at hand. Note that the model in equation ( <ref type=""formula"" target=""#formula_6"">4</ref>) is analogous to the popular Inception module <ref type=""bibr"" target=""#b43"">Szegedy et al. (2015)</ref> for classic CNN architectures (Figure <ref type=""figure"">1</ref>): it consists of convoluti",1
"with larger receptive fields on the graph nodes,</p><formula xml:id=""formula_4"">Y = softmax(A • • • σ(AXΘ (1) ) • • • Θ (L) ).</formula><p>Wu et al. <ref type=""bibr"" target=""#b46"">Wu et al. (2019)</ref> argued that graph convolutions with large filters is practically equivalent to multiple convolut simplicity, the S-GCN (3) model appears to be extremely efficient and to attain similar results to models with multiple stacked convolutional layers <ref type=""bibr"" target=""#b46"">Wu et al. (2019)</ref>; (ii) GCN aggregation schemes (2) have been essentially shown to learn low-pass filters NT and M",0
"al. (2016)</ref>, particle physics <ref type=""bibr"" target=""#b10"">Choma et al. (2018</ref><ref type=""bibr"">), chemistry Duvenaud et al. (2015)</ref>; <ref type=""bibr"" target=""#b15"">Gilmer et al. (2017)</ref>, medicine <ref type=""bibr"" target=""#b33"">Parisot et al. (2018)</ref>, drug repositioning <re",0
"efer the reader to recent review papers <ref type=""bibr"">Bronstein et al. (2017)</ref>; <ref type=""bibr"" target=""#b17"">Hamilton et al. (2017b)</ref>; <ref type=""bibr"" target=""#b1"">Battaglia et al. (2018)</ref>; <ref type=""bibr"">Zhang et al. (2018)</ref> for a comprehensive overview of deep learning",0
"earning models have been extremely successful in modeling relational data in a variety of different domains, including social network link prediction <ref type=""bibr"" target=""#b52"">Zhang and Chen (2018)</ref>, human-object interaction <ref type=""bibr"" target=""#b36"">Qi et al. (2018)</ref>, computer g",0
"""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b45"">46,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b42"">43]</ref> and have connections to the large literature on metric learning <ref ty ""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b45"">46,</ref><ref type=""bibr"" target=""#b40"">41,</ref><ref type=""bibr"" target=""#b5"">6]</ref>. In these works, the losses are inspired by noise contrastive estimation <ref type=""bibr"" target=""#b16"">[17,</r eve state of the art results <ref type=""bibr"" target=""#b35"">[36,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b45"">46,</ref><ref type=""bibr"" target=""#b5"">6]</ref>. Then we show how we can modify this loss to be suitable for fully supervised learning, while simultaneously pr on Learning Framework</head><p>Our representation learning framework is structurally similar to that used in <ref type=""bibr"" target=""#b45"">[46,</ref><ref type=""bibr"" target=""#b5"">6]</ref> for self-supervised contrastive learning and consists of the following components (see Fig. <ref type=""figure""> linear layer (for more details see Sec. 4). Similar to the results for self-supervised contrastive learning <ref type=""bibr"" target=""#b45"">[46,</ref><ref type=""bibr"" target=""#b5"">6]</ref>, we found representations from the encoder to give improved performance on downstream tasks than those from the creasing number of negatives <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b45"">46,</ref><ref type=""bibr"" target=""#b5"">6]</ref>. The supervised contrastive loss in Eq. 4 preserves this structure: adding larger numbers of negatives to the d <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b45"">46]</ref> or using data augmentation <ref type=""bibr"" target=""#b5"">[6]</ref>. The major difference is that many negative pairs are used for each data-point. These are usually chosen unifo tage of augmentation is applying a random crop to the image and then resizing that back to the image's native resolution. In light of the findings of <ref type=""bibr"" target=""#b5"">[6]</ref> that self-supervised contrastive loss requires significantly different data augmentation than cross-entropy lo AutoAugment: <ref type=""bibr"" target=""#b8"">[9]</ref> -RandAugment: <ref type=""bibr"" target=""#b9"">[10]</ref> -SimAugment: A variant of the strategy of <ref type=""bibr"" target=""#b5"">[6]</ref> to sequentially apply random color distortion and Gaussian blurring, where we probabilistically add an additio z p 2 − z a − z n 2 + 2τ</formula><p>which has the same form as a triplet loss with margin α = 2τ . This result is consistent with empirical results <ref type=""bibr"" target=""#b5"">[6]</ref> which show that contrastive loss performs better in general than triplet loss on representation tasks. Additio Eq. 4 varies from 1 to 5. Adding more positives benefits the final Top-1 accuracy. We compare against previous state of the art self-supervised work <ref type=""bibr"" target=""#b5"">[6]</ref> which has used one positive which is another data augmentation of the same sample; see text for details.</p></ ref type=""bibr"" target=""#b53"">[54]</ref> gives us the best results to train the embedding network, confirming what has been reported by previous work <ref type=""bibr"" target=""#b5"">[6]</ref>. With LARS we use a cosine learning rate decay. On the other hand we find that the RMSProp optimizer <ref type aster to implement than other augmentation schemes such as RandAugment <ref type=""bibr"" target=""#b9"">[10]</ref> or the data augmentations proposed in <ref type=""bibr"" target=""#b5"">[6]</ref>, which we denote SimAugment. As we show in Table <ref type=""table"">7</ref>, using the same data augmentation ( image, and let j(i) be the index of the other augmented image originating from the same source image. In self-supervised contrastive learning (e.g., <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b45"">46,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b24""> ype=""annex""> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>Supplementary 6. Effect of Temperature in Loss Function</head><p>Similar to previous work <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b45"">46]</ref>, we find that the temperature used in the loss function (for the softm",1
"e=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b45"">46,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b42"">43]</ref> and have connections to the large literature on metric learning <ref type=""bibr"" target=""#b47"">[48,</ref><ref f-supervised contrastive loss (Eq. 4) is largely motivated by noise contrastive estimation and N-pair losses <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b42"">43]</ref>, wherein the ability to discriminate between signal and noise (negatives) is improved by adding more examples osses are inspired by noise contrastive estimation <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b31"">32]</ref> or N-pair losses <ref type=""bibr"" target=""#b42"">[43]</ref>. Typically, the loss is applied at the last layer of a deep network. At test time, the embeddings from a pre",1
",</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b42"">43]</ref> and have connections to the large literature on metric learning <ref type=""bibr"" target=""#b47"">[48,</ref><ref type=""bibr"" target=""#b4"">5]</ref>.</p><p>As the name suggests, contrastive losses consist of two ""opposi e show analytically that the gradient of our loss function encourages learning from hard positives and hard negatives. We also show that triplet loss <ref type=""bibr"" target=""#b47"">[48]</ref> is a special case of our loss when only a single positive and negative are used.</p></div> <div xmlns=""http: e anchor only weakly benefits the encoder). The loss can thus be seen to be efficient in its training. Other contrastive losses, such as triplet loss <ref type=""bibr"" target=""#b47"">[48]</ref>, often use the computationally expensive technique of hard negative mining to increase training efficacy <re v xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.3."">Connections to Triplet Loss</head><p>Contrastive learning is closely related to the triplet loss <ref type=""bibr"" target=""#b47"">[48]</ref>, which is one of the widely-used alternatives to cross-entropy for supervised representation learning. As di r direct retrieval tasks.</p><p>Closely related to contrastive learning are metric learning and triplet losses <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b47"">48,</ref><ref type=""bibr"" target=""#b39"">40]</ref>. These losses have been used to learn powerful representations, often",1
"ous work <ref type=""bibr"" target=""#b5"">[6]</ref>. With LARS we use a cosine learning rate decay. On the other hand we find that the RMSProp optimizer <ref type=""bibr"" target=""#b46"">[47]</ref> works best for training the linear classifier. For RMSProp we use an exponential decay for the learning rate",0
"tworks often lack robustness to out of distribution data or natural corruptions. This has been shown not only with adversarially constructed examples <ref type=""bibr"" target=""#b15"">[16]</ref>, but also with naturally occurring variations such as noise, blur and JPEG compression <ref type=""bibr"" targ",0
"s well known to be sensitive to hyper-parameters and a large body of literature is devoted to finding efficient ways to perform hyperparameter tuning <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b2"">3]</ref>. We find that the contrastive supe",0
"tions such as AutoAugment <ref type=""bibr"" target=""#b8"">[9]</ref> and RandAugment <ref type=""bibr"" target=""#b9"">[10]</ref>; we also compare to CutMix <ref type=""bibr"" target=""#b54"">[55]</ref>). We show results on ResNet-50, ResNet-101 and ResNet-200, and compare against the same ResNet architectures rget=""#b44"">[45,</ref><ref type=""bibr"" target=""#b32"">33]</ref>, data augmentations such as Mixup <ref type=""bibr"" target=""#b55"">[56]</ref> and CutMix <ref type=""bibr"" target=""#b54"">[55]</ref>, and knowledge distillation <ref type=""bibr"" target=""#b23"">[24]</ref>.</p><p>Recent years have seen signific ther top-performing methods are shown in Table <ref type=""table"" target=""#tab_0"">1</ref>). Note that we also achieve a slight improvement over CutMix <ref type=""bibr"" target=""#b54"">[55]</ref>, which is considered to be a state of the art data augmentation strategy. Incorporating data augmentation st ""#b54"">[55]</ref>, which is considered to be a state of the art data augmentation strategy. Incorporating data augmentation strategies such as CutMix <ref type=""bibr"" target=""#b54"">[55]</ref> and MixUp <ref type=""bibr"" target=""#b55"">[56]</ref> into supervised contrastive learning could potentially i oth pre-training and training the linear classifier is optimal. We leave experimenting with MixUp <ref type=""bibr"" target=""#b55"">[56]</ref> or CutMix <ref type=""bibr"" target=""#b54"">[55]</ref>   Further we experiment with varying levels of augmentation magnitude for RandAugment since that has shown t",0
"ve achieved excellent performance in self-supervised learning in recent years in the image and video domains <ref type=""bibr"" target=""#b49"">[50,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b45"" on learning are collected under the umbrella of contrastive learning <ref type=""bibr"" target=""#b49"">[50,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b45"">46,</ref><ref type=""bibr"" target=""#b40"">41,</ref><ref type=""bibr"" target=""#b5""> Self-supervised contrastive losses similarly use just one positive pair, selected using either co-occurence <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b45"">46]</ref> or using data augmentation <ref type=""bibr"" target=""#b5"">[6]</ref>. T ed contrastive learning (e.g., <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b45"">46,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b24"">25]</ref>), the loss takes the following form.</p><formula xml:id=""formula_0"">L self = 2N i=1 L self i (1)</formula><fo",0
"th other augmentations in the supplementary. We experimented with standard optimizers such as LARS <ref type=""bibr"" target=""#b53"">[54]</ref>, RMSProp <ref type=""bibr"" target=""#b22"">[23]</ref> and SGD with momentum <ref type=""bibr"" target=""#b36"">[37]</ref> in different permutations for the initial pr",0
"e (see Fig. <ref type=""figure"">1</ref>). The gain in top-1 accuracy is also accompanied by increased robustness as measured on the ImageNet-C dataset <ref type=""bibr"" target=""#b21"">[22]</ref>. Our main contributions are summarized below: Figure <ref type=""figure"">3</ref>: Cross entropy, self-supervi ents</head><p>We evaluate our supervised contrastive loss by measuring classification accuracy on ImageNet and robustness to common image corruptions <ref type=""bibr"" target=""#b21"">[22]</ref>. After training the embedding network with supervised contrastive loss on ImageNet <ref type=""bibr"" target="" (fully connected) layer. This linear layer is trained with standard cross entropy while the parameters of the embedding network are kept unchanged.  <ref type=""bibr"" target=""#b21"">[22]</ref> (lower is better).</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.1."">ImageNet Classification constructed examples <ref type=""bibr"" target=""#b15"">[16]</ref>, but also with naturally occurring variations such as noise, blur and JPEG compression <ref type=""bibr"" target=""#b21"">[22]</ref>. To this end, <ref type=""bibr"" target=""#b21"">[22]</ref> made a benchmark dataset, ImageNet-C, which applies </ref>, but also with naturally occurring variations such as noise, blur and JPEG compression <ref type=""bibr"" target=""#b21"">[22]</ref>. To this end, <ref type=""bibr"" target=""#b21"">[22]</ref> made a benchmark dataset, ImageNet-C, which applies common naturally occuring perturbations such as noise, b ompare the supervised contrastive models to cross entropy using the mean Corruption Error (mCE) and relative mean Corruption Error (rel. mCE) metrics <ref type=""bibr"" target=""#b21"">[22]</ref>.</p><p>We see that the supervised contrastive models have lower mCE values across different corruptions, thu ion of top-1 accuracy as the corruption severity increases. This shows that the model learns better representations which are robust to corruptions.  <ref type=""bibr"" target=""#b21"">[22]</ref> for a given level of severity (lower is better); Bottom: Average Top-1 Accuracy over all the corruptions for http://www.tei-c.org/ns/1.0""><head n=""7."">Robustness</head><p>Along with measuring the mean Corruption Error (mCE) and mean relative Corruption Error <ref type=""bibr"" target=""#b21"">[22]</ref> on the ImageNet-C dataset (see paper, Section 4.2 and Table <ref type=""table"">2</ref>), we also measure the",0
"ef type=""bibr"" target=""#b40"">41,</ref><ref type=""bibr"" target=""#b5"">6]</ref>. In these works, the losses are inspired by noise contrastive estimation <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b31"">32]</ref> or N-pair losses <ref type=""bibr"" target=""#b42"">[43]</ref>. Typicall re negatives. The general form of the self-supervised contrastive loss (Eq. 4) is largely motivated by noise contrastive estimation and N-pair losses <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b42"">43]</ref>, wherein the ability to discriminate between signal and noise (negat",0
"ownstream transfer tasks, fine tuning or direct retrieval tasks.</p><p>Closely related to contrastive learning are metric learning and triplet losses <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b47"">48,</ref><ref type=""bibr"" target=""#b39"">40]</ref>. These losses have been used t",0
"se approximations. Furthermore, our specific loss formulation makes our learning gradient efficient (see Section 3.2.3 for more details). The work in <ref type=""bibr"" target=""#b14"">[15]</ref> also uses a similar loss formulation to ours; however it is used to entangle classes at intermediate layers",0
"the continued use of cross-entropy to achieve state of the art results <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b50"">51,</ref><ref type=""bibr"" target=""#b25"">26]</ref>.</p><p>Many proposed improvements to regular crossentropy in fact inv",0
"ref><ref type=""bibr"" target=""#b42"">43]</ref> and have connections to the large literature on metric learning <ref type=""bibr"" target=""#b47"">[48,</ref><ref type=""bibr"" target=""#b4"">5]</ref>.</p><p>As the name suggests, contrastive losses consist of two ""opposing forces"": for a given anchor point, the",0
"e"">1</ref>(b). LR-CNN uses CNN to encode potential words at different window sizes. However, RNN and CNN are hard to model long-distance dependencies <ref type=""bibr"" target=""#b19"">(Vaswani et al., 2017)</ref>, which may be useful in NER, such as coreference <ref type=""bibr"" target=""#b16"">(Stanislaw ential inductive bias, which makes the model complicated.</p><p>In this paper, we propose FLAT: Flat LAttice Transformer for Chinese NER. Transformer <ref type=""bibr"" target=""#b19"">(Vaswani et al., 2017)</ref> adopts fully-connected selfattention to model the long-distance dependencies in a sequence ? p d (ht) ij ? p d (tt) ij )), (8)</formula><p>where W r is a learnable parameter, ? denotes the concatenation operator, and p d is calculated as in <ref type=""bibr"" target=""#b19"">Vaswani et al. (2017)</ref>,</p><formula xml:id=""formula_11"">p (2k) d = sin d/10000 2k/d model ,<label>(9)</label></for",1
"e dimension of each head.</formula><p>The vanilla Transformer also uses absolute position encoding to capture the sequential information. Inspired by <ref type=""bibr"" target=""#b22"">Yan et al. (2019)</ref>, we think commutativity of the vector inner dot will cause the loss of directionality in self-a pe=""table"" target=""#tab_0"">1</ref>. We use the same train, dev, test split as <ref type=""bibr"">Gui et al. (2019b)</ref>. We take BiLSTM-CRF and TENER <ref type=""bibr"" target=""#b22"">(Yan et al., 2019)</ref> as baseline models. TENER is a Transformer using relative position encoding for NER, without e rget=""#tab_1"">2</ref>, our model outperforms baseline models and other lexicon-based models on four Chinese NER datasets. Our model outperforms TENER <ref type=""bibr"" target=""#b22"">(Yan et al., 2019</ref>) by 1.72 in average F1 score. For lattice LSTM, our model has an average F1 improvement of 1.51",1
"019)</ref>. 'YJ' denotes the lexicon released by <ref type=""bibr"" target=""#b25"">Zhang and Yang (2018)</ref>, and 'LS' denotes the lexicon released by <ref type=""bibr"" target=""#b11"">Li et al. (2018)</ref>. The result of other models are from their original paper.</p><formula xml:id=""formula_15"">A * i exicon-based methods. The embeddings and lexicons are the same as <ref type=""bibr"" target=""#b25"">Zhang and Yang (2018)</ref>. When comparing with CGN <ref type=""bibr"" target=""#b11"">(Li et al., 2018)</ref>, we use the same lexicon as CGN. The way to select hyper-parameters can be found in the supplem t al., 2019</ref>) by 1.72 in average F1 score. For lattice LSTM, our model has an average F1 improvement of 1.51 over it. When using another lexicon <ref type=""bibr"" target=""#b11"">(Li et al., 2018)</ref>, our model also outperforms CGN by 0.73 in average F1 score. Maybe due to the characteristic of",0
"ref type=""bibr"" target=""#b2"">Cui et al. (2019)</ref>. We use it by the BERTEmbedding in fastNLP 1 . makes FLAT more powerful in entity classification <ref type=""bibr"" target=""#b0"">(Agarwal et al., 2020)</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.6"">Compatibility with BERT</",0
"hard to model long-distance dependencies <ref type=""bibr"" target=""#b19"">(Vaswani et al., 2017)</ref>, which may be useful in NER, such as coreference <ref type=""bibr"" target=""#b16"">(Stanislawek et al., 2019)</ref>. Due to the dynamic lattice structure, these methods cannot fully utilize the parallel",0
"ake the absolute position of nodes' first characters and the relation between each pair of nodes as the structure information. In speech translation, <ref type=""bibr"" target=""#b15"">Sperber et al. (2019)</ref> used the longest distance to the start node to indicate lattice structure, and <ref type=""b",0
"018)</ref>. Compared with English NER <ref type=""bibr"" target=""#b9"">(Lample et al., 2016;</ref><ref type=""bibr"" target=""#b23"">Yang et al., 2017;</ref><ref type=""bibr"" target=""#b12"">Liu et al., 2017;</ref><ref type=""bibr"" target=""#b18"">Sun et al., 2020)</ref>, Chinese NER is more difficult since it u",0
"ith vanilla Transformer.</p><p>After FLAT, we only take the character representation into output layer, followed by a Condiftional Random Field (CRF) <ref type=""bibr"" target=""#b8"">(Lafferty et al., 2001)</ref>   <ref type=""bibr"" target=""#b7"">Sun, 2016)</ref>. We show statistics of these datasets in",0
"lice are scheduled in order but independently from the rest of the application using multiple parallel IQs <ref type=""bibr"" target=""#b14"">[15]</ref>, <ref type=""bibr"" target=""#b15"">[16]</ref>. This is effectively a limited form of OoO scheduling. However, the slice-based approaches could experience slowdown because the various shapes and sizes of dependence chains may impede the exploitation of ILP and MLP in such parallel InO scheduling windows <ref type=""bibr"" target=""#b15"">[16]</ref>, <ref type=""bibr"" target=""#b16"">[17]</ref>.</p><p>To sum up, discovered architectures so far suffer from eit get addresses and then issue the following loads, thereby eliminating the need for associative LQ searches <ref type=""bibr"" target=""#b14"">[15]</ref>, <ref type=""bibr"" target=""#b15"">[16]</ref>. However, this approach could significantly degrade performance if an address-generating instruction (AGI) b ure <ref type=""figure"" target=""#fig_4"">6</ref> shows the performance of Load Slice Core (LSC) <ref type=""bibr"" target=""#b14"">[15]</ref>, Freeway core <ref type=""bibr"" target=""#b15"">[16]</ref>, CASINO core, and OoO core, normalized to that of an InO core. CASINO achieves significant performance gains ive slices have a dependence relationship, and a dependent slice blocks the issue of younger, independent slices. To address this limitation, Freeway <ref type=""bibr"" target=""#b15"">[16]</ref> introduces a dependence-aware slice scheduling policy. In Freeway, dependent slices, which have at least one has proposed slice-based MLP exploitation techniques built upon an energy-efficient stall-on-use InO core <ref type=""bibr"" target=""#b14"">[15]</ref>, <ref type=""bibr"" target=""#b15"">[16]</ref>. However, the various shapes and sizes of dependence chains could restrict their ability to exploit ILP <ref f>, <ref type=""bibr"" target=""#b15"">[16]</ref>. However, the various shapes and sizes of dependence chains could restrict their ability to exploit ILP <ref type=""bibr"" target=""#b15"">[16]</ref>, <ref type=""bibr"" target=""#b16"">[17]</ref>.</p><p>2) Energy-Efficient Dynamic Scheduling: To reduce the powe",1
"ght instructions are scheduled by the IQ which consists of the wakeup logic, select logic, and payload RAM <ref type=""bibr"" target=""#b21"">[21]</ref>, <ref type=""bibr"" target=""#b22"">[22]</ref>, <ref type=""bibr"" target=""#b23"">[23]</ref>, <ref type=""bibr"" target=""#b24"">[24]</ref>. The wakeup and select",0
"ying the stored schedules on an InO engine for future iterations <ref type=""bibr"" target=""#b9"">[10]</ref>, <ref type=""bibr"" target=""#b10"">[11]</ref>, <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" target=""#b12"">[13]</ref>. These schemes achieve significant performance improvements, but osed to achieve near-OoO performance with high energy efficiency <ref type=""bibr"" target=""#b9"">[10]</ref>, <ref type=""bibr"" target=""#b10"">[11]</ref>, <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" target=""#b12"">[13]</ref>. The key idea is to store an instruction scheduling order generat",0
"target=""#b40"">[40]</ref>. For each application, we pick up the most representative region of 300 million instructions using the Sim-Point methodology <ref type=""bibr"" target=""#b41"">[41]</ref>. A simulation is performed using the reference input set after a warm-up phase of 300 million instructions.",0
".org/ns/1.0""><head>V. EXPERIMENTAL METHODOLOGY</head><p>In the evaluation, we use an execution-driven, cycle-level x86 processor simulator, Multi2Sim <ref type=""bibr"" target=""#b38"">[38]</ref>, which is heavily modified to implement and evaluate the proposed design. The memory system is modeled by in",0
"idirectional LSTM) and Huang et al., 2015 (Bidirectional LSTM + CRF), respectively, using public word embedding, character features and word features <ref type=""bibr"" target=""#b8"">[6,</ref><ref type=""bibr"">4]</ref>. We explore existing word embeddings that is distinct from the previous studies inclu",1
"ns=""http://www.tei-c.org/ns/1.0""><head>II. RELATED RESEARCH</head><p>The existing algorithms for the NER task can be classified into three approaches <ref type=""bibr"" target=""#b11"">[9]</ref> : rule-based, machine-deep learning, and hybrid. The rule-based algorithm applies a set of rules in order to",0
"based, machine-deep learning, and hybrid. The rule-based algorithm applies a set of rules in order to extract patterns, i.e., rule base for Malay NER <ref type=""bibr"" target=""#b12"">[10]</ref>.</p><p>With the emergence of the machine and deep learning, various models were proposed for the task. The m",0
"isting word embeddings that is distinct from the previous studies including (1) Glove 300 embedding of 42B, 840B word vectors trained on Common Crawl <ref type=""bibr"" target=""#b9"">[7]</ref> and (2) Fasttext 300 dimension of two million word vectors trained on Common Craw and one million word trained was processed to generate three kinds of input representations. (1) Word embeddings, we conduct the experiment using the public embedding i.e. Glove <ref type=""bibr"" target=""#b9"">[7]</ref> and Fasttext <ref type=""bibr"" target=""#b10"">[8]</ref> with a different dimension, corpus, and vocabulary size",0
"n of two million word vectors trained on Common Craw and one million word trained on Wikipedia 2017, UMBC web base corpus and statmt.org news dataset <ref type=""bibr"" target=""#b10"">[8]</ref>. The embeddings are fed into Bidirectional LSTM for predicting the name entities. A best embedding is selecte sentations. (1) Word embeddings, we conduct the experiment using the public embedding i.e. Glove <ref type=""bibr"" target=""#b9"">[7]</ref> and Fasttext <ref type=""bibr"" target=""#b10"">[8]</ref> with a different dimension, corpus, and vocabulary size (Table <ref type=""table"">2</ref>). As mentioned previ",0
"en to the direct design of robust frameworks in an attack-agnostic manner, except a few touches on denoising <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b28"">29]</ref> and obfuscating gradients <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b24"">25]</ref> t relying on obfuscating gradients, our differentiable FPD can circumvent the structure-replaced white-box attack. Our proposal is partially related to <ref type=""bibr"" target=""#b28"">[29]</ref>, as the denoising layers in our FPD are inspired by their feature denoising approach. Nevertheless, differen pe=""bibr"" target=""#b28"">[29]</ref>, as the denoising layers in our FPD are inspired by their feature denoising approach. Nevertheless, different from <ref type=""bibr"" target=""#b28"">[29]</ref>, the principle behind our FPD is to improve the intrinsic robustness, regardless of conducting adversarial t e enhanced for maintaining high-level abstract semantic information. We will compare the performance between FPD-enhanced CNN and the CNN enhanced by <ref type=""bibr"" target=""#b28"">[29]</ref> in Section 4.1.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3."">Feature Pyramid Decoder</hea ing the relationship between pixels. Compared with the Gaussian filtering operator, the dot product operator helps improve the adversarial robustness <ref type=""bibr"" target=""#b28"">[29]</ref>. Meanwhile, as the dot product operator does not involve extra parameters, it contributes to relatively lowe p>In this section, we firstly investigate the best framework structure through the exploration study. Moreover, we compare with the most related work <ref type=""bibr"" target=""#b28"">[29]</ref> as well. In the comparison experiments, we focus on comparing the robustness between the enhanced CNN and th cc T(m) Acc T(m) Acc T(m) Acc T(m) O 4% 0.</formula><p>Comparison with the Related Work As mentioned in Section 2, the denoising approach proposed in <ref type=""bibr"" target=""#b28"">[29]</ref> is similar to our denoising layers in FPD. Therefore, we conduct a comparison experiment with <ref type=""bib h proposed in <ref type=""bibr"" target=""#b28"">[29]</ref> is similar to our denoising layers in FPD. Therefore, we conduct a comparison experiment with <ref type=""bibr"" target=""#b28"">[29]</ref> as well. In Table <ref type=""table"" target=""#tab_5"">1</ref>, X represents the enhanced CNN by <ref type=""bib periment with <ref type=""bibr"" target=""#b28"">[29]</ref> as well. In Table <ref type=""table"" target=""#tab_5"">1</ref>, X represents the enhanced CNN by <ref type=""bibr"" target=""#b28"">[29]</ref>. We observe that our F 2I−Mid outperforms X . Especially, the performance of thwarting the white-box attack",1
"ge classification, object detection, and natural language processing <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b27"">28]</ref>. While deep learning has brought great convenience to our lives, its weakness is also catching researchers' a",0
"er-growing ability of deep learning has found numerous applications mainly in image classification, object detection, and natural language processing <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b27"">28]</ref>. While deep learning has bro e channel three times on MNIST for network consistency. For both CALTECH-101 and CALTECH-256, we randomly choose 866 and 1,422 images as test images  <ref type=""bibr"" target=""#b11"">[12]</ref> as well as ResNeXt-50 <ref type=""bibr"" target=""#b29"">[30]</ref> are enhanced in the following experiments. W",0
"><p>To thwart these attacks, many defence methods have been proposed. Most of them use adversarial training to increase the network robustness, e.g., <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b13"">14]</ref>. However, as training often targets a specific attack, the resulting d ) such as <ref type=""bibr"" target=""#b18"">[19]</ref>. However, the aforementioned approaches cannot thwart structure-replaced white-box attacks easily <ref type=""bibr"" target=""#b0"">[1]</ref>. Attackers could still conduct attacks by approximating gradients of their non-differentiable computations. In",0
"cifically, most convolutional layers are very sensitive to perturbations brought by adversarial samples (e.g., <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b30"">31]</ref>), resulting in misclassifications. These so-called adversarial attacks may adopt either white-box or black-bo",0
"hood estimation (MLE) method for existing invertible neural networks <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b28"">29,</ref><ref type=""bibr"" target=""#b3"">4]</ref> could achieve our goal, since the model distribution doesn't exist here vertible Neural Network</head><p>The invertible neural network (INN) <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b28"">29,</ref><ref type=""bibr"" target=""#b31"">32,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b7""> ], which is widely adopted by prevalent flow-based generative models <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b28"">29,</ref><ref type=""bibr"" target=""#b3"">4]</ref>. It is equivalent to minimizing the Kullback-Leibler (KL) divergence KL",1
"he upscaling task by modeling the distribution of lost information during downscaling. We note that according to the Nyquist-Shannon sampling theorem <ref type=""bibr"" target=""#b46"">[47]</ref>, the lost information during downscaling an HR image amounts to high-frequency contents. Thus we firstly emp",0
"""#b20"">[21]</ref> nor the maximum likelihood estimation (MLE) method for existing invertible neural networks <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b28"">29,</ref><ref type=""bibr"" target=""#b3"">4]</ref> could achieve our goal, since t //www.tei-c.org/ns/1.0""><head n=""2.2"">Invertible Neural Network</head><p>The invertible neural network (INN) <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b28"">29,</ref><ref type=""bibr"" target=""#b31"">32,</ref><ref type=""bibr"" target=""#b21"" e., max ? E q(x) [log f -1 ? # [p(y, z)]], which is widely adopted by prevalent flow-based generative models <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b28"">29,</ref><ref type=""bibr"" target=""#b3"">4]</ref>. It is equivalent to minimizing 30,</ref><ref type=""bibr"" target=""#b9"">10]</ref>.</p><p>INN is composed of invertible blocks. In this study, we employ the invertible architecture in <ref type=""bibr"" target=""#b15"">[16]</ref>. For the l-th block, input h l is split into h l 1 and h l 2 along the channel axis, and they undergo the ad -?(h l+1 1 ), h l 1 = h l+1 1 -?(h l 2 ),<label>(2)</label></formula><p>To enhance the transformation ability, the identity branch is often augmented <ref type=""bibr"" target=""#b15"">[16]</ref>:</p><formula xml:id=""formula_2"">h l+1 1 = h l 1 exp(?(h l 2 )) + ?(h l 2 ), h l+1 2 = h l 2 exp(?(h l+1 1 )) abstract the LR and latent representations. We leverage the general coupling layer architecture proposed in <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b15"">16]</ref>, i.e. Eqs. <ref type=""bibr"" target=""#b0"">(1,</ref><ref type=""bibr"" target=""#b2"">3)</ref>.</p><p>Utilizing the /></figure> <figure xmlns=""http://www.tei-c.org/ns/1.0"" type=""table"" xml:id=""tab_4""><head></head><label></label><figDesc>The mini-batch size is set to<ref type=""bibr"" target=""#b15"">16</ref>. The input HR image is randomly cropped into 144 ? 144 and augmented by applying random horizontal and vertica",0
"based image compression methods <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b44"">45,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b39"">40]</ref> show promising results on both visual effect and compression ratio. How",0
"test time? Inspired by recent research in computer vision <ref type=""bibr"" target=""#b51"">(Zheng et al., 2016)</ref>, Neural Machine Translation (NMT; <ref type=""bibr"" target=""#b12"">Cheng et al., 2018)</ref>, and ASR <ref type=""bibr"" target=""#b45"">(Sperber et al., 2017)</ref>, we propose two Noise-Aw . <ref type=""bibr"" target=""#b51"">Zheng et al. (2016)</ref> presented a general method to stabilize model predictions against small input distortions. <ref type=""bibr"" target=""#b12"">Cheng et al. (2018)</ref> continued their work and developed the adversarial stability training method for NMT by addin",1
"e=""bibr"" target=""#b51"">(Zheng et al., 2016)</ref>, Neural Machine Translation (NMT; <ref type=""bibr"" target=""#b12"">Cheng et al., 2018)</ref>, and ASR <ref type=""bibr"" target=""#b45"">(Sperber et al., 2017)</ref>, we propose two Noise-Aware Training (NAT) objectives that improve the accuracy of sequenc uccessfully applied in other domains, including computer vision <ref type=""bibr"" target=""#b27"">(Krizhevsky et al., 2012)</ref> and speech recognition <ref type=""bibr"" target=""#b45"">(Sperber et al., 2017)</ref>.</p><p>During training, we artificially induce noise into the original sentences using the",1
"two domains and design an approach that is transferable to different noise distributions at test time? Inspired by recent research in computer vision <ref type=""bibr"" target=""#b51"">(Zheng et al., 2016)</ref>, Neural Machine Translation (NMT; <ref type=""bibr"" target=""#b12"">Cheng et al., 2018)</ref>, n the input data to perform training of the neural model using a mixture of noisy and clean samples.</p><p>• We implement a stability training method <ref type=""bibr"" target=""#b51"">(Zheng et al., 2016)</ref>, adapted to the sequence labeling scenario, which explicitly addresses the noisy input data ook-up tables.</p><p>Robust representations Another method to improve robustness is to design a representation that is less sensitive to noisy input. <ref type=""bibr"" target=""#b51"">Zheng et al. (2016)</ref> presented a general method to stabilize model predictions against small input distortions. <r",1
"tribution during training.</p><p>Adversarial learning Adversarial attacks seek to mislead the neural models by feeding them with adversarial examples <ref type=""bibr"" target=""#b47"">(Szegedy et al., 2014)</ref>. In a white-box attack scenario <ref type=""bibr"" target=""#b22"">(Goodfellow et al., 2015;</",0
"nst state-of-the-art baseline models <ref type=""bibr"" target=""#b37"">(Peters et al., 2018;</ref><ref type=""bibr"" target=""#b2"">Akbik et al., 2018;</ref><ref type=""bibr"" target=""#b16"">Devlin et al., 2019)</ref> and demonstrate the effectiveness of our approach ( §4).</p><p>• To support future research e) for English and Wikipedia FastText embeddings <ref type=""bibr"" target=""#b8"">(Bojanowski et al., 2017</ref>; FLAIR + Wiki) for German.</p><p>• BERT <ref type=""bibr"" target=""#b16"">(Devlin et al., 2019)</ref> employs a Transformer encoder to learn a BiLM from large unlabeled text corpora and sub-wor",0
"epeated all experiments five times and reported mean and standard deviation.</p><p>Implementation We implemented our models using the FLAIR framework <ref type=""bibr"" target=""#b1"">(Akbik et al., 2019)</ref> <ref type=""foot"" target=""#foot_7"">8</ref> . We extended their sequence labeling model by inte",0
"8)</ref>. Although this problem is not new to NLP, only a few works addressed it explicitly <ref type=""bibr"" target=""#b38"">(Piktus et al., 2019;</ref><ref type=""bibr"" target=""#b25"">Karpukhin et al., 2019)</ref>. Other methods must rely on the noise that occurs naturally in the training data.</p><p>I",0
"h natural noise. The best accuracy was achieved for η train from 10 to 30%, which roughly corresponds to the label-preserving noise range. Similar to <ref type=""bibr"" target=""#b23"">Heigold et al. (2018)</ref> and <ref type=""bibr"" target=""#b11"">Cheng et al. (2019)</ref>, we conclude that a non-zero n ta augmentation A natural strategy to improve robustness to noise is to augment the training data with samples perturbed using a similar noise model. <ref type=""bibr"" target=""#b23"">Heigold et al. (2018)</ref>  </p><formula xml:id=""formula_5"">LD = 0 LD = 1 LD = 2 LD = 3 LD 4</formula><p>Levenshtein D",0
"e neural models by feeding them with adversarial examples <ref type=""bibr"" target=""#b47"">(Szegedy et al., 2014)</ref>. In a white-box attack scenario <ref type=""bibr"" target=""#b22"">(Goodfellow et al., 2015;</ref><ref type=""bibr"" target=""#b18"">Ebrahimi et al., 2018)</ref> we assume that the attacker",0
"get=""#b35"">Parada et al., 2011)</ref>. Sequence labeling is also often performed on user-generated text, which may contain spelling mistakes or typos <ref type=""bibr"" target=""#b15"">(Derczynski et al., 2013)</ref>. Errors introduced in an upstream task are propagated downstream, diminishing the perfo Unfortunately, this is rarely the case. User-generated text is a rich source of informal language containing misspellings, typos, or scrambled words <ref type=""bibr"" target=""#b15"">(Derczynski et al., 2013)</ref>. Noise can also be introduced in an upstream task, like OCR <ref type=""bibr"" target=""#b",0
"sequence models <ref type=""bibr"" target=""#b0"">(Afli et al., 2016;</ref><ref type=""bibr"" target=""#b42"">Schmaltz et al., 2017)</ref> and hybrid systems <ref type=""bibr"" target=""#b43"">(Schulz and Kuhn, 2017)</ref>.</p><p>In this paper, we have taken a different approach and attempted to make our models",0
"ned on clean text, although in real-world scenarios, they often follow an error-prone upstream component, such as Optical Character Recognition (OCR; <ref type=""bibr"" target=""#b34"">Neudecker, 2016)</ref> or Automatic Speech Recognition (ASR; <ref type=""bibr"" target=""#b35"">Parada et al., 2011)</ref>. ogy was more advanced than several years ago when many historical archives were digitized <ref type=""bibr"" target=""#b26"">(Kim and Cassidy, 2015;</ref><ref type=""bibr"" target=""#b34"">Neudecker, 2016)</ref>, the most widely used engines still had difficulties with non-standard or lower quality input.</",0
"e=""bibr"" target=""#b25"">Shi et al. (2016)</ref> proposed a local classification-based method based on structure similarity and side effect similarity. <ref type=""bibr"" target=""#b4"">Ferdousi et al. (2017)</ref> predicted DDIs based on drug functional similarities. <ref type=""bibr"" target=""#b10"">Kastri",1
"arget=""#b31"">Wang et al., 2009;</ref><ref type=""bibr"" target=""#b8"">Kanehisa et al., 2010;</ref><ref type=""bibr"" target=""#b12"">Kuhn et al., 2010;</ref><ref type=""bibr"" target=""#b15"">Li et al., 2010;</ref><ref type=""bibr"" target=""#b11"">Knox et al., 2011;</ref><ref type=""bibr"" target=""#b13"">Law et al.,",0
"tors based on seven types of drug-drug similarities to describe drug-drug pairs, and then built the logistic regression-based models to predict DDIs. <ref type=""bibr"" target=""#b1"">Cheng and Zhao (2014)</ref> combined several drug-drug similarities to describe drug-drug pairs and adopted four classif",0
"(pharmacokinetic mechanism), effect (pharmacodynamic mechanism), advice (not taking these two drugs together) and int (without further information). <ref type=""bibr"" target=""#b24"">Ryu et al. (2018)</ref> classified the biological events collected from DrugBank into 86 types and built the deep learn , so we can extract DDI events through the standard descriptions and conduct further study. Second, there are a variety of drug features in DrugBank. <ref type=""bibr"" target=""#b24"">Ryu et al. (2018)</ref> only made use of the drug chemical substructures. Considering more features is necessary for co ns=""http://www.tei-c.org/ns/1.0""><head n=""3.4"">Method Comparison</head><p>We compare DDIMDL with one state-of-the-art event prediction method DeepDDI <ref type=""bibr"" target=""#b24"">(Ryu et al., 2018)</ref>. We also consider several popular classification approaches, i.e. random forest (RF), k-neares learning. Thus, the models based on RF, KNN, LR and DNN are used as the baseline methods.</p><p>We implement DeepDDI according to the descriptions in <ref type=""bibr"" target=""#b24"">(Ryu et al., 2018)</ref>. The network of DeepDDI has eight hidden layers, and each layer has 2048 nodes. We adjust the event prediction, and our multimodal deep learning framework outperforms the traditional classifiers and deep network structures in previous studies <ref type=""bibr"" target=""#b24"">(Ryu et al., 2018;</ref><ref type=""bibr"" target=""#b14"">Lee et al., 2019)</ref>.</p></div> <div xmlns=""http://www.tei-c.",0
"ecent years, especially among the elders who suffer from multiple diseases. The recent study <ref type=""bibr"" target=""#b9"">(Kantor et al., 2015;</ref><ref type=""bibr"" target=""#b22"">Qato et al., 2016)</ref> showed that 67% of elderly Americans took five or more medications in 2010-2011, including pre",0
"kernel extracts matching patterns which provide a variety of relevance match signals and shows strong performance in various ad-hoc retrieval dataset <ref type=""bibr"" target=""#b3"">(Dai and Callan, 2019)</ref>. Recent research also has shown kernels can be integrated with contextualized representatio",1
"verification benchmark, KGAT achieves a 70.38% FEVER score, significantly outperforming previous BERT and Graph Neural Network (GNN) based approaches <ref type=""bibr"" target=""#b34"">(Zhou et al., 2019)</ref>. Our experiments demonstrate KGAT's strong effectiveness especially on facts that require mul ngOS <ref type=""bibr"" target=""#b30"">(Yin and Roth, 2018)</ref> further incorporates evidence identification to improve claim verification.</p><p>GEAR <ref type=""bibr"" target=""#b34"">(Zhou et al., 2019)</ref> formulates claim verification as a graph reasoning task and provides two kinds of attentions. nels (Sec. 3.4).</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.1"">Reasoning with Evidence Graph</head><p>Similar to previous research <ref type=""bibr"" target=""#b34"">(Zhou et al., 2019)</ref>, KGAT constructs the evidence graph G by using each claim-evidence pair as a node and connect es both multiple and single evidence reasoning scenarios and produces a probability P (y|c, D) to predict claim label y. Different from previous work <ref type=""bibr"" target=""#b34"">(Zhou et al., 2019)</ref>, we follow the standard graph label prediction setting in graph neural network <ref type=""bib n combines neighbor node information to node representation v p . The aggregation is done by a graph attention mechanism, the same with previous work <ref type=""bibr"" target=""#b34"">(Zhou et al., 2019)</ref>.</p><p>It first calculate the attention weight β q→p of n q node according to the p-th node n ><head n=""3.4"">Node Kernel for Evidence Aggregation</head><p>The per-node predictions are combined by the ""readout"" function in graph neural networks <ref type=""bibr"" target=""#b34"">(Zhou et al., 2019)</ref>, where KGAT uses node kernels to learn the importance of each evidence.</p><p>It first uses n ines, they significantly outperform previous methods without pre-training. BERT-pair, BERT-concat and GEAR are three baselines from the previous work <ref type=""bibr"" target=""#b34"">(Zhou et al., 2019)</ref>. BERT-pair and BERTconcat regard claim-evidence pair individually or concatenate all evidence less, for more fair comparisons, our following experiments are all based on ESIM sentence retrieval, which is the one used by GEAR, our main baseline <ref type=""bibr"" target=""#b34"">(Zhou et al., 2019)</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""5.2"">Performance on Different Sc ></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""6"">Case Study</head><p>Table <ref type=""table"">5</ref> shows the example claim used in GEAR <ref type=""bibr"" target=""#b34"">(Zhou et al., 2019)</ref> and the evidence sentences retrieved by ESIM, among which the first two are required evidence ) [Jardine] Ray Jardine American rock climber, lightweight backpacker, inventor, author and global adventurer. Label: SUPPORT Table5: An example claim<ref type=""bibr"" target=""#b34"">(Zhou et al., 2019)</ref> whose verification requires multiple pieces of evidence.</figDesc></figure> <figure xmlns=""ht n FEVER and achieved better performance <ref type=""bibr"" target=""#b5"">(Devlin et al., 2019;</ref><ref type=""bibr"" target=""#b13"">Li et al., 2019;</ref><ref type=""bibr"" target=""#b34"">Zhou et al., 2019;</ref><ref type=""bibr"" target=""#b24"">Soleimani et al., 2019)</ref>.</p><p>The recent development of n etrieval step retrieves related Wikipedia pages and is kept the same with previous work <ref type=""bibr"" target=""#b9"">(Hanselowski et al., 2018;</ref><ref type=""bibr"" target=""#b34"">Zhou et al., 2019;</ref><ref type=""bibr"" target=""#b24"">Soleimani et al., 2019)</ref>. For a given claim, it first utili ntence retrieval. The ESIM based sentence retrieval keeps the same as the previous work <ref type=""bibr"" target=""#b9"">(Hanselowski et al., 2018;</ref><ref type=""bibr"" target=""#b34"">Zhou et al., 2019)</ref>. The base version of BERT is used to implement our BERT based sentence retrieval model. We use ed sentences.</p><p>Compared with baseline models, KGAT is the best on all testing scenarios. With ESIM sentence retrieval, same as the previous work <ref type=""bibr"" target=""#b34"">(Zhou et al., 2019;</ref><ref type=""bibr"" target=""#b9"">Hanselowski et al., 2018)</ref>, KGAT outperforms the graph atte",1
"atenates all evidence together to verify the claim. One can also conduct reasoning for each claim evidence pair and aggregate them to the claim label <ref type=""bibr"" target=""#b15"">(Luken et al., 2018;</ref><ref type=""bibr"" target=""#b31"">Yoneda et al., 2018;</ref><ref type=""bibr"" target=""#b9"">Hansel",0
"eractions for joint reasoning over several evidence pieces.</p><p>Many fact verification systems leverage Natural Language Inference (NLI) techniques <ref type=""bibr"" target=""#b1"">(Chen et al., 2017b;</ref><ref type=""bibr"" target=""#b7"">Ghaeini et al., 2018;</ref><ref type=""bibr"" target=""#b20"">Parikh hemselves and there are often multiple evidence pieces. One of the most widely used NLI models in FEVER is Enhanced Sequential Inference Model (ESIM) <ref type=""bibr"" target=""#b1"">(Chen et al., 2017b)</ref>, which employs some forms of hard or soft alignment to associate the relevant sub-components",0
"et al., 2018;</ref><ref type=""bibr"" target=""#b31"">Yoneda et al., 2018;</ref><ref type=""bibr"" target=""#b9"">Hanselowski et al., 2018)</ref>. TwoWingOS <ref type=""bibr"" target=""#b30"">(Yin and Roth, 2018)</ref> further incorporates evidence identification to improve claim verification.</p><p>GEAR <ref",0
"h also has shown kernels can be integrated with contextualized representations, i.e., BERT, to better model the relevance between query and documents <ref type=""bibr"" target=""#b16"">(MacAvaney et al., 2019)</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3"">Kernel Graph Attention N "" target=""#b27"">(Xiong et al., 2017;</ref><ref type=""bibr"" target=""#b4"">Dai et al., 2018;</ref><ref type=""bibr"" target=""#b22"">Qiao et al., 2019;</ref><ref type=""bibr"" target=""#b16"">MacAvaney et al., 2019)</ref>:</p><formula xml:id=""formula_8"">K(M q→p i ) = {K1(M q→p i ), ..., KK (M q→p i )}. (7)</fo",0
"d aggregate them to the claim label <ref type=""bibr"" target=""#b15"">(Luken et al., 2018;</ref><ref type=""bibr"" target=""#b31"">Yoneda et al., 2018;</ref><ref type=""bibr"" target=""#b9"">Hanselowski et al., 2018)</ref>. TwoWingOS <ref type=""bibr"" target=""#b30"">(Yin and Roth, 2018)</ref> further incorporate is the best on all testing scenarios. With ESIM sentence retrieval, same as the previous work <ref type=""bibr"" target=""#b34"">(Zhou et al., 2019;</ref><ref type=""bibr"" target=""#b9"">Hanselowski et al., 2018)</ref>, KGAT outperforms the graph attention models GEAR and our GAT on both development and te s. The baselines include top models during FEVER 1.0 task and BERT based models.</p><p>Three top models in FEVER 1.0 shared task are compared. Athene <ref type=""bibr"" target=""#b9"">(Hanselowski et al., 2018)</ref> and UNC NLP <ref type=""bibr"" target=""#b17"">(Nie et al., 2019a)</ref> utilize ESIM to en to find relevant Wikipedia pages through the online Me-diaWiki API<ref type=""foot"" target=""#foot_1"">4</ref> . Then the convinced article are reserved <ref type=""bibr"" target=""#b9"">(Hanselowski et al., 2018)</ref>.</p><p>Sentence retrieval. The sentence retrieval part focuses on selecting related sen mlns=""http://www.tei-c.org/ns/1.0""><head>Model</head></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>Dev</head><p>Test LA FEVER LA FEVER Athene <ref type=""bibr"" target=""#b9"">(Hanselowski et al., 2018)</ref> 68.49 64.74 65.46 61.58 UCL MRG <ref type=""bibr"" target=""#b31"">(Yoneda et al., 2018)</r plementation details.</p><p>Document retrieval. The document retrieval step retrieves related Wikipedia pages and is kept the same with previous work <ref type=""bibr"" target=""#b9"">(Hanselowski et al., 2018;</ref><ref type=""bibr"" target=""#b34"">Zhou et al., 2019;</ref><ref type=""bibr"" target=""#b24"">So r experiments: ESIM based sentence retrieval and BERT based sentence retrieval. The ESIM based sentence retrieval keeps the same as the previous work <ref type=""bibr"" target=""#b9"">(Hanselowski et al., 2018;</ref><ref type=""bibr"" target=""#b34"">Zhou et al., 2019)</ref>. The base version of BERT is use",0
"sis. BERT, the pre-trained deep bidirectional Transformer, has also been used for better text representation in FEVER and achieved better performance <ref type=""bibr"" target=""#b5"">(Devlin et al., 2019;</ref><ref type=""bibr"" target=""#b13"">Li et al., 2019;</ref><ref type=""bibr"" target=""#b34"">Zhou et a epresentations are initialized by feeding the concatenated sequence of claim, document (Wiki) title, and evidence sentence, to pre-trained BERT model <ref type=""bibr"" target=""#b5"">(Devlin et al., 2019)</ref>. Specifically, in the node n p , the claim and evidence correspond to m tokens (with ""[SEP]""",0
"bel>(2)</label></formula><p>The joint reasoning probability P (y|n p , G) calculates node label prediction with multiple evidence. The readout module <ref type=""bibr"" target=""#b12"">(Knyazev et al., 2019)</ref> calculates the probability P (n p |G) and attentively combines per-node signals for predic",0
"es.</p><p>Many fact verification systems leverage Natural Language Inference (NLI) techniques <ref type=""bibr"" target=""#b1"">(Chen et al., 2017b;</ref><ref type=""bibr"" target=""#b7"">Ghaeini et al., 2018;</ref><ref type=""bibr"" target=""#b20"">Parikh et al., 2016;</ref><ref type=""bibr"">Radford et al., 201",0
"dom variables in information theory to strengthen the relationship between the preference factors and the disentangled embeddings.</p><p>According to <ref type=""bibr"" target=""#b30"">(Yang et al., 2018)</ref>, the mutual information maximization can be converted to the following form.</p><p>Given the",1
"a user by aggregating her clicked news history with different weights. However, these methods <ref type=""bibr"" target=""#b27"">(Wu et al., 2019b;</ref><ref type=""bibr"" target=""#b31"">Zhu et al., 2019;</ref><ref type=""bibr"" target=""#b0"">An et al., 2019)</ref> usually focus on news contents, and seldom ) is a content-based deep neural network to rank a set of documents given a query. Some works <ref type=""bibr"" target=""#b24"">(Wang et al., 2018;</ref><ref type=""bibr"" target=""#b31"">Zhu et al., 2019)</ref> propose to improve news representations via external knowledge, and learn representations of us the dimension of word (entity) and entity-type embeddings. These embeddings can be pre-trained from a large corpus or randomly initialized. Following <ref type=""bibr"" target=""#b31"">(Zhu et al., 2019)</ref>, we define the profile embedding</p><formula xml:id=""formula_1"">P = [e 1 , g(c 1 ), e 2 , g(c uding news title T and profile P . The content-based news representations would be taken as initial input embeddings of our model GNUD. Following DAN <ref type=""bibr"" target=""#b31"">(Zhu et al., 2019)</ref>, we use two parallel convolutional neural networks (PCNN) taking the title T and profile P of We use two datasets named Adressa-1week and Adressa-10week, which respectively collect news click logs as long as 1 week and 10 weeks. Following DAN <ref type=""bibr"" target=""#b31"">(Zhu et al., 2019)</ref>, we just select user id, news id, time-stamp, the title and profile of news to build our datas d knowledge-level representations. We model the news title and profile as semantic-level and knowledge-level representations, respectively.</p><p>DAN <ref type=""bibr"" target=""#b31"">(Zhu et al., 2019)</ref>, a deep attention neural network for news recommendation which can capture the dynamic diversi",1
"tion via multi-channel CNN and gets a representation of a user by aggregating her clicked news history with different weights. However, these methods <ref type=""bibr"" target=""#b27"">(Wu et al., 2019b;</ref><ref type=""bibr"" target=""#b31"">Zhu et al., 2019;</ref><ref type=""bibr"" target=""#b0"">An et al., propose to improve news representations via external knowledge, and learn representations of users from their browsed news using an attention module. <ref type=""bibr"" target=""#b27"">Wu et al. (2019b)</ref> applied attention mechanism at both word-and news-level to model different informativeness on n",0
"of users, news recommendation has been playing an increasingly important role for mining users' reading interest and providing personalized contents <ref type=""bibr"" target=""#b11"">(IJntema et al., 2010;</ref><ref type=""bibr"" target=""#b17"">Liu et al., 2010)</ref>.</p><p>A core problem in news recomm",0
"ifferent weights. However, these methods <ref type=""bibr"" target=""#b27"">(Wu et al., 2019b;</ref><ref type=""bibr"" target=""#b31"">Zhu et al., 2019;</ref><ref type=""bibr"" target=""#b0"">An et al., 2019)</ref> usually focus on news contents, and seldom consider the collaborative signal in the form of high- <ref type=""bibr"" target=""#b26"">Wu et al. (2019a)</ref> exploited different types of news information with an attentive multi-view learning framework. <ref type=""bibr"" target=""#b0"">An et al. (2019)</ref> considered both titles and topic categories of news, and learned both long-and shortterm user rep",0
"gio et al., 2013)</ref>, which has been successfully applied in the field of computer vision <ref type=""bibr"" target=""#b12"">(Kim and Mnih, 2018;</ref><ref type=""bibr"" target=""#b3"">Gidaris et al., 2018;</ref><ref type=""bibr"" target=""#b8"">Hsieh et al., 2018)</ref>. β−VAE <ref type=""bibr"" target=""#b7"">",0
"tion mechanism. However, these works seldom mine highorder structure information.</p><p>Graph neural networks. Recently, graph neu-ral networks (GNN) <ref type=""bibr"" target=""#b16"">(Kipf and Welling, 2016;</ref><ref type=""bibr"" target=""#b6"">Hamilton et al., 2017;</ref><ref type=""bibr"" target=""#b22"">",0
"rative approach that can automatically discover the independent latent factors of variation in unsupervised data, which is based on the VAE framework <ref type=""bibr"" target=""#b15"">(Kingma and Welling, 2013)</ref>. Recently, disentangled representation learning has been investigated on graph-structu",0
".org/ns/1.0""><head n=""5.1"">Datasets and Experimental Settings</head><p>Datasets. We conduct experiments on the realworld online news datasets Adressa <ref type=""bibr"" target=""#b4"">(Gulla et al., 2017)</ref> 2 from a Norwegian news portal to evaluate our model. We use two datasets named Adressa-1week",0
"n in the observed data <ref type=""bibr"" target=""#b1"">(Bengio et al., 2013)</ref>, which has been successfully applied in the field of computer vision <ref type=""bibr"" target=""#b12"">(Kim and Mnih, 2018;</ref><ref type=""bibr"" target=""#b3"">Gidaris et al., 2018;</ref><ref type=""bibr"" target=""#b8"">Hsieh",0
"actions specifically for text simplification. Concurrent work further shows the success of search-based unsupervised text generation for paraphrasing <ref type=""bibr"" target=""#b15"">(Liu et al., 2020)</ref> and summa-rization <ref type=""bibr"" target=""#b30"">(Schumann et al., 2020)</ref>.</p></div> <di",1
"the success of search-based unsupervised text generation for paraphrasing <ref type=""bibr"" target=""#b15"">(Liu et al., 2020)</ref> and summa-rization <ref type=""bibr"" target=""#b30"">(Schumann et al., 2020)</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3"">Model</head><p>In this se",1
"tability of a sentence, among many sentence probability-based scoring functions. SLOR was also shown to be effective in unsupervised text compression <ref type=""bibr"" target=""#b9"">(Kann et al., 2018)</ref>.</p><p>Given a trained language model (LM) and a sentence s, SLOR is defined as</p><formula xm",0
"ution, and 5.0 for extraction. The weights in the scoring function (α, β, γ, δ) are set to 0.5, 1.0, 0.25 and 1.0, respectively.</p><p>We use CoreNLP <ref type=""bibr"" target=""#b16"">(Manning et al., 2014)</ref> to construct the constituency tree and Spacy<ref type=""foot"" target=""#foot_3"">3</ref> to g",0
"s also considered in the PBMT framework, for example, constituency trees <ref type=""bibr"" target=""#b42"">(Zhu et al., 2010)</ref> and dependency trees <ref type=""bibr"" target=""#b1"">(Bingel and Søgaard, 2016)</ref>. <ref type=""bibr"" target=""#b21"">Narayan and Gardent (2014)</ref> performed probabilisti",0
"ke two important modifications to a plain LM.</p><p>First, we replace an LM's estimated sentence probability with the syntactic log-odds ratio (SLOR, <ref type=""bibr"" target=""#b25"">Pauls and Klein, 2012)</ref>, to better measure fluency and human acceptability. According to <ref type=""bibr"" target=""",0
"ype=""bibr"" target=""#b21"">Narayan and Gardent (2014)</ref> performed probabilistic sentence splitting and deletion, followed by MT-based paraphrasing. <ref type=""bibr"" target=""#b23"">Nisioi et al. (2017)</ref> employed neural machine translation (NMT) for text simplification, using a sequence-to-seque ation structures. Next, we compare our method with neural machine translation (NMT) systems: EncDecA, which is a vanilla Seq2Seq model with attention <ref type=""bibr"" target=""#b23"">(Nisioi et al., 2017)</ref>; Dress and Dress-Ls, which are based on deep reinforcement learning <ref type=""bibr"" target",0
"tion approach based on the paradigm of style transfer. However, their model is hard to interpret and control, like other neural network-based models. <ref type=""bibr"" target=""#b22"">Narayan and Gardent (2016)</ref> attempted to address both issues using a pipeline of lexical substitution, sentence sp phrase extraction, deletion and reordering). Our model seeks the best simplified candidate sentence according to the scoring function. Compared with <ref type=""bibr"" target=""#b22"">Narayan and Gardent (2016)</ref>, the order of our simplification operations is not fixed and is decided by the model. is less interpretable and controllable. They cannot perform syntactic simplification since syntax typically does not change in style-transfer tasks. <ref type=""bibr"" target=""#b22"">Narayan and Gardent (2016)</ref> built a pipeline-based unsupervised framework with lexical simplification, sentence sp",0
"e=""bibr"" target=""#b7"">Guo et al. (2018)</ref> showed that simplification benefits from multi-task learning with paraphrase and entailment generation. <ref type=""bibr"" target=""#b17"">Martin et al. (2019)</ref> enhanced the transformer architecture with conditioning parameters such as length, lexical a ussing on lexical simplification <ref type=""bibr"" target=""#b12"">(Kriz et al., 2019)</ref>; and Access, which is based on the transformer architecture <ref type=""bibr"" target=""#b17"">(Martin et al., 2019)</ref>. Finally, we compare with a supervised edit-based neural model, Edit-NTS <ref type=""bibr"" t",0
"ed approaches have recently been explored for natural language generation tasks, such as style transfer, paraphrasing, and sentence error correction. <ref type=""bibr"" target=""#b14"">Li et al. (2018)</ref> proposed edit-based style transfer without parallel supervision. They replaced style-specific ph",0
"used effectively for downstream NLP tasks in academic paper analysis.</p><p>The pipeline for creating S2ORC was used to construct the CORD-19 corpus <ref type=""bibr"" target=""#b2"">(Wang et al., 2020)</ref>, which saw fervent adoption as the canonical resource for COVID-19 text mining. CORD-19 is aim",1
"br"" target=""#b24"">(Kanakia et al., 2019;</ref><ref type=""bibr"" target=""#b17"">Eto, 2019;</ref><ref type=""bibr"" target=""#b19"">Haruna et al., 2018;</ref><ref type=""bibr"" target=""#b46"">Small, 1973)</ref> or bibliometric analysis <ref type=""bibr"" target=""#b14"">(Ding et al., 2014;</ref><ref type=""bibr"" ta",0
". (2019)</ref> are not able to perform post-hoc debiasing and require changing the data or underlying word embeddings and retraining which is costly. <ref type=""bibr"" target=""#b7"">Bordia and Bowman (2019)</ref> only study word-level language models and also requires re-training. Finally, <ref type=""",1
"is costly. <ref type=""bibr"" target=""#b7"">Bordia and Bowman (2019)</ref> only study word-level language models and also requires re-training. Finally, <ref type=""bibr"" target=""#b16"">Kurita et al. (2019)</ref> only measure bias on BERT by extending the word-level Word Embedding Association Test (WEAT)",1
"23"">(Manzini et al., 2019)</ref> bias attributes such as gender, race, and religion.</p><p>More recently, sentence-level representations such as ELMo <ref type=""bibr"" target=""#b31"">(Peters et al., 2018)</ref>, BERT <ref type=""bibr"" target=""#b10"">(Devlin et al., 2019)</ref>, and GPT <ref type=""bibr"" tions. Our experiments are performed on two widely popular sentence encoders BERT <ref type=""bibr"" target=""#b10"">(Devlin et al., 2019)</ref> and ELMo <ref type=""bibr"" target=""#b31"">(Peters et al., 2018)</ref>, showing that our approach reduces the bias while preserving performance on downstream sequ two widely-used sentence encoders: BERT<ref type=""foot"" target=""#foot_2"">2</ref>  <ref type=""bibr"" target=""#b10"">(Devlin et al., 2019)</ref> and ELMo <ref type=""bibr"" target=""#b31"">(Peters et al., 2018)</ref>. Note that the pre-trained BERT encoder must be fine-tuned on task-specific data. This impl",1
"get=""#b4"">Basta et al., 2019)</ref>, none of them have been able to successfully remove bias from pretrained sentence representations. In particular, <ref type=""bibr"" target=""#b47"">Zhao et al. (2019)</ref>, <ref type=""bibr"" target=""#b28"">Park et al. (2018), and</ref><ref type=""bibr"" target=""#b11"">Ga",0
"ext. For SST, it has been shown that sentiment analysis datasets have labels that correlate with gender information and therefore contain gender bias <ref type=""bibr"" target=""#b15"">(Kiritchenko and Mohammad, 2018)</ref>. As a result, we do expect possible decreases in accuracy after debiasing. Final",0
"anek et al., 2019)</ref>. As their usage proliferates across various real-world applications <ref type=""bibr"" target=""#b14"">(Huang et al., 2019;</ref><ref type=""bibr"" target=""#b1"">Alsentzer et al., 2019)</ref>, it becomes necessary to recognize the role they play in shaping social biases and stereot",0
"ised methods towards debiasing these word representations for both binary <ref type=""bibr"" target=""#b6"">(Bolukbasi et al., 2016)</ref> and multiclass <ref type=""bibr"" target=""#b23"">(Manzini et al., 2019)</ref> bias attributes such as gender, race, and religion.</p><p>More recently, sentence-level re esentations. To measure bias over multiple classes, we use the Mean Average Cosine similarity (MAC) metric which extends SEAT to a multiclass setting <ref type=""bibr"" target=""#b23"">(Manzini et al., 2019)</ref>. For the binary gender setting, we use words from the Caliskan Tests <ref type=""bibr"" targ lassification task. These results are   <ref type=""formula"">2017</ref>) row N . The last row measures bias in a multiclass religion setting using MAC <ref type=""bibr"" target=""#b23"">(Manzini et al., 2019)</ref> before and after debiasing. MAC score ranges from 0 to 2 and closer to 1 represents lower debiasing step to these sentence representations before they are used in downstream tasks <ref type=""bibr"" target=""#b6"">(Bolukbasi et al., 2016;</ref><ref type=""bibr"" target=""#b23"">Manzini et al., 2019)</ref>. Secondly, sentences display large variety in how they are composed from individual words. in the multiclass religion setting, we modify the Caliskan Tests used in <ref type=""bibr"" target=""#b24"">May et al. (2019)</ref> with lexicons used by <ref type=""bibr"" target=""#b23"">Manzini et al. (2019)</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.2"">Debiasing Setup</head><p>",0
"verage the sentence representations of a concept (e.g. man, woman, science, art) across its contexts (sentence templates) and plot the t-SNE (van der <ref type=""bibr"" target=""#b22"">Maaten and Hinton, 2008)</ref> Figure <ref type=""figure"">2</ref>: Influence of the number of template domains on the ef",0
"ded language learning <ref type=""bibr"" target=""#b39"">(Urbanek et al., 2019)</ref>. As their usage proliferates across various real-world applications <ref type=""bibr"" target=""#b14"">(Huang et al., 2019;</ref><ref type=""bibr"" target=""#b1"">Alsentzer et al., 2019)</ref>, it becomes necessary to recogniz",0
"sentation can also be debiased (after fine-tuning and normalization). We test the effect of SENT-DEBIAS on Question Natural Language Inference (QNLI) <ref type=""bibr"" target=""#b41"">(Wang et al., 2018)</ref> which converts the Stanford Question Answering Dataset (SQuAD) <ref type=""bibr"" target=""#b34""",0
"=""#b8"">(Caliskan et al., 2017)</ref> which measure biases in common stereotypes surrounding gendered names with respect to careers, math, and science <ref type=""bibr"" target=""#b13"">(Greenwald et al., 2009)</ref>. To evaluate biases in the multiclass religion setting, we modify the Caliskan Tests use",0
"hoice for text sequence encoding. When compared to word-level representations, these models have achieved better performance on multiple tasks in NLP <ref type=""bibr"" target=""#b44"">(Wu and Dredze, 2019)</ref>, multimodal learning <ref type=""bibr"" target=""#b45"">(Zellers et al., 2019;</ref><ref type=""",0
"a articles (we only use the first 10% of WikiText-2 which we found to be sufficient to capture formally written text), 2) Stanford Sentiment Treebank <ref type=""bibr"" target=""#b35"">(Socher et al., 2013)</ref>, a collection of 10000 polarized written movie reviews, 3) Reddit data collected from discu s are debiased. We apply SENT-DEBIAS on BERT fine-tuned on two single sentence datasets, Stanford Sentiment Treebank (SST-2) sentiment classification <ref type=""bibr"" target=""#b35"">(Socher et al., 2013)</ref> and Corpus of Linguistic Acceptability (CoLA) grammatical acceptability judgment <ref type= >The variant BERT post SST is BERT after being finetuned on the Stanford Sentiment Treebank(SST-2) task, a binary single-sentence classification task <ref type=""bibr"" target=""#b35"">(Socher et al., 2013)</ref>. During fine-tuning, we first normalize the sentence embedding and then feed it into a line",0
".tei-c.org/ns/1.0""><head n=""1"">Introduction</head><p>Identifying compound-protein interaction (CPI) plays an import role in discovering hit compounds <ref type=""bibr"" target=""#b39"">(Vamathevan et al., 2019)</ref>. Conventional methods, such as structure-based virtual screening and ligand-based virtu",1
"tp://www.tei-c.org/ns/1.0""><head n=""2.1"">Model architecture of TransformerCPI</head><p>The model we proposed is based on the transformer architecture <ref type=""bibr"" target=""#b41"">(Vaswani et al., 2017)</ref>, which was originally devised for neural machine translation tasks. Transformer is an auto",1
"b40"">van Laarhoven et al., 2011;</ref><ref type=""bibr"" target=""#b44"">Wang et al., 2011;</ref><ref type=""bibr"" target=""#b45"">Wang and Zeng, 2013;</ref><ref type=""bibr"" target=""#b47"">Yamanishi et al., 2008)</ref>.</p><p>With the rapid development of deep learning, many types of endto-end frameworks ha",0
"he representation of compounds. In addition, recurrent neural networks were used to extract feature vectors of compounds and proteins in DeepAffinity <ref type=""bibr"" target=""#b15"">(Karimi et al., 2019)</ref>, and <ref type=""bibr"" target=""#b9"">Gao et al. (2018)</ref> and <ref type=""bibr"" target=""#b5",0
"ning based algorithms have been proposed since then, which considers compound information and protein information at the same time in a unified model <ref type=""bibr"" target=""#b1"">(Bleakley and Yamanishi, 2009;</ref><ref type=""bibr"" target=""#b5"">Cheng et al., 2012;</ref><ref type=""bibr"" target=""#b11",0
"IC 50 , Ki and EC 50 as the binding affinity values, which were transformed into negative logarithm, pIC 50 , pK i and pEC 50 . Following early works <ref type=""bibr"" target=""#b22"">(Liu et al., 2007;</ref><ref type=""bibr"" target=""#b43"">Wan et al., 2019)</ref>, a threshold of 6.0 was set to divide or",0
"neralization ability. For the training, we used the LookAhead <ref type=""bibr"" target=""#b50"">(Zhang et al., 2019)</ref> optimizer combined with RAdam <ref type=""bibr"" target=""#b21"">(Liu et al., 2019)</ref> optimizer, which solved the most serious convergence problems caused by Adam optimizer without",0
"t have similar semantics map to the vectors that are close to each other. There have been some works to apply word2vec to represent protein sequences <ref type=""bibr"" target=""#b16"">(Kimothi et al., 2016;</ref><ref type=""bibr"" target=""#b18"">Kobeissy et al., 2015;</ref><ref type=""bibr"">Mazzaferro and",0
"get=""#b1"">(Bleakley and Yamanishi, 2009;</ref><ref type=""bibr"" target=""#b5"">Cheng et al., 2012;</ref><ref type=""bibr"" target=""#b11"">Gonen, 2012;</ref><ref type=""bibr"" target=""#b14"">Jacob and Vert, 2008;</ref><ref type=""bibr"" target=""#b40"">van Laarhoven et al., 2011;</ref><ref type=""bibr"" target=""#b4 ., 2016)</ref>, which ensures that data in KIBA is experimentally validated. Given that the majority of ligands only occur once, we followed SimBoost <ref type=""bibr"" target=""#b14"">(He et al., 2017)</ref> to filter original KIBA dataset to comprise only compounds and proteins with at least 10 intera proteins with at least 10 interactions, gaining a total of 229 proteins and 2111 compounds. Then, we used the suggested threshold KIBA value of 12.1 <ref type=""bibr"" target=""#b14"">(He et al., 2017;</ref><ref type=""bibr"" target=""#b36"">Tang et al., 2014)</ref> to divide dataset into a positive set an",0
"leveraging extra text data to improve lowresource end-to-end ASR under cross-lingual transfer learning setting. To this end, we extend our prior work <ref type=""bibr"" target=""#b0"">[1]</ref>, and propose a hybrid Transformer-LSTM based architecture. This architecture not only takes advantage of the h e=""bibr"" target=""#b4"">[5]</ref>. Such techniques not only require external language models but also lead to a slow inference. To tackle this problem, <ref type=""bibr"" target=""#b0"">[1]</ref> has proposed long short term memory (LSTM)-based encoderdecoder architecture which allows improving the LM cap employ extra text data to improve the decoder.</p><p>In this work, we propose a hybrid Transformer-LSTM architecture which combines the advantages of <ref type=""bibr"" target=""#b0"">[1]</ref> and <ref type=""bibr"" target=""#b5"">[6]</ref>. It not only has a high encoding capacity of the Transformer but a used to boost the decoder of the transferred model.</p><p>The paper is organized as follows. Section 2 describes baseline architectures mentioned in <ref type=""bibr"" target=""#b0"">[1]</ref> and <ref type=""bibr"" target=""#b5"">[6]</ref>. Then, the proposed techniques are presented in Section 3. Experim tei-c.org/ns/1.0""><head n=""2."">Baseline architectures 2.1. LSTM-based encoder-decoder architecture</head><p>A LSTM-based encoder-decoder architecture <ref type=""bibr"" target=""#b0"">[1]</ref>, denoted as A1 in the rest of this paper, consists of a Bidirectional LSTM encoder and a LSTM-based decoder wh edding() and proj() are embedding and projection layers respectively. Figure <ref type=""figure"">1</ref>: LSTM-based encoder-decoder architecture (A1) <ref type=""bibr"" target=""#b0"">[1]</ref>, where the decoder acts as an independent language model.</p><p>From Equation (1), the LSTM is only conditione den state and previous decoding output. In other words, the LSTM acts as an independent language model that can be easily updated with text-only data <ref type=""bibr"" target=""#b0"">[1]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2.2."">Transformer encoder-decoder architecture</h extra text and the labeled data together to fine-tune the transferred model. This avoids a so-called catastrophic forgetting problem as mentioned in <ref type=""bibr"" target=""#b0"">[1]</ref>. Specifically, at each training iteration, we mix a batch of labeled data consisting of B labeled utterances w labeled data and text data respectively. In the second step, the model is further fine-tuned with the labeled data of the target language. Similar to <ref type=""bibr"" target=""#b0"">[1]</ref>, we empirically found that the second step is necessary to improve overall performance.</p><p>Step 1: Fine-tun",1
"he limited resource problem in ASR <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b10"">11]</ref>, on the proposed architecture. Specifically, we first use labeled data",0
"ref> for sequence-to-sequence modeling in natural language processing tasks, then adopted to the ASR task in <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b12"">13]</ref>. The model architecture, denoted as A2, is shown in Fig. <ref type=""figure"" target=""#fig_1"">2</ref>. The enco",0
"This is a common scenario in real-world applications.</p><p>The extra text is usually employed to train language models (LM) applied during decoding <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b3"">4]</ref> and re-scoring stages <ref type=""b",0
"ntrast, the LSTM-based decoder (in Section 2.1) can be easily boosted using the text data. Another issue of the Transformer decoder is slow inference <ref type=""bibr"" target=""#b17"">[18]</ref>. Specifically, to generate an output yi, the decoder needs to process all previous decoding units y1:i−1. On",0
"6 for a detailed summary), D trains the matching models by fine-tuning pre-trained LMs in a simpler architecture.</p><p>Pre-trained LMs such as BERT <ref type=""bibr"" target=""#b12"">[13]</ref> and GPT-2 <ref type=""bibr"" target=""#b40"">[41]</ref> have demonstrated good performance on a wide range of NL /p><p>The result is a model fine-tuned for the EM task. See Appendix A for the model architecture. In D , we fine-tune the popular 12layer BERT model <ref type=""bibr"" target=""#b12"">[13]</ref>, RoBERTa <ref type=""bibr"" target=""#b28"">[29]</ref>, and a 6-layer smaller but faster variant DistilBERT <ref ary <ref type=""bibr"" target=""#b57"">[58]</ref>. We currently support 4 pre-trained models: Distil-BERT <ref type=""bibr"" target=""#b44"">[45]</ref>, BERT <ref type=""bibr"" target=""#b12"">[13]</ref>, RoBERTa <ref type=""bibr"" target=""#b28"">[29]</ref>, and XLNet <ref type=""bibr"" target=""#b60"">[61]</ref>. We l of building a BERT-like model for tables.</p><p>Figure <ref type=""figure"">6</ref> shows the model architecture of D 's language models such as BERT <ref type=""bibr"" target=""#b12"">[13]</ref>, DistilBERT <ref type=""bibr"" target=""#b44"">[45]</ref>, and RoBERTa <ref type=""bibr"" target=""#b28"">[29]</ref> ecall that we construct the baseline by taking the best performing pre-trained model among DistilBERT <ref type=""bibr"" target=""#b44"">[45]</ref>, BERT <ref type=""bibr"" target=""#b12"">[13]</ref>, XLNet <ref type=""bibr"" target=""#b60"">[61]</ref>, and RoBERTa <ref type=""bibr"" target=""#b28"">[29]</ref> foll",1
"The first two techniques help D focus on the right information for making matching decisions. The last technique, data augmentation, is adapted from <ref type=""bibr"" target=""#b30"">[31]</ref> for EM to help D learn ""harder"" to understand the data invariance properties that may exist but are beyond t l signals to distinguish the two entries.</p><p>To address this issue, D applies MixDA, a recently proposed data augmentation technique for NLP tasks <ref type=""bibr"" target=""#b30"">[31]</ref> illustrated in Figure <ref type=""figure"" target=""#fig_3"">3</ref>. Instead of using the augmented example dir able <ref type=""table"" target=""#tab_3"">2</ref> with the entry_swap operator. We compare the different combinations and report the best one. Following <ref type=""bibr"" target=""#b30"">[31]</ref>, we apply MixDA with the interpolation parameter λ sampled from a Beta distribution Beta(0.8, 0.8).</p></div e=""bibr"" target=""#b56"">57,</ref><ref type=""bibr"" target=""#b58"">59</ref>]. We designed a set of DA operators suitable for EM and apply them with MixDA <ref type=""bibr"" target=""#b30"">[31]</ref>, a recently proposed DA strategy based on convex interpolation. To our knowledge, this is the first time dat nsists of span-level operators, such as span_del and span_shuffle. These two operators are used in NLP tasks <ref type=""bibr"" target=""#b56"">[57,</ref><ref type=""bibr"" target=""#b30"">31]</ref> and shown to be effective for text classification. For span_del, we randomly delete from s a span of tokens o r preprocessing input entries. Data augmentation (DA) has been extensively studied in computer vision and has recently received more attention in NLP <ref type=""bibr"" target=""#b30"">[31,</ref><ref type=""bibr"" target=""#b56"">57,</ref><ref type=""bibr"" target=""#b58"">59</ref>]. We designed a set of DA ope",1
"ns of the span in v and the corresponding type of the span. D 's current implementation leverages an open-source Named-Entity Recognition (NER) model <ref type=""bibr"" target=""#b47"">[48]</ref> to identify known types such as persons, dates, or organizations and use regular expressions to identify spe",0
"the training, validation, and test sets using the ratio of 3:1:1. The same split of the datasets is also used in the evaluation of other EM solutions <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b33"">34]</ref>. We list the size of each da s including Magellan <ref type=""bibr"" target=""#b24"">[25]</ref>, DeepER <ref type=""bibr"" target=""#b13"">[14]</ref>, and follow-up works of Deep-Matcher <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b22"">23]</ref>. We also compare with variants of D without the data augmentation (D ef>). Table <ref type=""table"">5</ref>: F1 scores on the ER-Magellan EM datasets. The numbers of DeepMatcher+ (DM+) are the highest available found in <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b33"">34]</ref>     The use of a pre-trained ntation.</p><p>• DeepMatcher+: Follow-up work <ref type=""bibr"" target=""#b22"">[23]</ref> slightly outperforms Deep-Matcher in the DBLP-ACM dataset and <ref type=""bibr"" target=""#b16"">[17]</ref> achieves better F1 in the Walmart-Amazon and Amazon-Google datasets. According to <ref type=""bibr"" target=""# 4]</ref>, Magellan <ref type=""bibr"" target=""#b24"">[25]</ref>, DeepMatcher <ref type=""bibr"" target=""#b33"">[34]</ref>, and DeepMatcher's follow-up work <ref type=""bibr"" target=""#b16"">[17]</ref> and <ref type=""bibr"" target=""#b22"">[23]</ref>.</p><p>We summarize these baseline results in Table <ref type= ibr"" target=""#b24"">25]</ref>.</p><p>Recently, EM solutions used deep learning and achieved promising results <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b33"">34,</ref><ref type=""bibr"" target=""#b63"" an by taking the reported results from <ref type=""bibr"" target=""#b33"">[34]</ref> and the two follow-up works <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b16"">17]</ref>   . We also plot the score of DeepMatcher+ on the full datasets (denoted as DM+(full)) as reference. Recall t",0
"</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""6."">RELATED WORK AND DISCUSSION</head><p>EM solutions have tackled the blocking problem <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b34"">35",0
"pe=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b34"">35,</ref><ref type=""bibr"" target=""#b53"">54</ref>] and the matching problem with rules <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b14"">1",0
"reduce the amount of training data required.</p><p>• We evaluated the effectiveness of D on three benchmark datasets: the Entity Resolution benchmark <ref type=""bibr"" target=""#b25"">[26]</ref>, the Magellan dataset <ref type=""bibr"" target=""#b24"">[25]</ref>, and the WDC product matching dataset <ref t p://www.tei-c.org/ns/1.0""><head n=""4."">EXPERIMENTS</head><p>We present the experiment results on benchmark datasets for EM: the ER Benchmark datasets <ref type=""bibr"" target=""#b25"">[26]</ref>, the Magellan datasets <ref type=""bibr"" target=""#b24"">[25]</ref> and the WDC product data corpus <ref type="" licly available datasets used for evaluating DeepMatcher <ref type=""bibr"" target=""#b33"">[34]</ref>. These datasets are from the ER Benchmark datasets <ref type=""bibr"" target=""#b25"">[26]</ref> and the Magellan data repository <ref type=""bibr"" target=""#b11"">[12]</ref>. We summarize the datasets in Tab",0
"beddings as described in <ref type=""bibr"" target=""#b13"">[14]</ref> and with the same hyper-parameters (a learning rate of 0.01 and the Adam optimizer <ref type=""bibr"" target=""#b23"">[24]</ref>). We then evaluate DeepER in our evaluation settings. For each dataset, we report the best results obtained",0
"sing the ratio of 3:1:1. The same split of the datasets is also used in the evaluation of other EM solutions <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b33"">34]</ref>. We list the size of each dataset in Table <ref type=""table"">5</ref>. scores on the ER-Magellan EM datasets. The numbers of DeepMatcher+ (DM+) are the highest available found in <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b33"">34]</ref>     The use of a pre-trained LM contributes to a large portion of the ntly, EM solutions used deep learning and achieved promising results <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b33"">34,</ref><ref type=""bibr"" target=""#b63"">64]</ref>. DeepER <ref type=""bibr"" targ ctive learning is a recent trend in EM to train high-quality matching models with limited labeling resources <ref type=""bibr"" target=""#b18"">[19,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b29"">30,</ref><ref type=""bibr"" target=""#b39"">40]</ref>. Under the active learning fr get=""#b24"">[25]</ref>, DeepER <ref type=""bibr"" target=""#b13"">[14]</ref>, and follow-up works of Deep-Matcher <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b22"">23]</ref>. We also compare with variants of D without the data augmentation (DA) and/or domain knowledge (DK) optimizat =""#b38"">[39]</ref> for the WDC datasets. We also reproduced those results using the open-sourced implementation.</p><p>• DeepMatcher+: Follow-up work <ref type=""bibr"" target=""#b22"">[23]</ref> slightly outperforms Deep-Matcher in the DBLP-ACM dataset and <ref type=""bibr"" target=""#b16"">[17]</ref> achi 24"">[25]</ref>, DeepMatcher <ref type=""bibr"" target=""#b33"">[34]</ref>, and DeepMatcher's follow-up work <ref type=""bibr"" target=""#b16"">[17]</ref> and <ref type=""bibr"" target=""#b22"">[23]</ref>.</p><p>We summarize these baseline results in Table <ref type=""table"" target=""#tab_12"">10</ref> on the ER-Ma thers: We obtained the results for Magellan by taking the reported results from <ref type=""bibr"" target=""#b33"">[34]</ref> and the two follow-up works <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b16"">17]</ref>   . We also plot the score of DeepMatcher+ on the full datasets (den",0
"10"">[11,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b46"">47,</ref><ref type=""bibr"" target=""#b52"">53]</ref>, crowdsourcing <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b51"">52]</ref>, or machine learning <ref ty >52]</ref>, or machine learning <ref type=""bibr"" target=""#b45"">[46,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b24"">25]</ref>.</p><p>Recently, EM solutions used deep learning and achieved promisi",0
"ype=""bibr"" target=""#b0"">[1]</ref><ref type=""bibr"" target=""#b1"">[2]</ref><ref type=""bibr"" target=""#b2"">[3]</ref><ref type=""bibr"" target=""#b3"">[4]</ref><ref type=""bibr"" target=""#b4"">[5]</ref><ref type=""bibr"" target=""#b5"">[6]</ref><ref type=""bibr"" target=""#b6"">[7]</ref> processes speech input directly rocesses speech input directly into intent without going through an intermediate text transcript. There are many advantages of end-to-end SLU systems <ref type=""bibr"" target=""#b4"">[5]</ref>, the most significant of which is that E2E systems can directly optimize the end goal of intent recognition, w r learning approach whereby the model is gradually trained on increasingly relevant data until it is fine-tuned on the actual domain data. Similarly, <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b11"">12]</ref> advocate pre-training an ASR model on a large amount of transcribed sp",1
"ain an end-to-end speech-to-intent model, we need intent-labeled speech data, and such data is usually scarce. <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b10"">11]</ref> address this problem using a curriculum and transfer learning approach whereby the model is gradually trained",1
", an end-to-end (E2E) SLU system <ref type=""bibr"" target=""#b0"">[1]</ref><ref type=""bibr"" target=""#b1"">[2]</ref><ref type=""bibr"" target=""#b2"">[3]</ref><ref type=""bibr"" target=""#b3"">[4]</ref><ref type=""bibr"" target=""#b4"">[5]</ref><ref type=""bibr"" target=""#b5"">[6]</ref><ref type=""bibr"" target=""#b6"">[7]",0
"tead of specific in-domain intent data, which is usually limited.</p><p>In our work, we use a phone-based connectionist temporal classification (CTC) <ref type=""bibr"" target=""#b12"">[13]</ref> acoustic model (AM) trained on general speech data as the base ASR system. First, we initialize the S2I mode",0
"wed by a natural language understanding (NLU) system that interprets the meaning, or intent, of the text. In contrast, an end-to-end (E2E) SLU system <ref type=""bibr"" target=""#b0"">[1]</ref><ref type=""bibr"" target=""#b1"">[2]</ref><ref type=""bibr"" target=""#b2"">[3]</ref><ref type=""bibr"" target=""#b3"">[4]",0
"same spirit as learning a shared representation between modalities <ref type=""bibr"" target=""#b15"">[16]</ref><ref type=""bibr"" target=""#b16"">[17]</ref><ref type=""bibr"" target=""#b17"">[18]</ref><ref type=""bibr"" target=""#b18"">[19]</ref>. We employ the following steps to train the final model. (A) T2I mo",0
"with a limited amount of S2I data, in the same spirit as learning a shared representation between modalities <ref type=""bibr"" target=""#b15"">[16]</ref><ref type=""bibr"" target=""#b16"">[17]</ref><ref type=""bibr"" target=""#b17"">[18]</ref><ref type=""bibr"" target=""#b18"">[19]</ref>. We employ the following s",0
"ype=""bibr"" target=""#b1"">[2]</ref><ref type=""bibr"" target=""#b2"">[3]</ref><ref type=""bibr"" target=""#b3"">[4]</ref><ref type=""bibr"" target=""#b4"">[5]</ref><ref type=""bibr"" target=""#b5"">[6]</ref><ref type=""bibr"" target=""#b6"">[7]</ref> processes speech input directly into intent without going through an in ared to end-to-end SLU systems, cascaded systems are modular and each component can be optimized separately or jointly (also with end-to-end criteria <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b8"">9]</ref>). One key advantage of modular com",0
"n=""3.1."">Leveraging pre-trained text embedding</head><p>Leveraging text embedding (TE) from models pre-trained on large amounts of data, such as BERT <ref type=""bibr"" target=""#b13"">[14]</ref> and GPT-2 <ref type=""bibr"" target=""#b14"">[15]</ref>, has recently improved performance in a number of NLU ta >. We employ the following steps to train the final model. (A) T2I model pre-training: As in the standard process outlined in the original BERT paper <ref type=""bibr"" target=""#b13"">[14]</ref>, we first fine-tune BERT on the available text-to-intent data using a masked language model (LM) task as the layer.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.2.2."">BERT based T2I model</head><p>We start with pre-trained BERTbase model of <ref type=""bibr"" target=""#b13"">[14]</ref>. Using the implementation introduced in <ref type=""bibr"" target=""#b23"">[24]</ref>, we first pre-train using",0
"nt can be optimized separately or jointly (also with end-to-end criteria <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b8"">9]</ref>). One key advantage of modular components is that each component can be trained on data that may be more abunda",0
"ion. This makes automatic speech recognition with the goal of humancomputer interaction popular, and it has been a research hotspot in recent decades <ref type=""bibr"" target=""#b0"">[1]</ref>. Automatic Speech Recognition (ASR) refers to the task of an automatic conversion from speech to text by compu",1
"mula><p>= [0.0015, . . . , 0.0034, 0.934, 0.0019, . . . , 0.003] (6)</p><p>Feature-space Maximum Likelihood Linear Regression (FMLLR) was explored in <ref type=""bibr"" target=""#b31"">[32]</ref>, <ref type=""bibr"" target=""#b32"">[33]</ref> for speaker adaptive training and it is a feature space transform",1
") scores of some wrong sequences too high, which may impact the ability of language model to find good solutions and to recover from errors.</p><p>In <ref type=""bibr"" target=""#b39"">[40]</ref>, the authors consider a simple technique of adding time-dependent Gaussian noise to the gradient at every tr into the local minima during training. However, adding Gaussian noise to the gradient can not solve the problem of over-confidence.</p><p>Inspired by <ref type=""bibr"" target=""#b39"">[40]</ref>, we investigated a new label encoding method named ''Soft One-hot Label (SOL)''. It is a regularization mech",1
"full use of the data in the non-target domain to train a better initial model. It has shown promising results in many tasks such as image recognition <ref type=""bibr"" target=""#b9"">[10]</ref>, speech recognition <ref type=""bibr"" target=""#b10"">[11]</ref>, etc. Unsupervised pre-training also uses addit",0
"ssing (NLP) <ref type=""bibr"" target=""#b12"">[13]</ref>, <ref type=""bibr"" target=""#b13"">[14]</ref> and so on <ref type=""bibr"" target=""#b14"">[15]</ref>- <ref type=""bibr"" target=""#b16"">[17]</ref>.</p><p>Data augmentation <ref type=""bibr"" target=""#b17"">[18]</ref>- <ref type=""bibr"" target=""#b19"">[20]</ref",0
"er initial model. It has shown promising results in many tasks such as image recognition <ref type=""bibr"" target=""#b9"">[10]</ref>, speech recognition <ref type=""bibr"" target=""#b10"">[11]</ref>, etc. Unsupervised pre-training also uses additional data to train a better initial model, but unlike transf",0
"nique, not only in speech recognition but also in other fields such as image recognition <ref type=""bibr"" target=""#b24"">[25]</ref> and keyword search <ref type=""bibr"" target=""#b25"">[26]</ref>, <ref type=""bibr"" target=""#b26"">[27]</ref>. However, these methods are equivalent to adding training data, a",0
"the method of changing speed has the lowest implementation cost and achieve stateof-the-art performance <ref type=""bibr"" target=""#b22"">[23]</ref>. In <ref type=""bibr"" target=""#b23"">[24]</ref>, A new method called SpecAugment is proposed and it consists of warping the features, masking blocks of freq",0
"4.3% was observed across the 4 tasks. As far, the method of changing speed has the lowest implementation cost and achieve stateof-the-art performance <ref type=""bibr"" target=""#b22"">[23]</ref>. In <ref type=""bibr"" target=""#b23"">[24]</ref>, A new method called SpecAugment is proposed and it consists o",0
"ibr"" target=""#b13"">[14]</ref> and so on <ref type=""bibr"" target=""#b14"">[15]</ref>- <ref type=""bibr"" target=""#b16"">[17]</ref>.</p><p>Data augmentation <ref type=""bibr"" target=""#b17"">[18]</ref>- <ref type=""bibr"" target=""#b19"">[20]</ref> has been proposed for the purpose of studying low-resource langua",0
"t=""#b43"">[44]</ref>, Natural Language Processing (NLP) <ref type=""bibr"" target=""#b12"">[13]</ref>, <ref type=""bibr"" target=""#b13"">[14]</ref> and so on <ref type=""bibr"" target=""#b14"">[15]</ref>- <ref type=""bibr"" target=""#b16"">[17]</ref>.</p><p>Data augmentation <ref type=""bibr"" target=""#b17"">[18]</ref",0
"T</head><p>Our experiments are conducted on a ∼10 hours training set consisting of 3000 Mandarin utterances. The training set is a subset of THCHS-30 <ref type=""bibr"" target=""#b35"">[36]</ref>, the dev set and test set are the same as those of THCHS-30. THCHS-30 involves more than 30 hours of speech",0
"proving the ASR under the condition of low resource has become a research hotspot because the acquisition of labeled speech data is usually difficult <ref type=""bibr"" target=""#b7"">[8]</ref>. A common problem in lowresource environments is that the lack of training data often leads to overfitting of",0
"ed into two types, one is the Neural Network Hidden Markov Models (NN-HMMs), and the other is the End-toend models, such as Encoder-decoder structure <ref type=""bibr"" target=""#b1"">[2]</ref>- <ref type=""bibr"" target=""#b3"">[4]</ref> and Neural Network structure with Connectionist temporal classificati",0
"r"" target=""#b5"">[6]</ref>, <ref type=""bibr"" target=""#b6"">[7]</ref>, <ref type=""bibr"" target=""#b7"">[8]</ref>, <ref type=""bibr"" target=""#b8"">[9]</ref>, <ref type=""bibr"" target=""#b9"">[10]</ref>, <ref type=""bibr"" target=""#b10"">[11]</ref>, <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" targe ef>, <ref type=""bibr"" target=""#b4"">[5]</ref>, remarkable progresses have been witnessed for object detection <ref type=""bibr"" target=""#b7"">[8]</ref>, <ref type=""bibr"" target=""#b9"">[10]</ref>, <ref type=""bibr"" target=""#b10"">[11]</ref>, <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" targe",1
"s from adjacent frames in sampling stream.</p><p>Formally, given the reference frame I t and each support frame I t+τ , Feature Pyramid Network (FPN) <ref type=""bibr"" target=""#b55"">[56]</ref> is leveraged to extract multi-scale pyramidal feature maps of each frame for motion and sampling stream, sep org/ns/1.0""><head>B. Model Architecture Design</head><p>Feature Pyramid Network. FPN is built at the top of ResNet-101 pre-trained on ImageNet. As in <ref type=""bibr"" target=""#b55"">[56]</ref>, P3, P4, Algorithm 2 Inference Algorithm of our SSVD </p><formula xml:id=""formula_7"">f t+τ →t P i = W(f t+τ",1
">, <ref type=""bibr"" target=""#b44"">[45]</ref> which capitalize on FlowNet-s <ref type=""bibr"" target=""#b58"">[59]</ref> to produce optical flow, PWC-Net <ref type=""bibr"" target=""#b59"">[60]</ref> is particularly remould in our motion stream. Compared to a generic U-Net CNN built in FlowNets, the pyramid ial scale of P3, P4, P5, and P6 is 56 2 , 28 2 , 14 2 , and 7 2 , respectively. Two-stream Feature Aggregation. For motion stream, we utilize PWC-Net <ref type=""bibr"" target=""#b59"">[60]</ref> pre-trained on Flying Chairs dataset for optical flow estimation. For sampling stream, each conv layer in of bject moves extremely fast. This is due to the fact that the receptive field in sampling stream for offset prediction is smaller than that in PWC-Net <ref type=""bibr"" target=""#b59"">[60]</ref> for optical flow generation. As such, the range of estimated motion in sampling stream is shorter than that",1
"urelevel aggregation <ref type=""bibr"" target=""#b18"">[19]</ref>, <ref type=""bibr"" target=""#b19"">[20]</ref>, <ref type=""bibr"" target=""#b20"">[21]</ref>, <ref type=""bibr"" target=""#b21"">[22]</ref>, <ref type=""bibr"" target=""#b22"">[23]</ref>, <ref type=""bibr"" target=""#b23"">[24]</ref>. The former often appl e=""bibr"" target=""#b15"">[16]</ref>, <ref type=""bibr"" target=""#b16"">[17]</ref> and feature-level aggregation <ref type=""bibr"" target=""#b18"">[19]</ref>, <ref type=""bibr"" target=""#b21"">[22]</ref>, <ref type=""bibr"" target=""#b23"">[24]</ref>. Box-level tracking employs boxlevel operations and post-processi br"" target=""#b18"">[19]</ref> performs object detection in a frame by learning to spatially sample features from adjacent frames for aggregation. STMN <ref type=""bibr"" target=""#b21"">[22]</ref> adopts spatiotemporal memory module with spatial alignment mechanism to model long-term temporal appearance 8.4 D&amp;T (τ = 1) <ref type=""bibr"" target=""#b14"">[15]</ref> ResNet-101 79.8 STSN <ref type=""bibr"" target=""#b18"">[19]</ref> ResNet-101+DCN 80.4 STMN <ref type=""bibr"" target=""#b21"">[22]</ref> ResNet-101 80.5 HQ-link <ref type=""bibr"" target=""#b43"">[44]</ref> ResNet-101 80.  consistently demonstrate t",0
"target=""#b7"">[8]</ref>, <ref type=""bibr"" target=""#b8"">[9]</ref>, <ref type=""bibr"" target=""#b9"">[10]</ref>, <ref type=""bibr"" target=""#b10"">[11]</ref>, <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" target=""#b12"">[13]</ref>, <ref type=""bibr"" target=""#b13"">[14]</ref>. Nevertheless, directl ed for object detection <ref type=""bibr"" target=""#b7"">[8]</ref>, <ref type=""bibr"" target=""#b9"">[10]</ref>, <ref type=""bibr"" target=""#b10"">[11]</ref>, <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" target=""#b12"">[13]</ref>, <ref type=""bibr"" target=""#b24"">[25]</ref>, <ref type=""bibr"" targ tiple classes and relative bounding boxes coordinates are devised as a regression problem in YOLO. As another genre of one-stage object detector, SSD <ref type=""bibr"" target=""#b11"">[12]</ref> further utilizes multiple feature maps at different scales with predefined default boxes to boost detection n the first phase. To make the model more robust and adaptive to variant changes in videos, we perform the widely-adopted data augmentation as in SSD <ref type=""bibr"" target=""#b11"">[12]</ref>. The aggregation range K is set as 12 in all experiments. As in FGFA <ref type=""bibr"" target=""#b23"">[24]</re rence frame I t and two support frames I t+τ 1 , I t+τ 2 sampled from frame {I t+τ } K τ =−K . 2: Pre-processing: Data augmentation is performed as in<ref type=""bibr"" target=""#b11"">[12]</ref>. 3: Feature Extraction:</figDesc></figure> <figure xmlns=""http://www.tei-c.org/ns/1.0"" xml:id=""fig_7""><head>",0
"t=""#b47"">[48]</ref>, <ref type=""bibr"" target=""#b48"">[49]</ref>, <ref type=""bibr"" target=""#b49"">[50]</ref>, <ref type=""bibr"" target=""#b50"">[51]</ref>, <ref type=""bibr"" target=""#b51"">[52]</ref>, <ref type=""bibr"" target=""#b52"">[53]</ref>, <ref type=""bibr"" target=""#b53"">[54]</ref>, <ref type=""bibr"" targ",0
"et=""#b9"">[10]</ref>, <ref type=""bibr"" target=""#b10"">[11]</ref>, <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" target=""#b12"">[13]</ref>, <ref type=""bibr"" target=""#b24"">[25]</ref>, <ref type=""bibr"" target=""#b25"">[26]</ref>, <ref type=""bibr"" target=""#b26"">[27]</ref>, <ref type=""bibr"" targ ive search to generate region proposals and then classifies each proposal. Later on, SPP-Net <ref type=""bibr"" target=""#b32"">[33]</ref> and Fast R-CNN <ref type=""bibr"" target=""#b24"">[25]</ref> extend <ref type=""bibr"" target=""#b25"">[26]</ref> by devising SPP pooling or ROI pooling to enable the sharin xes with Focal Loss <ref type=""bibr"" target=""#b10"">[11]</ref> (L FL ) and regress from anchor boxes to ground-truth object boxes with Smooth L 1 Loss <ref type=""bibr"" target=""#b24"">[25]</ref> (L Loc ). Accordingly, the overall objective of our SSVD is computed as</p><formula xml:id=""formula_5"">L = 1",0
"t=""#b18"">[19]</ref>, <ref type=""bibr"" target=""#b19"">[20]</ref>, <ref type=""bibr"" target=""#b20"">[21]</ref>, <ref type=""bibr"" target=""#b21"">[22]</ref>, <ref type=""bibr"" target=""#b22"">[23]</ref>, <ref type=""bibr"" target=""#b23"">[24]</ref>. The former often applies a tracker to per-frame bounding box pro f> ResNet-101 76.3 PSLA <ref type=""bibr"" target=""#b64"">[65]</ref> ResNet-101 77.1 MANet <ref type=""bibr"" target=""#b20"">[21]</ref> ResNet-101 78.1 THP <ref type=""bibr"" target=""#b22"">[23]</ref> ResNet-101+DCN 78.6 STSN <ref type=""bibr"" target=""#b18"">[19]</ref> ResNet-101+DCN 78.9 in each frame unexplo ormance with FGFA under the same backbone of Deformable Convolution Network <ref type=""bibr"" target=""#b60"">[61]</ref> (DCN). Note that as reported in <ref type=""bibr"" target=""#b22"">[23]</ref>, the performance of FGFA with DCN is 78.8%. For fair comparisons, our SSVD is evaluated based on three commo -frame baseline of two-stage video object detectors, e.g., R-FCN and Faster R-CNN. Note that here we exclude several video object detection methods ( <ref type=""bibr"" target=""#b22"">[23]</ref>, <ref type=""bibr"" target=""#b44"">[45]</ref>) which are additionally equipped with the acceleration techniques",0
"object detection in images <ref type=""bibr"" target=""#b5"">[6]</ref>, <ref type=""bibr"" target=""#b6"">[7]</ref>, <ref type=""bibr"" target=""#b7"">[8]</ref>, <ref type=""bibr"" target=""#b8"">[9]</ref>, <ref type=""bibr"" target=""#b9"">[10]</ref>, <ref type=""bibr"" target=""#b10"">[11]</ref>, <ref type=""bibr"" target=",0
"ommon solution for video object detection is box-level tracking <ref type=""bibr"" target=""#b14"">[15]</ref>, <ref type=""bibr"" target=""#b15"">[16]</ref>, <ref type=""bibr"" target=""#b16"">[17]</ref>, <ref type=""bibr"" target=""#b17"">[18]</ref> and another branch is featurelevel aggregation <ref type=""bibr"" t as proceeded along two different directions: box-level tracking <ref type=""bibr"" target=""#b14"">[15]</ref>, <ref type=""bibr"" target=""#b15"">[16]</ref>, <ref type=""bibr"" target=""#b16"">[17]</ref> and feature-level aggregation <ref type=""bibr"" target=""#b18"">[19]</ref>, <ref type=""bibr"" target=""#b21"">[22] ref> builds a temporal graph across clips and then seeks the optimal path in this graph via dynamic programming for the selection of tubelets. Later, <ref type=""bibr"" target=""#b16"">[17]</ref>, <ref type=""bibr"" target=""#b40"">[41]</ref>, <ref type=""bibr"" target=""#b41"">[42]</ref> integrate per-frame pr EQ-NMS <ref type=""bibr"" target=""#b15"">[16]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>Methods</head><p>Backbone mAP (%) TPN+LSTM <ref type=""bibr"" target=""#b16"">[17]</ref> GoogLeNet 68.4 TCNN <ref type=""bibr"" target=""#b40"">[41]</ref> DeepID+Craft 73.8 FGFA <ref type=""bibr"" target SN adopt Seq-NMS as post-processing. TPN+LSTM exploits encoder-decoder LSTM to rescore generated tubelets proposal by Tubelet Proposal Networks (TPN) <ref type=""bibr"" target=""#b16"">[17]</ref> and the performance is 68.4%. TCNN performs tubelet rescoring by motion-guided propagation to stabilize dete",0
"es similar linking algorithm as D&amp;T. In addition, the performance of our SSVD with post-processing is even comparable to the winner of ILSVRC2016 <ref type=""bibr"" target=""#b67"">[68]</ref>, which is a comprehensive detection system. Specifically, by utilizing multi-scale testing and model ensembl f type=""bibr"" target=""#b67"">[68]</ref>, which is a comprehensive detection system. Specifically, by utilizing multi-scale testing and model ensemble, <ref type=""bibr"" target=""#b67"">[68]</ref> achieves 81.2% mAP, which is similar to the result of our SSVD with ResNeXt-101-32×4d.</p></div> <div xmlns=",0
"ng the exploitation of spatio-temporal coherence for object detection in videos. One common solution for video object detection is box-level tracking <ref type=""bibr"" target=""#b14"">[15]</ref>, <ref type=""bibr"" target=""#b15"">[16]</ref>, <ref type=""bibr"" target=""#b16"">[17]</ref>, <ref type=""bibr"" targ deteriorated by motion blur or occlusion. The research on object detection in videos has proceeded along two different directions: box-level tracking <ref type=""bibr"" target=""#b14"">[15]</ref>, <ref type=""bibr"" target=""#b15"">[16]</ref>, <ref type=""bibr"" target=""#b16"">[17]</ref> and feature-level aggr target=""#b41"">[42]</ref> integrate per-frame proposals into tubelets for re-scoring, which further improves the robustness of video object detection. <ref type=""bibr"" target=""#b14"">[15]</ref> extends R-FCN with a tracking module for simultaneous detection and tracking. DorT <ref type=""bibr"" target="" g and 555 validation videos in 30 classes. As the annotations of testing videos are not publicly available, we follow the widely adopted protocols in <ref type=""bibr"" target=""#b14"">[15]</ref>, <ref type=""bibr"" target=""#b41"">[42]</ref>, <ref type=""bibr"" target=""#b23"">[24]</ref> to report the results ion (mAP). The 30 object classes in ImageNet VID dataset are a subset of 200 classes of ImageNet object detection (DET) dataset. Therefore, we follow <ref type=""bibr"" target=""#b14"">[15]</ref>, <ref type=""bibr"" target=""#b23"">[24]</ref> and train SSVD on the intersection of ImageNet VID and ImageNet D orT <ref type=""bibr"" target=""#b42"">[43]</ref> ResNet-101 73.9 Faster R-CNN <ref type=""bibr"" target=""#b12"">[13]</ref> ResNet-101 75.4 D (&amp; T loss) <ref type=""bibr"" target=""#b14"">[15]</ref> ResNet-101 75.8 FGFA <ref type=""bibr"" target=""#b23"">[24]</ref> ResNet-101 76.3 LWDN <ref type=""bibr"" target= f type=""bibr"" target=""#b12"">[13]</ref>) are two representative twostage image object detectors only exploit single frame information. D (&amp;T loss) <ref type=""bibr"" target=""#b14"">[15]</ref> exhibits better performances than the two image object detectors by exploiting temporal coherence among adja 68.4 TCNN <ref type=""bibr"" target=""#b40"">[41]</ref> DeepID+Craft 73.8 FGFA <ref type=""bibr"" target=""#b23"">[24]</ref> ResNet-101 78.4 D&amp;T (τ = 1) <ref type=""bibr"" target=""#b14"">[15]</ref> ResNet-101 79.8 STSN <ref type=""bibr"" target=""#b18"">[19]</ref> ResNet-101+DCN 80.4 STMN <ref type=""bibr"" tar",0
"the reference frame. Taking the inspiration from the temporal coherence exploration in video understanding <ref type=""bibr"" target=""#b47"">[48]</ref>, <ref type=""bibr"" target=""#b48"">[49]</ref>, <ref type=""bibr"" target=""#b49"">[50]</ref>, <ref type=""bibr"" target=""#b50"">[51]</ref>, <ref type=""bibr"" targ",0
"RIMENTS A. Dataset Sampling and Evaluation</head><p>We evaluate our SSVD on the large-scale benchmark for video object detection task, i.e., ImageNet <ref type=""bibr"" target=""#b62"">[63]</ref> object detection from video (VID) dataset, which contains 3,862 training and 555 validation videos in 30 cla",0
"r"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b2"">[3]</ref>, <ref type=""bibr"" target=""#b3"">[4]</ref>, <ref type=""bibr"" target=""#b4"">[5]</ref> have successfully achieved remarkable improvements on object detection in images <ref type=""bibr"" target=""#b5"" onal Neural Networks (CNN) <ref type=""bibr"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b2"">[3]</ref>, <ref type=""bibr"" target=""#b3"">[4]</ref>, <ref type=""bibr"" target=""#b4"">[5]</ref>, remarkable progresses have been witnessed for object detection <ref type=""bibr"" target=""#b7"">[8]</ref>, <ref",0
"have gained significant attention in research, they are unfortunately not the only weak spot in machine learning systems.</p><p>Recently, Xiao et al. <ref type=""bibr"" target=""#b34"">[35]</ref> have demonstrated that data preprocessing used in machine learning can also suffer from vulnerabilities. In these major imaging libraries.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2.2"">Image-Scaling Attacks</head><p>Recently, Xiao et al. <ref type=""bibr"" target=""#b34"">[35]</ref> have shown that scaling algorithms are vulnerable to attacks and can be misused to fool machine learning sys fixed coefficients that depend on the selected scaling algorithm. Both matrices can be computed in advance and are reusable. We refer to Xiao et al. <ref type=""bibr"" target=""#b34"">[35]</ref> for a description how to calculate L and R.</p><p>Based on this matrix multiplication, the attack can also b ch image, we consider the minimum of both for this assignment.</p><p>We implement image-scaling attacks in the strong variant proposed by Xiao et al. <ref type=""bibr"" target=""#b34"">[35]</ref>. We make a slight improvement to the original attacks: Instead of using a fixed ε value, we increase its val employs a uniform weighting of pixels and operates on rectangular blocks instead of columns and rows. As a result, the original attack by Xiao et al. <ref type=""bibr"" target=""#b34"">[35]</ref> is not applicable to this scaling algorithm. To attack area scaling, we thus propose two novel attack strate",1
"is below 15 dB. We also experimented with more advanced methods for comparing the quality of images, such as feature matching based on SIFT analysis <ref type=""bibr"" target=""#b15"">[16]</ref>. This technique, however, shows the same trends as the simple PSNR measurement, and thus we omit these measu",0
"ense has thwarted O2. Consequently, we design our experiments along with these two objectives.</p><p>Dataset &amp; Setup. We use the ImageNet dataset <ref type=""bibr"" target=""#b24"">[25]</ref> with a pre-trained VGG19 model <ref type=""bibr"" target=""#b27"">[28]</ref> for our evaluation. This deep neura",0
"xmlns=""http://www.tei-c.org/ns/1.0""><head n=""1"">Introduction</head><p>Pre-training models <ref type=""bibr"" target=""#b14"">(Radford et al., 2018;</ref><ref type=""bibr"" target=""#b4"">Devlin et al., 2019;</ref><ref type=""bibr"">Radford et al., 2019b;</ref><ref type=""bibr"" target=""#b23"">Song et al., 2019; ://www.tei-c.org/ns/1.0""><head n=""2.1"">Background</head><p>The key of pre-training methods <ref type=""bibr"" target=""#b14"">(Radford et al., 2018;</ref><ref type=""bibr"" target=""#b4"">Devlin et al., 2019;</ref><ref type=""bibr"" target=""#b23"">Song et al., 2019;</ref><ref type=""bibr"" target=""#b30"">Yang et Qin: taoqin@microsoft.com 1 https://github.com/microsoft/MPNet the accuracy of NLP tasks in the past years. One of the most successful models is BERT <ref type=""bibr"" target=""#b4"">(Devlin et al., 2019)</ref>, which mainly adopts masked language modeling (MLM) for pre-training<ref type=""foot"" target= ining to exploit large language corpora for language understanding and generation. For language understanding, masked language modeling (MLM) in BERT <ref type=""bibr"" target=""#b4"">(Devlin et al., 2019)</ref> and permuted language modeling (PLM) in XLNet <ref type=""bibr"" target=""#b30"">(Yang et al., 2 019)</ref> are two representative objectives. In this section, we briefly review MLM and PLM, and discuss their pros and cons.</p><p>MLM in BERT BERT <ref type=""bibr"" target=""#b4"">(Devlin et al., 2019)</ref> is one of the most successful pre-training models for natural language understanding. It ado and x z&gt;c = (x 4 , x 6 , x 2 ) are taken as the predicted part. For the non-predicted part (x z&lt;=c , M z&gt;c ), we use bidirectional modeling <ref type=""bibr"" target=""#b4"">(Devlin et al., 2019)</ref>  Modeling Output Dependency with Two-Stream Self-Attention For the predicted part x z&gt;c , ch pre-training objective. We assume all the three objectives mask and predict the same amount of tokens (15%), following the common practice in BERT <ref type=""bibr"" target=""#b4"">(Devlin et al., 2019)</ref> and XLNet <ref type=""bibr"" target=""#b30"">(Yang et al., 2019)</ref> <ref type=""foot"" target="" v> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.1"">Experimental Setup</head><p>We conduct experiments under the BERT base setting (BERT BASE ) <ref type=""bibr"" target=""#b4"">(Devlin et al., 2019)</ref>, where the model consists of 12 transformer layers, with 768 hidden size, 12 attention heads t_4"">7</ref> , choose the rightmost 15% tokens as the predicted tokens, and prepare mask tokens following the same 8:1:1 replacement strategy in BERT <ref type=""bibr"" target=""#b4"">(Devlin et al., 2019)</ref>. Additionally, we also apply whole word mask <ref type=""bibr"" target=""#b2"">(Cui et al., 2019 </ref> and <ref type=""bibr"">Stories (Trinh and Le, 2018)</ref>, with 160GB data size in total. We use a subword dictionary with 30K BPE codes in BERT <ref type=""bibr"" target=""#b4"">(Devlin et al., 2019)</ref> to tokenize the sentences. We limit the length of sentences in each mini-batch up to 512 tok 3"">3</ref>. All of the listed results are reported in BERT BASE setting and from MNLI QNLI QQP RTE SST MRPC CoLA STS Avg Single model on dev set BERT <ref type=""bibr"" target=""#b4"">(Devlin et al., 2019)</ref> 84.5 91.7 91.3 68.6 93.2 87.3 58.9 89.5 83.1 XLNet <ref type=""bibr"" target=""#b30"">(Yang et a 5 84.5 RoBERTa <ref type=""bibr"">(Liu et al., 2019a)</ref> 87 </p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>SQuAD v1.1 EM F1</head><p>BERT <ref type=""bibr"" target=""#b4"">(Devlin et al., 2019)</ref> 80.8 88.5 RoBERTa <ref type=""bibr"">(Liu et al., 2019a)</ref>  single model without any data ibr"">(Liu et al., 2019a)</ref>  single model without any data augmentation for fair comparisons. On the dev set of GLUE tasks, MPNet outperforms BERT <ref type=""bibr"" target=""#b4"">(Devlin et al., 2019)</ref>, XLNet <ref type=""bibr"" target=""#b30"">(Yang et al., 2019)</ref> and RoBERTa <ref type=""bibr""",1
"ype=""bibr"" target=""#b4"">Devlin et al., 2019;</ref><ref type=""bibr"">Radford et al., 2019b;</ref><ref type=""bibr"" target=""#b23"">Song et al., 2019;</ref><ref type=""bibr"" target=""#b30"">Yang et al., 2019;</ref><ref type=""bibr"" target=""#b6"">Dong et al., 2019;</ref><ref type=""bibr"">Liu et al., 2019a;</ref> get=""#b14"">(Radford et al., 2018;</ref><ref type=""bibr"" target=""#b4"">Devlin et al., 2019;</ref><ref type=""bibr"" target=""#b23"">Song et al., 2019;</ref><ref type=""bibr"" target=""#b30"">Yang et al., 2019;</ref><ref type=""bibr"" target=""#b1"">Clark et al., 2020)</ref> is the design of self-supervised tasks/ >2</ref> . MLM leverages bidirectional context of masked tokens efficiently, but ignores the dependency among the masked (and to be predicted) tokens <ref type=""bibr"" target=""#b30"">(Yang et al., 2019)</ref>.</p><p>To improve BERT, XLNet <ref type=""bibr"" target=""#b30"">(Yang et al., 2019</ref>) introd nding, masked language modeling (MLM) in BERT <ref type=""bibr"" target=""#b4"">(Devlin et al., 2019)</ref> and permuted language modeling (PLM) in XLNet <ref type=""bibr"" target=""#b30"">(Yang et al., 2019)</ref> are two representative objectives. In this section, we briefly review MLM and PLM, and discus xml:id=""formula_2"">log P (x K |x \K ; ?) ? k?K log P (x k |x \K ; ?). (1)</formula><p>PLM in XLNet Permuted language model (PLM) is proposed in XLNet <ref type=""bibr"" target=""#b30"">(Yang et al., 2019)</ref> to retain the benefits of autoregressive modeling and also allow models to capture bidirectio ns x z&gt;c (usually c = 85% * n) are chosen to predict and the remaining tokens are used as condition in order to reduce the optimization difficulty <ref type=""bibr"" target=""#b30"">(Yang et al., 2019)</ref>.</p><p>Pros and Cons of MLM and PLM We compare MLM and PLM from two perspectives: the depende are independent with each other and predicts them separately, which is not sufficient to model the complicated context dependency in natural language <ref type=""bibr"" target=""#b30"">(Yang et al., 2019)</ref>. In contrast, PLM factorizes the predicted tokens with the product rule in any permuted order d occur in any position, which makes it difficult for normal autoregressive prediction. To this end, we follow PLM to adopt two-stream self-attention <ref type=""bibr"" target=""#b30"">(Yang et al., 2019)</ref> to autoregressively predict the tokens, which is illustrated in Figure <ref type=""figure"" tar nal sequence (in the above example, n = 6) 4 . For example, when predicting token x z 5 = x 6 , the query stream in the original two-stream attention <ref type=""bibr"" target=""#b30"">(Yang et al., 2019)</ref> takes mask token M z 5 = [M ] and position p z 5 = p 6 as the attention query, and can only s nd predict the same amount of tokens (15%), following the common practice in BERT <ref type=""bibr"" target=""#b4"">(Devlin et al., 2019)</ref> and XLNet <ref type=""bibr"" target=""#b30"">(Yang et al., 2019)</ref> <ref type=""foot"" target=""#foot_2"">5</ref> . As can be seen, MLM conditions on 85% tokens and 12 attention heads as 12, and 110M model parameters in total. For the pretraining objective of MPNet, we randomly permute the sentence following PLM <ref type=""bibr"" target=""#b30"">(Yang et al., 2019)</ref>  <ref type=""foot"" target=""#foot_4"">7</ref> , choose the rightmost 15% tokens as the predicted and Ba, 2014) with ? 1 = 0.9, ? 2 = 0.98 and = 1e -6. We pre-train our model for 500K steps to be comparable with state-of-the-art models like XLNet <ref type=""bibr"" target=""#b30"">(Yang et al., 2019)</ref>, RoBERTa <ref type=""bibr"">(Liu et al., 2019a)</ref> and ELECTRA <ref type=""bibr"" target=""#b1"" oLA STS Avg Single model on dev set BERT <ref type=""bibr"" target=""#b4"">(Devlin et al., 2019)</ref> 84.5 91.7 91.3 68.6 93.2 87.3 58.9 89.5 83.1 XLNet <ref type=""bibr"" target=""#b30"">(Yang et al., 2019)</ref> 86.8 91.7 91.4 74.0 94.7 88.2 60.2 89.5 84.5 RoBERTa <ref type=""bibr"">(Liu et al., 2019a)</re mentation for fair comparisons. On the dev set of GLUE tasks, MPNet outperforms BERT <ref type=""bibr"" target=""#b4"">(Devlin et al., 2019)</ref>, XLNet <ref type=""bibr"" target=""#b30"">(Yang et al., 2019)</ref> and RoBERTa <ref type=""bibr"">(Liu et al., 2019a)</ref> by 4.6, 3.2, 1.3 points on average. On under BERT BASE setting. For RACE, the results of BERT are from the RACE leaderboard 10 and the results of XL-Net are obtained from the original paper<ref type=""bibr"" target=""#b30"">(Yang et al., 2019)</ref>. ""Middle"" and ""High"" denote the accuracy on the middle school set and high school set in RACE ibr"" target=""#b24"">Sun et al. (2019)</ref> and the result of XLNet is ran by ourselves with only PLM pre-training objective but no long context memory<ref type=""bibr"" target=""#b30"">(Yang et al., 2019)</ref>. ""*"" represents pre-training only on Wikipedia and BooksCorpus (16GB size).</figDesc><table / s the dependency among the masked (and to be predicted) tokens <ref type=""bibr"" target=""#b30"">(Yang et al., 2019)</ref>.</p><p>To improve BERT, XLNet <ref type=""bibr"" target=""#b30"">(Yang et al., 2019</ref>) introduces permuted language modeling (PLM) for pre-training to capture the dependency among thus alleviates the position discrepancy of XLNet.</p><p>We pre-train MPNet on a large-scale text corpora (over 160GB data) following the practice in <ref type=""bibr"" target=""#b30"">Yang et al. (2019)</ref>; <ref type=""bibr"">Liu et al. (2019a)</ref>, and fine-tune on a variety of down-streaming bench kens and positions, as shown in Figure <ref type=""figure"" target=""#fig_2"">2a</ref>. For more details about two-stream self-attention, please refer to <ref type=""bibr"" target=""#b30"">Yang et al. (2019)</ref>. One drawback of two-stream self-attention in PLM is that it can only see the the previous Tab During fine-tuning, we do not use query stream in two-stream self-attention and use the original hiddens to extract context representations following <ref type=""bibr"" target=""#b30"">Yang et al. (2019)</ref>. The fine-tuning experiments on each downstream tasks are conducted 5 times and the median val 2018)</ref> <ref type=""foot"" target=""#foot_5"">8</ref> in our model pretraining since these tricks have been successfully validated in previous works <ref type=""bibr"" target=""#b30"">(Yang et al., 2019;</ref><ref type=""bibr"">Raffel et al., 2019b)</ref>.</p><p>For pre-training corpus, we follow the dat <note xmlns=""http://www.tei-c.org/ns/1.0"" place=""foot"" n=""2"" xml:id=""foot_0""><p>We do not consider next sentence prediction here since previous works<ref type=""bibr"" target=""#b30"">(Yang et al., 2019;</ref> Liu et al., 2019a;<ref type=""bibr"" target=""#b7"">Joshi et al., 2019)</ref> have achieved good",1
"</div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.2"">Results on GLUE Benchmark</head><p>The General Language Understanding Evaluation (GLUE) <ref type=""bibr"" target=""#b27"">(Wang et al., 2019)</ref> is a collection of 9 natural language understanding tasks, which include two single-sentence",0
"#b22"">(Socher et al., 2013)</ref>), three similarity and paraphrase tasks (MRPC <ref type=""bibr"" target=""#b5"">(Dolan and Brockett, 2005)</ref>, STS-B <ref type=""bibr"" target=""#b0"">(Cer et al., 2017)</ref>, QQP), four inference tasks (MNLI <ref type=""bibr"" target=""#b29"">(Williams et al., 2018)</ref>,",0
"nce tasks (MNLI <ref type=""bibr"" target=""#b29"">(Williams et al., 2018)</ref>, QNLI <ref type=""bibr"" target=""#b20"">(Rajpurkar et al., 2016)</ref>, RTE <ref type=""bibr"" target=""#b3"">(Dagan et al., 2006)</ref>, WNLI <ref type=""bibr"" target=""#b10"">(Levesque et al., 2012)</ref>). We follow RoBERTa hyper-",0
"><p>For pre-training corpus, we follow the data used in RoBERTa <ref type=""bibr"">(Liu et al., 2019a)</ref>, which includes: Wikipedia and BooksCorpus <ref type=""bibr"" target=""#b31"">(Zhu et al., 2015)</ref>, OpenWebText <ref type=""bibr"">(Radford et al., 2019a)</ref>, CC-News <ref type=""bibr"">(Liu et",0
"=""bibr"" target=""#b40"">[41,</ref><ref type=""bibr"" target=""#b38"">39,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b22"">23]</ref>.</p><p>Query-based black-box attacks can settle the susceptible direction of the victim model as per the resp et=""#b20"">[21,</ref><ref type=""bibr"" target=""#b31"">32]</ref> or images <ref type=""bibr"" target=""#b38"">[39,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b22"">23]</ref>. More related to our work is the regularization-based approach: transferable adversarial perturbation (TAP) i",1
"ith off-the-shelf local models (i.e., source models) and directly harness the resultant example to fool the remote target model (i.e., victim models) <ref type=""bibr"" target=""#b40"">[41,</ref><ref type=""bibr"" target=""#b7"">8]</ref>.</p><p>Among these two sorts of black-box attacks, the transfer-based ype=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b9"">10]</ref>, and the other one is transfer-based <ref type=""bibr"" target=""#b40"">[41,</ref><ref type=""bibr"" target=""#b38"">39,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b20""> ation term based on our preliminary experiments. For fair comparisons, we adopt default parameters as recommended in benchmark approaches and Foolbox <ref type=""bibr"" target=""#b40"">[41,</ref><ref type=""bibr"" target=""#b27"">28]</ref>. The random noise is sampled from a clipped normal distribution with are prone to overfit to the exclusive blind spots of the source model <ref type=""bibr"" target=""#b38"">[39,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b40"">41,</ref><ref type=""bibr"" target=""#b6"">7]</ref>. Specifically, although the crafted adversarial samples can attack Ince br"" target=""#b22"">23]</ref>. More related to our work is the regularization-based approach: transferable adversarial perturbation (TAP) introduced by <ref type=""bibr"" target=""#b40"">[41]</ref>. TAP injects two regularization terms into the vanilla training loss function of the model to guide the sear ased black-box attacks <ref type=""bibr"" target=""#b19"">[20,</ref><ref type=""bibr"" target=""#b3"">4]</ref>. We follow the protocol of the baseline method <ref type=""bibr"" target=""#b40"">[41]</ref> to curate experimental datasets and target models for fair comparisons.</p><p>Dataset. We need two sorts of ttacks. Since the original C&amp;W implementation cannot strictly meet the l ∞ budget, we employ the modified l ∞ version of C&amp;W as introduced by <ref type=""bibr"" target=""#b40"">[41]</ref>, which can explicitly satisfy the l ∞ norm constraint. Similar to our strategy, TAP <ref type=""bibr"" target= &amp;W as introduced by <ref type=""bibr"" target=""#b40"">[41]</ref>, which can explicitly satisfy the l ∞ norm constraint. Similar to our strategy, TAP <ref type=""bibr"" target=""#b40"">[41]</ref> boosts adversarial transferability through two regularization terms and is the state-of-the-art approach und </ref><ref type=""bibr"" target=""#b27"">28]</ref>. The random noise is sampled from a clipped normal distribution with mean 0 and variance 1.  Following <ref type=""bibr"" target=""#b40"">[41]</ref>, we fix the perturbation budget ǫ to 16 for all methods. We conduct grid search on the development dataset t r method outperforms TAP in almost all cases. We next attack models defended by adversarial training. For fair comparisons with the baseline approach <ref type=""bibr"" target=""#b40"">[41]</ref>, we stick to employing undefended models as local source models. Therefore, we explore a more challenging bl ped by <ref type=""bibr"" target=""#b7"">[8]</ref>, and the other one is the regularization-based transferable adversarial perturbation (TAP) proposed by <ref type=""bibr"" target=""#b40"">[41]</ref>. With the integrated attacks, we conduct experiments similar to Section 5.2. Specifically, the combination o",1
">. One is query-based, and the other one is transfer-based. Query-based black-box attacks usually require excessive queries before a successful trial <ref type=""bibr"" target=""#b15"">[16]</ref>. On the contrary, without the feedback information from the target model, transfer-based black-box attacks d br"" target=""#b40"">41,</ref><ref type=""bibr"" target=""#b6"">7]</ref>. Specifically, although the crafted adversarial samples can attack Inception V3 VGG <ref type=""bibr"" target=""#b15"">16</ref> ResNet V2</p><p>Figure <ref type=""figure"">1</ref>: The attention heatmaps of three representative models (VGG",0
". There are roughly two sorts of black-box attacks according to the mechanism they adopt. One is query-based <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b9"">10]</ref>, and the other one is transfer-based <ref type=""bibr"" target=""#b40"">[41, mate the loss gradient of the target model through training a local replica <ref type=""bibr"" target=""#b23"">[24]</ref> or finite difference techniques <ref type=""bibr"" target=""#b1"">[2]</ref>. However, such attacks usually require excessive queries before a successful trial and thus have limited appli",0
"o diminish malicious noises <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b0"">1]</ref>, defensive distillation to mask gradients <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b5"">6]</ref>, and feature squeezing to detect adversarial samples <ref type=""bibr""",0
"e attracted increasing attention recently. There are roughly two sorts of black-box attacks according to the mechanism they adopt. One is query-based <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b9"">10]</ref>, and the other one is transfer- type=""bibr"" target=""#b9"">[10]</ref>. Alternatively, attackers can approximate the loss gradient of the target model through training a local replica <ref type=""bibr"" target=""#b23"">[24]</ref> or finite difference techniques <ref type=""bibr"" target=""#b1"">[2]</ref>. However, such attacks usually requi",0
"pe=""bibr"" target=""#b17"">[18]</ref>. Projected gradient descent (PGD) extends BIM with random start to diversify the synthesized adversarial instances <ref type=""bibr"" target=""#b21"">[22]</ref>. Carlini and Wagner attacks (C&amp;W) devise a novel attack object to absorb the perturbation budget constra et=""#b36"">[37,</ref><ref type=""bibr"" target=""#b18"">19]</ref>, since adversarial training is arguably the most promising and effective defense to date <ref type=""bibr"" target=""#b21"">[22]</ref>. These adversarially trained models include adversarially trained Inception V3 (Adv-Inc-v3), adversarially t ft deceptive images for their model and augment the clean training data with such instances to train the model <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b19"">20]</ref>. Moreover, exploiting the malicious examples tailored for diverse hol",0
"rb the perturbation budget constraint <ref type=""bibr"" target=""#b4"">[5]</ref>, which also admits the employment of sophisticated optimizers like Adam <ref type=""bibr"" target=""#b16"">[17]</ref> during the search for deceptive noises. Jacobian-based Saliency Map Attack (JSMA) <ref type=""bibr"" target=""#",0
"signed features, deep learning-based image classifiers are renowned for their competence to automatically extract discriminative features from images <ref type=""bibr"" target=""#b14"">[15]</ref>. We can thus separate a DNN image classifier into two parts: a hierarchical feature extraction module and a",0
"""#b25"">[26,</ref><ref type=""bibr"" target=""#b5"">6]</ref>, and feature squeezing to detect adversarial samples <ref type=""bibr"" target=""#b39"">[40,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b37"">38]</ref>. Adversarial training arguably remains the most effective and promisi",0
"se pioneering works, many CNN-based methods have been proposed and achieved state-of-the-art results in SISR <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b39"">40,</ref><ref type=""bibr"" target=""#b37"">38,</ref><ref type=""bibr"" target=""#b2""> efficient sub-pixel layer, many very deep networks have been proposed for a better performance. Lim et al. proposed a very deep and wide network EDSR <ref type=""bibr"" target=""#b17"">[18]</ref> by stacking modified residual blocks in which the batch normalization (BN) layers are removed. Ledig et al. g is also introduced in image SR to further boost the performance. Fig. <ref type=""figure"">3</ref>(Left) depicts a basic residual module used in EDSR <ref type=""bibr"" target=""#b17"">[18]</ref> and ESRGAN <ref type=""bibr"" target=""#b30"">[31]</ref>. The residual modules are often stacked together to for bination with Residual Block</head><p>In this section, we investigate the combination of our RFA framework with the basic residual block used in EDSR <ref type=""bibr"" target=""#b17"">[18]</ref>. Different from the original residual block used in image classification, EDSR removes the Batch Normalizati R <ref type=""bibr"" target=""#b12"">[13]</ref>, LapSRN <ref type=""bibr"" target=""#b14"">[15]</ref>, MemNet <ref type=""bibr"" target=""#b24"">[25]</ref>, EDSR <ref type=""bibr"" target=""#b17"">[18]</ref>, SRMD <ref type=""bibr"" target=""#b35"">[36]</ref>, NLRN <ref type=""bibr"" target=""#b18"">[19]</ref>, DBPN <ref t ctivation ranges of the bottom row are smaller than the top row, which can ease the training difficulty to some extent (e.g. residual scaling in EDSR <ref type=""bibr"" target=""#b17"">[18]</ref>).</p><p>(2) Feature maps after the attention mechanism tend to contain more negative values, showing a stron ><p>Many recent SR networks have similar network architectures. Here we introduce one of the basic architecture used by some state-of-the-art methods <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b39"">40,</ref><ref type=""bibr"" target=""#b37"">38,</ref><ref type=""bibr"" target=""#b2""",1
"17"">[18]</ref> by stacking modified residual blocks in which the batch normalization (BN) layers are removed. Ledig et al. introduced the SRResNet in <ref type=""bibr"" target=""#b15"">[16]</ref> and are further improved in <ref type=""bibr"" target=""#b30"">[31]</ref> by introducing the dense connections.",0
"works</head><p>Attention mechanism are widely used in recent computer vision tasks, such as image captioning <ref type=""bibr"" target=""#b31"">[32,</ref><ref type=""bibr"" target=""#b1"">2]</ref>, image and video classification <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b29"">30]</ref>",0
"""#b39"">[40,</ref><ref type=""bibr"" target=""#b37"">38,</ref><ref type=""bibr"" target=""#b2"">3]</ref>, we use 800 highresolution training images from DIV2K <ref type=""bibr"" target=""#b25"">[26]</ref> dataset as training set. During training, data augmentation is performed by randomly rotating 90 • , 180 • ,",0
"> and DRCN <ref type=""bibr"" target=""#b13"">[14]</ref> with 20 layers based on residual learning. Later, Tai et al. introduced recursive blocks in DRRN <ref type=""bibr"" target=""#b23"">[24]</ref> and memory blocks in MemNet <ref type=""bibr"" target=""#b24"">[25]</ref>. These methods extract features from t",0
"finding external similar patches. It yields great limitations of deep SISR. SISR performance was boosted right after the non-local attention modules <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" target=""#b37"">38]</ref> were proposed. They explored n et al. <ref type=""bibr"" target=""#b18"">[19]</ref>   <ref type=""bibr"" target=""#b19"">[20]</ref>, RNAN <ref type=""bibr"" target=""#b36"">[37]</ref> and SAN <ref type=""bibr"" target=""#b1"">[2]</ref>, incorporate non-local operation into their networks in order to make better use of image structural cues, by al pathway connecting the convolutional features to the fusion structure. For the IS-NL branch, it contains a non-local attention module adopted from <ref type=""bibr"" target=""#b1"">[2]</ref> and a deconvolution layer for upscaling the module outputs. The IS-NL module is region-based in this paper. As type=""bibr"" target=""#b1"">[2]</ref> and a deconvolution layer for upscaling the module outputs. The IS-NL module is region-based in this paper. As in <ref type=""bibr"" target=""#b1"">[2]</ref>, we divide the feature maps into region grids, where the inter-dependencies are captured independently in each RN <ref type=""bibr"" target=""#b19"">[20]</ref>, SRFBN <ref type=""bibr"" target=""#b17"">[18]</ref>, OISR <ref type=""bibr"" target=""#b11"">[12]</ref> and SAN <ref type=""bibr"" target=""#b1"">[2]</ref>.</p><p>Quantitative Evaluations In Table <ref type=""table"" target=""#tab_1"">1</ref>, We report the quantitative /ref> OISR <ref type=""bibr"" target=""#b11"">[12]</ref> RDN <ref type=""bibr"" target=""#b38"">[39]</ref> RCAN <ref type=""bibr"" target=""#b36"">[37]</ref> SAN <ref type=""bibr"" target=""#b1"">[2]</ref> Ours Urban100 (4×): img 078 HR Bicubic LapSRN <ref type=""bibr"" target=""#b16"">[17]</ref> EDSR <ref type=""bibr"" /ref> OISR <ref type=""bibr"" target=""#b11"">[12]</ref> RDN <ref type=""bibr"" target=""#b38"">[39]</ref> RCAN <ref type=""bibr"" target=""#b36"">[37]</ref> SAN <ref type=""bibr"" target=""#b1"">[2]</ref> Ours Urban100 (4×): img 047 HR Bicubic LapSRN <ref type=""bibr"" target=""#b16"">[17]</ref> EDSR <ref type=""bibr"" /ref> OISR <ref type=""bibr"" target=""#b11"">[12]</ref> RDN <ref type=""bibr"" target=""#b38"">[39]</ref> RCAN <ref type=""bibr"" target=""#b36"">[37]</ref> SAN <ref type=""bibr"" target=""#b1"">[2]</ref> Ours which only needs 20% parameters of RCAN and SAN, but achieves the second best result. Therefore, our CSNL",1
"r self-similarity can be further extended to cross-scale cues. It has been verified that cross-scale patch similarity widely exists in natural images <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b41"">42]</ref>. Intuitively, in addition to non-local pixel-to-pixel matching, pixels lf-Similarity in Image SR The fact that small patches tend to recur within and across scale of a same image has been verified for most natural images <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b41"">42]</ref>. Since then, a category of self-similarity based approaches has been e nique source for reconstruction without relying on any external examples <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b25"">2 bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b30"">31]</ref>. In the pioneering work, Glasner et al. <ref type=""bibr"" target=""#b8"">[9]</ref> proposed to jointly exploit repeating patches within and across image scales by integrating the idea of multip",0
"te-of-the-arts</head><p>To verify the effectiveness of the proposed model, we compare our approach with 11 state-of-the-art methods, which are LapSRN <ref type=""bibr"" target=""#b16"">[17]</ref>, SRMDNF <ref type=""bibr"" target=""#b35"">[36]</ref>, MemNet <ref type=""bibr"" target=""#b26"">[27]</ref>, EDSR <r "">[39]</ref> RCAN <ref type=""bibr"" target=""#b36"">[37]</ref> SAN <ref type=""bibr"" target=""#b1"">[2]</ref> Ours Urban100 (4×): img 078 HR Bicubic LapSRN <ref type=""bibr"" target=""#b16"">[17]</ref> EDSR <ref type=""bibr"" target=""#b18"">[19]</ref> DBPN <ref type=""bibr"" target=""#b10"">[11]</ref> OISR <ref type "">[39]</ref> RCAN <ref type=""bibr"" target=""#b36"">[37]</ref> SAN <ref type=""bibr"" target=""#b1"">[2]</ref> Ours Urban100 (4×): img 047 HR Bicubic LapSRN <ref type=""bibr"" target=""#b16"">[17]</ref> EDSR <ref type=""bibr"" target=""#b18"">[19]</ref> DBPN <ref type=""bibr"" target=""#b10"">[11]</ref> OISR <ref type",0
"bypassing the common knowledge, thus improves the discriminative ability of the network.</p><p>Motivated by the traditional Image SR and recent DBPN <ref type=""bibr"" target=""#b10"">[11]</ref>, we adopt the back-projection approach to incorporate local information to regularize the feature and correc </ref>, we adopt the back-projection approach to incorporate local information to regularize the feature and correct reconstruction errors. Following <ref type=""bibr"" target=""#b10"">[11]</ref>, the final fused feature H is computed by,</p><formula xml:id=""formula_6"">e = F L − downsample(F IC ),<label DNF <ref type=""bibr"" target=""#b35"">[36]</ref>, MemNet <ref type=""bibr"" target=""#b26"">[27]</ref>, EDSR <ref type=""bibr"" target=""#b18"">[19]</ref>, DBPN <ref type=""bibr"" target=""#b10"">[11]</ref>, RDN <ref type=""bibr"" target=""#b38"">[39]</ref>, RCAN <ref type=""bibr"" target=""#b36"">[37]</ref>, NLRN <ref ty e shown examples, our method perceptually out-performs other state-of-the-arts by a large margin. EDSR <ref type=""bibr"" target=""#b18"">[19]</ref> DBPN <ref type=""bibr"" target=""#b10"">[11]</ref> OISR <ref type=""bibr"" target=""#b11"">[12]</ref> RDN <ref type=""bibr"" target=""#b38"">[39]</ref> RCAN <ref type= [2]</ref> Ours Urban100 (4×): img 078 HR Bicubic LapSRN <ref type=""bibr"" target=""#b16"">[17]</ref> EDSR <ref type=""bibr"" target=""#b18"">[19]</ref> DBPN <ref type=""bibr"" target=""#b10"">[11]</ref> OISR <ref type=""bibr"" target=""#b11"">[12]</ref> RDN <ref type=""bibr"" target=""#b38"">[39]</ref> RCAN <ref type= [2]</ref> Ours Urban100 (4×): img 047 HR Bicubic LapSRN <ref type=""bibr"" target=""#b16"">[17]</ref> EDSR <ref type=""bibr"" target=""#b18"">[19]</ref> DBPN <ref type=""bibr"" target=""#b10"">[11]</ref> OISR <ref type=""bibr"" target=""#b11"">[12]</ref> RDN <ref type=""bibr"" target=""#b38"">[39]</ref> RCAN <ref type=",0
"ype=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b30"">31]</ref>. In the pioneering work, Glasner et al. <ref type=""bibr"" target=""#b8"" ormations. The idea of internal data repetition has also been applied to solve SR with blur and noisy images <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b25"">26]</ref>.</p><p>Deep CNNs for Image SR The first work that introduced CNN to solve image SR was proposed by <ref type=",0
"vel. In each step of our process the input is an initial design space and the output is a refined design space of simpler or better models. Following <ref type=""bibr"" target=""#b20"">[21]</ref>, we characterize the quality of a design space by sampling models and inspecting their error distribution. F ). The overall process is analogous to manual design, elevated to the population level and guided via distribution estimates of network design spaces <ref type=""bibr"" target=""#b20"">[21]</ref>.</p><p>As a testbed for this paradigm, our focus is on exploring network structure (e.g., width, depth, grou t number of possible network design spaces, it is essential to use a reliable comparison metric to guide our design process. Recently, the authors of <ref type=""bibr"" target=""#b20"">[21]</ref> proposed a methodology for comparing and analyzing populations of networks sampled from a design space. This ettings (unlike a single model tuned for a specific scenario).</p><p>We rely on the concept of network design spaces introduced by Radosavovic et al. <ref type=""bibr"" target=""#b20"">[21]</ref>. A design space is a large, possibly infinite, population of model architectures. The core insight from <ref al. <ref type=""bibr"" target=""#b20"">[21]</ref>. A design space is a large, possibly infinite, population of model architectures. The core insight from <ref type=""bibr"" target=""#b20"">[21]</ref> is that we can sample models from a design space, giving rise to a model distribution, and turn to tools fro >We begin with an overview of tools for design space design. To evaluate and compare design spaces, we use the tools introduced by Radosavovic et al. <ref type=""bibr"" target=""#b20"">[21]</ref>, who propose to quantify the quality of a design space by sampling a set of models from that design space an epochs is roughly equivalent in flops to training a single ResNet-50 <ref type=""bibr"" target=""#b7"">[8]</ref> model at 4GF for 100 epochs.</p><p>As in <ref type=""bibr"" target=""#b20"">[21]</ref>, our primary tool for analyzing design space quality is the error empirical distribution function (EDF). The research.     </p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>Appendix C: Optimization Settings</head><p>Our basic training settings follow <ref type=""bibr"" target=""#b20"">[21]</ref> as discussed in §3. To tune the learning rate lr and weight decay wd for REG-NET models, we perform a study, evel. In each step of our process the input is an initial design space and the output is a refined design space of simpler or better models. Following<ref type=""bibr"" target=""#b20"">[21]</ref>, we characterize the quality of a design space by sampling models and inspecting their error distribution. F X</cell></row></table></figure> 			<note xmlns=""http://www.tei-c.org/ns/1.0"" place=""foot"" n=""1"" xml:id=""foot_0"">We use the term design space following<ref type=""bibr"" target=""#b20"">[21]</ref>, rather than search space, to emphasize that we are not searching for network instances within the space. In ost likely best value.</note> 			<note xmlns=""http://www.tei-c.org/ns/1.0"" place=""foot"" n=""5"" xml:id=""foot_4"">Our training setup in §3 exactly follows<ref type=""bibr"" target=""#b20"">[21]</ref>. We use SGD with momentum of 0.9, mini-batch size of 128 on 1 GPU, and a half-period cosine schedule with in",1
"edule with no regularization except weight decay, while most mobile networks use longer schedules with various enhancements, such as deep supervision <ref type=""bibr"" target=""#b15"">[16]</ref>, Cutout <ref type=""bibr"" target=""#b3"">[4]</ref>, DropPath <ref type=""bibr"" target=""#b13"">[14]</ref>, AutoAug",0
"type=""bibr"" target=""#b15"">[16]</ref>, Cutout <ref type=""bibr"" target=""#b3"">[4]</ref>, DropPath <ref type=""bibr"" target=""#b13"">[14]</ref>, AutoAugment <ref type=""bibr"" target=""#b1"">[2]</ref>, and so on. As such, we hope our strong results obtained with a short training schedule without enhancements c ves results slightly inferior to original results (bottom row), which additionally used RM-SProp<ref type=""bibr"" target=""#b29"">[30]</ref>, AutoAugment<ref type=""bibr"" target=""#b1"">[2]</ref>, etc. Without these enhancements to the training setup results are ∼2% lower (top row), highlighting the impor",0
"of visual recognition tasks. Examples include LeNet <ref type=""bibr"" target=""#b14"">[15]</ref>, AlexNet <ref type=""bibr"" target=""#b12"">[13]</ref>, VGG <ref type=""bibr"" target=""#b25"">[26]</ref>, and ResNet <ref type=""bibr"" target=""#b7"">[8]</ref>. This body of work advanced both the effectiveness of ne tbed for this paradigm, our focus is on exploring network structure (e.g., width, depth, groups, etc.) assuming standard model families including VGG <ref type=""bibr"" target=""#b25"">[26]</ref>, ResNet <ref type=""bibr"" target=""#b7"">[8]</ref>, and ResNeXt <ref type=""bibr"" target=""#b30"">[31]</ref>. We s ]</ref> catapulted network design into a thriving research area. In the following years, improved network designs were proposed; examples include VGG <ref type=""bibr"" target=""#b25"">[26]</ref>, Inception <ref type=""bibr"" target=""#b26"">[27,</ref><ref type=""bibr"" target=""#b27"">28]</ref>, ResNet <ref ty ace design process. Parameterization. Our final quantized linear parameterization shares similarity with previous work, e.g. how stage widths are set <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b31"">32,</ref><ref type=""bibr"" target=""#b10"">",0
"ng settings and flops, REGNET models outperform EFFICIENTNET models while being up to 5× faster on GPUs. We further test generalization on ImageNetV2 <ref type=""bibr"" target=""#b23"">[24]</ref>.</p><p>We note that network structure is arguably the simplest form of a design space design one can conside per we perform all experiments on the Im-ageNet <ref type=""bibr"" target=""#b2"">[3]</ref> validation set. Here we evaluate our models on the ImageNetV2 <ref type=""bibr"" target=""#b23"">[24]</ref> test set (original test set unavailable).</p><p>Evaluation setup. To study generalization of models develope >[24]</ref> test set (original test set unavailable).</p><p>Evaluation setup. To study generalization of models developed on ImageNet, the authors of <ref type=""bibr"" target=""#b23"">[24]</ref> collect a new test set following the original procedure (ImageNetV2). They find that the overall model ranks",0
"the immunity of quantized DNN to such malicious bit-flips in <ref type=""bibr"" target=""#b7"">[8]</ref>. However, a newly proposed Bit-Flip Attack (BFA) <ref type=""bibr"" target=""#b16"">[17]</ref> whose progressive bit searching algorithm can successfully identify and flip an extremely small number of vu org/ns/1.0""><head n=""2.1."">Bit-Flip based Adversarial Weight Attack</head><p>The bit-flip based adversarial weight attack, aka. Bit-Flip Attack (BFA) <ref type=""bibr"" target=""#b16"">[17]</ref>, is an adversarial attack variant which performs weight fault injection through flipping the bits. For the m intra-layer search through directly checking the loss increment. Thus, the bit searching in iteration i can be formulated as an optimization process <ref type=""bibr"" target=""#b16"">[17]</ref>:</p><formula xml:id=""formula_0"">max { Bi l } L f x; { Bi l } L l=1 , t s.t. t = f (x; {B l } L l=1 ); L l=1 ly plausible but also practically necessary for the acceleration of modern AI applications. To clarify, we use the same threat model as in prior work <ref type=""bibr"" target=""#b16"">[17]</ref>, which is listed in Table <ref type=""table"">1</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><hea ights, and cause large weight shift.</p><p>As depicted in Fig. <ref type=""figure"" target=""#fig_2"">2</ref>, the progressive bit search proposed in BFA <ref type=""bibr"" target=""#b16"">[17]</ref> is prone to identify vulnerable bit in the weight whose absolute value has a small magnitude (i.e., |w| → 0) ""#b15"">[16]</ref>, running on NVIDIA Titan-XP GPUs.   BFA Configuration. To evaluate the effectiveness of the proposed defense methods, the code from <ref type=""bibr"" target=""#b16"">[17]</ref> is utilized with further modification. The number of bit-flips N BF that degrades the prediction accuracy be uration and report the mean±std of N BF with 5 BFA trials. Note that, all the quantized DNN reported hereafter still uses the uniform quantizer as in <ref type=""bibr"" target=""#b16"">[17]</ref>, but with quantization-aware training instead of post-training quantization.</p></div> <div xmlns=""http://ww check.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""5.3."">Comparison of Alternative Defense Methods</head><p>Adversarial weight attack <ref type=""bibr"" target=""#b16"">[17]</ref> is a recently developed security threat model for modern DNN. Subsequently, the development of defensive app by attacking a strong input defense known as PGD Trained adversarial defense <ref type=""bibr"" target=""#b14"">[15]</ref> with strong weight attack BFA <ref type=""bibr"" target=""#b16"">[17]</ref>. Again, adversarial input defense fails to defend BFA, requiring even less number of N BF than the baseline",1
"oating-point representation can be easily hacked to fully malfunction, through single bit-flip (e.g. in an exponential bit of any weight) through RHA <ref type=""bibr"" target=""#b7"">[8]</ref>. Thanks to the DNN weight quantization technique, DNN is more compact since the weights are represented in a f nstrained representation. Such a representation has been proven to significantly enhance the immunity of quantized DNN to such malicious bit-flips in <ref type=""bibr"" target=""#b7"">[8]</ref>. However, a newly proposed Bit-Flip Attack (BFA) <ref type=""bibr"" target=""#b16"">[17]</ref> whose progressive b",0
"g/ns/1.0""><head n=""5.1."">Experiment Setup</head><p>Dataset and Network Architectures In this work, experiments are focused on visual dataset CIFAR-10 <ref type=""bibr"" target=""#b11"">[12]</ref>, which includes 60k 32 × 32 RGB images evenly sampled from 10 categories, with 50k and 10k samples for train",0
"e α is the hyper-parameter to balance the accuracy of the trained model on clean natural data and adversarial examples. t is the soft-label as in Eq. <ref type=""bibr"" target=""#b0"">(1)</ref>. Such adversarial training is also normally considered as a strong regularization technique.</p><p>Increasing ype=""bibr"" target=""#b2"">3]</ref>. Such pruning techniques involve the stochastic process during the inference which suffers from gradient obfuscation <ref type=""bibr"" target=""#b0"">[1]</ref> which is a common reason for the failure of adversarial input defenses. Nevertheless, we investigate the effec",0
"an extreme low bit-width model compression technique, which converts the weights from 32-bit floating-point to {-1,+1} binary format encoded by 1-bit <ref type=""bibr"" target=""#b17"">[18]</ref>. Here, the binarizationaware training is leveraged as a defense technique against BFA, which can be mathemat",0
"own in Figure <ref type=""figure"">1</ref>. Our proposed approach is more practical because the training data is generally inaccessible to the attacker <ref type=""bibr"" target=""#b31"">[32]</ref>. Our contributions can be summarized as follows:</p><p>• We propose to treat the DNN logits as a vector for ioned universal attacks require utilization of the original training data. However, in practice the attacker often has no access to the training data <ref type=""bibr"" target=""#b31"">[32]</ref>. To overcome this limitation, Mopuri et al. propose to generate universal perturbation without training data ref type=""bibr"" target=""#b31"">[32]</ref>. To overcome this limitation, Mopuri et al. propose to generate universal perturbation without training data <ref type=""bibr"" target=""#b31"">[32]</ref>. However, their approach is specifically designed for non-targeted attacks by maximizing the activation scor r"" target=""#b4"">5]</ref> and universal (i.e. image-agnostic) attacks <ref type=""bibr"" target=""#b26"">[27,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b31"">32,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b34"">35,</ref><ref type=""bibr"" target=""#b45""",1
"ication <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b47"">48]</ref> to motion regression <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b46"">47]</ref>. However, DNNs are also known to be vulnerable to adversarial attacks <ref type=""bibr"" target=""#b41"">[42,</re",0
"=""1."">Introduction</head><p>Deep neural networks (DNNs) have shown impressive performance in numerous applications, ranging from image classification <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b47"">48]</ref> to motion regression <ref type=""bibr"" target=""#b7"">[8,</ref><ref typ b0"">[1]</ref>. While the working mecha- nism of DNNs is not fully understood, one widely accepted interpretation considers DNNs as feature extractors <ref type=""bibr"" target=""#b15"">[16]</ref>, which inspires the recent work <ref type=""bibr"" target=""#b16"">[17]</ref> to link the existence of adversari",0
"it values contribute to the feature representation and therefore treat them as a 1 logit vector. We utilize the Pearson correlation coefficient (PCC) <ref type=""bibr"" target=""#b1"">[2]</ref> to analyze the extent of linear correlation between logit vectors. The PCC values computed between the logit v xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.2."">Pearson Correlation Coefficient</head><p>In statistics, the Pearson correlation coefficient (PCC) <ref type=""bibr"" target=""#b1"">[2]</ref> is a widely adopted metric to measure the linear correlation between two variables. In general, this coefficie",0
"learning rate of 0.005 with batch-size 32. As the proxy datasets, we use images from MS-COCO <ref type=""bibr"" target=""#b22"">[23]</ref> and Pascal VOC <ref type=""bibr"" target=""#b8"">[9]</ref>, two widely used object detection datasets, and Places365 <ref type=""bibr"" target=""#b49"">[50]</ref>, a large-s",0
"Networks (GNNs) <ref type=""bibr"" target=""#b18"">[19]</ref> have exhibited tremendous progress in representation learning for generic graphs (GraphSAGE <ref type=""bibr"" target=""#b5"">[6]</ref>, AS-GCN <ref type=""bibr"" target=""#b7"">[8]</ref>). In general, GNNs recursively update each node's feature by a r are inefficient in scalability.</p><p>In terms of computational inefficiency, scaling up is difficult. Although sampling methods, such as GraphSAGE <ref type=""bibr"" target=""#b5"">[6]</ref>, AS-GCN <ref type=""bibr"" target=""#b7"">[8]</ref> and FastGCN <ref type=""bibr"" target=""#b0"">[1]</ref>), have bee ervised representation learning baselines: Node2Vec <ref type=""bibr"" target=""#b4"">[5]</ref>, VGAE <ref type=""bibr"" target=""#b9"">[10]</ref>, GraphSAGE <ref type=""bibr"" target=""#b5"">[6]</ref>, and AS-GCN <ref type=""bibr"" target=""#b7"">[8]</ref>. We use a large-scale bipartite graph dataset from the Ten label></formula><p>𝑣 = 𝑋 𝑣 are actually input features).</p><p>As we can see from Eq.7, there are two distinctions between IDMP and conventional GCNs <ref type=""bibr"" target=""#b5"">[6]</ref>: 1. IDMP only performs aggregation on each node's neighbor nodes without involving the node itself, while conv trollable neighborhood expansion of each node layer by layer leads to a low computational speed and high memory cost. Sampling methods like GraphSAGE <ref type=""bibr"" target=""#b5"">[6]</ref> and AS-GCN <ref type=""bibr"" target=""#b7"">[8]</ref> have been proposed to deal with this issue by reducing the tp://www.tei-c.org/ns/1.0""><head n=""4.3"">Evaluation Results</head><p>We evaluated our BGNN results on a classification downstream task with 𝐹 1 score <ref type=""bibr"" target=""#b5"">[6]</ref> which is a popular metric for classification. For binary classification on the Tencent dataset, we report 𝐹 1 ) merely captures the one-hop topology structure of 𝐵 𝑢 (resp. 𝐵 𝑣 ) as well as feature information from 𝑋 𝑢 and 𝑋 𝑣 . As presented in previous works <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b10"">11]</ref>, the onehop aggregation does not sufficiently characterize diverse gra is used as an encoder and a simple inner product as a decoder to embed the nodes into a lowdimensional feature space. • GraphSAGE-MEAN, GraphSAGE-GCN <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b10"">11]</ref>: We implement two types of aggregator functions: GCN and MEAN aggregat entire graph. However, these methods are task-specific: in another word, they require labels in downstream tasks to supervise the models. Some works <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b13"">14]",1
"19]</ref> have exhibited tremendous progress in representation learning for generic graphs (GraphSAGE <ref type=""bibr"" target=""#b5"">[6]</ref>, AS-GCN <ref type=""bibr"" target=""#b7"">[8]</ref>). In general, GNNs recursively update each node's feature by aggregating its neighbors through message passing of computational inefficiency, scaling up is difficult. Although sampling methods, such as GraphSAGE <ref type=""bibr"" target=""#b5"">[6]</ref>, AS-GCN <ref type=""bibr"" target=""#b7"">[8]</ref> and FastGCN <ref type=""bibr"" target=""#b0"">[1]</ref>), have been proposed to deal with the scalability issue (u <ref type=""bibr"" target=""#b4"">[5]</ref>, VGAE <ref type=""bibr"" target=""#b9"">[10]</ref>, GraphSAGE <ref type=""bibr"" target=""#b5"">[6]</ref>, and AS-GCN <ref type=""bibr"" target=""#b7"">[8]</ref>. We use a large-scale bipartite graph dataset from the Tencent Platform and also construct three synthesized d by layer leads to a low computational speed and high memory cost. Sampling methods like GraphSAGE <ref type=""bibr"" target=""#b5"">[6]</ref> and AS-GCN <ref type=""bibr"" target=""#b7"">[8]</ref> have been proposed to deal with this issue by reducing the number of neighborhood nodes in each layer. Compare /ref>: We implement two types of aggregator functions: GCN and MEAN aggregator. Node-wise sampling is used to address the scalability issue. • AS-GCN <ref type=""bibr"" target=""#b7"">[8]</ref>: This method uses adaptive sampling between each layer to deal with node explosion in large-scale graphs. Sinc",1
"e an intra-domain alignment technique to minimize the divergence between raw features and the inter-domain representation by using adversarial models <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b15"">16]</ref>, a tool that has been applied successfully for distribution matching.< esign two types of alignment losses to align 𝐻 𝑣→𝑢 with 𝐻 𝑢 . Our first alignment employs adversarial learning <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b3"">4]</ref>. A discriminator is trained to discriminate between vectors randomly sampled from 𝐻 𝑣→𝑢 and 𝐻 𝑢 . Conversely, I",0
"p>Graph representation learning methods typically can be classified into two groups, supervised and unsupervised learning.</p><p>Unsupervised methods <ref type=""bibr"" target=""#b11"">[12]</ref> traditionally use the adjacency matrix to define and measure the similarity among nodes for graph embedding,",0
"ult. Although sampling methods, such as GraphSAGE <ref type=""bibr"" target=""#b5"">[6]</ref>, AS-GCN <ref type=""bibr"" target=""#b7"">[8]</ref> and FastGCN <ref type=""bibr"" target=""#b0"">[1]</ref>), have been proposed to deal with the scalability issue (uncontrollable neighborhood expansion across layers),",0
"rove the performance of downstream tasks <ref type=""bibr"" target=""#b24"">[25]</ref>. Early classical works include random walk-based methods (DeepWalk <ref type=""bibr"" target=""#b16"">[17]</ref>, Node2Vec <ref type=""bibr"" target=""#b4"">[5]</ref>), where only graph topology and node relations are embedde h is referred to as matrix factorization. Other works explore using random walks on graphs to learn representation with the skip-gram model. DeepWalk <ref type=""bibr"" target=""#b16"">[17]</ref> and Node2vec <ref type=""bibr"" target=""#b4"">[5]</ref> are typically representative of these methods to model",0
"o minimize the divergence between raw features and the inter-domain representation by using adversarial models <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b15"">16]</ref>, a tool that has been applied successfully for distribution matching.</p><p>Self-Supervised. Most notably, BG tation on the domain 𝐻 𝑢 , which is in the form of achieving a lower loss bounded by 𝐿 𝑀,𝑣→𝑢 on the downstream classification model 𝑀 in equation Eq. <ref type=""bibr"" target=""#b15"">16</ref>. The closer loss of these two domains produces a similar embedding space, which captures information from both",0
"the performance of our algorithm with several unsupervised representation learning baselines: Node2Vec <ref type=""bibr"" target=""#b4"">[5]</ref>, VGAE <ref type=""bibr"" target=""#b9"">[10]</ref>, GraphSAGE <ref type=""bibr"" target=""#b5"">[6]</ref>, and AS-GCN <ref type=""bibr"" target=""#b7"">[8]</ref>. We us ing biased random walks on the graph. We run Node2Vec on the bipartite graph and then concatenate the node embeddings with their own features. • VGAE <ref type=""bibr"" target=""#b9"">[10]</ref>: This method is based on a variational autoencoder, where GCN is used as an encoder and a simple inner produc y require labels in downstream tasks to supervise the models. Some works <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b13"">14]</ref> try to utilize GCN to do unsupervised learning on graphs by performing",0
"e represented by users and products, and an edge from a member from one partition to a member of the other represents the user purchasing the product <ref type=""bibr"" target=""#b12"">[13]</ref>. The ability to utilize information from the graphical structure, such as node features in the two distinct",0
"head><p>Graphs have been used to capture and represent complex structural relationships among data items in various domains, including drug discovery <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b26"">27]</ref>, social networks analysis <ref type=""bibr"" target=""#b17"">[18,</ref><re",0
"d n=""1"">INTRODUCTION</head><p>Speech synthesis (text to speech, TTS) <ref type=""bibr"" target=""#b27"">[28,</ref><ref type=""bibr"" target=""#b29"">30,</ref><ref type=""bibr"" target=""#b34"">35,</ref><ref type=""bibr"" target=""#b40"">41]</ref> and speech recognition (automatic speech recognition, ASR) <ref type= h-resource setting, both TTS <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b29"">30,</ref><ref type=""bibr"" target=""#b34"">35]</ref> and ASR <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target f> .</p><p>Data Proprocessing. For the speech data, we re-sample it to 16kHZ and convert the raw waveform into mel-spectrograms following Shen et al. <ref type=""bibr"" target=""#b34"">[35]</ref> with 50ms frame size, 12.5ms hop size. For the text, we use text normalization rules to convert the irregula",1
">30,</ref><ref type=""bibr"" target=""#b34"">35,</ref><ref type=""bibr"" target=""#b40"">41]</ref> and speech recognition (automatic speech recognition, ASR) <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b10"">11]</ref> are two key tasks in speech dom get=""#b21"">[22,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b29"">30,</ref><ref type=""bibr"" target=""#b34"">35]</ref> and ASR <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b10"">11]</ref> require a large amount of paire s=""http://www.tei-c.org/ns/1.0""><head n=""2.2"">Formulation of TTS and ASR</head><p>TTS and ASR are usually formulated as sequence to sequence problems <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b40"">41]</ref>. Denote the text and speech sequence pair (x, y) ∈ D, where D is the p",0
"target=""#b22"">23,</ref><ref type=""bibr"" target=""#b30"">31]</ref> while the multi-speaker low-quality paired data is reduced to dozens of hours in ASR <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b31"">32,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" target=""#b38",0
"ch and text data can be leveraged. • In the low-resource setting, the single-speaker high-quality paired data are reduced to dozens of minutes in TTS <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b30"">",0
"bel>(2)</label></formula><p>TTS and ASR models can be developed based on an encoder-attentiondecoder framework <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b39"">40]</ref>, where the encoder transforms the source sequence into a set of hidde",0
"e 50% accuracy on web queries <ref type=""bibr"" target=""#b40"">[41]</ref>. Using such noisy query entities in ranking often requires manual annotations <ref type=""bibr"" target=""#b11"">[12]</ref> or soft linking/diversification <ref type=""bibr"" target=""#b41"">[42]</ref>. Personalization provides a natura ument as a kind of relevance ranking features, such as term weight in queries according to entity descriptions <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b11"">12]</ref>. There are also some researches using entities as connections between the documents and queries for better ma",1
"""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b39"">40,</ref><ref type=""bibr"" target=""#b41"">42,</ref><ref type=""bibr"" target=""#b42"">43]</ref>.</p><p>Both personalized search and entity-oriented search leverage i ly reinforce each other. One key challenge of entity-oriented search is the difficulty of query entity linking. Queries are often short and ambiguous <ref type=""bibr"" target=""#b41"">[42]</ref>, making query entity linking a challenging task: A recent study shows that state-of-the-art entity linking t sing such noisy query entities in ranking often requires manual annotations <ref type=""bibr"" target=""#b11"">[12]</ref> or soft linking/diversification <ref type=""bibr"" target=""#b41"">[42]</ref>. Personalization provides a natural way to help resolve the ambiguity in query entity linking: For example, onstruct entity enhanced user profiles, using a memory network that represents user's search preferences in the word-entity duet representation space <ref type=""bibr"" target=""#b41"">[42]</ref>. KEPS then conducts personalized ranking to adapt document ranking to satisfy user's information need, using ntity representations in search model, and the interaction between bags of word representations and bags of entity representations is also studied in <ref type=""bibr"" target=""#b41"">[42]</ref>. Neuralbased search model EDRM <ref type=""bibr"" target=""#b20"">[21]</ref> study the interaction between word",1
"n <ref type=""bibr"" target=""#b0"">[1]</ref>.</p><p>We use the entity titles in Wikipedia. The candidate entities for queries are collected using DEXTER <ref type=""bibr"" target=""#b9"">[10]</ref> (note that entity linking for queries are done in Sec. 3.2). Since there is no entity annotation in documents",0
"using click features to improve personalized ranking effect. Other works <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b28"">29,</ref><ref type=""bibr"" target=""#b33""",0
"lainable, and incorporate external knowledge that is hard to capture by the word-based or embedding-based representations used in personalized search <ref type=""bibr"" target=""#b1"">[2]</ref>. In Fig. <ref type=""figure"" target=""#fig_1"">1</ref>, entities such as Sakura, Tokyo, Japan, and Travel may fre",0
"Recently, the context search dataset constructed by Wasi et al. <ref type=""bibr"" target=""#b0"">[1]</ref>, which is based on the public AOL search log <ref type=""bibr"" target=""#b25"">[26]</ref>, makes it possible to study personalized search in the public domain. We conduct experiments on this dataset",0
", we can retrieve related search history and obtain user's preference from the corresponding clicked documents. Here we use key-value memory networks <ref type=""bibr"" target=""#b23"">[24]</ref> to store user's history. Further, as stated in Sec 1, entities in user's search history can be utilized to m",0
"ized ranking effect. Other works <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b28"">29,</ref><ref type=""bibr"" target=""#b33"">34,</ref><ref type=""bibr"" target=""#b34""",0
"es of features using learning to rank.</p><p>Recently, deep learning has been applied in personalized search <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b29""",0
"ref> proposed SLTB to combine the two types of features using learning to rank.</p><p>Recently, deep learning has been applied in personalized search <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b24 because history in the current session tends to reflect user's session search intent, while the previous history may reflect user's global interests <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b21"">22]</ref>.</p><p>Suppose a query ? has ? entity mentions (text string in query d kernel functions in PEDRM are consistent with EDRM <ref type=""bibr"" target=""#b20"">[21]</ref>.</p><p>Evaluation Metrics. Following the previous work <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b21"">22]</ref>, we use MAP, MRR, P@K (precision in the top k positions) and AR (ave ref type=""figure"">5b</ref>, we still have that the effect of PEDRM is similar on both types of queries. However we find that different from stated in <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b21"">22]</ref>, the personalization models perform better on queries with entropy l because we count the improvement on MAP over the original ranking based on BM25, which is less efficient than the ranking from search engine used in <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b21"">22]</ref>. Compared with SLTB, we can still see HRNN, HRNN-Entity and PSGAN ha still see HRNN, HRNN-Entity and PSGAN have more improvement on queries with click entropy no less than 1, which is consistent with the previous works <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b21"">22]</ref>. KEPS has significant improvement over the baselines on both two typ proved personalization by learning the effective representations of user profiles and other personalized features from user's history. Ge et al. HRNN <ref type=""bibr"" target=""#b15"">[16]</ref> proposed to use a hierarchical RNN to model user's profile. PSGAN <ref type=""bibr"" target=""#b21"">[22]</ref> in the current session as short-term history ? ? , while that in previous sessions before the current session as long-term history ? ? following HRNN <ref type=""bibr"" target=""#b15"">[16]</ref>:</p><formula xml:id=""formula_1"">? ? = [? ? 1 , ..., ? ? |? ? | ] = [? ? , ...? ? ? -1 ], ? ? = [? ? 1 , ..., s and topic features, which is the state-of-art personalization model using traditional features; and the model based on deep learning: HRNN (HRNN+QA <ref type=""bibr"" target=""#b15"">[16]</ref>) using hierarchical RNN and PSGAN (we choose the document-selection based model <ref type=""bibr"" target=""#b2 zed effect.</p><p>Another challenge in personalized search is that many search logs are not publicly available <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b33""",0
"based on probabilistic models, following the long-lasting tradition started by social scientists. Some others <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b29"">29]</ref> noticed the advantages of neu om social science, probabilistic models have been widely used for such kinds of analysis since the early 1980s <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b28"">28]</ref>. On the other hand, on social network datasets, it is quite intuitive of links.</p><p>Existing works can not address the above challenges well. Even though some realized the importance of links <ref type=""bibr"">[9,</ref><ref type=""bibr"" target=""#b12"">13]</ref>, they failed to provide an embedding. Most people learn an embedding by separating the heterogeneous graph in /ref><ref type=""bibr"" target=""#b15"">[16]</ref><ref type=""bibr"" target=""#b16"">[17]</ref>, only a few paid attention to links <ref type=""bibr"">[9,</ref><ref type=""bibr"" target=""#b12"">13]</ref>. Our work differs from them all, since: (1) unlike probabilistic models, we use GNN approaches to solve this ype=""bibr"" target=""#b20"">20]</ref>, and directly collected such as from news articles <ref type=""bibr"" target=""#b1"">[2]</ref> or from social networks <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b16"">17]</ref>. Some studies take advantage",1
"arget=""#b25"">[25]</ref>.</p><p>Most existing approaches of ideology detection on social networks focus on text <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b14"">[15]</ref><ref type=""bibr"" target=""#b15"">[16]</ref><ref type=""bibr"" target=""#b16"" network datasets, it is quite intuitive trying to extract information from text data to do ideology-detection <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b14"">[15]</ref><ref type=""bibr"" target=""#b15"">[16]</ref><ref type=""bibr"" target=""#b16""",0
"=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b14"">[15]</ref><ref type=""bibr"" target=""#b15"">[16]</ref><ref type=""bibr"" target=""#b16"">[17]</ref>. Most of their methodologies based on probabilistic models, following the long-lasting tradition started by =""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b14"">[15]</ref><ref type=""bibr"" target=""#b15"">[16]</ref><ref type=""bibr"" target=""#b16"">[17]</ref>, only a few paid attention to links <ref type=""bibr"">[9,</ref><ref type=""bibr"" target=""#b12"">13]</ref>. Our g the long-lasting tradition started by social scientists. Some others <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b29"">29]</ref> noticed the advantages of neural networks, but seldom do they focus o cles <ref type=""bibr"" target=""#b1"">[2]</ref> or from social networks <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b16"">17]</ref>. Some studies take advantages from both sides, asking self-reported responses from a group of users selected",0
"[17]</ref>. Most of their methodologies based on probabilistic models, following the long-lasting tradition started by social scientists. Some others <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b29""> "" target=""#b23"">[23]</ref>. Emerging from social science, probabilistic models have been widely used for such kinds of analysis since the early 1980s <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b28"">28]</ref>. On the other hand, on social tegies like survey <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b20"">20]</ref>, and directly collected such as from news articles <ref type=""bibr"" target=""#b1"">[2]</ref> or from social networks <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref",0
"have naturally. Social networks, in return, has shaped people's habits, giving rise to opinion leaders, encouraging youngsters' political involvement <ref type=""bibr"" target=""#b25"">[25]</ref>.</p><p>Most existing approaches of ideology detection on social networks focus on text <ref type=""bibr"" targ",0
"of users selected from social networks <ref type=""bibr"" target=""#b29"">[29]</ref>, and some researchers admitted the limitations of survey experiments <ref type=""bibr"" target=""#b23"">[23]</ref>. Emerging from social science, probabilistic models have been widely used for such kinds of analysis since t",0
"-types |R| ends up in overwhelming parameters, thus they put some constraints on the weight matrices, referred to as weight-matrix decomposition. GEM <ref type=""bibr"" target=""#b22"">[22]</ref> is almost a special case of r-GCN. Unfortunately, their code is kept confidential. According to the descript >[40]</ref> are way too different from our approach at a very fundamental level, thus are not considered as baselines. Some other methods such as GEM <ref type=""bibr"" target=""#b22"">[22]</ref> and SHINE <ref type=""bibr"" target=""#b38"">[38]</ref> should be capable of handling the dataset at this scale,",0
"optimizing an MTL problem is to solve it by joint-training fashion, with joint loss computed as a weighted combination of losses from different tasks <ref type=""bibr"" target=""#b17"">[18]</ref>. It has a very wide range of applications, such as the DMT-Demographic Models <ref type=""bibr"" target=""#b37""",0
"n social networks focus on text <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b14"">[15]</ref><ref type=""bibr"" target=""#b15"">[16]</ref><ref type=""bibr"" target=""#b16"">[17]</ref>. Most of their methodologies based on probabilistic models, followi t data to do ideology-detection <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b14"">[15]</ref><ref type=""bibr"" target=""#b15"">[16]</ref><ref type=""bibr"" target=""#b16"">[17]</ref>, only a few paid attention to links <ref type=""bibr"">[9,</ref><ref",0
"audio, convolutional representations such as scattering transforms or certain CNN architectures have been shown to be stable to spatial deformations <ref type=""bibr"" target=""#b31"">[32,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b37"">38]</ref>. However the notion of deforma or bounded away from zero. In Section 4, we analyze the stability of GCNs to small deformation of the underlying random graph model. Similar to CNNs <ref type=""bibr"" target=""#b31"">[32,</ref><ref type=""bibr"" target=""#b3"">4]</ref>, studying GCNs in the continuous world allows us to define intuitive n bibr"" target=""#b31"">[32]</ref>. We note that studies of stability are often balanced by discussions on how the representation preserves signal (e.g., <ref type=""bibr"" target=""#b31"">[32,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b16"">17]</ref>). In our context, the empirica ith random models of large graphs allows us to define intuitive notions of deformations and stability in the continuous world like the Euclidean case <ref type=""bibr"" target=""#b31"">[32,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b37"">38]</ref>, with direct applications in c , fully explicit bounds with relaxed hypotheses.</p><p>Related work on stability. The study of stability to deformations has been pioneered by Mallat <ref type=""bibr"" target=""#b31"">[32]</ref> in the context of the scattering transform for signals on Euclidean domains such as images or audio signals perturbations based on deformations of random graph models and to obtain deformation stability bounds that are similar to those on Euclidean domains <ref type=""bibr"" target=""#b31"">[32]</ref>. We note that <ref type=""bibr"" target=""#b28"">[29]</ref> also considers GCN representations with continuous g GCNs to model deformations</head><p>Stability to deformations is an essential feature for the generalization properties of deep architectures. Mallat <ref type=""bibr"" target=""#b31"">[32]</ref> studied the stability to small deformations of the wavelet-based scattering transform, which was extended to ; 1, we have q τ (x) = det(I − ∇τ (x)) −1 ; then, for small enough ∇τ ∞ , we obtain N P (τ ) d ∇τ ∞ , recovering the more standard quantity of Mallat <ref type=""bibr"" target=""#b31"">[32]</ref>. In this case, we also have the bound</p><formula xml:id=""formula_37"">C Pτ 2 d if we assume ∇τ ∞ 1/2.</formu 2) and obtain a dependence on both ∇τ ∞ and N P (τ ). Once again we focus on invariant c-GCNs with pooling, similar to classical scattering transform <ref type=""bibr"" target=""#b31"">[32]</ref>.</p><p>Proposition 4 (Signal deformation). Consider a GCN representation Φ with no bias and a random graph Γ ebesgue measure, since N P (τ ) is controlled by ∇τ ∞ , the GCN is invariant to translations and stable to deformations, similar to Euclidean domains <ref type=""bibr"" target=""#b31"">[32]</ref>. We note that studies of stability are often balanced by discussions on how the representation preserves sig",1
"s that are less intuitive for capturing natural changes in structure <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b46"">47]</ref>.</p><p>In statistics and machine learning, there is a long history of modelling large graphs with random mode perturbations and metrics <ref type=""bibr"" target=""#b15"">[16]</ref><ref type=""bibr"" target=""#b16"">[17]</ref><ref type=""bibr"" target=""#b17"">[18]</ref><ref type=""bibr"" target=""#b46"">47]</ref>, which may however have limited interpretability without an underlying model. In contrast, our continuous set",0
"n discrete graphs, and most stability studies for GCNs use purely discrete metrics that are less intuitive for capturing natural changes in structure <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b46"">47]</ref>.</p><p>In statistics and mac an ∇τ , such as ∇τ ∞ = sup x ∇τ (x) . As we have seen in the introduction, it is not clear how to extend the notion of deformation on discrete graphs <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b17"">18]</ref>. We show here that it can be done in the continuous world. We first d approach in the study of stability (and graph theory in general) has been to define a metric that minimizes over permutations σ of the nodes (e.g., <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b17"">18]</ref>), thus we define MSE Σ (Z, Z ′ ) def.</p></div> <div xmlns=""http://w stability properties of GCNs or scattering representations on discrete graphs, by considering certain well-chosen discrete perturbations and metrics <ref type=""bibr"" target=""#b15"">[16]</ref><ref type=""bibr"" target=""#b16"">[17]</ref><ref type=""bibr"" target=""#b17"">[18]</ref><ref type=""bibr"" target=""#b r parameters, or more explicit stability bounds may be obtained when the (c-)GCN is a structured architecture like the scattering transform on graphs <ref type=""bibr"" target=""#b15"">[16]</ref>. Convergence results can also be obtained for many other models of random graphs like k-Nearest Neighbor gra",0
"f> in the context of the scattering transform for signals on Euclidean domains such as images or audio signals <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b1"">2]</ref>, and was later extended to more generic CNN architectures <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bib",0
"""><head n=""1"">Introduction</head><p>Graph Convolutional Networks (GCNs <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b24"">25]</ref>) are deep architectures defined on graphs inspired by classical Convolutional Neural Networks (CNNs <ref type t few years, they have been successfully applied to, for instance, node clustering <ref type=""bibr"" target=""#b9"">[10]</ref>, semi-supervised learning <ref type=""bibr"" target=""#b24"">[25]</ref>, or graph regression <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b18"">19]</ref>, and ture, including all spectral-based GCNs <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b13"">14]</ref>, or GCNs with order-1 filters <ref type=""bibr"" target=""#b24"">[25]</ref> which are assimilable to message-passing networks <ref type=""bibr"" target=""#b18"">[19]</ref>, see <ref type="" are going to prove Theorem 4 with C defined as <ref type=""bibr"" target=""#b25"">(26)</ref> and C ′ as <ref type=""bibr"" target=""#b27"">(28)</ref>. Using <ref type=""bibr"" target=""#b24"">(25)</ref> and Prop 5, we obtain that</p><formula xml:id=""formula_97"">W 2 (Φ W,Pτ (f ′ ) ♯ P τ , Φ W,P (f ) ♯ P ) T τ Φ",0
"ay for better predictions.</p><p>There are some works <ref type=""bibr"">(Xu et al., 2018b;</ref><ref type=""bibr"" target=""#b23"">Liao et al., 2019;</ref><ref type=""bibr"" target=""#b20"">Klicpera et al., 2018;</ref><ref type=""bibr"" target=""#b21"">Li et al., 2019)</ref> that tried to address this issue part sentation for vision tasks. This insight has been empirically demonstrated in many recent works <ref type=""bibr"" target=""#b34"">(Wu et al., 2019;</ref><ref type=""bibr"" target=""#b20"">Klicpera et al., 2018;</ref><ref type=""bibr"">Xu et al., 2018a)</ref>, showing that a two-layer fully-connected neural n METHODS</head><p>Connection with PPNP and APPNP. We also established a strong connection between AdaGCN and previous state-of-the-art PPNP and APPNP <ref type=""bibr"" target=""#b20"">(Klicpera et al., 2018)</ref> method that leverages personalized pagerank to reconstruct graph convolutions in order to ""table"" target=""#tab_0"">1</ref>. Recent graph neural networks suffer from overfitting to a single splitting of training, validation and test datasets <ref type=""bibr"" target=""#b20"">(Klicpera et al., 2018)</ref>. To address this problem, inspired by <ref type=""bibr"" target=""#b20"">(Klicpera et al., 20 plitting of training, validation and test datasets <ref type=""bibr"" target=""#b20"">(Klicpera et al., 2018)</ref>. To address this problem, inspired by <ref type=""bibr"" target=""#b20"">(Klicpera et al., 2018)</ref>, we test all approaches on multiple random splits and initialization to conduct a rigorou 34"">(Wu et al., 2019)</ref> in Figure <ref type=""figure"" target=""#fig_5"">3</ref>. In Table <ref type=""table"">2</ref>, we employ the same baselines as <ref type=""bibr"" target=""#b20"">(Klicpera et al., 2018)</ref>: V.GCN (vanilla GCN) (Kipf &amp; Welling, 2017) and GCN with our early stopping, N-GCN (n =""#b7"">(Chen et al., 2018)</ref> and GraphSAGE <ref type=""bibr"" target=""#b13"">(Hamilton et al., 2017)</ref>. We refer to the result of baselines from <ref type=""bibr"" target=""#b20"">(Klicpera et al., 2018)</ref> / and the implementation of AdaGCN is adapted from APPNP. For AdaGCN, after the line sear",1
"20 splittings of datasets under 100 runs.with APPNP, showing more efficiency on graphs with few labeled nodes. Inspired by the Layer Effect on graphs<ref type=""bibr"" target=""#b31"">(Sun et al., 2019)</ref>, we argue that the increase of layers in AdaGCN can result in more benefits on the efficient p",1
"ng neighbors while previous GCNs-based methods cannot. Proof can refer to Appendix A.4. Recap the definition of general layer-wise Neighborhood Mixing<ref type=""bibr"" target=""#b1"">(Abu-El-Haija et al., 2019)</ref> as follows:</figDesc></figure> <figure xmlns=""http://www.tei-c.org/ns/1.0"" xml:id=""fig",0
"layers' parameter matrix W (i) , i = 0, ..., l -1, which is definitely costly in computation. Besides, Multi-Scale Deep Graph Convolutional Networks <ref type=""bibr"" target=""#b25"">(Luan et al., 2019)</ref> also theoretically demonstrated that the output can only contain the stationary information o",0
"h-order information can be aggregated in an effective way for better predictions.</p><p>There are some works <ref type=""bibr"">(Xu et al., 2018b;</ref><ref type=""bibr"" target=""#b23"">Liao et al., 2019;</ref><ref type=""bibr"" target=""#b20"">Klicpera et al., 2018;</ref><ref type=""bibr"" target=""#b21"">Li et",0
"ign of GCN appears counterintuitive as deep versions of these models, in principle, have access to more information, but perform worse. Oversmoothing <ref type=""bibr"" target=""#b22"">(Li et al., 2018)</ref> has been proposed to explain why deep GCN fails, showing that by repeatedly applying Laplacian This also indicates that by stacking too many graph convolutional layers, the embedding of each node in GCN is inclined to converge to certain value <ref type=""bibr"" target=""#b22"">(Li et al., 2018)</ref>, making it harder for classification. These shallow model architectures restricted by oversmoot he removal of ReLU operation could also alleviate the oversmoothing issue, i.e. slowering the convergence of node embedding to indistinguishable ones <ref type=""bibr"" target=""#b22"">(Li et al., 2018)</ref>. Additionally, without ReLU this simplified graph convolution is also able to avoid the aforeme rg/ns/1.0""><head n=""4.1"">DESIGN OF DEEP GRAPH MODELS TO CIRCUMVENT OVERSMOOTHING EFFECT</head><p>It is well-known that GCN suffers from oversmoothing <ref type=""bibr"" target=""#b22"">(Li et al., 2018)</ref> with the stacking of more graph convolutions. However, combination of knowledge from each layer",0
"f type=""bibr"" target=""#b14"">(Hastie et al., 2009)</ref>. Moreover, boosting theory has been used to analyze the success of ResNets in computer vision <ref type=""bibr"" target=""#b16"">(Huang et al., 2018)</ref> and <ref type=""bibr"">AdaGAN (Tolstikhin et al., 2017)</ref> has already successfully incorpo",0
", 2018;</ref><ref type=""bibr"" target=""#b28"">McCallum et al., 2000)</ref>, PubMed <ref type=""bibr"" target=""#b29"">(Sen et al., 2008)</ref>, MS-Academic <ref type=""bibr"" target=""#b30"">(Shchur et al., 2018)</ref> and Reddit. Dateset statistics are summarized in Table <ref type=""table"" target=""#tab_0"">1<",0
")</ref> and <ref type=""bibr"">AdaGAN (Tolstikhin et al., 2017)</ref> has already successfully incorporated boosting algorithm into the training of GAN <ref type=""bibr"" target=""#b11"">(Goodfellow et al., 2014)</ref>.</p><p>In this work, we focus on incorporating AdaBoost into the design of deep graph c",0
"nderlying usercommunity interactions or user hierarchy which have shown an advantageous performance over paradigms using user-item interactions alone <ref type=""bibr"" target=""#b18"">[19]</ref>- <ref type=""bibr"" target=""#b20"">[21]</ref>.</p><p>Generally speaking, GNN methods are inherently flat and do enchmarks. Nonetheless, generating a hierarchical representation involves extensive and unscalable computation with the adjacent matrix of the graph. <ref type=""bibr"" target=""#b18"">[19]</ref> learns a hierarchical representation of graphs by decomposing user information into two orthogonal spaces, e features in graph classification and clustering, and becomes prevailing in several scenarios such as link prediction, e-commerce recommendation, etc, <ref type=""bibr"" target=""#b18"">[19]</ref>, <ref type=""bibr"" target=""#b21"">[22]</ref>. There are some recent works that learn hierarchical graph repres ctively. CGNN can be considered as a special case of our proposed method, which fixes the number of user levels to 2. The parameter of CGNN refers to <ref type=""bibr"" target=""#b18"">[19]</ref>. • DIN: A popular deep neural network method without graph structure information and hierarchical informatio",1
">.</p><p>Generally speaking, GNN methods are inherently flat and do not learn hierarchical representations of graphs. On one hand, it demonstrates in <ref type=""bibr"" target=""#b19"">[20]</ref> that hierarchical representations of graphs can be combined with various graph neural network architectures earn hierarchical graph representations by combining GNNs with different clustering processes. In particular, the recently proposed approach DIFFPOOL <ref type=""bibr"" target=""#b19"">[20]</ref>, a differentiable graph pooling module that can generate hierarchical representations of graphs and can be c ]</ref> illustrate a user's community-level embedding to be effective in graph classification tasks, in addition to a user's individual embedding. In <ref type=""bibr"" target=""#b19"">[20]</ref>, authors make some efforts in effectively co-training two embeddings by decomposing user information into tw hierarchical item attractiveness to predict real-world e-commerce tasks of such large scale, including <ref type=""bibr"" target=""#b29"">[30]</ref> and <ref type=""bibr"" target=""#b19"">[20]</ref>. Our baseline algorithms are as follows:</p><p>• CGNN: A graph neural network method learns two user embeddi",1
"rmation which limits its application in unsupervised learning for computing meaningful and interpretable clusters on input graphs. On the other hand, <ref type=""bibr"" target=""#b21"">[22]</ref> proposes an approach that automatically constructs an easyto-interpret taxonomy on a large-scale bi-partite ring, and becomes prevailing in several scenarios such as link prediction, e-commerce recommendation, etc, <ref type=""bibr"" target=""#b18"">[19]</ref>, <ref type=""bibr"" target=""#b21"">[22]</ref>. There are some recent works that learn hierarchical graph representations by combining GNNs with different . Another intriguing application of hierarchical graph representation is e-commerce taxonomy for offering a personalized dynamic shopping navigation. <ref type=""bibr"" target=""#b21"">[22]</ref> illstrates a topic-driven hierarchical taxonomy based on user-item bi-partite graph in presence of query int les of all items belonging to the same topic t k .</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>D. Experiments and Results</head><p>SHOAL <ref type=""bibr"" target=""#b21"">[22]</ref> is Alibaba's current topic-driven taxonomy solution deployed on Taobao platform, which also considers a hier erimental Results: To investigate the model effectiveness, we compare our proposed method with Alibaba's current topic-driven taxonomy solution SHOAL <ref type=""bibr"" target=""#b21"">[22]</ref>. In the parameter setting, we set the level number of the hierarchical structure L = 4 according to the obse",1
"ttp://www.tei-c.org/ns/1.0""><head>B. Graph-based Collaborative Filtering</head><p>Another line of research <ref type=""bibr"" target=""#b26"">[27]</ref>, <ref type=""bibr"" target=""#b27"">[28]</ref> exploits the user-item interaction graph to infer user preference in a collaborative fashion, assuming behav",0
""">[6]</ref>. More specific, collaborative filtering assumes that behaviorally similar users would exhibit similar preference on items, and vice versa <ref type=""bibr"" target=""#b6"">[7]</ref>. As a result, users and items are vectorized as embeddings to reconstruct historical interactions for efficien",0
"the statistics of samples in datasets.</p><p>We adopt the area under the receiver operator curve (AUC) to evaluate the performance of all the methods <ref type=""bibr"" target=""#b32"">[33]</ref>. AUC is the most popular evaluation metric on prediction tasks in both research and industry area. Larger AU",0
"lly expensive that make it less popular in handling large-scale graphs <ref type=""bibr"" target=""#b29"">[30]</ref>. On the other hand, some researchers <ref type=""bibr"" target=""#b30"">[31]</ref>, <ref type=""bibr"" target=""#b31"">[32]</ref> illustrate a user's community-level embedding to be effective in",0
"t of items and each item will be connected with a number of queries. To make the topic more interpretive, we follow the similar strategy described in <ref type=""bibr"" target=""#b36"">[37]</ref> to find the most representative query as the description for a specific topic. The topic description matchin",0
"te-of-the-art results through effectively learned node embeddings of non-linear interactions in tasks such as node classification and link prediction <ref type=""bibr"" target=""#b7"">[8]</ref>- <ref type=""bibr"" target=""#b16"">[17]</ref>. In particular, <ref type=""bibr"" target=""#b17"">[18]</ref> proposes",0
"er of layers increases, the representations of the nodes in GCN are inclined to converge to a certain value and thus become indistinguishable. ResNet <ref type=""bibr"" target=""#b13"">(He et al., 2016)</ref> solves a similar problem in computer vision with residual connections, which is effective for t yer H (0) ; 2) We add an identity mapping I n to the -th weight matrix W ( ) . Initial residual connection. To simulate the skip connection in ResNet <ref type=""bibr"" target=""#b13"">(He et al., 2016)</ref>, <ref type=""bibr"" target=""#b16"">(Kipf &amp; Welling, 2017)</ref> proposes residual connection t x W ( ) . In the following, we summarize the motivations for introducing identity mapping into our model.</p><p>• Similar to the motivation of ResNet <ref type=""bibr"" target=""#b13"">(He et al., 2016)</ref>, identity mapping ensures that a deep GCNII model achieves at least the same performance as its",1
"ure higher-order information in the graph by applying the K-th power of the graph convolution matrix in a single neural network layer. PPNP and APPNP <ref type=""bibr"" target=""#b17"">(Klicpera et al., 2019a)</ref> replace the power of the graph convolution matrix with the Personalized PageRank matrix L effectively shrinks the underlying graph spectrum.</p><formula xml:id=""formula_3"">sponds to D−1/2 Ã D−1/2 K x = I n − L K x. (Wu</formula><p>APPNP. <ref type=""bibr"" target=""#b17"">(Klicpera et al., 2019a)</ref> uses Personalized PageRank to derive a fixed filter of order K. Let f θ (X) denote the o .</formula><p>(2)</p><p>Due to the property of Personalized PageRank, such a filter preserves locality and thus is suitable for classification tasks. <ref type=""bibr"" target=""#b17"">(Klicpera et al., 2019a)</ref> also proposes APPNP, which</p><formula xml:id=""formula_5"">replaces α I n − (1 − α) Ã −1< ed neural network on X to obtain a lower-dimensional initial representation H (0) before the forward propagation.</p><p>Finally, we recall that APPNP <ref type=""bibr"" target=""#b17"">(Klicpera et al., 2019a)</ref> employs a similar approach to the initial residual connection in the context of Personal pe=""formula"" target=""#formula_6"">3</ref>)).</p><p>• It has been observed that frequent interaction between different dimensions of the feature matrix <ref type=""bibr"" target=""#b17"">(Klicpera et al., 2019a)</ref> degrades the performance of the model in semi-supervised tasks. Mapping the smoothed rep roper choice of θ , h (K) can carry information from both the input feature and the graph structure even with K going to infinity. For example, APPNP <ref type=""bibr"" target=""#b17"">(Klicpera et al., 2019a)</ref> and GDC <ref type=""bibr"" target=""#b18"">(Klicpera et al., 2019b)</ref> set θ i = α(1−α) i ls: GCN <ref type=""bibr"" target=""#b16"">(Kipf &amp; Welling, 2017)</ref>, GAT <ref type=""bibr"" target=""#b38"">(Veličković et al., 2018)</ref> and APPNP <ref type=""bibr"" target=""#b17"">(Klicpera et al., 2019a)</ref>.</p><p>We use the Adam SGD optimizer <ref type=""bibr"" target=""#b15"">(Kingma &amp; Ba, 20 ""#b17"">(Klicpera et al., 2019a)</ref> employs a similar approach to the initial residual connection in the context of Personalized PageRank. However, <ref type=""bibr"" target=""#b17"">(Klicpera et al., 2019a</ref>) also shows that performing multiple nonlinearity operations to the feature matrix will l",1
"rnational Conference on Machine <ref type=""bibr"">Learning, Vienna, Austria, PMLR 119, 2020.</ref> Copyright 2020 by the author(s).</p><p>puter vision <ref type=""bibr"" target=""#b48"">(Zhao et al., 2019;</ref><ref type=""bibr"" target=""#b27"">Ma et al., 2019)</ref>.</p><p>Despite their enormous success, m",0
"et al., 2019a)</ref> replace the power of the graph convolution matrix with the Personalized PageRank matrix to solve the over-smoothing problem. GDC <ref type=""bibr"" target=""#b18"">(Klicpera et al., 2019b)</ref> further extends APPNP by generalizing Personalized PageRank <ref type=""bibr"" target=""#b2 ature and the graph structure even with K going to infinity. For example, APPNP <ref type=""bibr"" target=""#b17"">(Klicpera et al., 2019a)</ref> and GDC <ref type=""bibr"" target=""#b18"">(Klicpera et al., 2019b)</ref> set θ i = α(1−α) i for some constant 0 &lt; α &lt; 1. As K goes to infinity, h (K) = K =",0
"g, Vienna, Austria, PMLR 119, 2020.</ref> Copyright 2020 by the author(s).</p><p>puter vision <ref type=""bibr"" target=""#b48"">(Zhao et al., 2019;</ref><ref type=""bibr"" target=""#b27"">Ma et al., 2019)</ref>.</p><p>Despite their enormous success, most of the current GCN models are shallow. Most of the r",0
"xmlns=""http://www.tei-c.org/ns/1.0""><head n=""5."">Other Related Work</head><p>Spectral-based GCN has been extensively studied for the past few years. <ref type=""bibr"" target=""#b25"">(Li et al., 2018c)</ref> improves flexibility by learning a task-driven adaptive graph for each graph data while traini",0
". However, stacking more layers and adding non-linearity tends to degrade the performance of these models. Such a phenomenon is called over-smoothing <ref type=""bibr"" target=""#b24"">(Li et al., 2018b)</ref>, which suggests that as the number of layers increases, the representations of the nodes in GC",0
"type=""bibr"" target=""#b25"">(Li et al., 2018c)</ref> improves flexibility by learning a task-driven adaptive graph for each graph data while training. <ref type=""bibr"" target=""#b42"">(Xu et al., 2019)</ref> uses the graph wavelet basis instead of the Fourier basis to improve sparseness and locality. A",0
"=""bibr"" target=""#b10"">(Guo et al., 2019;</ref><ref type=""bibr"">Li et al., 2019)</ref>, biology <ref type=""bibr"" target=""#b8"">(Fout et al., 2017;</ref><ref type=""bibr"" target=""#b36"">Shang et al., 2019)</ref>, recommender systems <ref type=""bibr"" target=""#b45"">(Ying et al., 2018)</ref>, and com-1 Scho",0
"l analysis <ref type=""bibr"" target=""#b32"">(Qiu et al., 2018;</ref><ref type=""bibr"" target=""#b21"">Li &amp; Goldwasser, 2019)</ref>, traffic prediction <ref type=""bibr"" target=""#b10"">(Guo et al., 2019;</ref><ref type=""bibr"">Li et al., 2019)</ref>, biology <ref type=""bibr"" target=""#b8"">(Fout et al., 20",0
"xml:lang=""en""> 		<body> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""1."">Introduction</head><p>Transformer models were originally introduced by <ref type=""bibr"" target=""#b36"">Vaswani et al. (2017)</ref> in the context of neural machine translation <ref type=""bibr"" target=""#b34"">(Sutskever et a ear time, similar to a recurrent neural network.</p><p>Initially, in § 3.1, we introduce a formulation for the transformer architecture introduced in <ref type=""bibr"" target=""#b36"">(Vaswani et al., 2017)</ref>. Subsequently, in § 3.2 and § 3.3 we present our proposed linear transformer and finally, is does not affect the results as we only measure the memory consumption with respect to the self attention layer. In all experiments, we use softmax <ref type=""bibr"" target=""#b36"">(Vaswani et al., 2017)</ref> to refer to the standard transformer architecture, linear for our proposed linear transfor",1
"(2020)</ref> proposed a new pretraining objective called replaced token detection that is more sample efficient and reduces the overall computation. <ref type=""bibr"" target=""#b15"">Lample et al. (2019)</ref> used product-key attention to increase the capacity of any layer with negligible computation",0
"t al., 2020)</ref>, where X denotes the hashing rounds.</p><p>For training the linear transformers, we use the feature map of equation 7. Our PyTorch <ref type=""bibr"" target=""#b24"">(Paszke et al., 2019)</ref> code with documentation and examples can be found at https:// linear-transformers.com/. The",0
"els cannot only be attributed to the properties of MI alone, and the choice of encoder and MI estimators have a significant impact on the performance <ref type=""bibr"" target=""#b52"">(Tschannen et al., 2020)</ref>.</p><p>Figure <ref type=""figure"">1</ref>. The proposed model for contrastive multi-view",1
"rom topological neighbors. Each iteration expands the receptive field by one-hop and after k iterations the nodes within k-hops influence one another <ref type=""bibr"" target=""#b21"">(Khasahmadi et al., 2020)</ref>. GNNs are applied to data with arbitrary topology such as point clouds <ref type=""bibr""",0
"ns=""http://www.tei-c.org/ns/1.0""><head n=""1."">Introduction</head><p>Graph neural networks (GNN) <ref type=""bibr"" target=""#b31"">(Li et al., 2015;</ref><ref type=""bibr"" target=""#b11"">Gilmer et al., 2017;</ref><ref type=""bibr"" target=""#b24"">Kipf &amp; Welling, 2017;</ref><ref type=""bibr"" target=""#b54"">",0
"&amp; Welling, 2016;</ref><ref type=""bibr"" target=""#b9"">Garcia Duran &amp; Niepert, 2017;</ref><ref type=""bibr"" target=""#b57"">Wang et al., 2017;</ref><ref type=""bibr"" target=""#b40"">Pan et al., 2018;</ref><ref type=""bibr"" target=""#b41"">Park et al., 2019)</ref> train encoders that impose the topologic 2016)</ref>, marginalized GAE (MGAE) <ref type=""bibr"" target=""#b57"">(Wang et al., 2017)</ref>, adversarially regularized GAE (ARGA) and VGAE (ARVGA) <ref type=""bibr"" target=""#b40"">(Pan et al., 2018)</ref>, and GALA <ref type=""bibr"" target=""#b41"">(Park et al., 2019)</ref>.</p><p>The results shown in",0
"(Gutmann &amp; Hyvärinen, 2010;</ref><ref type=""bibr"" target=""#b38"">Oord et al., 2018)</ref>, Jensen-Shannon (JSD) estimator following formulation in <ref type=""bibr"" target=""#b37"">(Nowozin et al., 2016)</ref>, normalized temperature-scaled cross-entropy (NT-Xent) <ref type=""bibr"" target=""#b5"">(Chen",0
"rs during training. TV is a metric used in the graph signal processing literature to measure the smoothness of a signal defined over nodes of a graph <ref type=""bibr"" target=""#b4"">(Chen et al., 2015)</ref>. More specifically, given a graph with the adjacency matrix A and a signal x defined over its",1
"stochastic variational inference <ref type=""bibr"" target=""#b32"">(Paisley et al., 2012;</ref><ref type=""bibr"" target=""#b34"">Rezende et al., 2014;</ref><ref type=""bibr"" target=""#b15"">Hajiramezanali et al., 2020;</ref><ref type=""bibr"" target=""#b5"">Dadaneh et al., 2020a)</ref>, Markov chain Monte Carlo",0
"n be robust to over-fitting and provide appropriate prediction uncertainty estimation <ref type=""bibr"" target=""#b9"">(Gal &amp; Ghahramani, 2016;</ref><ref type=""bibr"" target=""#b1"">Boluki et al., 2020)</ref>. Often, the standard Gaussian prior distribution is placed over the weights. With random weig nsidered random to enable Bayesian inference based on predictive posterior given training data <ref type=""bibr"" target=""#b10"">(Gal et al., 2017;</ref><ref type=""bibr"" target=""#b1"">Boluki et al., 2020)</ref>.</p><p>Here, we show that connection sampling in GDC can be transformed from the output featu the recently developed Augment-REINFORCE-Merge (ARM) method <ref type=""bibr"" target=""#b39"">(Yin &amp; Zhou, 2019)</ref>, which has been used in BNNs <ref type=""bibr"" target=""#b1"">(Boluki et al., 2020)</ref> and information retrieval <ref type=""bibr"" target=""#b6"">(Dadaneh et al., 2020b;</ref><ref ty oduces bias. Our other approach is to directly optimize the variational parameters using the original Bernoulli distribution in the formulation as in <ref type=""bibr"" target=""#b1"">Boluki et al. (2020)</ref>. We can calculate the gradient of the variational loss with respect to α = {logit(1 − π l )}",0
"uch as Bernoulli <ref type=""bibr"" target=""#b18"">(Hinton et al., 2012;</ref><ref type=""bibr"" target=""#b36"">Srivastava et al., 2014)</ref> and Gaussian <ref type=""bibr"" target=""#b22"">(Kingma et al., 2015;</ref><ref type=""bibr"" target=""#b36"">Srivastava et al., 2014)</ref>. Bernoulli dropout and its ext",0
"luate the predictive posterior uncertainty. An important corollary of this formulation is that any GNN with neighborhood sampling, such as Graph-SAGE <ref type=""bibr"" target=""#b16"">(Hamilton et al., 2017)</ref>, could be considered as its corresponding Bayesian approximation.</p></div> <div xmlns=""h 9)</ref>, have been used to prevent over-fitting and over-smoothing in GNNs. Sampling-based stochastic reduction by random walk neighborhood sampling <ref type=""bibr"" target=""#b16"">(Hamilton et al., 2017)</ref> and node sampling <ref type=""bibr"" target=""#b3"">(Chen et al., 2018)</ref> has been deploy ypes of random walk have been used in graph representation learning literature to reduce the size of input graphs. In GNNs, specifically in GraphSAGE <ref type=""bibr"" target=""#b16"">(Hamilton et al., 2017)</ref>, random walk sampling has been deployed to reduce the model complexity for very large gra",0
"ef type=""bibr"" target=""#b24"">(Kipf &amp; Welling, 2017;</ref><ref type=""bibr"">2016;</ref><ref type=""bibr"" target=""#b17"">Hasanzadeh et al., 2019;</ref><ref type=""bibr"" target=""#b14"">Hajiramezanali et al., 2019)</ref>. Despite their successes, GNNs have There exist a variety of methods to address thes",0
") = A Z (l) . Such a formulation is known to be capable of enforcing sparsity in random masks <ref type=""bibr"" target=""#b41"">(Zhou et al., 2009;</ref><ref type=""bibr"" target=""#b13"">Hajiramezanali et al., 2018)</ref>, which has been shown to be necessary for regularizing deep GNNs as discussed in Dro",0
"tion representations are more effective for natural language understanding and generation tasks <ref type=""bibr"" target=""#b9"">(Dai et al., 2019;</ref><ref type=""bibr"" target=""#b38"">Shaw et al., 2018)</ref>. The proposed Disentangled Attention mechanism differs from all existing approaches in that we t-to-content, content-to-position, position-to-content, and position-to-position<ref type=""foot"" target=""#foot_0"">1</ref> .</p><p>Existing approaches <ref type=""bibr"" target=""#b38"">(Shaw et al., 2018;</ref><ref type=""bibr"" target=""#b18"">Huang et al., 2018)</ref> to relative position encoding use a s ww.tei-c.org/ns/1.0""><head n=""3.1.1"">EFFICIENT IMPLEMENTATION</head><p>For an input sequence of length N , it requires a space complexity of OpN 2 dq <ref type=""bibr"" target=""#b38"">(Shaw et al., 2018;</ref><ref type=""bibr"" target=""#b18"">Huang et al., 2018;</ref><ref type=""bibr"" target=""#b9"">Dai et a",1
"2020)</ref>, T5 <ref type=""bibr"" target=""#b34"">(Raffel et al., 2019)</ref>, ALUM <ref type=""bibr"" target=""#b28"">(Liu et al., 2020)</ref>, StructBERT <ref type=""bibr"" target=""#b46"">(Wang et al., 2019)</ref> and ERINE <ref type=""bibr"" target=""#b42"">(Sun et al., 2019)</ref> . These PLMs have been fine",0
"ndling is an active research area as of late, there are a lot of works built on the Transformer to optimize its performance on long sequence handling <ref type=""bibr"" target=""#b1"">(Beltagy et al., 2020;</ref><ref type=""bibr"" target=""#b22"">Kitaev et al., 2020;</ref><ref type=""bibr"" target=""#b6"">Child",0
"9"">Liu et al., 2019c)</ref>. For training data, we use Wikipedia (English Wikipedia dump<ref type=""foot"" target=""#foot_2"">2</ref> ; 12GB), BookCorpus <ref type=""bibr"" target=""#b52"">(Zhu et al., 2015)</ref>  We evaluate DeBERTa on additional benchmarks: (1) Question Answering: SQuAD v1.1 <ref type=""b urces. It has 4 types of named entity.A.2 PRE-TRAINING DATASETFor DeBERTa pre-training, we use Wikipedia (English Wikipedia dump 6 ; 12GB), BookCorpus<ref type=""bibr"" target=""#b52"">(Zhu et al., 2015)</ref> 7 (6GB), OPENWEBTEXT (public Reddit content<ref type=""bibr"" target=""#b15"">(Gokaslan &amp; Cohe",0
"ork properly. This is because the loss value is decreasing with the increase of training iterations. Inspired by the dynamic gradient descent methods <ref type=""bibr"" target=""#b21"">[22]</ref>, we replace the fixed threshold with a dynamic threshold function 𝜏 (𝑇 ) w.r.t. the training iteration 𝑇 , w factor numbers of users and items are both 32. As to CDAE, the hidden size of MLP is set as 200. In addition, the batch size is always 1,024 and Adam <ref type=""bibr"" target=""#b21"">[22]</ref> is applied to optimize all the parameters with the learning rate initialized as 0.001. As to the ADT strateg",1
") the degree of weight reduction can be easily adjusted so that it can fit different models and datasets.</p><p>Inspired by the success of Focal Loss <ref type=""bibr"" target=""#b29"">[30]</ref>, we estimate 𝜔 (𝑢, 𝑖) with a function of 𝑓 ( ŷ𝑢𝑖 ) that takes the prediction score as the input. Note that t",1
"e inevitable noises in implicit feedback and eliminate the impact of false-positive interactions for recommender training.</p><p>Indeed, some efforts <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b40"">41]</ref> have been dedicated to elimina p between implicit feedback and the actual user preference, many researchers have paid attention to identify negative experiences in implicit signals <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b20"">21]</ref>. Prior work usually collects the various users' feedback (e.g., dwell arious users' feedback (e.g., dwell time <ref type=""bibr"" target=""#b20"">[21]</ref>, gaze patterns <ref type=""bibr"" target=""#b45"">[46]</ref>, and skip <ref type=""bibr"" target=""#b6"">[7]</ref>) and the item characteristics <ref type=""bibr"" target=""#b31"">[32,</ref><ref type=""bibr"" target=""#b32"">33]</ref",0
"mpirical evidence on the negative effects of false-positive interactions when we train a competitive recommender, Neural Matrix Factorization (NeuMF) <ref type=""bibr"" target=""#b15"">[16]</ref>, on two real-world datasets. In particular, we construct a ""clean"" testing set by removing the false-positiv hmarks, we test ADT trained with the Truncated Loss or Reweighted Loss over three representative recommenders: Generalized Matrix Factorization (GMF) <ref type=""bibr"" target=""#b15"">[16]</ref>, NeuMF <ref type=""bibr"" target=""#b15"">[16]</ref>, and Collaborative Denoising Auto-Encoder (CDAE) <ref type= ss or Reweighted Loss over three representative recommenders: Generalized Matrix Factorization (GMF) <ref type=""bibr"" target=""#b15"">[16]</ref>, NeuMF <ref type=""bibr"" target=""#b15"">[16]</ref>, and Collaborative Denoising Auto-Encoder (CDAE) <ref type=""bibr"" target=""#b39"">[40]</ref>. The results show ecommenders trained with T-CE or R-CE and normal training with standard CE. We selected two representative user-based neural CF models, GMF and NeuMF <ref type=""bibr"" target=""#b15"">[16]</ref>, and one item-based model, CDAE <ref type=""bibr"" target=""#b39"">[40]</ref>. Note that CDAE is also a represen >[40]</ref>. Note that CDAE is also a representative model of robust recommender which can defend random noises within implicit feedback.</p><p>• GMF <ref type=""bibr"" target=""#b15"">[16]</ref>: This is a generalized version of matrix factorization by replacing the inner product with the element-wise n of matrix factorization by replacing the inner product with the element-wise product and a linear neural layer as the interaction function. • NeuMF <ref type=""bibr"" target=""#b15"">[16]</ref>: NeuMF is a representative CF neural model, which models the relationship between users and items by combini er in the testing set, we predicted the preference score over all the items except the positive ones used during training. Following existing studies <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b37"">38]</ref>, we reported the recommendation performance w.r.t. two widely used m ional ones such as MF <ref type=""bibr"" target=""#b23"">[24]</ref> and SVD++ <ref type=""bibr"" target=""#b22"">[23]</ref> due to their inferior performance <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b39"">40]</ref>.</p><p>Parameter Settings. For the three testing recommenders, we fo",0
"s and items. Prior study on robust learning <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b19"">20]</ref> and curriculum learning <ref type=""bibr"" target=""#b1"">[2]</ref> demonstrate that noisy samples are relatively harder to fit into models, indicating distinct patterns of noisy fit in the early stages. In robust learning <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b19"">20]</ref> and curriculum learning <ref type=""bibr"" target=""#b1"">[2]</ref>, one theory is that easy samples are more likely to be the clean ones and fitting the hard samples may hurt th be more informative than easy samples and discarding hard samples would limit the model's learning ability. Indeed, as indicated in the prior studies <ref type=""bibr"" target=""#b1"">[2]</ref>, hard samples in the noisy data probably confuse the model rather than help it to establish the right decision hat the user didn't interact with (i.e., the negative samples). The findings also support the prior theory in robust learning and curriculum learning <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b12"">13]</ref>. Overall, the results are consistent with the memorization effect <ref",0
"<p>Incorporating Various Feedback. To alleviate the impact of false-positive interactions, previous approaches <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b30"">31,</ref><ref type=""bibr"" target=""#b40"">41,</ref><ref type=""bibr"" target=""#b43""",0
"oises, and then employs a MLP model to reconstruct the original input.</p><p>We only tested neural recommenders and omit conventional ones such as MF <ref type=""bibr"" target=""#b23"">[24]</ref> and SVD++ <ref type=""bibr"" target=""#b22"">[23]</ref> due to their inferior performance <ref type=""bibr"" targe",0
"he original input.</p><p>We only tested neural recommenders and omit conventional ones such as MF <ref type=""bibr"" target=""#b23"">[24]</ref> and SVD++ <ref type=""bibr"" target=""#b22"">[23]</ref> due to their inferior performance <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b39"">40",0
"licit interactions are easily affected by the first impression of users and other factors such as caption bias <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b32"">33]</ref> and position bias <ref type=""bibr"" target=""#b18"">[19]</ref>. Moreover",0
"table to explore the effectiveness of denoising implicit feedback although explicit feedback also exists in each interaction. We followed former work <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b37"">38]</ref> to remove users and items wi",0
"/knowledge-graph-identification</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>Class</head><p>Ontological Rule</p><p>Uncertain Extractions  <ref type=""bibr"" target=""#b0"">[Blum and Mitchell, 1998]</ref>, to combine the strengths of PSL-KGI and KG embeddings. The mechanism consists of two st",1
"y type and new fact predictions <ref type=""bibr"" target=""#b16"">[Nickel et al., 2012</ref><ref type=""bibr"" target=""#b23"">, Trouillon et al., 2016</ref><ref type=""bibr"" target=""#b3"">, Dettmers et al., 2018]</ref>. It is worth noting that embeddings, with a few recent exceptions <ref type=""bibr"" target use Probabilistic Soft Logic (PSL) that can incorporate inference rules and ontologies, along with state-of-the-art KG embedding methods,viz., ConvE <ref type=""bibr"" target=""#b3"">[Dettmers et al., 2018]</ref> and ComplEx <ref type=""bibr"" target=""#b23"">[Trouillon et al., 2016]</ref>, which do not ma o predict type labels of entities (the typeOf relation). We work with ComplEx <ref type=""bibr"" target=""#b23"">[Trouillon et al., 2016]</ref> and ConvE <ref type=""bibr"" target=""#b3"">[Dettmers et al., 2018]</ref> embeddings which have shown state of the art performance in many KG prediction tasks.</p>< embeddings like ComplEx <ref type=""bibr"" target=""#b23"">[Trouillon et al., 2016]</ref>, <ref type=""bibr"">SimplE [Kazemi and Poole, 2018]</ref>, ConvE <ref type=""bibr"" target=""#b3"">[Dettmers et al., 2018]</ref> cannot enforce subsumption. Taking a different approach <ref type=""bibr"" target=""#b7"">[Jai mple types) and the first level of YAGO taxonomy. Then all facts upto length 3 in the hierarchy of taxonomy were included.</p><p>FB15K-237: FB15K-237 <ref type=""bibr"" target=""#b3"">[Dettmers et al., 2018]</ref>, another popular benchmark does not have ontological and type label information. Therefore e of TypeE-X models in the KG refinement task, and compare them with Com-plEx <ref type=""bibr"" target=""#b23"">[Trouillon et al., 2016]</ref> and ConvE <ref type=""bibr"" target=""#b3"">[Dettmers et al., 2018]</ref>, two state-of-the-art KG embeddings methods, 4. https://www.w3.org/2006/03/wn/wn20/ and PS e split the test set into 2 equal halves preserving the same class balance, and use them as our validation and test split.</p><p>YAGO3-10: YAGO3-10 [ <ref type=""bibr"" target=""#b3"">Dettmers et al., 2018]</ref> is a subset of the YAGO3 <ref type=""bibr"" target=""#b22"">[Suchanek et al., 2007]</ref> knowl",1
"ten due to the lack of type compatibility between the entities due to their typeagnostic nature <ref type=""bibr"" target=""#b25"">[Xie et al., 2016</ref><ref type=""bibr"" target=""#b7"">, Jain et al., 2018]</ref>. Since PSL-KGI is able to predict entity types by making use of ontological information along up to 9% over those which do not have type supervision.</p><p>(ii) Explicit type supervised models also outperform the implict type supervised models <ref type=""bibr"" target=""#b7"">[Jain et al., 2018]</ref>.</p><p>The margin of improvement is large when the ontological information is sufficiently ric rengths and weaknesses, our focus in this paper is on the use of ontological rules (exemplified by PSL-KGI) and embeddings (we use ComplEx, ConvE and <ref type=""bibr"" target=""#b7"">[Jain et al., 2018]</ref>). Rule induction methods are orthogonal to our work, and may augment or replace the set of rul emi and Poole, 2018]</ref>, ConvE <ref type=""bibr"" target=""#b3"">[Dettmers et al., 2018]</ref> cannot enforce subsumption. Taking a different approach <ref type=""bibr"" target=""#b7"">[Jain et al., 2018]</ref> propose extending standard KG embeddings without explicit type supervision by representing ent in Section 6.4.  To incorporate the type inferences for entities generated by PSL-KGI in KG embeddings (the second stage), we modify the typed model <ref type=""bibr"" target=""#b7"">[Jain et al., 2018]</ref> as follows:</p><p>Instead of just using the implicit type embeddings, we concatenate them with plEx, ConvE and PSL-KGI. In addition, we also compare our explicitly supervised TypeE-X methods with the implicitly supervised embeddings proposed by <ref type=""bibr"" target=""#b7"">[Jain et al., 2018]</ref>.</p><p>• In Section 6.3, we analyse how our accuracy changes as we increase the number of feed erformance of TypeE-ComplEx which has explicit type supervision with the unsupervised type-compatible embeddings-based method proposed by Jain et al. <ref type=""bibr"" target=""#b7"">[Jain et al., 2018]</ref>. As these results indicate, while explicitly ensuring type compatibility helps to improve perf type inferences from PSL-KGI to TypeE-ComplEx significantly improves the relation scores, improving weighted F1 up to 18% (over NELL).</p><p>Dataset <ref type=""bibr"" target=""#b7"">[Jain et al., 2018]</ref>  </p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""6.3"">Analysis of feedback iterat om probabilistic rule based methods like PSL-KGI <ref type=""bibr"" target=""#b18"">[Pujara et al., 2013]</ref> to KG embedding methods like type-ComplEx <ref type=""bibr"" target=""#b7"">[Jain et al., 2018]</ref>. We showed their performance on existing datasets in the literature, and how the extent pf ont tei-c.org/ns/1.0"" type=""table"" xml:id=""tab_7""><head>Table 7 :</head><label>7</label><figDesc>Weighted F1 scores on relation triples in the test set by<ref type=""bibr"" target=""#b7"">[Jain et al., 2018]</ref> and TypeE-ComplEx.Anecdotes. Looking at the example predictions by both TypeE-ComplEx and Comp",1
"between entities (e.g., matt flynn, athleteplayssport, baseball is false since Matt Flynn is an NFL player), incompatible entity types, and many more <ref type=""bibr"" target=""#b18"">[Pujara et al., 2013]</ref>. It has also been observed that such noise can significantly degrade the performance of KG em. They can also make use of ontological rules effectively, and specifically, the PSL-KGI implementation uses rules defined on schema-level features <ref type=""bibr"" target=""#b18"">[Pujara et al., 2013]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""1.1"">Contributions</head><p>In AMEENT), entities and relations that are mutually exclusive (MUT and RMUT); and inverse relations (INV). We reproduce the list of information used in <ref type=""bibr"" target=""#b18"">[Pujara et al., 2013]</ref> in tabular form in Table <ref type=""table"" target=""#tab_0"">1</ref>.</p><p>(iii) Inference r generated by sampling them from two different normal distributions: N (0.7, 0.2) for facts in the original KG and N (0.3, 0.2) for added noisy facts <ref type=""bibr"" target=""#b18"">[Pujara et al., 2013]</ref>. The SAMEENT facts between entities are generated by calculating the average of the two Jac lEx and TypeE-ConvE respectively. We use a single hyper-parameter threshold as the cutoff for classifying a test triple based on the prediction score <ref type=""bibr"" target=""#b18"">[Pujara et al., 2013]</ref>. Our experiments were run on Intel(R) Xeon(R) x86-64 machine with 64 CPUs using 1 NVIDIA GT lusion and Future work</head><p>We have looked at the KG refinement task and methods for the same, from probabilistic rule based methods like PSL-KGI <ref type=""bibr"" target=""#b18"">[Pujara et al., 2013]</ref> to KG embedding methods like type-ComplEx <ref type=""bibr"" target=""#b7"">[Jain et al., 2018] rules. An important input to these formulations are the probabilistic sources of information such as the confidence scores obtained during extraction <ref type=""bibr"" target=""#b18"">[Pujara et al., 2013</ref><ref type=""bibr"" target=""#b8"">, Jiang et al., 2012]</ref> from multiple sources. Of these met r"" target=""#b18"">[Pujara et al., 2013</ref><ref type=""bibr"" target=""#b8"">, Jiang et al., 2012]</ref> from multiple sources. Of these methods, PSL-KGI <ref type=""bibr"" target=""#b18"">[Pujara et al., 2013</ref><ref type=""bibr"" target=""#b19"">[Pujara et al., , 2017] ]</ref> is shown not only to perform b The NELL subset taken from its 165 th iteration <ref type=""bibr"" target=""#b1"">[Carlson et al., 2010]</ref>) has been used for the KG refinement task <ref type=""bibr"" target=""#b18"">[Pujara et al., 2013</ref><ref type=""bibr"" target=""#b8"">, Jiang et al., 2012]</ref>. It comes with a rich ontology from",1
"ensor-based embeddings have seen significant success in entity type and new fact predictions <ref type=""bibr"" target=""#b16"">[Nickel et al., 2012</ref><ref type=""bibr"" target=""#b23"">, Trouillon et al., 2016</ref><ref type=""bibr"" target=""#b3"">, Dettmers et al., 2018]</ref>. It is worth noting that emb and ontologies, along with state-of-the-art KG embedding methods,viz., ConvE <ref type=""bibr"" target=""#b3"">[Dettmers et al., 2018]</ref> and ComplEx <ref type=""bibr"" target=""#b23"">[Trouillon et al., 2016]</ref>, which do not make use of any ontological rules.</p><p>The resulting framework called It t containing both positive and negative samples, training can be done for the refinement task with a negative log-likelihood loss function as follows <ref type=""bibr"" target=""#b23"">[Trouillon et al., 2016]</ref>.</p><formula xml:id=""formula_0"">L(G) = (s,r,o,y)∈G y log f (s, r, o) + (1 − y) log (1 − r to the setting for PSL-KGI, embedding-based methods can also be used to predict type labels of entities (the typeOf relation). We work with ComplEx <ref type=""bibr"" target=""#b23"">[Trouillon et al., 2016]</ref> and ConvE <ref type=""bibr"" target=""#b3"">[Dettmers et al., 2018]</ref> embeddings which h ., 2019]</ref> includes taxonomic information -i.e., subtype and subproperty information-and also shows that state-of-the-art embeddings like ComplEx <ref type=""bibr"" target=""#b23"">[Trouillon et al., 2016]</ref>, <ref type=""bibr"">SimplE [Kazemi and Poole, 2018]</ref>, ConvE <ref type=""bibr"" target="" <head n=""6."">Experimental Evaluation</head><p>We evaluate the performance of TypeE-X models in the KG refinement task, and compare them with Com-plEx <ref type=""bibr"" target=""#b23"">[Trouillon et al., 2016]</ref> and ConvE <ref type=""bibr"" target=""#b3"">[Dettmers et al., 2018]</ref>, two state-of-the- that are already present in the KG <ref type=""bibr"" target=""#b15"">[Nickel et al., 2011</ref><ref type=""bibr"" target=""#b21"">, Socher et al., 2013</ref><ref type=""bibr"" target=""#b23"">, Trouillon et al., 2016]</ref>.</p><p>An important step in learning is the generation of negative samples since the ex",1
"robabilistic sources of information such as the confidence scores obtained during extraction <ref type=""bibr"" target=""#b18"">[Pujara et al., 2013</ref><ref type=""bibr"" target=""#b8"">, Jiang et al., 2012]</ref> from multiple sources. Of these methods, PSL-KGI <ref type=""bibr"" target=""#b18"">[Pujara et a e=""bibr"" target=""#b1"">[Carlson et al., 2010]</ref>) has been used for the KG refinement task <ref type=""bibr"" target=""#b18"">[Pujara et al., 2013</ref><ref type=""bibr"" target=""#b8"">, Jiang et al., 2012]</ref>. It comes with a rich ontology from the NELL system, and contains multiple sources of inform ference rules -specifically, there are 7 general constraints that were first introduced in the earlier work on Markov Logic Networks (MLN) based work <ref type=""bibr"" target=""#b8"">[Jiang et al., 2012]</ref>. These rules are listed in Appendix A in Table <ref type=""table"" target=""#tab_1"">2</ref>. Bas",0
", Dettmers et al., 2018]</ref>. It is worth noting that embeddings, with a few recent exceptions <ref type=""bibr"" target=""#b6"">[Guo et al., 2016</ref><ref type=""bibr"" target=""#b12"">, Minervini et al., 2017</ref><ref type=""bibr"">, 2018</ref><ref type=""bibr"" target=""#b5"">, Fatemi et al., 2019]</ref>,",0
"s and o. A critical issue in large-scale KGs is the presence of noise from the automatic extraction methods used to populate them. For instance, NELL <ref type=""bibr"" target=""#b1"">[Carlson et al., 2010]</ref> is known to contain various kinds of errors including: different names for the same entity sets the test set also includes the facts that were part of the original benchmark test collection.   The NELL subset taken from its 165 th iteration <ref type=""bibr"" target=""#b1"">[Carlson et al., 2010]</ref>) has been used for the KG refinement task <ref type=""bibr"" target=""#b18"">[Pujara et al., 20",0
"soners <ref type=""bibr"" target=""#b14"">[Nakashole et al., 2011</ref>] and many more. A detailed survey of approaches for KG refinement is available in <ref type=""bibr"" target=""#b17"">[Paulheim, 2017]</ref>. On the other hand, neural and tensor-based embeddings have seen significant success in entity t 1"">[Ma et al., 2014]</ref>, classification with diverse extractors <ref type=""bibr"" target=""#b4"">[Dong et al., 2014]</ref>, crowdsourcing, etc., (see <ref type=""bibr"" target=""#b17"">[Paulheim, 2017]</ref> for an overview). While these works have their own strengths and weaknesses, our focus in this p",0
""">[Paulheim, 2017]</ref>. On the other hand, neural and tensor-based embeddings have seen significant success in entity type and new fact predictions <ref type=""bibr"" target=""#b16"">[Nickel et al., 2012</ref><ref type=""bibr"" target=""#b23"">, Trouillon et al., 2016</ref><ref type=""bibr"" target=""#b3"">,",0
"can help in eliminating incorrect facts <ref type=""bibr"" target=""#b11"">[Ma et al., 2014]</ref>; reconciling diverse evidence from multiple extractors <ref type=""bibr"" target=""#b4"">[Dong et al., 2014]</ref>; the use of ontology reasoners <ref type=""bibr"" target=""#b14"">[Nakashole et al., 2011</ref>] a lving the KG refinement problem such as rule induction <ref type=""bibr"" target=""#b11"">[Ma et al., 2014]</ref>, classification with diverse extractors <ref type=""bibr"" target=""#b4"">[Dong et al., 2014]</ref>, crowdsourcing, etc., (see <ref type=""bibr"" target=""#b17"">[Paulheim, 2017]</ref> for an overvi n methods are orthogonal to our work, and may augment or replace the set of rules we use. Further, evidence from diverse extractors as in the case of <ref type=""bibr"" target=""#b4"">[Dong et al., 2014]</ref> can be incorporated into the PSL-KGI framework in a straightforward manner (see details about",0
"mple score combination of two different methods (in contrast to the two stages with iterations of our method). We use the setting introduced in R-GCN <ref type=""bibr"" target=""#b20"">(Schlichtkrull et al. [2018]</ref>) to combine scores of KG embeddings and PSL-KGI methods using the equation given bel",0
"ent efforts to incorporate type hierarchy information in KG embeddings -e.g., TKRL <ref type=""bibr"" target=""#b25"">[Xie et al., 2016]</ref> and TransC <ref type=""bibr"" target=""#b10"">[Lv et al., 2018]</ref>. Recently, SimplE + <ref type=""bibr"" target=""#b5"">[Fatemi et al., 2019]</ref> includes taxonomi",0
"constraints.</p><p>Recently, there has been some work in modeling structural as well as uncertainty information of relations in the embedding space. <ref type=""bibr"" target=""#b2"">[Chen et al., 2019]</ref> uses Probabilistic Soft Logic to come up with plausibility scores for each fact which they tra",0
"ptions <ref type=""bibr"" target=""#b6"">[Guo et al., 2016</ref><ref type=""bibr"" target=""#b12"">, Minervini et al., 2017</ref><ref type=""bibr"">, 2018</ref><ref type=""bibr"" target=""#b5"">, Fatemi et al., 2019]</ref>, do not make use of rich taxonomic/ontological rules when available. Methods such as Probab ., TKRL <ref type=""bibr"" target=""#b25"">[Xie et al., 2016]</ref> and TransC <ref type=""bibr"" target=""#b10"">[Lv et al., 2018]</ref>. Recently, SimplE + <ref type=""bibr"" target=""#b5"">[Fatemi et al., 2019]</ref> includes taxonomic information -i.e., subtype and subproperty information-and also shows tha",0
"them as our validation and test split.</p><p>YAGO3-10: YAGO3-10 [ <ref type=""bibr"" target=""#b3"">Dettmers et al., 2018]</ref> is a subset of the YAGO3 <ref type=""bibr"" target=""#b22"">[Suchanek et al., 2007]</ref> knowledge graph. It is often used for evaluating the KG completion task. We have augmente",0
"e the plausibility of a triple 1 and learn embeddings in such a way as to maximise the plausibility of the triples that are already present in the KG <ref type=""bibr"" target=""#b15"">[Nickel et al., 2011</ref><ref type=""bibr"" target=""#b21"">, Socher et al., 2013</ref><ref type=""bibr"" target=""#b23"">, Tr",0
"llon et al., 2016</ref><ref type=""bibr"" target=""#b3"">, Dettmers et al., 2018]</ref>. It is worth noting that embeddings, with a few recent exceptions <ref type=""bibr"" target=""#b6"">[Guo et al., 2016</ref><ref type=""bibr"" target=""#b12"">, Minervini et al., 2017</ref><ref type=""bibr"">, 2018</ref><ref ty",0
"ectively embed Uncertain graphs. There has also been some research in using rule-based reasoning and KG embeddings together in an iterative manner in <ref type=""bibr"" target=""#b26"">[Zhang et al., 2019]</ref>.</p><p>They achieve improvements in the performance of link prediction tasks for sparse enti",0
"such a way as to maximise the plausibility of the triples that are already present in the KG <ref type=""bibr"" target=""#b15"">[Nickel et al., 2011</ref><ref type=""bibr"" target=""#b21"">, Socher et al., 2013</ref><ref type=""bibr"" target=""#b23"">, Trouillon et al., 2016]</ref>.</p><p>An important step in l",0
"bibr"" target=""#b18"">[Pujara et al., 2013]</ref>. It has also been observed that such noise can significantly degrade the performance of KG embeddings <ref type=""bibr"" target=""#b19"">[Pujara et al., 2017]</ref>.</p><p>The KG refinement task aims to reduce the noise in KG by not only predicting additio ations) and corrupt them by randomly changing their subject, relation label or object. Note that this was the same model followed in an earlier study <ref type=""bibr"" target=""#b19"">[Pujara et al., 2017]</ref>.</p><p>• We further refine the noise model by ensuring that half of the corrupted facts hav r"" target=""#b8"">, Jiang et al., 2012]</ref> from multiple sources. Of these methods, PSL-KGI <ref type=""bibr"" target=""#b18"">[Pujara et al., 2013</ref><ref type=""bibr"" target=""#b19"">[Pujara et al., , 2017] ]</ref> is shown not only to perform better with KG noise and sparsity, but also to be quite sc",0
"mispredictions by the embeddings based methods are often due to the lack of type compatibility between the entities due to their typeagnostic nature <ref type=""bibr"" target=""#b25"">[Xie et al., 2016</ref><ref type=""bibr"" target=""#b7"">, Jain et al., 2018]</ref>. Since PSL-KGI is able to predict entit "">Type and Taxonomy Enhanced Embeddings</head><p>There are some recent efforts to incorporate type hierarchy information in KG embeddings -e.g., TKRL <ref type=""bibr"" target=""#b25"">[Xie et al., 2016]</ref> and TransC <ref type=""bibr"" target=""#b10"">[Lv et al., 2018]</ref>. Recently, SimplE + <ref typ al., 2018]</ref>, another popular benchmark does not have ontological and type label information. Therefore, we use the type labels for entities from <ref type=""bibr"" target=""#b25"">[Xie et al., 2016]</ref> which also provides the domain and range information for relations. The subclass information i",0
"for noise reduction in KG include the use of association rule mining over the noisy KG to induce rules which can help in eliminating incorrect facts <ref type=""bibr"" target=""#b11"">[Ma et al., 2014]</ref>; reconciling diverse evidence from multiple extractors <ref type=""bibr"" target=""#b4"">[Dong et a ence rules and embeddings-based methods. There are other research directions for (partially) solving the KG refinement problem such as rule induction <ref type=""bibr"" target=""#b11"">[Ma et al., 2014]</ref>, classification with diverse extractors <ref type=""bibr"" target=""#b4"">[Dong et al., 2014]</ref>",0
"ofileDesc> 	</teiHeader> 	<text xml:lang=""en""> 		<body> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""1"">Introduction</head><p>Transformer models <ref type=""bibr"" target=""#b28"">(Vaswani et al., 2017)</ref> have become ubiquitous for wide variety of problems in natural language processing (NLP), nd inference speedups.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>Model Architecture</head><p>Complexity per Layer Sequential Operation <ref type=""bibr"" target=""#b28"">(Vaswani et al., 2017)</ref> O(n 2 ) O(1) Sparse Tansformer, <ref type=""bibr"" target=""#b5"">(Child et al., 2019)</ref> O",1
"sions hold after finetuning on downstream tasks. We finetune our Linformer on IMDB <ref type=""bibr"" target=""#b16"">(Maas et al., 2011)</ref> and SST-2 <ref type=""bibr"" target=""#b27"">(Socher et al., 2013)</ref> (sentiment classification), as well as QNLI (natural language inference) <ref type=""bibr"" t",0
"ning Transformers <ref type=""bibr"" target=""#b21"">(Ott et al., 2019)</ref>. This technique can be further improved through Quantization Aware Training <ref type=""bibr"" target=""#b11"">(Jacob et al., 2018;</ref><ref type=""bibr"" target=""#b8"">Fan et al., 2020)</ref>, where the weights are quantized during",0
"ibr"" target=""#b13"">Lewis et al., 2019)</ref>. Following <ref type=""bibr"" target=""#b6"">Devlin et al. (2019)</ref>, we pretrain our model on BookCorpus <ref type=""bibr"" target=""#b30"">(Zhu et al., 2015)</ref> plus English Wikipedia using masked-language-modeling objective. We observe similar pretrainin 5"">(Liu et al., 2019)</ref>, which is based on the Transformer. Following <ref type=""bibr"" target=""#b6"">Devlin et al. (2019)</ref>, we use BookCorpus <ref type=""bibr"" target=""#b30"">(Zhu et al., 2015)</ref> plus English Wikipedia as our pretraining set (3300M words). All models are pretrained with th",0
"ref type=""bibr"" target=""#b21"">22]</ref>. While early work mostly deals with simple graphs with unlabeled edges, recently proposed relation-aware GNNs <ref type=""bibr"" target=""#b37"">[38,</ref><ref type=""bibr"" target=""#b38"">39]</ref> consider multi-relational graphs with labels and directions on the e target=""#b24"">[25]</ref> and Weighted-GCN <ref type=""bibr"" target=""#b38"">[39]</ref> consider direction and relation types, respectively. Also, R-GCN <ref type=""bibr"" target=""#b37"">[38]</ref> considers direction and relation types simultaneously. Recently, Vashishth et al. <ref type=""bibr"" target=""# <ref type=""bibr"" target=""#b56"">[57,</ref><ref type=""bibr"" target=""#b44"">45]</ref>. 5) R-GCN. This is a GNN-based method for modeling relational data <ref type=""bibr"" target=""#b37"">[38]</ref>. 6) MEAN. 7) LAN. These are GNN models for a out-of-knowledge base task, which tackle unseen entities withou edge-conditioned convolution <ref type=""bibr"" target=""#b14"">[15]</ref>. 3) R-GCN. The same model used in the entity prediction on KG completion task <ref type=""bibr"" target=""#b37"">[38]</ref>. 4) I-GEN. Inductive GEN, which only uses feature representation of an entity e k , instead of a relation-en ed method for modeling relational data, which extends the graph convolutional network to consider multi-relational structure, by Schlichtkrull et al. <ref type=""bibr"" target=""#b37"">[38]</ref>.</p><p>6) MEAN. This model computes the embedding of entities by GNN based neighboring aggregation scheme, w , we use the basis decomposition on weight matrices W r and W r to prevent the excessive increase in the model size, proposed in Schlichtkrull et al. <ref type=""bibr"" target=""#b37"">[38]</ref>: W r = B b=1 a r b V b , where B is the number of basis, a r b is a coefficient of each relation r ∈ R and V =""bibr"" target=""#b44"">45]</ref> and deep neural network based methods <ref type=""bibr"" target=""#b30"">[31,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b37"">38,</ref><ref type=""bibr"" target=""#b29"">30]</ref>. While they require a large amount of training instances, many real-w h datasets, we consider the inverse relation as suggested by several recent works on multi-relational graphs <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b37"">38,</ref><ref type=""bibr"" target=""#b45"">46]</ref>, where directed relation information flows along with both directions ranked smaller than or equal to n. Moreover, as done in previous works <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b30"">31,</ref><ref type=""bibr"" target=""#b37"">38]</ref>, we measure the ranks in a filtered setting where we do not consider triplets that appeared in either trainin",1
"f>, mean pooling with layer-wise propagation rule <ref type=""bibr"" target=""#b21"">[22]</ref>, learnable attention-weighted combination of the features <ref type=""bibr"" target=""#b46"">[47]</ref>, to name a few. While most of the existing models work with simple undirected graphs, some recent work tackl",0
"ead><p>Graph Neural Network Existing GNNs encode the nodes by aggregating the features from the neighboring nodes, that use recurrent neural networks <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b36"">37]</ref>, mean pooling with layer-wise propagation rule <ref type=""bibr"" targ",0
"ibr"" target=""#b23"">[24]</ref> propose end-to-end GNNs to tackle this problem, which demonstrate comparatively better performance over non-GNN methods <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b33"">34,</ref><ref type=""bibr"" target=""#b57"">58]</ref>.</p><p>3 Few-Shot Out-Of-Graph",0
"lives around the globe. To eradicate the novel coronavirus, we may want to best utilize the accumulated knowledge about existing coronavirus variants <ref type=""bibr"" target=""#b52"">[53,</ref><ref type=""bibr"" target=""#b7"">8]</ref> by identifying the links between the seen (SARS and MERS) and unseen e",0
"er of buffer accesses, buffer size requirement, etc. We validated the performance statistics of MAESTRO against cycle-accurate RTL simulation results <ref type=""bibr"" target=""#b4"">5</ref> and reported performance in a previous work <ref type=""bibr"" target=""#b5"">6</ref> with the accuracy of 96.1% on and open-source repository. ? We validated MAESTRO's performance model against RTL simulation and reported processing delay of two accelerators-MAERI <ref type=""bibr"" target=""#b4"">5</ref> and Eyeriss 6 when running VGG16 and AlexNet, respectively. The latency estimated by MAESTRO are within 3.9% abs potential 37% latency and 10% energy reduction. Such an optimization opportunity can be exploited by flexible accelerators like Flexflow 15 and MAERI <ref type=""bibr"" target=""#b4"">5</ref> or via heterogeneous accelerators that employ multiple subaccelerators with various mapping styles in a single D",1
"cessing elements (PEs) and high energy efficiency by maximizing data reuse within PEs and on-chip scratchpads. <ref type=""bibr"" target=""#b0"">[1]</ref><ref type=""bibr"" target=""#b1"">[2]</ref><ref type=""bibr"" target=""#b2"">[3]</ref><ref type=""bibr"" target=""#b3"">[4]</ref> The efficiency (performance and",0
"oiting parallelism over hundreds of processing elements (PEs) and high energy efficiency by maximizing data reuse within PEs and on-chip scratchpads. <ref type=""bibr"" target=""#b0"">[1]</ref><ref type=""bibr"" target=""#b1"">[2]</ref><ref type=""bibr"" target=""#b2"">[3]</ref><ref type=""bibr"" target=""#b3"">[4] haviors, which is critical for energy efficiency, thus the prime optimization target of DNN accelerators. Data reuse pattern is dictated by dataflow, <ref type=""bibr"" target=""#b0"">1</ref> which are data/computation tile scheduling and spatial partitioning strategies without actual tile size as descr",0
"th of the shadow. There exists various kinds of causal model which could measure this causal relationship i.e. Linear Structual Equation Models (SEM) <ref type=""bibr"" target=""#b20"">(Shimizu et al., 2006)</ref>. Existing methods for disentangled representation learning like β-VAE <ref type=""bibr"" tar ttention in the past decades <ref type=""bibr"" target=""#b6"">(Hoyer et al., 2009;</ref><ref type=""bibr"" target=""#b24"">Zhang &amp; Hyvarinen, 2012;</ref><ref type=""bibr"" target=""#b20"">Shimizu et al., 2006)</ref>. <ref type=""bibr"" target=""#b19"">Pearl (2009)</ref> introduce a probabilistic graphical mode 006)</ref>. <ref type=""bibr"" target=""#b19"">Pearl (2009)</ref> introduce a probabilistic graphical model based framework to learn causality from data. <ref type=""bibr"" target=""#b20"">Shimizu et al. (2006)</ref> proposed an effective method called LiNGAM to learn the causal graph and they proved that t",1
"Mathieu et al., 2018;</ref><ref type=""bibr"" target=""#b14"">Locatello et al., 2018)</ref>. <ref type=""bibr"" target=""#b13"">Kulkarni et al. (2015)</ref>; <ref type=""bibr"" target=""#b15"">Locatello et al. (2019)</ref> use few labels to guide model training to reduce the parameter uncertainty. <ref type=""bi",0
"consider minimizing the mutual information between different latent factors. For example, <ref type=""bibr"" target=""#b4"">Higgins et al. (2017)</ref>; <ref type=""bibr"" target=""#b1"">Burgess et al. (2018)</ref> adjust the hyperparameter to force latent codes to be independent of each other. <ref type=""",0
"ey proved that the model is fully identifiable under the assumption that the causal relationship is linear and the noise is non-Gaussian distributed. <ref type=""bibr"" target=""#b25"">Zheng et al. (2018)</ref> introduces DAG constraints for graph learning under continuous optimization (NOTEARS). <ref t We introduce the acyclicity constraint. Instead of using traditional DAG constraint that is combinatorial, we adopt a continuous constraint function <ref type=""bibr"" target=""#b25"">(Zheng et al., 2018;</ref><ref type=""bibr"">Zhu &amp; Chen, 2019;</ref><ref type=""bibr"" target=""#b18"">Ng et al., 2019;</",0
"type=""bibr"" target=""#b25"">(Zheng et al., 2018;</ref><ref type=""bibr"">Zhu &amp; Chen, 2019;</ref><ref type=""bibr"" target=""#b18"">Ng et al., 2019;</ref><ref type=""bibr"" target=""#b23"">Yu et al., 2019)</ref> . The function achieves 0 if and only if the adjacency matrix A are directed acyclic graph <ref /ref><ref type=""bibr"" target=""#b23"">Yu et al., 2019)</ref> . The function achieves 0 if and only if the adjacency matrix A are directed acyclic graph <ref type=""bibr"" target=""#b23"">(Yu et al., 2019)</ref>.</p><formula xml:id=""formula_13"">H(A) ≡ tr((I + A • A) n ) − n = 0. (<label>11</label></formula pendulum, water) experiments are set to be 4, and 16 for CelebA experiments. The implementation of continuous DAG constraint H(A) follows the code of <ref type=""bibr"" target=""#b23"">(Yu et al., 2019)</ref>   </p></div><figure xmlns=""http://www.tei-c.org/ns/1.0"" xml:id=""fig_0""><head>Figure 1 .</head><",0
"mportance of DNNs' performance, researchers and industry practitioners have turned to search-based compilation <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b27"">29,</ref><ref type=""bibr"" target=""#b43"">45,</ref><ref type=""bibr"" target=""#b51""> s hard for compilation-based approaches to beat manually-written assembly code on large matrix multiplications <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b43"">45]</ref>, as the code has been hand-optimized for decades.  Ablation study. We , % ) Figure <ref type=""figure"">1</ref>: The computation definition of matrix multiplication. ple compiler techniques have been introduced (e.g., TVM <ref type=""bibr"" target=""#b9"">[10]</ref>, Halide <ref type=""bibr"" target=""#b36"">[38]</ref>, Tensor Comprehensions <ref type=""bibr"" target=""#b43"">[45]< ary exponential explosion of the search space. Typically, the compiler partitions the large computational graph of a DNN into several small subgraphs <ref type=""bibr"" target=""#b9"">[10]</ref>. This partition has a negligible effect on the performance thanks to the layer-by-layer construction nature o rget=""#b31"">33]</ref>. The latest one with beam search and learned cost model performs the best among them, which is also used in our evaluation. TVM <ref type=""bibr"" target=""#b9"">[10]</ref> utilizes a similar scheduling language and includes a template-guided search framework AutoTVM <ref type=""bib needs manual scheduling. TensorComprehensions can search for GPU code automatically, but it is not yet meant to be used for compute-bounded problems <ref type=""bibr"" target=""#b9"">[10]</ref>. It cannot outperform TVM on operators like conv2d and matmul <ref type=""bibr"" target=""#b9"">[10,</ref><ref ty meant to be used for compute-bounded problems <ref type=""bibr"" target=""#b9"">[10]</ref>. It cannot outperform TVM on operators like conv2d and matmul <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b42"">44]</ref>. This is because of the lack of certain optimizations and the inaccur tations of operators. The common optimizations at graph level include layout optimizations <ref type=""bibr"" target=""#b27"">[29]</ref>, operator fusion <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b33"">35]</ref>, constant folding <ref type=""bibr"" target=""#b37"">[39]</ref>, auto-bat",1
"rtant optimizations (e.g., operator fusion).</p><p>Polyhedral compilation models. Polyhedral compilation model <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b45"">47,</ref><ref type=""bibr"" target=""#b46"">48]</ref> formulates the optimization of programs as an integer linear programm",0
"or constructs the search space automatically. Traditional high-performance libraries such as ATLAS <ref type=""bibr"" target=""#b49"">[51]</ref> and FFTW <ref type=""bibr"" target=""#b15"">[16]</ref> also utilizes auto-tuning. More recent works NeuroVectorizer <ref type=""bibr"" target=""#b16"">[17]</ref> and A",0
"et=""#b30"">[32]</ref>.</p><p>Given the importance of DNNs' performance, researchers and industry practitioners have turned to search-based compilation <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b27"">29,</ref><ref type=""bibr"" target=""#b43"">4 ead><p>A cost model is necessary for estimating the performance of programs during the search. We adopt a learned cost model similar to related works <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b10"">11]</ref> with newly designed program features. A learned cost model has great p his language is suitable for both manual optimization and automatic search. Halide has three versions of auto-scheduler based on different techniques <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b26"">28,</ref><ref type=""bibr"" target=""#b31"">33]</ref>. The latest one with beam sear lexTensor <ref type=""bibr"" target=""#b51"">[53]</ref>) or aggressive pruning by inaccurately evaluating incomplete programs (e.g. Halide auto-scheduler <ref type=""bibr"" target=""#b1"">[2]</ref>), which prevents them from covering a large enough search space ( §2). The rules they use to construct the sea mpiler then uses an algorithm such as beam search <ref type=""bibr"" target=""#b29"">[31]</ref> to search for good decisions (e.g., Halide auto-scheduler <ref type=""bibr"" target=""#b1"">[2]</ref>). In this approach, the compiler constructs a tensor program by sequentially unfolding all nodes in the comput plates, so existing manual templates only cover a limited search space heuristically.</p><p>(2) Aggressive early pruning (e.g., Halide auto-scheduler <ref type=""bibr"" target=""#b1"">[2]</ref>). Aggressive early pruning based on evaluating incomplete programs prevents the search algorithm from explorin = 80) test cases. We run these test cases on the Intel CPU.</p><p>We include PyTorch <ref type=""bibr"" target=""#b34"">[36]</ref>, Halide auto-scheduler <ref type=""bibr"" target=""#b1"">[2]</ref>, Flex-Tensor <ref type=""bibr"" target=""#b51"">[53]</ref> and AutoTVM <ref type=""bibr"" target=""#b10"">[11]</ref> a te, which means the cost model performs very well for complete programs but fails to accurately predict the final performance of incomplete programs. <ref type=""bibr"" target=""#b1"">(2)</ref> The fixed order of sequential decisions limits the design of the search space. For example, some optimization",0
"e=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b27"">29,</ref><ref type=""bibr"" target=""#b43"">45,</ref><ref type=""bibr"" target=""#b51"">53]</ref> for automated generation of tensor programs, i.e. low-level implementations of tensor operators. For an opera tensor operators and hardware. It takes non-trivial research efforts <ref type=""bibr"" target=""#b27"">[29,</ref><ref type=""bibr"" target=""#b48"">50,</ref><ref type=""bibr"" target=""#b51"">53]</ref> to develop quality templates. Despite the huge efforts in developing templates, existing manual templates onl tion combinations, because they rely on either predefined manually-written templates (e.g., TVM <ref type=""bibr"" target=""#b10"">[11]</ref>, FlexTensor <ref type=""bibr"" target=""#b51"">[53]</ref>) or aggressive pruning by inaccurately evaluating incomplete programs (e.g. Halide auto-scheduler <ref type= l CPU.</p><p>We include PyTorch <ref type=""bibr"" target=""#b34"">[36]</ref>, Halide auto-scheduler <ref type=""bibr"" target=""#b1"">[2]</ref>, Flex-Tensor <ref type=""bibr"" target=""#b51"">[53]</ref> and AutoTVM <ref type=""bibr"" target=""#b10"">[11]</ref> as baseline frameworks.</p><p>PyTorch is backed by the nd includes a template-guided search framework AutoTVM <ref type=""bibr"" target=""#b10"">[11]</ref>. Similar to the motivation of this paper, FlexTensor <ref type=""bibr"" target=""#b51"">[53]</ref> attempts to reduce human efforts in writing templates. It proposes more general templates target-ing a set o",0
"uch representations, usually relying on visual pretext tasks. Among them, state-of-the-art contrastive methods <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b11"">12 #b12"">[13]</ref> by either relying on large batch sizes <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b11"">12]</ref>, memory banks <ref type=""bibr"" target=""#b8"">[9]</ref> or customized mining strategies <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b14"">15]</r rast, BYOL introduces an additional predictor on top of the online network, which prevents collapse.</p><p>Finally, in self-supervised learning, MoCo <ref type=""bibr"" target=""#b8"">[9]</ref> uses a slow-moving average network (momentum encoder) to maintain consistent representations of negative pairs τ ξ + (1 − τ )θ</formula><p>, where optimizer is an optimizer and η is a learning rate. At the end of training, we only keep the encoder f θ ; as in <ref type=""bibr"" target=""#b8"">[9]</ref>. When comparing to other methods, we consider the number of inference-time weights only in the final represent orms both the Supervised-IN baseline (+1.9 mIoU) and SimCLR (+1.1 mIoU).</p><p>Similarly, we evaluate on object detection by reproducing the setup in <ref type=""bibr"" target=""#b8"">[9]</ref> using a Faster R-CNN architecture <ref type=""bibr"" target=""#b81"">[82]</ref>, as detailed in Appendix E.5. We f <p>Simply adding a target network to SimCLR already improves performance (+1.6 points). This sheds new light on the use of the target network in MoCo <ref type=""bibr"" target=""#b8"">[9]</ref>, where the target network is used to provide more negative examples. Here, we show that by mere stabilization vel of detail required for image generation may not be necessary for representation learning.</p><p>Among discriminative methods, contrastive methods <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b31"">32,</ref><ref type=""bibr"" target=""#b32"">3 8"">[39,</ref><ref type=""bibr"" target=""#b39"">40]</ref>. Contrastive methods often require comparing each example with many other examples to work well <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b7"">8]</ref> prompting the question of whether using negative pairs is necessary.</p>",1
"arget=""#b7"">8,</ref><ref type=""bibr"" target=""#b11"">12]</ref>.</p><p>Our approach has some similarities with Predictions of Bootstrapped Latents (PBL, <ref type=""bibr"" target=""#b48"">[49]</ref>), a selfsupervised representation learning technique for reinforcement learning (RL). PBL jointly trains the",1
"ize the bootstrap mechanism in BYOL. While most RL methods use fixed target networks, BYOL uses a weighted moving average of previous networks (as in <ref type=""bibr"" target=""#b53"">[54]</ref>) in order to provide smoother changes in the target representation.</p><p>In the semi-supervised setting <re etwork provides the regression targets to train the online network, and its parameters ξ are an exponential moving average of the online parameters θ <ref type=""bibr"" target=""#b53"">[54]</ref>. More precisely, given a target decay rate τ ∈ [0, 1], after each training step we perform the following upd",1
"luate the representation learned by BYOL on ImageNet <ref type=""bibr"" target=""#b20"">[21]</ref> and other vision benchmarks using ResNet architectures <ref type=""bibr"" target=""#b21"">[22]</ref>. Under the linear evaluation protocol on ImageNet, consisting in training a linear classifier on top of the",0
"ype=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b11"">12]</ref> are trained by reducing the distance between representations of different augmented views of the same image ( l treatment of negative pairs <ref type=""bibr"" target=""#b12"">[13]</ref> by either relying on large batch sizes <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b11"">12]</ref>, memory banks <ref type=""bibr"" target=""#b8"">[9]</ref> or customized mining strategies <ref type=""bibr"" target choice of image augmentations <ref type=""bibr"" target=""#b33"">[34,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b11"">12]</ref>.</p><p>In this paper, we introduce Bootstrap Your Own Latent (BYOL), a new algorithm for self-supervised lear ce in self-supervised learning <ref type=""bibr"" target=""#b36"">[37,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b37"">38,</ref><ref type=""bibr"" target=""#b11"">12]</ref>. Contrastive approaches avoid a costly generation step in pixel space by bringing representation of different 48]</ref>, these methods are being outperformed by contrastive methods <ref type=""bibr"" target=""#b36"">[37,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b11"">12]</ref>.</p><p>Our approach has some similarities with Predictions of Bootstrapped Latents (PBL, <ref type=""bibr"" tar n the best and worst runs when it is larger than 0.25. Although previous works perform ablations at 100 epochs <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b11"">12]</ref>, we notice that relative improvements at 100 epochs do not always hold over longer training. For this reason, BYOL obtains 74.3% top-1 accuracy (91.6% top-5 accuracy), which is a 1.3% (resp. 0.5%) improvement over the previous self-supervised state of the art <ref type=""bibr"" target=""#b11"">[12]</ref>. This tightens the gap with respect to the supervised baseline of <ref type=""bibr"" target=""#b7"">[8]</ref>, 7",0
"particular, relative patch prediction <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b39"">40]</ref>, colorizing grayscale images <ref type=""bibr"" target=""#b40"">[41,</ref><ref type=""bibr"" target=""#b41"">42]</ref>, image inpainting <ref type=""bibr"" target=""#b42"">[43]</ref>, image j p of the frozen representation, following the procedure described in <ref type=""bibr"" target=""#b47"">[48,</ref><ref type=""bibr"" target=""#b73"">74,</ref><ref type=""bibr"" target=""#b40"">41,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b7"">8]</ref>, and appendix D.1; we report top",0
"ently achieve state-of-the-art performance in self-supervised learning <ref type=""bibr"" target=""#b36"">[37,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b37"">38,</ref><ref type=""bibr"" target=""#b11"">12]</ref>. Contrastive approaches avoid a costly generation step in pixel space",0
"""bibr"" target=""#b31"">32,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" target=""#b33"">34,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b34"">35,</ref><ref type=""bibr"" target=""#b35"">36]</ref> currently achieve state-of-the-art performance in self-supervised lea",0
"learned embeddings as image representations. Many of these approaches rely either on auto-encoding of images <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b25"">26]</ref> or on adversarial learning <ref type=""bibr"" target=""#b26"">[27]</ref>, epth <ref type=""bibr"" target=""#b39"">[40]</ref>. BYOL is better or on par with other methods for each metric. For instance, the challenging pct.&lt;1. <ref type=""bibr"" target=""#b24"">25</ref>   </p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""5"">Building intuitions with ablations</head><p>",0
"g more and more information in the online projection and avoids collapsed solutions.</p><p>We evaluate the representation learned by BYOL on ImageNet <ref type=""bibr"" target=""#b20"">[21]</ref> and other vision benchmarks using ResNet architectures <ref type=""bibr"" target=""#b21"">[22]</ref>. Under the /head><p>We assess the performance of BYOL's representation after self-supervised pretraining on the training set of the ImageNet ILSVRC-2012 dataset <ref type=""bibr"" target=""#b20"">[21]</ref>. We first evaluate it on ImageNet (IN) in both linear evaluation and semi-supervised setups. We then measure",0
"n visual pretext tasks. Among them, state-of-the-art contrastive methods <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b11"">12]</ref> are trained by reducing the di tion may not be necessary for representation learning.</p><p>Among discriminative methods, contrastive methods <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b31"">32,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" target=""#b33""> g the procedure described in <ref type=""bibr"" target=""#b47"">[48,</ref><ref type=""bibr"" target=""#b73"">74,</ref><ref type=""bibr"" target=""#b40"">41,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b7"">8]</ref>, and appendix D.1; we report top-1 and top-5 accuracies in % on the test org/ns/1.0""><head n=""3.2"">Intuitions on BYOL's behavior</head><p>As BYOL does not use an explicit term to prevent collapse (such as negative examples <ref type=""bibr"" target=""#b9"">[10]</ref>) while minimizing L BYOL θ,ξ with respect to θ, it may seem that BYOL should converge to a minimum of this lo to better understand where the improvement of BYOL over SimCLR comes from. Let us consider the following objective that extends the InfoNCE objective<ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b83"">84]</ref> (see Appendix G.4), InfoNCE α,β θ</note></figure> 			<note xmlns=""htt",0
"on by training a linear classifier on top of the frozen representation, following the procedure described in <ref type=""bibr"" target=""#b47"">[48,</ref><ref type=""bibr"" target=""#b73"">74,</ref><ref type=""bibr"" target=""#b40"">41,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b7"">8 on on a classification task with a small subset of ImageNet's train set, this time using label information. We follow the semi-supervised protocol of <ref type=""bibr"" target=""#b73"">[74,</ref><ref type=""bibr"" target=""#b75"">76,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b31""> mageNet-specific. We perform linear evaluation and fine-tuning on the same set of classification tasks used in <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b73"">74]</ref>, and carefully follow their evaluation protocol, as detailed in Appendix E. Performance is reported using sta",0
"ces <ref type=""bibr"" target=""#b16"">[17]</ref> or a handful of labels <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b19"">20]</ref>, we propose to directly bootstrap the representations. In particular, BYOL uses two neural networks, referred an unsupervised loss is combined with a classification loss over a handful of labels to ground the training <ref type=""bibr"" target=""#b18"">[19,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" target=""#b56"">57,</ref><ref type=""bibr"" target=""#b57"">58,</ref><ref type=""bibr"" target=""#b58"" bibr"" target=""#b59"">60,</ref><ref type=""bibr"" target=""#b60"">61,</ref><ref type=""bibr"" target=""#b61"">62]</ref>. Among these methods, mean teacher (MT) <ref type=""bibr"" target=""#b19"">[20]</ref> also uses a slow-moving average network, called teacher, to produce targets for an online network, called st work, called student. An 2 consistency loss between the softmax predictions of the teacher and the student is added to the classification loss. While <ref type=""bibr"" target=""#b19"">[20]</ref> demonstrates the effectiveness of MT in the semi-supervised learning case, in Section 5 we show that a simil e a predictor only mildly affects the performance of SimCLR.</p><p>Relationship with Mean Teacher Another semi-supervised approach, Mean Teacher (MT) <ref type=""bibr"" target=""#b19"">[20]</ref>, complements a supervised loss on few labels with an additional consistency loss. In <ref type=""bibr"" target ach, Mean Teacher (MT) <ref type=""bibr"" target=""#b19"">[20]</ref>, complements a supervised loss on few labels with an additional consistency loss. In <ref type=""bibr"" target=""#b19"">[20]</ref>, this consistency loss is the 2 distance between the logits from a student network, and those of a temporall",0
"""bibr"" target=""#b52"">, Ying et al., 2018]</ref>, and efficiently segmenting large point clouds <ref type=""bibr"" target=""#b45"">[Wang et al., 2018</ref><ref type=""bibr"" target=""#b29"">, Li et al., 2019b]</ref>. Recent works have looked at frameworks to train deeper GCN architectures <ref type=""bibr"">[L [He et al., 2016a</ref><ref type=""bibr"" target=""#b20"">, Huang et al., 2017</ref><ref type=""bibr"" target=""#b53"">, Yu and Koltun, 2016]</ref>, DeepGCNs <ref type=""bibr"" target=""#b29"">[Li et al., 2019b]</ref> propose to train very deep GCNs (56 layers) by adapting residual/dense connections (ResGCN/Den choose the message aggregation function ζ (l) (•) to be either SoftMax_Agg β (•) or PowerMean_Agg p (•).</p><p>Better Residual Connections. DeepGCNs <ref type=""bibr"" target=""#b29"">[Li et al., 2019b]</ref> show residual connections <ref type=""bibr"" target=""#b15"">[He et al., 2016a]</ref> to be quite malization <ref type=""bibr"" target=""#b1"">[Ba et al., 2016]</ref> is used in every layer before the activation function ReLU.</p><p>ResGCN. Similar to <ref type=""bibr"" target=""#b29"">Li et al. [2019b]</ref>, we construct ResGCN by adding residual connections to PlainGCN following the ordering: GraphGo",1
"orphism Network (GIN), with a sum aggregation that is able to have as large discriminative power as the Weisfeiler-Lehman (WL) graph isomorphism test <ref type=""bibr"" target=""#b47"">[Weisfeiler and Lehman, 1968]</ref>.</p><p>Training Deep GCNs. Despite the rapid and fruitful progress of GCNs, most pr on et al., 2017]</ref> and sum <ref type=""bibr"" target=""#b50"">[Xu et al., 2019b]</ref>. Inspired by the Weisfeiler-Lehman (WL) graph isomorphism test <ref type=""bibr"" target=""#b47"">[Weisfeiler and Lehman, 1968]</ref>, <ref type=""bibr"" target=""#b50"">Xu et al. [2019b]</ref> propose a theoretical frame",0
"ese works are however limited to 10 layers of depth before GCN performance would degrade. Inspired by the benefit of training deep CNN-based networks <ref type=""bibr"" target=""#b15"">[He et al., 2016a</ref><ref type=""bibr"" target=""#b20"">, Huang et al., 2017</ref><ref type=""bibr"" target=""#b53"">, Yu and or PowerMean_Agg p (•).</p><p>Better Residual Connections. DeepGCNs <ref type=""bibr"" target=""#b29"">[Li et al., 2019b]</ref> show residual connections <ref type=""bibr"" target=""#b15"">[He et al., 2016a]</ref> to be quite helpful in training very deep GCN architectures. They simply build the residual GC",0
"e the best performance. Graph attention networks (GATs) <ref type=""bibr"" target=""#b43"">[Veličković et al., 2018]</ref> employ the attention mechanism <ref type=""bibr"" target=""#b1"">[Bahdanau et al., 2015]</ref> to obtain different and trainable weights for neighbor nodes by learning the attention bet e in training deep GCNs. We apply normalization methods such as BatchNorm <ref type=""bibr"" target=""#b21"">[Ioffe and Szegedy, 2015]</ref> or LayerNorm <ref type=""bibr"" target=""#b1"">[Ba et al., 2016]</ref> to normalize vertex features. In addition to this, we also propose a message normalization (MsgN es the same message passing operator as GEN except the aggregation function is replaced by Sum(•), Mean(•) or Max(•) aggregation. Layer normalization <ref type=""bibr"" target=""#b1"">[Ba et al., 2016]</ref> is used in every layer before the activation function ReLU.</p><p>ResGCN. Similar to <ref type=""",0
"it of training deep CNN-based networks <ref type=""bibr"" target=""#b15"">[He et al., 2016a</ref><ref type=""bibr"" target=""#b20"">, Huang et al., 2017</ref><ref type=""bibr"" target=""#b53"">, Yu and Koltun, 2016]</ref>, DeepGCNs <ref type=""bibr"" target=""#b29"">[Li et al., 2019b]</ref> propose to train very de",0
"o train deeper GCNs is over-smoothing, first pointed out by <ref type=""bibr"">Li et al. [2018]</ref>. Recent works focus on addressing this phenomenon <ref type=""bibr"" target=""#b24"">[Klicpera et al., 2019</ref><ref type=""bibr"" target=""#b41"">, Rong et al., 2020</ref><ref type=""bibr"" target=""#b57"">, Zh 4"">[Klicpera et al., 2019</ref><ref type=""bibr"" target=""#b41"">, Rong et al., 2020</ref><ref type=""bibr"" target=""#b57"">, Zhao and Akoglu, 2020]</ref>. <ref type=""bibr"" target=""#b24"">Klicpera et al. [2019]</ref> proposes a PageRank-based message passing mechanism, involving the root node in the loop.",0
"ts as inputs. <ref type=""bibr"" target=""#b33"">Maron et al. [2019b]</ref> show the universality of invariant GNNs to any continuous invariant function. <ref type=""bibr"" target=""#b22"">Keriven and Peyré [2019]</ref> further extend it to the equivariant case. <ref type=""bibr"" target=""#b32"">Maron et al. [",0
"ion. In our experiments, we find normalization techniques play a crucial role in training deep GCNs. We apply normalization methods such as BatchNorm <ref type=""bibr"" target=""#b21"">[Ioffe and Szegedy, 2015]</ref> or LayerNorm <ref type=""bibr"" target=""#b1"">[Ba et al., 2016]</ref> to normalize vertex",0
"one important aspect that has not been studied adequately is the effect of the ordering of components, which has been shown to be quite important by <ref type=""bibr"" target=""#b16"">He et al. [2016b]</ref>. As suggested by <ref type=""bibr"" target=""#b16"">He et al. [2016b]</ref>, the output range of th of the ordering of components, which has been shown to be quite important by <ref type=""bibr"" target=""#b16"">He et al. [2016b]</ref>. As suggested by <ref type=""bibr"" target=""#b16"">He et al. [2016b]</ref>, the output range of the residual function should to be (−∞, +∞). Activation functions such as",0
"rns from T support i and then evaluates on T query i to see how well the model performs on that task. The goal of Model-Agnostic Meta-Learning (MAML) <ref type=""bibr"" target=""#b9"">[9]</ref> is to obtain a parameter initialization θ * that can adapt to unseen tasks quickly, such as D test , using gra meta-updated parameter θ * . θ * is learned from knowledge across meta-training tasks and is the optimal parameter to adapt to unseen tasks quickly.  <ref type=""bibr"" target=""#b9"">[9]</ref> switches ProtoNet to MAML as the meta-learner. All experiments use the same number of query points per label.",1
"xtensive training on datasets where majority of labels are available <ref type=""bibr"" target=""#b16"">[16,</ref><ref type=""bibr"" target=""#b43"">42,</ref><ref type=""bibr"" target=""#b45"">44]</ref>. In contrast, many problems require rapid learning from only a few labeled nodes or edges in the graph. Such e scarce. The GNN representations cannot fully capture large graphs structure because they are too complicated <ref type=""bibr"" target=""#b2"">[2,</ref><ref type=""bibr"" target=""#b45"">44]</ref>. However, they can learn representations that capture the structure of small graphs, such as our local subgra es local structural representations that enable direct structural similarity comparison using GNNs, based on its connection to Weisfeiler-Lehman test <ref type=""bibr"" target=""#b45"">[44,</ref><ref type=""bibr"" target=""#b49"">48]</ref>. Further, structural similarity enables G-META to form the much-need resentations that capture the structure of small graphs, such as our local subgraphs, as is evidenced by the connection to the Weisfeiler-Lehman test <ref type=""bibr"" target=""#b45"">[44,</ref><ref type=""bibr"" target=""#b49"">48]</ref>  <ref type=""formula"">3</ref>) Labels Labels Labels Labels Labels Lab ntroid node embedding instead of a readout function such as sum/mean over every node in the subgraph, as in the typical graph classification settings <ref type=""bibr"" target=""#b45"">[44,</ref><ref type=""bibr"" target=""#b4"">4]</ref>. The reason is that as we relax h to be a large number, the subgraph c",0
"ks (GNNs) have achieved remarkable results on many tasks such as recommender systems <ref type=""bibr"" target=""#b46"">[45]</ref>, molecular predictions <ref type=""bibr"" target=""#b15"">[15]</ref>, and knowledge graphs <ref type=""bibr"" target=""#b53"">[52]</ref>. Performance in such tasks is typically eval",0
"k r 4 = &lt; / l a t e x i t &gt; are specialized techniques specifically designed for a particular graph meta-learning problem and a particular task <ref type=""bibr"" target=""#b51"">[50,</ref><ref type=""bibr"" target=""#b2"">2,</ref><ref type=""bibr"" target=""#b19"">19,</ref><ref type=""bibr"" target=""#b5"">5 lso allow for effective feature propagation and label smoothing within a GNN.</p><p>(1) G-META is general. While previous meta graph learning methods <ref type=""bibr"" target=""#b51"">[50,</ref><ref type=""bibr"" target=""#b2"">2]</ref> apply only to one graph meta-learning problem (Figure <ref type=""figur ment because G-META's aggregation field is smaller than that of previous methods that operate on entire graphs <ref type=""bibr"" target=""#b2"">[2,</ref><ref type=""bibr"" target=""#b51"">50]</ref>. We further increase scalability by sub-sampling subgraphs with more than 1,000 nodes. ( <ref type=""formula""> graph meta-learning problems (Figure <ref type=""figure"">1</ref>) whereas previous methods apply to at most one <ref type=""bibr"" target=""#b2"">[2,</ref><ref type=""bibr"" target=""#b51"">50]</ref>. Unlike earlier methods, G-META works for node classification and also few-shot link prediction (i.e., via lo h. We use 16-shots for each task, i.e., using only 32 node pairs to predict links for an unseen graph. Hyperparameters are in Appendix G.  "" Meta-GNN <ref type=""bibr"" target=""#b51"">[50]</ref> "" Meta-Graph <ref type=""bibr"" target=""#b2"">[2]</ref> "" G-META (Ours) "" "" "" ""</p></div> <div xmlns=""http://ww",0
"nce of nodes' local network neighborhoods, prunes likely fake edges, and assigns less weight to suspicious edges based on network theory of homophily <ref type=""bibr"" target=""#b13"">[14]</ref>. The latter components stabilizes the evolution of graph structure by preserving, in part the memory from a pe=""bibr"" target=""#b18"">[19,</ref><ref type=""bibr"" target=""#b39"">40]</ref>), GNNGUARD determines importance weights using theory of network homophily <ref type=""bibr"" target=""#b13"">[14]</ref>, positing that similar nodes (i.e., nodes with similar features) are more likely to interact than dissimilar",1
"et=""#b44"">[45]</ref> and Citeseer <ref type=""bibr"" target=""#b45"">[46]</ref>. We also consider a directed graph with numeric node features, ogbn-arxiv <ref type=""bibr"" target=""#b46"">[47]</ref>, representing a citation network of CS papers published between 1971 and 2014. We use a Disease Pathway (DP)",0
"% in defense performance. Importantly, unlike existing GNN defenders <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b17"">18]</ref>, GNNGUARD is a general approach and can be effortlessly combined with defend targeted adversarial attacks on known and already existing GNNs, there has also been work on novel, robust GNN models. For example, RobustGCN <ref type=""bibr"" target=""#b16"">[17]</ref> is a novel GNN that adopts Gaussian distributions as the hidden representations of nodes in each convolution es fake edges as a GNN preprocessing step, GNNGUARD dynamically updates defense coefficients at every GNN layer for defense. In contrast to RobustGCN <ref type=""bibr"" target=""#b16"">[17]</ref>, which is limited to GCN, a particular GNN variant, and is challenging to use with other GNNs, GNNGUARD prov e defense algorithms. We compare GNNGUARD to three state-of-the-art graph defenders: GNN-Jaccard <ref type=""bibr"" target=""#b14"">[15]</ref>, RobustGCN <ref type=""bibr"" target=""#b16"">[17]</ref>, and GNN-SVD <ref type=""bibr"" target=""#b15"">[16]</ref>. Hyperparameters and model architectures are in Appen Graph-SAINT <ref type=""bibr"" target=""#b20"">[21]</ref>), baseline defense algorithms (GNN-Jaccard <ref type=""bibr"" target=""#b14"">[15]</ref>, RobustGCN <ref type=""bibr"" target=""#b16"">[17]</ref>, and GNN-SVD <ref type=""bibr"" target=""#b15"">[16]</ref>), and models for generating adversarial attacks (Nett",0
"raph Neural Networks (GNNs), in particular, have achieved remarkable success in a variety of application areas <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b3"">4]</ref>. The key to the success of GNNs is",0
"odes in the graph, and can catastrophically reduce the performance of even the strongest and most popular GNNs <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b9"">10]</ref>. This lack of GNN robustness is a critical issue in many application areas where adversarial perturbations can",0
"e dimension of z. Thus, we only need to calculate category loss term. To enable distribution q ϕ c (z|x q c ) differentiable, we follow previous work <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b18"">19]</ref> to use reparameterization trick",1
"calculate category loss term. To enable distribution q ϕ c (z|x q c ) differentiable, we follow previous work <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b18"">19]</ref> to use reparameterization trick to parameterize z. Reparameterization T",1
"category c.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2.2"">MAML</head><p>We give an overview of Model-Agnostic Meta-Learning method <ref type=""bibr"" target=""#b11"">[12]</ref> which is a representative algorithm of optimization-based metalearning approaches. First, we use our problem fit our framework is a key problem. To tackle this problem, we propose to encode the information from support set into our parameter inspired by MAML <ref type=""bibr"" target=""#b11"">[12]</ref> and further we can obtain a category-specific model to accelerate unseen category adaptation. We will introd entropy minimization, and then conduct inference on testing query data.</p><p>•Meta-Learning We select two state-of-the-art meta learning models MAML <ref type=""bibr"" target=""#b11"">[12]</ref> and Meta-SGD <ref type=""bibr"" target=""#b20"">[21]</ref> as baselines. The model architectures of two baseline tion-based methods aim to modify the gradient descent based learning procedure for new task quick adaptation. In the optimization-based methods, MAML <ref type=""bibr"" target=""#b11"">[12]</ref> is a recent promising model which learns a set of model parameters that are used to rapidly learn novel task",1
"∫ z p(z, y c |x c )dz requires integrating or summing over a potentially exponential number of configurations for z. As with variational autoencoders <ref type=""bibr"" target=""#b18"">[19]</ref>, we approximate the objective function using the evidence lower bound (ELBO) on the log likelihood. For the ansformers and then feed them into two fully connected layers with weight matrix W 2d ×d µ and W 2d ×d σ to output mean µ and log(σ ) as suggested in <ref type=""bibr"" target=""#b18"">[19]</ref>.</p><p>Decoder The decoder is a fully connected layer with weight matrix W d ×2 o to take samples from infer le distribution q ϕ c (z|x q c ) differentiable, we follow previous work <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b18"">19]</ref> to use reparameterization trick to parameterize z. Reparameterization Trick Instead of directly sampling from",1
"d Walmart) are contributed by individual retailers, the catalog information unavoidably contains noisy facts <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b30"">31]</ref>. The existence of such errors results in misleading information delivered to consumers and significantly down",0
"pus have promoted the development of many different neural NLI models <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b23"">24]</ref> that achieve promising performance. However, NLI task usually require",0
"is not optimized to allow fine-tuning with limited supervision and such models can still require large amounts of task-specific data for fine-tuning <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b28"">29]</ref>. Thus, how to train a NLI model with a small set of dataset for a spec",0
"ce of anomaly detection due to their powerful abilities <ref type=""bibr"" target=""#b7"">[8]</ref>. The deep learning anomaly detection (DAD) approaches <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b29"">30]</ref> model the log data as a natural language sequence and apply RNN and CN",0
"nce of predictions and is commonly used in the semisupervised learning <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b19"">20]</ref> and domain adaptation <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref t",0
"selines are deterministic. We implement all the deep learning baselines and the proposed approach with Py-Torch 1.2. For training models, we use Adam <ref type=""bibr"" target=""#b17"">[18]</ref> optimizer in the default setting. The learning rate α is 0.0001. We use mini-batch size of 64 and training e",0
"div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.3"">Implementation Details</head><p>The 300 dimensional FastText pre-trained word-embedding weights <ref type=""bibr"" target=""#b4"">[5]</ref> are used to initialize the parameters of the word embedding layer for deep learning models except for BERT. Th",0
"ropy minimization encourages the confidence of predictions and is commonly used in the semisupervised learning <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b19"">20]</ref> and domain adaptation <ref type=""bibr"" target=""#b15"">[16,</ref><ref t ing, the training data include unlabeled support data and labeled query data. We use the entropy minimization to define the loss on unlabeled data as <ref type=""bibr"" target=""#b14"">[15]</ref> and use the cross-entropy to define the loss on labeled data. The ratio of labeled loss and unlabeled loss i",0
"WORK</head><p>Attribute validation task is related to anomaly detection which aims to find patterns in data that do not conform to expected behavior <ref type=""bibr"" target=""#b8"">[9]</ref>. In the anomaly detection, the most related line of research is log anomaly detection which aims to find text,",0
"with limited supervision and such models can still require large amounts of task-specific data for fine-tuning <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b28"">29]</ref>. Thus, how to train a NLI model with a small set of dataset for a specific domain is still a very challenging",0
"the support set. Thus, instead of using prior p(z) in Eq. 2, we propose to use a more informative conditional prior distribution p(z|x s c ) as with <ref type=""bibr"" target=""#b12"">[13]</ref> and further rewrite our objective function as follows:</p><formula xml:id=""formula_4"">log p Θ (y c |x c ) =",0
"long been proposed as a form of learning that would allow systems to systematically build up and re-use knowledge across different but related tasks <ref type=""bibr"" target=""#b24"">[25]</ref>. More specifically, meta-Learning approaches can be broadly classified into three categories: optimization-b",0
"t related line of research is log anomaly detection which aims to find text, which can indicate the reasons and the nature of the failure of a system <ref type=""bibr"" target=""#b7"">[8]</ref>. The traditional methods typically extract features from unstructured texts and then detect anomalies based on with traditional learning, deep learning models have achieved an improvement in the performance of anomaly detection due to their powerful abilities <ref type=""bibr"" target=""#b7"">[8]</ref>. The deep learning anomaly detection (DAD) approaches <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr""",0
"pair that are incorrect for product I .</p><p>After defining our problem, we introduce our learning setting. Following the few-shot learning setting <ref type=""bibr"" target=""#b25"">[26]</ref>, in each category c ∼ C, we have a few unlabeled examples x s c = {x s c,i } N i=1 to constitute the support",0
"lse based on a text statement. Recently, powerful neural network based models, such as Transformer <ref type=""bibr"" target=""#b23"">[24]</ref> and BERT <ref type=""bibr"" target=""#b10"">[11]</ref> have shown promising performance towards NLI task. However, their success relies on sufficient high-quality length of two parts are usually very different, we use two Transformers to take two parts separately to obtain fixed-dimensional features. Following <ref type=""bibr"" target=""#b10"">[11]</ref>, the first token of every sequence is always a special classification token ([CLS]). Accordingly, the final m. We select three state-of-the-art models ESIM <ref type=""bibr"" target=""#b9"">[10]</ref>, Transformer <ref type=""bibr"" target=""#b23"">[24]</ref>, BERT <ref type=""bibr"" target=""#b10"">[11]</ref> as baselines. All sublayers of ESIM produce the output with dimension d = 16 except the last output layer. F target=""#b27"">[28]</ref> (MultiNLI) corpus have promoted the development of many different neural NLI models <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b23"">24]</ref> that achieve promising perfor",0
"werful abilities <ref type=""bibr"" target=""#b7"">[8]</ref>. The deep learning anomaly detection (DAD) approaches <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b29"">30]</ref> model the log data as a natural language sequence and apply RNN and CNN to detect anomalies. Different with l",0
"<ref type=""bibr"" target=""#b14"">(Kipf and Welling, 2017;</ref><ref type=""bibr"" target=""#b34"">Veličković et al., 2018)</ref> or variants of Transformer <ref type=""bibr"" target=""#b33"">(Vaswani et al., 2017)</ref> that apply self-attention on all nodes together, including those that are not directly con bibr"" target=""#b38"">(Zhu et al., 2019;</ref><ref type=""bibr"" target=""#b7"">Cai and Lam, 2020)</ref> base their encoder on the Transformer architecture <ref type=""bibr"" target=""#b33"">(Vaswani et al., 2017)</ref> and thus, in each layer, compute self-attention on all nodes, not only direct neighbors, f ns/1.0""><head n=""3"">The Graformer Model</head><p>Graformer follows the general multi-layer encoderdecoder pattern known from the original Transformer <ref type=""bibr"" target=""#b33"">(Vaswani et al., 2017)</ref>. In the following, we first describe our formalization of the KG input and then how it is elf-attention. Note that the formulas describe the computations for one head. The output of multiple heads is combined as in the original Transformer <ref type=""bibr"" target=""#b33"">(Vaswani et al., 2017)</ref>.</p><p>Text self-attention. <ref type=""bibr"" target=""#b30"">Shaw et al. (2018)</ref> introd </div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.4"">Graformer decoder</head><p>Our decoder follows closely the standard Transformer decoder <ref type=""bibr"" target=""#b33"">(Vaswani et al., 2017)</ref>, except for the modifications suggested by Chen et al. <ref type=""bibr"">(2018)</ref>. Hidd l(n i ) E where e l(n i ) is the one-hotencoding of the ith node's label.</p><p>To compute the node representation H (L) in the Lth layer, we follow <ref type=""bibr"" target=""#b33"">Vaswani et al. (2017)</ref>, i.e., we first normalize the input from the previous layer H (L−1) via layer normalization",1
"f type=""bibr"" target=""#b6"">(Bhowmik and de Melo, 2018)</ref>, datato-document generation <ref type=""bibr"" target=""#b21"">(Moryossef et al., 2019;</ref><ref type=""bibr"" target=""#b15"">Koncel-Kedziorski et al., 2019)</ref> and interpretability of KGs in general <ref type=""bibr"" target=""#b29"">(Schmitt et ion patterns. We call this new architecture Graformer.</p><p>Following previous work, we evaluate Graformer on two benchmarks: (i) the AGENDA dataset <ref type=""bibr"" target=""#b15"">(Koncel-Kedziorski et al., 2019)</ref>, i.e., the generation of scientific abstracts from automatically extracted entit ://www.tei-c.org/ns/1.0""><head n=""4.1"">Datasets</head><p>We evaluate our new architecture on two popular benchmarks for KG-to-text generation, AGENDA <ref type=""bibr"" target=""#b15"">(Koncel-Kedziorski et al., 2019)</ref> and WebNLG <ref type=""bibr"" target=""#b10"">(Gardent et al., 2017)</ref>. While th graph's topology as the encoder in their encoder-decoder architectures <ref type=""bibr"" target=""#b20"">(Marcheggiani and Perez-Beltrachini, 2018;</ref><ref type=""bibr"" target=""#b15"">Koncel-Kedziorski et al., 2019;</ref><ref type=""bibr"" target=""#b27"">Ribeiro et al., 2019;</ref><ref type=""bibr"" target= h different random seeds.</p><p>Our model outperforms previous Transformerbased models that only consider first-order neighborhoods per encoder layer <ref type=""bibr"" target=""#b15"">(Koncel-Kedziorski et al., 2019;</ref><ref type=""bibr"" target=""#b2"">An et al., 2019)</ref> The results on the test set",0
"cally created by applying an information extraction tool <ref type=""bibr"" target=""#b19"">(Luan et al., 2018)</ref> on a corpus of scientific abstracts <ref type=""bibr"" target=""#b1"">(Ammar et al., 2018)</ref>. As this process is noisy, we corrected 7 train instances where an entity name was erroneousl",0
"loading scheme inspired by the bucketing approach of Koncel-Kedziorski et al. ( <ref type=""formula"">2019</ref>) and length-based curriculum learning <ref type=""bibr"" target=""#b24"">(Platanios et al., 2019)</ref>: We sort the train set by target text length and split it into four buckets of two times",0
"pochs on AGENDA and 200 epochs on WebNLG. We report test results for the model yielding the best validation performance measured in corpus-level BLEU <ref type=""bibr"" target=""#b22"">(Papineni et al., 2002)</ref>. For model selection, we decode greedily. The final results are generated by beam search. org/ns/1.0""><head n=""5.1"">Overall performance</head><p>Table <ref type=""table"">2</ref> shows the results of our evaluation on AGENDA in terms of BLEU <ref type=""bibr"" target=""#b22"">(Papineni et al., 2002)</ref>, METEOR <ref type=""bibr"" target=""#b4"">(Banerjee and Lavie, 2005)</ref>, and CHRF++ <ref t mance of previous runs at the same epoch and if so, abort. For hyperparameter tuning, we decode greedily and measure performance in corpus-level BLEU <ref type=""bibr"" target=""#b22"">(Papineni et al., 2002)</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>B Qualitative examples</head><p",0
"o explicitly encode structural roles as part of message passing, in order to capture richer topological properties. Our method draws inspiration from <ref type=""bibr"" target=""#b32"">[33]</ref>, where it was shown that GNNs become universal when the vertices in the graph are uniquely identified, i.e w =""bibr"" target=""#b34"">[35]</ref>, <ref type=""bibr"" target=""#b35"">[36]</ref>.</p><p>It is important to note here that contrary to identifierbased GNNs <ref type=""bibr"" target=""#b32"">[33]</ref>, <ref type=""bibr"" target=""#b36"">[37]</ref>, <ref type=""bibr"" target=""#b37"">[38]</ref> that obtain universali d approaches. Whenever these subgraph counts can provide a unique identification of the vertices, then universality will also hold (Corollary 3.1. in <ref type=""bibr"" target=""#b32"">[33]</ref>).</p><p>We conjecture, that in real-world scenarios the number of subgraphs needed for unique, or near-uniqu ul variants and provided generalisation bounds.</p><p>Unique identifiers. From a different perspective, <ref type=""bibr"" target=""#b67"">[68]</ref> and <ref type=""bibr"" target=""#b32"">[33]</ref> showed the connections between GNNs and distributed local algorithms <ref type=""bibr"" target=""#b68"">[69]</re",1
"#b3"">[4]</ref>, <ref type=""bibr"" target=""#b4"">[5]</ref>, <ref type=""bibr"" target=""#b5"">[6]</ref> and physics <ref type=""bibr"" target=""#b6"">[7]</ref>, <ref type=""bibr"" target=""#b7"">[8]</ref>, to name a few. Most GNN architectures are based on message passing <ref type=""bibr"" target=""#b4"">[5]</ref>, w",0
"<head n=""5.3"">ZINC Molecular graphs</head><p>We evaluate GSN on the task of regressing the ""penalized water-octanol partition coefficient -logP"" (see <ref type=""bibr"" target=""#b110"">[111]</ref>, <ref type=""bibr"" target=""#b111"">[112]</ref>, <ref type=""bibr"" target=""#b112"">[113]</ref> for details) of",0
"with the best average accuracy across the 10 folds. Table <ref type=""table"" target=""#tab_0"">1</ref> lists all the methods evaluated with the split of <ref type=""bibr"" target=""#b100"">[101]</ref>. We select our model by tuning architecture and optimisation hyperparameters and substructure related para",0
"concept of graphlets <ref type=""bibr"" target=""#b43"">[44]</ref>, <ref type=""bibr"" target=""#b79"">[80]</ref>, <ref type=""bibr"" target=""#b80"">[81]</ref>, <ref type=""bibr"" target=""#b81"">[82]</ref>, different from motifs in being induced subgraphs, has been used to analyse the distribution of realworld ne",0
"the number of comparisons to random subsets of images during training <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b55"">56]</ref>. An alternative to approximate the loss is to approximate the task-that is to relax the instance discriminati evel classification considers each image in a dataset as its own class <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b55"">56]</ref>. Dosovitskiy et al. <ref type=""bibr"" target=""#b15"">[16]</ref> assign a class explicitly to each image and lea hod to scale to potentially unlimited amounts of data. In addition, SwAV works with small and large batch sizes and does not need a large memory bank <ref type=""bibr"" target=""#b55"">[56]</ref> or a momentum encoder <ref type=""bibr"" target=""#b23"">[24]</ref>.</p><p>Besides our online clustering-based m ly to each image and learn a linear classifier with as many classes as images in the dataset. As this approach becomes quickly intractable, Wu et al. <ref type=""bibr"" target=""#b55"">[56]</ref> mitigate this issue by replacing the classifier with a memory bank that stores previously-computed represent where we enforce consistency between codes from different augmentations of the same image. This solution is inspired by contrastive instance learning <ref type=""bibr"" target=""#b55"">[56]</ref> as we do not consider the codes as a target, but only enforce consistent mapping between views of the same i uld be possible to predict the code from the other feature. A similar comparison appears in contrastive learning where features are compared directly <ref type=""bibr"" target=""#b55"">[56]</ref>. In Fig. <ref type=""figure"" target=""#fig_0"">1</ref>, we illustrate the relation between contrastive learning "">(k) t = exp 1 τ z t c k k exp 1 τ z t c k . (<label>2</label></formula><formula xml:id=""formula_3"">)</formula><p>where τ is a temperature parameter <ref type=""bibr"" target=""#b55"">[56]</ref>. Taking this loss over all the images and pairs of data augmentations leads to the following loss function f computed during the previous epoch instead of dedicating pass forwards to the assignments. This is similar to the memory bank introduced by Wu et al. <ref type=""bibr"" target=""#b55"">[56]</ref>, without momentum.</p><p>Assignment phase in DeepCluster-v2. DeepCluster-v2 uses spherical k-means to get ps > <div xmlns=""http://www.tei-c.org/ns/1.0""><head>B.6 Image classification with KNN classifiers on ImageNet</head><p>Following previous work protocols <ref type=""bibr"" target=""#b55"">[56,</ref><ref type=""bibr"" target=""#b65"">66]</ref>, we evaluate the quality of our unsupervised features with K-nearest",1
"this issue by replacing the classifier with a memory bank that stores previously-computed representations. They rely on noise contrastive estimation <ref type=""bibr"" target=""#b21"">[22]</ref> to compare instances, which is a special form of contrastive learning <ref type=""bibr"" target=""#b27"">[28,</r",0
"ervised learning on ImageNet with a ResNet-50. We finetune the model with 1% and 10% labels and report top-1 and top-5 accuracies. *: uses RandAugment<ref type=""bibr"" target=""#b11"">[12]</ref>.</figDesc><table><row><cell></cell><cell></cell><cell cols=""2"">1% labels</cell><cell cols=""2"">10% labels</ce",0
"using the optimized implementation with kernels through CUDA/C-v2 extension from apex -2 . We also use apex library for training with mixed precision <ref type=""bibr"" target=""#b40"">[41]</ref>. Overall, thanks to these training optimizations (mixed precision, kernel batch-normalization and use of lar",0
"n multiple datasets. We implement in SwAV the improvements used in SimCLR, i.e., LARS <ref type=""bibr"" target=""#b61"">[62]</ref>, cosine learning rate <ref type=""bibr"" target=""#b37"">[38,</ref><ref type=""bibr"" target=""#b41"">42]</ref> and the MLP projection head <ref type=""bibr"" target=""#b9"">[10]</ref> 61"">[62]</ref> and a learning rate of 4.8 which is linearly ramped up during the first 10 epochs. After warmup, we use the cosine learning rate decay <ref type=""bibr"" target=""#b37"">[38,</ref><ref type=""bibr"" target=""#b41"">42]</ref> with a final value of 0.0048. To help the very beginning of the opti a weight decay of 10 −6 , LARS optimizer <ref type=""bibr"" target=""#b61"">[62]</ref> and a learning rate of 0.6. We use the cosine learning rate decay <ref type=""bibr"" target=""#b37"">[38]</ref> with a final value of 0.0006.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>A.7 Implementation de",0
"?</p><p>The similar question has also been asked and pursued in natural language processing <ref type=""bibr"" target=""#b9"">[10]</ref>, computer vision <ref type=""bibr"" target=""#b16"">[17]</ref>, and other domains. To date, the most powerful solution is to pre-train a representation learning model from ances from dissimilar instances. Next, we introduce how to define (dis)similar instances.</p><p>Q2: Define (dis)similar instances. In computer vision <ref type=""bibr"" target=""#b16"">[17]</ref>, two random data augmentations (e.g., random crop, random resize, random color jitering, random flip, etc) o ""#b41"">[42]</ref> as vertex features. After encoded by the graph encoder, the final d-dimensional output vectors are then normalized by their L2-Norm <ref type=""bibr"" target=""#b16"">[17]</ref>.</p><p>A running example. We illustrate a running example of GCC in Figure <ref type=""figure"">2</ref>. For s sually design and adopt economical strategies to effectively build and maintain the dictionary, such as end-to-end (E2E) and momentum contrast (MoCo) <ref type=""bibr"" target=""#b16"">[17]</ref>. We discuss the two strategies as follows.</p><p>E2E samples mini-batches of instances and considers samples es the parameters of f q (denoted by θ q ) by backpropagation. The parameters of f k (denoted by θ k ) are not updated by gradient descent. He et al. <ref type=""bibr"" target=""#b16"">[17]</ref> propose a momentum-based update rule for θ k . More formally, MoCo updates θ k by θ k ← mθ k + (1 − m)θ q , other contrastive learning mechanisms to maintain the dictionary, such as memory bank <ref type=""bibr"" target=""#b58"">[59]</ref>. Recently, He et al. <ref type=""bibr"" target=""#b16"">[17]</ref> show that MoCo is a more effective option than memory bank in computer vision tasks. Therefore, we mainly fo een pre-training data and down-stream tasks.</p><p>Contrastive loss mechanisms. The common belief is that MoCo has stronger expression power than E2E <ref type=""bibr"" target=""#b16"">[17]</ref>, and a larger dictionary size K always helps. We also observe such trends, as shown in Figure <ref type=""fig <ref type=""figure"" target=""#fig_1"">3</ref>. However, the effect of a large dictionary size is not as significant as reported in computer vision tasks <ref type=""bibr"" target=""#b16"">[17]</ref>. For example, MoCo (K = 16384) merely outperforms MoCo (K = 1024) by small margins in terms of accuracy -1.0 needs 9 hours. Detailed training time can be found in Table <ref type=""table"" target=""#tab_6"">5</ref> in the Appendix. Momentum. As mentioned in MoCo <ref type=""bibr"" target=""#b16"">[17]</ref>, momentum m plays a subtle role in learning high-quality representations. Table <ref type=""table"" target=""#t erent momentum values on US-Airport and COL-LAB datasets. For US-Airport, the best performance is reached by m = 0.999, which is the desired value in <ref type=""bibr"" target=""#b16"">[17]</ref>, showing that building a consistent dictionary is important for MoCo. However, in COLLAB, it seems that a la , in COLLAB, it seems that a larger momentum value brings better performance. Moreover, we do not observe the ""training loss oscillation"" reported in <ref type=""bibr"" target=""#b16"">[17]</ref> when setting m = 0. GCC (MoCo) converges well, but the accuracy is much worse.</p><p>Pre-training datasets. g words and negative sampling to learn word embeddings.</p><p>In computer vision, a large collection of work <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b49"">50,</ref><ref type=""bibr"" target=""#b58"">59</ref>] learns self-supervised image o design proper self-supervised tasks and learning objectives for graph structured data. Inspired by the recent success of contrastive learning in CV <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b58"">59]</ref> and NLP <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" targ ce of pre-training task and learning objective treats each instance as a distinct class of its own and learns to discriminate between these instances <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b58"">59]</ref>. The promise is that it can output instance representations that are",1
"Classification Datasets. 10  We download COLLAB, IMDB-BINARY, IMDB-MULTI, REDDIT-BINARY and REDDIT-MULTI5K from Benchmark Data Sets for Graph Kernels <ref type=""bibr"" target=""#b22"">[23]</ref>.</p></div>			</div> 			<div type=""references"">  				<listBibl>  <biblStruct xml:id=""b0""> 	<analytic> 		<titl",0
"scratch. For example, for node classification on the US-Airport network, GCC pre-trained on the Facebook, IMDB, and DBLP graphs outperforms GraphWave <ref type=""bibr"" target=""#b11"">[12]</ref>, ProNE <ref type=""bibr"" target=""#b64"">[65]</ref> and Struc2vec <ref type=""bibr"" target=""#b42"">[43]</ref> whi xplicit featurization. The second line of research leverages the spectral graph theory to model structural similarity. A recent example is Graph-Wave <ref type=""bibr"" target=""#b11"">[12]</ref>. In this work, we focus on structural similarity. Unlike the above two genres, we adopt contrastive learning tic regression or SVM. Examples include DGK <ref type=""bibr"" target=""#b60"">[61]</ref>, Struc2vec <ref type=""bibr"" target=""#b42"">[43]</ref>, GraphWave <ref type=""bibr"" target=""#b11"">[12]</ref>, graph2vec <ref type=""bibr"" target=""#b32"">[33]</ref> and InfoGraph <ref type=""bibr"" target=""#b45"">[46]</ref> the author is above or below the median.</p><p>Experimental results. We compare GCC with ProNE <ref type=""bibr"" target=""#b64"">[65]</ref>, Graph-Wave <ref type=""bibr"" target=""#b11"">[12]</ref>, and Struc2vec <ref type=""bibr"" target=""#b42"">[43]</ref>. Table <ref type=""table"" target=""#tab_1"">2</ref> re ferent methods. We compare GCC with RolX <ref type=""bibr"" target=""#b17"">[18]</ref>, Panther++ <ref type=""bibr"" target=""#b65"">[66]</ref> and GraphWave <ref type=""bibr"" target=""#b11"">[12]</ref>. We also provide random guess results for reference.</p><p>Experimental Results. Table <ref type=""table"" tar similarity search task in three co-author networks. We can see that, compared with Panther++ <ref type=""bibr"" target=""#b65"">[66]</ref> and GraphWave <ref type=""bibr"" target=""#b11"">[12]</ref> which are trained in place on co-author graphs, simply applying pre-trained GCC can be competitive.</p><p>Ov dictionary size. 7 The table shows the elapsed real time for pre-training, which might be affected by other programs running on the server. GraphWave <ref type=""bibr"" target=""#b11"">[12]</ref> We download the authors' official source code and keep all the training settings as the same. The implementa ults for these baselines from Zhang et al. <ref type=""bibr"" target=""#b65"">[66]</ref>.</p><p>Code: https://github.com/yuikns/panther/.</p><p>GraphWave <ref type=""bibr"" target=""#b11"">[12]</ref> Embeddings computed by the GraphWave method also have the ability to generalize across graphs. The authors e earning tasks -node classification, graph classification, and similarity search, which have been commonly used to benchmark graph learning algorithms <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b42"">43,</ref><ref type=""bibr"" target=""#b45"">46,</ref><ref type=""bibr"" target=""#b59",0
"ng, the restart probability controls the radius of ego-network (i.e., r ) which GCC conducts data augmentation on. In this work, we follow Qiu et al. <ref type=""bibr"" target=""#b41"">[42]</ref> to use 0.8 as the restart probability. The proposed GCC framework is flexible to other graph sampling algori al embedding, we also add the one-hot encoding of vertex degrees <ref type=""bibr"" target=""#b59"">[60]</ref> and the binary indicator of the ego vertex <ref type=""bibr"" target=""#b41"">[42]</ref> as vertex features. After encoded by the graph encoder, the final d-dimensional output vectors are then norm",0
"epresentation learning models aim to learn network-specific structural patterns dedicated for each dataset. For example, the DeepWalk embedding model <ref type=""bibr"" target=""#b38"">[39]</ref> learned on the Facebook social graph cannot be applied to other graphs. In view of <ref type=""bibr"" target="" target=""#b20"">[21]</ref>, etc. Most recently developed network embedding algorithms, such as LINE <ref type=""bibr"" target=""#b47"">[48]</ref>, DeepWalk <ref type=""bibr"" target=""#b38"">[39]</ref>, node2vec <ref type=""bibr"" target=""#b13"">[14]</ref>, also follow the neighborhood similarity assumption.</p> ork embedding models inspired by Word2vec <ref type=""bibr"" target=""#b29"">[30]</ref>, such as LINE <ref type=""bibr"" target=""#b47"">[48]</ref>, DeepWalk <ref type=""bibr"" target=""#b38"">[39]</ref>, node2vec <ref type=""bibr"" target=""#b13"">[14]</ref>, and metapath2vec <ref type=""bibr"" target=""#b10"">[11]</r structural representation model and apply it to unseen graphs, differing from traditional network embedding <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b38"">39,</ref><ref type=""bibr"" target=""#b47"">48]</ref> and recent attempts on pre-training graph neural networks with attrib",0
"me to encourage the student network to mimic the teacher network's label predictions. Thus, the distillation <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b22"">23]</ref> phase of our method using unlabeled data is reminiscent of the use of pseudo labels <ref type=""bibr"" target="" examples.</head><p>To further improve the network for the target task, here we leverage the unlabeled data directly for the target task. Inspired by <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b23 pare against in Appendix G. Our work also extends the ""unsupervised pretrain, supervised fine-tune"" paradigm by combining it with (self-)distillation <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b10"">11]</ref> using unlabeled data.</p><p>",0
"e=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b1"">2]</ref> or under different data augmentations <ref type=""bibr"" target=""#b12"">[13]</ref><ref type=""bibr"" target=""#b13"">[14]</ref><ref type=""bibr"" target=""#b14"">[15]</ref>.</p><p>Motivated by recent",0
"tead of a generative one as in <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b44"">45,</ref><ref type=""bibr"" target=""#b45"">46,</ref><ref type=""bibr"" target=""#b38"">39,</ref><ref type=""bibr"" target=""#b46"">47]</ref>. There are other approaches to self-supervised learning that are base",0
"th as well as whether or not to use selective kernels (SK) <ref type=""bibr"" target=""#b27"">[28]</ref>. 4 Whenever SK is used, we also use the ResNet-D <ref type=""bibr"" target=""#b35"">[36]</ref> variant of ResNet. The smallest model is the standard ResNet-50, and biggest model is ResNet-152 (3×+SK).</p",0
"cent advances in self-supervised learning of visual representations <ref type=""bibr"" target=""#b15"">[16]</ref><ref type=""bibr"" target=""#b16"">[17]</ref><ref type=""bibr"" target=""#b17"">[18]</ref><ref type=""bibr"" target=""#b18"">[19]</ref><ref type=""bibr"" target=""#b19"">[20]</ref><ref type=""bibr"">1]</ref>, etween Linear Evaluation and Fine-tuning</head><p>Most existing work <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b39"">40,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"">1]</ref> on s",0
"tation learning paradigm, there is a large and diverse set of approaches for semi-supervised learning, we refer readers to <ref type=""bibr"">[52]</ref><ref type=""bibr"" target=""#b52"">[53]</ref><ref type=""bibr"" target=""#b53"">[54]</ref> for surveys of classical approaches. Here we only review methods cl",0
"=""#b13"">[14]</ref><ref type=""bibr"" target=""#b14"">[15]</ref>.</p><p>Motivated by recent advances in self-supervised learning of visual representations <ref type=""bibr"" target=""#b15"">[16]</ref><ref type=""bibr"" target=""#b16"">[17]</ref><ref type=""bibr"" target=""#b17"">[18]</ref><ref type=""bibr"" target=""#b e also report performance when training a linear classifier on top of a fixed representation with all labels <ref type=""bibr"" target=""#b30"">[31,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"">1]</ref> to directly evaluate SimCLRv2 representation",0
"arget=""#fig_2"">2</ref> simulates the UncertainGCN sampling technique. Furthermore, we adapt the higher-order graph node information under the CoreSet <ref type=""bibr"" target=""#b30"">[31]</ref> for a new sampling technique by introducing latent space distancing. In principle, CoreSet <ref type=""bibr"" nder the CoreSet <ref type=""bibr"" target=""#b30"">[31]</ref> for a new sampling technique by introducing latent space distancing. In principle, CoreSet <ref type=""bibr"" target=""#b30"">[31]</ref> uses risk minimisation between core-sets on the learner feature space while we employ this operation over GC 35,</ref><ref type=""bibr"" target=""#b13"">14]</ref>), the first work applying it for CNNs as an active learning problem, CoreSet, has been presented in <ref type=""bibr"" target=""#b30"">[31]</ref>. The key principle depends on minimising the difference between the loss of a labelled set and a small unlab econd part, we discuss our two novel sampling methods: UncertainGCN and CoreGCN. UncertainGCN is based on the standard AL method uncertainty sampling <ref type=""bibr"" target=""#b30"">[31]</ref> which tracks the confidence scores of the designed graph nodes. Furthermore, CoreGCN adapts the highly succe r"" target=""#b30"">[31]</ref> which tracks the confidence scores of the designed graph nodes. Furthermore, CoreGCN adapts the highly successful CoreSet <ref type=""bibr"" target=""#b30"">[31]</ref> on the induced graph embeddings by the sequentially trained GCN network.</p></div> <div xmlns=""http://www.te CoreSet sampling on GCN. To integrate geometric information between the labelled and unlabelled graph representation, we approach a CoreSet technique <ref type=""bibr"" target=""#b30"">[31]</ref> in our sampling stage. This has shown better performance in comparison to uncertainty-based methods <ref typ </ref> in our sampling stage. This has shown better performance in comparison to uncertainty-based methods <ref type=""bibr"" target=""#b37"">[38]</ref>. <ref type=""bibr"" target=""#b30"">[31]</ref> shows how bounding the difference between the loss of the unlabelled samples and the one of the labelled is y on CIFAR10 Random UncertainGCN(Ours) VAAL <ref type=""bibr"" target=""#b32"">[33]</ref> Learning Loss <ref type=""bibr"" target=""#b41"">[42]</ref> CoreSet <ref type=""bibr"" target=""#b30"">[31]</ref> CoreGCN(Ours) FeatProp <ref type=""bibr"" target=""#b37"">[38]</ref> 2000 4000 6000 8000 10000 12000 14000 16000 e compare our method with a wide range of baselines which we describe here. Random sampling is by default the most common sampling technique. CoreSet <ref type=""bibr"" target=""#b30"">[31]</ref> on learner feature space is one of the best performing geometric techniques to date and it is another compet c: We compare our methods from the two ends of the spectrum of baselines. One is random sampling which is the default mechanism. The other is CoreSet <ref type=""bibr"" target=""#b30"">[31]</ref>, one of the best performing baselines from the previous experiments. We report the performance in terms of m earning. This has become a standard in deep learning system due to its successful deployment in recent methods <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b30"">31,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" target=""#b41"">42]</ref>. In this scenario, from a poo",1
"nlabelled data have been explored through uncertainty exploration of the convolutional neural networks (CNNs). A Bayesian approximation introduced in <ref type=""bibr"" target=""#b8"">[9]</ref> produce meaningful uncertainty measurements by variational inference of a Monte Carlo Dropout (MC Dropout) ada",0
"Carlo Dropout (MC Dropout) adapted architecture. Hence, it is successfully integrated into active learning by <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b27"">28]</ref>. With the rise of GPU computa",0
"ibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b27"">28]</ref>. With the rise of GPU computation power, <ref type=""bibr"" target=""#b1"">[2]</ref> ensemble-based method outperformed MC Dropout. Geometric-based methods. Although there have been studies explo dopt a pool-based scenario for active learning. This has become a standard in deep learning system due to its successful deployment in recent methods <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b30"">31,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" target=""#b41""> udget is fixed at 1,000 images for the 10class benchmarks and at 2,000 for CIFAR-100 which is a 100-class benchmark. Similar to the existing works of <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b41"">42]</ref>, we apply our selection on randomly selected subsets D S ? D U of unla",0
"ype=""bibr"" target=""#b21"">22]</ref> and 3D Hand Pose Estimation (HPE) <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b40"">41,</ref><ref type=""bibr"" target=""#b24"">25]</ref>. This has been possible due to the availability of both the powerful computing infrastructure and the large-s",0
"d equivariant nonlinearities. In the case of 3D roto-translations it has already been shown that a suitable structure for φ is a tensor field network <ref type=""bibr"" target=""#b24"">[25]</ref>, explained below. Note that Romero et al. <ref type=""bibr"" target=""#b20"">[21]</ref> recently introduced a 2D ing a feature vector f transforming according to Eq. <ref type=""bibr"" target=""#b4"">(5)</ref>.</p><p>Tensor Field Networks Tensor field networks (TFN) <ref type=""bibr"" target=""#b24"">[25]</ref> are neural networks, which map point clouds to point clouds under the constraint of SE(3)-equivariance, the formula><p>We can also include a sum over input channels, but we omit it here. Weiler et al. <ref type=""bibr"" target=""#b32"">[33]</ref>, Thomas et al. <ref type=""bibr"" target=""#b24"">[25]</ref> and Kondor <ref type=""bibr"" target=""#b12"">[13]</ref> showed that the kernel W k lies in the span of an equiv at W k J (0) = 0 only when k = and J = 0, which reduces the kernel to a scalar w multiplied by the identity, W = w I, referred to as self-interaction <ref type=""bibr"" target=""#b24"">[25]</ref>. As such we can rewrite the TFN layer as</p><formula xml:id=""formula_11"">f out,i = w f in,i self-interaction on of input channels using one set of weights w i,c c = w c c per representation degree, shared across all points.</p><p>As proposed in Thomas et al. <ref type=""bibr"" target=""#b24"">[25]</ref>, this is followed by a norm-based non-linearity.</p><p>Attentive: We propose an extension of linear self-int dicting future locations and velocities in an electron-proton simulation.</p><p>Linear DeepSet <ref type=""bibr"" target=""#b39"">[40]</ref> Tensor Field <ref type=""bibr"" target=""#b24"">[25]</ref> Set Transformer <ref type=""bibr"" target=""#b13"">[14]</ref>  classification task. Here, the network is confron ecifically, we compare to the Set-Transformer <ref type=""bibr"" target=""#b13"">[14]</ref>, a non-equivariant attention model, and Tensor Field Networks <ref type=""bibr"" target=""#b24"">[25]</ref>, which is similar to SE(3)-Transformer but does not leverage attention.</p><p>Similar to <ref type=""bibr"" ta to 3 orders of magnitudes. This speed-up allowed us to train significantly larger versions of both the SE(3)-Transformer and the Tensor Field network <ref type=""bibr"" target=""#b24"">[25]</ref> and to apply these models to real-world datasets.</p><p>Our experiments showed that adding attention to a ro ame of the Clebsch-Gordan coefficients. These can be found in many mathematical physics libraries.</p><p>Tensor Field Layers In Tensor Field Networks <ref type=""bibr"" target=""#b24"">[25]</ref> and 3D Steerable CNNs <ref type=""bibr"" target=""#b32"">[33]</ref>, the authors solve for the intertwiners betw stable training. However, we would like to stress that all the Tensor Field networks we trained were significantly bigger than in the original paper <ref type=""bibr"" target=""#b24"">[25]</ref>, mostly enabled by the faster computation of the spherical harmonics.</p><p>For the ablation study in Fig. < o reduce to 3 channels for the Tensor Field network to obtain stable training. We used a norm based non-linearity for the Tensor Field network (as in <ref type=""bibr"" target=""#b24"">[25]</ref>) and no extra non-linearity (beyond the softmax in the self-attention algorithm) for the SE(3) Transformer.< invariance (ordering of input points), but only the Tensor Field network and the linear baseline are SE(3) equivariant. For the Tensor Field Network <ref type=""bibr"" target=""#b24"">[25]</ref> baseline, we used the same hyper parameters as for the SE(3) Transformer but with a linear self-interaction arameters as for the SE(3) Transformer but with a linear self-interaction and an additional norm-based nonlinearity in each layer as in Thomas et al. <ref type=""bibr"" target=""#b24"">[25]</ref>. For the DeepSet <ref type=""bibr"" target=""#b39"">[40]</ref> baseline, we used 3 fully connected layers, a poo",1
"simplicity coupled with high efficacy on a wide range of tasks such as language modeling <ref type=""bibr"" target=""#b27"">[28]</ref>, image recognition <ref type=""bibr"" target=""#b15"">[16]</ref>, or graph-based problems <ref type=""bibr"" target=""#b28"">[29]</ref>, make them an attractive component to use d by a successes across a wide range of tasks in deep learning such as language modeling <ref type=""bibr"" target=""#b27"">[28]</ref>, image recognition <ref type=""bibr"" target=""#b15"">[16]</ref>, graph-based problems <ref type=""bibr"" target=""#b28"">[29]</ref>, and relational reasoning <ref type=""bibr"" t ies, keys, and values individually and then concatenate the results into a single vector of the original shape <ref type=""bibr"" target=""#b3"">(4,</ref><ref type=""bibr"" target=""#b15"">16)</ref>. The keys and queries are edge embeddings, and thus the embedding matrices are of TFN type (c.f. Eq. ( <ref t",0
"ReLU(f ) = ReLU(LN f ) • f f , where f = m=− (f m ) 2 , (<label>52</label></formula><formula xml:id=""formula_60"">)</formula><p>where LN is layer norm <ref type=""bibr"" target=""#b1"">[2]</ref> applied across all features within a feature type. For the TFN baseline, we used the exact same architecture b =""tab_6""><head>Table 5 :</head><label>5</label><figDesc>QM9 Radial Function Architecture. C is the number of output channels at each layer. Layer norm<ref type=""bibr"" target=""#b1"">[2]</ref> is computed per pair of input and output feature types, which have C in and C out channels each.LAYER TYPE C A",0
"rmations and general performance.</p><p>Point cloud data is ubiquitous across many fields, presenting itself in diverse forms such as 3D object scans <ref type=""bibr"" target=""#b25"">[26]</ref>, 3D molecular structures <ref type=""bibr"" target=""#b18"">[19]</ref>, or N -body particle simulations <ref typ",0
"""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b28"">29,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b22"">23]</ref>. These methods were unified by Wang et al. <ref type=""bibr"" target=""#b30"">[31]</ref> with the non-local neura",0
"ref type=""bibr"" target=""#b61"">62,</ref><ref type=""bibr"" target=""#b17"">18]</ref>. Learning on such data is possible using graph neural networks (GNNs) <ref type=""bibr"" target=""#b25"">[26]</ref> that typically operate by a message passing mechanism <ref type=""bibr"" target=""#b3"">[4]</ref> aggregating in",1
"learning models <ref type=""bibr"" target=""#b36"">[37]</ref> to dynamic graphs by ignoring the temporal evolution, this has been shown to be sub-optimal <ref type=""bibr"" target=""#b65"">[66]</ref>, and in some cases, it is the dynamic structure that contains crucial insights about the system. Learning on t) e i1 (t 1 ) ?(t -t 1 ), . . . , h (l-1) N (t) e iN (t N ) ?(t -t N )].<label>(9)</label></formula><p>Here, ?(?) represents a generic time encoding <ref type=""bibr"" target=""#b65"">[66]</ref>, is the concatenation operator and</p><formula xml:id=""formula_16"">z i (t) = emb(i, t) = h (L) i (t).</formu the reference node representation with the aggregated information. Differently from the original formulation of this layer (firstly proposed in TGAT <ref type=""bibr"" target=""#b65"">[66]</ref>) where no node-wise temporal features were used, in our case the input representation of each node h (0) j ( 2"">1</ref>). For example, Jodie <ref type=""bibr"" target=""#b35"">[36]</ref> uses the time projection embedding module emb(i, t) = (1+?tw)?s i (t). TGAT <ref type=""bibr"" target=""#b65"">[66]</ref> is a specific case of TGN when the memory and its related modules are missing, and graph attention is used a egrees. Dataset statistics together with more details are provided in the supplementary material.</p><p>Tasks. Our experimental setup closely follows <ref type=""bibr"" target=""#b65"">[66]</ref> and focuses on the tasks of future edge prediction and dynamic node classification. On the former, we use bo s of nodes never observed before. The transductive setting is used for node classification. We perform the same 70%-15%-15% chronological split as in <ref type=""bibr"" target=""#b65"">[66]</ref>.</p><p>Future Edge Prediction. The goal is to predict the probability of an edge occurring between two nodes oaches for continuous time dynamic graphs (CTDNE <ref type=""bibr"" target=""#b46"">[47]</ref>, Jodie <ref type=""bibr"" target=""#b35"">[36]</ref>, and TGAT <ref type=""bibr"" target=""#b65"">[66]</ref>) as well as state-of-the-art models for static graphs (GAE <ref type=""bibr"" target=""#b33"">[34]</ref>, VGAE < ype=""bibr"" target=""#b60"">[61]</ref> and GraphSAGE <ref type=""bibr"" target=""#b26"">[27]</ref>, CTDNE <ref type=""bibr"" target=""#b46"">[47]</ref> and TGAT <ref type=""bibr"" target=""#b65"">[66]</ref> are taken directly from the TGAT paper <ref type=""bibr"" target=""#b65"">[66]</ref>.</p><p>For Jodie <ref type= [27]</ref>, CTDNE <ref type=""bibr"" target=""#b46"">[47]</ref> and TGAT <ref type=""bibr"" target=""#b65"">[66]</ref> are taken directly from the TGAT paper <ref type=""bibr"" target=""#b65"">[66]</ref>.</p><p>For Jodie <ref type=""bibr"" target=""#b35"">[36]</ref>, we implement our own version in PyTorch, as a sp ><head>Hyperparameters</head><p>For all the models and datasets we used the same hyperparameters, which had been found to work well in the TGAT paper <ref type=""bibr"" target=""#b65"">[66]</ref>.</p></div>			</div> 			<div type=""references"">  				<listBibl>  <biblStruct xml:id=""b0""> 	<analytic> 		<titl",1
"""bibr"" target=""#b66"">67,</ref><ref type=""bibr"" target=""#b74"">75,</ref><ref type=""bibr"" target=""#b72"">73,</ref><ref type=""bibr"" target=""#b56"">57,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b49"">50]</ref>. Another line of work encodes",0
"appears. Other recent works have focused on dynamic knowledge graphs <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b64"">65,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b18"">19]</ref>.</p><p>Most recent CTDG learning models can be interpreted as specifi",0
"sed as models for systems of relations and interactions in many fields <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b51"">52,</ref><ref type=""bibr"" target=""#b41"">42,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b19""> target=""#b3"">[4]</ref> aggregating information in a neighborhood of a node and create node embeddings that are then used for node-wise classification <ref type=""bibr"" target=""#b41"">[42,</ref><ref type=""bibr"" target=""#b60"">61,</ref><ref type=""bibr"" target=""#b34"">35]</ref> or edge prediction <ref type",0
"target=""#b1"">2]</ref>, assemble snapshots into tensors and factorize <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b69"">70,</ref><ref type=""bibr"" target=""#b38"">39]</ref>, or encode each snapshot to produce a series of embeddings. In the latter case, the embeddings are either agg",0
"nodes not seen during training <ref type=""bibr"" target=""#b45"">[46,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b58"">59,</ref><ref type=""bibr"" target=""#b35"">36]</ref>. Such approaches are unsuitable for interesting real world settings such as social networks, where dynamic gr ameters, ?t is the time since the last interaction, and ? denotes element-wise vector product. This version of the embedding method was used in JODIE <ref type=""bibr"" target=""#b35"">[36]</ref>.</p><p>Temporal Graph Attention (attn): A series of L graph attention layers compute i's embedding by aggreg CTDG learning models can be interpreted as specific cases of our framework (see Table <ref type=""table"" target=""#tab_2"">1</ref>). For example, Jodie <ref type=""bibr"" target=""#b35"">[36]</ref> uses the time projection embedding module emb(i, t) = (1+?tw)?s i (t). TGAT <ref type=""bibr"" target=""#b65"">[ mlns=""http://www.tei-c.org/ns/1.0""><head n=""5.1"">Experimental Settings</head><p>Datasets. We use three datasets in our experiments: Wikipedia, Reddit <ref type=""bibr"" target=""#b35"">[36]</ref>, and Twitter. Reddit and Wikipedia are bipartite interaction graphs. In the Reddit dataset, users and sub-re ines. Our strong baselines are state-of-the-art approaches for continuous time dynamic graphs (CTDNE <ref type=""bibr"" target=""#b46"">[47]</ref>, Jodie <ref type=""bibr"" target=""#b35"">[36]</ref>, and TGAT <ref type=""bibr"" target=""#b65"">[66]</ref>) as well as state-of-the-art models for static graphs (G and TGAT <ref type=""bibr"" target=""#b65"">[66]</ref> are taken directly from the TGAT paper <ref type=""bibr"" target=""#b65"">[66]</ref>.</p><p>For Jodie <ref type=""bibr"" target=""#b35"">[36]</ref>, we implement our own version in PyTorch, as a specific case of our framework with the temporal embedding mo pe=""bibr"" target=""#b2"">3]</ref> that incorporate continuous time through constraints on transition probabilities. Sequence-based approaches for CTDGs <ref type=""bibr"" target=""#b35"">[36,</ref><ref type=""bibr"" target=""#b57"">58,</ref><ref type=""bibr"" target=""#b58"">59,</ref><ref type=""bibr"" target=""#b37",0
">54,</ref><ref type=""bibr"" target=""#b47"">48]</ref>, or learned by imposing a smoothness constraint over time <ref type=""bibr"" target=""#b32"">[33,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b66"">67,</ref><ref type=""bibr"" target=""#b74"">75,</ref><ref type=""bibr"" target=""#b72""",0
"o update representations of the source and destination node each time a new edge appears. Other recent works have focused on dynamic knowledge graphs <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b64"">65,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b18",0
"=""bibr"" target=""#b51"">52,</ref><ref type=""bibr"" target=""#b41"">42,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" target=""#b48"">49,</ref><ref type=""bibr"" target=""#b52"">53]</ref>, in particular, social scienc",0
"tside the scope of most research projects. Recently, projects such as Halide <ref type=""bibr"" target=""#b18"">(Ragan-Kelley et al., 2013)</ref> and TVM <ref type=""bibr"" target=""#b2"">(Chen et al., 2018a)</ref> have arisen that attempt to simplify the process of creating optimized schedules by separatin ast 3HW F ops.} 12: end for 13: Y = BinaryDense(a L )</formula><p>in both TensorFlow <ref type=""bibr"" target=""#b0"">(Abadi et al., 2016)</ref> and TVM <ref type=""bibr"" target=""#b2"">(Chen et al., 2018a)</ref>. We use TensorFlow for training binary networks and TVM for compiling efficient machine code. //www.tei-c.org/ns/1.0""><head n=""4.3"">Generating Efficient Code</head><p>To compile our described algorithms to efficient machine code, we extend TVM <ref type=""bibr"" target=""#b2"">(Chen et al., 2018a)</ref> to support bitserial operations. This allows Riptide to directly convert its TensorFlow train",1
"target=""#b12"">Hubara et al., 2016;</ref><ref type=""bibr"" target=""#b20"">Tang et al., 2017;</ref><ref type=""bibr"" target=""#b8"">Dong et al., 2017;</ref><ref type=""bibr"" target=""#b9"">Fromm et al., 2018;</ref><ref type=""bibr"" target=""#b4"">Choi et al., 2019)</ref> has therefore focused on closing the acc",0
"on AlexNet <ref type=""bibr"" target=""#b15"">(Krizhevsky et al., 2012)</ref>, VGGNet <ref type=""bibr"" target=""#b10"">(He et al., 2015)</ref>, and Resnets <ref type=""bibr"" target=""#b11"">(He et al., 2016)</ref>. To directly compare against these results, we train these three models with multiple bitwidth",0
"10: p k = Pooling(l k ) {HWF ops.} 11: a k = BitPack(p k ) {At least 3HW F ops.} 12: end for 13: Y = BinaryDense(a L )</formula><p>in both TensorFlow <ref type=""bibr"" target=""#b0"">(Abadi et al., 2016)</ref> and TVM <ref type=""bibr"" target=""#b2"">(Chen et al., 2018a)</ref>. We use TensorFlow for train",0
"is not clear how to efficiently and correctly implement mixed polarity models.</p><p>It is worth noting that there also exist Ternary Weight Networks <ref type=""bibr"" target=""#b16"">(Li et al., 2016)</ref> that use bipolar quantization and a mask tensor that specifies some bits as representing 0. Alt",0
"nefits, the accuracy they have been shown capable of achieving on challenging datasets like ImageNet has been underwhelming. For example, the AlexNet <ref type=""bibr"" target=""#b15"">(Krizhevsky et al., 2012)</ref> based BNN used by <ref type=""bibr"" target=""#b6"">Courbariaux et al. (2016)</ref> was onl high speed binary models and explore the impact of its various optimizations.</p><p>Most previous work in binarization has been evaluated on AlexNet <ref type=""bibr"" target=""#b15"">(Krizhevsky et al., 2012)</ref>, VGGNet <ref type=""bibr"" target=""#b10"">(He et al., 2015)</ref>, and Resnets <ref type=""",0
"r"" target=""#b20"">Tang et al., 2017;</ref><ref type=""bibr"" target=""#b8"">Dong et al., 2017;</ref><ref type=""bibr"" target=""#b9"">Fromm et al., 2018;</ref><ref type=""bibr"" target=""#b4"">Choi et al., 2019)</ref> has therefore focused on closing the accuracy gap. Despite considerable progress in develop-ing ts from being implemented as proposed by their authors.</p><p>Methods that use unipolar quantization for activations such as DoReFa-Net and PACT-SAWB <ref type=""bibr"" target=""#b4"">(Choi et al., 2019)</ref> are able to represent zeros but encounter other implementation issues. Because weights are alw HWGQ <ref type=""bibr"" target=""#b1"">(Cai et al., 2017)</ref> the current state-of-the-art for high accuracy binary AlexNets and VG-GNets and PACT-SAWB <ref type=""bibr"" target=""#b4"">(Choi et al., 2019)</ref> the state-ofthe-art for binarizing Resnets. For all models (including SqueezeNet), we binarize",0
"ations.</p><p>Most previous work in binarization has been evaluated on AlexNet <ref type=""bibr"" target=""#b15"">(Krizhevsky et al., 2012)</ref>, VGGNet <ref type=""bibr"" target=""#b10"">(He et al., 2015)</ref>, and Resnets <ref type=""bibr"" target=""#b11"">(He et al., 2016)</ref>. To directly compare agains",0
"by converting operations on large floating point vectors into ""bitserial"" versions that apply bitwise operations on packed bit vectors. For instance <ref type=""bibr"" target=""#b19"">(Rastegari et al., 2016)</ref> report convolution layers that use 58× fewer operations and 32 × less memory than the st inarization has been the focus of research in the space, with most papers introducing modifications to the core algorithm or new training techniques. <ref type=""bibr"" target=""#b19"">Rastegari et al. (2016)</ref> introduced XNOR-Net, which improved the accuracy of single bit binary models by adding th",0
"avatars where the accuracy of lip and eye contours is critical to realism.</p><p>In contrast to methods that use a parametric model of the human face <ref type=""bibr"" target=""#b0"">[1]</ref>, we directly predict the positions of face mesh vertices in 3D. We base our architecture on earlier efforts in",1
"is internally consistent compared to multiple disparate networks that are chained together.</p><p>We use an architecture similar to one described in <ref type=""bibr"" target=""#b6"">[7]</ref>, where the authors build a network that is robust to the initialization provided by different face detectors.",0
"he region submodels to them.</p><p>Attention mechanism Several attention mechanisms (soft and hard) have been developed for visual feature extraction <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b3"">4]</ref>. These attention mechanisms sample a grid of 2D points in feature space",0
"the face mesh is critical to applications like AR makeup where even small errors in alignment can drive the rendered effect into the ""uncanny valley"" <ref type=""bibr"" target=""#b7"">[8]</ref>. We built a lipstick rendering solution (Figure <ref type=""figure"" target=""#fig_3"">4</ref>) on top of our atte",0
"del to achieve the same quality as the cascaded approach by employing region-specific heads that transform the feature maps with spatial transformers <ref type=""bibr"" target=""#b3"">[4]</ref>, while being up to 30 percent faster during inference. We term this architecture as attention mesh. An added b to train architectures endto-end and enrich the features that are used by the attention mechanism. Specifically, we use a spatial transformer mod-ule <ref type=""bibr"" target=""#b3"">[4]</ref> to extract 24 × 24 region features from the 64 × 64 feature map. The spatial transformer is controlled by an a ntion mechanism Several attention mechanisms (soft and hard) have been developed for visual feature extraction <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b3"">4]</ref>. These attention mechanisms sample a grid of 2D points in feature space and extract the features under the samp",0
"gt; q(x t |x t 1 )</formula><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = "" e A Z 8 7  This paper presents progress in diffusion probabilistic models <ref type=""bibr"" target=""#b49"">[50]</ref>. A diffusion probabilistic model (which we will call a ""diffusion model"" for brevity) is a parameterized Mar ensured in part by the choice of Gaussian conditionals in p θ (x t−1 |x t ), because both processes have the same functional form when β t are small <ref type=""bibr"" target=""#b49"">[50]</ref>. A notable property of the forward process is that it admits sampling x t at an arbitrary timestep t in clos oint. These are the two extreme choices corresponding to upper and lower bounds on reverse process entropy for data with coordinatewise unit variance <ref type=""bibr"" target=""#b49"">[50]</ref>.</p><p>Second, to represent the mean µ θ (x t , t), we propose a specific parameterization motivated by the not tractable for high dimensional data. These algorithms serve as a compression interpretation of the variational bound (5) of Sohl-Dickstein et al. <ref type=""bibr"" target=""#b49"">[50]</ref>, not yet as a practical compression system. </p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>A Exte type=""formula"" target=""#formula_9"">5</ref>), the reduced variance variational bound for diffusion models. This material is from Sohl-Dickstein et al. <ref type=""bibr"" target=""#b49"">[50]</ref>; we include it here only for completeness.</p><formula xml:id=""formula_26"">L = E q − log p θ (x 0:T ) q(x 1: xperiments</head><p>We set T = 1000 for all experiments so that the number of neural network evaluations needed during sampling matches previous work <ref type=""bibr"" target=""#b49"">[50,</ref><ref type=""bibr"" target=""#b51"">52]</ref>. We set the forward process variances to constants increasing linear",1
"a U-Net <ref type=""bibr"" target=""#b44"">[45]</ref> based on a Wide ResNet <ref type=""bibr"" target=""#b63"">[64]</ref>. We replaced weight normalization <ref type=""bibr"" target=""#b45"">[46]</ref> with group normalization <ref type=""bibr"" target=""#b61"">[62]</ref> to make the implementation simpler. Apart",0
"ype=""formula"" target=""#formula_22"">14</ref>) is a weighted variational bound that emphasizes different aspects of reconstructions that θ must perform <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b19"">20]</ref>. We will see in our experiments that this reweighting leads to bette importance sampling <ref type=""bibr"" target=""#b21"">[22]</ref>. Our progressive decoding argument can be seen in convolutional DRAW and related models <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b36"">37]</ref> and may also lead to more general designs for subscale orderings or , all but fine details are preserved, and when t is large, only large scale features are preserved. Perhaps these are hints of conceptual compression <ref type=""bibr"" target=""#b15"">[16]</ref>.  Connection to autoregressive decoding Note that the variational bound ( <ref type=""formula"" target=""#formu ns=""http://www.tei-c.org/ns/1.0""><head>C Samples</head><p>Additional samples <ref type=""bibr"">Figure 11,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b18""",0
"as variational inference to train a Langevin-like sampler. Other methods for learning transition operators of Markov chains include infusion training <ref type=""bibr"" target=""#b1"">[2]</ref>, variational walkback <ref type=""bibr"" target=""#b12"">[13]</ref>, generative stochastic networks <ref type=""bib",0
"een diffusion models and denoising score matching over multiple noise levels with annealed Langevin dynamics <ref type=""bibr"" target=""#b51"">[52,</ref><ref type=""bibr"" target=""#b52"">53]</ref>. Diffusion models, however, admit straightforward log likelihood evaluation, and since the training procedure",0
"0]</ref>. Among them embedding, which is also called representation learning, has been proven to be successful techniques contributing to the success <ref type=""bibr"" target=""#b1"">[2]</ref>. In essence, embedding is a way to represent a sparse vector of ids as a dense feature vector, which is also c",1
"n system, it has been an active research topic in information retrieval community and search engine industry as the next generation search technology <ref type=""bibr"" target=""#b12"">[13]</ref>.</p><p>In general, a search engine comprises a recall layer targeting to retrieve a set of relevant document cher is. Because of this, embedding-based retrieval in Facebook search is not a text embedding problem, as is actively researched in the IR community <ref type=""bibr"" target=""#b12"">[13]</ref>. Instead it is a more complex problem that requires understanding of text, user, and the context altogether.",1
"are encoded with a neural network model into dense vectors, on which we use cosine similarity as the distance metric. We propose to use triplet loss <ref type=""bibr"" target=""#b13"">[14]</ref> to approximate the recall objective to learn the neural network encoder, which is also called embedding mode bedding learning. However, most of the research are from computer vision field and for the classification task <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b16"">17]</ref>, while search retrieval does",0
"he search intent from query text and represent the semantic meaning of documents, search techniques are mostly based on various term matching methods <ref type=""bibr"" target=""#b0"">[1]</ref>, which performs well for the cases that keyword match can address. It still remains a challenging problem for",0
"well as an active research area for embedding learning. However, most of the research are from computer vision field and for the classification task <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b16"">",0
"zation. There are different algorithms for coarse quantization. It is useful to compare between IMI <ref type=""bibr"" target=""#b10"">[11]</ref> and IVF <ref type=""bibr"" target=""#b14"">[15]</ref> algorithms. And it is important to tune number of coarse clusters num_cluster, which will affect both perf a",0
"tent.</p><p>In the last years, deep learning has made significant progress in speech recognition, computer vision, and natural language understanding <ref type=""bibr"" target=""#b9"">[10]</ref>. Among them embedding, which is also called representation learning, has been proven to be successful techniq",0
"coarse quantization which quantizes embedding vectors into coarse clusters typically through K-means algorithm, and the other is product quantization <ref type=""bibr"" target=""#b7"">[8]</ref> which does a fine-grained quantization to enable efficient calculation of embedding distances. There are sever",0
"arget=""#b0"">[1]</ref>, which performs well for the cases that keyword match can address. It still remains a challenging problem for semantic matching <ref type=""bibr"" target=""#b11"">[12]</ref>, which is to address desired results that are not exact match of the query text but can satisfy users' searc",0
"parameters we need to tune:</p><p>• Coarse quantization. There are different algorithms for coarse quantization. It is useful to compare between IMI <ref type=""bibr"" target=""#b10"">[11]</ref> and IVF <ref type=""bibr"" target=""#b14"">[15]</ref> algorithms. And it is important to tune number of coarse c",0
"<text xml:lang=""en""> 		<body> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""1"">Introduction</head><p>The task of Grounded video description (GVD) <ref type=""bibr"" target=""#b8"">[Zhou et al., 2019]</ref> aims to generate more grounded and accurate descriptions by linking the generated words with t attending them for text generation. On one hand, existing works either encode region proposals independently or using selfattention-based mechanisms <ref type=""bibr"" target=""#b8"">[Zhou et al., 2019]</ref>. Therefore, it either fails to consider implicit structural information among the region propo /ref><ref type=""bibr"" target=""#b3"">Li et al., 2019a]</ref>, many works model the video in both global video features and regional object features. In <ref type=""bibr"" target=""#b8"">[Zhou et al., 2019]</ref>, they encode the objects with transformer <ref type=""bibr"">[Vaswani et al., 2017]</ref> and th -c.org/ns/1.0""><head n=""3.1"">Video Global Encoder</head><p>We model the video's global level feature by a Bi-directional LSTM network like most works <ref type=""bibr"" target=""#b8"">[Zhou et al., 2019]</ref> given by: h = BiLST M (v) = {h 1 , h 2 , ..., h m } where v ∈ R n×d is the global feature extr finement Encoder</head><p>In this section, we propose a novel visual representation method from the perspective of regions. First of all, inspired by <ref type=""bibr"" target=""#b8"">[Zhou et al., 2019]</ref>, we enhance the proposal features by adding the position and class features. As for proposal m r knowledge considering the generality.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>Feature Enhancement</head><p>In this part, we follow <ref type=""bibr"" target=""#b8"">[Zhou et al., 2019]</ref>'s work, which fusing the spatial-temporal and class features with the original features to enr k+dsp) is the embedding weight. Then we will apply feature aggregation on the enhanced feature R.</p><p>We adopt the same classification loss just as <ref type=""bibr"" target=""#b8"">[Zhou et al., 2019]</ref> do denoted as L cls .</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>Spatial-tempora = LST M (h<ref type=""foot"" target=""#foot_1"">1</ref> t , h f rame + h attention ). h t is used to generate descriptions. We adopt the same MLE loss as <ref type=""bibr"" target=""#b8"">[Zhou et al., 2019]</ref> which denoted by L sent .</p><p>Finally, the overall loss function consists of four parts:</p> /div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.1"">Dataset</head><p>We conduct our experiments on the Grounded ActivityNet-Entities Dataset <ref type=""bibr"" target=""#b8"">[Zhou et al., 2019]</ref> for evaluation. It contains 15k video with 158k spatially annotated bounding boxes from 52k vi roduce some implementation details of our HAST-Graph2Seq method. Data processing. For a fair comparison, the data processing procedure is the same to <ref type=""bibr"" target=""#b8"">[Zhou et al., 2019]</ref>. For each video segment in the dataset, we uniformly sample 10 frames. And for each frame, we former <ref type=""bibr"" target=""#b7"">[Zhou et al., 2018]</ref>, BiM-STM+TempoAtnn <ref type=""bibr"" target=""#b7"">[Zhou et al., 2018]</ref> and ZhouGVD <ref type=""bibr"" target=""#b8"">[Zhou et al., 2019]</ref> on Grounded ActivityNet Captions Dataset to verify the effectiveness of our method. Moreover, . hierarchical attention (abbr: -hie. attn.). We remove the hierarchical attention and replace it with the coarsegrain proposal attention proposed by <ref type=""bibr"" target=""#b8"">[Zhou et al., 2019]</ref>.</p><p>Table <ref type=""table"" target=""#tab_2"">3</ref> gives all ablation results on the valid",1
"attention may confuse the model. Therefore, graph-based methods which model the regions with abundant semantic relations are introduced to this area. <ref type=""bibr"" target=""#b6"">[Yao et al., 2018]</ref>  </p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2.2"">Graph-to-sequence Learning</",0
"tive but they overlook the fine-grained object clues that separated in frames.</p><p>Borrowing the ideas of spatial-attention in image caption domain <ref type=""bibr"" target=""#b0"">[Anderson et al., 2018;</ref><ref type=""bibr"" target=""#b4"">Liu et al., 2018;</ref><ref type=""bibr"" target=""#b3"">Li et al",0
"with the regions in video frames. Compared to conventional video description task that generates a human-like sentence to describe the video contents <ref type=""bibr"" target=""#b7"">[Zhou et al., 2018]</ref>, GVD has advantages of modelling the video by objects and associating the generated text with ""http://www.tei-c.org/ns/1.0""><head n=""4.4"">Performance Comparisons</head><p>We compare HAST-Graph2Seq with the SOTA models, i.e., Masked Transformer <ref type=""bibr"" target=""#b7"">[Zhou et al., 2018]</ref>, BiM-STM+TempoAtnn <ref type=""bibr"" target=""#b7"">[Zhou et al., 2018]</ref> and ZhouGVD <ref ty p>We compare HAST-Graph2Seq with the SOTA models, i.e., Masked Transformer <ref type=""bibr"" target=""#b7"">[Zhou et al., 2018]</ref>, BiM-STM+TempoAtnn <ref type=""bibr"" target=""#b7"">[Zhou et al., 2018]</ref> and ZhouGVD <ref type=""bibr"" target=""#b8"">[Zhou et al., 2019]</ref> on Grounded ActivityNet Ca",0
"ial-attention in image caption domain <ref type=""bibr"" target=""#b0"">[Anderson et al., 2018;</ref><ref type=""bibr"" target=""#b4"">Liu et al., 2018;</ref><ref type=""bibr"" target=""#b3"">Li et al., 2019a]</ref>, many works model the video in both global video features and regional object features. In <ref",0
"learning in CV and NLP, video description begins to generate the description of a video using the attention-based encoder-decoder like architectures <ref type=""bibr"" target=""#b5"">[Venugopalan et al., 2015;</ref><ref type=""bibr"" target=""#b5"">Xu et al., 2018b;</ref><ref type=""bibr"" target=""#b4"">Liu e the description of a video using the attention-based encoder-decoder like architectures <ref type=""bibr"" target=""#b5"">[Venugopalan et al., 2015;</ref><ref type=""bibr"" target=""#b5"">Xu et al., 2018b;</ref><ref type=""bibr"" target=""#b4"">Liu et al., 2016]</ref>. These methods are effective but they overl ate sequential content from graph structured data, which learns a mapping between graph inputs to sequence outputs through attention-based mechanisms <ref type=""bibr"" target=""#b5"">[Xu et al., 2018a;</ref><ref type=""bibr"" target=""#b2"">Chen et al., 2020;</ref><ref type=""bibr"" target=""#b2"">Gao et al.,",0
"p>More recently, the graph-based method for video understanding started attracting more attentions in some close related fields such as image caption <ref type=""bibr"" target=""#b4"">[Li et al., 2019b]</ref>. However, due to the complexity of video understanding, there still remains significant challen by a pre-trained model trained on VG <ref type=""bibr"" target=""#b2"">[Krishna et al., 2017]</ref> dataset, we can train a semantic relation classifier <ref type=""bibr"" target=""#b4"">[Li et al., 2019b]</ref> on it. We adopt almost the same operation except replacing the KNN step by the classifier to fi encoder-decoder like architectures <ref type=""bibr"" target=""#b5"">[Venugopalan et al., 2015;</ref><ref type=""bibr"" target=""#b5"">Xu et al., 2018b;</ref><ref type=""bibr"" target=""#b4"">Liu et al., 2016]</ref>. These methods are effective but they overlook the fine-grained object clues that separated in f eparated in frames.</p><p>Borrowing the ideas of spatial-attention in image caption domain <ref type=""bibr"" target=""#b0"">[Anderson et al., 2018;</ref><ref type=""bibr"" target=""#b4"">Liu et al., 2018;</ref><ref type=""bibr"" target=""#b3"">Li et al., 2019a]</ref>, many works model the video in both global u et al., 2019]</ref> given by: h = BiLST M (v) = {h 1 , h 2 , ..., h m } where v ∈ R n×d is the global feature extracted by a pre-trained 3D-ConvNet <ref type=""bibr"" target=""#b4"">[Tran et al., 2015]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.2"">Graph with Refinement Encode rget=""#b8"">[Zhou et al., 2019]</ref>. For each video segment in the dataset, we uniformly sample 10 frames. And for each frame, we use a Faster R-CNN <ref type=""bibr"" target=""#b4"">[Ren et al., 2015]</ref> detector with ResNeXt-101 backbone to detect 100 region proposals and extract the feature. The",0
"ch learns a mapping between graph inputs to sequence outputs through attention-based mechanisms <ref type=""bibr"" target=""#b5"">[Xu et al., 2018a;</ref><ref type=""bibr"" target=""#b2"">Chen et al., 2020;</ref><ref type=""bibr"" target=""#b2"">Gao et al., 2019]</ref>. However, since there is no explicit graph utputs through attention-based mechanisms <ref type=""bibr"" target=""#b5"">[Xu et al., 2018a;</ref><ref type=""bibr"" target=""#b2"">Chen et al., 2020;</ref><ref type=""bibr"" target=""#b2"">Gao et al., 2019]</ref>. However, since there is no explicit graph structure for video, it is hard to adapt these method r i and R p .</p><p>(2) With external knowledge method: Relation Graph. Since the region features are extracted by a pre-trained model trained on VG <ref type=""bibr"" target=""#b2"">[Krishna et al., 2017]</ref> dataset, we can train a semantic relation classifier <ref type=""bibr"" target=""#b4"">[Li et a , 2015]</ref> detector with ResNeXt-101 backbone to detect 100 region proposals and extract the feature. The detector is pre-trained on Visual Genome <ref type=""bibr"" target=""#b2"">[Krishna et al., 2017]</ref>. Finally, for the video feature, the temporal feature map is a stack of frame-wise appearan the classic spectral graph convolutional network to aggregate the features of the nodes modeled by topology A.</p><p>Inspired by resnet architecture <ref type=""bibr"" target=""#b2"">[He et al., 2016]</ref>, we propose the basic module of our architecture as follows (the layer normalization and dropout",0
"2019]</ref>. This similar idea is widely used in information retrieval to capture the exact and soft matches between a query and a candidate document <ref type=""bibr"" target=""#b10"">[Xiong et al., 2017]</ref>. Specifically, we apply the basic BERT unit to obtain {C(e i )}</p></div> <div xmlns=""http:/ wo sentences. Different from sentences, the neighbors are disordered and independent from each other. Thus we adopt a RBF kernel aggregation function <ref type=""bibr"" target=""#b10"">[Xiong et al., 2017]</ref> to extract features about the accumulation of similarities.</p><p>Before making use of the R",1
"a large number of long-tail entities whose structural embeddings have low * Contact Author  <ref type=""bibr"">Yang et al., 2019)</ref> expressiveness <ref type=""bibr"" target=""#b5"">[Guo et al., 2019]</ref>. For example, in DBpedia-ZH<ref type=""foot"" target=""#foot_0"">1</ref> , the long-tail entities t",0
"e noises and thus harm performance <ref type=""bibr"">[Yang et al., 2019]</ref>. Although some works distinguish the influence from different neighbors <ref type=""bibr"" target=""#b0"">[Cao et al., 2019;</ref><ref type=""bibr"" target=""#b9"">Wu et al., 2019a;</ref><ref type=""bibr"" target=""#b11"">Xu et al., 2",0
"nsformer, e.g., the subspecialty of ""BERTology"" <ref type=""bibr"" target=""#b49"">(Rogers et al., 2020)</ref>, which specifically studies the BERT model <ref type=""bibr"" target=""#b11"">(Devlin et al., 2019)</ref>.</p><p>In this work, we adapt and extend this line of interpretability research to protein ""bibr"" target=""#b61"">(Vaswani et al., 2017)</ref>. In NLP, transformers are the backbone of state-of-the-art pre-trained language models such as BERT <ref type=""bibr"" target=""#b11"">(Devlin et al., 2019)</ref>. BERTology focuses on interpreting what the BERT model learns about language using a suite",1
"es of attention in Transformers <ref type=""bibr"" target=""#b61"">(Vaswani et al., 2017;</ref><ref type=""bibr"" target=""#b28"">Kovaleva et al., 2019;</ref><ref type=""bibr"" target=""#b22"">Hoover et al., 2019;</ref><ref type=""bibr"" target=""#b63"">Vig, 2019)</ref>. Recent work has begun to analyze attention i",0
"lth and the development of disease therapies. The decreasing cost of sequencing technology has enabled vast databases of naturally occurring proteins <ref type=""bibr"" target=""#b12"">(El-Gebali et al., 2019a)</ref>, which are rich in information for developing powerful machine learning models of prote",0
"nt work has begun to analyze attention in Transformer models outside of the domain of natural language <ref type=""bibr"">(Schwaller et al., 2020;</ref><ref type=""bibr"" target=""#b41"">Payne et al., 2020)</ref>.</p><p>Our work extends these methods to protein sequence models by considering particular bi",0
"r"" target=""#b2"">(AlQuraishi, 2019;</ref><ref type=""bibr"" target=""#b17"">Fox et al., 2013;</ref><ref type=""bibr"" target=""#b5"">Berman et al., 2000;</ref><ref type=""bibr"" target=""#b38"">Moult et al., 2018)</ref>, which contains amino acid sequences annotated with spatial coordinates (used for the contact r"" target=""#b2"">(AlQuraishi, 2019;</ref><ref type=""bibr"" target=""#b17"">Fox et al., 2013;</ref><ref type=""bibr"" target=""#b5"">Berman et al., 2000;</ref><ref type=""bibr"" target=""#b38"">Moult et al., 2018)</ref> and the Secondary Structure dataset <ref type=""bibr"" target=""#b45"">(Rao et al., 2019;</ref><r we use the Secondary Structure dataset <ref type=""bibr"" target=""#b45"">(Rao et al., 2019;</ref><ref type=""bibr"" target=""#b5"">Berman et al., 2000;</ref><ref type=""bibr"" target=""#b38"">Moult et al., 2018;</ref><ref type=""bibr"" target=""#b27"">Klausen et al., 2019)</ref> from TAPE. We considered a more fin f> and the Secondary Structure dataset <ref type=""bibr"" target=""#b45"">(Rao et al., 2019;</ref><ref type=""bibr"" target=""#b5"">Berman et al., 2000;</ref><ref type=""bibr"" target=""#b38"">Moult et al., 2018;</ref><ref type=""bibr"" target=""#b27"">Klausen et al., 2019)</ref>. The former was used for analysis o on, we measure F1 score; for contact prediction, we measure precision@L/5, where L is the length of the protein sequence, following standard practice <ref type=""bibr"" target=""#b38"">(Moult et al., 2018)</ref>; for binding site prediction, we measure precision@L/20, since approximately one in twenty a",0
"rvision, our approach leverages the power of pre-trained language models (e.g., ELMo <ref type=""bibr"" target=""#b29"">(Peters et al., 2018)</ref>, BERT <ref type=""bibr"" target=""#b5"">(Devlin et al., 2019)</ref>, XLnet <ref type=""bibr"" target=""#b45"">(Yang et al., 2019)</ref>) which are particularly attr information effectively. These language models have achieved state-of-the-art performance in many popular NLP benchmarks with appropriate fine-tuning <ref type=""bibr"" target=""#b5"">(Devlin et al., 2019;</ref><ref type=""bibr"" target=""#b22"">Liu et al., 2019b;</ref><ref type=""bibr"" target=""#b45"">Yang et n and recall for all methods. Note that our implementations of the fully supervised NER methods attain very close to the state-of-the-art performance <ref type=""bibr"" target=""#b5"">(Devlin et al., 2019;</ref><ref type=""bibr"" target=""#b19"">Limsopatham and Collier, 2016)</ref>. Our results are summariz",1
"corpora. To address the challenges in learning from distant supervision, our approach leverages the power of pre-trained language models (e.g., ELMo <ref type=""bibr"" target=""#b29"">(Peters et al., 2018)</ref>, BERT <ref type=""bibr"" target=""#b5"">(Devlin et al., 2019)</ref>, XLnet <ref type=""bibr"" tar",1
"models (e.g., ELMo <ref type=""bibr"" target=""#b29"">(Peters et al., 2018)</ref>, BERT <ref type=""bibr"" target=""#b5"">(Devlin et al., 2019)</ref>, XLnet <ref type=""bibr"" target=""#b45"">(Yang et al., 2019)</ref>) which are particularly attractive to this task due to the following merits: First, they are nchmarks with appropriate fine-tuning <ref type=""bibr"" target=""#b5"">(Devlin et al., 2019;</ref><ref type=""bibr"" target=""#b22"">Liu et al., 2019b;</ref><ref type=""bibr"" target=""#b45"">Yang et al., 2019;</ref><ref type=""bibr"" target=""#b17"">Lan et al., 2020b;</ref><ref type=""bibr"" target=""#b30"">Raffel et",1
"ical sequential models, such as Hidden Markov Model (HMM) <ref type=""bibr"" target=""#b46"">(Zhou and Su, 2002)</ref> and Conditional Random Field (CRF) <ref type=""bibr"" target=""#b15"">(Lafferty et al., 2001)</ref> based on hand-crafted features. To alleviate the burden of designing hand-crafted feature",0
""" target=""#b12"">(Karatay and Karagoz, 2015)</ref>, question answering <ref type=""bibr"" target=""#b13"">(Khalid et al., 2008)</ref> and dialogue systems <ref type=""bibr"" target=""#b1"">(Bowden et al., 2018)</ref>. Traditional approaches to NER mainly train statistical sequential models, such as Hidden Ma",0
"""#b15"">(Lafferty et al., 2001)</ref> based on hand-crafted features. To alleviate the burden of designing hand-crafted features, deep learning models <ref type=""bibr"" target=""#b24"">(Ma and Hovy, 2016;</ref><ref type=""bibr"" target=""#b10"">Huang et al., 2015)</ref> have been proposed for NER and shown ref type=""bibr"" target=""#b22"">(Liu et al., 2019b)</ref>-it adopts RoBERTa model with linear layers to perform token-level prediction; (ii) BiLSTM-CRF <ref type=""bibr"" target=""#b24"">(Ma and Hovy, 2016)</ref> adopts bi-directional LSTM with character-level CNN to produce token embeddings, which are fe >• Distantly-supervised Methods. The third group of baselines are recent deep learning models for distantly-supervised NER, including: (i) BiLSTM-CRF <ref type=""bibr"" target=""#b24"">(Ma and Hovy, 2016)</ref> is trained using the distant labels matched from KBs; (ii) AutoNER <ref type=""bibr"" target=""#",0
"ved state-of-the-art performance in many popular NLP benchmarks with appropriate fine-tuning <ref type=""bibr"" target=""#b5"">(Devlin et al., 2019;</ref><ref type=""bibr"" target=""#b22"">Liu et al., 2019b;</ref><ref type=""bibr"" target=""#b45"">Yang et al., 2019;</ref><ref type=""bibr"" target=""#b17"">Lan et al rained language models for tackling the two challenges, we propose a two-stage training framework. In the first stage, we fine-tune the RoBERTa model <ref type=""bibr"" target=""#b22"">(Liu et al., 2019b)</ref> with distantly-matched labels to essentially transfer the semantic knowledge in RoBERTa, whic p://www.tei-c.org/ns/1.0""><head n=""2.2"">Pre-trained Language Model</head><p>Pre-trained language models, such as BERT and its variants (e.g., RoBERTa <ref type=""bibr"" target=""#b22"">(Liu et al., 2019b)</ref>, ALBERT <ref type=""bibr"" target=""#b17"">(Lan et al., 2020b)</ref> and T5 <ref type=""bibr"" targ described in the appendix.</p><p>• Fully-supervised Methods. We also include fully-supervised NER methods for comparison, including: (i) RoBERTa-base <ref type=""bibr"" target=""#b22"">(Liu et al., 2019b)</ref>-it adopts RoBERTa model with linear layers to perform token-level prediction; (ii) BiLSTM-CRF p>Larger Pre-trained Language Models: To further improve the performance of BOND, we can use larger pre-trained language models such as RoBERTa-large <ref type=""bibr"" target=""#b22"">(Liu et al., 2019b)</ref> (Three times as big as RoBERT-base in our experiments) and T5 <ref type=""bibr"" target=""#b30"">",0
"e-trained BERT to the target NER task using stochastic gradient-type algorithms, e.g., ADAM <ref type=""bibr"" target=""#b14"">(Kingma and Ba, 2014;</ref><ref type=""bibr"" target=""#b21"">Liu et al., 2020)</ref>. Following <ref type=""bibr"" target=""#b30"">Raffel et al. (2019)</ref>, our adaptation process up",0
"et=""#b20"">(Liu et al., 2019a)</ref> augments a traditional language model with a KB and use entity type information to enhance the model. (ii) ConNET <ref type=""bibr"" target=""#b16"">(Lan et al., 2020a)</ref> leverages multiple crowd annotation and dynamically aggregates them by attention mechanism. I",0
"ngs as the initialization, we can efficiently adapt the pre-trained BERT to the target NER task using stochastic gradient-type algorithms, e.g., ADAM <ref type=""bibr"" target=""#b14"">(Kingma and Ba, 2014;</ref><ref type=""bibr"" target=""#b21"">Liu et al., 2020)</ref>. Following <ref type=""bibr"" target=""#",0
"016 NER shared task. This is an open-domain NER dataset that consists of 2400 tweets (comprising 34k tokens) with 10 entity types. (iii) OntoNotes5.0 <ref type=""bibr"" target=""#b40"">(Weischedel et al., 2013</ref>) contains text documents from multiple domains, including broadcast conversation, P2.5 d",0
"o various downstream applications such as user interest modeling <ref type=""bibr"" target=""#b12"">(Karatay and Karagoz, 2015)</ref>, question answering <ref type=""bibr"" target=""#b13"">(Khalid et al., 2008)</ref> and dialogue systems <ref type=""bibr"" target=""#b1"">(Bowden et al., 2018)</ref>. Traditional",0
"nsupervised manner. The stacked self-attention modules of the Table <ref type=""table"">1</ref>: Existing Gazetteer Matching Performance on Open-Domain <ref type=""bibr"" target=""#b34"">(Sang and De Meulder, 2003;</ref><ref type=""bibr"" target=""#b36"">Strauss et al., 2016)</ref> and Biomedical Domain NER D for each entity type in each dataset, a multi-sources gazetteer by crawling online data sources. Following the previous exact string matching methods <ref type=""bibr"" target=""#b34"">(Sang and De Meulder, 2003;</ref><ref type=""bibr"" target=""#b8"">Giannakopoulos et al., 2017)</ref>, we match an entity w ed in Figure <ref type=""figure"" target=""#fig_4"">2</ref>. We next collect gazetteers from multiple online resources to match more entities in the data <ref type=""bibr"" target=""#b34"">(Sang and De Meulder, 2003)</ref>. Please refer to the appendix for more technical details.</p><p>We then proceed with",0
"ed features. To alleviate the burden of designing hand-crafted features, deep learning models <ref type=""bibr"" target=""#b24"">(Ma and Hovy, 2016;</ref><ref type=""bibr"" target=""#b10"">Huang et al., 2015)</ref> have been proposed for NER and shown strong performance. However, most deep learning methods",0
"bibr"" target=""#b48"">[49,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b39"">40,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b28"">29]</ref> allow end-to-end differentable losses over data with arbitrary structure. They have been applied to an incred 1]</ref>. While GNNs are flexible enough to allow for unsupervised losses, most work follows the semi-supervised setting for node classification from <ref type=""bibr"" target=""#b28"">[29]</ref>. For a complete introductions to the vast topic we refer interested readers to detailed surveys <ref type=""b structure. For the purposes of this work, we consider transductive GNNs that output a single embedding per node. Graph convolutional networks (GCNs) <ref type=""bibr"" target=""#b28"">[29]</ref> are simple yet effective <ref type=""bibr"" target=""#b50"">[51]</ref> message-passing networks that fit our cri",1
"e modularity is proven to be NP-hard <ref type=""bibr"" target=""#b4"">[5]</ref>, however, a spectral relaxation of the problem can be solved efficiently <ref type=""bibr"" target=""#b36"">[37]</ref>. Let C ∈ 0, 1 n×k be the cluster assignment matrix and d be the degree vector. Then, with modularity matrix ed efficiently with iterative methods such as power iteration or Lanczos algorithm.</p><p>One can then obtain clusters by means of spectral bisection <ref type=""bibr"" target=""#b36"">[37]</ref> with iterative refinement akin to Kernighan-Lin algorithm <ref type=""bibr"" target=""#b27"">[28]</ref>. However",1
"aximized at 1, and is only comparable across graphs with the same degree distribution. While problems with the modularity metric have been identified <ref type=""bibr"" target=""#b21"">[22]</ref>, it remains one of the most commonly-used and eminently useful graph clustering metrics in scientific litera",0
"ul end-goal in itself -whether for data exploration <ref type=""bibr"" target=""#b44"">[45]</ref>, visualization <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b11"">12]</ref>, genomic feature discovery <ref type=""bibr"" target=""#b6"">[7]</ref>, anomaly detection <ref type=""bibr"" target",0
"te a power-law n-vector θ, where θ i is proportional to i's expected degree. With the memberships and the parameters D and θ.</p><p>We use graph-tool <ref type=""bibr"" target=""#b42"">[43]</ref> generate the graphs.</p><p>To generate s-dimensional features, we first generate feature memberships from k",0
"tworks and graph pooling methods.</p><p>Graph Neural Networks (GNNs) <ref type=""bibr"" target=""#b48"">[49,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b39"">40,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b28"">29]</ref> allow end-to-end differentabl",0
"in <ref type=""bibr"" target=""#b26"">[27]</ref> in the context of pre-training GNNs for producing representations of graphs, which was first tackled in <ref type=""bibr"" target=""#b35"">[36]</ref>.</p><p>Graph pooling aims to tackle the hierarchical nature of graphs via iterative coarsening. Early archit",0
"#b31"">[32]</ref> in ground-truth communities. This can be explained by the fact than a single node implicitly participates in many different clusters <ref type=""bibr"" target=""#b16"">[17]</ref>, e.g. a person in a social network is simultaneously connected with family and work friends, forcing the alg ibute that to extremely uneven structure of these graphs, as popular products are co-purchased with a lot of other items, so the effects discussed in <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b31"">32]</ref> are prohibiting good cuts. This corresponds to high values of d ad d",0
"maximizing mutual information <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b57"">58,</ref><ref type=""bibr"" target=""#b53"">54]</ref> in a self-supervised fashion. Deep Graph Infomax (DGI) <ref type=""bibr"" target=""#b60"">[61]</ref> adapted the",0
"rich line of research on graph neural networks and graph pooling methods.</p><p>Graph Neural Networks (GNNs) <ref type=""bibr"" target=""#b48"">[49,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b39"">40,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b28""",0
"c contrastive loss and propose new metrics to measure the representation quality. All of these works sample negative examples from p(x). Arora et al. <ref type=""bibr"" target=""#b0"">[1]</ref> theoretically analyze the effect of contrastive representation learning on a downstream, ""average"" classificat radius 1/t, where t is the temperature scaling hyperparameter. Without loss of generality, we set t = 1 for all theoretical results.</p><p>Similar to <ref type=""bibr"" target=""#b0"">[1]</ref>, we assume an underlying set of discrete latent classes C that represent semantic content, i.e., similar pairs n f as</p><formula xml:id=""formula_17"">L Sup (T , f ) = inf W∈R K×d L Softmax (T , W f ).<label>(10)</label></formula><p>In line with the approach of <ref type=""bibr"" target=""#b0"">[1]</ref> we analyze the supervised loss of a mean classifier <ref type=""bibr"" target=""#b29"">[30]</ref>, where for each dataset size T is much larger than N and M, as is commonly the case. The dependence on on N and T in Theorem 5 is roughly equivalent to the result in <ref type=""bibr"" target=""#b0"">[1]</ref>, but the two bounds are not directly comparable since the proof strategies differ.</p></div> <div xmlns=""http: N N ∑ i=1 a i − τ + 1 M M ∑ i=1 b i , e −1    .</formula><p>In order to derive our bound we will exploit a concentration of measure result due to <ref type=""bibr"" target=""#b0"">[1]</ref>. They consider an objective of the form</p><formula xml:id=""formula_62"">L un ( f ) = E ({ f (x) f (x i ) − f (",1
">25]</ref>.</p><p>Recently, self-supervised representation learning algorithms that use a contrastive loss have outperformed even supervised learning <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b23""> N in the objective function, in line with the observation that a larger number of negative/positive examples in the objective leads to better results <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b15"">16]</ref>. The last two terms in the bound grow slowly with N, but the effect of g and flipping <ref type=""bibr"" target=""#b26"">[27]</ref>, or different views of the same scene <ref type=""bibr"" target=""#b32"">[33]</ref>. Chen et al. <ref type=""bibr"" target=""#b1"">[2]</ref> extensively study verious data augmentation methods. For language, Logeswaran and Lee <ref type=""bibr"" target= nd STL10</head><p>First, for CIFAR10 <ref type=""bibr"" target=""#b22"">[23]</ref> and STL10 <ref type=""bibr"" target=""#b5"">[6]</ref>, we implement SimCLR <ref type=""bibr"" target=""#b1"">[2]</ref> with ResNet-50 <ref type=""bibr"" target=""#b14"">[15]</ref> as the encoder architecture and use the Adam optimize encoder architecture and use the Adam optimizer <ref type=""bibr"" target=""#b18"">[19]</ref> with learning rate 0.001 and weight decay 1e − 6. Following <ref type=""bibr"" target=""#b1"">[2]</ref>, we set the temperature t = 0.5 and the dimension of the latent vector to 128. All the models are trained for worked.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>B Experiment Details</head><p>Cifar10 and STL10 We adopt PyTorch to implement SimCLR <ref type=""bibr"" target=""#b1"">[2]</ref> with Resnet-50 <ref type=""bibr"" target=""#b14"">[15]</ref> as the encoder architecture and use the Adam optimize",1
"l></formula><p>where, for simplicity, we set Q to the finite N. The class prior τ + can be estimated from data <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b17"">18]</ref> or treated as a hyperparameter. Theorem 3 bounds the error due to finite N and M as decreasing with rate O(N",0
"ommon applications of PU learning are retrieval or outlier detection <ref type=""bibr"" target=""#b9"">[10]</ref><ref type=""bibr"" target=""#b10"">[11]</ref><ref type=""bibr"" target=""#b11"">[12]</ref>. Our approach is related to unbiased PU learning, where the unlabeled data is used as negative examples, but",0
"proaches to contrastive learning lies in their strategy of obtaining positive pairs. Examples in computer vision include random cropping and flipping <ref type=""bibr"" target=""#b26"">[27]</ref>, or different views of the same scene <ref type=""bibr"" target=""#b32"">[33]</ref>. Chen et al. <ref type=""bibr",0
"ref>. Remarkable success has also been achieved in the language domain <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b24"">25]</ref>.</p><p>Recently, self-supervised representation learning algorithms that use a contrastive loss have outperfo",0
"ormed even supervised learning <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b23"">24]</ref>. The key idea of contrastive learning is to contrast semantically similar (positive) and dissimilar (negative [33]</ref>. Chen et al. <ref type=""bibr"" target=""#b1"">[2]</ref> extensively study verious data augmentation methods. For language, Logeswaran and Lee <ref type=""bibr"" target=""#b23"">[24]</ref> treat the context sentences as positive samples to efficiently learn sentence representations. Srinivas et a nd paraphrase identification (MSRP) <ref type=""bibr"" target=""#b7"">[8]</ref>. Our experimental settings follow those for quick-thought (QT) vectors in <ref type=""bibr"" target=""#b23"">[24]</ref>.</p><p>In contrast to vision tasks, positive pairs here are chosen as neighboring sentences, which can form thod.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>Sentence Embedding</head><p>We adopt the official code 3 of quick-thought (QT) vectors <ref type=""bibr"" target=""#b23"">[24]</ref>. To implement the debiased objective, we only modify the ""src/s2v-model.py"" file and left the rest of the co",0
"arget=""#b22"">[23]</ref> and STL10 <ref type=""bibr"" target=""#b5"">[6]</ref>, we implement SimCLR <ref type=""bibr"" target=""#b1"">[2]</ref> with ResNet-50 <ref type=""bibr"" target=""#b14"">[15]</ref> as the encoder architecture and use the Adam optimizer <ref type=""bibr"" target=""#b18"">[19]</ref> with learni .0""><head>B Experiment Details</head><p>Cifar10 and STL10 We adopt PyTorch to implement SimCLR <ref type=""bibr"" target=""#b1"">[2]</ref> with Resnet-50 <ref type=""bibr"" target=""#b14"">[15]</ref> as the encoder architecture and use the Adam optimizer <ref type=""bibr"" target=""#b18"">[19]</ref> with learni",0
"augmentations of the same image. We again set M = 1 for a fair comparison. Methods are tested at 100k environment steps on the DeepMind control suite <ref type=""bibr"" target=""#b31"">[32]</ref>, which consists of several continuous control tasks. We retrain each model 3 times and show the mean and sta",0
"ised representation learning algorithms that use a contrastive loss have outperformed even supervised learning <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b23"">24]</ref>. The key idea of contrastive",0
"ameter scale of the model exceeds the memory limit of a single device.</p><p>Recently, pipeline parallelism <ref type=""bibr"" target=""#b9"">[10]</ref>- <ref type=""bibr"" target=""#b11"">[12]</ref> has been proposed as a promising approach for training large DNN models. The idea is to partition model laye l, and re-computing the parts of the graph in between those nodes during backpropagation. The other category is asynchronous(async) pipeline training <ref type=""bibr"" target=""#b11"">[12]</ref>. This manner inserts mini-batches into pipeline continuously and discards the original sync operations to ac optimization space includes DP, pipelined parallelism, and hybrid approaches combining both. Current state-of-theart pipeline partitioning algorithm <ref type=""bibr"" target=""#b11"">[12]</ref> is not able to be applied to synchronous training effectively. Some other work <ref type=""bibr"" target=""#b9"" .org/ns/1.0""><head>B. Device Assignment</head><p>Device assignment affects communication efficiency and computing resource utilization. Previous work <ref type=""bibr"" target=""#b11"">[12]</ref> uses hierarchical planning and works well for asynchronous training. However, it lacks consideration of sync v> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>D. Contributions over previous work</head><p>Previous works on pipeline planning includes PipeDream <ref type=""bibr"" target=""#b11"">[12]</ref> (for asynchronous training) and torchgpipe <ref type=""bibr"" target=""#b22"">[23]</ref>, a community implementa icated on two devices. For the first stage, we split the micro-batch further into 2 even slices, and assign each to a device. An alternative approach <ref type=""bibr"" target=""#b11"">[12]</ref> (Fig. <ref type=""figure"" target=""#fig_6"">8(b)</ref>) is not to split, but to schedule an entire micro-batch",1
"sm <ref type=""bibr"" target=""#b41"">[43]</ref> . Some prior studies <ref type=""bibr"" target=""#b8"">[9]</ref>, <ref type=""bibr"" target=""#b42"">[44]</ref>- <ref type=""bibr"" target=""#b46"">[48]</ref>   data parallelism. As a commonly used performance optimization method, gradients accumulation <ref type=""bi",0
"g only if DP optimizations <ref type=""bibr"" target=""#b4"">[5]</ref>, <ref type=""bibr"" target=""#b5"">[6]</ref>, <ref type=""bibr"" target=""#b7"">[8]</ref>, <ref type=""bibr"" target=""#b16"">[17]</ref>- <ref type=""bibr"" target=""#b19"">[20]</ref> are unable to achieve satisfactory efficiency, or the model size",0
"f>. For deep learning, a trend has been increasing the model scale up to the limit of modern AI hardware. Many state-of-the-art DNN models (e.g., NLP <ref type=""bibr"" target=""#b1"">[2]</ref>, Internet scale E-commerce search/recommendation systems <ref type=""bibr"" target=""#b2"">[3]</ref>, <ref type=""b",0
"t=""#b13"">[14]</ref>, <ref type=""bibr"" target=""#b37"">[39]</ref>, <ref type=""bibr"" target=""#b38"">[40]</ref>, <ref type=""bibr"" target=""#b49"">[51]</ref>- <ref type=""bibr"" target=""#b52"">[54]</ref>. This paper focuses on model partition between layers, namely, pipeline parallelism.</p><p>Pipeline parallel",0
"ww.tei-c.org/ns/1.0""><head>A. USMPep: Universal Sequence Models for Peptide Binding Prediction</head><p>The approach builds on the UDSMProt-framework <ref type=""bibr"" target=""#b11"">[12]</ref> and related work in natural language processing <ref type=""bibr"" target=""#b12"">[13]</ref>. We distinguish tw C binding prediction by replacing the output layer with a concat pooling layer and two fully connected layers. The setup closely follows that used in <ref type=""bibr"" target=""#b11"">[12]</ref>, where protein properties were predicted. The smaller dataset sizes and shorter sequence lengths in the pept the reduction of the number of layers from 3 to 1, of the number of hidden units from 1150 to 64 and of the embedding size from 400 to 50. Similar to <ref type=""bibr"" target=""#b11"">[12]</ref>, the training procedure included 1-cycle learning rate scheduling <ref type=""bibr"" target=""#b14"">[15]</ref> her efforts might be required to truly leverage the potential of unlabeled peptide data in order to observe similar improvements as seen for proteins <ref type=""bibr"" target=""#b11"">[12]</ref> in particular for small datasets.  Turning to MHC Class II binding prediction, we aim to demonstrate the uni ta, the protein language model only reaches an accuarcy of 0.137, which is is considerably lower than the accuracy of 0.41 reported in the literature <ref type=""bibr"" target=""#b11"">[12]</ref>. This effect is a direct consequence of the considerably smaller model size (1 instead of 3 layers; 64 inste",1
"tion</head><p>The approach builds on the UDSMProt-framework <ref type=""bibr"" target=""#b11"">[12]</ref> and related work in natural language processing <ref type=""bibr"" target=""#b12"">[13]</ref>. We distinguish two variants of our approach, either train the regression from scratch or employ language mo ]</ref>, the training procedure included 1-cycle learning rate scheduling <ref type=""bibr"" target=""#b14"">[15]</ref> and discriminative learning rates <ref type=""bibr"" target=""#b12"">[13]</ref> during finetuning. Target variables for the regression model were log-transformed half-maximal inhibitory co",0
"s a very challenging task that is, however, a crucial sub task for neoantigen identification for practical realizations of personalized immunotherapy <ref type=""bibr"" target=""#b0"">[1]</ref>.</p><p>The MHC binding prediction is a well-established problem in bioinformatics with a large number of exist",0
"usly for MHC binding prediction <ref type=""bibr"">[10; 11]</ref> and we discuss in more detail how USMPep stands out from these approaches. MHCnuggets <ref type=""bibr"" target=""#b9"">[10]</ref> is rather similar to the proposed approach (apart from the use of fixed embeddings), but relies on a complex",0
"xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2.1"">Knowledge Distillation</head><p>Hinton's work first proposes the concept of knowledge distillation <ref type=""bibr"" target=""#b9"">[10]</ref>. By introducing soft-targets related to teacher networks as part of the objective function, the training of s relax the label, training the student network to keep the same output as the teacher network on a soft label will result in a significant improvement <ref type=""bibr"" target=""#b9"">[10]</ref>. We follow a similar setup in this strategy. Note that in the experiments we must also use deep neural networ roaches such as considering the level of sample <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b26"">27]</ref> and model structure <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b21"">22]</ref>. Each method is based on different concerns to mine the potentially u",1
"label></label><figDesc>illustrates the main idea of stable feature distillation, which consists of a deep global balancing regression (DGBR) algorithm<ref type=""bibr"" target=""#b12"">[13]</ref>, a teacher network and a student network. The DGBR algorithm optimizes a deep autoencoder model for feature the true label of 𝑆 𝑐 in a more sophisticated manner is another promising direction.</p><p>Feature-Based Module. The current stable feature approach <ref type=""bibr"" target=""#b12"">[13]</ref> needs much time and computing resources. For implementing the industry recommender system, we need more effi",1
"larity bias <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b5"">6]</ref>, previous model bias <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b16"">17]</ref> and position bias <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""b bibr"" target=""#b23"">[24]</ref> and modeling with uniform data directly <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b22"">23]</ref>. In this paper, we would like to study methods for better use of the e the biases do not reflect the true performance of a recommender system, and that explicitly handling of the biases may help improve the performance <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b30"">31]</ref>. Most of the previous works "">[15,</ref><ref type=""bibr"" target=""#b16"">17]</ref>.</p><p>A recent work has shown that a uniform data can alleviate the previous model bias problem <ref type=""bibr"" target=""#b15"">[16]</ref>. But the uniform data is always few and expensive to collect in real recommender systems. To collect a unifo ll traffic (e.g., 1%).</p><p>In this paper, we focus on how to solve the bias problems in a recommender system with a uniform data. Along the line of <ref type=""bibr"" target=""#b15"">[16]</ref>, we conduct empirical studies on a real advertising system and a public dataset to validate the usefulness o a from the perspective of knowledge distillation.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3"">MOTIVATION</head><p>In a recent work <ref type=""bibr"" target=""#b15"">[16]</ref>, it is shown that a uniform (i.e., unbiased) data can alleviate the previous model bias problem. In this sec",1
"rriage of knowledge distillation and recommender systems has also attracted the attention of the researchers <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b29"">30,</ref><ref type=""bibr"" target=""#b33"">34]</ref>. Most of these works focus on using knowledge distillation to extract",0
"iasedMF). We first consider the case where the proposed framework is implemented using a lowrank model. We use biased matrix factorization (biasedMF) <ref type=""bibr"" target=""#b11"">[12]</ref> as the baseline, which is one of the most classic basic models in recommender systems. In this method, a use",0
"org/ns/1.0""><head n=""1"">INTRODUCTION</head><p>Recommender Systems as a feedback loop system may suffer from the bias problems such as popularity bias <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b5"">6]</ref>, previous model bias <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""b",0
"ifferent forms of knowledge (e.g., alignment of the hidden layers <ref type=""bibr"" target=""#b21"">[22]</ref> or the relation between the hidden layers <ref type=""bibr"" target=""#b31"">[32]</ref>). Some recent works are no longer limited to model structure, but considers sample-based knowledge distillat",0
"dge (instance, feature and model) and strategies (adaptive, collective and integrative) in transfer learning <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b18"">19]</ref>.</p><p>In addition, we must keep in mind that the different considerations when using these four distillation",0
"soft-targets related to teacher networks as part of the objective function, the training of student networks is guided to achieve knowledge transfer <ref type=""bibr"" target=""#b17"">[18]</ref>. A series of followup works develop different distillation structures (e.g., multiple teachers <ref type=""bi y are also related to the types of knowledge (instance, feature and model) and strategies (adaptive, collective and integrative) in transfer learning <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b18"">19]</ref>.</p><p>In addition, we must keep in mind that the different consider",0
"ommender systems has also attracted the attention of the researchers <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b29"">30,</ref><ref type=""bibr"" target=""#b33"">34]</ref>. Most of these works focus on using knowledge distillation to extract some useful knowledge from some auxilia",0
"s for both the observed and unobserved samples. The imputation model can be learned by machine learning models <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b13"">14]</ref> with the observed data. A doubly robust method <ref type=""bibr"" target=""#b6"">[7]</ref> combines the IPS metho",0
"dge transfer <ref type=""bibr"" target=""#b17"">[18]</ref>. A series of followup works develop different distillation structures (e.g., multiple teachers <ref type=""bibr"" target=""#b7"">[8]</ref> and cascade distillations <ref type=""bibr"" target=""#b3"">[4]</ref>) and different forms of knowledge (e.g., ali",0
"f followup works develop different distillation structures (e.g., multiple teachers <ref type=""bibr"" target=""#b7"">[8]</ref> and cascade distillations <ref type=""bibr"" target=""#b3"">[4]</ref>) and different forms of knowledge (e.g., alignment of the hidden layers <ref type=""bibr"" target=""#b21"">[22]</r",0
"ome recent works are no longer limited to model structure, but considers sample-based knowledge distillation <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b26"">27]</ref>. In this paper, we further expand the definition of distillation to include label-based and feature-based for in the study rather than the past knowledge distillation approaches such as considering the level of sample <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b26"">27]</ref> and model structure <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b21"">22]</ref>. Each me",0
"ref>, propensity computation <ref type=""bibr"" target=""#b23"">[24]</ref> and modeling with uniform data directly <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b22"">23]</ref>. In this paper, we would like",0
"years, many search result diversification approaches have been proposed <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b22"" pe=""bibr"" target=""#b22"">[23]</ref>, and NTN <ref type=""bibr"" target=""#b23"">[24]</ref>. The explicit approaches <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b24"" of query ambiguity, many models have been proposed to solve this problem <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b22"" regard the query as several subtopics, and explicitly leverage subtopics to determine the diversity of results <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b17"">18]</ref>. Most explicit approaches foc type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b8"">9]</ref> and supervised approaches such as DSSA <ref type=""bibr"" target=""#b11"">[12]</ref>.</p><p>Studies have shown that supervised approaches <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bib "" target=""#b5"">[6]</ref> is another unsupervised explicit method calculating the distribution by counting the relevant document of the subtopic. DSSA <ref type=""bibr"" target=""#b11"">[12]</ref> introduces the machine learning method into explicit approaches. It calculates the distribution using the RN selected document ranking 𝑆 or the candidate document 𝐶 to fool the discriminator. So it also needs a score function. In our method we adapt the DSSA <ref type=""bibr"" target=""#b11"">[12]</ref> score function for the generator. We introduce the score function in the form of Eq. ( <ref type=""formula"">2 to generate the final document ranking result.</p><p>In the training process, we first train R-LTR <ref type=""bibr"" target=""#b25"">[26]</ref> and DSSA <ref type=""bibr"" target=""#b11"">[12]</ref> respectively using MLE loss in both ways. It is because our framework needs a warm start to avoid the deviat bibr"" target=""#b25"">[26]</ref>, PAMM <ref type=""bibr"" target=""#b5"">[6]</ref>, R-LTR-NTN, PAMM-NTN <ref type=""bibr"" target=""#b23"">[24]</ref>, and DSSA <ref type=""bibr"" target=""#b11"">[12]</ref> as supervised baseline methods. Top 20 results of Lemur are used to train the supervised methods. Top 50(𝑍 ) cit method. We use LSTM <ref type=""bibr"" target=""#b7"">[8]</ref> as the RNN cell for comparison. In our experiments, we conduct the list-pairwise loss <ref type=""bibr"" target=""#b11"">[12]</ref> to train DSSA method. The feature vector 1 http://playbigdata.ruc.edu.cn/dou/hdiv/ 2 Lemur service: http://b =""#b8"">9]</ref> and supervised approaches such as DSSA <ref type=""bibr"" target=""#b11"">[12]</ref>.</p><p>Studies have shown that supervised approaches <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b25",1
"ow to generate training samples effectively is still a challenge for training diversification models.</p><p>To tackle this problem, inspired by IRGAN <ref type=""bibr"" target=""#b18"">[19]</ref>, we introduce Generative Adversarial Network (GAN) <ref type=""bibr"" target=""#b9"">[10]</ref> into search resu e text sequence generation area combined with Monte Carlo search. It is also used in the traditional information retrieval area, Wang proposed IR-GAN <ref type=""bibr"" target=""#b18"">[19]</ref> which consists of two information retrieval models in it. Comparing to other information retrieval models, I _7"">𝐷 𝜙 (𝑑 |𝑞, 𝑆) = 𝜎 𝑓 𝜙 (𝑑 |𝑞, 𝑆) = exp(𝑓 𝜙 (𝑑 |𝑞, 𝑆)) 1 + exp(𝑓 𝜙 (𝑑 |𝑞, 𝑆)) .<label>(7)</label></formula><p>Please note that different from IRGAN <ref type=""bibr"" target=""#b18"">[19]</ref>, DVGAN-doc has an additional component 𝑆 to represent the former selected documents.</p><p>As 𝑆 contain orde s put into practice in the contiguous area firstly, it is difficult to calculate the generator gradient due to its discrete nature. Inspired by IRGAN <ref type=""bibr"" target=""#b18"">[19]</ref>, we generate negative document set 𝐷 ′ by selecting the documents from the candidate document set with the h",1
"𝑑 |𝑞, 𝑆). In DVGAN-rank method, we calculate the score function of 𝜉 (particular 𝑙), i.e., D 𝜃 (𝑙 |𝑞, 𝐶) and G 𝜙 (𝑙 |𝑞, 𝐶), using Plackett-Luce model <ref type=""bibr"" target=""#b6"">[7]</ref>. Specifically, we have:</p><formula xml:id=""formula_3"">DVGAN-doc D 𝜙 (𝑑 |𝑞, 𝑆) = 𝑓 𝜙 (𝑑 |𝑞, 𝑆) G 𝜃 (𝑑 |𝑞, 𝑆) =",0
"ming the problem of passing gradients from the discriminator to the generator, GAN has just been introduced into a discrete area. For example, SeqGAN <ref type=""bibr"" target=""#b12"">[13]</ref> introduces the GAN to the text sequence generation area combined with Monte Carlo search. It is also used in",0
"er different user intents underlying an ambiguous or a broad query. In recent years, many search result diversification approaches have been proposed <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b16"">17 cording to the score function, approaches of diversification can be divided into explicit approaches and implicit approaches. The implicit approaches <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b25""> rget=""#b22"">23,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b25"">26</ref>] are able to outperform the heuristic approaches <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b17"">18]</ref> by learning an optimized ranking <p>As search result diversification is an effective way to solve the problem of query ambiguity, many models have been proposed to solve this problem <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b16"">17 d be different from the previously selected documents. The diversification score function of implicit approaches can be handcrafted rules such as MMR <ref type=""bibr"" target=""#b0"">[1]</ref> or a supervised measure such as R-LTR <ref type=""bibr"" target=""#b25"">[26]</ref>, PAMM <ref type=""bibr"" target= ance to the query and novelty to the selected documents. In the early years' research on diversification, implicit methods are most unsupervised. MMR <ref type=""bibr"" target=""#b0"">[1]</ref> can be regarded as the foundation of implicit methods. Its diversification score function is as follows.</p><f",0
"pe=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b22"">[23]</ref><ref type=""bibr"" target=""#b23"">[24]</ref><ref type=""bibr"" target=""#b2 /ref>. The explicit approaches <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b24"">25]</ref> stress the relevance between the documents and the subtopics of the q pe=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b22"">[23]</ref><ref type=""bibr"" target=""#b23"">[24]</ref><ref type=""bibr"" target=""#b2 roaches, explicit diversification approaches can also be categorized into heuristic approaches such as xQuAD <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b17"">18]</ref> and PM2 <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target= target=""#b25"">26</ref>] are able to outperform the heuristic approaches <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b17"">18]</ref> by learning an optimized ranking function. However, the large number of candidate documents with only few sub rmine the diversity of results <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b17"">18]</ref>. Most explicit approaches focus on the subtopic coverage of results, by calculating subtopic distribution bas ous documents. The 𝑆 rel function reflects the 𝑑's relevance to the query 𝑞 and the 𝑆 sub function reflects the 𝑑's relevance to the subtopics. xQuAD <ref type=""bibr"" target=""#b17"">[18]</ref> is one of the representative methods of unsupervised explicit approaches. It defines the subtopic distributi ><p>We compare DVGAN with several existing diversification methods. We use Lemur as our non-diversified baseline method. We use xQuAD, TxQuAD, HxQuAD <ref type=""bibr"" target=""#b17"">[18]</ref>, PM2 <ref type=""bibr"" target=""#b5"">[6]</ref>, TPM2 <ref type=""bibr"" target=""#b4"">[5]</ref>, and HPM2 <ref ty",0
"on approaches have been proposed <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b22"">[23]</ref><ref type=""bibr"" target=""#b23 NTN <ref type=""bibr"" target=""#b23"">[24]</ref>. The explicit approaches <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b24"">25]</ref> stress the relevance between n proposed to solve this problem <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b22"">[23]</ref><ref type=""bibr"" target=""#b23 nd explicitly leverage subtopics to determine the diversity of results <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b17"">18]</ref>. Most explicit approaches focus on the subtopic coverage of results, levance measures. Similar to implicit approaches, explicit diversification approaches can also be categorized into heuristic approaches such as xQuAD <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b17"">18]</ref> and PM2 <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" targ",0
"ambiguous or a broad query. In recent years, many search result diversification approaches have been proposed <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b17"">1 oaches such as xQuAD <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b17"">18]</ref> and PM2 <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b8"">9]</ref> and supervised approaches such as DSSA <ref type=""bibr"" target=""#b11"">[12 arget=""#b23"">24,</ref><ref type=""bibr"" target=""#b25"">26</ref>] are able to outperform the heuristic approaches <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b17"">18]</ref> by learning an optimized ranking function. However, the large number of an effective way to solve the problem of query ambiguity, many models have been proposed to solve this problem <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b17"">1 target=""#b25"">[26]</ref>, PAMM <ref type=""bibr"" target=""#b22"">[23]</ref>, and NTN <ref type=""bibr"" target=""#b23"">[24]</ref>. The explicit approaches <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b17""> between documents, explicit approaches regard the query as several subtopics, and explicitly leverage subtopics to determine the diversity of results <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b17""> ed explicit approaches. It defines the subtopic distribution by calculating the probability of the selected documents not covering the subtopics. PM2 <ref type=""bibr"" target=""#b5"">[6]</ref> is another unsupervised explicit method calculating the distribution by counting the relevant document of the rsification methods. We use Lemur as our non-diversified baseline method. We use xQuAD, TxQuAD, HxQuAD <ref type=""bibr"" target=""#b17"">[18]</ref>, PM2 <ref type=""bibr"" target=""#b5"">[6]</ref>, TPM2 <ref type=""bibr"" target=""#b4"">[5]</ref>, and HPM2 <ref type=""bibr"" target=""#b8"">[9]</ref> as our unsuper as our unsupervised baseline methods. We use ListMLE <ref type=""bibr"" target=""#b21"">[22]</ref>, R-LTR <ref type=""bibr"" target=""#b25"">[26]</ref>, PAMM <ref type=""bibr"" target=""#b5"">[6]</ref>, R-LTR-NTN, PAMM-NTN <ref type=""bibr"" target=""#b23"">[24]</ref>, and DSSA <ref type=""bibr"" target=""#b11"">[12]</",0
"ng vector for document 𝑑, which is the distributed representation of document. It can be constructed in different ways, In this paper, we use doc2vec <ref type=""bibr"" target=""#b13"">[14]</ref> to get document embeddings.</p><p>𝑥 𝑑,𝑞 and 𝑥 𝑑,𝑖 : Relevance feature vectors between the document 𝑑 and que",0
"target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b17"">18]</ref> and PM2 <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b8"">9]</ref> and supervised approaches such as DSSA <ref type=""bibr"" target=""#b11"">[12]</ref>.</p><p>Studies have shown that topics for each query. The relevance rating is given at subtopic level. We use google query suggestions as subtopics, which are released by Hu et al. <ref type=""bibr"" target=""#b8"">[9]</ref> on their website 1 . we only use the first level subtopics and will adapt the hierarchical structure in future HxQuAD <ref type=""bibr"" target=""#b17"">[18]</ref>, PM2 <ref type=""bibr"" target=""#b5"">[6]</ref>, TPM2 <ref type=""bibr"" target=""#b4"">[5]</ref>, and HPM2 <ref type=""bibr"" target=""#b8"">[9]</ref> as our unsupervised baseline methods. We use ListMLE <ref type=""bibr"" target=""#b21"">[22]</ref>, R-LTR <ref typ",0
"radient algorithm of REINFORCE. Similar MDP con gurations are used to model the sequential document selection process in search result diversi cation <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b34"">35]</ref> and multi-page search <ref type=""bibr"" target=""#b40"">[41]</ref>. The",1
"ibr"" target=""#b12"">[13]</ref>, DBGD was further improved so that the click data can be used to judge the user preference of the document rankings. In <ref type=""bibr"" target=""#b16"">[17]</ref>, a cascading bandits model is proposed to identify the K most attractive document for users.MDP has also sho",0
"oaches <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b15"">16]</ref>, and listwise approaches <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b35"">36]</ref>. Speci cally, the pairwise metho ion. We compared the proposed PPG to the traditional learning to rank baselines, including RankSVM <ref type=""bibr"" target=""#b15"">[16]</ref>, RankNet <ref type=""bibr"" target=""#b1"">[2]</ref>, ListNet <ref type=""bibr"" target=""#b3"">[4]</ref>, AdaRank <ref type=""bibr"" target=""#b35"">[36]</ref>, and MDPRa",0
"ype=""bibr"" target=""#b27"">[28]</ref> can be used to directly incorporate multi-hop neighborhood information of a node without explicit message-passing <ref type=""bibr"" target=""#b32"">[33]</ref>. Intuitively, propagation based on personalized PageRank corresponds to infinitely many neighborhood aggrega infinitely many neighborhood aggregation layers where the node influence decays exponentially with each layer. However, as proposed, Klicpera et al. <ref type=""bibr"" target=""#b32"">[33]</ref>'s approach does not easily scale to large graphs since it performs an expensive variant of power iteration d cking multiple layers may suffer from over-smoothing that can reduce predictive performance.</p><p>To tackle both of these challenges Klicpera et al. <ref type=""bibr"" target=""#b32"">[33]</ref> suggest decoupling the feature transformation from the propagation. In their PPNP model, predictions are fir power iteration to compute the final predictions instead. Unfortunately, even a moderate number of power iteration evaluations (e.g. Klicpera et al. <ref type=""bibr"" target=""#b32"">[33]</ref> used K = 10 to achieve a good approximation) is prohibitively expensive for large graphs since they need to ome denser, which significantly reduces the efficiency of the subsequent learning step. Both of these approaches are a special case of the PPNP model <ref type=""bibr"" target=""#b32"">[33]</ref> which experimentally shows higher classification performance <ref type=""bibr"" target=""#b17"">[18,</ref><ref t aset (10.5M nodes, 133M edges, 2.8M node features). In addition to the two scalable baselines, we also evaluate how PPRGo compares to the APPNP model <ref type=""bibr"" target=""#b32"">[33]</ref> which we build upon. The results are summarized in Table <ref type=""table"" target=""#tab_2"">2</ref>. We can s model <ref type=""bibr"" target=""#b32"">[33]</ref> which experimentally shows higher classification performance <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b32"">33]</ref>.</p><p>Approximating PageRank. Recent approaches combine basic techniques to create algorithms with enhanced",1
"d (seeded) PageRank vector of node i. PageRank and its many variants <ref type=""bibr"" target=""#b27"">[28,</ref><ref type=""bibr"" target=""#b37"">38,</ref><ref type=""bibr"" target=""#b46"">47]</ref> have been extensively studied in the literature. Here we are interested in efficient and scalable algorithms",0
"caling these methods to larger graphs for use in real-world problems <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b26"" ave been proposed to improve the efficiency of graph neural networks <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b26"" ef><ref type=""bibr"" target=""#b53"">54]</ref>. Unfortunately, there are few large graph baseline datasets available; apart from a handful of exceptions <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b53"">54]</ref>, the scalability of most GNN methods has been demonstrated on graphs scussed in § 1 the most prevalent approach to scalability is to sample a subset of the graph, e.g. based on different importance scores for the nodes <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b40 e. Huang et al. <ref type=""bibr"" target=""#b26"">[27]</ref> propose an adaptive sampling strategy with a trainable sampler per layer, and Chiang et al. <ref type=""bibr"" target=""#b15"">[16]</ref> sample a block of nodes corresponding to a dense subgraph identified by the clustering algorithm METIS <ref f type=""bibr"" target=""#b24"">[25]</ref> typically being the largest graph used for evaluation. <ref type=""foot"" target=""#foot_2"">4</ref> Chiang et al. <ref type=""bibr"" target=""#b15"">[16]</ref> recently introduced the Amazon2M graph (2.5M nodes, 61M edges, 100 node features) which is large in terms of t the benefits of PPRGo we compare the runtime, memory, and predictive performance with SGC <ref type=""bibr"" target=""#b48"">[49]</ref> and Cluster-GCN <ref type=""bibr"" target=""#b15"">[16]</ref>, two strong baselines that represent the current state-ofthe-art scalable GNNs. Since SGC and Cluster-GCN re",0
""" target=""#b41"">[42]</ref> and have since emerged as a powerful approach for solving many network mining tasks <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b21"">22, <head n=""5.1"">Large-Scale Datasets</head><p>The majority of previous approaches are evaluated on a small set of publicly available benchmark datasets <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b20"">",0
"are commonly used as benchmark datasets -Cora-Full <ref type=""bibr"" target=""#b6"">[7]</ref> (18.7K nodes, 62.4K edges, 8.7K node features) and PubMed <ref type=""bibr"" target=""#b52"">[53]</ref> (19.7K nodes, 44.3K edges, 0.5K node features) -as well as our newly introduced MAG-Scholar-C dataset (10.5M",0
"ithms (either reconstructing the adjacency matrix <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b30"">31]</ref> or feature matrix <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b31"">32]</ref>) are not compatible with real-world applications. To be specific, re "" target=""#b31"">[32]</ref> leverages marginalized denoising autoencoder to disturb the structure information. To build a symmetric graph autoencoder, <ref type=""bibr"" target=""#b23"">[24]</ref> proposes Laplacian sharpening as the counterpart of Laplacian smoothing in the encoder. The authors claim th l constraints to GAE and VGAE respectively, enforcing the latent representations to match a prior distribution for robust node embeddings.</p><p>GALA <ref type=""bibr"" target=""#b23"">[24]</ref> proposes a symmetric graph convolutional autoencoder recovering the feature matrix. The encoder is based on",1
"/features and are widely applied to represent network-structured data in social networks <ref type=""bibr"" target=""#b11"">[12]</ref>, citation networks <ref type=""bibr"" target=""#b15"">[16]</ref>, recommendation systems <ref type=""bibr"" target=""#b36"">[37]</ref>, etc. For tasks analyzing attributed graph has been a surge of approaches that focus on deep learning on graphs. Specifically, approaches from the family of graph convolutional networks (GCNs) <ref type=""bibr"" target=""#b15"">[16]</ref> have made great progress in many graph learning tasks <ref type=""bibr"" target=""#b38"">[39]</ref> and strength 2"">GCN-based Graph Embedding</head><p>As mentioned in the introduction, due to the strong representation power of graph convolutional networks (GCNs) <ref type=""bibr"" target=""#b15"">[16]</ref>, there are several GCN-based approaches for attributed graph embedding and they have achieved state-of-the-a",0
"r"" target=""#b33"">34,</ref><ref type=""bibr"" target=""#b35"">36]</ref> are matrix factorization extensions that add feature-related regularization terms. <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b4"">5]</ref> model features as latent variables in Bayesian networks.</p></div> <div",0
"ead n=""4.1"">Datasets</head><p>We conduct node clustering and link prediction experiments on four widely used network datasets (Cora, Citeseer, Pubmed <ref type=""bibr"" target=""#b25"">[26]</ref> and Wiki <ref type=""bibr"" target=""#b35"">[36]</ref>). Features in Cora and Citeseer are binary word vectors,",0
"oothing effect (Section 3.3.3).</p><p>Thirdly, we also argue that training objectives of these algorithms (either reconstructing the adjacency matrix <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b30"">31]</ref> or feature matrix <ref type=""bibr"" target=""#b23"">[24,</ref><ref type ""#b14"">[15]</ref> learn node embeddings by using GCN as the encoder, then decode by inner product with cross-entropy loss. As variants of GAE (VGAE), <ref type=""bibr"" target=""#b22"">[23]</ref> exploits adversarially regularized method to learn more robust node embeddings. <ref type=""bibr"" target=""#b3 r"" target=""#b14"">[15]</ref> combine graph convolutional networks with the (variational) autoencoder for representation learning.</p><p>ARGA and ARVGA <ref type=""bibr"" target=""#b22"">[23]</ref> add adversarial constraints to GAE and VGAE respectively, enforcing the latent representations to match a pr",0
"aining framework based on Bayesian Graph Neural Networks (BGNNs). The proposed BGNN incorporates a random graph generative model based on nodecopying <ref type=""bibr"" target=""#b22"">[24]</ref>. The node-copying model can be used to produce sample graphs that are similar to the observed graph, but the f type=""bibr"" target=""#b22"">24]</ref>, where <ref type=""bibr"" target=""#b21"">[23]</ref> uses a non-parametric model for the graph generative model and <ref type=""bibr"" target=""#b22"">[24]</ref> proposes a node copying model to achieve flexibility in the generative model and improve computational effic MMSBM is not an applicable graph model. As an alternative, we use a more general generative model for graphs based on copying nodes, as introduced in <ref type=""bibr"" target=""#b22"">[24]</ref>. We demonstrate in the following sections that this model can be adapted naturally to the recommender system can be adapted naturally to the recommender system setting.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.2"">Node Copying</head><p>In <ref type=""bibr"" target=""#b22"">[24]</ref>, Pal et al. introduce the node copying model for 𝑝 (G). Samples from this model are generated by probabilist recommender system setting.Bayesian Graph Neural Networks (BGNNs). The proposed BGNN incorporates a random graph generative model based on nodecopying<ref type=""bibr"" target=""#b22"">[24]</ref>. The node-copying model can be used to produce sample graphs that are similar to the observed graph, but the cant performance improvements in semi-supervised node classification when there are very few training labels <ref type=""bibr"" target=""#b21"">[23,</ref><ref type=""bibr"" target=""#b22"">24,</ref><ref type=""bibr"" target=""#b28"">30,</ref><ref type=""bibr"" target=""#b37"">39]</ref>.</p><p>In this paper, we prop icant performance improvements in semi-supervised node classification when there are very few training labels<ref type=""bibr"" target=""#b21"">[23,</ref><ref type=""bibr"" target=""#b22"">24,</ref><ref type=""bibr"" target=""#b28"">30,</ref><ref type=""bibr"" target=""#b37"">39]</ref>.In this paper, we propose a n and does not utilize the node attributes or labels. These limitations were addressed in the follow-up works <ref type=""bibr"" target=""#b21"">[23,</ref><ref type=""bibr"" target=""#b22"">24]</ref>, where <ref type=""bibr"" target=""#b21"">[23]</ref> uses a non-parametric model for the graph generative model a",1
"ormance in Netflix contest. MF models (such as pLAS <ref type=""bibr"" target=""#b13"">[15]</ref>, MF <ref type=""bibr"" target=""#b16"">[18]</ref> and SVD++ <ref type=""bibr"" target=""#b15"">[17]</ref>) learn user and item embeddings by reconstructing the historical user-item interactions. The learned user an ent Semantic Analysis (pLAS <ref type=""bibr"" target=""#b13"">[15]</ref>), Matrix Factorization (MF <ref type=""bibr"" target=""#b16"">[18]</ref>) and SVD++ <ref type=""bibr"" target=""#b15"">[17]</ref>. They learn user and item embeddings by reconstructing the historical user-item interactions. The learned us",0
"ch other, in that increasing one can result in sacrificing the other. This trade-off has been studied in both the natural language generation setting <ref type=""bibr"" target=""#b1"">[2]</ref>, where Caccia et al. evaluate diversity vs. quality and show the danger of focusing on one metric, and in reco",0
"between the probability that an item 𝑖 is recommended for a specific user and the probability that item 𝑖 is recommended for any user. It is computed <ref type=""bibr"" target=""#b25"">[27]</ref> as:</p><formula xml:id=""formula_23"">SRDP@k = 1 |U| 𝑢 ∈U 1 |𝐼 𝑘 (𝑢)| 𝑖 ∈𝐼 𝑘 (𝑢) max 𝑃 𝑖 (𝑢)−𝑃 𝑖 (U), 0 * 𝑟𝑒𝑙",0
"onships. An effective and common approach to alleviate the data sparsity problem is to leverage side information. For example, factorization machines <ref type=""bibr"" target=""#b23"">[25]</ref> can provide a mechanism for incorporating side information such as user demographics and item attributes. An",0
"n we apply multiple layers to leverage large receptive fields. Several recent works attribute this performance degradation to the oversmoothing issue <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b32"">33]</ref>, which states that representat has the similar trend. The node representations generated by multiple GCN layers, like 6 layers, are very difficult to be separated.  Several studies <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b14"">15]</ref> attribute this performance degradation phenomenon to the over-smoothin hors show that SGC corresponds to a low-pass-type filter on the spectral domain, thus deriving smoothing features across a graph. Another recent work <ref type=""bibr"" target=""#b2"">[3]</ref> verify that smoothing is the nature of most typical graph convolutions. It is showed that reasonable smoothing g and self-training to overcome the limitation of shallow architectures. A smoothness regularizer term and adaptive edge optimization are proposed in <ref type=""bibr"" target=""#b2"">[3]</ref> to relieve the over-smoothing problem. Jumping Knowledge Network <ref type=""bibr"" target=""#b32"">[33]</ref> dep",1
"large receptive fields. Several recent works attribute this performance degradation to the oversmoothing issue <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b32"">33]</ref>, which states that representations from different classes become inse aligns with the over-smoothing issue. The previous descriptions of the over-smoothing issue simplify the assumption of non-linear activation function <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b32"">33]</ref> or make approximations of different probabilities <ref type=""bibr"" t king many layers can bring the oversmoothing issue, which means that representations of nodes converge to indistinguishable limits. To our knowledge, <ref type=""bibr"" target=""#b14"">[15]</ref> is the first attempt to demystify the over-smoothing issue in the GCN model. The authors first demonstrate t re not enough to propagate training signals to the whole graph when the number of training nodes is limited under a semi-supervised learning setting. <ref type=""bibr"" target=""#b14"">[15]</ref> applies co-training and self-training to overcome the limitation of shallow architectures. A smoothness regu the smoothness metric value of graph node representations has a slight downward trend as the number of propagation iterations increases. According to <ref type=""bibr"" target=""#b14"">[15]</ref>, the node representations suffering from the oversmoothing issue will converge to the same value or be propo we provide a theoretical analysis of the above observation when building very deep graph neural networks, which aligns with the over-smoothing issue. <ref type=""bibr"" target=""#b14"">[15]</ref> and <ref type=""bibr"" target=""#b32"">[33]</ref> study the over-smoothing issue from the perspective of Laplaci ntations generated by multiple GCN layers, like 6 layers, are very difficult to be separated.  Several studies <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b14"">15]</ref> attribute this performance degradation phenomenon to the over-smoothing issue. However, we question this view",1
"The same problem is studied in <ref type=""bibr"" target=""#b32"">[33]</ref> by analyzing the connection of nodes' influence distribution and random walk <ref type=""bibr"" target=""#b16"">[17]</ref>. Recently, SGC <ref type=""bibr"" target=""#b30"">[31]</ref> is proposed by reducing unnecessary complexity in G",0
"ata built upon Pytorch. We consider the following baselines: Logistic Regression (LogReg), Multilayer Perceptron (MLP), Label Propagation (LabelProp) <ref type=""bibr"" target=""#b1"">[2]</ref>, Normalized Laplacian Label Propagation (LabelProp NL) <ref type=""bibr"" target=""#b1"">[2]</ref>, Cheb-Net <ref ilayer Perceptron (MLP), Label Propagation (LabelProp) <ref type=""bibr"" target=""#b1"">[2]</ref>, Normalized Laplacian Label Propagation (LabelProp NL) <ref type=""bibr"" target=""#b1"">[2]</ref>, Cheb-Net <ref type=""bibr"" target=""#b3"">[4]</ref>, Graph Convolutional Network (GCN) <ref type=""bibr"" target=""",0
"istinguishability only when an extremely receptive field, like 200-hop, is adopted.  According to the Perron-Frobenius Theorem for Primitive Matrices <ref type=""bibr"" target=""#b26"">[27]</ref>, there exists an eigenvalue r for an n × n non-negative primitive matrix such that r &gt; |λ| for any eigenv",0
"based on personalize PageRank, which can preserve the node's local information while gather information from a large neighborhood. Recently, Geom-GCN <ref type=""bibr"" target=""#b24"">[25]</ref> and non-local GNNs <ref type=""bibr"" target=""#b15"">[16]</ref> are proposed to capture longrange dependencies",0
"The detailed description of these datasets are provided in Appendix A.7. We implemented our proposed DAGNN and some necessary baselines using Pytorch <ref type=""bibr"" target=""#b23"">[24]</ref> and Pytorch Geometric <ref type=""bibr"" target=""#b4"">[5]</ref>, a library for deep learning on irregularly st",0
"is irreducible and aperiodic, then lim k →∞ P k = Π, where Π is the matrix with all rows equal to π and π can be computed by πP = π , s.t. i π i = 1 <ref type=""bibr"" target=""#b12"">[13]</ref>. It is obvious that π is the unique left eigenvector of P and is normalized such that all entries sum to 1.",0
"pe=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b31"">32,</ref><ref type=""bibr"" target=""#b33"">34,</ref><ref type=""bibr"" target=""#b34"">35,</ref><ref type=""bibr"" target=""#b37"" <ref type=""bibr"" target=""#b10"">[11]</ref>, GraphSAGE <ref type=""bibr"" target=""#b8"">[9]</ref>, GAT <ref type=""bibr"" target=""#b29"">[30]</ref>, and GIN <ref type=""bibr"" target=""#b31"">[32]</ref>, can be obtained under this framework by deploying different propagation and transformation mechanisms.</p><",0
"ons, such as node classification <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b29"">30,</ref><ref type=""bibr"" target=""#b30"" chanism to adaptively select a nodeâĂŹs sub-graph features at different ranges rather than to capture equally smoothed representations for all nodes. <ref type=""bibr"" target=""#b11"">[12]</ref> utilizes the relationship between GCN and PageRank <ref type=""bibr"" target=""#b22"">[23]</ref> to develop a pr sifier is applied to compute the classification probabilities. Notably, the separation of transformation and propagation processes is also adopted in <ref type=""bibr"" target=""#b11"">[12]</ref> and <ref type=""bibr"" target=""#b30"">[31]</ref> but for the sake of reducing complexity.</p><p>In this work, w ""#b29"">[30]</ref>, Mixture Model Network (MoNet) <ref type=""bibr"" target=""#b20"">[21]</ref>, Graph-SAGE <ref type=""bibr"" target=""#b8"">[9]</ref>, APPNP <ref type=""bibr"" target=""#b11"">[12]</ref>, and SGC <ref type=""bibr"" target=""#b30"">[31]</ref>. We aim to provide a rigorous and fair comparison between",0
"e fully trusted, which prevents their use in critical applications pertaining to fairness, privacy, and safety <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b39"">40]</ref>. For example, we can train a GNN model to predict the effects of drugs where we treat each drug as a molecula best of our knowledge, there is no existing study on interpreting GNNs at the model-level. The existing study <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b39"">40]</ref> only provides example-level explanations for graph models. As a radical departure from existing work, we prop of our knowledge, there are only a few existing studies focusing on the interpretability of deep graph models <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b39"">40]</ref>. The recent GNN interpretation tool GNN Explainer <ref type=""bibr"" target=""#b39"">[40]</ref> proposes to expla stigating model-level interpretations of GNNs, we have no baseline to compare with. Note that existing studies <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b39"">40]</ref> only focus on interpreting GNNs at example-level while ignoring the model-level explanations. Comparing with f deep graph models <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b39"">40]</ref>. The recent GNN interpretation tool GNN Explainer <ref type=""bibr"" target=""#b39"">[40]</ref> proposes to explain deep graph models at the example-level by learning soft masks. For a given example, it a",1
"tion <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b36"">37]</ref>, graph classification <ref type=""bibr"" target=""#b38"">[39,</ref><ref type=""bibr"" target=""#b46"">47]</ref>, and link prediction <ref type=""bibr"" target=""#b45"">[46]</ref>. In addition, extensive efforts have been made",0
"In addition, extensive efforts have been made towards different graph operations, such as graph convolution <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b18"">19]</ref>, graph pooling <ref type=""bibr"" target=""#b19"">[20,</ref><ref type=""bi",0
"plex networks (graphs), which widely exist in graphs from biochemistry, neurobiology, ecology, and engineering <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b29"">30]</ref>. Different motif sets can be fo",0
"#b44"">45,</ref><ref type=""bibr"" target=""#b47"">48]</ref> or model-level <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b24"">25]</ref> methods. Example-level interpretations explain the prediction for a given input example, by determining impor on, without respect to any specific input example. Input optimization <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b23"">[24]</ref><ref type=""bibr"" target=""#b24"">[25]</ref><ref type=""bibr"" target=""#b25"">[26]</ref> is the most popular model-level interpretation method. These two ca ep learning models on image data, known as input optimization methods <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b23"">[24]</ref><ref type=""bibr"" target=""#b24"">[25]</ref><ref type=""bibr"" target=""#b25"">[26]</ref>. These methods generally generate optimized input that can maximize for interpreting image and text models is known as input optimization <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b23"">[24]</ref><ref type=""bibr"" target=""#b24"">[25]</ref><ref type=""bibr"" target=""#b25"">[26]</ref><ref type=""bibr"" target=""#b42"">43]</ref>. However, as discussed in S",0
"ntionally instead of deliberately. Some recent news reported that these numerical errors brought about huge reputation risk, and even economic losses <ref type=""bibr"" target=""#b0"">[1]</ref>. Since the documents disclosed by the firm usually have the force of law, these errors should be thoroughly re check inconsistency errors in the null-hypothesis significance testing, presented in the academic papers in major psychology journals. A recent study <ref type=""bibr"" target=""#b0"">[1]</ref> published a system called AutoDoc, and introduced the module of cross-checking among only textual paragraphs. icient to organize and summarize data, there are much more numerical facts in tables than textual paragraphs. Therefore, as an important extension to <ref type=""bibr"" target=""#b0"">[1]</ref>, we propose Automatic Numerical Cross-Checking over Tables (ANCOT) in this study.</p><p>The key module of such m-Checking is an important issue in academic, financial, and politic fields. It has attracted a lot of research interests in recent years. Cao et al. <ref type=""bibr"" target=""#b0"">[1]</ref> propose a system to cross-check numerical facts by extracting structured formulas from textual paragraph in fi",1
"cells is not too long, thus we use character-based model in cell embedding network. The vocabulary contains 2,500 most frequent characters. Word2vec <ref type=""bibr"" target=""#b12"">[13]</ref> is adopted to initialize character embeddings. The dimensions of the character embedding and LSTM 1 are set",0
"lling to invest firms with clear and concise financial information, so accounting errors also deserve our attention. Choudhary, Merkley, and Schipper <ref type=""bibr"" target=""#b1"">[2]</ref> find that investors believe that even immaterial errors mean weak corporate governance or poor quality of fina",0
"<p>Therefore, it creates an opportunity for automated numerical cross-checking systems. There are some related systems developed, such as ClaimBuster <ref type=""bibr"" target=""#b4"">[5]</ref> and StatCheck <ref type=""bibr"" target=""#b11"">[12]</ref>. ClaimBuster focused on detecting check-worthy factual 17]</ref> propose a dataset to verify the claims made by public figures. Verifying such claims includes detecting whether a statement in check-worthy <ref type=""bibr"" target=""#b4"">[5]</ref>, retrieve information from large data source  to provide related evident paragraphs, and finally give a classi",0
"</ref>, ad hoc search over tables <ref type=""bibr"" target=""#b17"">[18]</ref>, transforming complex tables to the form that can be stored in a database <ref type=""bibr"" target=""#b13"">[14]</ref>. Our task, cross-checking over numerical tables, is also a table understanding task based on extracted table",0
"le yet efficient implementations for complex self-supervised tasks, e.g., discriminating whether two subsequences come from the same user's behaviors <ref type=""bibr"" target=""#b35"">[36]</ref>. We further improve upon CLRec and propose Multi-CLRec, which employs a multi-queue design for more accurate ad><p>Table <ref type=""table"">7</ref>: Task u2i is the regular task where 𝑥 is a sequence of clicks and 𝑦 is the next click to be predicted. Task u2u <ref type=""bibr"" target=""#b35"">[36]</ref> adds an auxiliary loss where 𝑥 and 𝑦 are both sequences from the same user (before and after a sampled times main queue and an additional secondary queue.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>B.4 Complex Pretext Tasks</head><p>In task u2u <ref type=""bibr"" target=""#b35"">[36]</ref>, 𝑥 and 𝑦 are both sequences from the same user, before and after a sampled timestamp, respectively. Intuitiv",1
"<ref type=""bibr"" target=""#b40"">41,</ref><ref type=""bibr"" target=""#b47"">48]</ref>. Our theory complements the previous studies of contrastive learning <ref type=""bibr"" target=""#b38"">[39]</ref>.</p><p>Based on the theoretical discovery, an easy-to-implement framework is developed to efficiently reduce esigning a well-performing proposal distribution 𝑝 𝑛 (𝑦 | 𝑥) <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b42"">43]</ref>. InfoNCE <ref type=""bibr"" target=""#b38"">[39]</ref> demonstrates that this loss maximizes a lower bound of the mutual information between 𝑥 and 𝑦 if we set the earn high-quality representations via self-supervised pretext tasks, recently achieves remarkable success in various domains, e.g., speech processing <ref type=""bibr"" target=""#b38"">[39]</ref>, computer vision <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type= nal environments <ref type=""bibr"" target=""#b29"">[30]</ref>. The contrastive loss we investigate in this paper is a generalization of the InfoNCE loss <ref type=""bibr"" target=""#b38"">[39]</ref>. InfoNCE is previously understood as a bound of the mutual information between two variables <ref type=""bibr e InfoNCE loss <ref type=""bibr"" target=""#b38"">[39]</ref>. InfoNCE is previously understood as a bound of the mutual information between two variables <ref type=""bibr"" target=""#b38"">[39]</ref>. Our work provides a new perspective on the effectiveness of the contrastive loss, by illustrating its conne type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" target=""#b36"">37]</ref>.</p><p>Contrastive Loss. We study the following type of contrastive loss <ref type=""bibr"" target=""#b38"">[39,</ref><ref type=""bibr"" target=""#b42"">43]</ref> under a negative sampler 𝑝 𝑛 (𝑦 | 𝑥): where {𝑦 𝑖 } 𝐿 𝑖=1 are again s n Eq. (3) has recently achieved remarkable success in various fields <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b38"">39]</ref>. Nevertheless, it still remains a question why the loss is effective. We will reveal that the contrastive los",1
"ing, where the latter is a well-studied technique for bias reduction <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b40"">41,</ref><ref type=""bibr"" target=""#b47"">48]</ref>. Our theory complements the previous studies of contrastive learning <ref type=""bibr"" target=""#b38"">[39]</ref reveal their connection with the inverse propensity weighting (IPW) <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b40"">41,</ref><ref type=""bibr"" target=""#b47"">48]</ref> techniques for bias reduction.</p><p>Sampled Softmax. The kind of contrastive loss we will investigate is sim ss, by illustrating its connection with inverse propensity weighting <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b40"">41,</ref><ref type=""bibr"" target=""#b47"">48]</ref>.</p><p>[63] Hao Zou, Peng Cui, Bo Li, Zheyan Shen, Jianxin Ma, Hongxia Yang, and Yue He.</p><p>2020. Counterf",0
"tmax will force the embeddings to form more compact clusters, which makes kNN search more easy.</p><p>The embeddings should use normal initialization <ref type=""bibr"" target=""#b15"">[16]</ref> instead of uniform initialization if 𝑙2-normalization is in use, so that the initial embeddings are uniforml",0
"amine the debiasing effects of CLRec, we first conduct offline experiments and compare CLRec with sampled softmax. We report the aggregated diversity <ref type=""bibr"" target=""#b1"">[2]</ref> in Table <ref type=""table"" target=""#tab_3"">1</ref>, and the distributions of the recommended items resulted fr nt by the users on reading the details of the items clicked by them, divided by the total number of clicks on our platform. • The aggregate diversity <ref type=""bibr"" target=""#b1"">[2]</ref>, measured on a sampled subset of users for testing, is the number of distinct items recommended to the subset e></figure> <figure xmlns=""http://www.tei-c.org/ns/1.0"" type=""table"" xml:id=""tab_3""><head>Table 1 :</head><label>1</label><figDesc>Aggregate diversity<ref type=""bibr"" target=""#b1"">[2]</ref>, i.e. the number of distinct items recommended to a randomly sampled subset of users.</figDesc><table><row><ce",0
"""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" target=""#b41"">42,</ref><ref type=""bibr"" target=""#b43"">44,</ref><ref type=""bibr"" target=""#b51"">52,</ref><ref type=""bibr"" target=""#b53"">54,</ref><ref type=""bibr"" target=""#b56"">57,</ref><ref type=""bibr"">63</ref>]. However, these existing works mostly focus =""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b41"">42,</ref><ref type=""bibr"" target=""#b43"">44,</ref><ref type=""bibr"" target=""#b51"">52,</ref><ref type=""bibr"" target=""#b53"">54]</ref>. Most of the existing methods assume that the recommendations 𝑂 are drawn from a multivariate bernoulli polic",0
"theoretical connection between contrastive learning and inverse propensity weighting, where the latter is a well-studied technique for bias reduction <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b40"">41,</ref><ref type=""bibr"" target=""#b47"">48]</ref>. Our theory complements the >We now introduce the family of contrastive losses that we are interested in, and reveal their connection with the inverse propensity weighting (IPW) <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b40"">41,</ref><ref type=""bibr"" target=""#b47"">48]</ref> techniques for bias reductio >. Our work provides a new perspective on the effectiveness of the contrastive loss, by illustrating its connection with inverse propensity weighting <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b40"">41,</ref><ref type=""bibr"" target=""#b47"">48]</ref>.</p><p>[63] Hao Zou, Peng Cu",0
"47]</ref> 0.7582 0.8745 DIEN <ref type=""bibr"" target=""#b59"">[60]</ref> 0.7770 0.8934 RUM <ref type=""bibr"" target=""#b12"">[13]</ref> 0.7464 0.8370 LSTM <ref type=""bibr"" target=""#b22"">[23]</ref> 0.7765 0.8681 SHAN <ref type=""bibr"" target=""#b54"">[55]</ref> 0.7763 0.8828 HPMN <ref type=""bibr"" target=""#b3",0
"rocessing <ref type=""bibr"" target=""#b38"">[39]</ref>, computer vision <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b21"">22]</ref>, graph data <ref type=""bibr"" target=""#b49"">[50]</ref>, and compositional environments <ref type=""bibr"" target",0
"part of many live industrial systems with its enhanced expressiveness.</p><p>Typical large-scale DCG models <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b31"">32,</ref><ref type=""bibr"" target=""#b60"">61]</ref> regard the problem of identif",0
"lution <ref type=""bibr"" target=""#b33"">[34]</ref> in the ResNet bottle block, which converts the multi-path structure into a unified operation. SE-Net <ref type=""bibr"" target=""#b28"">[29]</ref> introduces a channel-attention mechanism by adaptively recalibrating the channel feature responses. SK-Net < r"" target=""#b59"">60]</ref> with the same cardinality and number of channels.</p><p>Relation to Existing Attention Methods. First introduced in SE-Net <ref type=""bibr"" target=""#b28"">[29]</ref>, the idea of squeeze-and-attention (called excitation in the original paper) is to employ a global context t f network parameters and FLOPS, including: ResNet <ref type=""bibr"" target=""#b22"">[23]</ref>, ResNeXt <ref type=""bibr"" target=""#b59"">[60]</ref>, SENet <ref type=""bibr"" target=""#b28"">[29]</ref>, ResNet-D <ref type=""bibr"" target=""#b25"">[26]</ref> and SKNet <ref type=""bibr"" target=""#b37"">[38]</ref>. Rem Global contextual information with embedded channel-wise statistics can be gathered with global average pooling across spatial dimensions s k ∈ R C/K <ref type=""bibr"" target=""#b28"">[29,</ref><ref type=""bibr"" target=""#b37"">38]</ref>. Here the c-th component is calculated as:</p><formula xml:id=""formu r operation is not optimized for training efficiency and scaling to large neural networks. Our method generalizes prior work on feature-map attention <ref type=""bibr"" target=""#b28"">[29,</ref><ref type=""bibr"" target=""#b37"">38]</ref> within a cardinal group setting <ref type=""bibr"" target=""#b59"">[60]< prevent this, dropout regularization randomly masks out some neurons during training (but not during inference) to form an implicit network ensemble <ref type=""bibr"" target=""#b28"">[29,</ref><ref type=""bibr"" target=""#b48"">49,</ref><ref type=""bibr"" target=""#b67"">68]</ref>. A dropout layer with the dr ks operating on images that share the same crop size. ResNet variants<ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b28"">29,</ref><ref type=""bibr"" target=""#b59"">60]</ref> usually use a fixed training crop size of 224, while the Inception-Ne",1
"attention across two network branches. Inspired by the previous methods, our Fig. <ref type=""figure"">1</ref>: Comparing our ResNeSt block with SE-Net <ref type=""bibr"" target=""#b29"">[30]</ref> and SK-Net <ref type=""bibr"" target=""#b37"">[38]</ref>. A detailed view of Split-Attention unit is shown in Fi s {F 1 , F 2 , ...F G } to each individual group, then the intermediate representation of each group is Split Attention in Cardinal Groups. Following <ref type=""bibr"" target=""#b29"">[30,</ref><ref type=""bibr"" target=""#b37"">38]</ref>, a combined representation for each cardinal group can be obtained b ly the strided convolution at the 3 × 3 layer instead of the 1 × 1 layer to better preserve such information <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b29"">30]</ref>. Convolutional layers require handling featuremap boundaries with zero-padding strategies, which is often sub",1
"<ref type=""bibr"" target=""#b28"">[29]</ref> introduces a channel-attention mechanism by adaptively recalibrating the channel feature responses. SK-Net <ref type=""bibr"" target=""#b37"">[38]</ref> brings the feature-map attention across two network branches. Inspired by the previous methods, our Fig. <re e previous methods, our Fig. <ref type=""figure"">1</ref>: Comparing our ResNeSt block with SE-Net <ref type=""bibr"" target=""#b29"">[30]</ref> and SK-Net <ref type=""bibr"" target=""#b37"">[38]</ref>. A detailed view of Split-Attention unit is shown in Figure <ref type=""figure"" target=""#fig_0"">2</ref>. For ion operation to each cardinal group, while the SE-Net operates on top of the entire block regardless of multiple groups. Previous models like SK-Net <ref type=""bibr"" target=""#b37"">[38]</ref> introduced feature attention between two network branches, but their operation is not optimized for training f type=""bibr"" target=""#b59"">[60]</ref>, SENet <ref type=""bibr"" target=""#b28"">[29]</ref>, ResNet-D <ref type=""bibr"" target=""#b25"">[26]</ref> and SKNet <ref type=""bibr"" target=""#b37"">[38]</ref>. Remarkably, our ResNeSt-50 achieves 80.64 top-1 accuracy, which is the first 50-layer ResNet variant that s l group, then the intermediate representation of each group is Split Attention in Cardinal Groups. Following <ref type=""bibr"" target=""#b29"">[30,</ref><ref type=""bibr"" target=""#b37"">38]</ref>, a combined representation for each cardinal group can be obtained by fusing via an element-wise summation ac ed channel-wise statistics can be gathered with global average pooling across spatial dimensions s k ∈ R C/K <ref type=""bibr"" target=""#b28"">[29,</ref><ref type=""bibr"" target=""#b37"">38]</ref>. Here the c-th component is calculated as:</p><formula xml:id=""formula_0"">U i = F i (X), for i ∈ {1, 2, ...G} efficiency and scaling to large neural networks. Our method generalizes prior work on feature-map attention <ref type=""bibr"" target=""#b28"">[29,</ref><ref type=""bibr"" target=""#b37"">38]</ref> within a cardinal group setting <ref type=""bibr"" target=""#b59"">[60]</ref>, and its implementation remains com",1
"up or depth-wise convolution <ref type=""bibr"" target=""#b26"">[27,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b53"">54,</ref><ref type=""bibr"" target=""#b59"">60]</ref>. Despite their superior computation and accuracy tradeoff in classification tasks, these models do not transf rial. The number of parameters and FLOPS of a Split-Attention block are roughly the same as a residual block <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b59"">60]</ref> with the same cardinality and number of channels.</p><p>Relation to Existing Attention Methods. First introdu me crop size. ResNet variants<ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b28"">29,</ref><ref type=""bibr"" target=""#b59"">60]</ref> usually use a fixed training crop size of 224, while the Inception-Net family<ref type=""bibr"" target=""#b50"">[ prior work on feature-map attention <ref type=""bibr"" target=""#b28"">[29,</ref><ref type=""bibr"" target=""#b37"">38]</ref> within a cardinal group setting <ref type=""bibr"" target=""#b59"">[60]</ref>, and its implementation remains computationally efficient. Figure <ref type=""figure"">1</ref> shows an overal forms all ResNet variants with a similar number of network parameters and FLOPS, including: ResNet <ref type=""bibr"" target=""#b22"">[23]</ref>, ResNeXt <ref type=""bibr"" target=""#b59"">[60]</ref>, SENet <ref type=""bibr"" target=""#b28"">[29]</ref>, ResNet-D <ref type=""bibr"" target=""#b25"">[26]</ref> and SKN",1
"hown success in GoogleNet <ref type=""bibr"" target=""#b51"">[52]</ref>, in which each network block consists of different convolutional kernels. ResNeXt <ref type=""bibr"" target=""#b60"">[61]</ref> adopts group convolution <ref type=""bibr"" target=""#b33"">[34]</ref> in the ResNet bottle block, which convert n operations. Figure <ref type=""figure"">1</ref> (Right) depicts an overview of a Split-Attention Block.</p><p>Feature-map Group. As in ResNeXt blocks <ref type=""bibr"" target=""#b60"">[61]</ref>, the feature can be divided into several groups, and the number of feature-map groups is given by a cardinal",1
"etworks trained for image classification often serve as the backbone of the neural networks designed for other applications, such as object detection <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b45"">46]</ref>, semantic segmentation <ref type=""bibr"" target=""#b5"">[6,</ref><ref t downstream models to build upon. Instead, our model preserves ResNet meta structure, which can be directly applied on many existing downstream models <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b40"">41,</ref><ref type=""bibr"" target=""#b45"">46,</ref><ref type=""bibr"" target=""#b68 instance segmentation also predicts object masks, for which a more accurate dense image representation is desirable.</p><p>We evaluate the Mask-RCNN <ref type=""bibr"" target=""#b21"">[22]</ref> and Cascade-Mask-RCNN <ref type=""bibr"" target=""#b1"">[2]</ref> models with ResNeSt-50 and ResNeSt-101 as thei",0
"target=""#b25"">[26]</ref>. Batch Normalization <ref type=""bibr"" target=""#b31"">[32]</ref> is used after each convolutional layer before ReLU activation <ref type=""bibr"" target=""#b43"">[44]</ref>. Network weights are initialized using Kaiming Initialization <ref type=""bibr"" target=""#b23"">[24]</ref>. A d",0
"ns/1.0""><head n=""4.2"">Training Strategy</head><p>Large Mini-batch Distributed Training. Following prior work <ref type=""bibr"" target=""#b18"">[19,</ref><ref type=""bibr"" target=""#b36"">37]</ref>, we train our models using 8 servers (64 GPUs in total) in parallel. Our learning rates are adjusted accordin",0
"egies used in our experiments. First, we detail a couple of tweaks that further improve performance, some of which have been empirically validated in <ref type=""bibr"" target=""#b24"">[25]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.1"">Network Tweaks</head><p>Average Downsampli",0
"has significantly boosted image classification accuracy through large scale neural architecture search (NAS) <ref type=""bibr"" target=""#b44"">[45,</ref><ref type=""bibr"" target=""#b54"">55]</ref>. Despite their state-of-the-art performance, these NAS-derived models are usually not optimized for training bibr"" target=""#b70"">[71]</ref>.</p><p>not even trainable on a GPU with an appropriate per-device batch-size<ref type=""foot"" target=""#foot_0"">2</ref>  <ref type=""bibr"" target=""#b54"">[55]</ref>. This has limited the adoption of NAS-derived models for other applications, especially tasks involving dens computational efficiency and even achieves better speed-accuracy trade-offs than state-of-the-art CNN models produced via neural architecture search <ref type=""bibr"" target=""#b54"">[55]</ref> as shown in Table <ref type=""table"">1</ref>. Our single Cascade-RCNN <ref type=""bibr"" target=""#b2"">[3]</ref> ation performance, such as: Amoe-baNet <ref type=""bibr"" target=""#b44"">[45]</ref>, MNASNet <ref type=""bibr"" target=""#b53"">[54]</ref>, and EfficientNet <ref type=""bibr"" target=""#b54"">[55]</ref>. Despite their great success in image classification, the meta network structures are distinct from each oth these blocks scale to deeper networks with a good trade-off between speed, accuracy and memory usage. . EfficientNet variants b2-b7 are described in <ref type=""bibr"" target=""#b54"">[55]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""5.3"">Comparing against the</head><p>ResNeSt var <ref type=""bibr"" target=""#b51"">[52]</ref><ref type=""bibr"" target=""#b52"">[53]</ref> uses a training crop size of 299. Recently, the EfficientNet method<ref type=""bibr"" target=""#b54"">[55]</ref> has demonstrated that increasing the input image size for a deeper and wider network may better trade off ac",0
"ibr"" target=""#b68"">69]</ref> or introduce long-range connections <ref type=""bibr"" target=""#b55"">[56]</ref> or use cross-channel feature-map attention <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b64"">65]</ref>. While these approaches do improve the transfer learning performance",0
"e=""bibr"" target=""#b28"">29,</ref><ref type=""bibr"" target=""#b59"">60]</ref> usually use a fixed training crop size of 224, while the Inception-Net family<ref type=""bibr"" target=""#b50"">[51]</ref><ref type=""bibr"" target=""#b51"">[52]</ref><ref type=""bibr"" target=""#b52"">[53]</ref> uses a training crop size",0
"on task requires ""network surgery"" to modify the ResNet to be more effective for that particular task. For example, some methods add a pyramid module <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b68"">69]</ref> or introduce long-range connections <ref type=""bibr"" target=""#b55"">[56",0
"ction <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b45"">46]</ref>, semantic segmentation <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b42"">43,</ref><ref type=""bibr"" target=""#b72"">73]</ref> and pose estimation <ref type=""bibr"" target=""#b13"">[14,</ref><ref typ",0
"mentation <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b42"">43,</ref><ref type=""bibr"" target=""#b72"">73]</ref> and pose estimation <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b57"">58]</ref>. Recent work has significantly boosted image classification accuracy",0
"often serve as the backbone of the neural networks designed for other applications, such as object detection <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b45"">46]</ref>, semantic segmentation <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b42"">43,</ref><ref ty re, which can be directly applied on many existing downstream models <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b40"">41,</ref><ref type=""bibr"" target=""#b45"">46,</ref><ref type=""bibr"" target=""#b68"">69]</ref>. Our approach can also augment the search spaces for neural architect",0
"mance, these NAS-derived models are usually not optimized for training efficiency or memory usage on general/commercial processing hardware (CPU/GPU) <ref type=""bibr"" target=""#b35"">[36]</ref>. Due to excessive memory consumption, some of the larger versions of these models are Table <ref type=""table",0
"el>(5)</label></formula><p>with small constant ε &gt; 0. This mitigates network overconfidence and overfitting.</p><p>Auto Augmentation. Auto-Augment <ref type=""bibr"" target=""#b10"">[11]</ref> is a strategy that augments the training data with transformed images, where the transformations are learned",0
"ore accurate dense image representation is desirable.</p><p>We evaluate the Mask-RCNN <ref type=""bibr"" target=""#b21"">[22]</ref> and Cascade-Mask-RCNN <ref type=""bibr"" target=""#b1"">[2]</ref> models with ResNeSt-50 and ResNeSt-101 as their backbones. All models are trained along with FPN <ref type=""bi",0
"long-range connections <ref type=""bibr"" target=""#b55"">[56]</ref> or use cross-channel feature-map attention <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b64"">65]</ref>. While these approaches do improve the transfer learning performance for certain tasks, they raise the questi nnel information has demonstrated success in downstream applications <ref type=""bibr"" target=""#b55"">[56,</ref><ref type=""bibr"" target=""#b63"">64,</ref><ref type=""bibr"" target=""#b64"">65]</ref>, while recent image classification networks have focused more on group or depth-wise convolution <ref type=""b the standard COCO AP metric of single scale. We train all models with FPN <ref type=""bibr"" target=""#b40"">[41]</ref>, synchronized batch normalization <ref type=""bibr"" target=""#b64"">[65]</ref> and image scale augmentation (short size of a image is picked randomly from 640 to 800). 1x learning rate sc [6,</ref><ref type=""bibr"" target=""#b61"">62]</ref> is applied to the backbone network, resulting in a stride-8 model. Synchronized Batch Normalization <ref type=""bibr"" target=""#b64"">[65]</ref> is used during training, along with a polynomial-like learning rate schedule (with initial learning rate = 0 gits are upsampled 8 times to calculate the per-pixel cross entropy loss against the ground truth labels. We use multi-scale evaluation with flipping <ref type=""bibr"" target=""#b64"">[65,</ref><ref type=""bibr"" target=""#b68"">69,</ref><ref type=""bibr"" target=""#b72"">73]</ref>.</p><p>We first consider the",0
"esize each image to 1/0.875 of the crop size along the short edge and apply a center crop. Our code implementation for ImageNet training uses GluonCV <ref type=""bibr"" target=""#b20"">[21]</ref> with MXNet <ref type=""bibr"" target=""#b8"">[9]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head -c.org/ns/1.0""><head n=""6.3"">Semantic Segmentation</head><p>In transfer learning for the downstream task of semantic segmentation, we use the GluonCV <ref type=""bibr"" target=""#b20"">[21]</ref> implementation of DeepLabV3 <ref type=""bibr"" target=""#b6"">[7]</ref> as a baseline approach. Here a dilated n pose estimation task. The baseline model is SimplePose <ref type=""bibr"" target=""#b57"">[58]</ref> with ResNet50 and ResNet101 implemented in Glu-onCV <ref type=""bibr"" target=""#b20"">[21]</ref>. As comparison we replace the backbone with ResNeSt50 and ResNeSt101 respectively while keeping other settin",0
"ibr"" target=""#b64"">[65,</ref><ref type=""bibr"" target=""#b68"">69,</ref><ref type=""bibr"" target=""#b72"">73]</ref>.</p><p>We first consider the Cityscapes <ref type=""bibr"" target=""#b9"">[10]</ref> dataset, which consists of 5K highquality labeled images. We train each model on 2,975 images from the traini",0
"lang=""en""> 		<body> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""1"">Introduction</head><p>The majority of the research efforts on improving VAEs <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref> is dedicated to the statistical challenges, such as reducing the gap bet q(z z z i |x x x, z z z &lt;i ) is the approximate posterior up to the (l − 1) th group. The objective is trained using the reparameterization trick <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref>.</p><p>The main question here is how to implement the conditionals in p( ion, we review VAEs, their hierarchical extension, and bidirectional encoder networks <ref type=""bibr"" target=""#b3"">[4]</ref>.</p><p>The goal of VAEs <ref type=""bibr"" target=""#b0"">[1]</ref> is to train a generative model in the form of p(x x x, z z z) = p(z z z)p(x x x|z z z) where p(z z z) is a pri",1
"""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b22"">23]</ref>, or tackling posterior collapse <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b24"">25,</",0
"><ref type=""bibr"" target=""#b9"">10]</ref>, formulating tighter bounds <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b13"">14]</ref>, reducing the gradient noise <ref type=""bibr"" target=""#b14"">[15,</ref",0
").</p><p>In deep hierarchical VAEs <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b41"">42,</ref><ref type=""bibr"" target=""#b42"">43]</ref>, to increase the expressiveness of both the approximate posterior and Additional Implementation Details</head><p>Warming-up the KL Term: Similar to the previous work, we warm-up the KL term at the beginning of training <ref type=""bibr"" target=""#b41"">[42]</ref>. Formally, we optimize the following objective: E q(z z z|x x x) [log p(x x x|z z z)] − βKL(q(z z z|x x x)||",0
"educing the gap with autoregressive models. The main building block of our network is depthwise convolutions <ref type=""bibr"" target=""#b37"">[38,</ref><ref type=""bibr"" target=""#b38"">39]</ref> that rapidly increase the receptive field of the network without dramatically increasing the number of parame",0
"translation for Text Classification. When used as an augmentation method, backtranslation <ref type=""bibr"" target=""#b37"">(Sennrich et al., 2015;</ref><ref type=""bibr"" target=""#b14"">Edunov et al., 2018)</ref> refers to the procedure of translating an existing example x in language A into another lang ant performance improvements in our text classification experiments. We use random sampling instead of beam search for decoding similar to the work by<ref type=""bibr"" target=""#b14"">Edunov et al. (2018)</ref>.</figDesc><table /></figure> 			<note xmlns=""http://www.tei-c.org/ns/1.0"" place=""foot"" n=""1""",1
"/p><p>In a nutshell, consistency training methods simply regularize model predictions to be invariant to small noise applied to either input examples <ref type=""bibr"" target=""#b23"">(Miyato et al., 2018;</ref><ref type=""bibr"" target=""#b34"">Sajjadi et al., 2016;</ref><ref type=""bibr"" target=""#b6"">Clar he noise is defined: Pseudoensemble <ref type=""bibr"" target=""#b1"">(Bachman et al., 2014)</ref> directly applies Gaussian noise and Dropout noise; VAT <ref type=""bibr"" target=""#b23"">(Miyato et al., 2018;</ref><ref type=""bibr"">2016)</ref> defines the noise by approximating the direction of change in t e., x = q(x, ), as considered by prior works <ref type=""bibr"" target=""#b34"">(Sajjadi et al., 2016;</ref><ref type=""bibr"">Laine &amp; Aila, 2016;</ref><ref type=""bibr"" target=""#b23"">Miyato et al., 2018)</ref>. But different from existing work, we focus on the unattended question of how the form or ""q regularizing the predictions to have low entropy has been shown to be beneficial <ref type=""bibr"" target=""#b15"">(Grandvalet &amp; Bengio, 2005;</ref><ref type=""bibr"" target=""#b23"">Miyato et al., 2018)</ref>, we sharpen predictions when computing the target distribution on unlabeled examples by usin tation transformation and θ is a fixed copy of the current parameters θ indicating that the gradient is not propagated through θ, as suggested by VAT <ref type=""bibr"" target=""#b23"">(Miyato et al., 2018)</ref>. We set λ to 1 for most of our experiments and use different batch sizes for the labeled da ate UDA with varied supervised data sizes. Specifically, we compare UDA with two highly competitive baselines: (1) Virtual adversarial training (VAT) <ref type=""bibr"" target=""#b23"">(Miyato et al., 2018)</ref>, an algorithm that generates adversarial Gaussian noise on input, and (2) MixMatch <ref typ se and realistic images.   <ref type=""bibr"" target=""#b41"">(Tarvainen &amp; Valpola, 2017)</ref> Conv-Large 3.1M 12.31 ± 0.28 3.95 ± 0.19 VAT + EntMin <ref type=""bibr"" target=""#b23"">(Miyato et al., 2018)</ref> Conv-Large 3.1M 10.55 ± 0.05 3.86 ± 0.11 SNTG <ref type=""bibr"" target=""#b17"">(Luo et al., 2",1
"nt can also take advantage of deep architectures <ref type=""bibr"">(Kingma et al., 2014;</ref><ref type=""bibr"" target=""#b18"">Maaløe et al., 2016;</ref><ref type=""bibr"" target=""#b47"">Yang et al., 2017)</ref>. Besides the direct extensions, it was found that training neural classifiers to classify out- "" target=""#b3"">Carmon et al., 2019)</ref>. Enforcing consistency w.r.t data augmentation has also been shown to work well for representation learning <ref type=""bibr"" target=""#b47"">(Hu et al., 2017;</ref><ref type=""bibr"" target=""#b48"">Ye et al., 2019)</ref>. Invariant representation learning <ref ty",0
"arning <ref type=""bibr"" target=""#b47"">(Hu et al., 2017;</ref><ref type=""bibr"" target=""#b48"">Ye et al., 2019)</ref>. Invariant representation learning <ref type=""bibr"" target=""#b16"">(Liang et al., 2018;</ref><ref type=""bibr"" target=""#b35"">Salazar et al., 2018)</ref> applies the consistency loss not o",0
"paradigms of leveraging unlabeled data to address this weakness. The recent works in SSL are diverse but those that are based on consistency training <ref type=""bibr"" target=""#b1"">(Bachman et al., 2014;</ref><ref type=""bibr"" target=""#b32"">Rasmus et al., 2015;</ref><ref type=""bibr"">Laine &amp; Aila, et al., 2018;</ref><ref type=""bibr"" target=""#b34"">Sajjadi et al., 2016;</ref><ref type=""bibr"" target=""#b6"">Clark et al., 2018)</ref> or hidden states <ref type=""bibr"" target=""#b1"">(Bachman et al., 2014;</ref><ref type=""bibr"">Laine &amp; Aila, 2016)</ref>. This framework makes sense intuitively becau ing vision datasets, such as ImageNet.</p><p>Other works in the consistency training family mostly differ in how the noise is defined: Pseudoensemble <ref type=""bibr"" target=""#b1"">(Bachman et al., 2014)</ref> directly applies Gaussian noise and Dropout noise; VAT <ref type=""bibr"" target=""#b23"">(Miya",0
"but the class distributions of out-of-domain data are mismatched with those of in-domain data, which can result in performance loss if directly used <ref type=""bibr"" target=""#b26"">(Oliver et al., 2018)</ref>. To obtain data relevant to the domain for the task at hand, we adopt a common technique fo the most commonly used semi-supervised learning benchmarks CIFAR-10 and SVHN.</p><p>Vary the size of labeled data. Firstly, we follow the settings in <ref type=""bibr"" target=""#b26"">(Oliver et al., 2018)</ref> and employ Wide-ResNet-28-2 <ref type=""bibr"" target=""#b50"">(Zagoruyko &amp; Komodakis, 2016",0
"elpful in adversarial robustness <ref type=""bibr"" target=""#b40"">(Stanforth et al., 2019;</ref><ref type=""bibr"" target=""#b51"">Zhai et al., 2019a;</ref><ref type=""bibr"" target=""#b24"">Najafi et al., 2019;</ref><ref type=""bibr"" target=""#b3"">Carmon et al., 2019)</ref>. Enforcing consistency w.r.t data au",0
"e 3.1M 12.31 ± 0.28 3.95 ± 0.19 VAT + EntMin <ref type=""bibr"" target=""#b23"">(Miyato et al., 2018)</ref> Conv-Large 3.1M 10.55 ± 0.05 3.86 ± 0.11 SNTG <ref type=""bibr"" target=""#b17"">(Luo et al., 2018)</ref> Conv-Large 3.1M 10.93 ± 0.14 3.86 ± 0.27 VAdD <ref type=""bibr"" target=""#b28"">(Park et al., 201 2017)</ref>, fast-Stochastic Weight Averaging <ref type=""bibr"" target=""#b0"">(Athiwaratkun et al., 2018)</ref> and Smooth Neighbors on Teacher Graphs <ref type=""bibr"" target=""#b17"">(Luo et al., 2018)</ref>. For a complete version of related work, please refer to Appendix D.</p></div> <div xmlns=""htt hat improves Mean Teacher by encouraging the model to explore a diverse set of plausible parameters. In addition to parameter-level consistency, SNTG <ref type=""bibr"" target=""#b17"">(Luo et al., 2018)</ref>  There are also recent works on generating diverse translations <ref type=""bibr"">(He et al., 2",0
"ght data prefetchers employed at the L1 level. Well-established and recent spatial L2 prefetchers (prefetchers that prefetch within a spatial region) <ref type=""bibr"" target=""#b32"">[33]</ref>, <ref type=""bibr"" target=""#b12"">[13]</ref>, <ref type=""bibr"" target=""#b13"">[14]</ref>, <ref type=""bibr"" targ ve coverage. Prefetchers like variable length delta prefetching (VLDP) <ref type=""bibr"" target=""#b44"">[45]</ref> and signature path prefetching (SPP) <ref type=""bibr"" target=""#b32"">[33]</ref> are well known delta prefetchers. VLDP stores the history of deltas to predict future deltas. SPP is a state re available on the public domain. Recent prefetching proposals <ref type=""bibr"" target=""#b10"">[11]</ref>, <ref type=""bibr"" target=""#b13"">[14]</ref>, <ref type=""bibr"" target=""#b32"">[33]</ref> have also been coded and evaluated with ChampSim, helping the community for a fair comparison of techniques. It is well known that spatial prefetchers fail to improve performance for server workloads like CloudSuite <ref type=""bibr"" target=""#b13"">[14]</ref>, <ref type=""bibr"" target=""#b32"">[33]</ref>, <ref type=""bibr"" target=""#b52"">[53]</ref>, <ref type=""bibr"" target=""#b57"">[58]</ref>, <ref type=""bibr"" targ",1
"et irregular but temporal accesses. In general, spatial prefetchers demand less storage (closer to tens of KBs, except spatial memory streaming (SMS) <ref type=""bibr"" target=""#b46"">[47]</ref> and Bingo <ref type=""bibr"" target=""#b10"">[11]</ref>) as compared to the temporal ones (closer to hundreds of r"" target=""#b13"">[14]</ref>, <ref type=""bibr"" target=""#b12"">[13]</ref> are designed specifically for L2's access patterns.</p><p>Prefetchers like SMS <ref type=""bibr"" target=""#b46"">[47]</ref>, <ref type=""bibr"" target=""#b48"">[49]</ref> and Bingo <ref type=""bibr"" target=""#b10"">[11]</ref> are capable o re delta. It dynamically controls the prefetch aggressiveness based on the success probability of future deltas.</p><p>Spatial Memory Streaming (SMS) <ref type=""bibr"" target=""#b46"">[47]</ref>, <ref type=""bibr"" target=""#b48"">[49]</ref> is a spatial prefetcher that exploits the relationship between th ow the performance of VLDP and DSPatch as on average, SPP performs better than VLDP and DSPatch, at the L1. Similarly, Bingo performs better than SMS <ref type=""bibr"" target=""#b46"">[47]</ref> with relatively less storage demand. Note that Bingo demands 119KB at the L1-D. On average, Bingo provides s",0
"etch-Filter (EPF) <ref type=""bibr"" target=""#b42"">[43]</ref> have been proposed. Apart from filters, there are aggressiveness controllers (throttlers) <ref type=""bibr"" target=""#b15"">[16]</ref>, <ref type=""bibr"" target=""#b19"">[20]</ref>, <ref type=""bibr"" target=""#b29"">[30]</ref>, <ref type=""bibr"" targ . We sweep through the following pair of (PQ,MSHR) entries: (2,4), <ref type=""bibr"" target=""#b3"">(4,</ref><ref type=""bibr"" target=""#b7"">8)</ref>, and <ref type=""bibr"" target=""#b15"">(16,</ref><ref type=""bibr"" target=""#b31"">32)</ref>  Sensitivity to prefetch table size: IPCP, in its current form is ex",0
"the entire memory system, including the virtual memory system. ChampSim was used for the 2nd and 3rd data prefetching championships (DPC-2 and DPC-3) <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b2"">[3]</ref>. The simulation framework is enhanced with multi-level prefetching fo",0
"ail to improve performance for server workloads like CloudSuite <ref type=""bibr"" target=""#b13"">[14]</ref>, <ref type=""bibr"" target=""#b32"">[33]</ref>, <ref type=""bibr"" target=""#b52"">[53]</ref>, <ref type=""bibr"" target=""#b57"">[58]</ref>, <ref type=""bibr"" target=""#b58"">[59]</ref> and additional prefetc",0
"ication changes.</p><p>Sub-blocked TLBs <ref type=""bibr"" target=""#b55"">[56]</ref>, CoLT <ref type=""bibr"" target=""#b45"">[46]</ref>, and Clustered TLBs <ref type=""bibr"" target=""#b44"">[45]</ref> combine near virtual-to-physical page translations into single TLB entries. These approaches rely on the def",1
"To support TPS, we modify the L1 TLB to contain a 32 entry fully-associative (as in other commercial designs <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b46"">[47]</ref>) TPS TLB. The TPS TLB takes the place of the existing 32 entry and 4 entry larger page size L1 TLBs. The TPS",0
"B and 1GB are currently supported. Two currently available software mechanisms in Linux are able to leverage huge pages: Transparent Huge Pages (THP) <ref type=""bibr"" target=""#b15"">[16]</ref> and HugeTLBFS <ref type=""bibr"" target=""#b16"">[17]</ref>.</p><p>TLB: Hardware must translate from virtual add ng page table fine-grained locking scheme to manage both trees concurrently <ref type=""bibr"" target=""#b18"">[19]</ref>.</p><p>Huge pages or superpages <ref type=""bibr"" target=""#b15"">[16]</ref>, <ref type=""bibr"" target=""#b16"">[17]</ref>, <ref type=""bibr"" target=""#b41"">[42]</ref> are already utilized i s.</p><p>Ingens <ref type=""bibr"" target=""#b37"">[38]</ref> is a purely operating system proposal that significantly improves on Transparent Huge Pages <ref type=""bibr"" target=""#b15"">[16]</ref> by offering cleaner tradeoffs between memory consumption, performance, and latency. HawkEye <ref type=""bibr"" small regions of contiguous or clustered physical frames to contiguous virtual pages. However, these approaches are limited to a small number (e.g., <ref type=""bibr"" target=""#b15"">16</ref>) of page translations per TLB entry. This hinders the generality of their applicability to data sets of any si",0
"1536 entries for the 4KB/2MB page sizes. To support TPS, we modify the L1 TLB to contain a 32 entry fully-associative (as in other commercial designs <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b46"">[47]</ref>) TPS TLB. The TPS TLB takes the place of the existing 32 entry and",0
"easonably be able to meet similar timing constraints in similar processors. Alternative skewed-associative <ref type=""bibr"" target=""#b43"">[44]</ref>, <ref type=""bibr"" target=""#b52"">[53]</ref> TLB designs are possible.</p><p>In the newly added any-page size L1 TLB, we add a page mask field to each TL",0
"he z196 <ref type=""bibr"" target=""#b15"">[15]</ref> as a single table, but starting with z15 it exploits a variation of the TAGE algorithm based off of <ref type=""bibr"" target=""#b8"">[8]</ref>. Two TAGE PHT tables are employed in z15-a short and a long table -each 512 rows deep per BTB1 way for a total PHT table hits, it can provide the prediction. However, a weak TAGE PHT prediction can sometimes be detrimental, particularly after a context switch <ref type=""bibr"" target=""#b8"">[8]</ref>. As such, weak filtering is employed. Before allowing a weak TAGE PHT to provide the prediction direction, a w",1
"""bibr"" target=""#b7"">[7]</ref> provides references to various papers describing ways to reduce branch prediction latency. The Alpha EV8 line predictor <ref type=""bibr"" target=""#b11"">[11]</ref> fetches and predicts up to 16 conditional branches every cycle. Often branch prediction is integrated as par",0
"s using the queues can result in signal value changes of the model that create new events which trigger further processing until the model stabilizes <ref type=""bibr"" target=""#b0"">[1]</ref>.</p><p>Cycle-based simulation engines use much simpler algorithms, hence the speed up, but require the hardwar </p><p>Cycle-based simulation engines use much simpler algorithms, hence the speed up, but require the hardware language restrictions mentioned above <ref type=""bibr"" target=""#b0"">[1]</ref>. Constrained random verification environments support a symbolic language that allows a user to specify constr and checking of the DUT only at the interface level. White box approach involves monitoring and checking of interface and internal signals of the DUT <ref type=""bibr"" target=""#b0"">[1]</ref>. While certain design aspects can be functionally checked at architectural level using a black box approach, a",0
"above, the branch predictor augments the BHT with a Pattern History Table (PHT). The tagged PHT has been used on z branch predictions since the z196 <ref type=""bibr"" target=""#b15"">[15]</ref> as a single table, but starting with z15 it exploits a variation of the TAGE algorithm based off of <ref typ",0
"ance. To overcome this, several prior techniques have tried to predicate only those instances of H2P branches which have low confidence of prediction <ref type=""bibr"" target=""#b6"">[7]</ref>, <ref type=""bibr"" target=""#b10"">[11]</ref>- <ref type=""bibr"" target=""#b13"">[14]</ref>. Policies like Diverge M et=""#b6"">[7]</ref>, <ref type=""bibr"" target=""#b10"">[11]</ref>- <ref type=""bibr"" target=""#b13"">[14]</ref>. Policies like Diverge Merge Processor (DMP) <ref type=""bibr"" target=""#b6"">[7]</ref>, <ref type=""bibr"" target=""#b14"">[15]</ref> use careful compiler profiling to select target H2P branches, and t es a novel hardware mechanism to accurately detect control flow convergence using generic patterns of convergence. This is unlike previous approaches <ref type=""bibr"" target=""#b6"">[7]</ref>, <ref type=""bibr"" target=""#b11"">[12]</ref>- <ref type=""bibr"" target=""#b13"">[14]</ref> that were dependent upon anches as those branches whose taken and not-taken paths can converge to some later point in the program (using the same convergence criterion as DMP <ref type=""bibr"" target=""#b6"">[7]</ref>). Loops are naturally converging and contribute to another 13%. Remaining 13% conditional branches exhibit non al path. To mitigate this, past approaches have dynamically applied predication only on branch instances having low confidence from branch prediction <ref type=""bibr"" target=""#b6"">[7]</ref>, <ref type=""bibr"" target=""#b12"">[13]</ref>, <ref type=""bibr"" target=""#b13"">[14]</ref>.</p><p>Wish Branches <re mmocks which can be predicated dynamically (and profitably) and fetches both the directions of the hammock in hardware. Diverge Merge Processor (DMP) <ref type=""bibr"" target=""#b6"">[7]</ref> improves upon both Wish Branches and DHP. DMP uses compiler analysis-and-profiling to identify frequently misp on the predicated-false path, that does not produce register or flags (like stores or branches), instantly releases its resources.</p><p>Prior works <ref type=""bibr"" target=""#b6"">[7]</ref>, <ref type=""bibr"" target=""#b10"">[11]</ref> have relied on select-micro-op based approaches to handle correctne chitectural changes, we are able to achieve register transparency without resorting to complex RAT recovery mechanisms or re-execution as proposed in <ref type=""bibr"" target=""#b6"">[7]</ref>, <ref type=""bibr"" target=""#b10"">[11]</ref>.</p><p>3) Predicated-False Path Loads/Stores: All ACB body loads an o bad tuning. Dynamo was designed to holistically evaluate this trade-off for ACB.  In this section, we compare against Diverge-Merge Processor (DMP) <ref type=""bibr"" target=""#b6"">[7]</ref>, which relies on changes to the compiler, ISA and micro-architecture to perform selective predication on low c ref type=""bibr"" target=""#b16"">[17]</ref>, <ref type=""bibr"" target=""#b17"">[18]</ref> but due to large overheads, the realistic benefits are diminished <ref type=""bibr"" target=""#b6"">[7]</ref>, <ref type=""bibr"" target=""#b11"">[12]</ref>. Wish Branches <ref type=""bibr"" target=""#b11"">[12]</ref> rely on th ltipath execution was proposed in <ref type=""bibr"" target=""#b33"">[34]</ref>- <ref type=""bibr"" target=""#b35"">[36]</ref>. Diverge-Merge Processor (DMP) <ref type=""bibr"" target=""#b6"">[7]</ref>, <ref type=""bibr"" target=""#b14"">[15]</ref> uses branch prediction confidence to selectively predicate conditio",1
"sor <ref type=""bibr"" target=""#b0"">[1]</ref>. Performance potential for future processors is bound by the problem of mis-speculation.</p><p>predictors <ref type=""bibr"" target=""#b5"">[6]</ref>- <ref type=""bibr"" target=""#b7"">[8]</ref>. These branches cost not only performance but also significant power ss of branches that are still hard to predict. Many such branches are data dependent branches and are difficult to predict using just program history <ref type=""bibr"" target=""#b5"">[6]</ref>.</p><p>We characterized branch mispredictions on our selected workloads <ref type=""foot"" target=""#foot_0"">2</r",0
"ceptually understood as the sequence of (data/control) dependent instructions which determines the total execution cycles of a program. Fields et al. <ref type=""bibr"" target=""#b15"">[16]</ref> presented a graph-based definition of the critical path where the critical path is the maximum weighted path s of a branch, alters the critical path of execution. Figure <ref type=""figure"" target=""#fig_0"">2</ref>(a) shows an example DDG (using notations from <ref type=""bibr"" target=""#b15"">[16]</ref>) with and without predication. Without predication on a branch, a branch misprediction introduces the mispre",0
"der> 	<text xml:lang=""en""> 		<body> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>I. INTRODUCTION</head><p>High accuracy of modern branch predictors <ref type=""bibr"" target=""#b1"">[2]</ref>- <ref type=""bibr"" target=""#b4"">[5]</ref> has allowed Out-of-Order (OOO) processors to speculate aggressively o baseline is similar in parameters to the Intel Skylake processor <ref type=""bibr"" target=""#b0"">[1]</ref> and uses a branch predictor similar to TAGE <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b2"">[3]</ref>. We show the performance impact of perfect branch prediction on a con ww.tei-c.org/ns/1.0""><head>II. BACKGROUND AND MOTIVATION</head><p>Modern branch predictors use program history to predict future outcomes of a branch <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b3"">[4]</ref>, <ref type=""bibr"" target=""#b4"">[5]</ref>. Decades of research have ma . This effectively means that many more possible branch histories are possible, including wrong branch history. Recall that the TAGE branch predictor <ref type=""bibr"" target=""#b1"">[2]</ref> allocates a higher branch history prediction table on every misprediction, and the presence of unstable branch",0
"nzip, photoshop, sketchup, premiere SYSmark <ref type=""bibr"" target=""#b22"">[23]</ref> tabletmark <ref type=""bibr"" target=""#b23"">[24]</ref>, geekbench <ref type=""bibr"" target=""#b24"">[25]</ref>, compression, 3dmark <ref type=""bibr"" target=""#b25"">[26]</ref>, eembc <ref type=""bibr"" target=""#b26"">[27]</r",0
"apsed time for compression intense workloads. In the past, IBM z13 and many other systems have used FPGA based PCIe attached compression accelerators <ref type=""bibr"" target=""#b1"">[2]</ref>- <ref type=""bibr"" target=""#b4"">[5]</ref>. However, the FPGA cost and limited number of PCIe slots restrict the >[2]</ref>- <ref type=""bibr"" target=""#b4"">[5]</ref>. However, the FPGA cost and limited number of PCIe slots restrict their usage to high-end servers <ref type=""bibr"" target=""#b1"">[2]</ref> and specialized applications such as storage controllers.</p><p>IBM POWER9 (launched in 2017), and IBM z15 (la chips, 20 NXU units provide 280 GB/s total throughput <ref type=""bibr"" target=""#b5"">[6]</ref>. We would need 68 PCIe based compression cards such as <ref type=""bibr"" target=""#b1"">[2]</ref>, with 4GB/s peak throughput to match the NXU performance. Besides substantial improvements in performance, the nce, and compression ratio. We describe an area efficient LZ77 encoding hardware that improves state of the art in Section IV, where the related work <ref type=""bibr"" target=""#b1"">[2]</ref>- <ref type=""bibr"" target=""#b3"">[4]</ref>, <ref type=""bibr"" target=""#b13"">[14]</ref>- <ref type=""bibr"" target="" ted a true ""Dynamic Huffman"" mode in both POWER9 and z15 to achieve highest possible compression ratio, as described in Section V, where related work <ref type=""bibr"" target=""#b1"">[2]</ref>- <ref type=""bibr"" target=""#b3"">[4]</ref>, <ref type=""bibr"" target=""#b6"">[7]</ref>, <ref type=""bibr"" target=""#b >IBM has been employing compression hardware and software extensively in its products. IBM z13 and POWER systems used PCIe based Deflate accelerators <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b4"">[5]</ref>, <ref type=""bibr"" target=""#b14"">[15]</ref>, <ref type=""bibr"" target="" logic to meet throughput and compression ratio objectives which is not suitable for an on-chip design. In <ref type=""bibr"" target=""#b3"">[4]</ref> In <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b14"">[15]</ref>, the 32KB sliding window data is replicated M times stored directly Trade-offs</head><p>Many hardware designs implement a static table with an assumed LZ symbol distribution which typically degrades compression ratio <ref type=""bibr"" target=""#b1"">[2]</ref>- <ref type=""bibr"" target=""#b3"">[4]</ref>, <ref type=""bibr"" target=""#b13"">[14]</ref>. For example, Fig. <ref ty IBM z13 adapter uses a ""pseudo-dynamic"" approach where one of 15 different static Huffman tables yielding the smallest output is selected at run time <ref type=""bibr"" target=""#b1"">[2]</ref>. We implemented in contrast, a true ""Dynamic Huffman"" mode for both POWER9 and z15 to achieve highest possible",1
"are. A priority queue-based Huffman algorithm runs in O(n) time when symbols are pre-sorted by their count <ref type=""bibr"" target=""#b21"">[22]</ref>, <ref type=""bibr"" target=""#b22"">[23]</ref>. We implemented the 2dimensional parallel Shear-Sort algorithm to sort n counter values in O( ? n log 2 ? n) iting"" algorithms. The algorithms in the literature were found to be complex for a hardware implementation <ref type=""bibr"" target=""#b20"">[21]</ref>, <ref type=""bibr"" target=""#b22"">[23]</ref>. Our algorithm boosts small counter values to the effect of limiting the Huffman tree depth and then uses th",0
"mat and no entropy encoding. ZSTD has large search windows and uses the tANS encoder in addition to Huffman <ref type=""bibr"" target=""#b9"">[10]</ref>- <ref type=""bibr"" target=""#b12"">[13]</ref>. Compression and decompression throughput of these are ordered highest to lowest as LZ4, Snappy, ZSTD, and D Additionally, ZSTD uses an entropy algorithm called tANS that can encode some highly compressible files with fewer bits per symbol than Huffman codes <ref type=""bibr"" target=""#b12"">[13]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>C. Throughput</head><p>Figure <ref type=""figure"">1",0
", similar to that of z15. User threads atomically post CRBs to NXU job queues, using the POWER9 ""copy-paste"" instruction pair bypassing the OS kernel <ref type=""bibr"" target=""#b41"">[42]</ref>, <ref type=""bibr"" target=""#b42"">[43]</ref>.</p><p>POWER9 NXU has two 256 deep hardware managed job queues (h hput. The POWER9 processor includes a ""Nest MMU"" (NMMU), and an ""effective to real address translation"" table (ERAT) enabling virtually addressed NXU <ref type=""bibr"" target=""#b41"">[42]</ref>.</p><p>A notable design goal of these features and functions in both z15 and POWER9 is that NXU are accessed",0
"requency than the processor cores). The DMA engine moves data between the processor fabric and the Compress and Uncompress macros. and Silesia corpus <ref type=""bibr"" target=""#b38"">[39]</ref>. With increasing window size the compression ratio levels off.</p><p>The History FIFO is an SRAM that stores =""bibr"" target=""#b9"">[10]</ref>, <ref type=""bibr"" target=""#b11"">[12]</ref>. Silesia, Canterbury, and Calgary compression benchmark datasets were used <ref type=""bibr"" target=""#b38"">[39]</ref>, <ref type=""bibr"" target=""#b48"">[49]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>B. Comp d=""fig_2""><head>Fig. 5 .</head><label>5</label><figDesc>Fig.5. Compression ratio as a function of sliding window size (zlib 1.2.11) and Silesia corpus<ref type=""bibr"" target=""#b38"">[39]</ref>. With increasing window size the compression ratio levels off.</figDesc></figure> <figure xmlns=""http://www.",0
"t=""#b19"">[20]</ref>, <ref type=""bibr"" target=""#b20"">[21]</ref>, <ref type=""bibr"" target=""#b21"">[22]</ref>, <ref type=""bibr"" target=""#b22"">[23]</ref>, <ref type=""bibr"" target=""#b23"">[24]</ref>. The main concept behind these approaches is to interpret protein sequences as sentences and their constitue model parameters). Largely, we used configurations successfully transferred from NLP to protein sequences <ref type=""bibr"" target=""#b20"">[21]</ref>, <ref type=""bibr"" target=""#b23"">[24]</ref>, <ref type=""bibr"" target=""#b51"">[52]</ref>, with the exception of the number of layers that was increased to representing individual amino acids irrespective of their surrounding context (residues next to it). As previously established for another protein LM <ref type=""bibr"" target=""#b23"">[24]</ref>, the t-SNE projections (e.g. ProtBert Fig. <ref type=""figure"">6A</ref>) suggested that all LMs captured esse t=""#b18"">[19]</ref>, <ref type=""bibr"" target=""#b19"">[20]</ref>, <ref type=""bibr"" target=""#b20"">[21]</ref>, <ref type=""bibr"" target=""#b21"">[22]</ref>, <ref type=""bibr"" target=""#b23"">[24]</ref>, we might expect an upper limit for what protein LMs can learn when using auto-regressive or auto-encoding e",1
"t=""#b17"">[18]</ref>, <ref type=""bibr"" target=""#b18"">[19]</ref>, <ref type=""bibr"" target=""#b19"">[20]</ref>, <ref type=""bibr"" target=""#b20"">[21]</ref>, <ref type=""bibr"" target=""#b21"">[22]</ref>, <ref type=""bibr"" target=""#b22"">[23]</ref>, <ref type=""bibr"" target=""#b23"">[24]</ref>. The main concept behi t=""#b17"">[18]</ref>, <ref type=""bibr"" target=""#b18"">[19]</ref>, <ref type=""bibr"" target=""#b19"">[20]</ref>, <ref type=""bibr"" target=""#b20"">[21]</ref>, <ref type=""bibr"" target=""#b21"">[22]</ref>, <ref type=""bibr"" target=""#b23"">[24]</ref>, we might expect an upper limit for what protein LMs can learn wh",0
"rget=""#b10"">[11]</ref>, Natural Language Processing (NLP) has been benefiting more from advances in HPC than other fields. In particular Transformers <ref type=""bibr"" target=""#b11"">[12]</ref> have reached state-of-the-art performance in several tasks including translation, summarization and question",0
"as early stopping. The final performance was evaluated on three different data sets, each with &lt;25% PIDE to the training set: CB513 (513 proteins; <ref type=""bibr"" target=""#b41"">[42]</ref>), TS115 (115 proteins; <ref type=""bibr"" target=""#b42"">[43]</ref>) and CASP12 (21 proteins; <ref type=""bibr""",0
"s as words. Thereby, protein databases contain several orders of magnitude more tokens than corpora used in NLP, e.g., Google's Billion Word data set <ref type=""bibr"" target=""#b38"">[39]</ref> is one of the biggest for NLP with about 829 million tokens (words), i.e. about 500-times fewer than BFD wit",0
"modalities, i.e., the amount of agreement shared by all the modalities.</p><p>Inspired by such an assumption and the fact that the Total Correlation <ref type=""bibr"" target=""#b27"">[29]</ref> can measure the amount of information shared by M (M ≥ 2) variables, in this paper, we propose Total Correla arn the optimal, i.e., the Bayesian Posterior classifiers of each modality.</p><p>Total Correlation/Mutual information maximization Total Correlation <ref type=""bibr"" target=""#b27"">[29]</ref>, as an extension of Mutual Information, measures the amount of information shared by M (M ≥ 2) variables. Th",1
"label) shared by all modalities. The first branch applies the co-training algorithm proposed by Blum et. al <ref type=""bibr"" target=""#b4"">[6]</ref>. <ref type=""bibr"" target=""#b15"">[17,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b10"">12,</ref><ref type=""bibr"" target=""#b19"">",0
"<ref type=""figure"" target=""#fig_3"">3</ref>, our method TCGM has competitive performance compared to well established clustering algorithms K-means++ <ref type=""bibr"" target=""#b1"">[2]</ref> and spectral clustering <ref type=""bibr"" target=""#b23"">[25]</ref>. Based on the promising unsupervised learnin heimer's Disease Neuroimaging Initiative (ADNI) dataset <ref type=""foot"" target=""#foot_2"">10</ref> , with 3D images sMRI and PET. DARTEL VBM pipeline <ref type=""bibr"" target=""#b1"">[2]</ref> is implemented to pre-process the sMRI data, and then images of PET were reoriented into a standard 91 × 109 ×",0
"es: (i) co-training strategy <ref type=""bibr"" target=""#b4"">[6]</ref>; and (ii) learning joint representation across modalities in an unsupervised way <ref type=""bibr"" target=""#b24"">[26,</ref><ref type=""bibr"" target=""#b26"">28]</ref>. These methods suffer from either too strong assumptions or loss of same conditional distributions for data point labels in each modality, may not be consistent with the real settings.</p><p>The second branch of work <ref type=""bibr"" target=""#b24"">[26,</ref><ref type=""bibr"" target=""#b26"">28,</ref><ref type=""bibr"" target=""#b7"">9,</ref><ref type=""bibr"" target=""#b29"">",0
"ate γ u = 0.0001, γ l = 0.01 is used as the optimizer during training. Batch size is set to 32. We further compare with two additional baselines: VAT <ref type=""bibr"" target=""#b22"">[24]</ref> uses adversarial training for semi-supervised learning; PVCC <ref type=""bibr"" target=""#b31"">[33]</ref> that",0
"rowing body of research has been dedicated to answering what causes deep networks to be fragile to adversarial examples and how to improve robustness <ref type=""bibr"" target=""#b45"">[47,</ref><ref type=""bibr"" target=""#b11"">13,</ref><ref type=""bibr"" target=""#b34"">36,</ref><ref type=""bibr"" target=""#b33 ,</ref><ref type=""bibr"" target=""#b50"">52]</ref>. It has been theoretically shown that decreasing the input dimensionality of data improves robustness <ref type=""bibr"" target=""#b45"">[47]</ref>. Adversarial training <ref type=""bibr"" target=""#b33"">[35]</ref> improves robustness by dynamically augmentin improves adversarial robustness while also maintaining performance on natural examples.</p><p>Using the first order vulnerability of neural networks <ref type=""bibr"" target=""#b45"">[47]</ref>, we theoretically show that increasing output dimensionality -treating each output dimension as an individua </ref> decreases the input gradients norm. These methods can improve the model's robustness without compromising clean accuracy. Simon-Gabriel et al. <ref type=""bibr"" target=""#b45"">[47]</ref> conducted a theoretical analysis of the vulnerability of neural network classifiers, and connected gradient b29"">31,</ref><ref type=""bibr"" target=""#b25"">27]</ref>. We denote the multitask predictor as F and each individual task predictor as F c . Prior work <ref type=""bibr"" target=""#b45"">[47]</ref> showed that the norm of gradients captures the vulnerability of the model. We thus measure the multitask mod n any perturbation of the input. Given the adversarial noise is imperceptible, i.e., r → 0, we can approximate ∆L with a first-order Taylor expansion <ref type=""bibr"" target=""#b45"">[47]</ref>.</p><p>Lemma 1. For a given neural network F that predicts multiple tasks, the adversarial vulnerability is< 2 2 ) = 1 M E r i 2 = σ 2 M ∝ 1 M<label>(6)</label></formula><p>Remark 1. By increasing the number of output tasks M , the first order vulnerability <ref type=""bibr"" target=""#b45"">[47]</ref> of network decreases. In the ideal case, if the model has an infinite number of uncorrelated tasks, then it measures the adversarial vulnerability. Overall, as we add more tasks, the norm of the joint gradient decreases, indicating improvement to robustness <ref type=""bibr"" target=""#b45"">[47]</ref>. The only exception is the depth estimation task, which we believe is due to the large range of values (0 to /1.0"" place=""foot"" n=""1"" xml:id=""foot_0"">We use the same dimension for baselines and ours during comparison because input dimension impacts robustness<ref type=""bibr"" target=""#b45"">[47]</ref>.</note> 		</body> 		<back>  			<div type=""acknowledgement""> <div xmlns=""http://www.tei-c.org/ns/1.0""><p>We e",1
"al robustness. While previous work shows that multitask learning can improve the performance of specific tasks <ref type=""bibr"" target=""#b2"">[4,</ref><ref type=""bibr"" target=""#b46"">48]</ref>, we show that it increases robustness too. See Figure <ref type=""figure"">1</ref>. Unlike prior work that trad e=""bibr"" target=""#b2"">[4,</ref><ref type=""bibr"" target=""#b13"">15,</ref><ref type=""bibr"" target=""#b8"">10,</ref><ref type=""bibr"" target=""#b23"">25,</ref><ref type=""bibr"" target=""#b46"">48]</ref> aims to solve several tasks at once, and has been used to learn better models for semantic segmentation <ref esponding ground-truth label for task c. In this work, we focus on multitask learning with shared parameters <ref type=""bibr"" target=""#b22"">[24,</ref><ref type=""bibr"" target=""#b46"">48,</ref><ref type=""bibr"" target=""#b31"">33,</ref><ref type=""bibr"" target=""#b29"">31,</ref><ref type=""bibr"" target=""#b25"" l c = 1, ..., M , such that M c=1 λ c = 1. In our experiments on real-world datasets, we will adjust the λ c accordingly, following standard practice <ref type=""bibr"" target=""#b46"">[48,</ref><ref type=""bibr"" target=""#b25"">27]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.2"">Ad g only the semantic segmentation task, then add the depth estimation and input reconstruction task. For the Taskonomy dataset, following the setup in <ref type=""bibr"" target=""#b46"">[48]</ref>, we start with only semantic segmentation, and add depth estimation, normal, keypoints 2D, edge texture, and are in Figure <ref type=""figure"" target=""#fig_2"">6</ref>.</p><p>On the Taskonomy dataset, we conduct experiments on 11 tasks. Following the setup in <ref type=""bibr"" target=""#b46"">[48]</ref>, we use ResNet-18 <ref type=""bibr"" target=""#b17"">[19]</ref> as the shared encoding network, where each indiv",0
"46,</ref><ref type=""bibr"" target=""#b52"">54,</ref><ref type=""bibr"" target=""#b35"">37]</ref> to fool target models. While attacking single output models <ref type=""bibr"" target=""#b15"">[17,</ref><ref type=""bibr"" target=""#b24"">26]</ref> is straightforward, Arnab et. al. <ref type=""bibr"" target=""#b0"">[2]< ersarial Robustness: Adversarial training improves models' robustness against attacks, where the training data is augmented using adversarial samples <ref type=""bibr"" target=""#b15"">[17,</ref><ref type=""bibr"" target=""#b33"">35]</ref>. In combination with adversarial training, later works <ref type=""bi ses as the number of output dimension increases, which validates Theorem 1.</p><p>Besides the norm of gradient, we measure the performance under FGSM <ref type=""bibr"" target=""#b15"">[17]</ref> and PGD <ref type=""bibr"" target=""#b33"">[35]</ref> adversarial attacks, and show that it improves as output d s <ref type=""bibr"" target=""#b33"">[35]</ref>. We evaluate with four different attacks:</p><p>FGSM: We evaluate on the Fast Gradient Sign Method (FGSM) <ref type=""bibr"" target=""#b15"">[17]</ref>, which generates adversarial examples x adv by x adv = x + • sign(∇ x (F (x), y)). It is a single step, non-",0
"le to adversarial examples. A large body of work has demonstrated that images with human-imperceptible noise <ref type=""bibr"" target=""#b33"">[35,</ref><ref type=""bibr"" target=""#b1"">3,</ref><ref type=""bibr"" target=""#b9"">11,</ref><ref type=""bibr"" target=""#b38"">40]</ref> can be crafted to cause the mode",0
"type=""bibr"" target=""#b7"">9,</ref><ref type=""bibr"" target=""#b0"">2,</ref><ref type=""bibr"" target=""#b4"">6,</ref><ref type=""bibr"" target=""#b44"">46,</ref><ref type=""bibr"" target=""#b52"">54,</ref><ref type=""bibr"" target=""#b35"">37]</ref> to fool target models. While attacking single output models <ref type truction (A). We use the ""tiny"" version of their dataset splits <ref type=""bibr"">[1]</ref>. We resize the images to 256 × 256.  We do not use the DAG <ref type=""bibr"" target=""#b52"">[54]</ref> attack for segmentation because it is an unrestricted attack without controlling L ∞ bound. For all the iter",0
"y among foreground objects or among pixels with similar semantics.</p><p>Sharing a similar philosophy, there have been works on contrastive attention <ref type=""bibr"" target=""#b30"">[31,</ref><ref type=""bibr"" target=""#b41"">42]</ref>. MGCAM <ref type=""bibr"" target=""#b30"">[31]</ref> uses the contrastiv r philosophy, there have been works on contrastive attention <ref type=""bibr"" target=""#b30"">[31,</ref><ref type=""bibr"" target=""#b41"">42]</ref>. MGCAM <ref type=""bibr"" target=""#b30"">[31]</ref> uses the contrastive feature between persons and backgrounds, but it requires extra mask supervision for per , ACM explicitly uses direct comparison procedure for context modeling; instead of using extra supervision to localize regions to compare as in MGCAM <ref type=""bibr"" target=""#b30"">[31]</ref>, ACM automatically learns to focus on meaningful regions to compare. Importantly, the efficacy of our explic",1
"gn the class-level prediction with the potentially abnormal location <ref type=""bibr"" target=""#b34"">[35,</ref><ref type=""bibr"" target=""#b31"">32,</ref><ref type=""bibr"" target=""#b9"">10]</ref> without the text reports on the location of the disease. Some works observe that although getting annotations e=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b31"">32,</ref><ref type=""bibr"" target=""#b9"">10]</ref>. The dataset contains 112,120 images from 30,805 unique patients. Image-level labels are mined from imageattac",0
"</ref> with various backbones such as ResNet <ref type=""bibr"" target=""#b13"">[14]</ref>, ResNeXt <ref type=""bibr"" target=""#b39"">[40]</ref> or DenseNet <ref type=""bibr"" target=""#b15"">[16]</ref>. Experimental results on chest X-ray datasets and natural image datasets demonstrate that the explicit compa ulti-label classification with chest X-Rays, we mainly adopt ResNet-50 as our backbone network.</p><p>To show generality, we sometimes adopt DenseNet <ref type=""bibr"" target=""#b15"">[16]</ref> and ResNeXt <ref type=""bibr"" target=""#b39"">[40]</ref> as backbone networks. In classification tasks, we repo be classified and located in a weakly-supervised multi-label classification framework. ResNet <ref type=""bibr"" target=""#b13"">[14]</ref> and DenseNet <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b28"">29]</ref> pretrained on ImageNet <ref type=""bibr"" target=""#b7"">[8]</ref> have Also, ACM is designed to be light-weight, self-contained, and compatible with popular backbone architectures <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b39"">40]</ref>. We formulate ACM comprising three procedures as</p><formula xml:id=""",0
"outperforms other context-related modules, in both chest X-ray tasks and natural image tasks.</p><p>Experimental Setting Following the previous study <ref type=""bibr"" target=""#b1"">[2]</ref> on multi-label classification with chest X-Rays, we mainly adopt ResNet-50 as our backbone network.</p><p>To s",0
"ize the lesions. To improve recognition performance, Yao et al. <ref type=""bibr"" target=""#b40"">[41]</ref> handles varying lesion sizes and Mao et al. <ref type=""bibr"" target=""#b24"">[25]</ref> No Finding Nodule Fig. <ref type=""figure"">1</ref>: An example of a comparison procedure for radiologists. Li =""bibr"" target=""#b40"">[41,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b28"">29,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b31"">32,</ref><ref type=""bibr"" target=""#b9"">",0
"ow-level and fine-grained detailed information, while feature maps generated by the decoder contain high-level and coarse-gained semantic information <ref type=""bibr"" target=""#b14"">[15]</ref>. And skip connections, which combine the low-level and high-level feature maps, are an effective method to b w-level and high-level feature maps, are an effective method to boost the semantic extraction ability of encoder-decoder frameworks.</p><p>In U-Net++ <ref type=""bibr"" target=""#b14"">[15]</ref>, plain skip connections are substituted by nested and dense skip connections, which enhance the ability of s arget=""#b19"">[20]</ref>, Attention U-Net <ref type=""bibr"" target=""#b20"">[21]</ref>, FGC <ref type=""bibr"" target=""#b21"">[22]</ref>, MSFCN, and U-Net++ <ref type=""bibr"" target=""#b14"">[15]</ref>. The major C. Duan is with the State Key Laboratory of Information Engineering in Surveying, Mapping, and Re arget=""#b19"">[20]</ref>, Attention U-Net <ref type=""bibr"" target=""#b20"">[21]</ref>, FGC <ref type=""bibr"" target=""#b21"">[22]</ref>, MSFCN, and U-Net++ <ref type=""bibr"" target=""#b14"">[15]</ref>. Excluding SegNet, DeepLab V3 and DeepLab V3+, the remaining methods are all improved version of U-Net.</p><",1
"he semantic gap between the encoder and decoder. To make utmost of the multi-scale features, the full-scale skip connections are designed in U-Net 3+ <ref type=""bibr"" target=""#b15"">[16]</ref>. However, the design philosophy of full-scale skip connections impliedly indicates that all channels of feat",1
"d capture refined features by summing up the outputs of three convolutions, which just cause finite increasing in additional computational complexity <ref type=""bibr"" target=""#b16"">[17]</ref>. The effectiveness of ACB has been verified in the fields including image classification <ref type=""bibr"" ta ational complexity <ref type=""bibr"" target=""#b16"">[17]</ref>. The effectiveness of ACB has been verified in the fields including image classification <ref type=""bibr"" target=""#b16"">[17]</ref>, image denoising <ref type=""bibr"" target=""#b17"">[18]</ref>, and medical image segmentation <ref type=""bibr"" ained in remote sensing images, the neural network should be robust to rotation and renders consistent results in different rotations. As reported in <ref type=""bibr"" target=""#b16"">[17]</ref>, different asymmetric convolutions are robust with different rotation objects. As can be seen from Fig. <ref mes to boost the rotation robustness of the neural network.</p><p>Based on above-mentioned insight, we modify the asymmetric convolutions proposed in <ref type=""bibr"" target=""#b16"">[17]</ref> and design an asymmetric convolution block (ACB) to capture features from different receptive fields, which",1
"or mid-level semantic features restricts the flexibility and adaptability of these methods.</p><p>More recently, Convolutional Neural Networks (CNN) <ref type=""bibr"" target=""#b8"">[9]</ref> have demonstrated its powerful capacity of automatically capture nonlinear and hierarchical features from imag",0
"r"" target=""#b9"">[10]</ref>. For semantic segmentation, the encoder-decoder frameworks such as SegNet <ref type=""bibr"" target=""#b10"">[11]</ref>, U-Net <ref type=""bibr"" target=""#b11"">[12]</ref>, and DeepLab <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b13"">14]</ref> have become t o verify the effectiveness of MACU-Net, we compare the performance of proposed algorithm with SegNet <ref type=""bibr"" target=""#b10"">[11]</ref>, U-Net <ref type=""bibr"" target=""#b11"">[12]</ref>, DeepLab V3 <ref type=""bibr"" target=""#b12"">[13]</ref>, DeepLab V3+ <ref type=""bibr"" target=""#b13"">[14]</ref> org/ns/1.0""><head>B. Experimental Setting</head><p>To evaluate the effectiveness of MACU-Net, SegNet <ref type=""bibr"" target=""#b10"">[11]</ref>, U-Net <ref type=""bibr"" target=""#b11"">[12]</ref>, DeepLab V3 <ref type=""bibr"" target=""#b12"">[13]</ref>, DeepLab V3+ <ref type=""bibr"" target=""#b13"">[14]</ref>",0
"head><p>emantic segmentation using remote sensing images, i.e., the assignment of assigning the precise category to every pixel contained in an image <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref>, plays a critical role in wide range of application scenarios such as la",0
"target=""#b11"">[12]</ref>, DeepLab V3 <ref type=""bibr"" target=""#b12"">[13]</ref>, DeepLab V3+ <ref type=""bibr"" target=""#b13"">[14]</ref>, FC-DenseNet57 <ref type=""bibr"" target=""#b19"">[20]</ref>, Attention U-Net <ref type=""bibr"" target=""#b20"">[21]</ref>, FGC <ref type=""bibr"" target=""#b21"">[22]</ref>, M 11"">[12]</ref>, DeepLab V3 <ref type=""bibr"" target=""#b12"">[13]</ref>, DeepLab V3+ <ref type=""bibr"" target=""#b13"">[14]</ref>, FC-DenseNet57 (tiramisu) <ref type=""bibr"" target=""#b19"">[20]</ref>, Attention U-Net <ref type=""bibr"" target=""#b20"">[21]</ref>, FGC <ref type=""bibr"" target=""#b21"">[22]</ref>, M",0
"e classification <ref type=""bibr"" target=""#b16"">[17]</ref>, image denoising <ref type=""bibr"" target=""#b17"">[18]</ref>, and medical image segmentation <ref type=""bibr"" target=""#b18"">[19]</ref>. In <ref type=""bibr"" target=""#b18"">[19]</ref>, only the convolutional layers of encoder are replaced by ACB. "">[17]</ref>, image denoising <ref type=""bibr"" target=""#b17"">[18]</ref>, and medical image segmentation <ref type=""bibr"" target=""#b18"">[19]</ref>. In <ref type=""bibr"" target=""#b18"">[19]</ref>, only the convolutional layers of encoder are replaced by ACB. Aiming at the task of high-resolution remote",0
"herto the remote sensing community has tried to design assorted classifiers from diverse perspectives, from orthodox methods such as distance measure <ref type=""bibr"" target=""#b5"">[6]</ref>, to advanced methods including support vector machine (SVM) <ref type=""bibr"" target=""#b6"">[7]</ref> and random",0
"the encoder-decoder frameworks such as SegNet <ref type=""bibr"" target=""#b10"">[11]</ref>, U-Net <ref type=""bibr"" target=""#b11"">[12]</ref>, and DeepLab <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b13"">14]</ref> have become the frequently-used schemes. Generally, feature maps gen performance of proposed algorithm with SegNet <ref type=""bibr"" target=""#b10"">[11]</ref>, U-Net <ref type=""bibr"" target=""#b11"">[12]</ref>, DeepLab V3 <ref type=""bibr"" target=""#b12"">[13]</ref>, DeepLab V3+ <ref type=""bibr"" target=""#b13"">[14]</ref>, FC-DenseNet57 <ref type=""bibr"" target=""#b19"">[20]</r evaluate the effectiveness of MACU-Net, SegNet <ref type=""bibr"" target=""#b10"">[11]</ref>, U-Net <ref type=""bibr"" target=""#b11"">[12]</ref>, DeepLab V3 <ref type=""bibr"" target=""#b12"">[13]</ref>, DeepLab V3+ <ref type=""bibr"" target=""#b13"">[14]</ref>, FC-DenseNet57 (tiramisu) <ref type=""bibr"" target=""#b",0
"br"" target=""#b13"">[14]</ref>, FC-DenseNet57 <ref type=""bibr"" target=""#b19"">[20]</ref>, Attention U-Net <ref type=""bibr"" target=""#b20"">[21]</ref>, FGC <ref type=""bibr"" target=""#b21"">[22]</ref>, MSFCN, and U-Net++ <ref type=""bibr"" target=""#b14"">[15]</ref>. The major C. Duan is with the State Key Labor ""#b13"">[14]</ref>, FC-DenseNet57 (tiramisu) <ref type=""bibr"" target=""#b19"">[20]</ref>, Attention U-Net <ref type=""bibr"" target=""#b20"">[21]</ref>, FGC <ref type=""bibr"" target=""#b21"">[22]</ref>, MSFCN, and U-Net++ <ref type=""bibr"" target=""#b14"">[15]</ref>. Excluding SegNet, DeepLab V3 and DeepLab V3+,",0
"te sensing images, i.e., the assignment of assigning the precise category to every pixel contained in an image <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref>, plays a critical role in wide range of application scenarios such as land resource management, yield estimatio",0
"et=""#b12"">[13]</ref>, DeepLab V3+ <ref type=""bibr"" target=""#b13"">[14]</ref>, FC-DenseNet57 <ref type=""bibr"" target=""#b19"">[20]</ref>, Attention U-Net <ref type=""bibr"" target=""#b20"">[21]</ref>, FGC <ref type=""bibr"" target=""#b21"">[22]</ref>, MSFCN, and U-Net++ <ref type=""bibr"" target=""#b14"">[15]</ref> 13]</ref>, DeepLab V3+ <ref type=""bibr"" target=""#b13"">[14]</ref>, FC-DenseNet57 (tiramisu) <ref type=""bibr"" target=""#b19"">[20]</ref>, Attention U-Net <ref type=""bibr"" target=""#b20"">[21]</ref>, FGC <ref type=""bibr"" target=""#b21"">[22]</ref>, MSFCN, and U-Net++ <ref type=""bibr"" target=""#b14"">[15]</ref>",0
"ad>A. Datasets</head><p>The effectiveness of MACU-Net is verified using Wuhan Dense Labeling Dataset (WHDLD) <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b24"">25]</ref> and Gaofen Image Dataset (GID) <ref type=""bibr"">[30]</ref>. WHDLD contains 4940 RGB images in the size of 256",0
"s, from orthodox methods such as distance measure <ref type=""bibr"" target=""#b5"">[6]</ref>, to advanced methods including support vector machine (SVM) <ref type=""bibr"" target=""#b6"">[7]</ref> and random forest (RF) <ref type=""bibr"" target=""#b7"">[8]</ref>. However, the high dependency on hand-crafted v",0
"-level control of their generated content.</p><p>In this paper, we introduce TOAD-GAN as a solution to these problems. Our work is inspired by SinGAN <ref type=""bibr"" target=""#b15"">(Shaham, Dekel, and Michaeli 2019)</ref>, a recent Generative Adversarial Network (GAN) <ref type=""bibr"">(Goodfellow et abel>3</label><figDesc>Figure 3: Generation process of TOAD-GAN on Super Mario Bros. level 1-2. The architecture is adapted from SinGAN (cf. Fig. 4 of<ref type=""bibr"" target=""#b15"">(Shaham, Dekel, and Michaeli 2019)</ref>). We use a downsampling method on a one-hot encoded version of the level that have to be optimized, which further complicates the training process.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>SinGAN</head><p>SinGAN <ref type=""bibr"" target=""#b15"">(Shaham, Dekel, and Michaeli 2019</ref>) is a novel GAN architecture that enables learning a generative model from a si is determined by the size of the initial noise map at the lowest scale. For a more in-depth explanation please refer to the original SinGAN paper by <ref type=""bibr"" target=""#b15"">Shaham, Dekel, and Michaeli (2019)</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>TOAD-GAN</head><p>Fi",1
"tein distance <ref type=""bibr"" target=""#b0"">(Arjovsky, Chintala, and Bottou 2017)</ref> and penalizing the norm of the gradients of the discriminator <ref type=""bibr"" target=""#b7"">(Gulrajani et al. 2017)</ref>. The resulting Wasserstein GAN with Gradient Penalty (WGAN-GP) is able to model a variety",0
"ays. This section only covers a selection, due to the vast amount of PCG approaches.</p><p>For a review of pattern-based level generators for SMB see <ref type=""bibr"" target=""#b11"">Khalifa et al. (2019)</ref>.</p><p>Super Mario Bros. Level Generation <ref type=""bibr"" target=""#b3"">Dahlskog and Togeli",0
"Many different extensions to the basic architecture were proposed to stabilize the training process, for example minimizing the Wasserstein distance <ref type=""bibr"" target=""#b0"">(Arjovsky, Chintala, and Bottou 2017)</ref> and penalizing the norm of the gradients of the discriminator <ref type=""bib",0
"g game levels to identify patterns <ref type=""bibr"" target=""#b3"">(Dahlskog and Togelius 2012</ref>) and combined them using simple statistical models <ref type=""bibr"" target=""#b17"">(Snodgrass and Ontanón 2013)</ref>. The quality of these algorithms critically depends on the extracted patterns and co nds on the extracted patterns and co-occurrence relations, which have to be defined manually. Recent approaches used PCG via Machine Learning (PCGML) <ref type=""bibr"" target=""#b17"">(Summerville et al. 2018)</ref> to learn the patterns and relations from the data automatically <ref type=""bibr"" target chine Learning (PCGML) <ref type=""bibr"" target=""#b17"">(Summerville et al. 2018)</ref> to learn the patterns and relations from the data automatically <ref type=""bibr"" target=""#b17"">(Summerville and Mateas 2016;</ref><ref type=""bibr"" target=""#b22"">Volz et al. 2020</ref>). However, simply applying the th an EA to generate new levels. The network predicts the height of a token in a level slice, given the heights of all tokens in the previous slices. <ref type=""bibr"" target=""#b17"">Summerville and Mateas (2016)</ref> trained their model on levels by predicting the tokens sequentially. They used a ne _0"">3</ref> shows the pipeline of TOAD-GAN for the generation of SMB levels. There are 15 original SMB levels provided by the Video Game Level Corpus <ref type=""bibr"" target=""#b17"">(Summerville et al. 2016)</ref>, each with different characteristics. The levels are placed in three worlds (overworld,",0
"ting, either for documents <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b28"">29]</ref> or for member profiles (personalization) <ref type=""bibr"" target=""#b9"">[10]</ref>. It requires a huge amount of hard disk space to store the embedding, as well as a sophisticated system desig",1
"documents. This approach, in the category of interaction based models <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b27"">28]</ref>, comes with a significant challenge in online serving: a) the heavy BERT computation on the fly is not afford the query and each word embedding in the document. The final document score is computed based on the pairwise word similarity score histogram. K-NRM <ref type=""bibr"" target=""#b27"">[28]</ref> and Conv-KNRM <ref type=""bibr"" target=""#b6"">[7]</ref> extended DRMM by kernel pooling and pairwise ngram sim",0
"get=""#b19"">[20,</ref><ref type=""bibr"" target=""#b25"">26]</ref> encodes word order information using CNN <ref type=""bibr"" target=""#b15"">[16]</ref>/LSTM <ref type=""bibr"" target=""#b11"">[12]</ref>, respectively. All these three works assume that there is only one field on the document side, and the docum",0
"vior features, are usually informative for ranking. To integrate them with deep NLP features, we use standard normalization and elementwise rescaling <ref type=""bibr"" target=""#b0"">[1]</ref> to better process the features:</p><formula xml:id=""formula_0"">x (1) i = x i -? ? x (2) i = wx (1) i + b</form",0
"ores as input. DeText provides the flexibility of pointwise, pairwise or listwise LTR <ref type=""bibr"" target=""#b2"">[3]</ref>, as well as Lambda rank <ref type=""bibr"" target=""#b3"">[4]</ref>. Binary classification loss (pointwise learning-to-rank) can be used for systems where click probability is im",0
"ech in order to provide the visual cues when they are not available <ref type=""bibr"" target=""#b4"">[5]</ref>, <ref type=""bibr"" target=""#b5"">[6]</ref>, <ref type=""bibr"" target=""#b6"">[7]</ref>, <ref type=""bibr"" target=""#b7"">[8]</ref>, <ref type=""bibr"" target=""#b8"">[9]</ref>, <ref type=""bibr"" target=""#b proposed by Chen et al. <ref type=""bibr"" target=""#b4"">[5]</ref>. The system first predicts 68 face landmarks from speech using an LSTM-based network <ref type=""bibr"" target=""#b6"">[7]</ref>, and then predicts a few talking face images from the condition image and the face landmarks. They employ a di",1
"ention from researchers in recent years. One approach is to first convert the speech input to face landmarks <ref type=""bibr"" target=""#b5"">[6]</ref>, <ref type=""bibr"" target=""#b15"">[16]</ref>, <ref type=""bibr"" target=""#b8"">[9]</ref>, <ref type=""bibr"" target=""#b16"">[17]</ref>, <ref type=""bibr"" target ref type=""bibr"" target=""#b11"">[12]</ref> and then estimate video frames using the predicted face landmarks. In Suwajanakorn et al.'s two-stage system <ref type=""bibr"" target=""#b15"">[16]</ref>, an LSTM network first predicts the principal component analysis (PCA) coefficients of face landmarks from s",0
"Visual cues, when present, also play a vital role. The presence of visual cues improves speech comprehension <ref type=""bibr"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b2"">[3]</ref>, <ref type=""bibr"" target=""#b3"">[4]</ref> in noisy environments and fo",0
"visual stimuli has been examined in recent years. Existing work <ref type=""bibr"" target=""#b22"">[23]</ref>, <ref type=""bibr"" target=""#b23"">[24]</ref>, <ref type=""bibr"" target=""#b24"">[25]</ref>, <ref type=""bibr"" target=""#b25"">[26]</ref>, <ref type=""bibr"" target=""#b26"">[27]</ref> concludes that differe "" target=""#b25"">[26]</ref> showed that perception is sensitive to stimuli from multiple modalities in acted data and naturalistic data. Jessen et al. <ref type=""bibr"" target=""#b24"">[25]</ref> suggested that emotional visual content allows more reliable prediction of auditory information. Schirmer et",0
">B. Multimodal Human Emotion Perception</head><p>Emotion perception from auditory and visual stimuli has been examined in recent years. Existing work <ref type=""bibr"" target=""#b22"">[23]</ref>, <ref type=""bibr"" target=""#b23"">[24]</ref>, <ref type=""bibr"" target=""#b24"">[25]</ref>, <ref type=""bibr"" targ type=""bibr"" target=""#b24"">[25]</ref> suggested that emotional visual content allows more reliable prediction of auditory information. Schirmer et al. <ref type=""bibr"" target=""#b22"">[23]</ref> explored modalities in term of neural responses and showed that each modality provides a distinct insight an",0
"implementation for deployment in production. Typical works include XLA <ref type=""bibr"" target=""#b8"">[9]</ref> (applicable to training as well), TVM <ref type=""bibr"" target=""#b9"">[10]</ref>, Glow <ref type=""bibr"" target=""#b10"">[11]</ref>, Tensor Comprehensions <ref type=""bibr"" target=""#b11"">[12]</r ell. XLA lowers operators into primitive linear algebra operations and calls into backend-specific libraries for execution on different backends. TVM <ref type=""bibr"" target=""#b9"">[10]</ref> is an end-to-end compiler framework with Halide at the core, which first optimizes a computational graph, the",1
"ures and refinement in primitive implementations. Existing deep learning frameworks heavily rely on these highly engineered libraries such as MKL-DNN <ref type=""bibr"" target=""#b15"">[16]</ref> on CPUs and cuDNN <ref type=""bibr"" target=""#b16"">[17]</ref> on GPUs. Although these libraries are usually ve",0
"mlns=""http://www.tei-c.org/ns/1.0""><head n=""1"">Introduction</head><p>Convolutional neural network (CNN) models <ref type=""bibr"" target=""#b0"">[1]</ref><ref type=""bibr"" target=""#b1"">[2]</ref><ref type=""bibr"" target=""#b2"">[3]</ref><ref type=""bibr"" target=""#b3"">[4]</ref> usually have high computational hat will participate in the crossover. In this case, the cumulative probability P (a i ) (1 ≤ i ≤ m) of the i-th individual is calculated by Equation <ref type=""bibr"" target=""#b1"">(2)</ref>.</p><formula xml:id=""formula_1"">P (a i ) = i j=1 p(a j )<label>(2)</label></formula><p>Based on this equation,",0
"imization framework that leverages multifaceted optimizations based on local and global graph optimization, automated searches with genetic algorithm <ref type=""bibr"" target=""#b24"">[25]</ref> and reinforcement learning(RL) <ref type=""bibr"" target=""#b25"">[26]</ref>, automatic high-quality code genera",0
"ent overhead between operator calls, making operator fusion ineffective at all. Taking GPU as an example, one effective approach is to write one CUDA <ref type=""bibr"" target=""#b22"">[23]</ref> kernel function for the fused operator and complete the whole computation within only one kernel launch to e GPUs, WPK naturally supports these architectures as well. Nonetheless, our paper will merely investigate optimization techniques on CUDA-enabled GPUs <ref type=""bibr"" target=""#b22"">[23]</ref>.</p><p>Halide compiler Halide is a DSL compiler based on the concept of functional programming. A Halide pro",0
"target model acoustic robustness, TL is also crucial for training large and complex deep learning architectures. RNN-T models are difficult to train <ref type=""bibr"" target=""#b10"">[11]</ref> and also require significantly large amount of data to jointly train the acoustic as well as language model . Initializing the encoder with connectionist temporal classification (CTC) model <ref type=""bibr"" target=""#b1"">[2]</ref> or cross entropy (CE) model <ref type=""bibr"" target=""#b10"">[11]</ref>, and the prediction network with LSTM language model (LM) is proven to be beneficial <ref type=""bibr"" target ef type=""figure"">2</ref>: en-US RNN-T initialization.</p><p>model for encoder and prediction network in the context of TL the RNN-T model. Authors in <ref type=""bibr"" target=""#b10"">[11]</ref> have shown that CE initialized RNN-T models perform better than CTC initialized models, and hence, we only e ndomly initialized layers are represented with plain blocks. gets (necessary for CE training), is obtained from word level alignments as discussed in <ref type=""bibr"" target=""#b10"">[11]</ref>. From the word alignments, the start frame, end frame and total number of frames corresponding to each word l. The word piece targets for en-US model is obtained by using byte pair encoding <ref type=""bibr"" target=""#b25"">[26]</ref> algorithm as described in <ref type=""bibr"" target=""#b10"">[11]</ref>.</p><p>We also report the word error rate (WER) on hybrid ASR model trained with same amount of data. The AM",1
"f type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b7"">8,</re",0
"prior work</head><p>Several methods have been proposed to improve the performance of low-resource ASR models <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b15""",0
"ve to conventional hybrid ASR systems. They replace the acoustic model (AM), language model (LM) and pronunciation model with a single neural network <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b3"">4,</r bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b9"">10]</ref>. Recurrent neural network transducer (RNN-T) <ref type=""bibr"" target=""#b0"">[1]</ref> is one such E2E system that allow streaming input and is suitable for real-time ASR applications. Therefore th Section 7.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3."">RNN Transducer model</head><p>The RNN-T model was proposed by Alex Graves <ref type=""bibr"" target=""#b0"">[1]</ref>. The RNN-T model architecture has three components; an encoder, prediction network and joint network as shown argets, p(y|t, u) is obtained after softmax operation on the output of joint network. The whole network is trained jointly to minimize the RNN-T loss <ref type=""bibr"" target=""#b0"">[1]</ref>. In our implementation, the encoder consists of 6 long short-term memory (LSTM) <ref type=""bibr"" target=""#b23""",0
"ef type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b9"">10]</ref>. Recurrent neural network transduc",0
"rning <ref type=""bibr"" target=""#b26"">[27]</ref>, etc. There are also studies that leverage other architectures <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b23"">24]</ref> for sequential recommendation. However, these approaches neglect the em sequence. It is beneficial to incorporate context from both directions for sequence representation learning <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b22"">23]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.2.3"">Prediction Layer.</head><p>In the final l ults on HR@{1, 5, 10}, NGCG@{5, 10}, and MRR. Following previous works <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b22"">23]</ref>, we apply the leave-oneout strategy for evaluation. Concretely, for each user interaction sequence, the last a self-attention based sequential recommendation model, which uses the multi-head attention mechanism to recommend the next item.</p><p>(7) BERT4Rec <ref type=""bibr"" target=""#b22"">[23]</ref> uses a Cloze objective loss for sequential recommendation by the bidirectional self-attention mechanism.</p>",1
"information (such as item attributes) to neural sequential recommenders <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b28"">29]</ref>. It has been demonstrated that contextual information is important to consider for improving the performance se attribute with items or sequential contexts, which is able to achieve the same effect as previous methods <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b28"">29]</ref>. Besides, the pre-trained data representations can be also applied to improve existing methods.  <ref type=""t we group the interaction records by users and sort them by the interaction timestamps ascendingly. Following <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b28"">29]</ref>, we only keep the 5-core datasets, and filter unpopular items and inactive users with fewer than five interac (NDCG@k), and Mean Reciprocal Rank (MRR) to evaluate the performance, which are widely used in related works <ref type=""bibr"" target=""#b20"">[21,</ref><ref type=""bibr"" target=""#b28"">29]</ref>. Since HR@1 is equal to NDCG@1, we report results on HR@{1, 5, 10}, NGCG@{5, 10}, and MRR. Following previous =""bibr"" target=""#b15"">[16]</ref> utilized Factorization Machines to incorporate arbitrary real-valued features to the sequential recommendation. FDSA <ref type=""bibr"" target=""#b28"">[29]</ref> employed a feature-level self-attention block to leverage the attribute information about items in user hist lize bidirectional sequential information.</p><p>Attribute-aware sequential models such as TransFM <ref type=""bibr"" target=""#b15"">[16]</ref> and FDSA <ref type=""bibr"" target=""#b28"">[29]</ref> leverage the contextual features to improve the sequential recommender models, in which these features are t ec, which concatenates the representations of item and attribute as the input to the model. ( <ref type=""formula"" target=""#formula_16"">11</ref>) FDSA <ref type=""bibr"" target=""#b28"">[29]</ref> constructs a feature sequence and uses a featurelevel self-attention block to model the feature transition p",1
"ation maximization (MIM) method <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b29"">30]</ref>. It has been shown to be particularly effective to capture the correlation between different views (or parts)",0
"lower bound on I (X , Y ). One particular lower bound that has been shown to work well in practice is InfoNCE <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b24"">25]</ref>, which is based on Noise Contrastive Estimation (NCE) <ref type=""bibr",0
"fusion between context data and sequence data has not been well captured in data representations. As shown in increasing evidence from various fields <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b9"">10]</ref>, effective data representation (e .</p><p>To address the above issues, we borrow the idea of self-supervised learning for improving sequential recommendation. Self-supervised learning <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b14"">15]</ref> is a newly emerging paradigm, which aims to let the model learn from t and modeled sufficiently.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2.2"">Self-supervised Learning</head><p>Self-supervised learning <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b14"">15]</ref> aims at training a network on an n-visual but intrinsically correlated features to guide the visual feature learning <ref type=""bibr"" target=""#b4"">[5]</ref>. As for language modeling <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b14"">15]</ref>, it is a popular self-supervised objective for natural language proces re representation of each item in an item sequence. It is beneficial to incorporate context from both directions for sequence representation learning <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b22"">23]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.2.3"">Pr us sequences. The learned representations of words or sequences can improve the performance of downstream tasks such as machine reading comprehension <ref type=""bibr"" target=""#b0"">[1]</ref> and natural language understanding <ref type=""bibr"" target=""#b9"">[10]</ref>.</p><p>Mutual information maximiza the multi-view correlation, we unify these self-supervised learning objectives with the recently proposed pretraining framework in language modeling <ref type=""bibr"" target=""#b0"">[1]</ref>.</p><p>The overview of S 3 -Rec is presented in Fig. <ref type=""figure"" target=""#fig_0"">1</ref>. In the follow we apply the mask operation for the output of the multi-head self-attention function to remove all connections between Q i and K i . Inspired by BERT <ref type=""bibr"" target=""#b0"">[1]</ref>, at the pre-training stage, we remove the mask mechanism to acquire the bidirectional context-aware representa s noted that the entire interaction sequence is indeed observed by the model in the training process. Inspired by the masked language model like BERT <ref type=""bibr"" target=""#b0"">[1]</ref>, we propose to model the bidirectional information in item sequence by a Cloze task. For our task, the Cloze s",0
"hers have incorporated rich contextual information (such as item attributes) to neural sequential recommenders <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b28"">29]</ref>. It has been demonstrated that contextual information is important to c nd a surge of following variants modified this model by introducing pair-wise loss functions <ref type=""bibr"" target=""#b3"">[4]</ref>, memory networks <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b6"">7]</ref>, hierarchical structures <ref type=""bibr"" target=""#b16"">[17]</ref>, copy",0
"""bibr"" target=""#b3"">[4]</ref>, memory networks <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b6"">7]</ref>, hierarchical structures <ref type=""bibr"" target=""#b16"">[17]</ref>, copy mechanism <ref type=""bibr"" target=""#b17"">[18]</ref> and reinforcement learning <ref type=""bibr"" target",0
"4]</ref> for sequential recommendation. However, these approaches neglect the rich attribute information about items. To tackle this problem, TransFM <ref type=""bibr"" target=""#b15"">[16]</ref> utilized Factorization Machines to incorporate arbitrary real-valued features to the sequential recommendati target=""#b20"">21]</ref> except that it can also utilize bidirectional sequential information.</p><p>Attribute-aware sequential models such as TransFM <ref type=""bibr"" target=""#b15"">[16]</ref> and FDSA <ref type=""bibr"" target=""#b28"">[29]</ref> leverage the contextual features to improve the sequentia the L AAP loss and L M AP loss aim to fuse attribute with items or sequential contexts, which is able to achieve the same effect as previous methods <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b28"">29]</ref>. Besides, the pre-trained data representations can be also applied t",0
"e data has not been well captured in data representations. As shown in increasing evidence from various fields <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b9"">10]</ref>, effective data representation (e.g., pre-trained contextualized embeddi v xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2.2"">Self-supervised Learning</head><p>Self-supervised learning <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b14"">15]</ref> aims at training a network on an auxiliary objective where the ground-t ach to characterizing such data correlations. For this problem, we are inspired by the recently proposed mutual information maximization (MIM) method <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b29"">3 type=""bibr"" target=""#b0"">[1]</ref> and natural language understanding <ref type=""bibr"" target=""#b9"">[10]</ref>.</p><p>Mutual information maximization <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b10"">11]</ref> is a special branch of the self ainly emphasize the effect of sequential characteristics using an item-level optimization objective alone.</p><p>Inspired by recent progress with MIM <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b27"">28]</ref>, we take a different perspective to develop neural sequential recommen Several self-supervised objectives have been introduced to use non-visual but intrinsically correlated features to guide the visual feature learning <ref type=""bibr"" target=""#b4"">[5]</ref>. As for language modeling <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b14"">15]</ref>, it nspired by the InfoMax principle <ref type=""bibr"" target=""#b10"">[11]</ref> and has made important progress in several domains such as computer vision <ref type=""bibr"" target=""#b4"">[5]</ref>, audio processing <ref type=""bibr"" target=""#b24"">[25]</ref>, and nature language understanding <ref type=""bibr its context <ref type=""bibr"" target=""#b9"">[10]</ref> or a dot product between encoded representations of an image and the local regions of the image <ref type=""bibr"" target=""#b4"">[5]</ref>), and Ỹ is a set of samples drawn from a proposal distribution q( Ỹ ), which contains a positive sample y and",0
"bibr"" target=""#b22"">[23]</ref> uses a Cloze objective loss for sequential recommendation by the bidirectional self-attention mechanism.</p><p>(8) HGN <ref type=""bibr"" target=""#b12"">[13]</ref> is recently proposed and adopts hierarchical gating networks to capture long-term and short-term user intere",0
"et=""#b35"">36]</ref>. Despite their excellent performance, most of them ignore the interactions' timestamp values. While recent works such as TiSASRec <ref type=""bibr"" target=""#b13"">[14]</ref> successfully incorporated time information, their usage of time was also limited to a single embedding schem et=""#b18"">[19]</ref> improved SASRec by adopting Transformer <ref type=""bibr"" target=""#b21"">[22]</ref> and Cloze-task based training method. TiSASRec <ref type=""bibr"" target=""#b13"">[14]</ref> enhanced SASRec by merging timestamp information into self-attention operations. Our work seeks to further e r overlooked in these models for many reasons. First, they don't utilize timestamp values which hold important contextual information. While TiSASRec <ref type=""bibr"" target=""#b13"">[14]</ref> successfully addressed this issue, they also used a simple embedding scheme for temporal values. This can po ith the state-of-the-art baselines: MARank <ref type=""bibr"" target=""#b31"">[32]</ref>,</p><p>SASRec <ref type=""bibr"" target=""#b9"">[10]</ref>, TiSASRec <ref type=""bibr"" target=""#b13"">[14]</ref>, and BERT4Rec <ref type=""bibr"" target=""#b18"">[19]</ref>. These models are evaluated based on the code provid b15"">[16]</ref>. We follow the common data preprocessing procedure from <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b18"">19]</ref>. We convert each dataset into an implicit dataset by treating each nu estamps to form a sequence for each user. Following the custom practice <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b18"">19]</ref>, we discard users and items with less than five interactions to ensur or test, and the second last item for validation. We use the rest for training. We follow the common practice <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b18"">19]</ref> of letting the model rank the ground truth item together with 100 ran",1
"ndation. Since the first suggestion by GRU4Rec <ref type=""bibr"" target=""#b8"">[9]</ref>, many RNN-based methods <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b30""",0
"target=""#b9"">[10]</ref> successfully employed self-attention mechanism, which was a huge success in NLP areas <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b28"">29]</ref>. BERT4Rec <ref type=""bibr"" targ length considering the trade-off between matrix size and the effectiveness. Pos-embedding is analogous to the learnable positional embedding used in <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b18"">19]</ref>, where each position has a corresponding embedding in M P ∈ R N ×h . W recommendation tasks <ref type=""bibr"" target=""#b18"">[19]</ref>. MEANTIME is also based on this structure. Traditionally, self-attention based models <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b18"">19</ref>] employ a simple absolute positi aset.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.3.2"">Stacking</head><p>Layers. After self-attention, MEANTIME operates similar to <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b18"">19]</ref>. We apply Position-wise Feed Forward Network (FFN) to the result of se",0
"0,</ref><ref type=""bibr"" target=""#b32"">33]</ref> were proposed. Some works adopted graph neural network (GNN) to understand user's session as a graph <ref type=""bibr"" target=""#b25"">[26]</ref>.</p><p>As for attention mechanisms, NARM <ref type=""bibr"" target=""#b12"">[13]</ref> and STAMP <ref type=""bibr",0
"</ref>, many RNN-based methods <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b30"">31,</ref><ref type=""bibr"" target=""#b33"">34,</ref><ref type=""bibr"" target=""#b34""",0
", theoretical studies prove the limitations and vulnerabilities of GNNs when graphs have noisy nodes and edges <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b32"">33]</ref>. Therefore, failing to tackle t </ref>. Therefore, failing to tackle the camouflaged fraudsters would sabotage the performance of GNN-based fraud detectors. Though some recent works <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b24"">25 type=""bibr"" target=""#b19"">20]</ref>. Those works learn new graph structures from original graphs, which could Another approach is the metric learning <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b12"">13]</ref>. Those works devise new metrics to measure the similarity between conn graphs (see Table <ref type=""table"" target=""#tab_3"">2</ref>) and d is the feature dimension.</p><p>Label-aware Similarity Measure. Inspired by LAGCN <ref type=""bibr"" target=""#b3"">[4]</ref> which uses a Multi-layer Perceptron (MLP) as the edge label predictor, we employ a one-layer MLP as the node l work). To save the computational cost, we only take the embedding of the node itself as the input instead of using combined embeddings like the LAGCN <ref type=""bibr"" target=""#b3"">[4]</ref>. Therefore, taking the S Optimization. To train the similarity measure together with GNNs, a heuristic approac . Those works devise new metrics to measure the similarity between connect nodes and aggregate neighbors according to the metrics. Among those works, <ref type=""bibr"" target=""#b3"">[4]</ref> proposes a neural network to predict the labels of neighboring nodes. <ref type=""bibr"" target=""#b12"">[13]</ref",1
"arameters cannot be well-updated through the back-propagation process. To train the similar measure with a direct supervised signal from labels, like <ref type=""bibr"" target=""#b34"">[35]</ref>, we define the cross-entropy loss of the MLP at l-layer as:</p><formula xml:id=""formula_6"">L (l ) Simi = v ∈",1
"tween the neighbor selector and the GNN with the similarity measure. A is the action space, f is the reward function, and T is the terminal condition <ref type=""bibr"" target=""#b35"">[36]</ref>. Given an initial p (l ) r , the neighbor selector choose to increase or decrease p (l ) r as actions and th",1
"ttention from both researchers <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b48"">49]</ref> and practitioners <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""b ypass feature-based detectors <ref type=""bibr"" target=""#b40"">[41]</ref>. Relation camouflage: previous works <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b48"">49]</ref> show that crowd workers are actively committing opinion fraud on online social networks. They can probe the g",0
"udsters, which have been drawing great attention from both researchers <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b48"">49]</ref> and practitioners <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""b acters to a fake review, which helps to bypass feature-based detectors <ref type=""bibr"" target=""#b40"">[41]</ref>. Relation camouflage: previous works <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b48"">49]</ref> show that crowd workers are actively committing opinion fraud on onl",0
"ic <ref type=""bibr"" target=""#b6"">[7,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b37"">38]</ref> and industrial communities <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b49"">50]</ref>. Graphbased methods connect en "">[8,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b48"">49]</ref> and practitioners <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b40"">41]</ref>. Meanwhile, theoretical studie",0
"=""bibr"" target=""#b11"">[12]</ref>), many GNN-based fraud detectors have been proposed to detect opinion fraud <ref type=""bibr"" target=""#b18"">[19,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b38"">39]</ref>, financial fraud <ref type=""bibr"" target=""#b22"">[23,</ref><ref type="" ectors. Though some recent works <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b40"">41]</ref> have noticed similar challenges, their solutions either fail to fit t intriguing reviews as node features, it will probably smooth out the suspiciousness of the center fraudster <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b24"">25]</ref>. Similarly, if we aggregate all neighbors under Relation II, where there are more dissimilar neighbors, it wi mlns=""http://www.tei-c.org/ns/1.0""><head>3.3.2</head><p>Finding the Optimal Thresholds with RL. Previous works <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b24"">25]</ref> set the filtering threshold as a hyperparameter and tune it with validation to find the optimal value. Howeve benign and 1 represents suspicious. The relations R are rules, interactions, or shared attributes between nodes, e.g., two reviews from the same user <ref type=""bibr"" target=""#b24"">[25]</ref> or transactions from the same devices <ref type=""bibr"" target=""#b23"">[24]</ref>. The graph-based fraud detec needed to filter the camouflaged neighbors before applying GNNs. Previous works have proposed unsupervised similarity metrics like Cosine Similarity <ref type=""bibr"" target=""#b24"">[25]</ref> or Neural Networks <ref type=""bibr"" target=""#b44"">[45]</ref>. However, many fraud problems like financial fr r"" target=""#b22"">[23]</ref>, Player2Vec <ref type=""bibr"" target=""#b47"">[48]</ref>, SemiGNN <ref type=""bibr"" target=""#b36"">[37]</ref>, and GraphConsis <ref type=""bibr"" target=""#b24"">[25]</ref> as four state-of-the-art GNN-based fraud detectors. Their detailed introduction can be found in Section 5. W > <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.2"">Camouflage Evidence</head><p>We analyze fraudster camouflage using two metrics introduced in <ref type=""bibr"" target=""#b24"">[25]</ref>. For the feature camouflage, we compute the feature similarity of neighboring nodes based on their feature v etectors transfer the heterogeneous data into homogeneous data before applying GNNs. Fdgars <ref type=""bibr"" target=""#b38"">[39]</ref> and GraphConsis <ref type=""bibr"" target=""#b24"">[25]</ref> construct a single homo-graph based on multiple relations and employ GNNs to aggregate neighborhood informat ef> have noticed the camouflage behaviors of fraudsters. While <ref type=""bibr"" target=""#b40"">[41]</ref> only crafts new but inflexible features, and <ref type=""bibr"" target=""#b24"">[25]</ref> suffers from unsupervised similarity measures and fixed filtering thresholds. CARE-GNN remedies those shortc tively.</p><p>In this paper, CARE-GNN constructs multiple homo-graphs with only one node type like GEM and ASA. Among the above works, only two works <ref type=""bibr"" target=""#b24"">[25,</ref><ref type=""bibr"" target=""#b40"">41]</ref> have noticed the camouflage behaviors of fraudsters. While <ref type",0
"nion fraud <ref type=""bibr"" target=""#b18"">[19,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b38"">39]</ref>, financial fraud <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b36"">37]</ref>, mobile fraud <ref type=""bib review on the review website <ref type=""bibr"" target=""#b18"">[19,</ref><ref type=""bibr"" target=""#b28"">29]</ref> or a transaction in the trading system <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b36"">37]</ref>. The node has a label y v ∈ {0, 1} ∈ Y where 0 represents benign and bors under each relation, the next step is to aggregate the neighbor information from different relations. Previous methods adopt attention mechanism <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b36"">37,</ref><ref type=""bibr"" target=""#b47"">48]</ref> or devise weighting paramete f type=""bibr"" target=""#b30"">[31]</ref>, and GraphSAGE <ref type=""bibr"" target=""#b11"">[12]</ref> to represent general GNN models. We choose Ge-niePath <ref type=""bibr"" target=""#b22"">[23]</ref>, Player2Vec <ref type=""bibr"" target=""#b47"">[48]</ref>, SemiGNN <ref type=""bibr"" target=""#b36"">[37]</ref>, an "" target=""#b24"">[25]</ref> construct a single homo-graph based on multiple relations and employ GNNs to aggregate neighborhood information. GeniePath <ref type=""bibr"" target=""#b22"">[23]</ref> learns convolutional layers and neighbor weights using LSTM and the attention mechanism <ref type=""bibr"" tar",0
"To improve the training efficiency and avoid overfitting, we employ mini-batch training <ref type=""bibr"" target=""#b10"">[11]</ref> and under-sampling <ref type=""bibr"" target=""#b21"">[22]</ref> techniques to train CARE-GNN and other baselines. Specifically, under each mini-batch, we randomly sample th",0
"end to connect with each other <ref type=""bibr"" target=""#b0"">[1]</ref>.</p><p>Recently, as the development of Graph Neural Networks (GNNs) (e.g., GCN <ref type=""bibr"" target=""#b16"">[17]</ref>, GAT <ref type=""bibr"" target=""#b33"">[34]</ref>, and GraphSAGE <ref type=""bibr"" target=""#b11"">[12]</ref>), ma ative influence induced by camouflaged fraudsters, we compare it with various GNN baselines under the semi-supervised learning setting. We select GCN <ref type=""bibr"" target=""#b16"">[17]</ref>, GAT <ref type=""bibr"" target=""#b33"">[34]</ref>, RGCN <ref type=""bibr"" target=""#b30"">[31]</ref>, and GraphSAG most popular deep learning framework on graph data, GNNs have two major types <ref type=""bibr"" target=""#b41"">[42]</ref>: 1) Spectral-based GNNs (GCN <ref type=""bibr"" target=""#b16"">[17]</ref>, AGCN <ref type=""bibr"" target=""#b19"">[20]</ref>): they turn a graph into a Laplacian matrix and make convolu n problem on the graph. Graph-based fraud detectors are trained based on the labeled node information along  <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b33"">34]</ref>. Based on the defined multi-relation graph in Definition 2.2, we unif",0
"ignoring the camouflage behaviors of fraudsters, which have been drawing great attention from both researchers <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b48"">49]</ref> and practitioners <ref type="" <ref type=""bibr"" target=""#b18"">[19,</ref><ref type=""bibr"" target=""#b40"">41]</ref> (so-called spamouflage), or employ deep language generation models <ref type=""bibr"" target=""#b14"">[15]</ref> to gloss over explicit suspicious outcomes. Like Figure <ref type=""figure"" target=""#fig_0"">1</ref> shows, a ample, in opinion fraud, unsupervised similarity measures could not identify the camouflaged fake reviews, which are even indistinguishable by humans <ref type=""bibr"" target=""#b14"">[15]</ref>. Therefore, we need a parameterized similarity measure to compute node similarity with supervised signals fr troduced various fraudster camouflage types from behavior <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b9"">10]</ref> and semantic <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b40"">41]</ref> perspectives. Those camouflages could make the features of fraudster",0
"""#b48"">49]</ref> show that crowd workers are actively committing opinion fraud on online social networks. They can probe the graphs used by defenders <ref type=""bibr"" target=""#b42"">[43]</ref> and adjust their behavior to alleviate the suspiciousness <ref type=""bibr"" target=""#b43"">[44]</ref>. Specifi",0
"http://www.tei-c.org/ns/1.0""><head n=""1"">INTRODUCTION</head><p>As Internet services thrive, they also incubate various kinds of fraudulent activities <ref type=""bibr"" target=""#b13"">[14]</ref>. Fraudsters disguise as regular users to bypass the anti-fraud system and disperse disinformation <ref type=",0
"NN are optimized during training GNN which retains the end-to-end learning fashion.</p><p>GNN sampling methods <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b45"">46,</ref><ref type=""bibr"" target=""#b50"">51]</ref> also filter the neighbors. While these works only consider selecting",0
""">25,</ref><ref type=""bibr"" target=""#b38"">39]</ref>, financial fraud <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b36"">37]</ref>, mobile fraud <ref type=""bibr"" target=""#b40"">[41]</ref>, and cyber criminal <ref type=""bibr"" target=""#b47"">[4 ibr"" target=""#b18"">[19,</ref><ref type=""bibr"" target=""#b28"">29]</ref> or a transaction in the trading system <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b36"">37]</ref>. The node has a label y v ∈ {0, 1} ∈ Y where 0 represents benign and 1 represents suspicious. The relations R s to aggregate the neighbor information from different relations. Previous methods adopt attention mechanism <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b36"">37,</ref><ref type=""bibr"" target=""#b47"">48]</ref> or devise weighting parameters <ref type=""bibr"" target=""#b23"">[24]</r ent general GNN models. We choose Ge-niePath <ref type=""bibr"" target=""#b22"">[23]</ref>, Player2Vec <ref type=""bibr"" target=""#b47"">[48]</ref>, SemiGNN <ref type=""bibr"" target=""#b36"">[37]</ref>, and GraphConsis <ref type=""bibr"" target=""#b24"">[25]</ref> as four state-of-the-art GNN-based fraud detector hbor weights using LSTM and the attention mechanism <ref type=""bibr"" target=""#b33"">[34]</ref>. GEM <ref type=""bibr"" target=""#b23"">[24]</ref>, SemiGNN <ref type=""bibr"" target=""#b36"">[37]</ref>, ASA <ref type=""bibr"" target=""#b40"">[41]</ref>, and Player2Vec <ref type=""bibr"" target=""#b47"">[48]</ref> all",0
"ef type=""bibr"" target=""#b26"">[27]</ref> and PSNR, were developed to evaluate overall image quality and not fine-grained lip-sync errors. Although LMD <ref type=""bibr"" target=""#b3"">[4]</ref> focuses on the lip region, we found that lip landmarks can be quite inaccurate on generated faces. Thus, there",1
"itation of existing works is in terms of the vocabulary. Several works <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b27"">28]</ref> train on datasets with a limited set of words such as GRID <ref type=""bibr"" target=""#b9"">[10]</ref> (56 words",0
"We train our expert lip-sync discriminator on the LRS2 train split (≈ 29 hours) with a batch size of 64, with T v = 5 frames using the Adam optimizer <ref type=""bibr"" target=""#b11"">[12]</ref> with an initial learning rate of 1e −3 . Our expert lip-sync discriminator is about 91% accurate on the LRS2 tors.</p><p>We train our model only on the LRS2 train set <ref type=""bibr"" target=""#b0"">[1]</ref>, with a batch size of 80. We use the Adam optimizer <ref type=""bibr"" target=""#b11"">[12]</ref> with an initial learning rate of 1e −4 and betas β 1 = 0.5, β 2 = 0.999 for both the generator and visual qu",0
"n the generated frames like it is done in LipGAN. One such network that has been used to correct lip-sync errors for creating large lip-sync datasets <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b2"">3]</ref> is the SyncNet <ref type=""bibr"" target=""#b8"">[9]</ref> model. We propose twork is optimized for both superior sync-accuracy and quality using two disjoint discriminators.</p><p>We train our model only on the LRS2 train set <ref type=""bibr"" target=""#b0"">[1]</ref>, with a batch size of 80. We use the Adam optimizer <ref type=""bibr"" target=""#b11"">[12]</ref> with an initial the lip-sync accuracy of the videos generated using Wav2Lip is almost as good as real synced videos. Note that we only train on the train set on LRS2 <ref type=""bibr"" target=""#b0"">[1]</ref>, but we comfortably generalize across all datasets without any further fine-tuning. We also report the FID sco a pseudo-randomly chosen audio as a consistent test set. We create three consistent benchmarks test sets, one each using the test set videos of LRS2 <ref type=""bibr"" target=""#b0"">[1]</ref>, LRW <ref type=""bibr"" target=""#b7"">[8]</ref>, and LRS3 <ref type=""bibr"" target=""#b2"">[3]</ref> respectively. F",0
"ef type=""bibr"" target=""#b17"">18]</ref> exist in the current literature. Note that <ref type=""bibr"" target=""#b16"">[17]</ref> is an extended version of <ref type=""bibr"" target=""#b6"">[7]</ref>. Both these works <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b17"">18]</ref> formulate",0
"2"">[3]</ref>, <ref type=""bibr"" target=""#b4"">[5]</ref>, the number of authors (i.e., cluster size) is usually a pre-specified parameter. Current works <ref type=""bibr"" target=""#b5"">[6]</ref>, <ref type=""bibr"" target=""#b6"">[7]</ref> did not efficiently handle the change of discriminative attributes an disambiguation methods can be divided into supervised <ref type=""bibr"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b7"">[8]</ref>, unsupervised <ref type=""bibr"" target=""#b5"">[6]</ref>, <ref type=""bibr"" target=""#b8"">[9]</ref> and graph-based ones <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref ty are applied to estimate the probability of a pair of author mentions belonging to the same author and are essential in the name disambiguation task. <ref type=""bibr"" target=""#b5"">[6]</ref> first learns representation for every name mention in a pairwise or tripletwise way and refines the representa . We use the author's affiliation as the discriminative attribute to separate papers into small blocks and we use the same trainset and testset as in <ref type=""bibr"" target=""#b5"">[6]</ref>.</p><p>In Semantic Scholar, the selected meta-paths of our method consist of Paper-Paper, Paper-Author-Paper, tive author attributes; 2 Arrange papers in every block as sequence s ∈ S; 3 Construct meta-path based view {G p1</figDesc><table /><note>• Aminer-AND<ref type=""bibr"" target=""#b5"">[6]</ref>: This dataset contains 70,285 records of 12,798 unique authors with 100 ambiguous name references.Algorithm 1:",1
"4"">[5]</ref>, the number of authors (i.e., cluster size) is usually a pre-specified parameter. Current works <ref type=""bibr"" target=""#b5"">[6]</ref>, <ref type=""bibr"" target=""#b6"">[7]</ref> did not efficiently handle the change of discriminative attributes and inductive paper embedding problem in th r tripletwise way and refines the representation by a graph auto-encoder, but this method neglects linkage between paper and author and coauthorship. <ref type=""bibr"" target=""#b6"">[7]</ref> addresses the pairwise classification problem by extracting both structure-aware features and global features",1
"ame.</p><p>In existing clustering based name disambiguation methods <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b2"">[3]</ref>, <ref type=""bibr"" target=""#b4"">[5]</ref>, the number of authors (i.e., cluster size) is usually a pre-specified parameter. Current works <ref type=""bib rvised <ref type=""bibr"" target=""#b5"">[6]</ref>, <ref type=""bibr"" target=""#b8"">[9]</ref> and graph-based ones <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b4"">[5]</ref>. Graph-based works exploit graph topological features in the academic network to enhance the representation of o enhance the representation of papers. For instance, GHOST <ref type=""bibr"" target=""#b1"">[2]</ref> constructs document graph based on co-authorship. <ref type=""bibr"" target=""#b4"">[5]</ref> leverages only relational data in the form of anonymized graphs to preserve author privacy. Pairwise classific",0
"b9"">[10]</ref> has attracted rising attention due to effective representation ability. While most GNN works <ref type=""bibr"" target=""#b9"">[10]</ref>- <ref type=""bibr"" target=""#b11"">[12]</ref> focus on transductive setting, there have been some recent works adopting an inductive learning setting. Dee",0
"</profileDesc> 	</teiHeader> 	<text xml:lang=""en""> 		<body> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>I. INTRODUCTION</head><p>Namesake problem <ref type=""bibr"" target=""#b0"">[1]</ref> poses a huge challenge on many applications, e.g., information retrieval, bibliographic data analysis. When se .</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>A. Name Disambiguation</head><p>Name disambiguation methods can be divided into supervised <ref type=""bibr"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b7"">[8]</ref>, unsupervised <ref type=""bibr"" target=""#b5"">[6]</ref>, <ref type=""bib",0
"5]</ref> aggregates neighbors with both rule-based and network-based attention weights for knowledge graphs.</p><p>Heterogeneous information networks <ref type=""bibr"" target=""#b15"">[16]</ref>- <ref type=""bibr"" target=""#b18"">[19]</ref> have been studied in recent years. Meta-path is designed to prese",0
"2020 are associated with jpei@cs.sfu.ca and Simon Fraser University. The change of discriminative attributes may lead to the paper separation problem <ref type=""bibr"" target=""#b3"">[4]</ref>, i.e., papers of an author are regarded as belonging to different authors, which commonly occurs in digital li",0
"type=""bibr"" target=""#b11"">[12]</ref> focus on transductive setting, there have been some recent works adopting an inductive learning setting. DeepGL <ref type=""bibr"" target=""#b12"">[13]</ref> aggregates a set of base graph features by relational functions that can generalize across networks. GraphSa",0
"to supervised <ref type=""bibr"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b7"">[8]</ref>, unsupervised <ref type=""bibr"" target=""#b5"">[6]</ref>, <ref type=""bibr"" target=""#b8"">[9]</ref> and graph-based ones <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b4"">[5]</ref>. Graph-b",0
"rget=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b2"">[3]</ref> to automatically designed neural architectures <ref type=""bibr"" target=""#b3"">[4]</ref>, <ref type=""bibr"" target=""#b4"">[5]</ref>, <ref type=""bibr"" target=""#b5"">[6]</ref>, <ref type=""bibr"" target=""#b owever, manually designing one architecture requires human experts to frequently try and evaluate numerous different operation and connection options <ref type=""bibr"" target=""#b3"">[4]</ref>. In contrast to architectures that are manually designed, those automatically found by neural architecture sea h less human interaction and expert effort. These NAS-generated architectures have shown promising results in many domains, such as image recognition <ref type=""bibr"" target=""#b3"">[4]</ref>, <ref type=""bibr"" target=""#b4"">[5]</ref>, <ref type=""bibr"" target=""#b5"">[6]</ref> and sequence modeling <ref t candidates for the number of channels, which generates a total search space of itive training procedure of each selected architecture can be avoided <ref type=""bibr"" target=""#b3"">[4]</ref>, <ref type=""bibr"" target=""#b15"">[16]</ref> so that researchers can target on the essence of NAS, i.e., search",1
"ch spaces and search algorithms have been proposed. They brought great advancements in many applications of neural network, such as visual perception <ref type=""bibr"" target=""#b21"">[22]</ref>, <ref type=""bibr"" target=""#b22"">[23]</ref>, <ref type=""bibr"" target=""#b23"">[24]</ref>, language modelling <r",1
") <ref type=""bibr"" target=""#b26"">[27]</ref>. (II) ES methods, e.g., REA <ref type=""bibr"" target=""#b5"">[6]</ref>. (III) RL algorithms, e.g., REINFORCE <ref type=""bibr"" target=""#b47"">[48]</ref>, ENAS <ref type=""bibr"" target=""#b4"">[5]</ref>. (IV) Differentiable algorithms. e.g., first order DARTS (DART",0
"ref type=""bibr"" target=""#b10"">[11]</ref>, different scheduler <ref type=""bibr"" target=""#b14"">[15]</ref>, and different selections of hyper-parameters <ref type=""bibr"" target=""#b15"">[16]</ref>, <ref type=""bibr"" target=""#b16"">[17]</ref>. <ref type=""bibr"" target=""#b2"">(3)</ref> The validation set for t hich generates a total search space of itive training procedure of each selected architecture can be avoided <ref type=""bibr"" target=""#b3"">[4]</ref>, <ref type=""bibr"" target=""#b15"">[16]</ref> so that researchers can target on the essence of NAS, i.e., search algorithm. Another benefit is that the va",0
"ION</head><p>T HE deep learning community is undergoing a transition from hand-designed neural architectures <ref type=""bibr"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b2"">[3]</ref> to automatically designed neural architectures <ref type=""bibr"" targe Our NATS-Bench solves this problem by sacrificing the number of nodes and including all possible edges so that our search space is algorithmagnostic. <ref type=""bibr"" target=""#b1"">(2)</ref> We provide extra diagnostic information, such as architecture computational cost, fine-grained training and ev",0
"f the NAS algorithms <ref type=""bibr"" target=""#b17"">[18]</ref>, <ref type=""bibr"" target=""#b19"">[20]</ref>, <ref type=""bibr"" target=""#b24"">[25]</ref>, <ref type=""bibr"" target=""#b25"">[26]</ref>, <ref type=""bibr"" target=""#b26"">[27]</ref>, <ref type=""bibr"" target=""#b26"">[27]</ref>. It is essentially not",0
"many applications of neural network, such as visual perception <ref type=""bibr"" target=""#b21"">[22]</ref>, <ref type=""bibr"" target=""#b22"">[23]</ref>, <ref type=""bibr"" target=""#b23"">[24]</ref>, language modelling <ref type=""bibr"" target=""#b4"">[5]</ref>, <ref type=""bibr"" target=""#b6"">[7]</ref>, <ref t",0
"roposed. They brought great advancements in many applications of neural network, such as visual perception <ref type=""bibr"" target=""#b21"">[22]</ref>, <ref type=""bibr"" target=""#b22"">[23]</ref>, <ref type=""bibr"" target=""#b23"">[24]</ref>, language modelling <ref type=""bibr"" target=""#b4"">[5]</ref>, <ref",0
"ifferent scheduler <ref type=""bibr"" target=""#b14"">[15]</ref>, and different selections of hyper-parameters <ref type=""bibr"" target=""#b15"">[16]</ref>, <ref type=""bibr"" target=""#b16"">[17]</ref>. <ref type=""bibr"" target=""#b2"">(3)</ref> The validation set for testing the performance of the selected arch rst order DARTS (DARTS-V1) <ref type=""bibr"" target=""#b7"">[8]</ref>, second order DARTS (DARTS-V2), GDAS <ref type=""bibr"" target=""#b6"">[7]</ref>, SETN <ref type=""bibr"" target=""#b16"">[17]</ref>, TAS <ref type=""bibr"" target=""#b20"">[21]</ref>, FBNet-V2 <ref type=""bibr"" target=""#b43"">[44]</ref>, TuNAS <r",0
"""#b12"">[13]</ref>, <ref type=""bibr"" target=""#b13"">[14]</ref>, different regularization <ref type=""bibr"" target=""#b10"">[11]</ref>, different scheduler <ref type=""bibr"" target=""#b14"">[15]</ref>, and different selections of hyper-parameters <ref type=""bibr"" target=""#b15"">[16]</ref>, <ref type=""bibr"" ta literature to set up the hyper-parameters and training strategies <ref type=""bibr"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b10"">[11]</ref>, <ref type=""bibr"" target=""#b14"">[15]</ref>. We train each architecture with the same strategy, which is shown in Table <ref type=""table"" target=""#tab_1 terov momentum SGD, using the cross-entropy loss. We set the weight decay to 0.0005 and decay the learning rate from 0.1 to 0 with a cosine annealing <ref type=""bibr"" target=""#b14"">[15]</ref>. We use the same H 0 on different datasets, except for the data augmentation which is slightly different due",0
"ve greatest potential to leverage cross modality information, they suffer from sensitivity to data alignment, often involve complicated architectures <ref type=""bibr"" target=""#b10"">[11]</ref>, <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" target=""#b12"">[13]</ref>, <ref type=""bibr"" targ ding cropped region in the image. But the 2D image-based proposal generation might fail in some cases that could only be observed from 3D space. MV3D <ref type=""bibr"" target=""#b10"">[11]</ref> and AVOD <ref type=""bibr"" target=""#b11"">[12]</ref> project the raw point cloud into bird's eye view (BEV) to",1
"ality information, they suffer from sensitivity to data alignment, often involve complicated architectures <ref type=""bibr"" target=""#b10"">[11]</ref>, <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" target=""#b12"">[13]</ref>, <ref type=""bibr"" target=""#b13"">[14]</ref>, and typically require based proposal generation might fail in some cases that could only be observed from 3D space. MV3D <ref type=""bibr"" target=""#b10"">[11]</ref> and AVOD <ref type=""bibr"" target=""#b11"">[12]</ref> project the raw point cloud into bird's eye view (BEV) to form a multi-channel BEV image. A deep fusion base",1
"icated architectures <ref type=""bibr"" target=""#b10"">[11]</ref>, <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" target=""#b12"">[13]</ref>, <ref type=""bibr"" target=""#b13"">[14]</ref>, and typically require pixel-level correspondences of sensor data. On the other hand, late fusion systems ar use features from different sensor modalities with better correspondence, MMF <ref type=""bibr"" target=""#b25"">[26]</ref> adopts continuous convolution <ref type=""bibr"" target=""#b13"">[14]</ref> to build dense LiDAR BEV feature maps and do point-wise feature fusion with dense image feature maps. MMF is",1
"hich has been well-studied <ref type=""bibr"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b2"">[3]</ref>, <ref type=""bibr"" target=""#b3"">[4]</ref>, 3D object detection is more challenging with more output parameters needed to specify 3D oriented bounding bo lns=""http://www.tei-c.org/ns/1.0""><head>D. Loss</head><p>We use a cross entropy entropy loss for target classification, modified by the focal loss in <ref type=""bibr"" target=""#b3"">[4]</ref> with parameters α = 0.25 and γ = 2 to address the large class imbalance between targets and background.</p></d",0
"ocus on the latter two categories.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>A. 3D Detection Using 2D Images</head><p>Mousavian et al. <ref type=""bibr"" target=""#b14"">[15]</ref> leverage the geometric constraints between 2D and 3D bounding boxes to recover 3D information. <ref type=""bi",0
"ased object detection enables spatial path planning for object avoidance and navigation. Compared to 2D object detection, which has been well-studied <ref type=""bibr"" target=""#b0"">[1]</ref>, <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b2"">[3]</ref>, <ref type=""bibr"" target=""#b",0
"re output parameters needed to specify 3D oriented bounding boxes around targets. In addition, LiDAR methods <ref type=""bibr"" target=""#b4"">[5]</ref>, <ref type=""bibr"" target=""#b5"">[6]</ref>, <ref type=""bibr"" target=""#b6"">[7]</ref>, <ref type=""bibr"" target=""#b7"">[8]</ref>, <ref type=""bibr"" target=""#b aw point cloud, and 3D CNNs (Convolutonal Neural Networks) are applied to learn voxel features for classification and bounding box regression. SECOND <ref type=""bibr"" target=""#b5"">[6]</ref> is the upgrade version of <ref type=""bibr"" target=""#b4"">[5]</ref>, since raw LiDAR point cloud has very sparse on network only needs to learn from these intersected examples. Because we take the raw predictions before NMS, k and n are large numbers, for SECOND <ref type=""bibr"" target=""#b5"">[6]</ref>, there are 70400 (200 × 176 × 2) predictions in each frame. It would be impractical to do 1 × 1 convolution on N <ref type=""bibr"" target=""#b29"">[30]</ref> and Cascade R-CNN <ref type=""bibr"" target=""#b30"">[31]</ref>. The 3D detectors we incorporated are: SECOND <ref type=""bibr"" target=""#b5"">[6]</ref>, PointPillars <ref type=""bibr"" target=""#b8"">[9]</ref>, PointRCNN <ref type=""bibr"" target=""#b7"">[8]</ref> and P issions, we only show the results evaluated on the official KITTI test server from three fusion combinations of 2D and 3D detectors, which are SECOND <ref type=""bibr"" target=""#b5"">[6]</ref> and Cascade R-CNN <ref type=""bibr"" target=""#b30"">[31]</ref>, written as CLOCs SecCas, PointRCNN <ref type=""bib on results of pedestrian and cyclist on KITTI validation set. The IoU threshold for pedestrian and cyclist is 0.5. Here for 3D detectors, only SECOND <ref type=""bibr"" target=""#b5"">[6]</ref> and PointPillars <ref type=""bibr"" target=""#b8"">[9]</ref> publish their training configurations for class pedes",0
"on; it enables inference at 62 Hz; Compared with one-stage methods discussed above, PointRCNN <ref type=""bibr"" target=""#b7"">[8]</ref>, Fast PointRCNN <ref type=""bibr"" target=""#b19"">[20]</ref> and STD <ref type=""bibr"" target=""#b20"">[21]</ref> applies a two-stage architecture that first generate 3D pr",0
"]</ref> estimate 3D object information by calculating the similarity between 3D objects and CAD models. <ref type=""bibr"" target=""#b17"">[18]</ref> and <ref type=""bibr"" target=""#b18"">[19]</ref> explore using stereo images to generate dense point cloud and conduct object detection using that cloud. The",0
"ect detection is more challenging with more output parameters needed to specify 3D oriented bounding boxes around targets. In addition, LiDAR methods <ref type=""bibr"" target=""#b4"">[5]</ref>, <ref type=""bibr"" target=""#b5"">[6]</ref>, <ref type=""bibr"" target=""#b6"">[7]</ref>, <ref type=""bibr"" target=""#b r, object detection performance at longer distance is still relatively poor. Methods vary by how they encode and learn features from raw point cloud. <ref type=""bibr"" target=""#b4"">[5]</ref> uses voxels to encode the raw point cloud, and 3D CNNs (Convolutonal Neural Networks) are applied to learn vox lied to learn voxel features for classification and bounding box regression. SECOND <ref type=""bibr"" target=""#b5"">[6]</ref> is the upgrade version of <ref type=""bibr"" target=""#b4"">[5]</ref>, since raw LiDAR point cloud has very sparse data structure, it uses sparse 3D CNNs which reduces the inferenc",0
"avian et al. <ref type=""bibr"" target=""#b14"">[15]</ref> leverage the geometric constraints between 2D and 3D bounding boxes to recover 3D information. <ref type=""bibr"" target=""#b15"">[16]</ref>, <ref type=""bibr"" target=""#b16"">[17]</ref> estimate 3D object information by calculating the similarity betw",0
"r"" target=""#b4"">[5]</ref>, <ref type=""bibr"" target=""#b5"">[6]</ref>, <ref type=""bibr"" target=""#b6"">[7]</ref>, <ref type=""bibr"" target=""#b7"">[8]</ref>, <ref type=""bibr"" target=""#b8"">[9]</ref> are hampered by typically lower input data resolution than video which has a large adverse impact on accuracy ref>, since raw LiDAR point cloud has very sparse data structure, it uses sparse 3D CNNs which reduces the inference time significantly. PointPillars <ref type=""bibr"" target=""#b8"">[9]</ref> uses PointNets <ref type=""bibr"" target=""#b6"">[7]</ref> in an encoder that represents point clouds organized in e R-CNN <ref type=""bibr"" target=""#b30"">[31]</ref>. The 3D detectors we incorporated are: SECOND <ref type=""bibr"" target=""#b5"">[6]</ref>, PointPillars <ref type=""bibr"" target=""#b8"">[9]</ref>, PointRCNN <ref type=""bibr"" target=""#b7"">[8]</ref> and PV-RCNN <ref type=""bibr"" target=""#b21"">[22]</ref>. Whil set. The IoU threshold for pedestrian and cyclist is 0.5. Here for 3D detectors, only SECOND <ref type=""bibr"" target=""#b5"">[6]</ref> and PointPillars <ref type=""bibr"" target=""#b8"">[9]</ref> publish their training configurations for class pedestrian and cyclist; for 2D detectors, only MSCNN <ref type",0
"r the evaluation of testing samples, one needs to submit the detection results to KITTI server. For experimental studies, we follow the convention in <ref type=""bibr"" target=""#b27"">[28]</ref> to split the original training samples into 3712 training samples and 3769 validation samples. We compare ou",0
"vity to data alignment, often involve complicated architectures <ref type=""bibr"" target=""#b10"">[11]</ref>, <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" target=""#b12"">[13]</ref>, <ref type=""bibr"" target=""#b13"">[14]</ref>, and typically require pixel-level correspondences of sensor data his section since this is the most common sensor setup for self-driving cars. Frustum PointNet <ref type=""bibr"" target=""#b23"">[24]</ref>, Pointfusion <ref type=""bibr"" target=""#b12"">[13]</ref> and Frustum ConvNet <ref type=""bibr"" target=""#b24"">[25]</ref> are the representatives of 2D driven 3D detect",0
"n</head><p>We focus on camera-LiDAR fusion methods in this section since this is the most common sensor setup for self-driving cars. Frustum PointNet <ref type=""bibr"" target=""#b23"">[24]</ref>, Pointfusion <ref type=""bibr"" target=""#b12"">[13]</ref> and Frustum ConvNet <ref type=""bibr"" target=""#b24"">[2",0
", have recently developed into an exciting and promising application area for deep learning <ref type=""bibr"" target=""#b11"">(Graves et al., 2020;</ref><ref type=""bibr"" target=""#b16"">Ingraham et al., 2019;</ref><ref type=""bibr"" target=""#b24"">Pereira et al., 2016;</ref><ref type=""bibr"" target=""#b29"">To gid shape. Recent state-of-the-art GNNs include GraphQA <ref type=""bibr"" target=""#b1"">(Baldassarre et al., 2020)</ref> on MQA, Structured Transformer <ref type=""bibr"" target=""#b16"">(Ingraham et al., 2019)</ref> on CPD, and ProteinSolver <ref type=""bibr"" target=""#b27"">(Strokach et al., 2020)</ref> on the-art performance on CATH 4.2, representing a substantial improvement both in terms of perplexity and sequence recovery over Structured Transformer <ref type=""bibr"" target=""#b16"">(Ingraham et al., 2019)</ref>, a GNN method which was trained using the same training and validation sets (Table <ref t onal protein design, the network learns a generative model over the space of protein sequences conditioned on the given backbone structure. Following <ref type=""bibr"" target=""#b16"">Ingraham et al. (2019)</ref>, we frame this as an autoregressive task and use a masked encoder-decoder architecture to 2019)</ref>, a GNN method which was trained using the same training and validation sets (Table <ref type=""table"" target=""#tab_3"">3</ref>). Following <ref type=""bibr"" target=""#b16"">Ingraham et al. (2019)</ref>, we report evaluation on short (100 or fewer amino acid residues) and single-chain subsets e <ref type=""table"" target=""#tab_6"">6</ref>.</p><p>Protein design As described in the main text, we use the CATH 4.2 dataset and splits as curated by <ref type=""bibr"" target=""#b16"">Ingraham et al. (2019)</ref> and the TS50 test set as curated by <ref type=""bibr"" target=""#b20"">Li et al. (2014)</ref>. sign of novel structures, the heldout evaluation set should bear minimal similarity to the training structures. We use the CATH 4.2 dataset curated by<ref type=""bibr"" target=""#b16"">Ingraham et al. (2019)</ref> in which all available structures with 40% nonredudancy are partitioned by their CATH (cla",1
"or dense feedforward network. Although these methods only indirectly represent the full 3D structure of the protein, a number of them, such as ProQ4 <ref type=""bibr"" target=""#b15"">(Hurtado et al., 2018)</ref>, VoroMQA <ref type=""bibr"" target=""#b22"">(Olechnovič &amp; Venclovas, 2017)</ref>, and SBRO or statistical potential on top of such structural features. Finally, MULTICOM-NOVEL<ref type=""bibr"" target=""#b14"">(Hou et al., 2019)</ref> and ProQ4<ref type=""bibr"" target=""#b15"">(Hurtado et al., 2018)</ref> employ one-dimensional deep convolutional networks on top of sequential representations. G",0
"mino acids, or flexible structural motifs described as a connectivity pattern rather than a rigid shape. Recent state-of-the-art GNNs include GraphQA <ref type=""bibr"" target=""#b1"">(Baldassarre et al., 2020)</ref> on MQA, Structured Transformer <ref type=""bibr"" target=""#b16"">(Ingraham et al., 2019)</ type=""bibr"" target=""#b9"">(Derevyanko et al., 2018)</ref> and Ornate <ref type=""bibr"" target=""#b23"">(Pagès et al., 2019)</ref>, the GNN method GraphQA <ref type=""bibr"" target=""#b1"">(Baldassarre et al., 2020)</ref>, and three methods that use sequential representations-VoroMQA <ref type=""bibr"" target= 63, T0966, T0968s1, T0968s2, T1003, T1005, T1008, T1009, T1011, and T1016. This information is obtained from the CASP download center as described by <ref type=""bibr"" target=""#b1"">Baldassarre et al. (2020)</ref>. The exact numbers of targets and structures in each set can be found in Table <ref type target=""#b5"">Cheng et al. (2019)</ref> are in our comparison for CASP 13. All predictions were obtained from the CASP download center as described by <ref type=""bibr"" target=""#b1"">Baldassarre et al. (2020)</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>E.3 CPD: RESULTS ON TS50 TEST",0
"atures, leveraging the geometric aspect of the domain. The MQA methods 3DCNN <ref type=""bibr"" target=""#b9"">(Derevyanko et al., 2018)</ref> and Ornate <ref type=""bibr"" target=""#b23"">(Pagès et al., 2019)</ref> and a number of CPD methods <ref type=""bibr"" target=""#b0"">(Anand et al., 2020;</ref><ref typ set in Table <ref type=""table"">1</ref>. These include the CNN methods 3DCNN <ref type=""bibr"" target=""#b9"">(Derevyanko et al., 2018)</ref> and Ornate <ref type=""bibr"" target=""#b23"">(Pagès et al., 2019)</ref>, the GNN method GraphQA <ref type=""bibr"" target=""#b1"">(Baldassarre et al., 2020)</ref>, and candidate structures for a specific target <ref type=""bibr"">(Cao &amp; Cheng, 2016;</ref><ref type=""bibr"" target=""#b9"">Derevyanko et al., 2018;</ref><ref type=""bibr"" target=""#b23"">Pagès et al., 2019)</ref>. We follow this convention in our experiments.</p><p>We train and validate on 79200 candidate",0
"optimization of deep neural networks. Curiously, different domains require specialized normalization methods. In computer vision, batch normalization <ref type=""bibr"" target=""#b15"">[16]</ref> is a standard component. While in natural language processing (NLP), layer normalization <ref type=""bibr"" ta g different existing normalization methods is which set of feature values the normalization is applied to. For example, in computer vision, BatchNorm <ref type=""bibr"" target=""#b15"">[16]</ref> is the de facto method that normalizes the feature values in the same channel across different samples in th </ref><ref type=""bibr"" target=""#b31"">32]</ref>. During testing, the estimated dataset-level statistics are used instead of the batch-level statistics <ref type=""bibr"" target=""#b15"">[16]</ref>.</p><p>In GNNs, for each feature dimension, the BatchNorm normalizes the feature values of the dimension ove values further scaling to unit norm enjoys ""scale-invariant"" property <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b15"">16]</ref>. In comparison, BatchNorm in the lower branch suffers from heavy batch noise. Overall, GraphNorm significantl adation; further scaling to unit norm enjoys ""scale-invariant"" property<ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b15"">16]</ref>. In comparison, BatchNorm in the lower branch suffers from heavy batch noise. Overall, GraphNorm significantl ifferent normalization techniques have been proposed to improve the training process in different applications <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b26"" decouples the optimization of direction and length of the parameters; <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b20"">21]</ref> show that the normalization implicitly tunes the learning rate. <ref e of the batch size. We visualize the statistics from BatchNorm layers under different settings of batch sizes <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b31"">32</ref>, 64], as in Figure <ref type=""figure"">9</ref>. We can see that the obs bor aggregation, and W (k) is the weight/parameter matrix in layer k. We apply the normalization after the linear transformation as in previous works <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" target=""#b36"">37]</ref>. We can instantiate Eq. ( <r eviation in a sampled batch are random variables which try to provide accurate estimations for the mean and standard deviation over the whole dataset <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b31"">32]</ref>. During testing, the estimat",1
"""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" target=""#b35"">36]</ref>. The reason behind the effectiveness of normalization has been intens",1
"eural networks (DCNN) <ref type=""bibr"" target=""#b1"">[2]</ref>, Deep Graph CNN (DGCNN) <ref type=""bibr"">[42]</ref> and Anonymous Walk Embeddings (AWL) <ref type=""bibr"" target=""#b16"">[17]</ref>. We report the accuracies reported in the original paper <ref type=""bibr"" target=""#b36"">[37]</ref>. For the",0
"ines as in xu2018how, including the WL subtree kernel model <ref type=""bibr"" target=""#b29"">[30]</ref>, diffusion-convolutional neural networks (DCNN) <ref type=""bibr"" target=""#b1"">[2]</ref>, Deep Graph CNN (DGCNN) <ref type=""bibr"">[42]</ref> and Anonymous Walk Embeddings (AWL) <ref type=""bibr"" targe",0
"cess in different applications <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" target=""#b32"">33,</ref><ref type=""bibr"" target=""#b35""",0
"een proposed to improve the training process in different applications <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" target=""#b32""",0
"riables which try to provide accurate estimations for the mean and standard deviation over the whole dataset <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b31"">32]</ref>. During testing, the estimated dataset-level statistics are used inst",0
"target=""#b5"">6]</ref>, and similar ideas are also used to accelerate the optimization of deep neural networks <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b17"">18]</ref>. In the case of optimizing the weight matrix W (k) , we can see from Eq. ( <ref type=""formula"" target=""#formu onnection. Following <ref type=""bibr"" target=""#b14"">[15]</ref>, we set the hidden dimension as 300.</p><p>Hyper-parameter configurations. We use Adam <ref type=""bibr"" target=""#b17"">[18]</ref> optimizer with a linear learning rate decay schedule. For the drawing of the training curves in Figure <ref",0
"ng such information in images will not change the semantics of the objects and thus will not hurt the classification performance.  Graph-agnostic MLP <ref type=""bibr"" target=""#b14"">[15]</ref> 68.19 ± 0.71 GCN <ref type=""bibr"" target=""#b14"">[15]</ref> 76.06 ± 0.97 GIN <ref type=""bibr"" target=""#b14"">[ of the objects and thus will not hurt the classification performance.  Graph-agnostic MLP <ref type=""bibr"" target=""#b14"">[15]</ref> 68.19 ± 0.71 GCN <ref type=""bibr"" target=""#b14"">[15]</ref> 76.06 ± 0.97 GIN <ref type=""bibr"" target=""#b14"">[15]</ref> 75 This analysis inspires us to modify the curren erformance.  Graph-agnostic MLP <ref type=""bibr"" target=""#b14"">[15]</ref> 68.19 ± 0.71 GCN <ref type=""bibr"" target=""#b14"">[15]</ref> 76.06 ± 0.97 GIN <ref type=""bibr"" target=""#b14"">[15]</ref> 75 This analysis inspires us to modify the current normalization method with a learnable parameter to automa e summarized in Table <ref type=""table"" target=""#tab_3"">2</ref>. Those information can be also found in <ref type=""bibr"" target=""#b36"">[37]</ref> and <ref type=""bibr"" target=""#b14"">[15]</ref>. Social networks datasets. IMDB-BINARY is a movie collaboration dataset. Each graph corresponds to an ego-ne arge-scale ogbgmolhiv dataset, we also use 5-layer GIN/GCN <ref type=""bibr"" target=""#b36"">[37]</ref> architecture with residual connection. Following <ref type=""bibr"" target=""#b14"">[15]</ref>, we set the hidden dimension as 300.</p><p>Hyper-parameter configurations. We use Adam <ref type=""bibr"" targ 5e − 3, 5e − 4, 5e − 5} ∪ {0.0}, the learning rate ∈ {1e − 4, 1e − 3, 1e − 2}. We follow previous work <ref type=""bibr"" target=""#b36"">[37]</ref> and <ref type=""bibr"" target=""#b14"">[15]</ref> to select the best hyper-parameter based on validation performance.</p><p>Baselines. For the medium-scale bi accuracies reported in the original paper <ref type=""bibr"" target=""#b36"">[37]</ref>. For the large-scale ogbg-molhiv dataset, we use the baselines in <ref type=""bibr"" target=""#b14"">[15]</ref>, including the Graph-agnostic MLP model, GCN <ref type=""bibr"" target=""#b18"">[19]</ref> and GIN <ref type=""bi e=""bibr"" target=""#b18"">[19]</ref> and GIN <ref type=""bibr"" target=""#b36"">[37]</ref>. We also report the roc-auc values reported in the original paper <ref type=""bibr"" target=""#b14"">[15]</ref>.</p><p>Evaluation, Using the chosen hyper-parameter, we report the averaged test performance over different ean and standard deviation of the validation accuracies across the 10 folds are reported. For the ogbg-molhiv dataset, we follow the official setting <ref type=""bibr"" target=""#b14"">[15]</ref>. We repeat the training process with 10 different random seeds, and average the roc-auc value on the test se",0
"ium-scale bioinformatics and social network datasets, we compare several competitive baselines as in xu2018how, including the WL subtree kernel model <ref type=""bibr"" target=""#b29"">[30]</ref>, diffusion-convolutional neural networks (DCNN) <ref type=""bibr"" target=""#b1"">[2]</ref>, Deep Graph CNN (DGC",0
", much of the current work on attacking graph neural networks has concentrated on node classification task <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" target=""#b12"">[13]</ref>, <ref type=""bibr"" target=""#b13"">[14]</ref>, <ref type=""bibr"" target=""#b14"">[15]</ref>. In order to fool a cl er, we focus on the targeted attack that aims to make a specific node (e.g., a person in social networks) misclassified. In this scenario, Dai et al. <ref type=""bibr"" target=""#b12"">[13]</ref> study the adversarial attack on graph structure data and propose a gradient-based method, namely GradArgmax, previous works. Inspired by Simplified Graph Convolutional Network (SGC) <ref type=""bibr"" target=""#b16"">[17]</ref> and gradient-based attack methods <ref type=""bibr"" target=""#b12"">[13]</ref>, <ref type=""bibr"" target=""#b14"">[15]</ref>, we propose a novel Simplified Gradient-based Attack (SGA) framew t, along or against the direction of it is an efficient way to generate destructive adversarial examples. Focusing on the targeted attack, Dai et al. <ref type=""bibr"" target=""#b12"">[13]</ref> propose GradArgmax, which extracts gradients of the surrogate model, and flips edges with the largest magnit /ns/1.0""><head n=""3.2.1"">Vanilla Graph Convolution Network (GCN)</head><p>Since a number of existing works <ref type=""bibr"" target=""#b11"">[12]</ref>, <ref type=""bibr"" target=""#b12"">[13]</ref>, <ref type=""bibr"" target=""#b14"">[15]</ref>, <ref type=""bibr"" target=""#b24"">[25]</ref> use vanilla GCN <ref t , here we briefly introduce other proposed state-of-the-art targeted attack methods: Nettack <ref type=""bibr"" target=""#b11"">[12]</ref> and GradArgmax <ref type=""bibr"" target=""#b12"">[13]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.3.1"">Nettack</head><p>Similar to SGC, Nettack the graph. Besides, there is a constraint that only nodes belonging to the same class/different classes will be disconnected/connected. • GradArgmax <ref type=""bibr"" target=""#b12"">[13]</ref>. Since the attack budget ∆ is defined as the degrees of target node t, the original GradArgmax will remove a",1
"arget=""#b8"">[9]</ref>. Undoubtedly, graph plays a crucial role in many high impact applications in the world <ref type=""bibr"" target=""#b7"">[8]</ref>, <ref type=""bibr"" target=""#b9"">[10]</ref>, <ref type=""bibr"" target=""#b10"">[11]</ref>. Therefore, the importance of the robustness of deep learning mode",0
"ref> is unfit to serve as a surrogate model despite being used frequently in previous works. Inspired by Simplified Graph Convolutional Network (SGC) <ref type=""bibr"" target=""#b16"">[17]</ref> and gradient-based attack methods <ref type=""bibr"" target=""#b12"">[13]</ref>, <ref type=""bibr"" target=""#b14""> la GCN <ref type=""bibr"" target=""#b7"">[8]</ref> as a surrogate model to conduct attacks, thus we first introduce GCN and further draw attention on SGC <ref type=""bibr"" target=""#b16"">[17]</ref> -a simplified variant of GCN. Refer to this work <ref type=""bibr"" target=""#b7"">[8]</ref>, GCN is recursively egation between nodes and their neighboring nodes, which is repeatedly and unnecessarily computed during training. To address this problem, Wu et al. <ref type=""bibr"" target=""#b16"">[17]</ref> theoretically analyze the structure of GCN and further propose a linear variant, namely SGC. SGC replaces th the transferability of our method, we conduct attack on several commonly used graph neural networks: GCN <ref type=""bibr"" target=""#b7"">[8]</ref>, SGC <ref type=""bibr"" target=""#b16"">[17]</ref>, GAT <ref type=""bibr"" target=""#b40"">[41]</ref>, GraphSAGE <ref type=""bibr"" target=""#b39"">[40]</ref>, Cluster he most representative graph neural networks that learns hidden representations by encoding both local graph structure and node features.</p><p>• SGC <ref type=""bibr"" target=""#b16"">[17]</ref>. SGC is a linear variant of GCN that has been proposed recently, which achieves competitive results and even",0
"the enormous advancement of deep learning, many domains like speech recognition <ref type=""bibr"" target=""#b0"">[1]</ref> and visual object recognition <ref type=""bibr"" target=""#b1"">[2]</ref>, have achieved a dramatic improvement out of the state-of-the-art methods. Despite the great success, deep lea",0
"sed attack</head><p>As well as methods focused on gradients, attackers prefer to explore other heuristic algorithms to conduct attacks. Waniek et al. <ref type=""bibr"" target=""#b25"">[26]</ref> propose ""Disconnect Internally, Connect Externally"" (DICE), conducting attack by dropping edges between node",0
"random parameters (baselines with only the neighborhood aggregation function), GVAE with GCN encoder Kipf &amp; Welling (2016), DGI with GIN encoder <ref type=""bibr"" target=""#b45"">Velickovic et al. (2019)</ref>, and EGI with GIN encoder. We train GVAE, DGI and EGI on one graph from either set (F an type=""formula"">2017</ref>) since the networks are small), our t-tests have shown the improvements of EGI to be significant. 55.56% ? 6.83% DGI (GIN) <ref type=""bibr"" target=""#b45"">Velickovic et al. (2019)</ref> 57.75% ? 4.47% 62.44% ? 4.46% 68.15% ? 6.24% Mask- <ref type=""bibr"">GIN Hu et al. (2019a",1
"tions to spectral graph theory <ref type=""bibr"" target=""#b9"">Defferrard et al. (2016)</ref>; <ref type=""bibr"" target=""#b5"">Bruna et al. (2014)</ref>; <ref type=""bibr"" target=""#b15"">Hammond et al. (2011)</ref>. While most GNN architectures are not very complicated, the training of GNNs can still be c",0
"nly depending on the structural information of the graph. In practice, the computation of eigenvalues on the small ego-graphs can be rather efficient <ref type=""bibr"" target=""#b1"">Arora et al. (2005)</ref>, and we do not need to enumerate all pairs of ego-graphs. Suppose we need to sample M pairs of",0
", allowing pretrained LMs to solve them without any or with only very few labeled examples <ref type=""bibr"" target=""#b26"">(Radford et al., 2019;</ref><ref type=""bibr"" target=""#b30"">Schick and Schütze, 2020a)</ref>.</p><p>Very recently, <ref type=""bibr"" target=""#b1"">Brown et al. (2020)</ref> introduc w examples as the context window of most LMs is limited to a few hundred tokens.</p><p>An alternative to priming is pattern-exploiting training (PET) <ref type=""bibr"" target=""#b30"">(Schick and Schütze, 2020a)</ref>, which combines the idea of reformulating tasks as cloze questions with regular gradi ways to reformulate tasks as cloze questions that are understood well by LMs is difficult <ref type=""bibr"" target=""#b12"">(Jiang et al., 2019)</ref>, <ref type=""bibr"" target=""#b30"">Schick and Schütze (2020a)</ref> propose PET, a method that uses knowledge distillation <ref type=""bibr"" target=""#b11""> t al., 2019)</ref> and RTE <ref type=""bibr"" target=""#b4"">(Dagan et al., 2006)</ref> are textual entailment tasks like MNLI, so we use PVPs similar to <ref type=""bibr"" target=""#b30"">Schick and Schütze (2020a)</ref>. For a premise p and hypothesis h, we use</p><formula xml:id=""formula_6"">h? | , p , ""h is performed on the regular, full size training sets. 5 We run PET on the FewGLUE training sets for all SuperGLUE tasks using the exact same setup as <ref type=""bibr"" target=""#b30"">Schick and Schütze (2020a)</ref>. For COPA, WSC and ReCoRD, we use our proposed modification of PET to support verbaliz model (Table <ref type=""table"" target=""#tab_2"">3</ref>). Given 32 examples, PET clearly outperforms both baselines, which is in line with findings by <ref type=""bibr"" target=""#b30"">Schick and Schütze (2020a)</ref>.</p><p>We next compare PET directly to priming. However, we cannot do so using ALBERT rmat s | n where | is the boundary between two text segments and mark p in s with asterisks.</p><p>MultiRC Deviating from the hyperparameters used by <ref type=""bibr"" target=""#b30"">Schick and Schütze (2019)</ref>, we use a maximum sequence length of 512 tokens for MultiRC both during training and in",1
"y.</p><p>In this work, we modify PET to also work for tasks that require predicting more than one token. We then show that in combination with ALBERT <ref type=""bibr"" target=""#b15"">(Lan et al., 2020)</ref>, PET and its iterative variant (iPET) both outperform GPT-3 on SuperGLUE with 32 training exam final classifier.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.2"">Setup</head><p>As underlying LM for PET we choose ALBERTxxlarge-v2 <ref type=""bibr"" target=""#b15"">(Lan et al., 2020)</ref>, the best-performing MLM on SuperGLUE when training is performed on the regular, full size tra",0
"ples <ref type=""bibr"" target=""#b26"">(Radford et al., 2019;</ref><ref type=""bibr"" target=""#b30"">Schick and Schütze, 2020a)</ref>.</p><p>Very recently, <ref type=""bibr"" target=""#b1"">Brown et al. (2020)</ref> introduced GPT-3, a pretrained LM with an enormous 175 billion parameters, and showed that it ne several reformulations. In contrast to PET, which uses gradient-based optimization, <ref type=""bibr"" target=""#b26"">Radford et al. (2019)</ref> and <ref type=""bibr"" target=""#b1"">Brown et al. (2020)</ref> investigate priming, where examples are given as context but no parameter updates are performe 2019;</ref><ref type=""bibr"" target=""#b1"">Brown et al., 2020)</ref>, we select only positive examples for WSC; for both MultiRC and ReCoRD, we follow <ref type=""bibr"" target=""#b1"">Brown et al. (2020)</ref> and select a total of 32 questions.</p><p>We additionally create sets of up to 20,000 unlabele tween two text segments.</p><p>WSC Unlike other SuperGLUE tasks, the WSC formulation of <ref type=""bibr"" target=""#b27"">Raffel et al. (2019)</ref> and <ref type=""bibr"" target=""#b1"">Brown et al. (2020)</ref> requires free-form completion, meaning that for each sentence s and pronoun p, we only have a omly selecting 32 examples for each task using a fixed random seed. Following previous work <ref type=""bibr"" target=""#b27"">(Raffel et al., 2019;</ref><ref type=""bibr"" target=""#b1"">Brown et al., 2020)</ref>, we select only positive examples for WSC; for both MultiRC and ReCoRD, we follow <ref type=""b n p and noun n, and the task is to determine whether p refers to n. We follow previous work <ref type=""bibr"" target=""#b27"">(Raffel et al., 2019;</ref><ref type=""bibr"" target=""#b1"">Brown et al., 2020)</ref> and treat WSC as a generative task. We highlight p in s by putting it in asterisks and use the ch is larger by two orders of magnitude. Importantly, PET with GPT-2 performs much worse than with both other models. As anticipated by previous work <ref type=""bibr"" target=""#b1"">(Brown et al., 2020)</ref>, a key reason for this drop in performance could be that like GPT-3, GPT-2 is a unidirectiona",0
"by providing task descriptions was proposed by <ref type=""bibr"" target=""#b26"">Radford et al. (2019)</ref> and has been applied to text classification <ref type=""bibr"" target=""#b24"">(Puri and Catanzaro, 2019)</ref>, commonsense knowledge mining <ref type=""bibr"" target=""#b5"">(Davison et al., 2019)</re",0
"(Opitz, 2019)</ref>. It is also commonly used for probing the knowledge contained within LMs <ref type=""bibr"" target=""#b33"">(Trinh and Le, 2018;</ref><ref type=""bibr"" target=""#b22"">Petroni et al., 2019;</ref><ref type=""bibr"" target=""#b32"">Talmor et al., 2019;</ref><ref type=""bibr"" target=""#b31"">Schi",0
"s better?</p><p>To resolve the first issue, taking two well-accepted propositions-deep neural networks learn meaningful patterns before fitting noise <ref type=""bibr"" target=""#b2"">[3]</ref> and minimum entropy regularisation principle [10]-we propose a novel end-to-end method named ProSelfLC, which y phase, during which the model is learning simple meaningful patterns before fitting noise, even when severe label noise exists in human annotations <ref type=""bibr"" target=""#b2"">[3]</ref>. (2) As a learner attains confident knowledge as time progresses, we leverage it to revise annotated labels. T ly phase, during which the model is learning simple meaningful patterns before fitting noise, even when severe label noise exists in human annotations<ref type=""bibr"" target=""#b2"">[3]</ref>. (2) As a learner attains confident knowledge as time progresses, we leverage it to revise annotated labels. T",1
"evise annotated labels. This is surrounded by minimum entropy regularisation, which is widely evaluated in unsupervised and semi-supervised scenarios <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b9"">10]</ref>.</p><p>Secondly, note that OR methods penalise low entropy while LC rew revise annotated labels. This is surrounded by minimum entropy regularisation, which is widely evaluated in unsupervised and semi-supervised scenarios<ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b9"">10]</ref>.Secondly, note that OR methods penalise low entropy while LC rewards it y minimisation is the most widely used principle in machine learning <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b37"">38,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b21"">22]</ref>. In standard classification, min",1
"bibr"" target=""#b27"">[28,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b53"">54,</ref><ref type=""bibr"" target=""#b46"">47,</ref><ref type=""bibr"" target=""#b33"">34,</ref><ref type=""bibr"" target=""#b24"">25]</ref> and exploit their 'disagreeme",0
"ploiting an auxiliary trusted training set to differentiate examples <ref type=""bibr"" target=""#b44"">[45,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b15"">16]</ref>. This requires extra annotation cost;</p><p>(3) Co-training strategies, which train two or more learners <ref ""bibr"" target=""#b44"">45,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b52"">53,</ref><ref type=""bibr"" target=""#b59"">60]</ref> which use a clean dataset to",0
"f type=""bibr"" target=""#b41"">[42,</ref><ref type=""bibr"" target=""#b28"">29]</ref> and confidence penalty (CP) <ref type=""bibr"" target=""#b32"">[33]</ref>; <ref type=""bibr"" target=""#b1"">(2)</ref> Label correction (LC). On the one hand, LC regularises neural networks by adding the similarity structure info confident outputs without leveraging easily accessible knowledge from other learners or itself (Figure <ref type=""figure"" target=""#fig_1"">1a</ref>); <ref type=""bibr"" target=""#b1"">(2)</ref> Non-self LC relies on accurate auxiliary models to generate predictions (Figure <ref type=""figure"" target=""#fi bibr"" target=""#b7"">[8]</ref> uses an extra softmax layer, while Masking <ref type=""bibr"" target=""#b11"">[12]</ref> exploits human cognition. MD-DYR-SH <ref type=""bibr"" target=""#b1"">[2]</ref> is a combination of three techniques: dynamic mixup (MD), dynamic bootstrapping together with label regularisa",0
"""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b52"">53,</ref><ref type=""bibr"" target=""#b59"">60]</ref> which use a clean dataset to train a network's learning parameters.</",0
"bel noise: (1) Loss correction, in which we are given or we need to estimate a noisetransition matrix, which defines the distribution of noise labels <ref type=""bibr"" target=""#b25"">[26,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b40"">41,</ref><ref type=""bibr"" target=""#b43""> bibr"" target=""#b43"">[44,</ref><ref type=""bibr"" target=""#b44"">45,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b52"">53,</ref><ref type=""bibr"" target=""#b59""",0
"complex to estimate in practice; (2) Exploiting an auxiliary trusted training set to differentiate examples <ref type=""bibr"" target=""#b44"">[45,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b15"">16]</ref>. This requires extra annotation cost;</p><p>(3) Co-training strategie",0
"exploits the predictions of other model(s), usually termed teacher(s) <ref type=""bibr"" target=""#b16"">[17]</ref>. Self LC methods include Pseudo-Label <ref type=""bibr"" target=""#b22"">[23]</ref>, bootstrapping (Boot-soft and Boot-hard) <ref type=""bibr"" target=""#b34"">[35]</ref>, Joint Optimisation (Join target. The double-ended arrow means factual equivalence, because an output is definitely non-negative after a softmax layer. (b) LC contains Self LC <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b34"">35,</ref><ref type=""bibr"" target=""#b42"">43,</ref><ref type=""bibr"" target=""#b54 ef> and exploit their 'disagreement' information to differentiate data points; (4) Label engineering methods <ref type=""bibr"" target=""#b39"">[40,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b34"">35,</ref><ref type=""bibr"" target=""#b42"">43,</ref><ref type=""bibr"" target=""#b51""",0
"itself. A typical approach of Non-self LC is knowledge distillation (KD), which exploits the predictions of other model(s), usually termed teacher(s) <ref type=""bibr"" target=""#b16"">[17]</ref>. Self LC methods include Pseudo-Label <ref type=""bibr"" target=""#b22"">[23]</ref>, bootstrapping (Boot-soft an 2"">[23,</ref><ref type=""bibr"" target=""#b34"">35,</ref><ref type=""bibr"" target=""#b42"">43,</ref><ref type=""bibr"" target=""#b54"">55]</ref> and Non-self LC <ref type=""bibr"" target=""#b16"">[17]</ref>. The parameter ǫ defines how much a predicted label distribution is trusted. To improve Self LC, we propose r strategy is to annotate unlabelled samples or correct noisy labels.</p><p>LC and knowledge distillation (KD) <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b16"">17]</ref>. Mathematically, we derive that some KD methods also modify labels. We use the term label correction instead tion of negative log-likelihood, and q serves as the probability mass function.</p><p>Label smoothing. In LS <ref type=""bibr"" target=""#b41"">[42,</ref><ref type=""bibr"" target=""#b16"">17]</ref>, we soften one-hot targets by adding a uniform distribution: qLS = (1 − ǫ)q + ǫu, u ∈ R C , and ∀j, u j = 1 C nnotation: <ref type=""bibr"" target=""#b0"">(1)</ref> In KD, an auxiliary teacher model can provide a student model the similarity structure information <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b28"">29]</ref>; (2) In Self LC, e.g., Boot-soft, a model helps itself by exploiting",0
"architecture could be tuned to be wider if it had the support of better branch prediction, which could potentially offer more IPC gains. Prior works <ref type=""bibr"" target=""#b12"">[13]</ref>, <ref type=""bibr"" target=""#b13"">[14]</ref> have tried to address different types of hardto-predict branches. ta-dependent branches are dependent on the loads whose address is very predictable. Moreover, we do not make any modifications to the ISA. Gao et al. <ref type=""bibr"" target=""#b12"">[13]</ref> proposed a closely related work. They correlate the branch outcome to the load address and provide a predict",1
"overhead.</p><p>To evaluate the results, we use the GAP benchmark suite <ref type=""bibr"" target=""#b15"">[16]</ref> and the SPEC2006 integer benchmarks <ref type=""bibr"" target=""#b16"">[17]</ref> having less than 95% prediction accuracy on our IMLI baseline. GAP is a collection of graph algorithm benchm",0
"d the bar for the prediction accuracy. Yeh and Patt came up with the two-level branch predictors <ref type=""bibr"" target=""#b24"">[25]</ref>. McFarling <ref type=""bibr"" target=""#b25"">[26]</ref> proposed optimizations over their work. These works leverage the high correlation between the outcome of the",0
"s/1.0""><head>V. RELATED WORK</head><p>The strides in branch prediction accuracy have improved several folds since the counter-based bimodal predictor <ref type=""bibr"" target=""#b23"">[24]</ref>. The ensuing works on branch prediction gradually raised the bar for the prediction accuracy. Yeh and Patt c",0
"ivation one is due to both low bitwdith and ReLU functions. To exploit the sparsity of DNNs to improve energy efficiency, many architectures, such as <ref type=""bibr"" target=""#b24"">[25]</ref>, are proposed to detect and skip the multiplications associated with zeros. To locate the irregular appearan",1
"hniques into PIM architectures. LeCun, a prominent scholar in DNN algorithms and a pioneer of DNN hardware, attributed this phenomenon to two factors <ref type=""bibr"" target=""#b15"">[16]</ref>. One is lossy conversions from and to digital due to analog computing, and the other is lack of hardware mul",0
"htly coupling memory and processing units first spawned processing-near-memory systems that fabricated processors in DRAM chips, dating back to 1990s <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b2"">3]</ref>. Furthermore, processing-in-memory (PIM) architectures <ref type=""bibr""",0
"ing number of the matched entries, i.e., N pos and N neg . To address these both steps, the proposed design adopts a content addressable memory (CAM, <ref type=""bibr"" target=""#b25"">[26]</ref>) based digital implementation instead of analog MAC circuits.</p><p>A CAM based in-memory entry-counting sch rate a ""0"" if the input line is ""1"", and vice versa. Therefore, the XOR function is embedded in the 10T hit cell. In conventional NAND-type CAM cells <ref type=""bibr"" target=""#b25"">[26]</ref>, the pass transistors are NMOS only, making the high voltage level of the output node V DD -V th . A latch-b",0
"searchers have turned more attention to 2?4-bit quantization models <ref type=""bibr"" target=""#b20"">[21]</ref><ref type=""bibr"" target=""#b21"">[22]</ref><ref type=""bibr"" target=""#b22"">[23]</ref>, which have the potentials to bring its inference accuracy close to the original floating-point baseline.</p aining results. The first and last layers are not quantized to low-bitwidth, because the model accuracy is sensitive to the precision of these layers <ref type=""bibr"" target=""#b22"">[23]</ref>.</p><p>The design of the LSQ quantizer in our implementation follows Esser's practice <ref type=""bibr"" targe n Most of the previous low-bitwidth quantization schemes, including <ref type=""bibr"" target=""#b20"">[21]</ref><ref type=""bibr"" target=""#b21"">[22]</ref><ref type=""bibr"" target=""#b22"">[23]</ref>, do not quantize these parameters into fixedpoint integers. Our experiments show that after these data are d",0
"ate data for storing candidates. For example, RStream generates about 1.2TB intermediate data to count 4-motif on the MiCo graph with 1 million edges <ref type=""bibr"" target=""#b17"">[18]</ref>.</p><p>Recently, specialized systems have been developed for pattern matching <ref type=""bibr"" target=""#b11"" some real graph mining applications.</p><p>Algorithms in these specialized pattern matching systems can be described with nested loops, and AutoMine <ref type=""bibr"" target=""#b17"">[18]</ref> and GraphZero <ref type=""bibr"" target=""#b11"">[12]</ref> represent relatively good performance in such system <ref type=""bibr"" target=""#b25"">[26]</ref>, the state-of-the-art singlemachine pattern matching systems. GraphZero is an upgraded version of AutoMine <ref type=""bibr"" target=""#b17"">[18]</ref>, and it outperforms AutoMine by up to 40×. Fractal is a JVM-based system, and it outperforms several JVM-bas [21]</ref>, <ref type=""bibr"" target=""#b28"">[29]</ref>, <ref type=""bibr"" target=""#b38"">[39]</ref>, <ref type=""bibr"" target=""#b39"">[40]</ref>. Automine <ref type=""bibr"" target=""#b17"">[18]</ref> is built upon a set-based representation and uses compilation techniques to generate efficient pattern match",1
"by appending edges to candidate subgraphs and filtering the newly generated candidates using the userdefined filter and process functions. G-thinker <ref type=""bibr"" target=""#b15"">[16]</ref> provides an intuitive graph-exploration API for implementing various graph mining algorithms and an efficien",0
"ON</head><p>Graph data and algorithms are widely used in many fields, such as social networks <ref type=""bibr"" target=""#b0"">[1]</ref>, bioinformatics <ref type=""bibr"" target=""#b1"">[2]</ref>, and fraud detection <ref type=""bibr"" target=""#b2"">[3]</ref>. With the increasing amount of graph data, proces nt data graphs. Approximate Subgraph Counting Some computation engines and systems have been designed to estimate an approximate number of embeddings <ref type=""bibr"" target=""#b1"">[2]</ref>, <ref type=""bibr"" target=""#b41"">[42]</ref>, <ref type=""bibr"" target=""#b42"">[43]</ref>. ASAP <ref type=""bibr"" t vertices, there are n! possible relative magnitudes of n vertices in an embedding (e.g., when n = 5, they are <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b4"">5]</re type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b4"">5]</ref>, <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b3"">4]</re type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b3"">4]</ref>, <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b4"">5]</re",0
"he-art pattern matching system by several orders of magnitude. Specifically, GraphPi is up to 105× faster than GraphZero and 154× faster than Fractal <ref type=""bibr"" target=""#b25"">[26]</ref> running on the same single node. After using the Inclusion-Exclusion Principle (IEP) for counting the number ferent schedules for them.</p><p>Comparison We evaluate GraphPi's performance against GraphZero <ref type=""bibr"" target=""#b11"">[12]</ref> and Fractal <ref type=""bibr"" target=""#b25"">[26]</ref>, the state-of-the-art singlemachine pattern matching systems. GraphZero is an upgraded version of AutoMine <",0
"rget=""#b0"">[1]</ref>. However, unexpected DDI can also trigger side effects, adverse reactions, and even serious toxicity, leading patients in danger <ref type=""bibr"" target=""#b1"">[2]</ref>. As there exists increasing needs of multi-drug treatments, the identification of DDIs is more and more urgent d databases. However, those methods cannot detect unannotated DDIs, and cannot give alerts to potential DDIs before a combinational treatment is made <ref type=""bibr"" target=""#b1"">[2]</ref>. In contrast, machine learning-based methods provide a promising way to identify unannotated potential drug-dr the supervised predictor. The feature extractor represents drugs in a form of feature vector according to drug properties, such as chemical structure <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b5"">[6]</ref><ref type=""bibr"" target=""#b6"">[7]</ref><ref type=""bibr"" target=""#b7"">[8] ""#b10"">[11]</ref><ref type=""bibr"" target=""#b11"">[12]</ref><ref type=""bibr"" target=""#b12"">[13]</ref><ref type=""bibr"" target=""#b13"">[14]</ref>, targets <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b7"">[8]</ref><ref type=""bibr"" target=""#b8"">[9]</ref><ref type=""bibr"" target=""#b9"">[10 classification algorithms, such as KNN <ref type=""bibr"" target=""#b11"">[12]</ref>, SVM <ref type=""bibr"" target=""#b11"">[12]</ref>, logistic regression <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b9"">10]</ref>, decision tree <ref type=""bibr"" t ectors/similarity matrices and annotated DDI labels, then deduces potential DDIs with the well-trained model. Most methods utilize a single predictor <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b4"">[5]</ref><ref type=""bibr"" target=""#b5"">[6]</ref><ref type=""bibr"" target=""#b6"">[7]",1
"the drugs that lack a certain drug property, which results in small-scale pruned datasets and thus is not pragmatic and suitable in the real scenario <ref type=""bibr"" target=""#b16"">[17]</ref>. In addition, some handcrafted drug features may not be precise enough to represent or characterize the prop g., drugs) of the network, and thus can be used as features in building machine learning models for various downstream tasks, such as link prediction <ref type=""bibr"" target=""#b16"">[17]</ref>. Recently, the GCN has been applied to the field of drug development and discovery <ref type=""bibr"" target=""",0
"pe=""bibr"" target=""#b6"">[7]</ref><ref type=""bibr"" target=""#b7"">[8]</ref><ref type=""bibr"" target=""#b8"">[9]</ref><ref type=""bibr"" target=""#b9"">[10]</ref><ref type=""bibr"" target=""#b10"">[11]</ref><ref type=""bibr"" target=""#b11"">[12]</ref><ref type=""bibr"" target=""#b12"">[13]</ref><ref type=""bibr"" target=""#b pe=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b7"">[8]</ref><ref type=""bibr"" target=""#b8"">[9]</ref><ref type=""bibr"" target=""#b9"">[10]</ref><ref type=""bibr"" target=""#b10"">[11]</ref>, Anatomical Therapeutic Chemical classification (ATC) codes <ref type=""bibr"" target=""#b7"">[8]</ref><ref type ibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b13"">14]</ref>, medication and/or clinical observations <ref type=""bibr"" target=""#b10"">[11]</ref>.</p><p>The supervised predictor is usually implemented by classification algorithms, such as KNN <ref type="" =""bibr"" target=""#b6"">7]</ref>, label propagationbased method (named as LP) <ref type=""bibr"" target=""#b12"">[13]</ref> and Zhang's method (named as CE) <ref type=""bibr"" target=""#b10"">[11]</ref> in 5-CV test. Vilar et al <ref type=""bibr"" target=""#b5"">[6]</ref> integrates a Tanimoto similarity matrix of n to assign labels from known DDIs to previously unlabeled nodes by computing drug similarityderived weights of edges on the DDI network. Zhang et al <ref type=""bibr"" target=""#b10"">[11]</ref> collects a variety of drug-related data (e.g., known drug-drug interactions, drug substructures, proteins, p many base classifiers, then performed the prediction with an ensemble (CE) classifier model.</p><p>To ensure a fair comparison, the DB2 dataset from <ref type=""bibr"" target=""#b10"">[11]</ref> is adopted. In the DB2 dataset, all unlabeled drug pairs are considered as the negative samples. The compari AUC, AUPR, Recall, Precision, Accuracy, and F1-score, respectively. Although the AUC and ACC of DPDDI are slightly lower than that of Zhang's method <ref type=""bibr"" target=""#b10"">[11]</ref>, the AUPR and F 1 of DPDDI are higher. AUPR is often believed to be a more significant quality measure than rug non-interaction pairs. For the prediction of drug-drug interaction, F 1 should be more effective measure than ACC.</p><p>In addition, Zhang et al <ref type=""bibr"" target=""#b10"">[11]</ref> used 9 drug-related data sources, while our DPDDI just use the known drug-drug interaction data. If we integ e.g., drug substructure, drug target, drug enzyme, drug transporter, drug pathway, drug indication, drug side effect and drug off side effect used in <ref type=""bibr"" target=""#b10"">[11]</ref>) to construct the dug-drug similarity network, using DPDDI framework to predict DDIs, DPDDI should be able t 576 annotated drug-drug interactions. In order to compare with other state-of-the-art methods, a smaller dataset (named as DB2) built by Zhang et al. <ref type=""bibr"" target=""#b10"">[11]</ref> was adopted to evaluate the performance of our DPDDI. DB2 contains 548 drugs and 48,584 annotated drug-drug ell><cell>0.754</cell><cell>0.940</cell><cell>0.840</cell></row></table><note><p><p><p><p><p>a</p>The results are taken from Table</p>5</p>in Ref.</p><ref type=""bibr"" target=""#b10"">[11]</ref> </p></note></figure> <figure xmlns=""http://www.tei-c.org/ns/1.0"" type=""table"" xml:id=""tab_3""><head>Table 3</ t=""#b9"">[10]</ref><ref type=""bibr"" target=""#b11"">12]</ref>, side effects <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b13"">14]</ref>, medication and/or clinical o f type=""bibr"" target=""#b6"">[7]</ref><ref type=""bibr"" target=""#b7"">[8]</ref>, label propagation <ref type=""bibr"" target=""#b12"">[13]</ref>, random walk <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b14"">15]</ref>, probabilistic soft logic <ref type=""bibr"" target=""#b8"">[9,</ref><re",0
"tational time and evaluate performance of a predictor, in this study, we adopted the 5-fold cross-validation (5CV) test as done by most investigators <ref type=""bibr"" target=""#b25"">[26]</ref><ref type=""bibr"" target=""#b26"">[27]</ref><ref type=""bibr"" target=""#b27"">[28]</ref><ref type=""bibr"" target=""#b",0
"</p><p>In statistical prediction, the jackknife test and q-fold cross-validation (CV) test are often used to examine the effectiveness of a predictor <ref type=""bibr"" target=""#b23"">[24]</ref>. Of the two test methods, the jackknife test is deemed the least arbitrary that can always yield a unique re",0
">33]</ref>, mispredictions from data-dependent branches remain the single most important category of mispredictions to tackle. Prior works like EXACT <ref type=""bibr"" target=""#b8"">[9]</ref>, SLB <ref type=""bibr"" target=""#b17"">[18]</ref> have proposed different techniques to lower the mispredictions ir limitations.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2.2"">Existing Techniques to Handle Data Dependent Branches</head><p>EXACT <ref type=""bibr"" target=""#b8"">[9]</ref> is a branch prediction technique proposed to lower mispredictions from data dependent branches. By distinguish , the set of distinct load-branch and store-branch pairs that need to get tracked quickly increase the storage requirement over 10KB. As mentioned in <ref type=""bibr"" target=""#b8"">[9]</ref>, EXACT predictor does not win over an similarly sized TAGE predictor.</p><p>Learning from EXACT's limitations, load and only simple computations in the data path leading to the branch. To enable this, we employ a scheme similar to EXACT predictor ID generation <ref type=""bibr"" target=""#b8"">[9]</ref> where the ARF is extended to track the load addresses. Instead of tracking the load address, we track the load imization is to find the store prior to the load that wrote to the same address and use the store data value for override. EXACT's active update unit <ref type=""bibr"" target=""#b8"">[9]</ref> and SLB <ref type=""bibr"" target=""#b17"">[18]</ref> use this observation, but the hardware requirements were pro based on the correlation with prior data values seen. But, when data has high entropy, it affects their accuracy. Among recent works, EXACT predictor <ref type=""bibr"" target=""#b8"">[9]</ref> proposes associating data dependent branches with their corresponding producer load address and using this inf iate operations and 3D-Branch PC) are sent to the BT. We assume that opcode and other instruction metadata is available with the ROB entry similar to <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b27"">28]</ref>.</p><p>1) 3D-Branch Tracker Table (BT): Each entry has 1 valid bit, 8",1
"er.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""8"">RELATED WORK</head><p>Prior works such as <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b38"" han loads, the storage overhead to track these values is higher as compared to using the load to read the value in timely manner.</p><p>Works such as <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b21"">22]</ref> use data dependence tracking for enhancing the branch predictor. The",0
"s about 0.03KB.</p><p>3) ARF Extensions: For tracking and identifying 3D-branches using an extended ARF, we require 14 bytes per entry. Since x64 ISA <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b37"">38]</ref>, only has 16 general purpose architectural registers and one flag regi",0
"ve instruction-level parallelism (ILP) as well as memory-hierarchy parallelism (MHP). <ref type=""foot"" target=""#foot_0"">1</ref> Load Slice Core (LSC) <ref type=""bibr"" target=""#b4"">[5]</ref> was the first work to propose an sOoO core; Freeway <ref type=""bibr"" target=""#b9"">[10]</ref> builds upon the L ww.tei-c.org/ns/1.0""><head n=""2"">BACKGROUND AND MOTIVATION</head><p>In this section, we briefly cover the background on the two prior sOoO cores -LSC <ref type=""bibr"" target=""#b4"">[5]</ref> and Freeway <ref type=""bibr"" target=""#b9"">[10]</ref> -and we elaborate on their shortcomings. Figure <ref type or separating critical instruction slices, unlike FSC.</p><p>Restricted Out-of-Order Microarchitectures. We extensively discussed the Load Slice Core <ref type=""bibr"" target=""#b4"">[5]</ref> and Freeway <ref type=""bibr"" target=""#b9"">[10]</ref> throughout the paper. Shioya et al. <ref type=""bibr"" targ",1
"oO logic with simple FIFO queues.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""5.8"">Comparison Against CESP</head><p>Palacharla et al. <ref type=""bibr"" target=""#b12"">[13]</ref> propose the complexity-effective superscalar processor (CESP) architecture which steers chains of dependent complexity-effective and power-efficient. We now point out the most closely related work.</p><p>Complexity-Effective Architectures. Palacharla et al. <ref type=""bibr"" target=""#b12"">[13]</ref> propose the complexity-effective superscalar processors (CESP) architecture which steers chains of dependent",1
"nd point out a large performance gap with a traditional OoO core because of frequent dispatch stalls. A similar steering policy is used by Kim et al. <ref type=""bibr"" target=""#b8"">[9]</ref> in their Instruction-Level Distributed Processing (ILDP) work, which proposes an ISA with in-order accumulator",0
"ive cache with 2/2 read/write ports. We estimate power consumption and chip area using McPAT <ref type=""bibr"" target=""#b10"">[11]</ref> and CACTI v6.5 <ref type=""bibr"" target=""#b11"">[12]</ref> assuming a 22 nm technology node. Area and per-access power estimates for the newly added FSC hardware struc ssume a physical register file (PRF) with 32 integer and floating-point registers. The MSHR is extended to support 8 outstanding misses. We use CACTI <ref type=""bibr"" target=""#b11"">[12]</ref> to estimate chip area. CACTI accounts for the area of circuit-level structures such as hierarchically repeat et=""#b10"">[11]</ref> to calculate InO and OoO core power consumption. power consumed by the additional FSC hardware structures is modeled using CACTI <ref type=""bibr"" target=""#b11"">[12]</ref>. Table <ref type=""table"" target=""#tab_3"">3</ref> reports power consumption for the newly added components.</",0
"by allocating back-end resources for critical instructions while buffering non-critical instructions in the front-end. More recently, Alipour et al. <ref type=""bibr"" target=""#b0"">[1]</ref> leverage instruction criticality and readiness to bypass the out-of-order back-end. Instructions that do not b",0
"the Load Slice Core <ref type=""bibr"" target=""#b4"">[5]</ref> and Freeway <ref type=""bibr"" target=""#b9"">[10]</ref> throughout the paper. Shioya et al. <ref type=""bibr"" target=""#b17"">[18]</ref> propose the front-end execution architecture which executes instructions that have their operands ready in t",0
"om the timing model, which are then added to the power consumption numbers provided by McPAT.</p><p>We create representative 1B-instruction SimPoints <ref type=""bibr"" target=""#b16"">[17]</ref> for the SPEC CPU2017 benchmarks. We sort the benchmarks by increasing number of last-level cache (LLC) misse",0
">, flea-flicker multipass pipelining <ref type=""bibr"" target=""#b2"">[3]</ref>, braid processing <ref type=""bibr"" target=""#b21"">[22]</ref> and OUTRIDER <ref type=""bibr"" target=""#b5"">[6]</ref> also exploit critical instruction slices <ref type=""bibr"" target=""#b23"">[24]</ref> for improving performance.",0
"f dependent instructions to in-order queues. Dispatch stalls when an independent instruction cannot be steered to an empty queue. Salverda and Zilles <ref type=""bibr"" target=""#b13"">[14]</ref> evaluate CESP in the context of a realistic baseline and point out a large performance gap with a traditiona",0
"hrough coordinated queues. Proposals such as speculative-slice execution <ref type=""bibr"" target=""#b22"">[23]</ref>, flea-flicker multipass pipelining <ref type=""bibr"" target=""#b2"">[3]</ref>, braid processing <ref type=""bibr"" target=""#b21"">[22]</ref> and OUTRIDER <ref type=""bibr"" target=""#b5"">[6]</re",0
"ds under a more realistic experimental setup by using significantly fewer training data comparing to the few previous works which address heterophily <ref type=""bibr"" target=""#b22"">(Pei et al. 2020;</ref><ref type=""bibr"" target=""#b38"">Zhu et al. 2020)</ref>. These experiments demonstrate the effecti ng as the test set. Notice that we are using a significantly smaller fraction of training samples compared to previous works that address heterophily <ref type=""bibr"" target=""#b22"">(Pei et al. 2020;</ref><ref type=""bibr"" target=""#b38"">Zhu et al. 2020)</ref>. This is a more realistic assumption in ma bibr"" target=""#b27"">(Sen et al. 2008;</ref><ref type=""bibr"" target=""#b20"">Namata et al. 2012)</ref>. We use the features and class labels provided by <ref type=""bibr"" target=""#b22"">Pei et al. (2020)</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.2"">Node Classification with Cont 2017)</ref> by replacing the node features X in each benchmark with an identity matrix I. We use the training, validation and test splits provided by <ref type=""bibr"" target=""#b22"">Pei et al. (2020)</ref>.</p><p>Heterophily. We report results on graphs with strong heterophily under the featureless s",1
"ć et al. 2018)</ref> and AGNN <ref type=""bibr"" target=""#b30"">(Thekumparampil et al. 2018)</ref>   <ref type=""bibr"" target=""#b14"">(Li et al. 2019</ref><ref type=""bibr"" target=""#b15"">(Li et al. , 2020;;</ref><ref type=""bibr"" target=""#b23"">Rong et al. 2020)</ref>.</p><p>Although many of these GNN metho",0
"affected the design of many GNN models, which tend to generate similar representations for nodes within close proximity, as studied in previous works <ref type=""bibr"" target=""#b1"">(Ahmed et al. 2018;</ref><ref type=""bibr"" target=""#b23"">Rossi et al. 2020;</ref><ref type=""bibr"" target=""#b32"">Wu et al.",0
"/ref>. Among these methods, our approach is related to belief propagation (BP) <ref type=""bibr"" target=""#b36"">(Yedidia, Freeman, and Weiss 2003;</ref><ref type=""bibr"" target=""#b25"">Rossi et al. 2018)</ref>, a message-passing approach where each node iteratively sends its neighboring nodes estimation",0
"enerate similar representations for nodes within close proximity, as studied in previous works <ref type=""bibr"" target=""#b1"">(Ahmed et al. 2018;</ref><ref type=""bibr"" target=""#b23"">Rossi et al. 2020;</ref><ref type=""bibr"" target=""#b32"">Wu et al. 2019)</ref>. However, there are also many instances in ve focused only on graphs that have contextual features on the nodes. However, the vast majority of graph data does not have such node-level features <ref type=""bibr"" target=""#b23"">(Rossi and Ahmed 2015)</ref>, which greatly limits the utility of the methods proposed in prior work that assume such f b30"">(Thekumparampil et al. 2018)</ref>   <ref type=""bibr"" target=""#b14"">(Li et al. 2019</ref><ref type=""bibr"" target=""#b15"">(Li et al. , 2020;;</ref><ref type=""bibr"" target=""#b23"">Rong et al. 2020)</ref>.</p><p>Although many of these GNN methods work well when the data exhibits strong homophily, no",0
"urbations. Following the progress in graph adversarial attacks, designing defense mechanisms or building robust variants of GNNs have become critical <ref type=""bibr"" target=""#b25"">(Zhu et al. 2019)</ref>.</p><p>In this paper, we propose a new approach UM-GNN aimed at improving the robustness of GNN odels. It employs a variance-based attention mechanism to attenuate the influence of neighbors with large variance (potentially corrupted). Following <ref type=""bibr"" target=""#b25"">(Zhu et al. 2019)</ref>, we set hidden dimensions at 16 and assume a diagonal covariance for each node. For all baselin l perturbations;</p><p>• Across a suite of global poisoning attacks, UM-GNN consistently outperforms existing methods including the recent Robust GCN <ref type=""bibr"" target=""#b25"">(Zhu et al. 2019</ref>);</p><p>• UM-GNN achieves significantly lower misclassification rate (&gt; 50% improvement) agai cess.</p><p>While there exist very few GNN formulations for specifically defending against adversarial attacks, the recent robust GCN (RGCN) approach <ref type=""bibr"" target=""#b25"">(Zhu et al. 2019</ref>) has been the most effective, when compared to standard GCN and GAT models. At its core, RGCN re rn the hidden representations for each node through a weighted aggregation of features in a closed neighborhood where the weights are trainable. RGCN <ref type=""bibr"" target=""#b25"">(Zhu et al. 2019</ref>): This is a recently proposed ap-proach that explicitly enhances the robustness of GCNs. RGCN mo omparison, GAT appears to be the most sensitive to random structural perturbations and its low performance strongly corroborates with the findings in <ref type=""bibr"" target=""#b25"">(Zhu et al. 2019</ref>).</p><p>(ii) DICE Attack: In this challenging attack, where the attacker can both delete and add can defend against specific types of graph attack <ref type=""bibr"" target=""#b26"">(Zügner, Akbarnejad, and Günnemann 2018)</ref>. Recently, Zhu et al. <ref type=""bibr"" target=""#b25"">(Zhu et al. 2019</ref>) introduced a robust variant of GCN based on a variance-weighted attention mechanism, and showed",1
"2014;</ref><ref type=""bibr"" target=""#b18"">Szegedy et al. 2013)</ref> and their countermeasures <ref type=""bibr"" target=""#b13"">(Ren et al. 2020;</ref><ref type=""bibr"" target=""#b3"">Chakraborty et al. 2018)</ref>, designing attack strategies for graphs is a more recent topic of research. In general, d th the graph structure and node features <ref type=""bibr"" target=""#b23"">(Wu et al. 2020)</ref>. While GNNs based on spectral convolutional approaches <ref type=""bibr"" target=""#b3"">(Bruna et al. 2013;</ref><ref type=""bibr"" target=""#b6"">Defferrard, Bresson, and Vandergheynst 2016;</ref><ref type=""bibr",0
"ior performance of UM-GNN even against targeted attacks. The implementations for Mettack, PGD and FGA were based on the publicly available DeepRobust <ref type=""bibr"" target=""#b11"">(Jin et al. 2020)</ref> library. Due to the lack of computationally efficient implementations, we could not generate th ed in <ref type=""bibr"" target=""#b26"">(Zügner, Akbarnejad, and Günnemann 2018)</ref>. Since then, several graph adversarial attacks have been proposed <ref type=""bibr"" target=""#b11"">(Jin et al. 2020;</ref><ref type=""bibr"" target=""#b17"">Sun et al. 2018)</ref>. Adversarial attacks on graphs can be broa et=""#b22"">(Wu et al. 2019)</ref>, edges with low Jaccard similarity between the constituent nodes were removed prior to training a GNN. Similarly, in <ref type=""bibr"" target=""#b11"">(Jin et al. 2019)</ref>, explicit graph smoothing was performed by training on a family of graphs to defend against eva",0
", there also exists models that implement convolutions directly using spatial neighborhoods <ref type=""bibr"" target=""#b7"">(Duvenaud et al. 2015;</ref><ref type=""bibr"" target=""#b0"">Atwood and Towsley 2016;</ref><ref type=""bibr"" target=""#b10"">Hamilton, Ying, and Leskovec 2017)</ref>. The vulnerability",0
"r"" target=""#b23"">(Wu et al. 2020)</ref>. While GNNs based on spectral convolutional approaches <ref type=""bibr"" target=""#b3"">(Bruna et al. 2013;</ref><ref type=""bibr"" target=""#b6"">Defferrard, Bresson, and Vandergheynst 2016;</ref><ref type=""bibr"" target=""#b12"">Kipf and Welling 2017)</ref> have been",0
"target=""#b3"">Caliskan et al., 2017;</ref><ref type=""bibr"" target=""#b7"">Garg et al., 2017;</ref><ref type=""bibr"" target=""#b19"">May et al., 2010;</ref><ref type=""bibr"" target=""#b38"">Zhao et al., 2018;</ref><ref type=""bibr"" target=""#b27"">Rudinger et al., 2017)</ref>. Models that have learnt representa",1
", follow-up work by <ref type=""bibr"" target=""#b11"">Gonen and Goldberg (2019)</ref> shows that this method only hides the bias and does not remove it. <ref type=""bibr"" target=""#b15"">Liang et al. (2020)</ref> introduce a debiasing algorithm and they report lower bias scores on the SEAT while maintaini",0
"</ref>.</p><p>We never mask the modified tokens: those that differ between the two sentences, shown in grey. The full data statement is in Appendix A <ref type=""bibr"" target=""#b1"">(Bender and Friedman, 2018)</ref>.</p><p>In Table <ref type=""table"" target=""#tab_0"">1</ref> we provide examples from eac",0
"s well with crowd judgements. <ref type=""bibr"" target=""#b26"">Rozado (2020)</ref>   <ref type=""bibr"" target=""#b5"">(Devlin et al., 2019)</ref> and ELMo <ref type=""bibr"" target=""#b24"">(Peters et al., 2018)</ref> for the angry black woman and double bind stereotypes. However they find no clear patterns",0
"es are infrequent.</p><p>There are also some examples in the dataset where the disagreement on the label for bias type should not be considered noise <ref type=""bibr"" target=""#b22"">(Pavlick and Kwiatkowski, 2019)</ref>. For example, <ref type=""bibr"">(5)</ref> [Chinese/American] people are known to e",0
"More task details and hyperparameters setting are reported in Appendix A.1.</p><p>End-to-End Entity Linking (EL) For EL, we reproduce the setting of <ref type=""bibr"" target=""#b27"">Kolitsas et al. (2018)</ref> using the same in-domain and out-of-domain datasets as well as evaluating the InKB micro-F return a set of tuples m i , e i where m i is a entity mentions (a span contained in d j ) and e i ∈ E its corresponding entity in the KB. Following <ref type=""bibr"" target=""#b27"">Kolitsas et al. (2018)</ref>, we considered only mentions that have entities in the KB (i.e., Wikipedia) and we used th setting the learning rate and the optimizer statistics for 10k steps and we do model selection on the validation set. Again, following previous works <ref type=""bibr"" target=""#b27"">(Kolitsas et al., 2018)</ref>, we considered only mentions that have entities in Wikipedia. Training was done on 64 GPU",1
"entity retrieval (e.g. <ref type=""bibr"" target=""#b18"">Hoffart et al., 2011;</ref><ref type=""bibr"" target=""#b48"">Piccinno &amp; Ferragina, 2014;</ref><ref type=""bibr"" target=""#b21"">Huang et al., 2015;</ref><ref type=""bibr"" target=""#b30"">Le &amp; Titov, 2018;</ref><ref type=""bibr"" target=""#b37"">Loges",0
"iaQA <ref type=""bibr"" target=""#b24"">(Joshi et al., 2017)</ref>, ELI5 <ref type=""bibr"" target=""#b12"">(Fan et al., 2019)</ref>; slot filling with T-REx <ref type=""bibr"" target=""#b11"">(Elsahar et al., 2018)</ref>, Zero Shot RE <ref type=""bibr"" target=""#b33"">(Levy et al., 2017)</ref>; entity disambiguat",0
"N3-Reuters-128 (R128), N3-RSS-500 (R500) <ref type=""bibr"" target=""#b51"">(Röder et al., 2014)</ref>, and OKE challenge 2015 and 2016 (OKE15 and OKE16) <ref type=""bibr"" target=""#b42"">(Nuzzolese et al., 2015)</ref>. More task details and hyperparameters setting are reported in Appendix A.2.</p><p>Page-",0
"h for entity retrieval, it requires a limited list of candidates documents, obtained with BM25 for instance, in order to be computationally possible. <ref type=""bibr"" target=""#b39"">Massarelli et al. (2019)</ref>; <ref type=""bibr"" target=""#b46"">Petroni et al. (2020a)</ref> explore the idea of using a",0
"et al., 2020)</ref>, semantic role labelling <ref type=""bibr"" target=""#b7"">(Daza &amp; Frank, 2018)</ref>, discourse representation structure parsing <ref type=""bibr"" target=""#b36"">(Liu et al., 2018)</ref>  <ref type=""bibr"" target=""#b28"">(Konstas et al., 2017)</ref>. In these works a structured repr",0
"estions <ref type=""bibr"" target=""#b29"">(Kwiatkowski et al., 2019)</ref>, HotpotQA <ref type=""bibr"" target=""#b70"">(Yang et al., 2018c)</ref>, TriviaQA <ref type=""bibr"" target=""#b24"">(Joshi et al., 2017)</ref>, ELI5 <ref type=""bibr"" target=""#b12"">(Fan et al., 2019)</ref>; slot filling with T-REx <ref",0
"br"" target=""#b14"">(Ferrucci, 2012;</ref><ref type=""bibr"" target=""#b5"">Chen et al., 2017;</ref><ref type=""bibr"" target=""#b35"">Lewis et al., 2020;</ref><ref type=""bibr"" target=""#b53"">Roller et al., 2020)</ref>.</p><p>Although there has been extensive previous work on entity retrieval (e.g. <ref type=""",0
"bility to retrieve the correct entity from large Knowledge Bases (KBs) given a textual input is a fundamental building block for several applications <ref type=""bibr"" target=""#b14"">(Ferrucci, 2012;</ref><ref type=""bibr"" target=""#b56"">Slawski, 2015;</ref><ref type=""bibr"" target=""#b69"">Yang et al., 20 etrieval components to surface specific KB entries (e.g., Wikipedia articles) to find knowledge for sustaining a conversation or answering a question <ref type=""bibr"" target=""#b14"">(Ferrucci, 2012;</ref><ref type=""bibr"" target=""#b5"">Chen et al., 2017;</ref><ref type=""bibr"" target=""#b35"">Lewis et al.",0
"s recently shown to be a highly effective way of learning visual representations in a self-supervised manner <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b31"">30]</ref>. Pushing the embeddings of two transformed versions of the same image (forming the positive pair) close to ea s used to evaluate the quality of visual representations learned in an unsupervised way. It is however shown <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b31"">30]</ref> that increasing the memory/batch size leads to diminishing returns in terms of performance: more negative sam ave witnessed a surge of successful approaches that also use contrastive learning losses. These include MoCo <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b31"">30]</ref>, SimCLR <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b11"">12]</ref>, PIRL <ref type=""bi e=""bibr"" target=""#b78"">77]</ref> use memories that contain the whole training set, while the recent Momentum Contrast (or MoCo) approach of He et al. <ref type=""bibr"" target=""#b31"">[30]</ref> keeps a queue with features of the last few batches as memory. The MoCo approach with the modifications pres negatives, has so far been neglected in the context of self-supervised representation learning. We delve deeper into learning with a momentum encoder <ref type=""bibr"" target=""#b31"">[30]</ref> and show evidence that harder negatives are required to facilitate better and faster learning. Based on thes mlns=""http://www.tei-c.org/ns/1.0""><head>Contributions. a)</head><p>We delve deeper into a top-performing contrastive self-supervised learning method <ref type=""bibr"" target=""#b31"">[30]</ref> and observe the need for harder negatives; b) We propose hard negative mixing, i.e. to synthesize hard negat e query q and key k embeddings form the positive pair, which is contrasted with every feature n in the bank of negatives (Q) also called the queue in <ref type=""bibr"" target=""#b31"">[30]</ref>. A popular and highly successful loss function for contrastive learning <ref type=""bibr"" target=""#b10"">[11,< ref type=""bibr"" target=""#b47"">[46,</ref><ref type=""bibr"" target=""#b65"">64,</ref><ref type=""bibr"" target=""#b78"">77]</ref>, a queue of the last batches <ref type=""bibr"" target=""#b31"">[30]</ref>, or simply be every other image in the current minibatch <ref type=""bibr"" target=""#b10"">[11]</ref>.</p><p>Th esentations and the computational overhead for re-computing them as the encoder keeps changing. The Momentum Contrast (or MoCo) approach of He et al. <ref type=""bibr"" target=""#b31"">[30]</ref> offers a compromise between the two negative sampling extremes: it keeps a queue of the latest K features fr much higher momentum. For MoCo, the key feature k and all features in Q are encoded with the key encoder.</p><p>How hard are MoCo negatives? In MoCo <ref type=""bibr"" target=""#b31"">[30]</ref> (resp. SimCLR <ref type=""bibr"" target=""#b10"">[11]</ref>) the authors show that increasing the memory (resp. t most of the memory negatives are practically not helping a lot towards learning the proxy task.</p><p>On the difficulty of the proxy task. For MoCo <ref type=""bibr"" target=""#b31"">[30]</ref>, SimCLR <ref type=""bibr"" target=""#b10"">[11]</ref>, InfoMin <ref type=""bibr"" target=""#b66"">[65]</ref>, and ot re"">2b</ref>, we plot the proxy task performance, i.e. the percentage of queries where the key is ranked over all negatives, across training for MoCo <ref type=""bibr"" target=""#b31"">[30]</ref> and MoCo-v2 <ref type=""bibr"" target=""#b12"">[13]</ref>. MoCo-v2 enjoys a high performance gain over MoCo by t For ImageNet-1K, we report accuracy for a single-crop testing. For object detection on PASCAL VOC <ref type=""bibr"" target=""#b18"">[19]</ref> we follow <ref type=""bibr"" target=""#b31"">[30]</ref> and fine-tune a Faster R-CNN <ref type=""bibr"" target=""#b55"">[54]</ref>, R50-C4 on trainval07+12 and test on test2007. We use the open-source detectron2<ref type=""foot"" target=""#foot_2"">3</ref> code and report the common AP, AP50 and AP75 metrics. Similar to <ref type=""bibr"" target=""#b31"">[30]</ref>, we do not perform hyperparameter tuning for the object detection task. See the Appendix for more implementa we present results for object detection and semantic segmentation on the COCO dataset <ref type=""bibr"" target=""#b42"">[41]</ref>. Following He et al. <ref type=""bibr"" target=""#b31"">[30]</ref>, we use Mask R-CNN <ref type=""bibr"" target=""#b29"">[29]</ref> with a C4 backbone, with batch normalization tu 800 at inference. We fine-tune all layers end-to-end on the train2017 set (118k images) and evaluate on val2017. We adopt feature normalization as in <ref type=""bibr"" target=""#b31"">[30]</ref> when fine-tuning. MoCHi and MoCo use the same hyper-parameters as the ImageNet supervised counterpart (i.e. ing to get large clean datasets already exists. Following our discussion in Section 3.2, we wanted to verify that hardness of the proxy task for MoCo <ref type=""bibr"" target=""#b31"">[30]</ref> is directly correlated to the difficulty of the transformations set, i.e. proxy task hardness can modulated g_3"">4</ref>, we plot the proxy task performance, i.e. the percentage of queries where the key is ranked over all negatives, across training for MoCo <ref type=""bibr"" target=""#b31"">[30]</ref>, MoCo-v2 <ref type=""bibr"" target=""#b12"">[13]</ref> and some variants inbetween. In Figure <ref type=""figure"" tudy, Purushwalkam and Gupta <ref type=""bibr"" target=""#b53"">[52]</ref> study the robustness of contrastive self-supervised learning methods like MoCo <ref type=""bibr"" target=""#b31"">[30]</ref> and PIRL <ref type=""bibr"" target=""#b47"">[46]</ref> and saw that despite the fact that they learn occlusionin ations over SimCLR <ref type=""bibr"" target=""#b10"">[11]</ref>, e.g. the addition of a target network whose parameter update is lagging similar to MoCo <ref type=""bibr"" target=""#b31"">[30]</ref>  Synthesizing for supervised metric learning. Recently, synthesizing negatives was explored in metric learni the fine-tuning part. For MoCHi runs we also report in parenthesis the difference to MoCo-v2. * denotes reproduced results. † results are copied from<ref type=""bibr"" target=""#b31"">[30]</ref>. We bold (resp. underline) the highest results overall (resp. for MoCHi).</note></figure> <figure xmlns=""htt ble /></figure> 			<note xmlns=""http://www.tei-c.org/ns/1.0"" place=""foot"" n=""1"" xml:id=""foot_0"">In this section we study contrastive learning for MoCo<ref type=""bibr"" target=""#b31"">[30]</ref> on ImageNet-100, a subset of ImageNet consisting of 100 classes introduced in<ref type=""bibr"" target=""#b65""> leverage data augmentations <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b12"">13,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b31"">30,</ref><ref type=""bibr"" target=""#b47"">46,</ref><ref type=""bibr"" target=""#b65"">64]</ref>. As revealed by recent studie type=""bibr"" target=""#b31"">[30]</ref>. A popular and highly successful loss function for contrastive learning <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b31"">30,</ref><ref type=""bibr"" target=""#b65"">64]</ref> is the following:</p><formula xml:id=""formula_0"">L q,k,Q = − log exp( is a temperature parameter and all embeddings are 2 -normalized. In a number of recent successful approaches <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b31"">30,</ref><ref type=""bibr"" target=""#b47"">46,</ref><ref type=""bibr"" target=""#b66"">65]</ref> the query and key are the emb esults. It is unfortunate that many recent self-supervised learning papers do not discuss variance ; in fact only papers from highly resourceful labs <ref type=""bibr"" target=""#b31"">[30,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b66"">65]</ref> report averaged results, but",1
", and achieve improved performance. Our work is also related to metric learning works that employ generators <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b88"">87]</ref>. Apart from not requiring labels, our method exploits the memory component, something not present in <ref typ ]</ref>. Apart from not requiring labels, our method exploits the memory component, something not present in <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b88"">87]</ref>. It has no extra parameters or loss terms that need to be optimized.</p><p>3 Understanding hard negatives in #b17"">[18,</ref><ref type=""bibr"" target=""#b88"">87,</ref><ref type=""bibr"" target=""#b36"">35]</ref>. Works like <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b88"">87]</ref> use generators to synthesize negatives in a supervised scenario over common metric learning losses. Apart fro our case we focus on a specific contrastive loss and exploit its memory component. What is more, and unlike <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b88"">87]</ref>, we do not require a generator, i.e. have no extra parameters or loss terms that need to be optimized. We dis for supervised metric learning. Recently, synthesizing negatives was explored in metric learning literature <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b88"">87,</ref><ref type=""bibr"" target=""#b36"">35]</ref>. Works like <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr""",1
"type=""bibr"" target=""#b48"">[47,</ref><ref type=""bibr"" target=""#b39"">38,</ref><ref type=""bibr"" target=""#b80"">79]</ref>, predicting the ""arrow of time"" <ref type=""bibr"" target=""#b75"">[74]</ref>, pace <ref type=""bibr"" target=""#b73"">[72]</ref> or predicting the ""odd"" element <ref type=""bibr"" target=""#b2",0
"e audio-visual nature of video <ref type=""bibr"" target=""#b38"">[37,</ref><ref type=""bibr"" target=""#b0"">1,</ref><ref type=""bibr"" target=""#b52"">51,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b2"">3]</ref> to learn visual representation, e.g. via learning intra-modality synchr",0
"ing. A number of recent approaches have explored the connections between supervised and contrastive learning <ref type=""bibr"" target=""#b35"">[34,</ref><ref type=""bibr"" target=""#b87"">86,</ref><ref type=""bibr"" target=""#b43"">42]</ref>. Very recently, Khosla et al. <ref type=""bibr"" target=""#b35"">[34]</re successfully perform supervised learning for classification, by simply discarding same-class negatives. This is something that is further utilized in <ref type=""bibr"" target=""#b87"">[86]</ref>. For MoCHi, we can further ensure that all hard negatives come from other classes. In the bottom section of",0
"39]</ref>, while Mutual Information theory has been the basis and inspiration for a number such studies and self-supervised learning algorithms, e.g. <ref type=""bibr"" target=""#b33"">[32,</ref><ref type=""bibr"" target=""#b69"">68,</ref><ref type=""bibr"" target=""#b77"">76,</ref><ref type=""bibr"" target=""#b5""",0
"ation on the COCO dataset <ref type=""bibr"" target=""#b42"">[41]</ref>. Following He et al. <ref type=""bibr"" target=""#b31"">[30]</ref>, we use Mask R-CNN <ref type=""bibr"" target=""#b29"">[29]</ref> with a C4 backbone, with batch normalization tuned and synchronize across GPUs. The image scale is in [640,",0
"es of a transformation (e.g. rotations, orderings, relative positions or channels) applied on a single image <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b37"">36,</ref><ref type=""bibr"" target=""#b49""",0
">Large pre-trained language models are the cornerstone of many state-of-the-art models in various natural language understanding and generation tasks <ref type=""bibr"" target=""#b5"">(Devlin et al., 2019;</ref><ref type=""bibr"" target=""#b31"">Liu et al., 2019;</ref><ref type=""bibr"" target=""#b27"">Lewis et fectively incorporate content plans into large models to generate more rele-vant and coherent text. We first study a planning model trained from BERT <ref type=""bibr"" target=""#b5"">(Devlin et al., 2019)</ref> to produce the initial content plan, which assigns keyphrases to different sentences and pre",1
"age understanding and generation tasks <ref type=""bibr"" target=""#b5"">(Devlin et al., 2019;</ref><ref type=""bibr"" target=""#b31"">Liu et al., 2019;</ref><ref type=""bibr"" target=""#b27"">Lewis et al., 2020</ref>), yet they are far from perfect. In generation tasks, although models like GPT-2 <ref type=""bi ons. Next, we propose a contentcontrolled text generation framework, built upon the pre-trained sequence-to-sequence (seq2seq) Transformer model BART <ref type=""bibr"" target=""#b27"">(Lewis et al., 2020)</ref>. As shown in Figure <ref type=""figure"" target=""#fig_0"">1</ref>, our generation model takes i puts <ref type=""bibr"" target=""#b12"">(Ghazvininejad et al., 2019;</ref><ref type=""bibr"" target=""#b25"">Lawrence et al., 2019)</ref>. Our work uses BART <ref type=""bibr"" target=""#b27"">(Lewis et al., 2020)</ref>, a state-of-the-art seq2seq model that offers better generalizability and stronger capacity training and decoding, we utilize the Titan RTX GPU card with 24 GB memory.</p><p>Model Sizes. Our generation model has the same architecture as BART <ref type=""bibr"" target=""#b27"">(Lewis et al., 2020)</ref> with 406M parameters. The content planner is built on top of BERT base , which has 110M para",1
"articles for testing, another 5k for validation, and the rest (23k for news and 10k for opinion) are used for training. We apply the BPE tokenization <ref type=""bibr"" target=""#b46"">(Sennrich et al., 2016)</ref> for the generation model as BART does, and use WordPiece <ref type=""bibr"" target=""#b54"">( p>Auto-Correction for Content Plan. When the content plan is predicted by the planner, the following post-processing steps are employed prior to the  <ref type=""bibr"" target=""#b46"">(Sennrich et al., 2016)</ref>. KP distance denotes the average number of tokens between two keyphrases that are in the",0
"section.</p><p>Our method is substantially different from prior work that uses constrained decoding to enforce words to appear at specific positions <ref type=""bibr"" target=""#b15"">(Hokamp and Liu, 2017;</ref><ref type=""bibr"" target=""#b39"">Post and Vilar, 2018;</ref><ref type=""bibr"">Hu et al., 2019)",0
"R correctly generate discourse markers? Since discourse markers are crucial for coherence <ref type=""bibr"" target=""#b14"">(Grote and Stede, 1998;</ref><ref type=""bibr"" target=""#b1"">Callaway, 2003)</ref> and have received dedicated research efforts in rulebased systems <ref type=""bibr"" target=""#b42"">(",0
"precision (up to 4-grams); ROUGE-L <ref type=""bibr"" target=""#b29"">(Lin, 2004)</ref>, measuring recall of the longest common subsequences; and METEOR <ref type=""bibr"" target=""#b24"">(Lavie and Agarwal, 2007)</ref>, which accounts for paraphrase. For our models PAIR full and PAIR light , we evaluate b",0
"ibr"" target=""#b47"">Stent et al., 2004)</ref>. Specially designed control codes and auxiliary planning modules have been integrated into neural models <ref type=""bibr"" target=""#b22"">(Keskar et al., 2019;</ref><ref type=""bibr"" target=""#b35"">Moryossef et al., 2019;</ref><ref type=""bibr"" target=""#b19"">H",0
"t=""#b26"">(Lee et al., 2018;</ref><ref type=""bibr"" target=""#b11"">Freitag et al., 2019;</ref><ref type=""bibr"" target=""#b33"">Mansimov et al., 2019;</ref><ref type=""bibr"" target=""#b21"">Kasai et al., 2020)</ref> to gradually improve translation quality. Refinement is also used with masked language models",0
"to denoise the masked input with contextual understanding, it naturally benefits our method.</p><p>Decoding. We employ the nucleus sampling strategy <ref type=""bibr"" target=""#b16"">(Holtzman et al., 2019)</ref>, which is shown to yield superior output quality in long text generation. In addition to",0
"a and harm model performance.</p><p>We propose SeqMix, a data augmentation method for generating sub-sequences along with their labels based on mixup <ref type=""bibr"" target=""#b46"">(Zhang et al., 2018)</ref>. Under the active sequence labeling framework, Se-qMix is capable of generating plausible ps above procedure in Algorithm 1.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.2"">Sequence Mixup in the Embedding Space</head><p>Mixup <ref type=""bibr"" target=""#b46"">(Zhang et al., 2018)</ref> is a data augmentation method that implements linear interpolation in the input space. Given tation for active sequence labeling.</p><p>Interpolation-based Regularizations Mixup implements interpolation in the input space to regularize models <ref type=""bibr"" target=""#b46"">(Zhang et al., 2018)</ref>. Recently, the Mixup variants <ref type=""bibr"" target=""#b40"">(Verma et al., 2019;</ref><ref nt rate setting, we finally choose 500 as the upper limit rather than a too narrow score range setting.</p><p>For the mixing coefficient λ, we follow <ref type=""bibr"" target=""#b46"">(Zhang et al., 2018)</ref> to sample it from Beta(α, α) and explore α ranging from <ref type=""bibr"">[0.5, 16]</ref>. We",1
"target=""#b23"">Liang et al., 2020;</ref><ref type=""bibr"" target=""#b30"">Ren et al., 2020;</ref><ref type=""bibr"" target=""#b47"">Zhang et al., 2019;</ref><ref type=""bibr"" target=""#b45"">Yu et al., 2020)</ref> and active learning <ref type=""bibr"" target=""#b34"">(Shen et al., 2017;</ref><ref type=""bibr"" tar",0
"target=""#b38"">Tomanek et al. (2007)</ref>; <ref type=""bibr"" target=""#b34"">Shen et al. (2017)</ref> select query samples based on data uncertainties; <ref type=""bibr"" target=""#b15"">Hazra et al. (2019)</ref> compute model-aware similarity to eliminate redundant examples and improve the diversity of q et al., 2019;</ref><ref type=""bibr"" target=""#b45"">Yu et al., 2020)</ref> and active learning <ref type=""bibr"" target=""#b34"">(Shen et al., 2017;</ref><ref type=""bibr"" target=""#b15"">Hazra et al., 2019;</ref><ref type=""bibr"" target=""#b25"">Liu et al., 2018;</ref><ref type=""bibr"" target=""#b11"">Fang et a",0
"sed on specific task, <ref type=""bibr"" target=""#b16"">Hu et al. (2017)</ref> propose to augment text data in an encoder-decoder manner. Very recently, <ref type=""bibr"" target=""#b1"">(Anaby-Tavor et al., 2020;</ref><ref type=""bibr"" target=""#b20"">Kobayashi, 2018)</ref> harness the power of pre-trained l",0
"put as f (•|x; θ). Our method is generic to any query policies γ(•). Below, we introduce several representative policies.</p><p>Least Confidence (LC) <ref type=""bibr"" target=""#b7"">Culotta and McCallum (2005)</ref> measure the uncertainty of sequence models by the most likely predicted sequence. For which select samples based on the query policy design. So far, various uncertainty-based <ref type=""bibr"" target=""#b31"">(Scheffer et al., 2001;</ref><ref type=""bibr"" target=""#b7"">Culotta and McCallum, 2005;</ref><ref type=""bibr"" target=""#b19"">Kim et al., 2006)</ref> and committee-based approaches <",0
"etween the variables of the multivariate time series input. They use and evaluate their method only for the task of anomaly detection.</p><p>Finally, <ref type=""bibr"" target=""#b15"">Jansen et al. (2018)</ref> rely on a triplet loss and the idea of temporal proximity (the loss rewards similarity of re",0
"w.tei-c.org/ns/1.0""><head n=""4.2"">CLASSIFICATION</head><p>We select a set of 11 multivariate datasets from the UEA Time Series Classification Archive <ref type=""bibr"" target=""#b1"">(Bagnall et al., 2018)</ref> with diverse characteristics in terms of the number, dimensionality and length of time seri",0
"Autoencoding into this approach, but focused on clustering and the visualization of shifting sample topology with time. As an example of the lat-ter, <ref type=""bibr"" target=""#b24"">Malhotra et al. (2017)</ref> presented a multi-layered RNN sequence-to-sequence autoencoder, while <ref type=""bibr"" tar",0
"ple of the lat-ter, <ref type=""bibr"" target=""#b24"">Malhotra et al. (2017)</ref> presented a multi-layered RNN sequence-to-sequence autoencoder, while <ref type=""bibr"" target=""#b22"">Lyu et al. (2018)</ref> developed a multi-layered LSTM with an attention mechanism and evaluated both an input reconstr",0
"f reliable alignment information that bridges different KGs. Recent works on multilingual KG embeddings provide support for automated entity matching <ref type=""bibr"" target=""#b5"">(Chen et al., 2017</ref><ref type=""bibr"" target=""#b4"">(Chen et al., , 2018b;;</ref><ref type=""bibr"" target=""#b26"">Sun et <p>Multilingual KG Embeddings. Recent studies have extended embedding models to bridge multiple KGs, typically for KGs of multiple languages. MTransE <ref type=""bibr"" target=""#b5"">(Chen et al., 2017)</ref> jointly learns a transformation across two separate translational embedding spaces along with ns/1.0""><head n=""3.2"">Embedding Learning</head><p>The embedding learning process jointly trains the knowledge model and the alignment model following <ref type=""bibr"" target=""#b5"">Chen et al. (2017)</ref>, while self-learning is added to improve the alignment learning. The details are described belo ↔G j A</formula><p>is the alignment loss between G i and G j . λ is a positive hyperparameter that weights the two model components.</p><p>Following <ref type=""bibr"" target=""#b5"">Chen et al. (2017)</ref>, instead of directly optimizing J in Eq. ( <ref type=""formula"">5</ref>), our implementation opt",1
"the sample distribution.</p><p>Representative methods include AdaBoost <ref type=""bibr"" target=""#b10"">(Freund and Schapire, 1997)</ref> and RankBoost <ref type=""bibr"" target=""#b9"">(Freund et al., 2004)</ref>, which target at classification and ranking respectively. AdaBoost starts with a pool of wea s and compensating for alignment error. An embedding model that makes more accurate predictions should receive a higher weight. Inspired by RankBoost <ref type=""bibr"" target=""#b9"">(Freund et al., 2004)</ref>, we reduce the ranking combination problem to a classifier ensemble problem. KEnS b therefor (The Tale of Genji, genre, Love Story)} Queries: Q = {q1 = (The Tale of Genji, country, ?t) q2 = (The Tale of Genji, genre, ?t)} Similar to RankBoost <ref type=""bibr"" target=""#b9"">(Freund et al., 2004)</ref> Ranking loss. The overall objective of KEnS b is to minimize the sum of ranks of all correct cal entity pairs in the combined ranking list.</p><p>Let the set of all the critical entity pairs from all the validation queries of an entity as P . <ref type=""bibr"" target=""#b9"">Freund et al. (2004)</ref> have proved that, when using RankBoost, this ranking loss is bounded as follows:</p><formula",1
"ibr"" target=""#b38"">(Yang et al., 2015)</ref>, as well as neural models like HolE <ref type=""bibr"" target=""#b19"">(Nickel et al., 2016)</ref> and ConvE <ref type=""bibr"" target=""#b8"">(Dettmers et al., 2018)</ref>. Due to the large body of work in this line of research, we only provide a highly selectiv",0
"ledge-driven applications <ref type=""bibr"" target=""#b16"">(Koncel-Kedziorski et al., 2019;</ref><ref type=""bibr"" target=""#b3"">Chen et al., 2018a;</ref><ref type=""bibr"" target=""#b1"">Bordes et al., 2014)</ref>. Recently, extensive efforts have been invested in KG embedding models, which encode entities",0
"so include a baseline named RotatE+PARIS, which trains RotatE on 5 KGs and uses the representative non-embedding symbolic entity alignment tool PARIS <ref type=""bibr"" target=""#b24"">(Suchanek et al., 2011)</ref> for entity matching. PARIS delivered entity matching predictions for 58%-62% entities in",0
"ia <ref type=""bibr"" target=""#b17"">(Lehmann et al., 2015)</ref>, Wikidata <ref type=""bibr"" target=""#b33"">(Vrandečić and Krötzsch, 2014)</ref> and YAGO <ref type=""bibr"" target=""#b22"">(Rebele et al., 2016)</ref>. In contrast, KGs often consist of numerous entities that cannot be easily aligned, and ent",0
"/ref><ref type=""bibr"" target=""#b28"">Sun et al., 2019a</ref><ref type=""bibr"" target=""#b27"">Sun et al., , 2020a) )</ref> and degree centrality measures <ref type=""bibr"" target=""#b21"">(Pei et al., 2019)</ref>. A systematic summary of relevant approaches is given in a recent survey by <ref type=""bibr"" t",0
"ured representations of real-world entities and relations, constituting actionable knowledge that is crucial to various knowledge-driven applications <ref type=""bibr"" target=""#b16"">(Koncel-Kedziorski et al., 2019;</ref><ref type=""bibr"" target=""#b3"">Chen et al., 2018a;</ref><ref type=""bibr"" target=""#",0
"ly provide a highly selective summary here. Interested readers are referred to recent surveys <ref type=""bibr"" target=""#b34"">(Wang et al., 2017;</ref><ref type=""bibr"" target=""#b14"">Ji et al., 2020)</ref> for more information.</p><p>Multilingual KG Embeddings. Recent studies have extended embedding m",0
"network operators also cannot guarantee that the user packets are not detoured in transmission, numerous network attack surfaces are opened up today <ref type=""bibr"" target=""#b1"">[2]</ref>- <ref type=""bibr"" target=""#b3"">[4]</ref>. For example, an attacker may try to flood arbitrary packets from mul re are several proposals addressing both source and path validation that fill the void, such as ICING <ref type=""bibr"" target=""#b2"">[3]</ref> and OPT <ref type=""bibr"" target=""#b1"">[2]</ref>. However, for the targeting environment which is adversarial, high-speed, and variable-path, their protocols s lt upon the prototype using the normal IP routing performance of the Click router as the baseline, and compare our PSVM with the-state-of-the-art OPT <ref type=""bibr"" target=""#b1"">[2]</ref>.  Method and parameter setup. For fairness, we use the same 128-bit AES algorithm to compute the PSVM's authen both source authentication and path verification have been proposed in ICING <ref type=""bibr"" target=""#b2"">[3]</ref>, the Origin and Path Trace (OPT) <ref type=""bibr"" target=""#b1"">[2]</ref>, Orthogonal Sequence Verification (OSV) <ref type=""bibr"" target=""#b3"">[4]</ref>, and PPV <ref type=""bibr"" targ",1
"that the user packets are not detoured in transmission, numerous network attack surfaces are opened up today <ref type=""bibr"" target=""#b1"">[2]</ref>- <ref type=""bibr"" target=""#b3"">[4]</ref>. For example, an attacker may try to flood arbitrary packets from multiple spoofed sources to waste downstream PSVM architecture that separates general service functions from routing nodes, and its design is inspired by <ref type=""bibr"" target=""#b2"">[3]</ref>, <ref type=""bibr"" target=""#b3"">[4]</ref>, <ref type=""bibr"" target=""#b11"">[12]</ref>, some of which were actually used. Fig. <ref type=""figure"" target="" f type=""bibr"" target=""#b2"">[3]</ref>, the Origin and Path Trace (OPT) <ref type=""bibr"" target=""#b1"">[2]</ref>, Orthogonal Sequence Verification (OSV) <ref type=""bibr"" target=""#b3"">[4]</ref>, and PPV <ref type=""bibr"" target=""#b27"">[29]</ref>. In ICING, it requires each router to verify the optimized",0
"ing CGA. Then, CGA is able to derive a session symmetric key (Key Session N i</p><p>) from the node's master key (without seeking help from node N i) <ref type=""bibr"" target=""#b12"">[13]</ref> to calculate the authentication structure for source and path validation, which is treated as a path policy ocols <ref type=""bibr"" target=""#b14"">[15]</ref>, <ref type=""bibr"" target=""#b15"">[16]</ref>, or employing the existing control plane routing protocols <ref type=""bibr"" target=""#b12"">[13]</ref> (especially Pathlet <ref type=""bibr"" target=""#b16"">[17]</ref> or SCION <ref type=""bibr"" target=""#b17"">[18]</ to prevent cryptanalysis attacks. The session symmetric key of N i (Key Session N i</p><p>) can be derived through pseudo-random operation functions <ref type=""bibr"" target=""#b12"">[13]</ref> by N i and its CGA, respectively, without long-term storage or re-exchange. Finally, we assume all the nodes",0
".org/ns/1.0""><head>I. INTRODUCTION</head><p>It is inevitable that current networks are so popular that we cannot image our today's lives without them <ref type=""bibr"" target=""#b0"">[1]</ref>. Nevertheless, it is still very challenging to make sure a given policy has been properly implemented in curre",0
"t contents on the path and insert other loads such as malicious code.</p><p>Although a great deal of attention has been paid to source authentication <ref type=""bibr"" target=""#b6"">[7]</ref>, <ref type=""bibr"" target=""#b7"">[8]</ref>, the verification of a packet's actual path has been neglected by com",0
">[35,</ref><ref type=""bibr"" target=""#b3"">4]</ref>.</p><p>The maximum length limit in BERT naturally reminds us the limited capacity of Working Memory <ref type=""bibr"" target=""#b1"">[2]</ref>, a human cognitive system storing information for logical reasoning and decision-making. Experiments <ref type rmation"", and ""functions like a limited-capacity attentional system capable of selecting and operating control processes and strategies"", as Baddeley <ref type=""bibr"" target=""#b1"">[2]</ref> pointed out in his 1992 classic. Later research detailed that the contents in the working memory decay over ti",1
"in the same BERT input window in the sliding window method, hence we fail to answer the question.</p><p>Pretrained language models, pioneered by BERT <ref type=""bibr"" target=""#b11"">[12]</ref>, have emerged as silver bullets for many NLP tasks, such as question answering <ref type=""bibr"" target=""#b37 d><p>Challenge of long texts. The direct and superficial obstacle for long texts is that the pretrained max position embedding is usually 512 in BERT <ref type=""bibr"" target=""#b11"">[12]</ref>. However, even if the embeddings for larger positions are provided, the memory consumption is unaffordable b",1
"not essential for classification and is marked as ""irrelevant"".</p><p>4 Experiments In all experiments, the judge and reasoner are finetuned by Adam <ref type=""bibr"" target=""#b17"">[18]</ref> with learning rate 4 × 10 −5 and 10 −4 respectively. The learning rates warmup over the first 10% steps, and",0
"in the original relative ordering in x.</p><p>The key-blocks assumption is strongly related to latent variable models, which are usually solved by EM <ref type=""bibr"" target=""#b10"">[11]</ref> or variational bayes <ref type=""bibr"" target=""#b18"">[19]</ref>. However, these methods estimate the distribu latent variable models. Unsupervised CogLTX can be viewed as a generalization of (conditional) latent variable model p(y|x; θ) ∝ p(z|x)p(y|z; θ). EM <ref type=""bibr"" target=""#b10"">[11]</ref> infers the distribution of z as posterior p(z|y, x; θ) in E-step, while variational bayes methods <ref type=",0
"netune RoBERTa for 6 epochs in CogLTX.  <ref type=""bibr"" target=""#b15"">[16]</ref> 79.4 MS-CNN <ref type=""bibr"" target=""#b31"">[32]</ref> 86.1 Text GCN <ref type=""bibr"" target=""#b53"">[54]</ref> 86.3 MLP over BERT <ref type=""bibr"" target=""#b32"">[33]</ref> 85.5 LSTM over BERT <ref type=""bibr"" target=""#b",0
"g Memory <ref type=""bibr"" target=""#b1"">[2]</ref>, a human cognitive system storing information for logical reasoning and decision-making. Experiments <ref type=""bibr"" target=""#b26"">[27,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b30"">31]</ref> already showed that the workin",0
"at they can combine CogLTX to handle longer texts, so they will not be compared or discussed anymore in this paper. A detailed survey can be found in <ref type=""bibr"" target=""#b24"">[25]</ref>.</p><p>3 Method</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.1"">The CogLTX methodology</hea",0
"=""#b22"">[23]</ref>, and there are also previous works to extract important sentences in unsupervised ways, e.g. based on the metadata about structure <ref type=""bibr"" target=""#b23"">[24]</ref>. Experiments on 4 different large datasets show its competitive performance. CogLTX is expected to become a",0
"sumption is strongly related to latent variable models, which are usually solved by EM <ref type=""bibr"" target=""#b10"">[11]</ref> or variational bayes <ref type=""bibr"" target=""#b18"">[19]</ref>. However, these methods estimate the distribution of z and require multiple-sampling, thus not efficient eno here n, m are the number of blocks and the capacity of z respectively. In some cases, sampling for hundreds of times to train BERTs might be required <ref type=""bibr"" target=""#b18"">[19]</ref>, whose expensive time consumption force us turn to point estimation for z,<ref type=""foot"" target=""#foot_1""> z; θ). EM <ref type=""bibr"" target=""#b10"">[11]</ref> infers the distribution of z as posterior p(z|y, x; θ) in E-step, while variational bayes methods <ref type=""bibr"" target=""#b18"">[19,</ref><ref type=""bibr"" target=""#b38"">39]</ref> use an estimationfriendly q(z|y, x). However, in CogLTX z has a disc",0
"wer of the normalized adjacency matrix during the preprocessing step and performs standard logistic regression to remove redundant computation. PPRGo <ref type=""bibr"" target=""#b3"">[4]</ref> uses Personalized PageRank to capture multi-hop neighborhood information and uses a forward push algorithm <re peatedly perform multiplication of normalized adjacency matrix Ã and feature matrix X in the precomputation phase, which requires O(LmF ) time. PPRGo <ref type=""bibr"" target=""#b3"">[4]</ref> calculates approximate the Personalized PageRank (PPR) matrix ∞ =0 α(1 − α) Ã by forward push algorithm <ref t ich case P becomes the Personalized PageRank used in APPNP and PPRGo <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b3"">4]</ref>; 2) w = 0 for = 0, . . . , L − 1 and w L = 1, in which case P degenerates to the L-th transition probability ma ]</ref>, GraphSAINT (graph sampling) <ref type=""bibr"" target=""#b36"">[37]</ref>, SGC and PPRGo (linear model) <ref type=""bibr"" target=""#b29"">[30,</ref><ref type=""bibr"" target=""#b3"">4]</ref>.</p><p>We implement GBP in PyTorch and C++, and employ initial residual connection <ref type=""bibr"" target=""#b1",1
"To achieve high scalability, we borrow the idea of decoupling prediction and propagation from SGC <ref type=""bibr"" target=""#b29"">[30]</ref> and APPNP <ref type=""bibr"" target=""#b15"">[16]</ref>. In particular, we precompute the feature propagation with the following Generalized PageRank matrix <ref ty GCN <ref type=""bibr"" target=""#b14"">[15]</ref>, GAT <ref type=""bibr"" target=""#b28"">[29]</ref>, GDC <ref type=""bibr"" target=""#b16"">[17]</ref> and APPNP <ref type=""bibr"" target=""#b15"">[16]</ref> as the baselines for evaluation on small graphs. We also use one state-ofthe-art scalable GNN from each of t D r−1 AD −r represents the symmetric normalization adjacency matrix <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b29"">30,</ref><ref type=""bibr"" target=""#b15"">16]</ref>, the transition probability matrix AD −1 <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b s in this paper: 1) w = α(1 − α) for some constant decay factor α ∈ (0, 1), in which case P becomes the Personalized PageRank used in APPNP and PPRGo <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b3"">4]</ref>; 2) w = 0 for = 0, . . . , L −",0
"Go <ref type=""bibr"" target=""#b3"">[4]</ref> uses Personalized PageRank to capture multi-hop neighborhood information and uses a forward push algorithm <ref type=""bibr"" target=""#b1"">[2]</ref> to accelerate computation.</p><p>While the above methods significantly speed up the training time of GNNs, the PPRGo <ref type=""bibr"" target=""#b3"">[4]</ref> calculates approximate the Personalized PageRank (PPR) matrix ∞ =0 α(1 − α) Ã by forward push algorithm <ref type=""bibr"" target=""#b1"">[2]</ref> and then applies the PPR matrix to the feature matrix X to derive the propagation matrix. Let ε denote the err",0
".0""><head n=""4"">Experiments</head><p>Datasets. We use seven open graph datasets with different size: three citation networks Cora, Citeser and Pubmed <ref type=""bibr"" target=""#b24"">[25]</ref>, a Protein-Protein interaction network PPI <ref type=""bibr"" target=""#b10"">[11]</ref>, a customer interaction",0
"d><p>Recently, the field of Graph Neural Networks (GNNs) has drawn increasing attention due to its wide range of applications such as social analysis <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" target=""#b27"">28]</ref>, biology <ref type=""bibr"" ta",0
"hts can be pessimistic in the non-anonymous case, where permutation equivariance is either learned from data <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b22"">23]</ref> or obtained by design <ref type=""bibr"" target=""#b23"">[24]</ref>. With node features acting as identifiers, MP ained by design <ref type=""bibr"" target=""#b23"">[24]</ref>. With node features acting as identifiers, MPNN were shown to become universal in the limit <ref type=""bibr"" target=""#b22"">[23]</ref>, which implies that they can solve the graph isomorphism testing problem if their size is allowed to depend the limit. Along those lines, recent work provided evidence that the power of MPNN grows as a function of depth and width for certain graph problems <ref type=""bibr"" target=""#b22"">[23]</ref>, showing that (both anonymous and non-anonymous) MPNN cannot solve many tasks when the product of their dept distinguish graphs. Even further, it is unclear whether depth and width needs to grow with the number of nodes solely in the worst-case (as proven in <ref type=""bibr"" target=""#b22"">[23]</ref>) or with certain probability over the input distribution.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0"" e cutstructure of the input graph. Communication capacity is an effective generalization of the previously considered product between depth and width <ref type=""bibr"" target=""#b22"">[23]</ref>, being able to consolidate more involved properties, as well as to characterize MPNN with global state <ref from simpler arguments. In addition, the proposed lower bounds rely on a new technique which renders them applicable not only to worst-case instances <ref type=""bibr"" target=""#b22"">[23]</ref>, but in expectation over the input distribution.</p><p>An empirical study reveals strong qualitative and qua uracy (Table 2 in Appendix A), which corroborates previous theoretical findings that non-anonymous MPNN are universal and can solve graph isomorphism <ref type=""bibr"" target=""#b22"">[23,</ref><ref type=""bibr"" target=""#b4"">5]</ref>, as well as that they can learn to be permutation invariant <ref type=",1
"is built using injective aggregation functions (i.e., of unbounded width <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b3"">4,</ref><ref type=""bibr"" target=""#b46"">47]</ref>), the equivalence does not imply a relevant lower bound on the width/message-size/global-state-size of MPNN.",0
"r"" target=""#b11"">[12]</ref><ref type=""bibr"" target=""#b12"">[13]</ref><ref type=""bibr"" target=""#b13"">[14]</ref><ref type=""bibr"" target=""#b14"">[15]</ref><ref type=""bibr"" target=""#b15"">[16]</ref>.</p><p>Roughly two types of analyses of MPNN may be distinguished. The first bound the expressive power of a",0
"determine under what conditions current networks can (or perhaps cannot) distinguish between different graphs <ref type=""bibr"" target=""#b0"">[1]</ref><ref type=""bibr"" target=""#b1"">[2]</ref><ref type=""bibr"" target=""#b2"">[3]</ref><ref type=""bibr"" target=""#b3"">[4]</ref><ref type=""bibr"" target=""#b4"">[5]",0
"ssing neural networks (MPNN). Since its inception by Scarselli et al. <ref type=""bibr"" target=""#b6"">[7]</ref>, MPNN has been extended to include edge <ref type=""bibr"" target=""#b7"">[8]</ref> and global features <ref type=""bibr"" target=""#b8"">[9]</ref>. The model also encompasses many of the popular gr idth <ref type=""bibr"" target=""#b22"">[23]</ref>, being able to consolidate more involved properties, as well as to characterize MPNN with global state <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b27"">28]</ref> and adaptive architecture <ref t",0
"res and aggregating them within various graph neighborhoods via different mechanisms (e.g., averaging, LSTM) <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b35"">36]</ref>. However, in the real world, there are also settings where ""opposites target=""#b35"">[36]</ref> models the influence of different neighbors more precisely as a weighted average of the ego-and neighbor-features. GraphSAGE <ref type=""bibr"" target=""#b10"">[11]</ref> generalizes the aggregation beyond averaging, and models the ego-features distinctly from the neighbor-featu yers) is critical: a simple way to combine the ego-and the aggregated neighbor-embeddings without 'mixing' them is with concatenation as in GraphSAGE <ref type=""bibr"" target=""#b10"">[11]</ref>-rather than averaging all of them as in the GCN model by Kipf and Welling <ref type=""bibr"" target=""#b16"">[17 N <ref type=""bibr"" target=""#b16"">[17]</ref> GAT <ref type=""bibr"" target=""#b35"">[36]</ref> GCN-Cheby <ref type=""bibr"" target=""#b6"">[7]</ref> GraphSAGE <ref type=""bibr"" target=""#b10"">[11]</ref> MixHop <ref type=""bibr"" target=""#b0"">[1]</ref> H2GCN (proposed)</p><p>Comparison of H 2 GCN to existing GNN re critical in heterophily.</p><p>Non-linear embedding transformations per round in H 2 GCN? GCN <ref type=""bibr"" target=""#b16"">[17]</ref>, GraphSAGE <ref type=""bibr"" target=""#b10"">[11]</ref> and other GNN models embed the intermediate representations per round of feature propagation and aggregation Setup &amp; Hyperparameter Tuning</head><p>• GCN &amp; GCN-Cheby <ref type=""bibr"" target=""#b16"">[17]</ref>: https://github.com/tkipf/gcn • GraphSAGE <ref type=""bibr"" target=""#b10"">[11]</ref>: https://github.com/williamleif/graphsage-simple (PyTorch implementation) • MixHop <ref type=""bibr"" target="" } * dropout: 0.5 * weight_decay: 5e-4 * max_degree: 3 * early_stopping: 40 We report the best performance, for Set 1 with a = 64, b = 5e-4.• GraphSAGE<ref type=""bibr"" target=""#b10"">[11]</ref>:-hid_units: a ∈ {64, 128} lr: b ∈ {0.1, 0.7} epochs: 500</note> 		</body> 		<back>  			<div type=""acknowledg : 2 -early_stopping: 40 epochs: 2000 We also disabled the default feature normalization in the official implementation for this baseline. • GraphSAGE <ref type=""bibr"" target=""#b10"">[11]</ref>:</p><p>-hid_units: a ∈ {64, 128} lr: b ∈ {0.1, 0.7} epochs: 500 We report the performance with a = 128, b = 36]</ref>:</p><p>-hid_units: 8</p><p>We also disabled the default feature normalization in the official implementation for this baseline. • GraphSAGE <ref type=""bibr"" target=""#b10"">[11]</ref>:</p><p>-hid_units: 64 lr: {0.1, 0.7} epochs: 500</p><p>• MixHop <ref type=""bibr"" target=""#b0"">[1]</ref>:</p> gher-order neighborhoods; and (D3) combination of intermediate representations. While these designs have been utilized separately in some prior works <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b0"">1,</ref><ref type=""bibr"" target=""#b37"">38 1) + (|Y| − 2 + h)d)δ 1 (d + 1)(|Y| − 1 + (|Y|h − 1)d)</formula><p>Solving the above inequality for δ 1 , we get the amount of perturbation needed as <ref type=""bibr"" target=""#b10"">(11)</ref> and the least absolute amount of perturbation needed is |δ</p><formula xml:id=""formula_25"">δ 1 &gt; −h|Y|d−|",1
"ibr"" target=""#b17"">[18]</ref> replaces the adjacency matrix with a sparsified version of a diffusion matrix (e.g., heat kernel or PageRank). Geom-GCN <ref type=""bibr"" target=""#b25"">[26]</ref> precomputes unsupervised node embeddings and uses neighborhoods defined by geometric relationships in the re erophily (benchmarks with h ≤ 0.3), homophily (h ≥ 0.7), and across the full spectrum (""Overall""). The ""*"" denotes ranks based on results reported in <ref type=""bibr"" target=""#b25"">[26]</ref>. Real datasets &amp; setup We now evaluate the performance of our model and existing GNNs on a variety of re , class labels, and 10 random splits (48%/32%/20% of nodes per class for train/validation/test<ref type=""foot"" target=""#foot_2"">2</ref> ) provided by <ref type=""bibr"" target=""#b25"">[26]</ref>.</p><p>For Cora-Full, we generate 3 random splits, with 25%/25%/50% of nodes per class for train/validation/ n Table <ref type=""table"">4</ref>, we also report the best results among the three recentlyproposed GEOM-GCN variants ( § 4), directly from the paper <ref type=""bibr"" target=""#b25"">[26]</ref>: other models (including ours) outperform this method significantly under heterophily. We note that MLP is a senting links between web pages of the corresponding universities, originally collected by the CMU WebKB project. We used the preprocessed version in <ref type=""bibr"" target=""#b25"">[26]</ref>. In these networks, nodes are web pages, which are classified into 5 categories: course, faculty, student, p e corresponding topics, collected by <ref type=""bibr"" target=""#b28"">[29]</ref>. For the classification task, we utilize the class labels generated by <ref type=""bibr"" target=""#b25"">[26]</ref>, where the nodes are categorized into 5 classes based on the amount of their average traffic. • Actor is a g zed into 5 classes based on the amount of their average traffic. • Actor is a graph representing actor co-occurrence in Wikipedia pages, processed by <ref type=""bibr"" target=""#b25"">[26]</ref> based on the film-director-actor-writer network in <ref type=""bibr"" target=""#b34"">[35]</ref>. We also use th 5"">[26]</ref> based on the film-director-actor-writer network in <ref type=""bibr"" target=""#b34"">[35]</ref>. We also use the class labels generated by <ref type=""bibr"" target=""#b25"">[26]</ref>. • Cora, Pubmed and Citeseer are citation graphs originally introduced in <ref type=""bibr"" target=""#b29"">[30 ><figDesc>Real data: mean accuracy ± stdev over different data splits. Best model per benchmark highlighted in gray. The ""*"" results are obtained from<ref type=""bibr"" target=""#b25"">[26]</ref> and ""N/A"" denotes non-reported results.</figDesc><table><row><cell></cell><cell>Texas</cell><cell>Wisconsin< 2  , where I is the identity and D the degree matrix of A + I.</note> 			<note xmlns=""http://www.tei-c.org/ns/1.0"" place=""foot"" n=""2"" xml:id=""foot_2""><ref type=""bibr"" target=""#b25"">[26]</ref> claims that the ratios are 60%/20%/20%, which is different from the actual data splits shared on GitHub.</no ined by geometric relationships in the resulting latent space to define graph convolution. Some of these works <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b11"">12]</ref> acknowledge the challenges of learning from graphs with heterophily.",1
"d separately in some prior works <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b0"">1,</ref><ref type=""bibr"" target=""#b37"">38]</ref>, we are the first to discuss their importance under heterophily by providing novel theoretical justifications xplicitly capture local and global information via COMBINE functions that leverage each representation separately-e.g., concatenation, LSTM-attention <ref type=""bibr"" target=""#b37"">[38]</ref>. This design is introduced in jumping knowledge networks <ref type=""bibr"" target=""#b37"">[38]</ref> and shown ion separately-e.g., concatenation, LSTM-attention <ref type=""bibr"" target=""#b37"">[38]</ref>. This design is introduced in jumping knowledge networks <ref type=""bibr"" target=""#b37"">[38]</ref> and shown to increase the representation power of GCNs under homophily.</p><p>Intuition. Intuitively, each r ns. By concatenating the intermediate representations from two rounds with the embedded ego-representation (following the jumping knowledge framework <ref type=""bibr"" target=""#b37"">[38]</ref>), GCN's accuracy increases to 58.93%±3.17 for h = 0.1, a 20% improvement over its counterpart without design v = COMBINE r (0) v , r (1) v , . . . , r (K) v ,<label>(7)</label></formula><p>where we empirically find concatenation works better than max-pooling <ref type=""bibr"" target=""#b37"">[38]</ref> as the COMBINE function.</p><p>In the classification stage (S3), the node is classified based on its final e D3) Combination of Intermediate Representations. We compare GraphSAGE, GCN-Cheby and GCN to their corresponding variants enhanced with JK connections <ref type=""bibr"" target=""#b37"">[38]</ref>. GCN and GCN-Cheby benefit significantly from D3 in heterophily: their average ranks improve (9.8 vs. 7.2 an aphSAGE employs. Under homophily, the performance with and without JK connections is similar (gaps mostly less than 2%), matching the observations in <ref type=""bibr"" target=""#b37"">[38]</ref>.</p><p>While other design choices and implementation details may confound a comparative evaluation of D1-D3 5e-5, 1e- For GCN+JK, GCN-Cheby+JK and GraphSAGE+JK, we enhanced the corresponding base model with jumping knowledge (JK) connections using JK-Concat <ref type=""bibr"" target=""#b37"">[38]</ref> without changing the number of layers or other hyperparameters for the base method.</p></div> <div xmlns=""ht .5}</p><p>For GCN+JK, GCN-Cheby+JK and GraphSAGE+JK, we enhanced the corresponding base model with jumping knowledge (JK) connections using JK-Concat <ref type=""bibr"" target=""#b37"">[38]</ref> without changing the number of layers or other hyperparameters for the base method.</p><p>Cora Full Benchmar",1
"=""bibr"" target=""#b42"">[43]</ref> method models more complex label correlations by integrating the compatibility matrix notion from belief propagation <ref type=""bibr"" target=""#b9"">[10]</ref> into GNNs. GCN <ref type=""bibr"" target=""#b16"">[17]</ref> GAT <ref type=""bibr"" target=""#b35"">[36]</ref> GCN-Ch learning, which can be used for graphs exhibiting homophily or heterophily <ref type=""bibr"" target=""#b18"">[19]</ref> and has fast linearized versions <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b7"">8]</ref>. Different from the setup where GNNs are employed, BP does not by itsel",0
"homophily principle by propagating features and aggregating them within various graph neighborhoods via different mechanisms (e.g., averaging, LSTM) <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b35"">36]</ref>. However, in the real world, bedding function f . A typical definition of neighborhood is N 1 (v)-i.e., the 1-hop neighbors of v. As for f , in graph convolutional networks (GCN) <ref type=""bibr"" target=""#b16"">[17]</ref> each node repeatedly averages its own features and those of its neighbors to update its own feature represen ne that relies solely on the node features for classification (differences in accuracy of MLP for different h are due to randomness). Especially, GCN <ref type=""bibr"" target=""#b16"">[17]</ref> and GAT <ref type=""bibr"" target=""#b35"">[36]</ref> show up to 42% worse performance than MLP, highlighting th ith concatenation as in GraphSAGE <ref type=""bibr"" target=""#b10"">[11]</ref>-rather than averaging all of them as in the GCN model by Kipf and Welling <ref type=""bibr"" target=""#b16"">[17]</ref>.</p><p>Intuition. In heterophily settings, by definition (Dfn. 2), the class label y v and original features x u ) : u ∈ N (v)} (esp. the direct neighbors N1 (v)) may be different. However, the typical GCN design that mixes the embeddings through an average <ref type=""bibr"" target=""#b16"">[17]</ref> or weighted average <ref type=""bibr"" target=""#b35"">[36]</ref> as the COMBINE function results in final embed lex label correlations by integrating the compatibility matrix notion from belief propagation <ref type=""bibr"" target=""#b9"">[10]</ref> into GNNs. GCN <ref type=""bibr"" target=""#b16"">[17]</ref> GAT <ref type=""bibr"" target=""#b35"">[36]</ref> GCN-Cheby <ref type=""bibr"" target=""#b6"">[7]</ref> GraphSAGE <r sion or not of designs D1-D3. Here we give some additional conceptual and mechanism differences.</p><p>As we have mentioned, H 2 GCN differs from GCN <ref type=""bibr"" target=""#b16"">[17]</ref> in a number of ways: <ref type=""bibr"" target=""#b0"">(1)</ref> In each round of propagation/aggregation, GCN "" der to represent the high-frequency components that are critical in heterophily.</p><p>Non-linear embedding transformations per round in H 2 GCN? GCN <ref type=""bibr"" target=""#b16"">[17]</ref>, GraphSAGE <ref type=""bibr"" target=""#b10"">[11]</ref> and other GNN models embed the intermediate representat s on GitHub.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>F Experimental Setup &amp; Hyperparameter Tuning</head><p>• GCN &amp; GCN-Cheby <ref type=""bibr"" target=""#b16"">[17]</ref>: https://github.com/tkipf/gcn • GraphSAGE <ref type=""bibr"" target=""#b10"">[11]</ref>: https://github.com/will mension of Feature Embedding p: 64 -Non-linearity Function σ: ReLU -Dropout Rate: a ∈ {0, 0.5}</p><p>We report the best performance, for a = 0. • GCN <ref type=""bibr"" target=""#b16"">[17]</ref>:</p><p>-hidden1: a ∈ {16, 32, 64} -early_stopping: b ∈ {40, 100, 200} epochs: 2000</p><p>We report the best ><p>-hidden1: a ∈ {16, 32, 64} -early_stopping: b ∈ {40, 100, 200} epochs: 2000</p><p>We report the best performance, for a = 32, b = 40. • GCN-Cheby <ref type=""bibr"" target=""#b16"">[17]</ref>:</p><p>-Set 1:</p><p>-hidden1: 64 -early_stopping: {40, 100, 200} -weight_decay: {5e-5, 1e- For GCN+JK, GCN- ormalized, and we found the default normalization method hurts the performance significantly. We report the best performance, for a = 40. • GCN-Cheby <ref type=""bibr"" target=""#b16"">[17]</ref>:</p><p>-hidden1: 64 -max_degree: 2 -early_stopping: 40 epochs: 2000 We also disabled the default feature nor ension of Feature Embedding p: 64 -Non-linearity Function σ: {ReLU, None} -Dropout Rate: {0, 0.5} -L2 Regularization Weight: {1e-5, 1e-6}</p><p>• GCN <ref type=""bibr"" target=""#b16"">[17]</ref>: </p></div>			</div> 			<div type=""references"">  				<listBibl>  <biblStruct xml:id=""b0""> 	<analytic> 		<tit",0
", the higher-order neighborhoods may be homophily-dominant and thus provide more relevant context. This observation is also confirmed by recent works <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b5"">6]</ref> in the context of binary attribute prediction.</p><p>Theoretical Justifi",0
"classification task with an agreement task, co-training a model to predict whether pairs of nodes share the same label; Graph Markov Neural Networks <ref type=""bibr"" target=""#b26"">[27]</ref> model the joint label distribution with a conditional random field, trained with expectation maximization us",0
"likely to connect in protein structures, fraudsters are more likely to connect to accomplices than to other fraudsters in online purchasing networks <ref type=""bibr"" target=""#b23"">[24]</ref>. Since many existing GNNs assume strong homophily, they fail to generalize to networks with heterophily (or nt pairs of node classes, i.e., there is different tendency of connection between each pair of classes. For instance, in an online purchasing network <ref type=""bibr"" target=""#b23"">[24]</ref> with three classes-fraudsters, accomplices, and honest users-, fraudsters connect with higher probability to",0
"b20"">[21]</ref>. For example, friends are likely to have similar political beliefs or age, and papers tend to cite papers from the same research area <ref type=""bibr"" target=""#b22"">[23]</ref>. GNNs model the homophily principle by propagating features and aggregating them within various graph neighb",0
"n outside of immediate neighborhoods. For example, apart from MixHop <ref type=""bibr"" target=""#b0"">[1]</ref> (cf. § 3.1), Graph Diffusion Convolution <ref type=""bibr"" target=""#b17"">[18]</ref> replaces the adjacency matrix with a sparsified version of a diffusion matrix (e.g., heat kernel or PageRank",0
"head>Synthetic datasets &amp; setup</head><p>We generate synthetic graphs with various homophily ratios h (Tab. 3) by adopting an approach similar to <ref type=""bibr"" target=""#b15"">[16]</ref>. In App. G, we describe the data generation process, the experimental setup, and the data statistics in deta aph generation We generate synthetic graphs with various heterophily levels by adopting an approach similar to <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b15"">16]</ref>. In general, the synthetic graphs are generated by a modified preferential attachment process <ref type=""bibr",0
"odel the joint label distribution with a conditional random field, trained with expectation maximization using GNNs; Correlated Graph Neural Networks <ref type=""bibr"" target=""#b14"">[15]</ref> model the correlation structure in the residuals of a regression task with a multivariate Gaussian, and can",0
"graph neighborhoods via different mechanisms (e.g., averaging, LSTM) <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b35"">36]</ref>. However, in the real world, there are also settings where ""opposites attract"", leading to networks with hete each node repeatedly averages its own features and those of its neighbors to update its own feature representation. Using an attention mechanism, GAT <ref type=""bibr"" target=""#b35"">[36]</ref> models the influence of different neighbors more precisely as a weighted average of the ego-and neighbor-fea ssification (differences in accuracy of MLP for different h are due to randomness). Especially, GCN <ref type=""bibr"" target=""#b16"">[17]</ref> and GAT <ref type=""bibr"" target=""#b35"">[36]</ref> show up to 42% worse performance than MLP, highlighting that methods that work well under high homophily (h different. However, the typical GCN design that mixes the embeddings through an average <ref type=""bibr"" target=""#b16"">[17]</ref> or weighted average <ref type=""bibr"" target=""#b35"">[36]</ref> as the COMBINE function results in final embeddings that are similar across neighboring nodes (especially wi atibility matrix notion from belief propagation <ref type=""bibr"" target=""#b9"">[10]</ref> into GNNs. GCN <ref type=""bibr"" target=""#b16"">[17]</ref> GAT <ref type=""bibr"" target=""#b35"">[36]</ref> GCN-Cheby <ref type=""bibr"" target=""#b6"">[7]</ref> GraphSAGE <ref type=""bibr"" target=""#b10"">[11]</ref> MixHop com/williamleif/graphsage-simple (PyTorch implementation) • MixHop <ref type=""bibr"" target=""#b0"">[1]</ref>: https://github.com/samihaija/mixhop • GAT <ref type=""bibr"" target=""#b35"">[36]</ref>: https://github.com/PetarV-/GAT. (For large datasets, we make use of the sparse version provided by the auth tei-c.org/ns/1.0""><head>• MixHop [1]:</head><p>-hidden_dims_csv: a ∈ {64, 192} -adj_pows: 0, 1, 2</p><p>We report the performance with a = 192. • GAT <ref type=""bibr"" target=""#b35"">[36]</ref>:</p><p>-hid_units: a ∈ {8, 16, 32, 64} For syn-products, we test the following command-line arguments for ea tei-c.org/ns/1.0""><head>• MixHop [1]:</head><p>-hidden_dims_csv: a ∈ {64, 192} -adj_pows: 0, 1, 2</p><p>We report the performance with a = 192. • GAT <ref type=""bibr"" target=""#b35"">[36]</ref>:</p><p>-hid_units: 8</p><p>We also disabled the default feature normalization in the official implementation 4 lr: {0.1, 0.7} epochs: 500</p><p>• MixHop <ref type=""bibr"" target=""#b0"">[1]</ref>:</p><p>-hidden_dims_csv: {64, 192} -adj_pows: 0, 1, 2</p><p>• GAT <ref type=""bibr"" target=""#b35"">[36]</ref>:</p><p>-hid_units: 8</p><p>• MLP -Dimension of Feature Embedding p: 64 -Non-linearity Function σ: {ReLU, Non",0
"xhibiting homophily or heterophily <ref type=""bibr"" target=""#b18"">[19]</ref> and has fast linearized versions <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b7"">8]</ref>. Different from the setup where GNNs are employed, BP does not by itself leverage node features, and usually as",0
"While these designs have been utilized separately in some prior works <ref type=""bibr"" target=""#b10"">[11,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b0"">1,</ref><ref type=""bibr"" target=""#b37"">38]</ref>, we are the first to discuss their importance under heterophily by prov applied to different neighborhoods can be the same or different. This design-employed in GCN-Cheby <ref type=""bibr"" target=""#b6"">[7]</ref> and MixHop <ref type=""bibr"" target=""#b0"">[1]</ref>-augments the implicit aggregation over higher-order neighborhoods that most GNN models achieve through multipl ocess. To capture more graph information, other works generalize graph convolution outside of immediate neighborhoods. For example, apart from MixHop <ref type=""bibr"" target=""#b0"">[1]</ref> (cf. § 3.1), Graph Diffusion Convolution <ref type=""bibr"" target=""#b17"">[18]</ref> replaces the adjacency matr ref type=""bibr"" target=""#b35"">[36]</ref> GCN-Cheby <ref type=""bibr"" target=""#b6"">[7]</ref> GraphSAGE <ref type=""bibr"" target=""#b10"">[11]</ref> MixHop <ref type=""bibr"" target=""#b0"">[1]</ref> H2GCN (proposed)</p><p>Comparison of H 2 GCN to existing GNN models As shown in Table <ref type=""table"" target cribed in (1) and ( <ref type=""formula"" target=""#formula_2"">3</ref>)-(4) above. It explicitly considers higher-order neighborhoods up to N 2 , though <ref type=""bibr"" target=""#b0"">[1]</ref> defines the 2-hop neighborhoods as that including neighbors up to 2-hop away neighbors. In our framework, we d om/tkipf/gcn • GraphSAGE <ref type=""bibr"" target=""#b10"">[11]</ref>: https://github.com/williamleif/graphsage-simple (PyTorch implementation) • MixHop <ref type=""bibr"" target=""#b0"">[1]</ref>: https://github.com/samihaija/mixhop • GAT <ref type=""bibr"" target=""#b35"">[36]</ref>: https://github.com/Petar plementation for this baseline. • GraphSAGE <ref type=""bibr"" target=""#b10"">[11]</ref>:</p><p>-hid_units: 64 lr: {0.1, 0.7} epochs: 500</p><p>• MixHop <ref type=""bibr"" target=""#b0"">[1]</ref>:</p><p>-hidden_dims_csv: {64, 192} -adj_pows: 0, 1, 2</p><p>• GAT <ref type=""bibr"" target=""#b35"">[36]</ref>:</ e embeddings and uses neighborhoods defined by geometric relationships in the resulting latent space to define graph convolution. Some of these works <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b11"">12]</ref> acknowledge the challenges of ocess &amp; Setup</head><p>Synthetic graph generation We generate synthetic graphs with various heterophily levels by adopting an approach similar to <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b15"">16]</ref>. In general, the synthetic graphs are generated by a modified preferen eptual and mechanism differences.</p><p>As we have mentioned, H 2 GCN differs from GCN <ref type=""bibr"" target=""#b16"">[17]</ref> in a number of ways: <ref type=""bibr"" target=""#b0"">(1)</ref> In each round of propagation/aggregation, GCN ""mixes"" the ego-and neighbor-representations by repeatedly avera",0
"ges usually come at a high computational cost and high memory footprint, both of which grow quadratically with respect to the image height (or width) <ref type=""bibr"" target=""#b37"">[38]</ref>. In real-world applications like content-based image search <ref type=""bibr"" target=""#b53"">[54]</ref> or aut ef type=""bibr"" target=""#b49"">[50]</ref>, can be deployed for higher efficiency. This differentiates our method from early recurrent attention methods <ref type=""bibr"" target=""#b37"">[38]</ref> which adopt pure recurrent models. In addition, we focus on improving the computational efficiency under the arget=""#b11"">12,</ref><ref type=""bibr"" target=""#b7"">8]</ref>.</p><p>One similar work to our GFNet is the recurrent visual attention model proposed in <ref type=""bibr"" target=""#b37"">[38]</ref>. However, our method differs from it in two important aspects: 1) we adopt a flexible and general CNN-based n to be able to produce correct classification results with only a few class-discriminative patches, such as the head of a dog or the wings of a bird <ref type=""bibr"" target=""#b37"">[38,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b7"">8]</ref>. These regions are typically s",1
"tial redundancy. Recent research has revealed that considerable spatial redundancy occurs when inferring CNNs <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b61"">62]</ref>. Several approaches have been proposed to reduce the redundant computation in the spatial dimension. The OctC dynamically adjusts the number of executed layers for different image regions. The methods proposed in <ref type=""bibr"" target=""#b18"">[19]</ref> and <ref type=""bibr"" target=""#b61"">[62]</ref> skip the computation on some less important regions of feature maps. These works mainly reduce the spatial r",1
"bibr"" target=""#b46"">[47,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b60"">61,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b21"">22]</ref>. Recent works <ref type=""bibr"" target=""#b49"">[50,</ref><ref type=""bibr"" target=""#b22"">23]</ref> scale up the ref>, with 1.2 million images for training and 50,000 images for validation. We adopt the same data augmentation and pre-processing configurations as <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b56"">57]</ref>. In our implementation, we e bibr"" target=""#b39"">[40]</ref>, EfficientNet <ref type=""bibr"" target=""#b49"">[50]</ref>, ResNet <ref type=""bibr"" target=""#b13"">[14]</ref> and DenseNet <ref type=""bibr"" target=""#b21"">[22]</ref>. These networks serve as the two deep encoders in our methods. In Budgeted batch classification, for each so en units <ref type=""bibr"" target=""#b6"">[7]</ref> in the patch proposal network π. For ResNets <ref type=""bibr"" target=""#b13"">[14]</ref> and DenseNets <ref type=""bibr"" target=""#b21"">[22]</ref>, we adopt 1024 hidden units and remove the convolutional layer in π. This does not hurt the efficiency since gible compared with the two encoders. With regards to the recurrent classifier f C , for ResNets <ref type=""bibr"" target=""#b13"">[14]</ref>, DenseNets <ref type=""bibr"" target=""#b21"">[22]</ref> and RegNets <ref type=""bibr"" target=""#b39"">[40]</ref>, we use a GRU with 1024 hidden units. For MobileNets-V pre-trained models. For H × W fine-tuning, we use the same training hyper-parameters as the training process <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b39"">40,</ref><ref type=""bibr"" target=""#b49"">50,</ref><ref type=""bibr"" target=""#b15"" ial for the final performance of the Glance Step.</p><p>Stage I. We train all networks using a SGD optimizer <ref type=""bibr"" target=""#b13"">[14,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b55"">56]</ref> with a cosine learning rate annealing technique and a Nesterov moment",0
"ition, the attention mechanism is typically exploited to extract information from some task-relevant regions <ref type=""bibr"" target=""#b28"">[29,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b7"">8]",0
"ing, models are trained to concentrate on the related regions of the image when generating the word sequence <ref type=""bibr"" target=""#b62"">[63,</ref><ref type=""bibr"" target=""#b52"">53,</ref><ref type=""bibr"" target=""#b50"">51,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" target=""#b64""",0
"f><ref type=""bibr"" target=""#b34"">35]</ref> or quantizing the weights <ref type=""bibr"" target=""#b40"">[41,</ref><ref type=""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b24"">25]</ref>. Another technique is knowledge distillation <ref type=""bibr"" target=""#b14"">[15]</ref>, which trains a small",0
"b60"">61,</ref><ref type=""bibr"" target=""#b17"">18,</ref><ref type=""bibr"" target=""#b21"">22]</ref>. Recent works <ref type=""bibr"" target=""#b49"">[50,</ref><ref type=""bibr"" target=""#b22"">23]</ref> scale up the image resolution to 480×480 or even larger for higher accuracy. However, large images usually co",0
"ibr"" target=""#b32"">[33]</ref>.</p><p>Spatial redundancy. Recent research has revealed that considerable spatial redundancy occurs when inferring CNNs <ref type=""bibr"" target=""#b9"">[10,</ref><ref type=""bibr"" target=""#b61"">62]</ref>. Several approaches have been proposed to reduce the redundant comput ref type=""bibr"" target=""#b5"">[6]</ref> reduces the spatial resolution by using low-frequency features. The Spatially Adaptive Computation Time (SACT) <ref type=""bibr"" target=""#b9"">[10]</ref> dynamically adjusts the number of executed layers for different image regions. The methods proposed in <ref t f type=""bibr"" target=""#b58"">[59]</ref>, ProxylessNAS <ref type=""bibr"" target=""#b4"">[5]</ref>, SkipNet <ref type=""bibr"" target=""#b54"">[55]</ref>, SACT <ref type=""bibr"" target=""#b9"">[10]</ref>, GoogLeNet <ref type=""bibr"" target=""#b47"">[48]</ref> and MSDNet <ref type=""bibr"" target=""#b19"">[20]</ref>.  <",0
"=""#b2"">[3]</ref>, computation usually translates into latency and power consumption, which should be minimized for both safety and economical reasons <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b42"">43]</ref>.</p><p>In this paper, we see nd the region proposal network are treated as two independent modules. Therefore, most of the stateof-the-art light-weighted CNNs, such as MobileNets <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b42"">43,</ref><ref type=""bibr"" target=""#b15"">16]</ref>, CondenseNet <ref type=""bibr h works focus on reducing the inference cost of the networks. A promising direction is to develop efficient network architectures, such as MobileNets <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b42"">43,</ref><ref type=""bibr"" target=""#b15"">16]</ref>, CondenseNet <ref type=""bibr Phone XS Max (with Apple A12 Bionic) using TFLite <ref type=""bibr"" target=""#b0"">[1]</ref>. The single-thread mode with batch size 1 is used following <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b42"">43,</ref><ref type=""bibr"" target=""#b15"">16]</ref>. We first measure the time c",0
"riminative patches, such as the head of a dog or the wings of a bird <ref type=""bibr"" target=""#b37"">[38,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b7"">8]</ref>. These regions are typically smaller than the whole image, and thus require much less computational resources. =""bibr"" target=""#b28"">[29,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b11"">12,</ref><ref type=""bibr"" target=""#b7"">8]</ref>.</p><p>One similar work to our GFNet is the recurrent visual attention model proposed in <ref type=""bibr"" targe",0
"sting task, adjacent roads naturally interplay with each other. Current state-of-the-art models highly depend on Graph Convoluational Networks (GCNs) <ref type=""bibr"" target=""#b12"">[13]</ref> originated from the theory of Graph Fourier Transform (GFT). These models <ref type=""bibr"" target=""#b30"">[31 lied on the final output.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>Spectral Graph Convolution</head><p>The Spectral Graph Convolution <ref type=""bibr"" target=""#b12"">[13]</ref> is composed of three steps.</p><p>(1) The multivariate time-series input is projected to the spectral domain",1
"pe=""bibr"" target=""#b12"">[13]</ref> originated from the theory of Graph Fourier Transform (GFT). These models <ref type=""bibr"" target=""#b30"">[31,</ref><ref type=""bibr"" target=""#b16"">17]</ref> stack GCN and temporal modules (e.g., LSTM, GRU) directly, which only capture temporal patterns in the time d <ref type=""bibr"" target=""#b17"">18]</ref> and multivariate techniques <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b30"">31,</ref><ref type=""bibr"" target=""#b2"">3,</ref><ref type=""bibr"" target=""#b28"">2 TM <ref type=""bibr"" target=""#b25"">[26]</ref>, SFM <ref type=""bibr"" target=""#b31"">[32]</ref>, N-BEATS <ref type=""bibr"" target=""#b18"">[19]</ref>, DCRNN <ref type=""bibr"" target=""#b16"">[17]</ref>, LSTNet <ref type=""bibr"" target=""#b13"">[14]</ref>, ST-GCN <ref type=""bibr"" target=""#b30"">[31]</ref>, DeepSta",0
"of StemGNN on nine public datasets, ranging from traffic, energy and electrocardiogram domains with other state-of-the-art models, including FC-LSTM <ref type=""bibr"" target=""#b25"">[26]</ref>, SFM <ref type=""bibr"" target=""#b31"">[32]</ref>, N-BEATS <ref type=""bibr"" target=""#b18"">[19]</ref>, DCRNN <re a) Forecasting result for the 28th day</cell><cell></cell><cell cols=""2"">(b) Inter-country correlations</cell><cell></cell></row></table><note>FC-LSTM<ref type=""bibr"" target=""#b25"">[26]</ref> SFM<ref type=""bibr"" target=""#b31"">[32]</ref> N-BEATS<ref type=""bibr"" target=""#b18"">[19]</ref> TCN<ref type=""",0
"><ref type=""bibr"" target=""#b31"">32,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b17"">18]</ref> and multivariate techniques <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b20"">21,</ref><ref type=""bibr"" target=""#b16"">17,</ref><ref type=""bibr"" target=""#b30 stack of fully-connected layers with basis expansion.</p><p>Multivariate techniques consider a collection of multiple time-series as a unified entity <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b8"">9]</ref>. TCN <ref type=""bibr"" target=""#b2"">[3]</ref> is a representative work",0
"nk what is a ""good"" representation for graph-structured data. In particular, the Information Bottleneck (IB) <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b18"">19]</ref> provides a critical principle for representation learning: an optimal representation should contain the minim",1
"target=""#b43"">[44]</ref>, biology <ref type=""bibr"" target=""#b44"">[45]</ref>, geographical mapping <ref type=""bibr"" target=""#b45"">[46]</ref>, finances <ref type=""bibr"" target=""#b46"">[47]</ref> and recommender systems <ref type=""bibr"" target=""#b47"">[48]</ref>, because of their flexibility in modeling",0
"e edges of the graph also makes it prone to noise and adversarial attacks that target at the graph structure <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b15"">16]</ref>.</p><p>Here we address the above problems and rethink what is a ""good"" representation for graph-structured da renders our models extremely robust to structural perturbations/attacks where traditional GNNs are sensitive <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b15"">16]</ref>. Both our models also keep robustness to the feature perturbation that is similar to other IB-based DNN model NNs are far from robust and can be easily attacked by malicious manipulation on either features or structure <ref type=""bibr"" target=""#b14"">[15,</ref><ref type=""bibr"" target=""#b15"">16]</ref>. Accordingly, several defense models are proposed to increase the robustness by injecting random noise in the",0
"representations of graph-structured data for downstream tasks such as node classification and link prediction <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b1"">2]</ref>. Graph representation learning is a challenging task since both node features as well as graph structure carry",0
"target=""#b6"">[7]</ref> have demonstrated impressive performance, by learning to fuse information from both the node features and the graph structure <ref type=""bibr"" target=""#b7"">[8]</ref>.</p><p>Recently, many works have been focusing on developing more powerful GNNs <ref type=""bibr"" target=""#b7""> tures and the graph structure <ref type=""bibr"" target=""#b7"">[8]</ref>.</p><p>Recently, many works have been focusing on developing more powerful GNNs <ref type=""bibr"" target=""#b7"">[8]</ref><ref type=""bibr"" target=""#b8"">[9]</ref><ref type=""bibr"" target=""#b9"">[10]</ref><ref type=""bibr"" target=""#b10"">[ has the following objective: min </p><p>It also has an equivalent form: max P(Z|D):I(D;Z)≤Ic</p><p>Intuitively, Eq. ( <ref type=""formula"">7</ref>) or <ref type=""bibr"" target=""#b7"">(8)</ref> encourages the representation Z to maximally capture the information in Y , while controlling the complexity o 0 to some large value, we are essentially using a straight line with slope β to sweep out the Pareto frontier of I(Y ; Z) vs. I(X; Z) as given by Eq. <ref type=""bibr"" target=""#b7"">(8)</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>𝒟 𝑌</head><p>minimal sufficient info.</p><p>irreleva",0
"g., anchor generation, rule-based training target assignment, non-maximum suppression (NMS) post-processing. They are not fully end-to-end. Recently, <ref type=""bibr"" target=""#b2"">Carion et al. (2020)</ref> proposed DETR to eliminate the need for such hand-crafted components, and built the first ful e third term, as O(N q N k C). Thus, the multi-head attention module suffers from a quadratic complexity growth with the feature map size. DETR. DETR <ref type=""bibr"" target=""#b2"">(Carion et al., 2020)</ref> is built upon the Transformer encoder-decoder architecture, combined with a set-based Hungar the deformable Transformer encoder are shared among different feature levels. Other hyper-parameter setting and training strategy mainly follow DETR <ref type=""bibr"" target=""#b2"">(Carion et al., 2020)</ref>, except that Focal Loss <ref type=""bibr"" target=""#b18"">(Lin et al., 2017b)</ref> with loss w d as DETR-DC5 + . By default, models are trained for 50 epochs and the learning rate is decayed at the 40-th epoch by a factor of 0.1. Following DETR <ref type=""bibr"" target=""#b2"">(Carion et al., 2020)</ref>, we train our models using Adam optimizer <ref type=""bibr"" target=""#b13"">(Kingma &amp; Ba, 2",1
"hing. We briefly review the network architecture as follows.</p><p>Given the input feature maps x ∈ R C×H×W extracted by a CNN backbone (e.g., ResNet <ref type=""bibr"" target=""#b8"">(He et al., 2016)</ref>), DETR exploits a standard Transformer encoder-decoder architecture to transform the input featu resolutions. In encoder, we extract multi-scale feature maps {x l } L−1 l=1 (L = 4) from the output feature maps of stages C 3 through C 5 in ResNet <ref type=""bibr"" target=""#b8"">(He et al., 2016)</ref> (transformed by a 1 × 1 convolution), where C l is of resolution 2 l lower than the input image. on the val set and test-dev set.</p><p>Implementation Details. ImageNet <ref type=""bibr"" target=""#b6"">(Deng et al., 2009)</ref> pre-trained ResNet-50 <ref type=""bibr"" target=""#b8"">(He et al., 2016)</ref> is utilized as the backbone for ablations. Multi-scale feature maps are extracted without FPN <r",0
"ath to combine multi-scale features. PANet <ref type=""bibr"" target=""#b21"">(Liu et al., 2018b)</ref> further adds an bottom-up path on the top of FPN. <ref type=""bibr"" target=""#b15"">Kong et al. (2018)</ref> combines features from all scales by a global attention operation. <ref type=""bibr"" target=""#b",0
"r"" target=""#b10"">Hu et al., 2019;</ref><ref type=""bibr"" target=""#b23"">Parmar et al., 2019;</ref><ref type=""bibr"" target=""#b24"">Qiu et al., 2019;</ref><ref type=""bibr"" target=""#b1"">Beltagy et al., 2020;</ref><ref type=""bibr"" target=""#b0"">Ainslie et al., 2020;</ref><ref type=""bibr"" target=""#b38"">Zahee >; <ref type=""bibr"" target=""#b9"">Ho et al. (2019)</ref> attend key elements at fixed intervals to significantly increase the receptive field on keys. <ref type=""bibr"" target=""#b1"">Beltagy et al. (2020)</ref>; <ref type=""bibr"" target=""#b0"">Ainslie et al. (2020)</ref>; <ref type=""bibr"" target=""#b38"">Z",0
"e=""bibr"" target=""#b16"">(Lin et al., 2014)</ref> benchmark, DETR needs 500 epochs to converge, which is around 10 to 20 times slower than Faster R-CNN <ref type=""bibr"" target=""#b25"">(Ren et al., 2015)</ref>. (2) DETR delivers relatively low performance at detecting small objects. Modern object detect",0
"1"">(Hu &amp; Liu, 2004)</ref>, and sentiment of movie reviews (MR) <ref type=""bibr"" target=""#b36"">(Pang &amp; Lee, 2005)</ref>.</p><p>Comparison with <ref type=""bibr"" target=""#b23"">Kalantidis et al. (2020)</ref>: <ref type=""bibr"" target=""#b23"">Kalantidis et al. (2020)</ref> also consider ways to sam MR) <ref type=""bibr"" target=""#b36"">(Pang &amp; Lee, 2005)</ref>.</p><p>Comparison with <ref type=""bibr"" target=""#b23"">Kalantidis et al. (2020)</ref>: <ref type=""bibr"" target=""#b23"">Kalantidis et al. (2020)</ref> also consider ways to sample negatives, and propose a mixing strategy for hard negatives",1
"_2"">2</ref>  <ref type=""bibr"" target=""#b39"">(Schütte &amp; Van der Waerden, 1951;</ref><ref type=""bibr"" target=""#b32"">Musin &amp; Tarasov, 2015;</ref><ref type=""bibr"" target=""#b46"">Tammes, 1930)</ref>. When the distribution ρ over classes is uniform this problem is solved by a set of|C| points on th",0
"d n=""1"">INTRODUCTION</head><p>Owing to their empirical success, contrastive learning methods <ref type=""bibr"" target=""#b7"">(Chopra et al., 2005;</ref><ref type=""bibr"" target=""#b15"">Hadsell et al., 2006)</ref> have become one of the most popular self-supervised approaches for learning representations",0
"lti-view and contrastive learning <ref type=""bibr"" target=""#b2"">(Blum &amp; Mitchell, 1998;</ref><ref type=""bibr"" target=""#b54"">Xu et al., 2013;</ref><ref type=""bibr"" target=""#b1"">Bachman et al., 2019;</ref><ref type=""bibr"" target=""#b4"">Chen et al., 2020a;</ref><ref type=""bibr"" target=""#b48"">Tian et",0
"et al., 2004)</ref>, question type classification (TREC) <ref type=""bibr"" target=""#b50"">(Voorhees &amp; Harman, 2002)</ref>, opinion polarity (MPQA) <ref type=""bibr"" target=""#b52"">(Wiebe et al., 2005)</ref>, subjectivity classification (SUBJ) <ref type=""bibr"" target=""#b35"">(Pang &amp; Lee, 2004)</r",0
"satisfactory user experience.</p><p>As a representative set of latency-critical applications, we use the benchmarks of the TailBench benchmark suite <ref type=""bibr"" target=""#b18"">[20]</ref>. This suite includes eight representative applications of today's latency-critical applications. For the sak comparison with other speech recognition services. However, we found that these values are in line with the results of this application presented in <ref type=""bibr"" target=""#b18"">[20]</ref>.</p><p>Once we have defined the QoS requirements for each application, we can study the effect of adding ano threads from one to two reduces considerably the saturation QPS (i.e., point at which the LQoS is achieved). This issue has been also pointed out in <ref type=""bibr"" target=""#b18"">[20]</ref>, where authors realized of this unexpected behavior and demonstrated by simulation that this poor scalabilit",1
"utilization.</p><p>Next, we move to works that consider just LXC virtualization <ref type=""bibr"" target=""#b5"">[6]</ref> (i.e., Linux containers). In <ref type=""bibr"" target=""#b6"">[7]</ref>, D. Lo et al. aim to increase server utilization by tolerating some interference of Best-Effort (BE) tasks, pr",0
"ocessor is deployed with 12 cores with a 16.5GB 11-way LLC. It supports Intel Cache Allocation Technology (CAT) and Memory Bandwidth Allocation (MBA) <ref type=""bibr"" target=""#b11"">[12]</ref>, allowing to perform cache and memory bandwidth partitioning studies.</p><p>The client node is only used to udied applications is far below the maximum supported consumption.</p><p>To carry out this study, we use the Intel Resource Director Technology (RDT) <ref type=""bibr"" target=""#b11"">[12]</ref>, which implements the Cache Monitoring Technology (CMT) and Cache Allocation Technology (CAT) that allow mon",0
"step is to select the applications to be studied. Special interest is taken in latency-critical workloads. Many interactive services such as MongoDB <ref type=""bibr"" target=""#b0"">[1]</ref> and NGINX <ref type=""bibr"" target=""#b1"">[2]</ref> are examples of latency-critical workloads. An important cha",0
") that run a mix of batch workloads and LC workloads. However, authors do not consider reducing unpredictability through resource partitioning. AVMMC <ref type=""bibr"" target=""#b9"">[10]</ref>, on the other hand, determines dynamically the best VM to core mapping on the studied system. In this ap- pro",0
"stored indices or addresses of the data elements to gather to the requesting core, only the requested elements from the data array.</p><p>Song et al. <ref type=""bibr"" target=""#b10"">[11]</ref> propose a graph processor based on sparse matrix algebra, building on the observation that many graph applic",1
"ection, 8 byte accesses and offload memory engines are important contributors to PUMA's performance and energy efficiency.</p><p>The Emu architecture <ref type=""bibr"" target=""#b8"">[9]</ref> is a recently proposed architecture for big data analysis, including graph analysis workloads. Similar to PIUM",0
"these classified objects, typically represented as a graph. Determining the relationships between entities in a graph is the basis of graph analytics <ref type=""bibr"" target=""#b1"">[2]</ref>. Graph analytics poses important challenges on existing processor architectures due to its sparse structure. T",0
"for classification is predominantly oriented toward ""dense"" compute, such as matrix computations. The continuing exponential growth in generated data <ref type=""bibr"" target=""#b0"">[1]</ref> has shifted compute to offload to GPUs and other focused accelerators across multiple domains that are denseco",0
"and then proceed to add their value to all neighbors along outgoing edges. This basic computation is ubiquitous in graph algorithms such as PageRank <ref type=""bibr"" target=""#b2"">[3]</ref>. The resulting access stream (Figure <ref type=""figure"" target=""#fig_1"">1b</ref>) is § Ankit More and Shaden S",0
"from negative examples from other classes <ref type=""bibr"" target=""#b31"">(Weinberger &amp; Saul, 2009)</ref>. In this paradigm, selecting the hardest <ref type=""bibr"" target=""#b5"">(Bucher et al., 2016)</ref> or harder <ref type=""bibr"" target=""#b23"">(Schroff et al., 2015)</ref>  </p></div> <div xmlns",1
"ab_6""><head></head><label></label><figDesc>negatives has improved both the rate of learning and final performance. Similar to our findings about MoCo,<ref type=""bibr"" target=""#b32"">Wu et al. (2017)</ref> find that mining the very hardest negatives hurts performance (purportedly because it increases",1
"g methods. Beyond CID, a number of other approaches for self-supervised have been proposed that do not work within the CID paradigm, including RotNet <ref type=""bibr"" target=""#b15"">(Gidaris et al., 2018)</ref>, Jigsaw <ref type=""bibr"" target=""#b22"">(Noroozi &amp; Favaro, 2016)</ref>, DeepCluster <re",0
"representations such as alignment and uniformity <ref type=""bibr"" target=""#b30"">(Wang &amp; Isola, 2020)</ref>, and proposed a theoretical framework <ref type=""bibr"" target=""#b0"">(Arora et al., 2019)</ref>, among others. However, existing works on CID have not investigated the relative importance o ; Isola (2020)</ref> suggest that contrastive objectives implicitly try to align similar instances while uniformly utilizing the embedding space, and <ref type=""bibr"" target=""#b0"">Arora et al. (2019)</ref> propose a theoretical framework for understanding contrastive learning. Recent work attempted",0
"s are more useful later. While developing a negative curriculum is beyond the scope of this work, curricula have shown utility in many other contexts <ref type=""bibr"" target=""#b3"">(Bengio et al., 2009)</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>A Appendix</head><p>A </p></div><f",0
"n et al., 2020c)</ref>, SimCLR <ref type=""bibr"">(Chen et al., 2020a,b)</ref>, SwAV <ref type=""bibr"" target=""#b8"">(Caron et al., 2020)</ref>, and BYOL <ref type=""bibr"" target=""#b16"">(Grill et al., 2020)</ref> have dramatically reduced this performance gap.</p><p>The MoCo and SimCLR pre-training tasks al., 2020)</ref>, SeLa <ref type=""bibr"" target=""#b2"">(Asano et al., 2020)</ref>, PCL <ref type=""bibr"" target=""#b20"">(Li et al., 2020)</ref>, and BYOL <ref type=""bibr"" target=""#b16"">(Grill et al., 2020)</ref>. Since these do not employ negatives in the same way as CID, our results do not directly rel",0
"ds. It also suggests that there may be further room to choose specific examples for training-for example hard negative mining and curriculum learning <ref type=""bibr"" target=""#b9"">(Chen et al., 2020a;</ref><ref type=""bibr"" target=""#b12"">Chuang et al., 2020;</ref><ref type=""bibr"" target=""#b19"">Kaya & 20c)</ref>, an improved version of MoCo which combines MoCo v1 <ref type=""bibr"" target=""#b18"">(He et al., 2019)</ref> with several features of SimCLR <ref type=""bibr"" target=""#b9"">(Chen et al., 2020a)</ref>. We use ImageNet for pre-training and evaluate performance using linear classification on Ima t=""#b33"">(Wu et al., 2018)</ref>, CMC <ref type=""bibr"">(Tian et al., 2019)</ref>, Moco <ref type=""bibr"" target=""#b18"">(He et al., 2019)</ref>, SimCLR <ref type=""bibr"" target=""#b9"">(Chen et al., 2020a)</ref>, MoCo v2 <ref type=""bibr"" target=""#b11"">(Chen et al., 2020c)</ref>, in chronological order. I",0
"metric learning <ref type=""bibr"" target=""#b19"">(Kaya &amp; Bilge, 2019)</ref> and on the impact of different training examples in supervised learning <ref type=""bibr"" target=""#b4"">(Birodkar et al., 2019)</ref> suggests that understanding the relative importance of different training data can be frui 19"">(Kaya &amp; Bilge, 2019)</ref>. However, in some supervised contexts, much or all training data seems important for reaching the highest accuracy <ref type=""bibr"" target=""#b4"">(Birodkar et al., 2019)</ref>. We aim to experimentally assess which of these perspectives applies when pre-training MoC inate more than 20% of examples fromCIFAR-10 (Toneva et al., 2018)  or 10% from ImageNet<ref type=""bibr"" target=""#b29"">(Vodrahalli et al., 2018;</ref><ref type=""bibr"" target=""#b4"">Birodkar et al., 2019)</ref> without decreases in accuracy. However, not all examples are learned at the same time: the",0
"hoose specific examples for training-for example hard negative mining and curriculum learning <ref type=""bibr"" target=""#b9"">(Chen et al., 2020a;</ref><ref type=""bibr"" target=""#b12"">Chuang et al., 2020;</ref><ref type=""bibr"" target=""#b19"">Kaya &amp; Bilge, 2019)</ref>-to reduce costs and improve perf label data. This observation is also consistent with recent work which has attempted to ""debias"" contrastive learning away from same-class negatives <ref type=""bibr"" target=""#b12"">(Chuang et al., 2020)</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4"">Understanding negatives by ical framework for understanding contrastive learning. Recent work attempted to mitigate the effects of same-class negatives via a reweighting scheme <ref type=""bibr"" target=""#b12"">(Chuang et al., 2020)</ref>, but does not study negatives by difficulty, which is our focus here.</p><p>Non-instance-di",0
"et <ref type=""bibr"" target=""#b15"">(Gidaris et al., 2018)</ref>, Jigsaw <ref type=""bibr"" target=""#b22"">(Noroozi &amp; Favaro, 2016)</ref>, DeepCluster <ref type=""bibr"" target=""#b7"">(Caron et al., 2018)</ref>, SwAV <ref type=""bibr"" target=""#b8"">(Caron et al., 2020)</ref>, SeLa <ref type=""bibr"" target=",0
"by considering each image as its own class and dissimilar from every other image in the dataset <ref type=""bibr"" target=""#b50"">(Wu et al., 2018;</ref><ref type=""bibr"" target=""#b1"">Chen et al., 2020)</ref>. We propose to train feature representations on the novel target domain to replicate this induc rget=""#b50"">(Wu et al., 2018;</ref><ref type=""bibr"" target=""#b26"">Misra &amp; Maaten, 2020;</ref><ref type=""bibr"" target=""#b15"">He et al., 2020;</ref><ref type=""bibr"" target=""#b1"">Chen et al., 2020)</ref> which aims to learn representations by considering each image together with its augmentations a itional useful knowledge specific to the target domain. We use a state-of-the-art self-supervised loss function based on contrastive learning: SimCLR <ref type=""bibr"" target=""#b1"">(Chen et al., 2020)</ref>. The SimCLR loss encourages two augmentations of the same image to be closer in feature space nlabeled data.</p><p>We also compare to another baseline, SimCLR that uses the novel domain unlabeled data D u to train a representation using SimCLR <ref type=""bibr"" target=""#b1"">(Chen et al., 2020)</ref>, and then uses the resulting representation to learn linear classifiers for few-shot tasks. Th",1
"ef><ref type=""bibr"" target=""#b18"">Hou et al., 2019)</ref> or transfer learning-based approach <ref type=""bibr"" target=""#b2"">(Chen et al., 2019a;</ref><ref type=""bibr"" target=""#b46"">Wang et al., 2019;</ref><ref type=""bibr"" target=""#b19"">Kolesnikov et al., 2020;</ref><ref type=""bibr"" target=""#b39"">Tia",0
"approach leverages the unlabeled data during representation learning. The two are orthogonal innovations and can be combined. Knowledge distillation <ref type=""bibr"" target=""#b16"">(Hinton et al., 2015)</ref> is similar but aims to compress a large teacher network by training a student network to mi",0
"get=""#b41"">(Tzeng et al., 2017;</ref><ref type=""bibr"" target=""#b17"">Hoffman et al., 2018;</ref><ref type=""bibr"" target=""#b24"">Long et al., 2018;</ref><ref type=""bibr"" target=""#b52"">Xu et al., 2019;</ref><ref type=""bibr"" target=""#b20"">Laradji &amp; Babanezhad, 2020;</ref><ref type=""bibr"" target=""#b45",0
"mine the trade-off between node and topological feature exploration, we first describe the recently proposed contextual stochastic block model (cSBM) <ref type=""bibr"" target=""#b10"">(Deshpande et al., 2018)</ref>. The cSBM allows for smoothly controlling the ""informativeness ratio"" between node featu ic data. In order to test the ability of label learning of GNNs on graphs with arbitrary levels of homophily and heterophily, we propose to use cSBMs <ref type=""bibr"" target=""#b10"">(Deshpande et al., 2018)</ref> to generate synthetic graphs. We consider the case with two equal-size classes. In cSBMs rn on both homophilic and heterophilic graph should have similar performances for φ and −φ. Due to space limitation we refer the interested reader to <ref type=""bibr"" target=""#b10"">(Deshpande et al., 2018)</ref> for a review of all formal theoretical results and only outline the cSBM properties need lic graphs while negative λ s correspond to heterophilic graphs. The information-theoretic limits of reconstruction for the cSBM are characterized in <ref type=""bibr"" target=""#b10"">Deshpande et al. (2018)</ref>. The results show that, asymptotically, one needs λ 2 + µ 2 /ξ &gt; 1 to ensure a vanishi vely.</p><p>One reason for using the cSBM to generate synthetic data is that the information-theoretic limit of the model is already characterized in <ref type=""bibr"" target=""#b10"">Deshpande et al. (2018)</ref>. This result is summarized below.</p><p>Theorem A.7 (Informal main result in <ref type=""b cterized in <ref type=""bibr"" target=""#b10"">Deshpande et al. (2018)</ref>. This result is summarized below.</p><p>Theorem A.7 (Informal main result in <ref type=""bibr"" target=""#b10"">Deshpande et al. (2018)</ref>). Assume that n, f → ∞, n f → ξ and d → ∞. Then there exists an estimator v such that lim",1
"/ref> and they are not easy to interpret as our GPR-GNN method. Some prior work also emphasizes adaptively learning the importance of different steps <ref type=""bibr"" target=""#b1"">(Abu-El-Haija et al., 2018;</ref><ref type=""bibr"">Berberidis et al., 2018)</ref>. Nevertheless, none of the above works",0
""" target=""#b29"">(Sen et al., 2008;</ref><ref type=""bibr"" target=""#b38"">Yang et al., 2016)</ref> and the Amazon co-purchase graphs Computers and Photo <ref type=""bibr"" target=""#b25"">(McAuley et al., 2015;</ref><ref type=""bibr"" target=""#b30"">Shchur et al., 2018)</ref>. We also use 5 heterophilic bench",0
"disambiguation separately <ref type=""bibr"" target=""#b3"">[5]</ref>, <ref type=""bibr"" target=""#b4"">[6]</ref>, <ref type=""bibr"" target=""#b7"">[9]</ref>, <ref type=""bibr"" target=""#b9"">[11]</ref>, <ref type=""bibr"" target=""#b11"">[13]</ref>, <ref type=""bibr"" target=""#b13"">[15]</ref>, <ref type=""bibr"" targe ndently, these methods neglect the connection between these sub-problems. For example, coauthors, which are used as a strong evidence in many methods <ref type=""bibr"" target=""#b9"">[11]</ref>, <ref type=""bibr"" target=""#b30"">[32]</ref>, <ref type=""bibr"" target=""#b37"">[39]</ref>, may also be ambiguous. are two different ""Wei Xu"" and two different ""Ying Zhang"" in this example. More troubles may appear when multi-hop coauthorships are used as features <ref type=""bibr"" target=""#b9"">[11]</ref>, <ref type=""bibr"" target=""#b30"">[32]</ref>. For instance, ""Jianxin Li"" in the University of Western Australia ind that our method NDCC is both effective and efficient, compared with the state-ofthe-art methods CE <ref type=""bibr"" target=""#b5"">[7]</ref>, GHOST <ref type=""bibr"" target=""#b9"">[11]</ref>, CSLR <ref type=""bibr"" target=""#b17"">[19]</ref>, MIX <ref type=""bibr"" target=""#b16"">[18]</ref>, and AM <ref t thorship paths connecting authors i and j. To avoid the redundant information, we only consider valid 2-hop coauthorship paths connecting two authors <ref type=""bibr"" target=""#b9"">[11]</ref>. Specifically, a valid 2-hop coauthorship path in G is an AP AP A path a i -p i -a j -p j -a k , where a i = xperiments to evaluate (1) the effectiveness and efficiency of NDCC versus state-of-the-art methods CE <ref type=""bibr"" target=""#b5"">[7]</ref>, GHOST <ref type=""bibr"" target=""#b9"">[11]</ref>, CSLR <ref type=""bibr"" target=""#b17"">[19]</ref>, MIX <ref type=""bibr"" target=""#b16"">[18]</ref>, and AM <ref t both attributes and relational information, and a greedy agglomerative clustering method is used to merge the most similar clusters.</p><p>(2) GHOST <ref type=""bibr"" target=""#b9"">[11]</ref> is a graph-based method employing coauthorship only. Its similarity function considers both quantity and qual Efficiency performance comparison. Among the chosen baselines, only CE and GHOST analyze the time complexity <ref type=""bibr"" target=""#b5"">[7]</ref>, <ref type=""bibr"" target=""#b9"">[11]</ref>. The time complexity of CE is O(|A (0) |k log |A (0) |), where |A (0) | is the number of atomic authors, and ]</ref>, <ref type=""bibr"" target=""#b42"">[44]</ref> and unsupervised <ref type=""bibr"" target=""#b5"">[7]</ref>, <ref type=""bibr"" target=""#b7"">[9]</ref>, <ref type=""bibr"" target=""#b9"">[11]</ref>, <ref type=""bibr"" target=""#b17"">[19]</ref>, <ref type=""bibr"" target=""#b26"">[28]</ref>, <ref type=""bibr"" targe g <ref type=""bibr"" target=""#b7"">[9]</ref>, <ref type=""bibr"" target=""#b17"">[19]</ref>, <ref type=""bibr"" target=""#b37"">[39]</ref>, affinity propagation <ref type=""bibr"" target=""#b9"">[11]</ref> and Markov clustering <ref type=""bibr"" target=""#b39"">[41]</ref>, or topic modeling <ref type=""bibr"" target=""#",1
".</p><p>Most existing methods tackle name disambiguation separately <ref type=""bibr"" target=""#b3"">[5]</ref>, <ref type=""bibr"" target=""#b4"">[6]</ref>, <ref type=""bibr"" target=""#b7"">[9]</ref>, <ref type=""bibr"" target=""#b9"">[11]</ref>, <ref type=""bibr"" target=""#b11"">[13]</ref>, <ref type=""bibr"" target= /ref>, <ref type=""bibr"" target=""#b38"">[40]</ref>, <ref type=""bibr"" target=""#b42"">[44]</ref> and unsupervised <ref type=""bibr"" target=""#b5"">[7]</ref>, <ref type=""bibr"" target=""#b7"">[9]</ref>, <ref type=""bibr"" target=""#b9"">[11]</ref>, <ref type=""bibr"" target=""#b17"">[19]</ref>, <ref type=""bibr"" target= eling data is time-consuming and impractical when the bibliography data is large. Unsupervised methods use clustering, e.g., agglomerative clustering <ref type=""bibr"" target=""#b7"">[9]</ref>, <ref type=""bibr"" target=""#b17"">[19]</ref>, <ref type=""bibr"" target=""#b37"">[39]</ref>, affinity propagation <r",0
"rget=""#b7"">[9]</ref>, <ref type=""bibr"" target=""#b9"">[11]</ref>, <ref type=""bibr"" target=""#b11"">[13]</ref>, <ref type=""bibr"" target=""#b13"">[15]</ref>, <ref type=""bibr"" target=""#b15"">[17]</ref>, <ref type=""bibr"" target=""#b16"">[18]</ref>, <ref type=""bibr"" target=""#b17"">[19]</ref>, <ref type=""bibr"" targ r name disambiguation can be divided into two classes: supervised <ref type=""bibr"" target=""#b2"">[4]</ref>, <ref type=""bibr"" target=""#b13"">[15]</ref>, <ref type=""bibr"" target=""#b15"">[17]</ref>, <ref type=""bibr"" target=""#b16"">[18]</ref>, <ref type=""bibr"" target=""#b33"">[35]</ref>, <ref type=""bibr"" targ =""#b40"">[42]</ref>. Supervised methods use labeled data to train a classifier, e.g., SVM <ref type=""bibr"" target=""#b36"">[38]</ref> and random forests <ref type=""bibr"" target=""#b15"">[17]</ref>, <ref type=""bibr"" target=""#b16"">[18]</ref>, <ref type=""bibr"" target=""#b33"">[35]</ref>, which is then used to",0
"<p>Scholar name ambiguity is a common data quality problem for digital libraries such as DBLP <ref type=""bibr"" target=""#b0"">[1]</ref>, Google Scholar <ref type=""bibr"" target=""#b1"">[2]</ref> and Microsoft Academic Search [3], and has raised various troubles in scholar search, document retrieval and s milar venue pairs are considered. Exp-4.3: Impacts of ?. To evaluate the impacts of ?, we vary ? from 1 to 100 <ref type=""bibr"" target=""#b0"">(1,</ref><ref type=""bibr"" target=""#b1"">2,</ref><ref type=""bibr"" target=""#b3"">5,</ref><ref type=""bibr"" target=""#b8"">10,</ref><ref type=""bibr"" target=""#b18"">20,<",0
"ted number of authors for each name is used as the stop condition, essentially a cluster estimation problem <ref type=""bibr"" target=""#b8"">[10]</ref>, <ref type=""bibr"" target=""#b24"">[26]</ref> Specifically, a name is considered as fully disambiguated if the number of authors of this name reaches the",0
"nostic strategy to accelerate the learning of a new compact model (student) by transferring knowledge from a previously trained large model (teacher) <ref type=""bibr"" target=""#b6"">[7]</ref>. The knowledge transfer is conducted as follows: First, the teacher model is trained with the user-item intera er model, and also has lower inference latency due to its small size. Most KD methods have focused on the image classification problem. An early work <ref type=""bibr"" target=""#b6"">[7]</ref> matches the softmax distribution of the teacher and the student. The predicted label distribution contains mor transferring knowledge from a previously trained ""large"" model (teacher) <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b21"">22]</ref>. The student model trained with KD has comparable performance to that o",1
"rowing scale of users (and items) and sophisticated model architecture to capture complex patterns make the size of the model continuously increasing <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b29 large model to real-time platform.</p><p>Motivated by the significant success of knowledge distillation (KD) in the computer vision field, a few work <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b24"">25]</ref> have employed KD for RS to reduce the size of models while maintaini inary labels. The student model trained with KD has comparable performance to that of the teacher, and also has a lower latency due to its small size <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b24"">25]</ref>.</p><p>The core idea behind this process is that the soft labels pre y labeled as '0' <ref type=""bibr"" target=""#b12"">[13]</ref>. By using the additional supervisions from the teacher model, the state-of-the-art methods <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b24"">25]</ref> have achieved comparable or even better performance to the teacher m omparable or even better performance to the teacher models with faster inference time.</p><p>However, there are still limitations in existing methods <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b24"">25]</ref>. First, the learning of the student is only guided by the teacher's the teacher, which is used to make such predictions, contains more detailed information that Figure <ref type=""figure"">1</ref>: The existing methods <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b24"">25]</ref> distill the knowledge only based on the teacher's predictions (b). T -RRD-that transfers the knowledge from the teacher's predictions with direct consideration of ranking orders among items. Unlike the existing methods <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b24"">25]</ref> that distill the knowledge of an item at a time, RRD formulates this interesting items that the user would not be interested in. RRD achieves superior recommendation performance compared to the state-of-the-art methods <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b24"">25]</ref>. An unified framework. We propose a novel framework-DE-RRD-which ena e recommender model is continuously increasing, and the computational time and memory cost required for the inference are also increasing accordingly <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b29 =""#b4"">[5]</ref>. Knowledge Distillation in Recommender System. Recently, inspired by the huge success of KD in the computer vision field, a few work <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b24"">25]</ref> have adopted KD to RS. A pioneer work is Ranking Distillation (RD) < er of parameters based on the size of the last hidden layer. The limiting ratios (𝜙) are {0.1, 0.5, 1.0}. Following the notation of the previous work <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b24"">25]</ref>, we call the student model trained without the help of the teacher m shing the items that each user would like and the items that each user would not be interested in among numerous unobserved items only labeled as '0' <ref type=""bibr"" target=""#b12"">[13]</ref>. By using the additional supervisions from the teacher model, the state-of-the-art methods <ref type=""bibr"" nking from the student and the teacher, for reflecting relative importance among top-𝐾 items.</p><p>A subsequent work Collaborative Distillation (CD) <ref type=""bibr"" target=""#b12"">[13]</ref> first samples unobserved items from the teacher's recommendation list according to their ranking; high-ranke mender system that uses items with the highest ranking from the teacher's predictions for distilling the knowledge. • Collaborative Distillation (CD) <ref type=""bibr"" target=""#b12"">[13]</ref>: The state-of-the-art KD method for recommender system. CD samples items from teacher's predictions based on",1
"histicated model architecture to capture complex patterns make the size of the model continuously increasing <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b29"">30]</ref>. A large model with numerous asing, and the computational time and memory cost required for the inference are also increasing accordingly <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b29"">30]</ref>. Due to the high latency, it Motivated by the significant success of knowledge distillation (KD) in the computer vision field, a few work <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b24"">25]</ref> have employed KD for RS to reduce the size of models while maintaining the performance. KD is a model-agnosti ith KD has comparable performance to that of the teacher, and also has a lower latency due to its small size <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b24"">25]</ref>.</p><p>The core idea behind this process is that the soft labels predicted by the teacher model reveal hidden ""#b12"">[13]</ref>. By using the additional supervisions from the teacher model, the state-of-the-art methods <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b24"">25]</ref> have achieved comparable or even better performance to the teacher models with faster inference time.</p><p>H he teacher models with faster inference time.</p><p>However, there are still limitations in existing methods <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b24"">25]</ref>. First, the learning of the student is only guided by the teacher's prediction results, which is not sufficie predictions, contains more detailed information that Figure <ref type=""figure"">1</ref>: The existing methods <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b24"">25]</ref> distill the knowledge only based on the teacher's predictions (b). The proposed framework directly distills t e teacher's predictions with direct consideration of ranking orders among items. Unlike the existing methods <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b24"">25]</ref> that distill the knowledge of an item at a time, RRD formulates this as a ranking matching problem between th be interested in. RRD achieves superior recommendation performance compared to the state-of-the-art methods <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b24"">25]</ref>. An unified framework. We propose a novel framework-DE-RRD-which enables the student model to learn both from in Recommender System. Recently, inspired by the huge success of KD in the computer vision field, a few work <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b24"">25]</ref> have adopted KD to RS. A pioneer work is Ranking Distillation (RD) <ref type=""bibr"" target=""#b24"">[25]</ref> m the wide range of the rest. To sample the interesting items, we adopt a ranking position importance scheme <ref type=""bibr"" target=""#b19"">[20,</ref><ref type=""bibr"" target=""#b24"">25]</ref> that places more emphasis on the higher positions in the ranking list. In the scheme, the probability of the last hidden layer. The limiting ratios (𝜙) are {0.1, 0.5, 1.0}. Following the notation of the previous work <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b24"">25]</ref>, we call the student model trained without the help of the teacher model (i.e., no distillation) as ""Student"" ically, the items ranked near the top of a user's recommendation list would have strong correlations to the items that the user has interacted before <ref type=""bibr"" target=""#b24"">[25]</ref>. Also, the soft labels provide guidance for distinguishing the items that each user would like and the items <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b24"">25]</ref> have adopted KD to RS. A pioneer work is Ranking Distillation (RD) <ref type=""bibr"" target=""#b24"">[25]</ref> which applies KD for the ranking problem; Providing recommendations of top-𝑁 unobserved items that have not l hidden patterns among entities (i.e., users and items); the high-ranked items in the recommendation list would have strong correlations to the user <ref type=""bibr"" target=""#b24"">[25]</ref>. By using such additional supervisions from the teacher, they have achieved the comparable performance to th this experiment sections.</p><p>Comparison Methods. The proposed framework is compared with the following methods:</p><p>• Ranking Distillation (RD) <ref type=""bibr"" target=""#b24"">[25]</ref>: A KD method for recommender system that uses items with the highest ranking from the teacher's predictions",1
"ching problem between the recommendation list of the teacher and that of the student. To this end, RRD adopts the list-wise learning-to-rank approach <ref type=""bibr"" target=""#b28"">[29]</ref> and learns to ensure the student to preserve the ranking orders predicted by the teacher. However, directly the recommendation list of the teacher model and that of the student model. To this end, RRD adopts the classical list-wise learning-to-rank approach <ref type=""bibr"" target=""#b28"">[29]</ref>. Its core idea is to define a probability of a permutation (i.e., a ranking order) based on the ranking scor del, and train the model to maximize the likelihood of the ground-truth ranking order. For more details about the list-wise approach, please refer to <ref type=""bibr"" target=""#b28"">[29]</ref>.</p><p>However, merely adopting the list-wise loss can have adverse effects on the ranking performance. Beca =""http://www.tei-c.org/ns/1.0""><head>4.2.2</head><p>Relaxed permutation probability. Then, RRD defines a relaxed permutation probability motivated by <ref type=""bibr"" target=""#b28"">[29]</ref>. For user 𝑢, 𝝅 𝒖 denotes a ranked list of all the sampled items (𝐾 + 𝐿) sorted by the original order in the",1
"r"" target=""#b1"">[2]</ref>, data compression techniques <ref type=""bibr"" target=""#b25"">[26]</ref>, and approximated nearest neighbor search techniques <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b22"">23]</ref> have been successfully adopted to reduce the search costs. However, th",0
"reduce the search costs. However, they still have problems such as applicable only to specific models (e.g., k-d tree for metric learningbased models <ref type=""bibr"" target=""#b11"">[12]</ref>), or easily falling into a local optimum due to the local search. Knowledge Distillation. Knowledge distilla",0
"ared to models that use real-values representations. In addition, several work has focused on accelerating the inference of the existing recommenders <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b25"">26]</ref>. Specifically, tree-based data items), we build a binary matrix 𝑹 ∈ {0, 1} | U |× | I | . Each element of 𝑹 has a binary value indicating whether a user has interacted with an item <ref type=""bibr"" target=""#b0"">(1)</ref> or not (0). Note that an unobserved interaction does not necessarily mean a user's negative preference on an i",0
"Gumbel-Softmax is a continuous distribution on the simplex that can approximate samples from a categorical distribution; it uses the Gumbel-Max trick <ref type=""bibr"" target=""#b17"">[18]</ref> to draw samples from the categorical distribution, then uses the softmax function as a continuous approximat",0
"n=""5.1"">Experimental Setup</head><p>Datasets. We use two public real-world datasets: CiteULike <ref type=""bibr"" target=""#b26"">[27]</ref>, Foursquare <ref type=""bibr"" target=""#b16"">[17]</ref>. We remove users and items having fewer than five ratings for CiteULike, twenty ratings for Foursquare as do",0
"mpetitors. Also, DE-RRD shows the largest performance gain when the student has the identical structure to the teacher model (i.e., self-distillation <ref type=""bibr"" target=""#b4"">[5]</ref>). Furthermore, we provide both qualitative and quantitative analyses to further investigate the superiority of ayers to bridge the different dimensions. Interestingly, KD has turned out to be effective in improving the teacher model itself by self-distillation <ref type=""bibr"" target=""#b4"">[5]</ref>. Knowledge Distillation in Recommender System. Recently, inspired by the huge success of KD in the computer vi of a new ""compact"" model (student) by transferring knowledge from a previously trained ""large"" model (teacher) <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b21"">22]</ref>. The student model trained with K",0
"resentations. In addition, several work has focused on accelerating the inference of the existing recommenders <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b25"">26]</ref>. Specifically, tree-based data structures <ref type=""bibr"" target=""#b",0
"remove users and items having fewer than five ratings for CiteULike, twenty ratings for Foursquare as done in <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b20"">21]</ref>. Data statistics are summarized in Table <ref type=""table"" target=""#tab ing orders among items.</p><p>Evaluation Protocol. We follow the widely used leave-one-out evaluation protocol <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b18"">19]</ref>. For each user, we leave out a single interacted item for testing, and based on implicit feedback, we evaluate the performance of each method with widely used three ranking metrics <ref type=""bibr"" target=""#b5"">[6,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b9"">10]</ref>: hit ratio (H@𝑁 ), normalized discounted cumulative gain (N@𝑁 ), and mea",0
"></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""5.1"">Experimental Setup</head><p>Datasets. We use two public real-world datasets: CiteULike <ref type=""bibr"" target=""#b26"">[27]</ref>, Foursquare <ref type=""bibr"" target=""#b16"">[17]</ref>. We remove users and items having fewer than five rati",0
"(i.e., (𝑢,𝑖) ∈𝐵 L (𝑢, 𝑖)). Also, we adopt a simple temperature annealing schedule, which gradually decays the temperature from 𝜏 0 to 𝜏 𝑃 as done in <ref type=""bibr"" target=""#b10"">[11]</ref>: 𝜏 (𝑝) = 𝜏 0 (𝜏 𝑃 /𝜏 0 ) 𝑝/𝑃 where 𝜏 (𝑝) is the temperature at epoch 𝑝, and 𝑃 is the total training epochs.<",0
"ly trained ""large"" model (teacher) <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b21"">22]</ref>. The student model trained with KD has comparable performance to that of the teacher model, and also has lowe ation) than the one-hot class label, which leads to improved learning of the student model. Subsequent methods <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b21"">22]</ref> have focused on distilling knowledge from intermediate layers. Because teacher's intermediate layers are gene intermediate layers. Because teacher's intermediate layers are generally bigger than that of the student, they <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b21"">22]</ref> utilize additional layers to bridge the different dimensions. Interestingly, KD has turned out to be effectiv",0
"ent flows and disable the end-to-end training. As a workaround, we adopt a continuous relaxation of the discrete distribution by using Gumbel-Softmax <ref type=""bibr"" target=""#b7"">[8]</ref>. The Gumbel-Softmax is a continuous distribution on the simplex that can approximate samples from a categorica",0
"sion techniques <ref type=""bibr"" target=""#b25"">[26]</ref>, and approximated nearest neighbor search techniques <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b22"">23]</ref> have been successfully adopted to reduce the search costs. However, they still have problems such as applicab",0
"ibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b13"">14,</ref><ref type=""bibr"" target=""#b25"">26]</ref>. Specifically, tree-based data structures <ref type=""bibr"" target=""#b1"">[2]</ref>, data compression techniques <ref type=""bibr"" target=""#b25"">[26]</ref>, and approximated nearest neighbor sear",0
"complex patterns make the size of the model continuously increasing <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b29"">30]</ref>. A large model with numerous parameters has a high capacity, and thus mory cost required for the inference are also increasing accordingly <ref type=""bibr"" target=""#b12"">[13,</ref><ref type=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b27"">28,</ref><ref type=""bibr"" target=""#b29"">30]</ref>. Due to the high latency, it becomes difficult to apply such large re",0
"prove the learning and the performance of a new ""compact"" model (student) by transferring knowledge from a previously trained ""large"" model (teacher) <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b4"">5,</ref><ref type=""bibr"" target=""#b6"">7,</ref><ref type=""bibr"" target=""#b21"">22]< h information (e.g., inter-class correlation) than the one-hot class label, which leads to improved learning of the student model. Subsequent methods <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b21"">22]</ref> have focused on distilling knowledge from intermediate layers. Because e focused on distilling knowledge from intermediate layers. Because teacher's intermediate layers are generally bigger than that of the student, they <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b21"">22]</ref> utilize additional layers to bridge the different dimensions. Interest",0
"interactions, such as low-order interactions <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b7"">8]</ref> or high-order interactions <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b4"">5]</ref>, as shown in Fig. <ref type=""figure"" target=""#fig_0"">1</ref>. Similarly, a useful information for CTR prediction and boost the performance of existing state-of-the-art methods such as <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b2"">3]</ref>, especially when the size of embedding dimension is high.</p><p>The main contributions of this paper are conclu 0"">[1]</ref> 0.8076 0.4436 0.7869 0.3755 0.6685 0.1432 AFM <ref type=""bibr"" target=""#b10"">[11]</ref> 0.8038 0.4478 0.7817 0.3792 0.6618 0.1457 DeepFM <ref type=""bibr"" target=""#b2"">[3]</ref> 0.8091 0.4423 0.7878 0.3753 0.6708 0.1372 DCN <ref type=""bibr"" target=""#b9"">[10]</ref> 0.8093 0.4420 0.7875 0. er ensure the generality of above conclusion, we conduct the comparisons on another two datasets: Avazu and JD.com, and the results are shown in Tab. <ref type=""bibr"" target=""#b2"">3</ref>. We can see that, in all three datasets, our FED outperforms other models, which again demonstrates that FED can",1
"rnable. Usually, the raw fields are not independent, thus it's effective to learn information from their interactions, such as low-order interactions <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b7"">8]</ref> or high-order interactions <ref type=""bibr"" target=""#b2"">[3,</ref><ref t",1
"model such relations to improve CTR prediction. Specifically, we propose a novel module based on dimension recalibration and self-attention mechanism <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b8"">9]</ref> to learn the relations among latent fields. Since each latent field corr",0
"the proposed DRM module can catch extra useful information for CTR prediction and boost the performance of existing state-of-the-art methods such as <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b2"">3]</ref>, especially when the size of embedding dimension is high.</p><p>The main work on the Criteo dataset in Table <ref type=""table"" target=""#tab_0"">1</ref>. We find that both architectures get improvements in AUC and decreases  <ref type=""bibr"" target=""#b0"">[1]</ref> 0.8076 0.4436 0.7869 0.3755 0.6685 0.1432 AFM <ref type=""bibr"" target=""#b10"">[11]</ref> 0.8038 0.4478 0.7817 0",0
"=""#b2"">[3]</ref> 0.8091 0.4423 0.7878 0.3753 0.6708 0.1372 DCN <ref type=""bibr"" target=""#b9"">[10]</ref> 0.8093 0.4420 0.7875 0.3759 0.6702 0.1361 PNN <ref type=""bibr"" target=""#b6"">[7]</ref> 0.8094 0.4414 0.7879 0.3752 0.6705 0.1360 xDeepFM <ref type=""bibr"" target=""#b4"">[5]</ref> 0.8096 0.4412 0.7877",0
"explicit description for a given sample. Different samples are supposed to pay attention to different fields and its interactions for the given task <ref type=""bibr"" target=""#b10"">[11]</ref>. We propose to utilize attention mechanism to model field-wise feature interactions.</p><p>In the field-wise that both architectures get improvements in AUC and decreases  <ref type=""bibr"" target=""#b0"">[1]</ref> 0.8076 0.4436 0.7869 0.3755 0.6685 0.1432 AFM <ref type=""bibr"" target=""#b10"">[11]</ref> 0.8038 0.4478 0.7817 0.3792 0.6618 0.1457 DeepFM <ref type=""bibr"" target=""#b2"">[3]</ref> 0.8091 0.4423 0.787",0
"10"">[11]</ref> 0.8038 0.4478 0.7817 0.3792 0.6618 0.1457 DeepFM <ref type=""bibr"" target=""#b2"">[3]</ref> 0.8091 0.4423 0.7878 0.3753 0.6708 0.1372 DCN <ref type=""bibr"" target=""#b9"">[10]</ref> 0.8093 0.4420 0.7875 0.3759 0.6702 0.1361 PNN <ref type=""bibr"" target=""#b6"">[7]</ref> 0.8094 0.4414 0.7879 0. increase at 10 −3 level in Criteo dataset is already clear compared with recent works such as xDeepFM <ref type=""bibr"" target=""#b4"">[5]</ref> and DCN <ref type=""bibr"" target=""#b9"">[10]</ref>.</p><p>Since DRM learns the relations of dimensions in embedding features space, its performance is affected",0
""">[7]</ref> 0.8094 0.4414 0.7879 0.3752 0.6705 0.1360 xDeepFM <ref type=""bibr"" target=""#b4"">[5]</ref> 0.8096 0.4412 0.7877 0.3753 0.6710 0.1356 FGCNN <ref type=""bibr"" target=""#b5"">[6]</ref> 0.8093 0.4422 0.7878 0.3752 0.6709 0.1361 FED (Ours) 0.8113 0.4403 0.7889 0.3748 0.6735 0.1341 in LogLoss, whi",0
"diction. Specifically, we propose a novel module based on dimension recalibration and self-attention mechanism <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b8"">9]</ref> to learn the relations among latent fields. Since each latent field corresponds to a specific dimension in the",0
"independent, thus it's effective to learn information from their interactions, such as low-order interactions <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b7"">8]</ref> or high-order interactions <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b4"">5]</ref>, as sh",0
"2) ≥ • • • ≥ 𝐾 (𝐿)  reflecting the number of aspects decrease along with neighbor hops increasing. Inspired by the architecture design of Transformer <ref type=""bibr"" target=""#b30"">[31]</ref>, we leverage attention mechanism to learn the weight of source nodes under different aspects. And the import",1
"t=""#b18"">[19]</ref>, graph convolutional network (GCN) <ref type=""bibr"" target=""#b16"">[17]</ref>, graph inductive representation learning (GraphSAGE) <ref type=""bibr"" target=""#b8"">[9]</ref> and graph attention network (GAT) <ref type=""bibr"" target=""#b31"">[32]</ref>, have been attracting considerable ome relations tends to obey a long-tail distribution, so the padding would lead to both high space and time complexity. Followed by the previous work <ref type=""bibr"" target=""#b8"">[9]</ref>, we sample fixed number of source nodes and pad zeros when source nodes less than the fixed number.</p></div>",0
"tic data, such as images <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b12"">13]</ref>, texts <ref type=""bibr"" target=""#b15"">[16]</ref> and user behaviors <ref type=""bibr"" target=""#b21"">[22]</ref>. For graph-structure data, GAT performs multi-h",0
"representations are demonstrated to be more robust, i.e., bring enhanced generalization ability as well as improved robustness to adversarial attack <ref type=""bibr"" target=""#b1"">[2]</ref>, but also make the downstream process more interpretable which can directly find applications in various field",0
"]</ref> employ various types of meta paths between users and items in matrix factorization to generate latent features for recommendation. Shi et al. <ref type=""bibr"" target=""#b24"">[25]</ref> and Hu et al. <ref type=""bibr"" target=""#b13"">[14]</ref> explicitly extract meta paths connecting users to it",0
"obe the model's mechanism without the support of background knowledge <ref type=""bibr"" target=""#b0"">[1]</ref>. Although Neural Attention Models (NAM) <ref type=""bibr"" target=""#b1"">[2]</ref> are endowed with a certain degree of interpretability in visualizing parts of the sentence which are focused o natural disasters <ref type=""bibr"" target=""#b7"">[8]</ref>), (c) discover inherent bias in the model's predictive strategy (e.g., contextual modeling <ref type=""bibr"" target=""#b1"">[2]</ref>[9]), (d) prevent prediction errors in unintuitive scenarios (e.g. adversarial examples <ref type=""bibr"" target pe=""bibr"" target=""#b10"">[11]</ref>. Also, knowledge-infusion during the model learning using information-theoretic loss function (e.g., KL divergence <ref type=""bibr"" target=""#b1"">[2]</ref>) can check conceptual drifting at the representational level through weak-supervision from KG. Alternatively, and interpretations.</p><p>Infusing domain knowledge in DL models can be categorized into the shallow infusion, semi-deep infusion, and deep infusion <ref type=""bibr"" target=""#b1"">[2]</ref>. In shallow infusion, both the external information and method of knowledge infusion is shallow, i.e., Word2Ve in natural language understanding: (1) Anaphora-where sentences are purposefully paraphrased to elicit meaningful responses from an agent (or user); <ref type=""bibr"" target=""#b1"">(2)</ref> The clinical conversation contains implicit references to healthcare conditions, developing sparsity in the cl",1
"planation correlates with the model's decision making. It is considered plausible when it has a human-understandable justification for the prediction <ref type=""bibr"" target=""#b12"">[13]</ref> <ref type=""bibr"" target=""#b13"">[14]</ref>. Such systems are considered to be potentially useful for real-wor pe=""bibr"" target=""#b16"">[17]</ref> <ref type=""bibr"" target=""#b15"">[16]</ref>, and (b) Engagement and learning patterns from historical student's data <ref type=""bibr"" target=""#b12"">[13]</ref>. Students' historical knowledge state could be derived from this data <ref type=""bibr"" target=""#b17"">[18]</r",0
"ct answer is no).</p><p>In such scenarios, it is exceptionally challenging to probe the model's mechanism without the support of background knowledge <ref type=""bibr"" target=""#b0"">[1]</ref>. Although Neural Attention Models (NAM) <ref type=""bibr"" target=""#b1"">[2]</ref> are endowed with a certain deg",0
"tterns from historical student's data <ref type=""bibr"" target=""#b12"">[13]</ref>. Students' historical knowledge state could be derived from this data <ref type=""bibr"" target=""#b17"">[18]</ref>.</p><p>The domain knowledge infused model would be able to explain student's performance by tracing the pred traced deeper to the previous grade concept ""Quadratic Equation"".</p><p>Students' behavior is an equally important part of tracing learning outcomes <ref type=""bibr"" target=""#b17"">[18]</ref>. Assessments or diagnostic tests are also created using such behavioral profiling <ref type=""bibr"" target=""# on could also be derived and it could furnish as a foundation for recommendations and estimating learning outcomes upon student's action for the same <ref type=""bibr"" target=""#b17"">[18]</ref>. </p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>VIII. DISCUSSION</head><p>In this article, we hig",0
"mic knowledge graph, set of concepts, it's metadata and relations between concepts that plays an important role in tracing student's concepts mastery <ref type=""bibr"" target=""#b16"">[17]</ref> <ref type=""bibr"" target=""#b15"">[16]</ref>, and (b) Engagement and learning patterns from historical student' tracing learning outcomes <ref type=""bibr"" target=""#b17"">[18]</ref>. Assessments or diagnostic tests are also created using such behavioral profiling <ref type=""bibr"" target=""#b16"">[17]</ref>. Student's tendency to answer a question without comprehending or thinking through can lead to ""Careless Mis",0
"br"" target=""#b9"">[10]</ref><ref type=""bibr"" target=""#b10"">[11]</ref><ref type=""bibr"" target=""#b11"">[12]</ref><ref type=""bibr"" target=""#b12"">[13]</ref><ref type=""bibr"" target=""#b13"">[14]</ref>. To generate diversified results, these methods either explicitly model subtopic coverage of the results <re AD <ref type=""bibr"" target=""#b6"">[7]</ref>, PM2 <ref type=""bibr"" target=""#b7"">[8]</ref>, HxQuAD/HPM2 <ref type=""bibr"" target=""#b8"">[9]</ref> and DSSA <ref type=""bibr"" target=""#b13"">[14]</ref>.</p><p>Those existing approaches used greedy document sequential selection. They compare every single candid the candidate documents are not independent, this strategy may not lead to global optimal rankings. Based on the reinforced learning approach MDP-DIV <ref type=""bibr"" target=""#b13"">[14]</ref>, Feng <ref type=""bibr"" target=""#b14"">[15]</ref> proposed the M2DIV model with the Monte-Caro Tree Search (MC =""formula_4"">𝑠 𝑞 𝑖 = 𝒙 𝑇 𝑞 𝑖 𝒘 𝑟 (𝑖 ∈ [1, 𝑘]).</formula><p>Here 𝒘 𝑟 is a learnable parameter. We use the same relevance features as the previous work <ref type=""bibr"" target=""#b13"">[14]</ref> for 𝒙 𝑞 and 𝒙 𝑞 𝑖 , including BM25, TF-IDF, language model scores, Page Rank, the numbers of incoming links TF-IDF, language model scores, Page Rank, the numbers of incoming links and outgoing links, et al. More details about these features can be found in <ref type=""bibr"" target=""#b13"">[14]</ref> and we omit the details due to space limitation. In the future, we plan to explore more neural-based feature topics. This is because the subtopic embeddings we used are actually the document embeddings. We use the subtopic embeddings released by Jiang et al. <ref type=""bibr"" target=""#b13"">[14]</ref> based on doc2vec. The subtopic embeddings is produced from the pseudo documents of those corresponding subqu irwise sampling. Since the dataset of search result diversification task is limited, we inherit the list-pairwise sampling approach from Jiang et al. <ref type=""bibr"" target=""#b13"">[14]</ref> in order to get enough training samples. We are using pairs of training samples (𝐶, 𝑑 1 , 𝑑 1 ) with common a fair comparison, we are using the document relevance features and embeddings exactly the same as the DSSA, which have been released by Jiang et al. <ref type=""bibr"" target=""#b13"">[14]</ref> in the repository on GitHub<ref type=""foot"" target=""#foot_1"">2</ref> . Those training data includes 18 relev irit of the previous works <ref type=""bibr"" target=""#b10"">[11]</ref><ref type=""bibr"" target=""#b11"">[12]</ref><ref type=""bibr"" target=""#b12"">[13]</ref><ref type=""bibr"" target=""#b13"">[14]</ref>, all those metrics are computed on top 20 results of a document ranking list. Two-tailed paired t-test are u t=""#b10"">[11]</ref>, PAMM <ref type=""bibr"" target=""#b11"">[12]</ref> and PAMM-NTN <ref type=""bibr"" target=""#b12"">[13]</ref>. Inspired by previous work <ref type=""bibr"" target=""#b13"">[14]</ref>, we use the metric of 𝛼 − 𝑛DCG@20 to tune the parameters. The neural tensor network(NTN) is used with both R distributed representations of documents here are 100-dimensional vectors generated by the LDA <ref type=""bibr"" target=""#b31"">[32]</ref>.</p><p>DSSA <ref type=""bibr"" target=""#b13"">[14]</ref>. We train the DSSA model with the code and data released by Jiang et al. on GitHub<ref type=""foot"" target=""# embedding released for a fair comparison. The result is denoted as DSSA (doc2vec).</p><p>Since the deep reinforced learning based models e.g. MDP-DIV <ref type=""bibr"" target=""#b13"">[14]</ref> and M2DIV <ref type=""bibr"" target=""#b14"">[15]</ref> are taking too much time to train, we do not take those ype=""bibr"" target=""#b5"">[6]</ref><ref type=""bibr"" target=""#b6"">[7]</ref><ref type=""bibr"" target=""#b7"">[8]</ref><ref type=""bibr"" target=""#b8"">[9]</ref><ref type=""bibr"" target=""#b13"">14]</ref> (i.e., explicit approaches), or directly reduce result redundancy by comparing document-document similarity r",1
"te the representation for each hidden state of the sequence. Recently, the models fully based on self-attention mechanism (denoted as self-attention  <ref type=""bibr"" target=""#b15"">[16]</ref> in the Neural Machine Translation (NMT) task, have achieved great successes on many NLP tasks. Researchers h",1
"y evaluation metrics of Web Track include ERR-IA <ref type=""bibr"" target=""#b27"">[28]</ref>, 𝛼-nDCG <ref type=""bibr"" target=""#b28"">[29]</ref> and NRBP <ref type=""bibr"" target=""#b29"">[30]</ref>, which are used in our experiments. Besides the metrics above, we also include the metrics of Precision-IA <",0
"cation in order to learn an optimized ranking function automatically <ref type=""bibr"" target=""#b9"">[10]</ref><ref type=""bibr"" target=""#b10"">[11]</ref><ref type=""bibr"" target=""#b11"">[12]</ref><ref type=""bibr"" target=""#b12"">[13]</ref><ref type=""bibr"" target=""#b13"">[14]</ref>. To generate diversified r gardless the use of subtopics <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b9"">[10]</ref><ref type=""bibr"" target=""#b10"">[11]</ref><ref type=""bibr"" target=""#b11"">[12]</ref><ref type=""bibr"" target=""#b12"">[13]</ref> (i.e., implicit approaches).</p><p>To simplify the problem and acce ve also proposed supervised methods, such as SVM-DIV <ref type=""bibr"" target=""#b9"">[10]</ref>, R-LTR <ref type=""bibr"" target=""#b10"">[11]</ref>), PAMM <ref type=""bibr"" target=""#b11"">[12]</ref>, and PAMM-NTN <ref type=""bibr"" target=""#b12"">[13]</ref>), for learning a better document similarity function ll <ref type=""bibr"" target=""#b30"">[31]</ref> (denoted as S-rec). Inheriting the spirit of the previous works <ref type=""bibr"" target=""#b10"">[11]</ref><ref type=""bibr"" target=""#b11"">[12]</ref><ref type=""bibr"" target=""#b12"">[13]</ref><ref type=""bibr"" target=""#b13"">[14]</ref>, all those metrics are com get=""#b9"">[10]</ref> is used to learn a prior relevance function with no diversification.</p><p>R-LTR <ref type=""bibr"" target=""#b10"">[11]</ref>, PAMM <ref type=""bibr"" target=""#b11"">[12]</ref> and PAMM-NTN <ref type=""bibr"" target=""#b12"">[13]</ref>. Inspired by previous work <ref type=""bibr"" target=""#",0
"=""en""> 		<body> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""1"">INTRODUCTION</head><p>Research shows that most queries issued by users are short <ref type=""bibr"" target=""#b0"">[1]</ref><ref type=""bibr"" target=""#b1"">[2]</ref><ref type=""bibr"" target=""#b2"">[3]</ref><ref type=""bibr"" target=""#b3"">[4]",0
"tive ranking 𝜏 − are tuned per query for the PAMM. The distributed representations of documents here are 100-dimensional vectors generated by the LDA <ref type=""bibr"" target=""#b31"">[32]</ref>.</p><p>DSSA <ref type=""bibr"" target=""#b13"">[14]</ref>. We train the DSSA model with the code and data releas",0
"chers have used self-attention networks, e.g. GPT <ref type=""bibr"" target=""#b16"">[17]</ref>, BERT <ref type=""bibr"" target=""#b17"">[18]</ref> and ERNIE <ref type=""bibr"" target=""#b18"">[19]</ref>, as alternatives to RNNs and CNNs in many NLP tasks. However, to the best of our knowledge, only a few resea",0
"e achieved great successes on many NLP tasks. Researchers have used self-attention networks, e.g. GPT <ref type=""bibr"" target=""#b16"">[17]</ref>, BERT <ref type=""bibr"" target=""#b17"">[18]</ref> and ERNIE <ref type=""bibr"" target=""#b18"">[19]</ref>, as alternatives to RNNs and CNNs in many NLP tasks. How veral deeplearning based technologies for feature extraction and document representation e.g. K-NRM <ref type=""bibr"" target=""#b26"">[27]</ref> or BERT <ref type=""bibr"" target=""#b17"">[18]</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>4.1.2</head><p>The evaluation metrics. The officia",0
"e used. Most of the traditional approaches to search result diversification are unsupervised and they are based on handcrafted features and functions <ref type=""bibr"" target=""#b4"">[5]</ref><ref type=""bibr"" target=""#b5"">[6]</ref><ref type=""bibr"" target=""#b6"">[7]</ref><ref type=""bibr"" target=""#b7"">[8] e candidate document is to the selected documents, the more diversified it will be. The most typical implicit model is the MMR (Max Margin Relevance) <ref type=""bibr"" target=""#b4"">[5]</ref> model:</p><formula xml:id=""formula_0"">Score MMR = 𝜆score(𝑑 𝑖 , 𝑞) − (1 − 𝜆)max 𝑑 𝑗 ∈𝑆 sim(𝑑 𝑖 , 𝑑 𝑗 ),</formul 14]</ref> (i.e., explicit approaches), or directly reduce result redundancy by comparing document-document similarity regardless the use of subtopics <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b9"">[10]</ref><ref type=""bibr"" target=""#b10"">[11]</ref><ref type=""bibr"" target=""#b11""",0
"king function automatically <ref type=""bibr"" target=""#b9"">[10]</ref><ref type=""bibr"" target=""#b10"">[11]</ref><ref type=""bibr"" target=""#b11"">[12]</ref><ref type=""bibr"" target=""#b12"">[13]</ref><ref type=""bibr"" target=""#b13"">[14]</ref>. To generate diversified results, these methods either explicitly m bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b9"">[10]</ref><ref type=""bibr"" target=""#b10"">[11]</ref><ref type=""bibr"" target=""#b11"">[12]</ref><ref type=""bibr"" target=""#b12"">[13]</ref> (i.e., implicit approaches).</p><p>To simplify the problem and accelerate the online ranking, existing metho f type=""bibr"" target=""#b9"">[10]</ref>, R-LTR <ref type=""bibr"" target=""#b10"">[11]</ref>), PAMM <ref type=""bibr"" target=""#b11"">[12]</ref>, and PAMM-NTN <ref type=""bibr"" target=""#b12"">[13]</ref>), for learning a better document similarity function automatically. The explicit approaches model the underl ef> (denoted as S-rec). Inheriting the spirit of the previous works <ref type=""bibr"" target=""#b10"">[11]</ref><ref type=""bibr"" target=""#b11"">[12]</ref><ref type=""bibr"" target=""#b12"">[13]</ref><ref type=""bibr"" target=""#b13"">[14]</ref>, all those metrics are computed on top 20 results of a document ran function with no diversification.</p><p>R-LTR <ref type=""bibr"" target=""#b10"">[11]</ref>, PAMM <ref type=""bibr"" target=""#b11"">[12]</ref> and PAMM-NTN <ref type=""bibr"" target=""#b12"">[13]</ref>. Inspired by previous work <ref type=""bibr"" target=""#b13"">[14]</ref>, we use the metric of 𝛼 − 𝑛DCG@20 to tu",0
"re space by adding gradient-based adversarial perturbations to the input node features with graph structures unchanged. FLAG leverages ""free"" methods <ref type=""bibr"" target=""#b27"">(Shafahi et al., 2019)</ref> to conduct efficient adversarial training so that it is highly scalable to large-scale dat ions for graphs in that it happens in the input node feature space.</p><p>Augmentation for ""free"". We leverage the ""free"" adversarial training method <ref type=""bibr"" target=""#b27"">(Shafahi et al., 2019)</ref> to craft adversarial data augmentations. PGD is a strong but inefficient way to solve the",1
"challenging large-scale datasets. <ref type=""bibr"" target=""#b28"">Shchur et al. (2018)</ref>; <ref type=""bibr"" target=""#b7"">Errica et al. (2019);</ref><ref type=""bibr"" target=""#b6"">Dwivedi et al. (2020)</ref> showed that traditional graph datasets suffered from problems such as unrealistic and arbitr",0
"on large-scale datasets usually suffers from overfitting, and realistic graph datasets often involve a high volume of out-of-distribution test nodes <ref type=""bibr"" target=""#b15"">(Hu et al., 2020)</ref>, posing significant challenges for prediction problems.</p><p>One promising solution to combat ent adversarial training so that it is highly scalable to large-scale datasets. We verify the effectiveness of FLAG on the Open Graph Benchmark (OGB) <ref type=""bibr"" target=""#b15"">(Hu et al., 2020)</ref>, which is a collection of large-scale, realistic, and diverse graph datasets for both node and f cross-validation, etc. In order to empirically study FLAG's effects in a fair and reliable manner, we conduct experiments on the newly released OGB <ref type=""bibr"" target=""#b15"">(Hu et al., 2020)</ref> datasets, which have tackled those major issues and brought more realistic challenges to the gr 020)</ref> datasets, which have tackled those major issues and brought more realistic challenges to the graph research community. We refer readers to <ref type=""bibr"" target=""#b15"">Hu et al. (2020)</ref>  Unless otherwise stated, all of the baseline test statistics come from the official OGB leaderb ode feature? One natural question can be raised: what if no input node features are provided? ogbn-proteins is a dataset without input node features. <ref type=""bibr"" target=""#b15"">Hu et al. (2020)</ref> proposed to average incoming edge features to obtain initial node features, while <ref type=""bib model is augmented with virtual nodes <ref type=""bibr"" target=""#b22"">(Li et al., 2017;</ref><ref type=""bibr"" target=""#b12"">Gilmer et al., 2017;</ref><ref type=""bibr"" target=""#b15"">Hu et al., 2020)</ref>. As adversarial perturbations are crafted by gradient ascent, it would be unnatural to perturb d",0
"neficial to standard accuracy <ref type=""bibr"" target=""#b13"">(Goodfellow et al., 2014;</ref><ref type=""bibr"" target=""#b30"">Tsipras et al., 2018;</ref><ref type=""bibr"" target=""#b24"">Miyato et al., 2018)</ref>. With an increasing amount of attention paid to leverage adversarial training for better cle ngs within communities similar. All three methods assigned pseudo-labels to test nodes during training time and utilized virtual adversarial training <ref type=""bibr"" target=""#b24"">(Miyato et al., 2018)</ref> to make test node predictions similar to their pseudo-labels. This makes them workable for",0
"SSL allows us to exploit the unlabeled data space via making changes on the input labeled data, achieving remarkable improvements in downstream tasks <ref type=""bibr"" target=""#b4"">[5]</ref>.</p><p>Here we wish to bring the SSL's superiority into recommendation representation learning, which differs ws of the same node for prediction, while the supervision of negative pairs enforces the divergence among different nodes. Formally, we follow SimCLR <ref type=""bibr"" target=""#b4"">[5]</ref> and adopt the contrastive loss, InfoNCE <ref type=""bibr"" target=""#b11"">[12]</ref>, to maximize the agreement o >3</ref>). Our self-supervised task is able to offer a better initialization for LightGCN, which is consistent to the observation in previous studies <ref type=""bibr"" target=""#b4"">[5]</ref>. However, the better performance of joint training admits that the representations in the main and auxiliary t <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.1"">Data Augmentation</head><p>Directly grafting the data augmentation adopted in CV and NLP tasks <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b44"">45 models <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b28"">29,</ref><ref type=""bibr"" target=""#b36"">37]</ref> and contrastive models <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b37"">38 be in either global-local contrast <ref type=""bibr"" target=""#b19"">[20,</ref><ref type=""bibr"" target=""#b37"">38]</ref> or global-global contrast manner <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b14"">15]</ref>. The former focuses on modeling the relationship between the local par b27"">[28]</ref>.While the latter directly performs comparison between different samples, which typically requires multiple different views of samples <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b34"">35]</ref>. There are both pros and cons e is 𝑂 (|𝐸| 𝑑 (2 + |𝑉 |)𝑠).An alternative to reduce the time complexity is treating only the users (or the items) within the batch as negative samples<ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b46"">47]</ref>, resulting in total time complexity of 𝑂 (|𝐸| 𝑑 (2 + 2𝐵)𝑠).</note></fi",1
"ffective end-to-end way to integrate multi-hop neighbors into node representation learning and achieve state-ofthe-art performance for recommendation <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" target=""#b43"">44,</ref><ref type=""bibr"" target=""#b47 ls suffer from some limitations:</p><p>• Sparse Supervision Signal. Most models approach the recommendation task under a supervised learning paradigm <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b18"">19,</ref><ref type=""bibr"" target=""#b31"">32]</ref>, where the supervision signa |𝑢 ∈ U, 𝑖 ∈ I} be the observed interactions between users and items, where 𝑦 𝑢𝑖 indicates that user 𝑢 has adopted item 𝑖 before. Most existing models <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" target=""#b43"">44]</ref> construct a bipartite graph s=""http://www.tei-c.org/ns/1.0""><head n=""4.1"">Experimental Settings</head><p>We conduct experiments on three widely used benchmark datasets: Yelp2018 <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b43"">44]</ref>, Amazon-Book <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bib ents on three widely used benchmark datasets: Yelp2018 <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b43"">44]</ref>, Amazon-Book <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b43"">44]</ref>, and Alibaba-iFashion <ref type=""bibr"" target=""#b5"">[6]</ref> <ref t pe=""bibr"" target=""#b43"">44]</ref>, and Alibaba-iFashion <ref type=""bibr"" target=""#b5"">[6]</ref> <ref type=""foot"" target=""#foot_0"">1</ref> . Following <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b43"">44]</ref>, we use the same 10-core setting for Yelp2018 and Amazon-Book. Aliba get=""#b18"">[19]</ref>, GC-MC <ref type=""bibr"" target=""#b35"">[36]</ref>, and PinSage <ref type=""bibr"" target=""#b47"">[48]</ref> since the previous work <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b26"">27,</ref><ref type=""bibr"" target=""#b43"">44]</ref> has validated the superiorit e graph structure as transition probabilities <ref type=""bibr"" target=""#b1"">[2]</ref>, to GCN that propagates user and item embeddings over the graph <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" target=""#b43"">44,</ref><ref type=""bibr"" target=""#b47 can be applied to any model that consists of user embedding and item embedding. Here we implement it on a state-of-the-art GCN-based model, LightGCN <ref type=""bibr"" target=""#b16"">[17]</ref>. Experimental studies on three benchmark datasets demonstrate the effectiveness of SGL, which significantly f type=""bibr"" target=""#b35"">[36,</ref><ref type=""bibr"" target=""#b47"">48]</ref>, concatenation <ref type=""bibr"" target=""#b43"">[44]</ref>, or summation <ref type=""bibr"" target=""#b16"">[17]</ref> over the representations of all layers.</p><p>Supervised Learning Loss. Thereafter, a prediction layer is bu message during message passing. We tune the regularization coefficient 𝜆 2 and the number of GCN layers within the suggested ranges.</p><p>• LightGCN <ref type=""bibr"" target=""#b16"">[17]</ref>. This is the state-of-the-art graph-based CF method which devises a light graph convolution to ease the trai gDesc>The comparison of analytical time complexity between LightGCN and SGL-ED.trainable parameters, the space complexity remains the same as LightGCN<ref type=""bibr"" target=""#b16"">[17]</ref>. The time complexity of model inference is also the same, since there is no change on the model structure. I",1
"ction history to exploiting higher-order neighbors. This leads to the success of graph convolution networks (GCNs) for recommendation such as PinSage <ref type=""bibr"" target=""#b47"">[48]</ref> and LightGCN [17]. Despite effectiveness, we argue that they suffer from two limitations: (1) high-degree no f type=""bibr"" target=""#b31"">[32]</ref>, NeuMF <ref type=""bibr"" target=""#b18"">[19]</ref>, GC-MC <ref type=""bibr"" target=""#b35"">[36]</ref>, and PinSage <ref type=""bibr"" target=""#b47"">[48]</ref> since the previous work <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b26"">27,</ref><re rformance for recommendation <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" target=""#b43"">44,</ref><ref type=""bibr"" target=""#b47"">48]</ref>.</p><p>Despite effectiveness, current GCN-based recommendation models suffer from some limitations:</p><p>• S 𝑙 = [0, • • • , 𝐿]} ,<label>(3)</label></formula><p>which can be simply set as the last-layer representation <ref type=""bibr"" target=""#b35"">[36,</ref><ref type=""bibr"" target=""#b47"">48]</ref>, concatenation <ref type=""bibr"" target=""#b43"">[44]</ref>, or summation <ref type=""bibr"" target=""#b16"">[17]</r em embeddings over the graph <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b35"">36,</ref><ref type=""bibr"" target=""#b43"">44,</ref><ref type=""bibr"" target=""#b47"">48]</ref>. Recently, attention mechanism is introduced into GCN-based recommendation models <ref type=""bibr"" target=""#b",0
"signal comes from the observed user-item interactions. However, the observed interactions are extremely sparse <ref type=""bibr"" target=""#b2"">[3,</ref><ref type=""bibr"" target=""#b15"">16]</ref> compared to the whole interaction space, making it insufficient to learn quality representations. • Skewed Da",0
"mining the user-item graph. The research attention has been evolved from the random walk that encodes the graph structure as transition probabilities <ref type=""bibr"" target=""#b1"">[2]</ref>, to GCN that propagates user and item embeddings over the graph <ref type=""bibr"" target=""#b16"">[17,</ref><ref de information apart from user-item interactions, which ranges from user social relations <ref type=""bibr"" target=""#b0"">[1]</ref>, item co-occurrence <ref type=""bibr"" target=""#b1"">[2]</ref>, to user and item attributes <ref type=""bibr"" target=""#b25"">[26]</ref>. Recently, Knowledge Graph (KG) is also",0
"ser social relations <ref type=""bibr"" target=""#b0"">[1]</ref>, item co-occurrence <ref type=""bibr"" target=""#b1"">[2]</ref>, to user and item attributes <ref type=""bibr"" target=""#b25"">[26]</ref>. Recently, Knowledge Graph (KG) is also unified with the user-item graph, which enables considering the deta",0
"e=""bibr"" target=""#b24"">25,</ref><ref type=""bibr"" target=""#b37"">38]</ref> and natural language processing (NLP) <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b23"">24]</ref>, SSL is relatively less explored in recommendation. The idea is to set an auxiliary task that distills superv",0
"=""#b31"">[32]</ref> projects single ID of each user (or item) into an embedding vector. Some followon studies <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b22"">23]</ref> enrich the single ID with interaction history for learning better representations. More recently, representat",0
"actorization (MF) <ref type=""bibr"" target=""#b31"">[32]</ref> projects single ID of each user (or item) into an embedding vector. Some followon studies <ref type=""bibr"" target=""#b17"">[18,</ref><ref type=""bibr"" target=""#b22"">23]</ref> enrich the single ID with interaction history for learning better re",0
"d><p>Directly grafting the data augmentation adopted in CV and NLP tasks <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b7"">8,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b44"">45]</ref> is infeasible for graph-based recommendation, due to specific charact 29,</ref><ref type=""bibr"" target=""#b36"">37]</ref> and contrastive models <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b37"">38]</ref>. Auto-encoding is the most popular generative model which learns to r ly performs comparison between different samples, which typically requires multiple different views of samples <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b14"">15,</ref><ref type=""bibr"" target=""#b34"">35]</ref>. There are both pros and cons of the generative model and contrastive type=""bibr"" target=""#b19"">[20,</ref><ref type=""bibr"" target=""#b37"">38]</ref> or global-global contrast manner <ref type=""bibr"" target=""#b4"">[5,</ref><ref type=""bibr"" target=""#b14"">15]</ref>. The former focuses on modeling the relationship between the local part of a sample and its global context re",0
"""> 		<body> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""1"">INTRODUCTION</head><p>Self-attention-based architectures, in particular Transformers <ref type=""bibr"" target=""#b43"">(Vaswani et al., 2017)</ref>, have become the model of choice in natural language processing (NLP). The dominant approa sed in prior works.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3"">METHOD</head><p>In model design we follow the original Transformer <ref type=""bibr"" target=""#b43"">(Vaswani et al., 2017)</ref> as closely as possible. An advantage of this intentionally simple setup is that scalable N D-aware position embeddings (Appendix D.3). The resulting sequence of embedding vectors serves as input to the encoder.</p><p>The Transformer encoder <ref type=""bibr"" target=""#b43"">(Vaswani et al., 2017)</ref> consists of alternating layers of multiheaded selfattention (MSA, see Appendix A) and MLP 63% on the VTAB suite of 19 tasks.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2"">RELATED WORK</head><p>Transformers were proposed by <ref type=""bibr"" target=""#b43"">Vaswani et al. (2017)</ref> for machine translation, and have since become the state of the art method in many NLP task >  			<div type=""annex""> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>APPENDIX A MULTIHEAD SELF-ATTENTION</head><p>Standard qkv self-attention (SA, <ref type=""bibr"" target=""#b43"">Vaswani et al. (2017)</ref>) is a popular building block for neural architectures. For each element in an input sequenc",1
"lfattention (MSA, see Appendix A) and MLP blocks (Eq. 2, 3). Layernorm (LN) is applied before every block, and residual connections after every block <ref type=""bibr"" target=""#b46"">(Wang et al., 2019;</ref><ref type=""bibr"" target=""#b2"">Baevski &amp; Auli, 2019)</ref>. The MLP contains two layers wit",0
"l architectures remain dominant <ref type=""bibr"" target=""#b25"">(LeCun et al., 1989;</ref><ref type=""bibr"" target=""#b24"">Krizhevsky et al., 2012;</ref><ref type=""bibr"" target=""#b15"">He et al., 2016)</ref>. Inspired by NLP successes, multiple works try combining CNN-like architectures with self-attent l to the square of the patch size, thus models with smaller patch size are computationally more expensive.</p><p>For the baseline CNNs, we use ResNet <ref type=""bibr"" target=""#b15"">(He et al., 2016)</ref>, but replace the Batch Normalization layers <ref type=""bibr"" target=""#b21"">(Ioffe &amp; Szegedy",0
"exploration of contrastive pre-training <ref type=""bibr"" target=""#b8"">(Chen et al., 2020b;</ref><ref type=""bibr"" target=""#b16"">He et al., 2020;</ref><ref type=""bibr"" target=""#b1"">Bachman et al., 2019;</ref><ref type=""bibr"">Hénaff et al., 2020)</ref> to future work.</p></div> <div xmlns=""http://www.",0
"Net co-trained on ImageNet and Youtube <ref type=""bibr"">(Tschannen et al., 2020)</ref>, and S4L -supervised plus semi-supervised learning on ImageNet <ref type=""bibr"" target=""#b52"">(Zhai et al., 2019a)</ref>.</p><p>ViT-H/14 outperforms BiT-R152x4, and other methods, on the Natural and Structured tas",0
"p>The Approximate Personalized Propagation of Neural Predictions (APPNP) framework is most relevant to our work, as they also smooth base predictions <ref type=""bibr"" target=""#b22"">(Klicpera et al., 2018)</ref>. However, they focus on integrating this smoothing into the training process so that thei lation, the smoothing here is a post-processing step, decoupled from the other steps. This type of prediction smoothing is similar in spirit to APPNP <ref type=""bibr"" target=""#b22"">(Klicpera et al., 2018)</ref>, which we compare against later. However, APPNP is trained end-to-end, propagates on fina",1
"t=""#b42"">(Wu et al., 2019)</ref>, as well as algorithms designed to increase scalability <ref type=""bibr"" target=""#b1"">(Bojchevski et al., 2020;</ref><ref type=""bibr"" target=""#b47"">Zeng et al., 2019;</ref><ref type=""bibr"" target=""#b35"">Rossi et al., 2020)</ref>. The primary focus of our approach, ho",0
"tive input features to make predictions. There are numerous ways to get useful features from just the graph topology to augment the raw node features <ref type=""bibr"" target=""#b15"">(Henderson et al., 2011;</ref><ref type=""bibr"">2012;</ref><ref type=""bibr"" target=""#b14"">Hamilton et al., 2017b)</ref>.",0
"ments the Simplified Graph Convolution <ref type=""bibr"" target=""#b42"">(Wu et al., 2019)</ref>, as well as algorithms designed to increase scalability <ref type=""bibr"" target=""#b1"">(Bojchevski et al., 2020;</ref><ref type=""bibr"" target=""#b47"">Zeng et al., 2019;</ref><ref type=""bibr"" target=""#b35"">Ros",0
"on, 2020)</ref>. Finally, we use an email dataset of a European research institute, where classes are department membership and there are no features <ref type=""bibr"" target=""#b24"">(Leskovec et al., 2007;</ref><ref type=""bibr"" target=""#b46"">Yin et al., 2017)</ref>.</p><p>Data splits. The training/va #b10"">(Getoor et al., 2001;</ref><ref type=""bibr"" target=""#b9"">Getoor, 2005;</ref><ref type=""bibr"" target=""#b29"">Namata et al., 2012)</ref> and Email <ref type=""bibr"" target=""#b24"">(Leskovec et al., 2007;</ref><ref type=""bibr"" target=""#b46"">Yin et al., 2017)</ref>: 3 layers and 64 hidden channels wi",0
"ere used to construct the graph. Since then, these techniques have been used for learning on relational data from just the labels (i.e., no features) <ref type=""bibr"" target=""#b23"">(Koutra et al., 2011;</ref><ref type=""bibr"" target=""#b11"">Gleich &amp; Mahoney, 2015;</ref><ref type=""bibr"">Peel, 2017;",0
"rom just the graph topology to augment the raw node features <ref type=""bibr"" target=""#b15"">(Henderson et al., 2011;</ref><ref type=""bibr"">2012;</ref><ref type=""bibr"" target=""#b14"">Hamilton et al., 2017b)</ref>. In our pipeline, we augment features with a regularized spectral embedding <ref type=""bi",0
"ions of nodes and edges respectively, without explicit data augmentation. Moreover, to supplement the input graph with more global information, MVGRL <ref type=""bibr"" target=""#b14"">[15]</ref> proposes to augment the input graph using graph diffusion. Then, it constructs graph views by uniformly samp 0]</ref> employs two discriminators to directly measure MI between input and representations of both nodes and edges without data augmentation; MVGRL <ref type=""bibr"" target=""#b14"">[15]</ref> proposes to learn both node-level and graph-level representations by performing node diffusion and contrasti mmary, we provide a brief comparison between the  <ref type=""bibr"" target=""#b43"">[44]</ref>, GMI <ref type=""bibr"" target=""#b29"">[30]</ref>, and MVGRL <ref type=""bibr"" target=""#b14"">[15]</ref> in Table <ref type=""table"" target=""#tab_0"">1</ref>, where the two columns ""Topology"" and ""Attribute"" denote ref>, Graphical Mutual Information Maximization (GMI) <ref type=""bibr"" target=""#b29"">[30]</ref>, and Multi-View Graph Representation Learning (MVGRL) <ref type=""bibr"" target=""#b14"">[15]</ref>. Furthermore, we report the performance obtained using a logistic regression classifier on raw node features",1
"Then, an objective based on MI maximization is proposed to maximize the MI between node embeddings and a global summary embedding. Following DGI, GMI <ref type=""bibr"" target=""#b29"">[30]</ref> proposes two node-level contrastive objectives to directly measure MI between input and representations of n it graph embedding, but rather focus on maximizing the agreement of node embeddings across two corrupted views of the graph.</p><p>Following DGI, GMI <ref type=""bibr"" target=""#b29"">[30]</ref> employs two discriminators to directly measure MI between input and representations of both nodes and edges ith related graph contrastive learning methods. In summary, we provide a brief comparison between the  <ref type=""bibr"" target=""#b43"">[44]</ref>, GMI <ref type=""bibr"" target=""#b29"">[30]</ref>, and MVGRL <ref type=""bibr"" target=""#b14"">[15]</ref> in Table <ref type=""table"" target=""#tab_0"">1</ref>, whe =""bibr"" target=""#b20"">[21]</ref>, Deep Graph Infomax (DGI) <ref type=""bibr"" target=""#b43"">[44]</ref>, Graphical Mutual Information Maximization (GMI) <ref type=""bibr"" target=""#b29"">[30]</ref>, and Multi-View Graph Representation Learning (MVGRL) <ref type=""bibr"" target=""#b14"">[15]</ref>. Furthermore",1
"samples as all other nodes in the two views. Therefore, negative samples are from two sources, intra-view (in purple) and inter-view nodes (in red). <ref type=""bibr"" target=""#b47"">[48]</ref>, the data augmentation strategies should be adaptive to the input graph to reflect its intrinsic patterns. A g methods that maximize agreement between views seek to learn representation that is invariant to perturbation introduced by the augmentation schemes <ref type=""bibr"" target=""#b47"">[48]</ref>. In the model, we propose to design augmentation schemes that tend to keep important structures and attribut",0
"usually find a global summary vector <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b17"">18]</ref> or patches in neighboring views <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b41"">42]</ref> as the positive sample, and contrast them with negative-sampled coun",0
"ith the highest degrees in citation networks are likely to correspond to influential papers.</p><p>Eigenvector centrality. The eigenvector centrality <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b26"">27]</ref> of a node is calculated as its eigenvector corresponding to the larges",0
"<ref type=""bibr"" target=""#b38"">39]</ref> and natural language processing <ref type=""bibr"" target=""#b3"">[4,</ref><ref type=""bibr"" target=""#b5"">6,</ref><ref type=""bibr"" target=""#b25"">26]</ref>. These CL methods seek to maximize the Mutual Information (MI) between the input (i.e. images) and its repres",0
"dant labeled nodes for training. Recently, Contrastive Learning (CL), as revitalization of the classical information maximization (InfoMax) principle <ref type=""bibr"" target=""#b23"">[24]</ref>, achieves great success in many fields, e.g., visual representation learning <ref type=""bibr"" target=""#b0"">[ ""#b32"">[33]</ref>. Objectives used in these methods can be seen as maximizing the lower bounds of MI between input features and their representations <ref type=""bibr"" target=""#b23"">[24]</ref>. However, recent work <ref type=""bibr"" target=""#b40"">[41]</ref> reveals that downstream performance in evalu",0
"arget=""#b6"">7]</ref>, consisting of color jitter, random flip, cropping, resizing, rotation <ref type=""bibr"" target=""#b8"">[9]</ref>, color distortion <ref type=""bibr"" target=""#b22"">[23]</ref>, etc. Existing work <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b38"">39,</ref><ref ty",0
"n-batch negative samples. For an image patch as the anchor, these methods usually find a global summary vector <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b17"">18]</ref> or patches in neighboring views <ref type=""bibr"" target=""#b16"">[17,</ref><ref type=""bibr"" target=""#b41"">42]</ =""#b41"">42]</ref> as the positive sample, and contrast them with negative-sampled counterparts, such as patches of other images within the same batch <ref type=""bibr"" target=""#b17"">[18]</ref>.</p><p>Theoretical analysis sheds light on the reasons behind their success <ref type=""bibr"" target=""#b32"">[",0
"e=""bibr"" target=""#b23"">[24]</ref>, achieves great success in many fields, e.g., visual representation learning <ref type=""bibr"" target=""#b0"">[1,</ref><ref type=""bibr"" target=""#b15"">16,</ref><ref type=""bibr"" target=""#b38"">39]</ref> and natural language processing <ref type=""bibr"" target=""#b3"">[4,</re cropping, resizing, rotation <ref type=""bibr"" target=""#b8"">[9]</ref>, color distortion <ref type=""bibr"" target=""#b22"">[23]</ref>, etc. Existing work <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b38"">39,</ref><ref type=""bibr"" target=""#b46"">47]</ref> employs a memory bank for st",0
"icality-awareness needed to increase oversubscription.</p><p>Accurately predicting (black-box) workload criticality is itself a challenge. Prior work <ref type=""bibr"" target=""#b5"">[6]</ref> associated a diurnal utilization pattern with user interactivity and the critical need for high performance. I ww.tei-c.org/ns/1.0""><head>D. Azure's existing ML system</head><p>To integrate predictions into VM scheduling in practice, we target Resource Central <ref type=""bibr"" target=""#b5"">[6]</ref>, the existing ML and predictionserving system in Azure. The system provides a REST service for clients (in our method to determine the VM labels, i.e. whether the workload of each VM is performance-critical or not, before we can train a model. As in prior work <ref type=""bibr"" target=""#b5"">[6]</ref>, we consider a workload critical if it is user-facing, i.e. a human is interacting with the workload (e.g., fr daily (e.g., high during the day, low at night), the problem reduces to identifying VMs whose time series of CPU utilizations exhibit 24-hour periods <ref type=""bibr"" target=""#b5"">[6]</ref>.</p><p>Obviously, some background VMs may exhibit 24-hour periods. This is not a problem as we seek to be cons ng periodicity. There are statistical methods for identifying periods in time series, such as FFT or the autocorrelation function (ACF). For example, <ref type=""bibr"" target=""#b5"">[6]</ref> assumes a workload is user-facing if the FFT indicates a 24-hour period. We evaluated ACF and FFT methods on 8 dict resource demand, resource utilization, or job/task length for provisioning or scheduling purposes, e.g. <ref type=""bibr"" target=""#b4"">[5]</ref>, <ref type=""bibr"" target=""#b5"">[6]</ref>, <ref type=""bibr"" target=""#b9"">[10]</ref>, <ref type=""bibr"" target=""#b15"">[16]</ref>, <ref type=""bibr"" target=",1
"scheduling purposes, e.g. <ref type=""bibr"" target=""#b4"">[5]</ref>, <ref type=""bibr"" target=""#b5"">[6]</ref>, <ref type=""bibr"" target=""#b9"">[10]</ref>, <ref type=""bibr"" target=""#b15"">[16]</ref>, <ref type=""bibr"" target=""#b16"">[17]</ref>, <ref type=""bibr"" target=""#b27"">[28]</ref>. In contrast, we intro",0
"t=""#b12"">[13]</ref>, <ref type=""bibr"" target=""#b14"">[15]</ref>, <ref type=""bibr"" target=""#b19"">[20]</ref>, <ref type=""bibr"" target=""#b21"">[22]</ref>- <ref type=""bibr"" target=""#b24"">[25]</ref>, <ref type=""bibr"" target=""#b26"">[27]</ref>, <ref type=""bibr"" target=""#b33"">[34]</ref>. Both modeling/optimiz",0
"structures efficiently in a parallel program can be challenging. A number of GPM frameworks have been proposed to reduce the burden on the programmer <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b28"">29,</ref><ref type=""bibr"" target=""#b37"">38,</ref><ref type=""bibr"" target=""#b38 but they may not allow expressing more efficient algorithms. Low-level systems such as RStream <ref type=""bibr"" target=""#b55"">[56]</ref> and Pangolin <ref type=""bibr"" target=""#b11"">[12]</ref> provide low-level API functions for the user to control the details of mining process, and they can be used ions written using Sandslash high-level API outperform the state-of-the-art GPM systems, AutoMine <ref type=""bibr"" target=""#b38"">[39]</ref>, Pangolin <ref type=""bibr"" target=""#b11"">[12]</ref>, and Peregrine <ref type=""bibr"" target=""#b28"">[29]</ref> by 7.7×, 6.2× and 3.9× on average, respectively. Ap We call this bit-vector the connectivity code (see an example in Appendix B.6). This technique is called Memoization of Embedding Connectivity (MEC) <ref type=""bibr"" target=""#b11"">[12]</ref>. • For edge-induced extension, a set of edges instead of vertices is stored for each embedding. There is no /p><p>Fine-Grained Pruning (FP) and Customized Pattern Classification (CP): FP and CP are low-level optimizations enabled in a prior system, Pangolin <ref type=""bibr"" target=""#b11"">[12]</ref>, so we describe them in Appendices B.4 and B.5. To support FP, Sandslash exposes API calls toExtend() and to dslash with the state-of-the-art GPM systems <ref type=""foot"" target=""#foot_0"">3</ref> : AutoMine <ref type=""bibr"" target=""#b38"">[39]</ref>, Pangolin <ref type=""bibr"" target=""#b11"">[12]</ref>, and Peregrine <ref type=""bibr"" target=""#b28"">[29]</ref>. We use the five applications (also used in previou mory consumption. G-Miner <ref type=""bibr"" target=""#b10"">[11]</ref> is a distributed GPM system which incorporates task-parallel processing. Pangolin <ref type=""bibr"" target=""#b11"">[12]</ref> is a shared-memory GPM system targeting both CPU and GPU. Instead of the BFS exploration used in the above s the connectivity check can be used to identify different patterns. For small implicit patterns, Sandslash uses customized pattern classification (CP) <ref type=""bibr"" target=""#b11"">[12]</ref>. For example, in FSM, the labeled wedge patterns can be differentiated by hashing the labels of the three ve",1
"tern tree.</p><p>3 Sandslash API Fig. <ref type=""figure"" target=""#fig_2"">3</ref> shows the overview of Sandslash, which is built on top of the Galois <ref type=""bibr"" target=""#b41"">[42]</ref> parallel system. In this section, we describe the high-level and low-level API of Sandslash.</p></div> <div",0
"the burden on the programmer <ref type=""bibr"" target=""#b11"">[12,</ref><ref type=""bibr"" target=""#b28"">29,</ref><ref type=""bibr"" target=""#b37"">38,</ref><ref type=""bibr"" target=""#b38"">39,</ref><ref type=""bibr"" target=""#b53"">54,</ref><ref type=""bibr"" target=""#b55"">56,</ref><ref type=""bibr"" target=""#b60"" gramming compared to hand-optimized code, but they make different tradeoffs and have different limitations.</p><p>High-level systems such as AutoMine <ref type=""bibr"" target=""#b38"">[39]</ref> and Peregrine <ref type=""bibr"" target=""#b28"">[29]</ref> take specifications of patterns as input and leverag aluation on a 56-core CPU demonstrates that applications written using Sandslash high-level API outperform the state-of-the-art GPM systems, AutoMine <ref type=""bibr"" target=""#b38"">[39]</ref>, Pangolin <ref type=""bibr"" target=""#b11"">[12]</ref>, and Peregrine <ref type=""bibr"" target=""#b28"">[29]</ref> gh-level and low-level optimizations. We compare Sandslash with the state-of-the-art GPM systems <ref type=""foot"" target=""#foot_0"">3</ref> : AutoMine <ref type=""bibr"" target=""#b38"">[39]</ref>, Pangolin <ref type=""bibr"" target=""#b11"">[12]</ref>, and Peregrine <ref type=""bibr"" target=""#b28"">[29]</ref> rthogonal to our work. The focus of our work is utilizing in-memory CPU platforms to get the best performance.</p><p>High-level GPM Systems: AutoMine <ref type=""bibr"" target=""#b38"">[39]</ref> is a DFS based system targeting a single-machine. It provides a high-level programming interface and employs",0
"ing a CNN network where to look and what to pay attention to, attention networks achieve a better performance with fewer layers. As an example, SENet <ref type=""bibr"" target=""#b5"">[5]</ref> introduces Squeeze-and-Excitation (SE) blocks to study the channel dependencies in a CNN architecture. Althoug s not introduce any additional parameter. All layers in the spatial pyramid structure are not learnable, which is nearly cost-free. Compared to SENet <ref type=""bibr"" target=""#b5"">[5]</ref>, our structure only modifies the first fully-connected layer to tackle the large input size. The small computa All these paths are integrated together using filter concatenation as input to the next block. More recently, attention based networks such as SENet <ref type=""bibr"" target=""#b5"">[5]</ref> and CBAM <ref type=""bibr"">[6]</ref> provide an independent attention path to learn the weight of each channel o selectively emphasize salient features as well as suppress insignificant features. Thus, visual features could be better captured and exploited. In <ref type=""bibr"" target=""#b5"">[5]</ref>, a Squeeze-and-Extraction block was proposed to learn the channel-wise attention for each convolutional layer, at existing work on global average pooling used the last feature map which is small in size (7 ? 7 for example). However, attention based CNNs (e.g., <ref type=""bibr"" target=""#b5"">[5]</ref>, <ref type=""bibr"">[6]</ref>, <ref type=""bibr"" target=""#b7"">[7]</ref>, etc.) apply global average pooling on ea endency and its non-linear expression affects the effectiveness of the attention mechanism. To address this problem, we leverage the excitation block <ref type=""bibr"" target=""#b5"">[5]</ref> to encode v and generate a 1D attention map ?. The excitation block employs two fully-connected layers. Then a mula><formula xml:id=""formula_6"">)</formula><p>where ? is a rectified linear unit (ReLU) function and sig denotes the sigmoid function. Like in SENet <ref type=""bibr"" target=""#b5"">[5]</ref>, we set r to 16.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.3.3."">Point-wise Convolution</h 0 and a down-sampled ImageNet dataset. Without bells and whistles, SPANet outperforms related stateof-art work <ref type=""bibr"" target=""#b2"">[2,</ref><ref type=""bibr"" target=""#b5"">5,</ref><ref type=""bibr"" target=""#b10"">10,</ref><ref type=""bibr"" target=""#b11"">11]</ref>. Experimental results show that rks follow a path design pattern: they learn an attention map from a feature map and then apply the learned attention map to the original feature map <ref type=""bibr"" target=""#b5"">[5,</ref><ref type=""bibr"" target=""#b18"">18]</ref> . However, being confined to aforementioned schema compromises the exp pecified.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.3."">Spatial Pyramid Attention</head><p>Many existing attention based networks <ref type=""bibr"" target=""#b5"">[5,</ref><ref type=""bibr"">6,</ref><ref type=""bibr"">6,</ref><ref type=""bibr"" target=""#b17"">17]</ref> aggregate input feat",1
"orks represent different network architectures. MobileNetV2 is typically designed for lightweight models like<ref type=""bibr"" target=""#b19"">[19,</ref><ref type=""bibr"" target=""#b27"">27,</ref><ref type=""bibr"" target=""#b28"">28]</ref>. DenseNet includes shortcut connections. ResNeXt is the first one tha",0
"erforms related stateof-art work <ref type=""bibr"" target=""#b2"">[2,</ref><ref type=""bibr"" target=""#b5"">5,</ref><ref type=""bibr"" target=""#b10"">10,</ref><ref type=""bibr"" target=""#b11"">11]</ref>. Experimental results show that structural information in the attention mechanism, which we focus on, is a cr ResNet. We also apply SPANet and SENet to several other base CNN architectures, including VGG <ref type=""bibr"" target=""#b25"">[25]</ref>, MobileNetV2 <ref type=""bibr"" target=""#b11"">[11]</ref>, DenseNet <ref type=""bibr"" target=""#b10"">[10]</ref>, and ResNext <ref type=""bibr"" target=""#b26"">[26]</ref>, head><p>We compare the performance of our SPANet with SENet and the base networks. We employ four base networks, i.e., light-weight model MobileNetV2 <ref type=""bibr"" target=""#b11"">[11]</ref>, heavy-weight model DenseNet <ref type=""bibr"" target=""#b10"">[10]</ref>, ResNeXt <ref type=""bibr"" target=""#b2",0
"parameters and prolong inference time.</p><p>In addition to making neural networks deeper, other efforts focus on investigating attention mechanisms <ref type=""bibr"" target=""#b4"">[4]</ref> in CNNs. By informing a CNN network where to look and what to pay attention to, attention networks achieve a b dependent attention path to learn the weight of each channel and achieve state-of-the-art performance.</p><p>Attention Mechanism. Attention Mechanism <ref type=""bibr"" target=""#b4"">[4]</ref> has been prevailed in computer vision for years <ref type=""bibr"" target=""#b16"">[16]</ref>. By adopting a gatin",0
"ition and image classification etc. In this active research area, the studies using deep learning approaches mostly focus on dimensionality reduction <ref type=""bibr"" target=""#b1"">[2]</ref>- <ref type=""bibr"" target=""#b5"">[6]</ref> and anomaly-based intrusion detection <ref type=""bibr"" target=""#b6"">[",1
"e provided as inputs to the multi-layer group of SVMs which were constructed through the iterative reduce paradigm based on Spark. Vinayakumar et al. <ref type=""bibr"" target=""#b22"">[23]</ref> explored a deep neural network (DNN) to create a useful and flexible IDS, named ''scale-hybrid-IDS-AlertNet' es various types of attacks, which had been carried out on networks recently, according to a McAfee report <ref type=""bibr"" target=""#b61"">[62]</ref>, <ref type=""bibr"" target=""#b22"">[23]</ref>, 2) It is up-to-date, 3) It is a labelled dataset consisting of flow-based features expanded by measuring so isting of flow-based features expanded by measuring some parameters statistically, 4) It possesses the characteristics of a real-time network traffic <ref type=""bibr"" target=""#b22"">[23]</ref>, 5) It is non-linearly separable <ref type=""bibr"" target=""#b22"">[23]</ref>. Table <ref type=""table"" target="" ically, 4) It possesses the characteristics of a real-time network traffic <ref type=""bibr"" target=""#b22"">[23]</ref>, 5) It is non-linearly separable <ref type=""bibr"" target=""#b22"">[23]</ref>. Table <ref type=""table"" target=""#tab_0"">1</ref> presents the number of flow examples in CICDS2017 categoriz",0
"s used to evaluate the methods in the studies <ref type=""bibr"" target=""#b5"">[6]</ref>, <ref type=""bibr"" target=""#b11"">[12]</ref>. The NSL-KDD dataset <ref type=""bibr"" target=""#b12"">[13]</ref>, which is a revised version of KDDCUP99, is used for evaluating the methods proposed in the studies <ref typ",0
""" target=""#fig_1"">2</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>B. VARIATIONAL AUTOENCODER</head><p>Variational Autoencoder (VAE) <ref type=""bibr"" target=""#b42"">[43]</ref> is defined as a directed probabilistic graphical model, which is obtained by approximation of an artificial",0
"ions by using NetFlow traffic data.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>III. BACKGROUND A. AUTOENCODER</head><p>Autoencoder (AE) <ref type=""bibr"" target=""#b38"">[39]</ref>, <ref type=""bibr"" target=""#b39"">[40]</ref> is a neural network method, which has an operating logic that tra",0
"sted method used two different sets, which were common flow attributes and TCF flags attributes, extracted from the datasets. Stevanovic and Pedersen <ref type=""bibr"" target=""#b35"">[36]</ref> presented an effective botnet detection approach using flow records of a 39-feature set employing a collecti",0
"ata was considered was carried out by feeding the Netflow records into kernel function and forwarding the calculated results to an OCSVM. Umer et al. <ref type=""bibr"" target=""#b25"">[26]</ref> proposed an intrusion detection model, which handled the flow data. The two-stage model developed used OCSVM",0
"s, to cooperatively distinguish between normal and abnormal traffic patterns with a performance comparable to offline anomaly detectors. Marir et al. <ref type=""bibr"" target=""#b21"">[22]</ref> presented a novel distributed method for identifying abnormal behavior utilizing a group of multi-layer SVMs",0
"dataset <ref type=""bibr"" target=""#b12"">[13]</ref>, which is a revised version of KDDCUP99, is used for evaluating the methods proposed in the studies <ref type=""bibr"" target=""#b2"">[3]</ref>, <ref type=""bibr"" target=""#b3"">[4]</ref>, <ref type=""bibr"" target=""#b5"">[6]</ref>- <ref type=""bibr"" target=""#b",0
"ositioning (EDITOR), which builds on recent progress on non-autoregressive sequence generation <ref type=""bibr"" target=""#b26"">(Lee et al., 2018;</ref><ref type=""bibr"" target=""#b20"">Ghazvininejad et al., 2019)</ref>. 1  Specifically, the Levenshtein Transformer <ref type=""bibr"" target=""#b22"">(Gu et a "" target=""#b22"">(Gu et al., 2019)</ref>, or 2) the maximum number of decoding steps is reached <ref type=""bibr"" target=""#b26"">(Lee et al., 2018;</ref><ref type=""bibr"" target=""#b20"">Ghazvininejad et al., 2019)</ref>.<ref type=""foot"" target=""#foot_3"">5</ref> </p><p>Incorporating Soft Constraints Altho l., 2018;</ref><ref type=""bibr"" target=""#b46"">Stern et al., 2018)</ref> or multi-pass decoding <ref type=""bibr"" target=""#b26"">(Lee et al., 2018;</ref><ref type=""bibr"" target=""#b20"">Ghazvininejad et al., 2019;</ref><ref type=""bibr"" target=""#b22"">Gu et al., 2019)</ref>. This work adopts multipass deco ing, where the model generates the target sequences by iteratively editing the outputs from previous iterations. Edit operations such as substitution <ref type=""bibr"" target=""#b20"">(Ghazvininejad et al., 2019)</ref> and insertion-deletion <ref type=""bibr"" target=""#b22"">(Gu et al., 2019)</ref> have r",1
".</p><p>Our model is an Edit-Based TransfOrmer with Repositioning (EDITOR), which builds on recent progress on non-autoregressive sequence generation <ref type=""bibr"" target=""#b26"">(Lee et al., 2018;</ref><ref type=""bibr"" target=""#b20"">Ghazvininejad et al., 2019)</ref>. 1  Specifically, the Levensht llel decoding <ref type=""bibr"" target=""#b51"">(Wang et al., 2018;</ref><ref type=""bibr"" target=""#b46"">Stern et al., 2018)</ref> or multi-pass decoding <ref type=""bibr"" target=""#b26"">(Lee et al., 2018;</ref><ref type=""bibr"" target=""#b20"">Ghazvininejad et al., 2019;</ref><ref type=""bibr"" target=""#b22""> two consecutive iterations are the same <ref type=""bibr"" target=""#b22"">(Gu et al., 2019)</ref>, or 2) the maximum number of decoding steps is reached <ref type=""bibr"" target=""#b26"">(Lee et al., 2018;</ref><ref type=""bibr"" target=""#b20"">Ghazvininejad et al., 2019)</ref>.<ref type=""foot"" target=""#foot distillation from autoregressive teacher models as widely used in nonautoregressive generation <ref type=""bibr"" target=""#b21"">(Gu et al., 2018;</ref><ref type=""bibr"" target=""#b26"">Lee et al., 2018;</ref><ref type=""bibr"" target=""#b22"">Gu et al., 2019)</ref>. Specifically, when training the non-autor",1
"d ones -EDITOR with soft constraints achieves translation quality on par or better than both EDITOR and Levenshtein Transformer with hard constraints <ref type=""bibr"" target=""#b47"">(Susanto et al., 2020)</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""2"">Background</head><p>Non-Au further compare it with prior work <ref type=""bibr"" target=""#b16"">(Dinu et al., 2019;</ref><ref type=""bibr"" target=""#b36"">Post and Vilar, 2018;</ref><ref type=""bibr"" target=""#b47"">Susanto et al., 2020)</ref>. Compared to <ref type=""bibr"" target=""#b36"">Post and Vilar (2018)</ref> and <ref type=""bibr er BLEU improvements over its counterpart without constraints (Table <ref type=""table"" target=""#tab_6"">6</ref>). Consistent with previous findings by <ref type=""bibr"" target=""#b47"">Susanto et al. (2020)</ref>, incorporating soft constraints in LevT improves BLEU by +0.3 on Wiktionary and by +0.4 on et al. (2020)</ref>, incorporating soft constraints in LevT improves BLEU by +0.3 on Wiktionary and by +0.4 on IATE. Enforcing hard constraints as in <ref type=""bibr"" target=""#b47"">Susanto et al. (2020)</ref> increases the term usage by +8-10% and improves BLEU by +0.3-0.6 over LevT using soft const tle benefit to enforcing hard constraints instead: they help close the small gap to reach 100% term usage and do not 14 We use our implementations of <ref type=""bibr"" target=""#b47"">Susanto et al. (2020)</ref>'s technique for a more controlled comparison. The LevT baseline in <ref type=""bibr"" target= our implementations of <ref type=""bibr"" target=""#b47"">Susanto et al. (2020)</ref>'s technique for a more controlled comparison. The LevT baseline in <ref type=""bibr"" target=""#b47"">Susanto et al. (2020)</ref> achieves higher BLEU than ours on the small Wiktionary and IATE test sets, while it underpe",1
"e domain-specific knowledge and lexicons which is particularly helpful in low-resource cases <ref type=""bibr"" target=""#b2"">(Arthur et al., 2016;</ref><ref type=""bibr"" target=""#b48"">Tang et al., 2016)</ref>. Despite their success at domain adaptation for MT <ref type=""bibr"" target=""#b23"">(Hokamp and",0
", p).</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.2"">Dual-Path Imitation Learning</head><p>We train EDITOR using imitation learning <ref type=""bibr"" target=""#b15"">(Daumé III et al., 2009;</ref><ref type=""bibr"" target=""#b39"">Ross et al., 2011;</ref><ref type=""bibr"" target=""#b38"">Ros",0
"l., 2016)</ref>. Despite their success at domain adaptation for MT <ref type=""bibr"" target=""#b23"">(Hokamp and Liu, 2017)</ref> and caption generation <ref type=""bibr"" target=""#b1"">(Anderson et al., 2017)</ref>, they suffer from several issues: constrained training requires building dedicated models",0
"le actions. As prior work suggests that cost functions close to the cross-entropy loss are better suited to deep neural models than the squared error <ref type=""bibr"" target=""#b25"">(Leblond et al., 2018;</ref><ref type=""bibr"" target=""#b11"">Cheng et al., 2018)</ref>, we define the cost function as th",0
"nslation <ref type=""bibr"" target=""#b19"">(Foster et al., 2002;</ref><ref type=""bibr"" target=""#b5"">Barrachina et al., 2009)</ref> and domain adaptation <ref type=""bibr"" target=""#b23"">(Hokamp and Liu, 2017)</ref>. Lexical constraints or preferences have previously been incorporated by re-training NMT m r"" target=""#b2"">(Arthur et al., 2016;</ref><ref type=""bibr"" target=""#b48"">Tang et al., 2016)</ref>. Despite their success at domain adaptation for MT <ref type=""bibr"" target=""#b23"">(Hokamp and Liu, 2017)</ref> and caption generation <ref type=""bibr"" target=""#b1"">(Anderson et al., 2017)</ref>, they s (Song et al., 2019;</ref><ref type=""bibr"" target=""#b16"">Dinu et al., 2019)</ref> or with constrained beam search that drastically slows down decoding <ref type=""bibr"" target=""#b23"">(Hokamp and Liu, 2017;</ref><ref type=""bibr"" target=""#b36"">Post and Vilar, 2018)</ref>.</p><p>In this work, we introduc target=""#b16"">Dinu et al., 2019)</ref>, or 2) constrained decoding where beam search is modified to include constraint words or phrases in the output <ref type=""bibr"" target=""#b23"">(Hokamp and Liu, 2017;</ref><ref type=""bibr"" target=""#b36"">Post and Vilar, 2018)</ref>. These mechanisms can incorporat",0
"le by seamlessly allowing users to specify preferences in output lexical choice. Building on recent models for non-autoregressive sequence generation <ref type=""bibr"" target=""#b22"">(Gu et al., 2019)</ref>, EDI-TOR generates new sequences by iteratively editing hypotheses. It relies on a novel reposi ion learning and parallel edits at decoding time. Empirically, EDITOR uses soft lexical constraints more effectively than the Levenshtein Transformer <ref type=""bibr"" target=""#b22"">(Gu et al., 2019)</ref> while speeding up decoding dramatically compared to constrained beam search <ref type=""bibr"" ta target=""#b26"">(Lee et al., 2018;</ref><ref type=""bibr"" target=""#b20"">Ghazvininejad et al., 2019)</ref>. 1  Specifically, the Levenshtein Transformer <ref type=""bibr"" target=""#b22"">(Gu et al., 2019)</ref> showed that iteratively refining output sequences via insertions and deletions yields a fast an nd English-Japanese MT show that EDITOR achieves comparable or better translation quality with faster decoding speed than the Levenshtein Transformer <ref type=""bibr"" target=""#b22"">(Gu et al., 2019)</ref> on the standard MT tasks and exploit soft lexical constraints better: it achieves significantly om previous iterations. Edit operations such as substitution <ref type=""bibr"" target=""#b20"">(Ghazvininejad et al., 2019)</ref> and insertion-deletion <ref type=""bibr"" target=""#b22"">(Gu et al., 2019)</ref> have reduced the quality gap between non-autoregressive and autoregressive models. However, we arget=""#fig_1"">4</ref> shows an example for creating the roll-in sequences: we first create the initial sequence y 0 by applying random word dropping <ref type=""bibr"" target=""#b22"">(Gu et al., 2019)</ref> and random word shuffle <ref type=""bibr"">(Lample et al., 2018)</ref> with probability of 0.5 an h iteration given the model policy in Eqs.</p><p>(1) to (3). We stop refining if 1) the output sequences from two consecutive iterations are the same <ref type=""bibr"" target=""#b22"">(Gu et al., 2019)</ref>, or 2) the maximum number of decoding steps is reached <ref type=""bibr"" target=""#b26"">(Lee et a -TOR and LevT achieve close translation quality to their AR teachers with 2-4 times speedup. BLEU differences are small (∆ &lt; 1.1) as in prior work <ref type=""bibr"" target=""#b22"">(Gu et al., 2019)</ref>. The RIBES trends are more surprising: both NAR models significantly outperform the AR models ( cal choice preferences. Extensive experiments showed that ED-ITOR exploits soft lexical constraints more effectively than the Levenshtein Transformer <ref type=""bibr"" target=""#b22"">(Gu et al., 2019)</ref> while speeding up decoding dramatically compared to constrained beam search <ref type=""bibr"" ta )</ref> or multi-pass decoding <ref type=""bibr"" target=""#b26"">(Lee et al., 2018;</ref><ref type=""bibr"" target=""#b20"">Ghazvininejad et al., 2019;</ref><ref type=""bibr"" target=""#b22"">Gu et al., 2019)</ref>. This work adopts multipass decoding, where the model generates the target sequences by iterativ dely used in nonautoregressive generation <ref type=""bibr"" target=""#b21"">(Gu et al., 2018;</ref><ref type=""bibr"" target=""#b26"">Lee et al., 2018;</ref><ref type=""bibr"" target=""#b22"">Gu et al., 2019)</ref>. Specifically, when training the non-autoregressive models, we replace the reference sequences y oftmax function captures the similarity between the hidden state h i and each input embedding e j or the deletion vector b.</p><p>Insertion Following <ref type=""bibr"" target=""#b22"">Gu et al. (2019)</ref>, the insertion operation consists of two phases: (1) placeholder insertion: given an input seque f u &lt; α E(y 0 , r), otherwise<label>(7)</label></formula><p>where the mixture factor α ∈ [0, 1] and random variable u ∼ Uniform(0, 1).</p><p>While <ref type=""bibr"" target=""#b22"">Gu et al. (2019)</ref> define roll-in using only the model's insertion policy, we call our approach dual-path because r ine translation (Sections 4.3-4.4).</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.1"">Experimental Settings</head><p>Dataset Following <ref type=""bibr"" target=""#b22"">Gu et al. (2019)</ref>, we experiment on three language pairs spanning different language families and data conditions",0
"rences have previously been incorporated by re-training NMT models with constraints as inputs <ref type=""bibr"" target=""#b43"">(Song et al., 2019;</ref><ref type=""bibr"" target=""#b16"">Dinu et al., 2019)</ref> or with constrained beam search that drastically slows down decoding <ref type=""bibr"" target="" llel samples augmented with constraint target phrases in both the source and target sequences <ref type=""bibr"" target=""#b43"">(Song et al., 2019;</ref><ref type=""bibr"" target=""#b16"">Dinu et al., 2019)</ref>, or 2) constrained decoding where beam search is modified to include constraint words or phras s lexical constraints. We then randomly shuffle the constraints and apply BPE to the constraint sequence. Different from the terminology test sets in <ref type=""bibr"" target=""#b16"">Dinu et al. (2019)</ref> which contain only several hundred sentences with mostly nominal constraints, our constructed mlns=""http://www.tei-c.org/ns/1.0""><head n=""4.4"">MT with Terminology Constraints</head><p>We evaluate EDITOR on the terminology test sets released by <ref type=""bibr"" target=""#b16"">Dinu et al. (2019)</ref> to test its ability to incorporate terminology constraints and to further compare it with prio ar, 2018;</ref><ref type=""bibr"" target=""#b47"">Susanto et al., 2020)</ref>. Compared to <ref type=""bibr"" target=""#b36"">Post and Vilar (2018)</ref> and <ref type=""bibr"" target=""#b16"">Dinu et al. (2019)</ref>, EDITOR with soft constraints achieves higher absolute BLEU, and higher BLEU improvements over ed im-</cell></row><row><cell>plementation of EDITOR and Sockeye-based</cell></row></table><note>De test sets with terminology constraints released by<ref type=""bibr"" target=""#b16"">Dinu et al. (2019)</ref>. Machine Translation Results. For each metric, we underline the top scores among all models an n incorrect output by using constraints in the provided order, EDI-TOR's reposition operation helps generate a more fluent and adequate translation.  <ref type=""bibr"" target=""#b16"">(Dinu et al., 2019)</ref> provided with correct terminology entries (exact matches on both source and target sides). ED pe=""bibr"" target=""#b16"">Dinu et al. (2019)</ref> to test its ability to incorporate terminology constraints and to further compare it with prior work <ref type=""bibr"" target=""#b16"">(Dinu et al., 2019;</ref><ref type=""bibr"" target=""#b36"">Post and Vilar, 2018;</ref><ref type=""bibr"" target=""#b47"">Susan",0
"metric, we underline the top scores among all models and boldface the top scores among NAR models based on the paired bootstrap test with p &lt; 0.05<ref type=""bibr"" target=""#b14"">(Clark et al., 2011)</ref>. EDITOR decodes 6-7% faster than LevT on Ro-En and En-De, and 33% faster on En-Ja, while ach",0
"bibr"" target=""#b0"">(Abu Sheikha and Inkpen, 2011;</ref><ref type=""bibr"" target=""#b29"">Mei et al., 2016)</ref>, or via segmentlevel ""side-constraints"" <ref type=""bibr"" target=""#b41"">(Sennrich et al., 2016a;</ref><ref type=""bibr"" target=""#b18"">Ficler and Goldberg, 2017;</ref><ref type=""bibr"" target=""#",0
"ls that decode from left-to-right are the de facto standard for many sequence generation tasks <ref type=""bibr"" target=""#b12"">(Cho et al., 2014;</ref><ref type=""bibr"" target=""#b13"">Chorowski et al., 2015;</ref><ref type=""bibr"" target=""#b50"">Vinyals and Le, 2015)</ref>, non-autoregressive models offe",0
"f type=""bibr"" target=""#b4"">(Bangalore et al., 2007)</ref>, or the Operation Sequence Model <ref type=""bibr"" target=""#b17"">(Durrani et al., 2015;</ref><ref type=""bibr"" target=""#b44"">Stahlberg et al., 2018)</ref>, which views translation as a sequence of translation and reordering operations over bili",0
">. In fact, selfsupervised visual representation learning has been closing the gap with, and in some cases even surpassing its supervised counterpart <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b9"">1 fur of another dog image on the right (negative pair), creating contradicting objectives.</p><p>While recent efforts focus on improved architectures <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b21"">22]</ref> and data augmentation <ref type tectures <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b9"">10,</ref><ref type=""bibr"" target=""#b21"">22]</ref> and data augmentation <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b42"">43]</ref>, relatively little work considers the effects of negative samples, esp ""bibr"" target=""#b23"">24,</ref><ref type=""bibr"" target=""#b41"">42,</ref><ref type=""bibr"" target=""#b33"">34,</ref><ref type=""bibr"" target=""#b21"">22,</ref><ref type=""bibr"" target=""#b8"">9]</ref>.</p><p>In contrastive learning, the embedding space is governed by two opposing forces, the attraction of posit negative pairs are formed by sampling views from different images, regardless of their semantic information <ref type=""bibr"" target=""#b21"">[22,</ref><ref type=""bibr"" target=""#b8"">9,</ref><ref type=""bibr"" target=""#b33"">34]</ref>. Figure <ref type=""figure"" target=""#fig_0"">1</ref> illustrates this pro <ref type=""bibr"" target=""#b21"">[22]</ref> addresses this problem by maintaining a momentum encoder and a limited queue of previous samples. SimCLR v1 <ref type=""bibr"" target=""#b8"">[9]</ref> eschews a momentum encoder in favor of a large batch size, and proposes updates to the projection head and dat br"" target=""#b21"">[22]</ref> 60.6 -PIRL <ref type=""bibr"" target=""#b33"">[34]</ref> 63.6 -PCL <ref type=""bibr"" target=""#b31"">[32]</ref> 65.9 -SimCLR v1 <ref type=""bibr"" target=""#b8"">[9]</ref> 69.3 89.0 MoCo v2 <ref type=""bibr"" target=""#b10"">[11]</ref> 71.1 -SimCLR v2 <ref type=""bibr"" target=""#b9"">[10]",1
"ef type=""bibr"" target=""#b28"">[29]</ref>, CIFAR100 <ref type=""bibr"" target=""#b28"">[29]</ref>, Birdsnap <ref type=""bibr"" target=""#b3"">[4]</ref>, SUN397 <ref type=""bibr"" target=""#b45"">[46]</ref>, Cars <ref type=""bibr"" target=""#b27"">[28]</ref>, Aircraft <ref type=""bibr"" target=""#b32"">[33]</ref>, VOC2007",0
"me the backbone of most modern AI agents, in which good pretrained representations have proven essential to improving performance on downstream tasks <ref type=""bibr"" target=""#b15"">[16,</ref><ref type=""bibr"" target=""#b19"">20,</ref><ref type=""bibr"" target=""#b48"">49,</ref><ref type=""bibr"" target=""#b26",0
"includes CPC <ref type=""bibr"" target=""#b43"">[44,</ref><ref type=""bibr"" target=""#b22"">23]</ref>, Deep InfoMax <ref type=""bibr"" target=""#b23"">[24,</ref><ref type=""bibr"" target=""#b1"">2]</ref>, and contrastive multiview coding <ref type=""bibr"" target=""#b41"">[42]</ref>. Recognizing that contrastive loss",0
"ning as they have different formulations at their core and do not contrast against negative samples, a defining element of contrastive learning. SwAV <ref type=""bibr"" target=""#b6"">[7]</ref> is an online clustering-based method that employs swapping prediction of different views from an image, while action, and top-8 filtering for elimination.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4.1.2"">With Multi-crop</head><p>Caron et al. <ref type=""bibr"" target=""#b6"">[7]</ref> propose a multi-crop data augmentation strategy that increases the number of positive views attracted to each 4 InfoMin <ref type=""bibr"" target=""#b42"">[43]</ref> 73.0 91.  Method AP50 Supervised 81.3 MoCo v2 <ref type=""bibr"" target=""#b10"">[11]</ref> 82.5 SwAV <ref type=""bibr"" target=""#b6"">[7]</ref> 82.6 FNC (ours) 82.8 crop <ref type=""bibr"" target=""#b6"">[7]</ref> with a support size of eight, which is share od AP50 Supervised 81.3 MoCo v2 <ref type=""bibr"" target=""#b10"">[11]</ref> 82.5 SwAV <ref type=""bibr"" target=""#b6"">[7]</ref> 82.6 FNC (ours) 82.8 crop <ref type=""bibr"" target=""#b6"">[7]</ref> with a support size of eight, which is shared with false negative cancellation. We use the attraction strategy accuracy of 74.4%, a 2.7% improvement over SimCLR v2 and a 1.4% boost from the previous best. Among all approaches, our method is second only to SwAV <ref type=""bibr"" target=""#b6"">[7]</ref>, a clustering-based method, and is even better than BYOL <ref type=""bibr"" target=""#b20"">[21]</ref>, a recent s",0
"g noise scales during generation. Denoising diffusion probabilistic modeling (DDPM) <ref type=""bibr"" target=""#b39"">(Sohl-Dickstein et al., 2015;</ref><ref type=""bibr"" target=""#b17"">Ho et al., 2020)</ref> trains a sequence of probabilistic models to reverse each step of the noise corruption, using kn "">Goyal et al., 2017)</ref>, have proven effective at generation of images <ref type=""bibr"">(Song &amp; Ermon, 2019;</ref><ref type=""bibr"">2020;</ref><ref type=""bibr"" target=""#b17"">Ho et al., 2020)</ref>, audio <ref type=""bibr"" target=""#b6"">(Chen et al., 2020;</ref><ref type=""bibr"" target=""#b26"">Kon out re-training.</p><p>Unified picture: The methods of SMLD and DDPM can be unified into our framework as discretizations of different SDEs. Although <ref type=""bibr"" target=""#b17"">Ho et al. (2020)</ref> has reported higher sample quality than <ref type=""bibr"">Song &amp; Ermon (2019;</ref><ref type= al model ś N i""1 p θ px i´1 | x i q. The objective Eq. ( <ref type=""formula"" target=""#formula_3"">3</ref>) described here is equivalent to L simple in <ref type=""bibr"" target=""#b17"">Ho et al. (2020)</ref>, but we re-write it in a slightly different form to expose more similarity to Eq. (1). Like Eq. ith variational dequantization <ref type=""bibr"" target=""#b16"">(Ho et al., 2019)</ref> or discrete data). Main results: (i) For the same DDPM model in <ref type=""bibr"" target=""#b17"">Ho et al. (2020)</ref>, we obtain better bits/dim compared to the upper bound given by ELBO, since our likelihoods are 2020)</ref>, whereas for DDPM it is 3.17 <ref type=""bibr"" target=""#b17"">(Ho et al., 2020)</ref>. With PC samplers and the same model architecture in <ref type=""bibr"" target=""#b17"">Ho et al. (2020)</ref>, the gaps can be reduced, but score-based models trained with the VE SDE still perform slightly qdw,<label>(28)</label></formula><p>where xp0q "" p data pxq. In our experiments, we let βmin "" 0.1 and βmax "" 20, which correspond to the settings in <ref type=""bibr"" target=""#b17"">Ho et al. (2020)</ref>. The perturbation kernel is given by</p><formula xml:id=""formula_36"">p 0t pxptq | xp0qq "" N ´xpt For producing figures in Fig. <ref type=""figure"" target=""#fig_4"">4</ref>, we use a DDPM model trained on 256 ˆ256 CelebA-HQ with the same settings in <ref type=""bibr"" target=""#b17"">Ho et al. (2020)</ref>. We use the RK45 ODE solver <ref type=""bibr"" target=""#b10"">(Dormand &amp; Prince, 1980)</ref> pr re"" target=""#fig_7"">7</ref>, including interpolation and temperature scaling. The model tested here is a DDPM model trained with the same settings in <ref type=""bibr"" target=""#b17"">Ho et al. (2020)</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>D.5 UNIQUELY IDENTIFIABLE ENCODING</he pe=""formula"" target=""#formula_4"">4</ref>) is essentially a different discretization to the same reverse-time SDE. This unifies the sampling method in <ref type=""bibr"" target=""#b17"">Ho et al. (2020)</ref> as a numerical solver to the reverse-time VP SDE in our continuous framework.</p></div> <div xml mula_54"">ppx i | x i´1 q "" N px i ; x i´1 , pσ 2 i ´σ2 i´1 qIq, i "" 1, 2, ¨¨¨, N.</formula><p>Here we assume σ 0 "" 0 to simplify notations. Following <ref type=""bibr"" target=""#b17"">Ho et al. (2020)</ref>, we can compute</p><formula xml:id=""formula_55"">qpx i´1 | x i , x 0 q "" N ˆxi´1 ; σ 2 i´1 σ 2 i via</p><formula xml:id=""formula_57"">µ θ px i , iq "" x i `pσ 2 i ´σ2 i´1 qs θ px i , iq,</formula><p>where s θ px i , iq is to estimate z{σ i . As in <ref type=""bibr"" target=""#b17"">Ho et al. (2020)</ref>, we let τ i ""</p><formula xml:id=""formula_58"">c σ 2 i´1 pσ 2 i ´σ2 i´1 q σ 2 i .</formula></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>G ADDITIONAL DETAILS ON PREDICTOR-CORRECTOR SAMPLERS</head><p>Training We use the same architecture in <ref type=""bibr"" target=""#b17"">Ho et al. (2020)</ref> for our score-based models. For the VE SDE, we train a model with the original SMLD objective in noise scales, we need to interpolate between 1000 noise scales at test time. The specific architecture of the noise-conditional score-based model in <ref type=""bibr"" target=""#b17"">Ho et al. (2020)</ref> uses sinusoidal positional embeddings for conditioning on integer time steps. This allows us to er 0.5M iterations. The FIDs are computed on 50k samples with TF-GAN. For sampling, we use the PC sampler discretized at 1000 noise scales. We follow <ref type=""bibr"" target=""#b17"">Ho et al. (2020)</ref> for optimization, including the learning rate, gradient clipping, and learning rate warm-up sche h the original SMLD objective Eq. ( <ref type=""formula"" target=""#formula_0"">1</ref>) and use a batch size of 128. Our architecture is mostly based on <ref type=""bibr"" target=""#b17"">Ho et al. (2020)</ref>. We additionally search over the following components to explore the potential of score-based mo e in order to match the convention used in <ref type=""bibr"" target=""#b20"">Karras et al. (2018)</ref>; <ref type=""bibr"">Song &amp; Ermon (2019)</ref>; <ref type=""bibr"" target=""#b17"">Ho et al. (2020)</ref>, the FID value here is the lowest over the course of training, rather than the average over chec ign choices.</p><p>To further improve the NCSN++ model upon conditioning on continuous time variables, we change positional embeddings, the layers in <ref type=""bibr"" target=""#b17"">Ho et al. (2020)</ref> for conditioning on discrete time steps, to random Fourier feature embeddings, as advocated in < JORD <ref type=""bibr"" target=""#b14"">(Grathwohl et al., 2018)</ref> 3.40 -Flow++ <ref type=""bibr"" target=""#b16"">(Ho et al., 2019)</ref> 3.29 -DDPM (L) <ref type=""bibr"" target=""#b17"">(Ho et al., 2020)</ref> ď 3.70 13.51 DDPM (Lsimple) <ref type=""bibr"" target=""#b17"">(Ho et al., 2020)</ref> ď 3.75 3.17 f type=""bibr"" target=""#b16"">(Ho et al., 2019)</ref> 3.29 -DDPM (L) <ref type=""bibr"" target=""#b17"">(Ho et al., 2020)</ref> ď 3.70 13.51 DDPM (Lsimple) <ref type=""bibr"" target=""#b17"">(Ho et al., 2020)</ref> ď 3.75 3.17   <ref type=""bibr"">(Song &amp; Ermon, 2019)</ref> 25.32 8.87 ˘.12 NCSNv2 <ref type= 75 3.17   <ref type=""bibr"">(Song &amp; Ermon, 2019)</ref> 25.32 8.87 ˘.12 NCSNv2 <ref type=""bibr"">(Song &amp; Ermon, 2020)</ref> 10.87 8.40 ˘.07 DDPM <ref type=""bibr"" target=""#b17"">(Ho et al., 2020)</ref> 3.17 9.46 ˘.11 Exact likelihood computation Leveraging the connection to neural ODEs, we can co previous papers. The best FID values of SMLD models on CIFAR-10 is 10.23 <ref type=""bibr"">(Song &amp; Ermon, 2020)</ref>, whereas for DDPM it is 3.17 <ref type=""bibr"" target=""#b17"">(Ho et al., 2020)</ref>. With PC samplers and the same model architecture in <ref type=""bibr"" target=""#b17"">Ho et al. ( tion strategy in Eq. ( <ref type=""formula"" target=""#formula_52"">41</ref>)) reverse diffusion samplers.</p><p>Note that the ancestral sampling of DDPM <ref type=""bibr"" target=""#b17"">(Ho et al., 2020)</ref> (Eq. ( <ref type=""formula"" target=""#formula_4"">4</ref>)) matches its reverse diffusion counterp",1
"versing this process, we can smoothly mold random noise into data for sample generation. Crucially, this reverse process satisfies a reverse-time SDE <ref type=""bibr"" target=""#b1"">(Anderson, 1982)</ref>, which can be derived from the forward SDE given the score of the marginal probability densities /formula><p>where f p¨, tq : R d Ñ R d and Gp¨, tq : R d Ñ R dˆd . We follow the Itô interpretation of SDEs throughout this paper.</p><p>According to <ref type=""bibr"" target=""#b1"">(Anderson, 1982)</ref>, the reverse-time SDE is given by (cf ., Eq. ( <ref type=""formula"" target=""#formula_7"">6</ref>))< RSING THE SDE</head><p>By starting from samples of xpT q "" p T and reversing the process, we can obtain samples xp0q "" p 0 . A remarkable result from <ref type=""bibr"" target=""#b1"">Anderson (1982)</ref> states that the reverse of a diffusion process is also a diffusion process, running backwards in t , tqdw, and suppose the initial state distribution is p 0 pxp0q | yq. The density at time t is p t pxptq | yq when conditioned on y. Therefore, using <ref type=""bibr"" target=""#b1"">Anderson (1982)</ref>, the reverse-time SDE is given by dx "" tf px, tq ´∇ ¨rGpx, tqGpx, tq T s ´Gpx, tqGpx, tq To test t",0
"inite Impulse Response (FIR) <ref type=""bibr"" target=""#b48"">(Zhang, 2019)</ref>. We follow the same implementation and hyper-parameters in StyleGAN-2 <ref type=""bibr"" target=""#b23"">(Karras et al., 2020b)</ref>. • Rescaling all skip connections by 1 { ? 2. This has been demonstrated effective in seve",0
"Residual Flow <ref type=""bibr"">(Chen et al., 2019)</ref> 3.28 46.37 FFJORD <ref type=""bibr"" target=""#b14"">(Grathwohl et al., 2018)</ref> 3.40 -Flow++ <ref type=""bibr"" target=""#b16"">(Ho et al., 2019)</ref> 3.29 -DDPM (L) <ref type=""bibr"" target=""#b17"">(Ho et al., 2020)</ref> ď 3.70 13.51 DDPM (Lsimpl hoods on uniformly dequantized data, and only compare to models evaluated in the same way (excluding models evaluated with variational dequantization <ref type=""bibr"" target=""#b16"">(Ho et al., 2019)</ref> or discrete data). Main results: (i) For the same DDPM model in <ref type=""bibr"" target=""#b17"">",0
"5 -MintNet <ref type=""bibr"" target=""#b43"">(Song et al., 2019b)</ref> 3.32 -Residual Flow <ref type=""bibr"">(Chen et al., 2019)</ref> 3.28 46.37 FFJORD <ref type=""bibr"" target=""#b14"">(Grathwohl et al., 2018)</ref> 3.40 -Flow++ <ref type=""bibr"" target=""#b16"">(Ho et al., 2019)</ref> 3.29 -DDPM (L) <ref he probability flow ODE in Eq. ( <ref type=""formula"" target=""#formula_39"">33</ref>). In many cases computing ∇ ¨f θ px, tq is expensive, so we follow <ref type=""bibr"" target=""#b14"">Grathwohl et al. (2018)</ref> to estimate it with the Skilling-Hutchinson trace estimator <ref type=""bibr"" target=""#b38 nce, 1980)</ref> provided by scipy.integrate.solve_ivp in all cases. The bits/dim values in Tab. 2 are computed with atol=1e-5 and rtol=1e-5, same as <ref type=""bibr"" target=""#b14"">Grathwohl et al. (2018)</ref>. To give the results for DDPM (probability flow) and DDPM cont. (probability flow) in Tab",0
"he similarity of vertices. Due to the inefficiency of computing paths and cycles with larger networks, IUAD adopts Weisfeiler-Lehman sub-graph kernel <ref type=""bibr"" target=""#b38"">[39]</ref> to evaluate the similarity between two vertices. WL-kernel captures topological information to quantify the K h (v a i , v a j ) K h (v a i , v a i ) ? K h (v a j , v a j ) .<label>(4)</label></formula><p>Due to page limitation, more details please refer to <ref type=""bibr"" target=""#b38"">[39]</ref>, <ref type=""bibr"" target=""#b39"">[40]</ref>.</p><p>For the WL sub-graph kernel, the more similar in topologic",1
"ns in database, information retrieval, and data mining. To date, existing solutions can be roughly classified into two categories: supervised methods <ref type=""bibr"" target=""#b15"">[16]</ref>- <ref type=""bibr"" target=""#b19"">[20]</ref> and unsupervised methods <ref type=""bibr"" target=""#b3"">[4]</ref>, ""http://www.tei-c.org/ns/1.0""><head>A. Supervised Methods</head><p>Some existing works train classifiers to address the author disambiguation problem <ref type=""bibr"" target=""#b15"">[16]</ref>- <ref type=""bibr"" target=""#b19"">[20]</ref>, <ref type=""bibr"" target=""#b31"">[32]</ref>. Han et al. adopt coau bibr"" target=""#b31"">[32]</ref>. Han et al. adopt coauthor names, paper titles, and journal titles, etc., to train classifiers to disambiguate authors <ref type=""bibr"" target=""#b15"">[16]</ref>. Treeratpituk et al. extract a set of features, including similarities of authors, affiliations, coauthors,",0
"that modern graph neural networks can still fail at very simple tasks, like detecting small cycles <ref type=""bibr"" target=""#b16"">(Loukas, 2019;</ref><ref type=""bibr"" target=""#b3"">Chen et al., 2020)</ref>. This hints at the fact that current networks fail to catch information about the local structu tworks can have trouble solving even basic structure related tasks, such as detecting small cycles <ref type=""bibr"" target=""#b16"">(Loukas, 2019;</ref><ref type=""bibr"" target=""#b3"">Chen et al., 2020)</ref>. This seems like a serious problem, as biochemistry is mainly based on the analysis of local gr target=""#b18"">(Maron et al., 2019;</ref><ref type=""bibr"">2018)</ref> or relational pooling <ref type=""bibr"" target=""#b20"">(Murphy et al., 2019;</ref><ref type=""bibr"" target=""#b3"">Chen et al., 2020)</ref>. Despite being provably more powerful, the amount of calculation required by these methods is s itation, in particular when dealing with molecular graphs such as found in organic chemistry. This problem has been investigated by many recent works <ref type=""bibr"" target=""#b3"">(Chen et al., 2020;</ref><ref type=""bibr"" target=""#b21"">Nikolentzos et al., 2020;</ref><ref type=""bibr"" target=""#b0"">Abu",1
"hile increasing the predictive accuracy <ref type=""bibr"" target=""#b8"">(Gilmer et al., 2017;</ref><ref type=""bibr"" target=""#b22"">Wu et al., 2018;</ref><ref type=""bibr"" target=""#b6"">Fey et al., 2020)</ref>. Graph neural networks have been proven to be particularly effective in this task <ref type=""bib ""#b21"">Nikolentzos et al., 2020;</ref><ref type=""bibr"" target=""#b0"">Abu-El-Haija et al., 2019;</ref><ref type=""bibr"" target=""#b16"">Loukas, 2019;</ref><ref type=""bibr"" target=""#b6"">Fey et al., 2020)</ref>. In the context of molecular property prediction, we highlight the work of <ref type=""bibr"" targ put graph, it was proposed to perform coupled convolution on both the input graph and a coarser version of it from which all cycles have been removed <ref type=""bibr"" target=""#b6"">(Fey et al., 2020)</ref>. Despite interesting results, this approach gives more information to the network and considera "">Loukas, 2019;</ref><ref type=""bibr"" target=""#b6"">Fey et al., 2020)</ref>. In the context of molecular property prediction, we highlight the work of <ref type=""bibr"" target=""#b6"">Fey et al. (2020)</ref> who proposed to perform message passing between the original graph and a coarser graph containin",1
"s in the last few years, recent studies have shown that modern graph neural networks can still fail at very simple tasks, like detecting small cycles <ref type=""bibr"" target=""#b16"">(Loukas, 2019;</ref><ref type=""bibr"" target=""#b3"">Chen et al., 2020)</ref>. This hints at the fact that current network with enough layers, it was shown that practical networks can have trouble solving even basic structure related tasks, such as detecting small cycles <ref type=""bibr"" target=""#b16"">(Loukas, 2019;</ref><ref type=""bibr"" target=""#b3"">Chen et al., 2020)</ref>. This seems like a serious problem, as bioch SIZE FOR SMALL CYCLE DETECTION</head><p>Without virtual node, convolution based networks are at most as discriminative as the Weisfeiler-Lehman test <ref type=""bibr"" target=""#b16"">(Loukas, 2019)</ref>. More practically, this means that the information contained in the Standard convolution (layer l= >(Chen et al., 2020;</ref><ref type=""bibr"" target=""#b21"">Nikolentzos et al., 2020;</ref><ref type=""bibr"" target=""#b0"">Abu-El-Haija et al., 2019;</ref><ref type=""bibr"" target=""#b16"">Loukas, 2019;</ref><ref type=""bibr"" target=""#b6"">Fey et al., 2020)</ref>. In the context of molecular property predicti",1
"her order Weisfeiler Leman kernels <ref type=""bibr"" target=""#b19"">(Morris et al., 2019)</ref>, or to increase the receptive field of each convolution <ref type=""bibr"" target=""#b7"">(Flam-Shepherd et al., 2020;</ref><ref type=""bibr"" target=""#b21"">Nikolentzos et al., 2020;</ref><ref type=""bibr"" target=",0
"ge passing between the original graph and a coarser graph containing no cycles. The authors rely on fixed rules similar to the junction tree approach <ref type=""bibr"" target=""#b11"">(Jin et al., 2018)</ref> to annotate graph cycles and represent them into specific nodes.</p></div> <div xmlns=""http://",0
"eld of each convolution <ref type=""bibr"" target=""#b7"">(Flam-Shepherd et al., 2020;</ref><ref type=""bibr"" target=""#b21"">Nikolentzos et al., 2020;</ref><ref type=""bibr"" target=""#b0"">Abu-El-Haija et al., 2019)</ref>. However, these approaches make considerable changes to the architecture of the network nvestigated by many recent works <ref type=""bibr"" target=""#b3"">(Chen et al., 2020;</ref><ref type=""bibr"" target=""#b21"">Nikolentzos et al., 2020;</ref><ref type=""bibr"" target=""#b0"">Abu-El-Haija et al., 2019;</ref><ref type=""bibr"" target=""#b16"">Loukas, 2019;</ref><ref type=""bibr"" target=""#b6"">Fey et a",0
"learned parameter, σ is a nonlinearity such as ReLU and E(e ij ) is an embedding of the label of the edge (i, j).</p><p>In addition to convolutions, <ref type=""bibr"" target=""#b15"">Li et al. (2017)</ref> proposed the introduction of a virtual node, initially in order to allow the network to grasp lo",0
"n be seen as a computationally simple first-order approximation of spectral graph convolution <ref type=""bibr"" target=""#b1"">(Bruna et al., 2014;</ref><ref type=""bibr"" target=""#b4"">Defferrard et al., 2016)</ref>  <ref type=""table"">1</ref>: Main characteristics of the datasets used in this study. The",0
"present a great potential as the required time for a prediction could be reduced by many orders of magnitude while increasing the predictive accuracy <ref type=""bibr"" target=""#b8"">(Gilmer et al., 2017;</ref><ref type=""bibr"" target=""#b22"">Wu et al., 2018;</ref><ref type=""bibr"" target=""#b6"">Fey et al. of handcrafting them or using other representation is an idea that emerged a few years ago <ref type=""bibr"" target=""#b5"">(Duvenaud et al., 2015;</ref><ref type=""bibr"" target=""#b8"">Gilmer et al., 2017;</ref><ref type=""bibr"" target=""#b13"">Kong et al., 2020)</ref>. Many additions and modifications of t",0
"perform more traditional methods. Another approach is to extend the current framework, to make it equivalent to higher order Weisfeiler Leman kernels <ref type=""bibr"" target=""#b19"">(Morris et al., 2019)</ref>, or to increase the receptive field of each convolution <ref type=""bibr"" target=""#b7"">(Flam",0
"any of those other improvements. For example, regularisation or global pooling functions which may be more suited to small graphs have been proposed <ref type=""bibr"" target=""#b2"">(Cai et al., 2020;</ref><ref type=""bibr"" target=""#b13"">Kong et al., 2020), and</ref><ref type=""bibr"">Hu* et al. (2020)</",0
">Learning features directly on the molecular graph instead of handcrafting them or using other representation is an idea that emerged a few years ago <ref type=""bibr"" target=""#b5"">(Duvenaud et al., 2015;</ref><ref type=""bibr"" target=""#b8"">Gilmer et al., 2017;</ref><ref type=""bibr"" target=""#b13"">Kong",0
"wed by a point-wise nonlinearity. The GCN convolution can be seen as a computationally simple first-order approximation of spectral graph convolution <ref type=""bibr"" target=""#b1"">(Bruna et al., 2014;</ref><ref type=""bibr"" target=""#b4"">Defferrard et al., 2016)</ref>  <ref type=""table"">1</ref>: Main",0
"ey have been utilized to perform multi-purpose text generation with a single unified model <ref type=""bibr"" target=""#b36"">(Radford et al., 2019;</ref><ref type=""bibr"" target=""#b1"">Brown et al., 2020)</ref>. In the CTRLsum framework, prompts are a kind of control token sequence, and we always use suc =""bibr"" target=""#b28"">(McCann et al., 2018;</ref><ref type=""bibr"" target=""#b36"">Radford et al., 2019;</ref><ref type=""bibr"">Keskar et al., 2019;</ref><ref type=""bibr"" target=""#b1"">Brown et al., 2020)</ref>.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""4"">EXPERIMENTS</head><p>Our exper",1
"ref>, thus need to collect annotations for training and cannot generalize to unseen control aspects easily at test time.</p><p>We use pretrained BART <ref type=""bibr"" target=""#b24"">(Lewis et al., 2019)</ref> as the underlying architecture and perform experiments on three datasets in three distinct d 8)</ref>, and BIGPATENT patent articles <ref type=""bibr"" target=""#b43"">(Sharma et al., 2019)</ref>. For all datasets the source documents are trun-   <ref type=""bibr"" target=""#b24"">(Lewis et al., 2019)</ref>, which achieves state-of-the-art performance on several summarization benchmarks. The automa",1
"quences used as the target prefix to constrain decoding. They have been utilized to perform multi-purpose text generation with a single unified model <ref type=""bibr"" target=""#b36"">(Radford et al., 2019;</ref><ref type=""bibr"" target=""#b1"">Brown et al., 2020)</ref>. In the CTRLsum framework, prompts 16)</ref> and also to demonstrate the multi-task ability present in large pretrained models <ref type=""bibr"" target=""#b28"">(McCann et al., 2018;</ref><ref type=""bibr"" target=""#b36"">Radford et al., 2019;</ref><ref type=""bibr"">Keskar et al., 2019;</ref><ref type=""bibr"" target=""#b1"">Brown et al., 2020) or answers during training. In addition to comparing with the vanilla BART model, we also include the zero-shot performance from GPT2 language models <ref type=""bibr"" target=""#b36"">(Radford et al., 2019)</ref> (without fine-tuning) as a reference point. We omit the largest GPT2 model with 1.5B param",1
"et al., 1999;</ref><ref type=""bibr"" target=""#b23"">Leuski et al., 2003)</ref>. More broadly, controllable text generation has been studied for styles <ref type=""bibr"" target=""#b16"">(Hu et al., 2017;</ref><ref type=""bibr"" target=""#b7"">Fu et al., 2018;</ref><ref type=""bibr"" target=""#b13"">He et al., 20",0
"lf et al., 2019)</ref>. Complete setup and training details can be found in Appendix A.1.</p><p>For evaluation, we measure commonly used ROUGE scores <ref type=""bibr"" target=""#b26"">(Lin, 2004)</ref> and the recently proposed BERTScore <ref type=""bibr"">(Zhang et al., 2020)</ref> when ground-truth is",0
"s a way for users to continuously control the information that is included in the summary <ref type=""bibr"" target=""#b0"">(Bornstein et al., 1999;</ref><ref type=""bibr"" target=""#b23"">Leuski et al., 2003)</ref>. More broadly, controllable text generation has been studied for styles <ref type=""bibr"" tar",0
". This triggers a concise summary focused on patent purpose.</p><p>Question-guided summarization. Human summarization can be constrained by questions <ref type=""bibr"" target=""#b22"">(Kryściński et al., 2019)</ref> that require answers to be found in the summary. This points to an important connection",0
"et=""#b7"">Fu et al., 2018;</ref><ref type=""bibr"" target=""#b13"">He et al., 2020b)</ref>, topics <ref type=""bibr"" target=""#b44"">(Tang et al., 2019;</ref><ref type=""bibr"" target=""#b17"">Huang et al., 2019)</ref>, and templates <ref type=""bibr"" target=""#b12"">(Guu et al., 2018;</ref><ref type=""bibr"" target",0
"t <ref type=""bibr"" target=""#b33"">(Ott et al., 2019)</ref> and the automatic keyword extraction model is based on the HuggingFace Transformers library <ref type=""bibr"" target=""#b50"">(Wolf et al., 2019)</ref>. Complete setup and training details can be found in Appendix A.1.</p><p>For evaluation, we m",0
"eterogeneous graphs, most of them are not tailored for modeling bipartite graphs. As a result, they are suboptimal to learn bipartite graph embedding <ref type=""bibr"" target=""#b7"">[7,</ref><ref type=""bibr"" target=""#b8"">8]</ref>. To remedy such a problem, several studies have been specifically propos proposed for modeling bipartite graphs. They can be roughly divided into two branches: random walk-based and reconstruction-based methods. The former <ref type=""bibr"" target=""#b7"">[7,</ref><ref type=""bibr"" target=""#b8"">8,</ref><ref type=""bibr"" target=""#b44"">44]</ref> relies on designing the heuristi t they mainly focus on learning local graph structures with the assumption that nodes within the sliding window or neighborhoods are closely relevant <ref type=""bibr"" target=""#b7"">[7,</ref><ref type=""bibr"" target=""#b37"">37,</ref><ref type=""bibr"" target=""#b41"">41]</ref>. We argue that they lack the c artite graph are hard to be preserved by them. IGE <ref type=""bibr"" target=""#b44"">[44]</ref>, PinSage <ref type=""bibr"" target=""#b40"">[40]</ref>, BiNE <ref type=""bibr"" target=""#b7"">[7]</ref> and FOBE <ref type=""bibr"" target=""#b32"">[32]</ref> are specially designed for bipartite graphs. However, as me oyed to large-scale bipartite graphs.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""5.1.1"">Data Preprocessing.</head><p>As used in BiNE <ref type=""bibr"" target=""#b7"">[7]</ref>, we select 60% edges for training and remaining edges for test in both of DBLP and ML-10M. We use the same div x objective in DGI <ref type=""bibr"" target=""#b36"">[36]</ref>. • Bipartite graph embedding: PinSage <ref type=""bibr"" target=""#b40"">[40]</ref> and BiNE <ref type=""bibr"" target=""#b7"">[7]</ref>.</p><p>PinSage integrates random walk into GNN architectures for high-scalable performances. BiNE jointly opti",1
"s the infomax objective used in DGI for modeling split graphs. So DMGI still puts more emphasis on learning the correlation of homogeneous nodes. GMI <ref type=""bibr"" target=""#b26"">[26]</ref> proposes a new approach  The yellow dotted lines (Eq.( <ref type=""formula"" target=""#formula_0"">1</ref>) and",1
"ing challenge. Recently, a significant amount of progresses have been made toward the graph embedding paradigm <ref type=""bibr"" target=""#b2"">[2,</ref><ref type=""bibr"" target=""#b5"">5,</ref><ref type=""bibr"" target=""#b14"">14]</ref>. Although they work pretty well in the settings of homogeneous and hete 9"">)</formula><p>The local attentive representation also combines different local environments together via the same composition function used in Eq. <ref type=""bibr"" target=""#b5"">(5)</ref>. It not only highlights the central role of (𝑢, 𝑣), but also adaptively assigns different importance factors t",0
"n node representations via predicting context nodes within a sliding window <ref type=""bibr"" target=""#b45"">[45]</ref>. The reconstruction-based works <ref type=""bibr"" target=""#b15"">[15,</ref><ref type=""bibr"" target=""#b32"">32,</ref><ref type=""bibr"" target=""#b34"">34,</ref><ref type=""bibr"" target=""#b37 e latent space.</p><p>Matrix completion <ref type=""bibr"" target=""#b34"">[34,</ref><ref type=""bibr"" target=""#b42"">42]</ref> and collaborative filtering <ref type=""bibr"" target=""#b15"">[15,</ref><ref type=""bibr"" target=""#b37"">37]</ref> are also connected with modeling bipartite graphs closely. They prop a bilinear decoder. IGMC proposes a novel GNN based on local subgraphs for the task of inductive matrix completion. • Collaborative filtering: NeuMF <ref type=""bibr"" target=""#b15"">[15]</ref> and NGCF <ref type=""bibr"" target=""#b37"">[37]</ref>. NeuMF uses MLP to learn the nonlinear interactions betwe",0
"5"">[35]</ref>. However, estimating MI is generally intractable in highdimensional continuous settings <ref type=""bibr"" target=""#b25"">[25]</ref>. MINE <ref type=""bibr"" target=""#b1"">[1]</ref> derives a lower bound of MI and works by training a discriminator to distinguish samples coming from the joint",0
"erative adversarial network (GAN) <ref type=""bibr"" target=""#b10"">[10]</ref>, and the ""GAN"" distance and Jensen-Shannon divergence are closely related <ref type=""bibr"" target=""#b24"">[24]</ref>.</p><p>From Eq.( <ref type=""formula"" target=""#formula_4"">5</ref>), we can observe that the information of tw",0
"type=""bibr"" target=""#b42"">42]</ref> train graph neural networks (GNNs) <ref type=""bibr"" target=""#b9"">[9,</ref><ref type=""bibr"" target=""#b20"">20,</ref><ref type=""bibr"" target=""#b22"">22,</ref><ref type=""bibr"" target=""#b38"">38]</ref> to learn node representations via aggregating features of neighborhoo",0
"rmula><p>The negative sampling used in Eq.( <ref type=""formula"" target=""#formula_17"">14</ref>) is similar to <ref type=""bibr"" target=""#b12"">[12,</ref><ref type=""bibr"" target=""#b21"">21]</ref>: 𝐸 ′ (𝑢,𝑣) is composed of real interactions with either the head or tail replaced by a random node from the s",0
"by training a discriminator to distinguish samples coming from the joint distribution of two random variables or the product of their marginals. DIM <ref type=""bibr"" target=""#b16"">[16]</ref> introduces the structural information into input patches and adopts different infomax objectives.</p><p>DGI",0
"amount of progresses have been made toward the graph embedding paradigm <ref type=""bibr"" target=""#b2"">[2,</ref><ref type=""bibr"" target=""#b5"">5,</ref><ref type=""bibr"" target=""#b14"">14]</ref>. Although they work pretty well in the settings of homogeneous and heterogeneous graphs, most of them are not",0
"ning task directly reconstructs the cold-start user/item embeddings by mimicking the meta-learning setting via episode based training, as proposed in <ref type=""bibr"" target=""#b33"">[34]</ref>. Specifically, we pick the users/items with sufficient interactions as the target users/items and learn thei f the methods is meta-learning <ref type=""bibr"" target=""#b8"">[9,</ref><ref type=""bibr"" target=""#b22"">23,</ref><ref type=""bibr"" target=""#b25"">26,</ref><ref type=""bibr"" target=""#b33"">34]</ref>, which consists of metric-based recommendation <ref type=""bibr"" target=""#b28"">[29]</ref> and model-based reco",1
". To achieve this goal, we cast the task of neighbor sampler as a hierarchical Markov Decision Process (MDP) <ref type=""bibr"" target=""#b27"">[28,</ref><ref type=""bibr"" target=""#b46"">47]</ref>. Specifically, we formulate the neighbor sampler as 𝐿 − 1 MDP subtasks where the 𝑙-th subtask indicates sampl rain the meta learner, the meta aggregator and the neighbor sampler together (Line 4). Same as the settings of <ref type=""bibr"" target=""#b7"">[8,</ref><ref type=""bibr"" target=""#b46"">47]</ref>, to have a stable update during joint training, each parameter Θ ∈ {Θ 𝑓 , Θ 𝑔 , Θ 𝑠 } is updated by a linear tal Setup</head><p>Dataset. We evaluate on three public datasets including MovieLens-1M (Ml-1M) 3  <ref type=""bibr"" target=""#b11"">[12]</ref>, MOOCs 4 <ref type=""bibr"" target=""#b46"">[47]</ref> and Last.fm 5 . Table <ref type=""table"">1</ref> illustrates the statistics of these datasets. The code is av",0
", we use cosine similarity to measure the difference between the predicted target embedding h 𝐿 𝑢 and the ground-truth embedding h 𝑢 , as proposed by <ref type=""bibr"" target=""#b15"">[16]</ref>, due to its popularity as an indicator for the semantic similarity between embeddings:</p><formula xml:id=""f d 𝑇 𝑒𝑠𝑡 ′ 𝑇 . The embeddings in both the proposed models and the GNN models are initialized by the NCF embedding results. We use Spearman correlation <ref type=""bibr"" target=""#b15"">[16]</ref> to measure the agreement between the ground truth embedding and the predicted embedding.</p><p>Overall Perfo",0
"due to the incompleteness and ambiguation of the entities.</p><p>On another line, inspired by the recent development of graph neural networks (GNNs) <ref type=""bibr"" target=""#b1"">[2,</ref><ref type=""bibr"" target=""#b10"">11,</ref><ref type=""bibr"" target=""#b18"">19]</ref>, NGCF <ref type=""bibr"" target=",0
"emantic properties. Examples include 1) graph-level pretext task, which either distinguishes subgraphs of a certain node from those of other vertices <ref type=""bibr"" target=""#b24"">[25]</ref> or maximize the mutual information between the local node representation and the global graph representation",0
"on. To further discuss the self-attention mechanism, let q i , k i , v i stand for the i-th row in Q, K, V respectively. Following the formulation in <ref type=""bibr"" target=""#b28"">(Tsai et al. 2019)</ref>, the i-th query's attention is defined as a kernel smoother in a probability form:</p><formula",1
"r models be improved to be computation, memory, and architecture efficient, as well as maintain higher prediction capacity?</p><p>Vanilla Transformer <ref type=""bibr"" target=""#b29"">(Vaswani et al. 2017</ref>) has three significant limitations when solving LSTF:</p><p>1. The quadratic computation of ons for details.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>Efficient Self-attention Mechanism</head><p>The canonical self-attention in <ref type=""bibr"" target=""#b29"">(Vaswani et al. 2017</ref>) is defined on receiving the tuple input (query, key, value) and performs the scaled dot-pro representation of encoder.</p><p>Decoder: Generating long sequential outputs through one forward procedure</p><p>We use a standard decoder structure <ref type=""bibr"" target=""#b29"">(Vaswani et al. 2017)</ref> in Fig.</p><p>(2), and it is composed of a stack of two identical multihead attention layer",1
") incorporates both the row outputs and column inputs, in which the sparsity arises from the separated spatial correlation. The LogSparse Transformer <ref type=""bibr"" target=""#b15"">(Li et al. 2019</ref>) notices the cyclical pattern in self-attention and forces each cell to attend to its previous on city Consuming Load)<ref type=""foot"" target=""#foot_2"">3</ref> : It collects the electricity consumption (Kwh) of 321 clients. Due to the missing data <ref type=""bibr"" target=""#b15"">(Li et al. 2019)</ref>, we convert the dataset into hourly consumption of 2 years and set 'MT 320' as the target value. fficient variant Reformer <ref type=""bibr"" target=""#b13"">(Kitaev, Kaiser, and Levskaya 2019)</ref> and the most related work LogSparse self-attention <ref type=""bibr"" target=""#b15"">(Li et al. 2019)</ref> in the experiments.</p><p>Hyper-parameter tuning: We conduct grid search over the hyper-paramete",0
"</ref><ref type=""bibr"">Yu et al. 2017;</ref><ref type=""bibr"" target=""#b17"">Liu et al. 2019;</ref><ref type=""bibr"" target=""#b20"">Qin et al. 2017;</ref><ref type=""bibr"" target=""#b31"">Wen et al. 2017)</ref>. The increasingly long sequences strain the models' prediction capacity to the point where some",0
"s are designed under limited problem setting, like predicting 48 points or less <ref type=""bibr"" target=""#b12"">(Hochreiter and Schmidhuber 1997;</ref><ref type=""bibr"" target=""#b16"">Li et al. 2018;</ref><ref type=""bibr"">Yu et al. 2017;</ref><ref type=""bibr"" target=""#b17"">Liu et al. 2019;</ref><ref ty develop an encoder-decoder prediction paradigm by using RNN and their variants <ref type=""bibr"" target=""#b12"">(Hochreiter and Schmidhuber 1997;</ref><ref type=""bibr"" target=""#b16"">Li et al. 2018;</ref><ref type=""bibr"">Yu et al. 2017)</ref>. Our proposed Informer holds the encoder-decoder architectu",0
"rforming a univariate forecasting or a multivariate one.</p><p>Generative Inference Start token is an efficient technique in NLP's ""dynamic decoding"" <ref type=""bibr"" target=""#b9"">(Devlin et al. 2018)</ref>, and we extend it into a generative way. Instead of choosing a specific flag as the token, we",0
"Fig. <ref type=""figure"" target=""#fig_1"">(1c</ref>). There are some prior works on improving the efficiency of self-attention. The Sparse Transformer <ref type=""bibr"" target=""#b5"">(Child et al. 2019</ref><ref type=""bibr"">), LogSparse Transformer (Li et al. 2019)</ref>, and Longformer <ref type=""bibr , and they have designed some ""selective"" counting strategies on all p(k j |q i ) without significantly affecting performance. The Sparse Transformer <ref type=""bibr"" target=""#b5"">(Child et al. 2019</ref>) incorporates both the row outputs and column inputs, in which the sparsity arises from the sep",0
"long sequence time-series forecasting (LSTF). However, existing methods are designed under limited problem setting, like predicting 48 points or less <ref type=""bibr"" target=""#b12"">(Hochreiter and Schmidhuber 1997;</ref><ref type=""bibr"" target=""#b16"">Li et al. 2018;</ref><ref type=""bibr"">Yu et al. 2 Salinas, and Flunkert 2016)</ref>, and deep learning techniques mainly develop an encoder-decoder prediction paradigm by using RNN and their variants <ref type=""bibr"" target=""#b12"">(Hochreiter and Schmidhuber 1997;</ref><ref type=""bibr"" target=""#b16"">Li et al. 2018;</ref><ref type=""bibr"">Yu et al. 2",0
"ations in attention block, and Conv1d(•) performs an 1-D convolutional filters (kernel width=3) on time dimension with the ELU(•) activation function <ref type=""bibr"" target=""#b7"">(Clevert, Unterthiner, and Hochreiter 2016)</ref>.</p><p>We add a max-pooling layer with stride 2 and down-sample X t in",0
"eam tasks such as representation learning, generating new samples, etc. Two of the most popular methods are Generative Adversarial Networks (GANs) by <ref type=""bibr"" target=""#b2"">Goodfellow et al. (2014)</ref> and Variational 2.z combines with x for the target gene and is fed into an MLP model for",1
"14)</ref> and Variational 2.z combines with x for the target gene and is fed into an MLP model for predicting gene expression. Autoencoders (VAEs) by <ref type=""bibr"" target=""#b8"">Kingma and Welling (2013)</ref>. While GANs were shown to generate high quality samples, the lack of theoretical support m solution that can be directly trained with a gradient-based method. While the expectation of the likelihood term can be hard to derive in practice, <ref type=""bibr"" target=""#b8"">Kingma and Welling (2013)</ref> introduce a Stochastic Gradient Variational Bayes (SGVB) method to approximate the expec",1
"type=""bibr"" target=""#b0"">Cheng et al. (2011)</ref>, Random Forests by <ref type=""bibr"" target=""#b1"">Dong et al. (2012)</ref>, Rule-Based Learning by <ref type=""bibr"" target=""#b5"">Ho et al. (2015)</ref>, and Deep Learning by <ref type=""bibr"" target=""#b13"">Singh et al. (2017</ref><ref type=""bibr"" tar",0
"ession by <ref type=""bibr"" target=""#b6"">Karlic et al. (2010)</ref>, SVM by <ref type=""bibr"" target=""#b0"">Cheng et al. (2011)</ref>, Random Forests by <ref type=""bibr"" target=""#b1"">Dong et al. (2012)</ref>, Rule-Based Learning by <ref type=""bibr"" target=""#b5"">Ho et al. (2015)</ref>, and Deep Learning",0
"pplied a slew of different techniques. These techniques include Linear Regression by <ref type=""bibr"" target=""#b6"">Karlic et al. (2010)</ref>, SVM by <ref type=""bibr"" target=""#b0"">Cheng et al. (2011)</ref>, Random Forests by <ref type=""bibr"" target=""#b1"">Dong et al. (2012)</ref>, Rule-Based Learning expression. Similarly, other studies have tried to model the same problem as a regression task that includes SVR (Support Vector Regression) model by <ref type=""bibr"" target=""#b0"">Cheng et al. (2011)</ref> and DeepDIFF (Attention Based). These models attempt to estimate the differential gene express",0
"idated that there exists a correlation between histone modifications and Gene expression. Following computational methods, most notably DeepChrome by <ref type=""bibr"" target=""#b12"">Singh et al. (2016)</ref> and AttentiveChrome by <ref type=""bibr"" target=""#b13"">Singh et al. (2017)</ref> that employ d e-Based Learning by <ref type=""bibr"" target=""#b5"">Ho et al. (2015)</ref>, and Deep Learning by <ref type=""bibr"" target=""#b13"">Singh et al. (2017</ref><ref type=""bibr"" target=""#b12"">Singh et al. ( , 2016))</ref>. DeepChrome and AttentiveChrome are cell-specific gene expression prediction frameworks t n model.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head n=""3.3.1"">Data Preparation and Dimension Reduction</head><p>Following the work from <ref type=""bibr"" target=""#b12"">Singh et al. (2016</ref><ref type=""bibr"" target=""#b13"">Singh et al. ( , 2017))</ref>, we use five core Histone Modifica",0
"ensions, downstream tasks such as predictions will greatly benefit.There exist multiple dimension reduction techniques in literature, specifically by <ref type=""bibr"" target=""#b11"">Scholz et al. (2008)</ref>; <ref type=""bibr"" target=""#b7"">Kasun et al. (2016)</ref>, such as Principle Component Analys",0
"es and also avoid gradients vanishing and exploding and thus may converge to a better model. Beside, multiple embedding modules were also proposed by <ref type=""bibr"" target=""#b4"">Guo et al. (2020b)</ref> to learn DNA representations which was shown with better performance compared with just RNN, or",0
"ons for downstream supervised tasks can significantly improve prediction accuracy, as seen by <ref type=""bibr"" target=""#b15"">Yu and Lee (2019)</ref>; <ref type=""bibr"" target=""#b10"">Makhzani et al. (2016)</ref>; <ref type=""bibr"" target=""#b9"">Kingma et al. (2014)</ref>. Therefore, we propose to combin",0
"pression. Following computational methods, most notably DeepChrome by <ref type=""bibr"" target=""#b12"">Singh et al. (2016)</ref> and AttentiveChrome by <ref type=""bibr"" target=""#b13"">Singh et al. (2017)</ref> that employ deep learning to learn complex combinatorial interactions outperform all the prev type=""bibr"" target=""#b1"">Dong et al. (2012)</ref>, Rule-Based Learning by <ref type=""bibr"" target=""#b5"">Ho et al. (2015)</ref>, and Deep Learning by <ref type=""bibr"" target=""#b13"">Singh et al. (2017</ref><ref type=""bibr"" target=""#b12"">Singh et al. ( , 2016))</ref>. DeepChrome and AttentiveChrome ar 1.0""><head n=""3.3.1"">Data Preparation and Dimension Reduction</head><p>Following the work from <ref type=""bibr"" target=""#b12"">Singh et al. (2016</ref><ref type=""bibr"" target=""#b13"">Singh et al. ( , 2017))</ref>, we use five core Histone Modification marks for 56 different cell types derived from REM",0
"Function F for PSL Rule Grounding and Distance Calculation.</p><p>Input: PSL Rules R, Prediction ŷi , and Probability P(y|s i ), i = {1, 2, 3}; BERT <ref type=""bibr"" target=""#b17"">(Devlin et al. 2018)</ref>, to derive the sentence representation v i of d s -dimension to encode the input sequence s ttp://www.tei-c.org/ns/1.0""><head>Implementation Details</head><p>In the framework of CTRL-PG, any contextualized word embedding method, such as BERT <ref type=""bibr"" target=""#b17"">(Devlin et al. 2018)</ref>, ELMo <ref type=""bibr"" target=""#b41"">(Peters et al. 2018)</ref>, and RoBERTa <ref type=""bibr ""bibr"" target=""#b41"">(Peters et al. 2018)</ref>, and RoBERTa <ref type=""bibr"" target=""#b37"">(Liu et al. 2019b)</ref>, can be utilized. We choose BERT <ref type=""bibr"" target=""#b17"">(Devlin et al. 2018)</ref> to derive contextualized sentence embeddings without loss of generality. BERT adds a special adds a special token [CLS] at the beginning of each tokenized sequence and learns an embedding vector for it. We follow the experimental settings in <ref type=""bibr"" target=""#b17"">(Devlin et al. 2018)</ref> to use 12 Transformer layers and attention heads and set the embedding size d s as 768. The the fused Adam optimizer <ref type=""bibr"" target=""#b29"">(Kingma and Ba 2014)</ref> to optimize the parameters. We follow the experimental settings in <ref type=""bibr"" target=""#b17"">(Devlin et al. 2018)</ref> to set the dropout rate, and batch size as 10 1 and 8. We perform grid search for the initia",1
"alth crises like the recent COVID-19 pandemic. Clinical reports describe chronicle events, elucidating a chain of clinical observations and reasoning <ref type=""bibr"" target=""#b46"">(Sun, Rumshisky, and Uzuner 2013;</ref><ref type=""bibr"" target=""#b11"">Chen, Podchiyska, and Altman 2016)</ref>. Extract will lead to the same value for Precision, Recall, and F1. For I2B2-2012, we leverage the TempEval evaluation metrics used by the official challenge <ref type=""bibr"" target=""#b46"">(Sun, Rumshisky, and Uzuner 2013)</ref>, which also calculates the Precision, Recall, and Micro-average F1 scores. This r"" target=""#b21"">Graff 2002)</ref>, the corpora in the clinical domain require rich domain knowledge for annotating the temporal relations. I2b2-2012 <ref type=""bibr"" target=""#b46"">(Sun, Rumshisky, and Uzuner 2013)</ref> and Clinical TempEval <ref type=""bibr"" target=""#b3"">(Bethard et al. 2015</ref>< 1"" xml:id=""foot_0"">https://github.com/yuyanislearning/CTRL-PG.</note> 			<note xmlns=""http://www.tei-c.org/ns/1.0"" place=""foot"" n=""2"" xml:id=""foot_1""><ref type=""bibr"" target=""#b46"">Sun, Rumshisky, and Uzuner (2013)</ref> merged 7 original temporal relations</note> 			<note xmlns=""http://www.tei-c.or",0
"sely curate the clinical case reports into structured knowledge, i.e. extract important clinical named entities and relationships from the narratives <ref type=""bibr"" target=""#b1"">(Aronson and Lang 2010;</ref><ref type=""bibr"" target=""#b44"">Savova et al. 2010;</ref><ref type=""bibr"" target=""#b45"">Soys",0
"et=""#b1"">(Aronson and Lang 2010;</ref><ref type=""bibr"" target=""#b44"">Savova et al. 2010;</ref><ref type=""bibr"" target=""#b45"">Soysal et al. 2018;</ref><ref type=""bibr"" target=""#b8"">Caufield et al. 2019;</ref><ref type=""bibr"" target=""#b0"">Alfattni, Peek, and Nenadic 2020)</ref>. This would greatly ena",0
"""1"">Introduction</head><p>There has been a tremendous success in the field of natural language processing (NLP) since the development of Transformers <ref type=""bibr"" target=""#b27"">(Vaswani et al. 2017)</ref> which are currently the best performing neural network architectures for handling long-term nd Niyogi 2003)</ref> and use them as node positional information. Since Laplacian PEs are generalization of the PE used in the original transformers <ref type=""bibr"" target=""#b27"">(Vaswani et al. 2017)</ref> to graphs and these better help encode distance-aware information (i.e., nearby nodes have www.tei-c.org/ns/1.0""><head>Graph Transformer Layer</head><p>The Graph Transformer is closely the same transformer architecture initially proposed in <ref type=""bibr"" target=""#b27"">(Vaswani et al. 2017)</ref>, see Figure <ref type=""figure"">1</ref> (Left). We now proceed to define the node update equ eline surpassing GAT (see Table <ref type=""table"" target=""#tab_2"">2</ref>), which employs multi-headed attention inspired by the original transformer <ref type=""bibr"" target=""#b27"">(Vaswani et al. 2017)</ref> and have been often used in the literature as a baseline for attention-based GNN models. • eractions available. It thereby makes sense to have each word attending to each other word in a sentence, as followed by the Transformer architecture <ref type=""bibr"" target=""#b27"">(Vaswani et al. 2017</ref>). -b) Next, the so-called graph considered in an NLP transformer often has less than tens or",1
"t=""#b36"">Zhang et al. 2020)</ref> with few focused on specialized cases such as on heterogeneous graphs, temporal networks, generative modeling, etc. <ref type=""bibr"" target=""#b35"">(Yun et al. 2019;</ref><ref type=""bibr"" target=""#b33"">Xu, Joshi, and Bresson 2019;</ref><ref type=""bibr"">Hu et al. 2020",1
"o develop graph transformers <ref type=""bibr"" target=""#b15"">(Li et al. 2019;</ref><ref type=""bibr"" target=""#b20"">Nguyen, Nguyen, and Phung 2019;</ref><ref type=""bibr"" target=""#b36"">Zhang et al. 2020)</ref> with few focused on specialized cases such as on heterogeneous graphs, temporal networks, gene are other ways to incorporate the same instead of letting go sparsity and local contexts. For example, the use of graph-specific positional features <ref type=""bibr"" target=""#b36"">(Zhang et al. 2020)</ref>, or node Laplacian position eigenvectors <ref type=""bibr"" target=""#b2"">(Belkin and Niyogi 200 3</label><figDesc></figDesc><table /><note>Analysis of GraphTransformer (GT) using different PE schemes. Notations x: No PE; L: LapPE (ours); W: WL-PE<ref type=""bibr"" target=""#b36"">(Zhang et al. 2020)</ref>. Bold: the best performing model for each dataset.</note></figure> 			<note xmlns=""http://www cian eigenvectors as a suitable candidate PE for Graph Transformer in this section, by its comparison with different PE schemes applied in Graph-BERT <ref type=""bibr"" target=""#b36"">(Zhang et al. 2020</ref>).<ref type=""foot"" target=""#foot_0"">2</ref> In Graph-BERT, which operates on fixed size sampled ses unless we use similar sampling strategy. The WL-PE which are absolute structural roles of nodes in the original graph computed using WL algorithm <ref type=""bibr"" target=""#b36"">(Zhang et al. 2020;</ref><ref type=""bibr"" target=""#b21"">Niepert, Ahmed, and Kutzkov 2016)</ref>, are not variant to the",1
"ns/1.0""><head n=""1.1"">Related Work</head><p>As a preliminary, we highlight the most recent research works which attempt to develop graph transformers <ref type=""bibr"" target=""#b15"">(Li et al. 2019;</ref><ref type=""bibr"" target=""#b20"">Nguyen, Nguyen, and Phung 2019;</ref><ref type=""bibr"" target=""#b36 Xu, Joshi, and Bresson 2019;</ref><ref type=""bibr"">Hu et al. 2020;</ref><ref type=""bibr"" target=""#b37"">Zhou et al. 2020</ref>). The model proposed in <ref type=""bibr"" target=""#b15"">Li et al. (2019)</ref> employs attention to all graph nodes instead of a node's local neighbors for the purpose of capt",0
":id=""formula_10"">W 1 , ∈ R 2d×d , W 2 , ∈ R d×2d , ĥ +1 i</formula><p>, ĥ +1 i denote intermediate representations, and Norm can either be Layer-Norm <ref type=""bibr"" target=""#b1"">(Ba, Kiros, and Hinton 2016)</ref> or BatchNorm <ref type=""bibr"" target=""#b10"">(Ioffe and Szegedy 2015)</ref>. The bias",0
"ef>, in social sciences <ref type=""bibr"" target=""#b18"">(Monti et al. 2019)</ref>, in physics <ref type=""bibr"" target=""#b6"">(Cranmer et al. 2019;</ref><ref type=""bibr"" target=""#b24"">Sanchez-Gonzalez et al. 2020)</ref>, etc.</p><p>In particular, GNNs exploit the given arbitrary graph structure while l",0
"rations</head><p>For experiments, we follow the benchmarking protocol introduced in Dwivedi et al. ( <ref type=""formula"">2020</ref>) based on PyTorch <ref type=""bibr"" target=""#b22"">(Paszke et al. 2019)</ref> and DGL <ref type=""bibr"">(Wang et al. 2019)</ref>. We use 10 layers of Graph Transformer lay",0
"esson, and Vandergheynst 2016;</ref><ref type=""bibr"" target=""#b13"">Kipf and Welling 2017;</ref><ref type=""bibr"" target=""#b17"">Monti et al. 2017;</ref><ref type=""bibr"" target=""#b9"">Gilmer et al. 2017;</ref><ref type=""bibr"" target=""#b28"">Veličković et al. 2018;</ref><ref type=""bibr"" target=""#b3"">Bress",0
"mbining feature information from other words in a sentence can alternatively be viewed as a case of a GNN applied on a fully connected graph of words <ref type=""bibr"" target=""#b12"">(Joshi 2020)</ref>. Transformers based models have led to state-of-the-art performance on several NLP applications <ref",0
"f Graph-BERT positional encoding schemes, along with experimental comparison with the model we present in this paper in Section 4.1.</p><p>Yun et al. <ref type=""bibr"" target=""#b30"">(2019)</ref> developed Graph Transformer Networks (GTN) to learn on heterogeneous graphs with a target to transform a g",0
"g/ns/1.0""><head n=""3"">Numerical Experiments</head><p>We evaluate the performance of proposed Graph Transformer on three benchmark graph datasets-ZINC <ref type=""bibr"" target=""#b11"">(Irwin et al. 2012)</ref>, PATTERN and CLUSTER (Abbe 2017) from a recent GNN benchmark <ref type=""bibr"" target=""#b8"">(D TERN and CLUSTER (Abbe 2017) from a recent GNN benchmark <ref type=""bibr"" target=""#b8"">(Dwivedi et al. 2020)</ref>.</p><p>ZINC, Graph Regression ZINC <ref type=""bibr"" target=""#b11"">(Irwin et al. 2012</ref>) is a molecular dataset with the task of graph property regression for constrained solubility.",0
"pe=""bibr"" target=""#b35"">(Yun et al. 2019;</ref><ref type=""bibr"" target=""#b33"">Xu, Joshi, and Bresson 2019;</ref><ref type=""bibr"">Hu et al. 2020;</ref><ref type=""bibr"" target=""#b37"">Zhou et al. 2020</ref>). The model proposed in <ref type=""bibr"" target=""#b15"">Li et al. (2019)</ref> employs attention of relative temporal positional encoding which is based on the timestamp differences of the central node and the message-passing nodes. Furthermore, <ref type=""bibr"" target=""#b37"">Zhou et al. (2020)</ref> proposed a transformer based generative model which generates temporal graphs by directly lear",0
"such datasets. On these accounts, it is ideal and practical to have a Graph Transformer where a node attends to local node neighbors, same as in GNNs <ref type=""bibr"" target=""#b7"">(Defferrard, Bresson, and Vandergheynst 2016;</ref><ref type=""bibr"" target=""#b13"">Kipf and Welling 2017;</ref><ref type=",0
"denote intermediate representations, and Norm can either be Layer-Norm <ref type=""bibr"" target=""#b1"">(Ba, Kiros, and Hinton 2016)</ref> or BatchNorm <ref type=""bibr"" target=""#b10"">(Ioffe and Szegedy 2015)</ref>. The bias terms are omitted for clarity of presentation.</p><p>Graph Transformer Layer w",0
"e MSA can be generalised to other choices of kernels and normalisation that are equally valid <ref type=""bibr"" target=""#b51"">(Wang et al., 2018;</ref><ref type=""bibr"" target=""#b46"">Tsai et al., 2019)</ref>.</p><formula xml:id=""formula_10"">M SA(X) [f 1 (X), . . . , f M (X)]W O ∈ R N ×D . (6)</formula kernel or attention module <ref type=""bibr"" target=""#b6"">(Cohen &amp; Welling, 2016b;</ref><ref type=""bibr"" target=""#b55"">Worrall et al., 2017;</ref><ref type=""bibr"" target=""#b46"">Thomas et al., 2018;</ref><ref type=""bibr"">Kondor et al., 2018;</ref><ref type=""bibr"" target=""#b54"">Weiler et al., 2018",0
"f (g) for g ∈ G f . It is easy to show that any such pointwise operations are equivariant with respect to the regular representation.</p><p>LayerNorm <ref type=""bibr"" target=""#b1"">(Ba et al., 2016)</ref> is defined as follows:</p><p>Inputs: {g, f (g)} g∈G f where • f (g) ∈ R dv • G f defined as in S",0
"=""#b36"">(Oliphant, 2006;</ref><ref type=""bibr"" target=""#b49"">Walt et al., 2011;</ref><ref type=""bibr"" target=""#b18"">Harris et al., 2020)</ref>, SciPy <ref type=""bibr"" target=""#b24"">(Jones et al., 2001)</ref>, and Matplotlib <ref type=""bibr"" target=""#b22"">(Hunter, 2007)</ref>.</p></div> 			</div>",0
"2011;</ref><ref type=""bibr"" target=""#b18"">Harris et al., 2020)</ref>, SciPy <ref type=""bibr"" target=""#b24"">(Jones et al., 2001)</ref>, and Matplotlib <ref type=""bibr"" target=""#b22"">(Hunter, 2007)</ref>.</p></div> 			</div>  			<div type=""annex""> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>Appendi",0
"target=""#b40"">Parmar et al., 2019b)</ref>, reinforcement learning <ref type=""bibr"" target=""#b38"">(Parisotto et al., 2020)</ref>, and audio generation <ref type=""bibr"" target=""#b21"">(Huang et al., 2019)</ref>. (2) While convolutions are inherently translation equivariant, self-attention is permutatio",0
"ends to use EBM representations inside the training loops of neural networks, blurring different dimensions of the problem. By contrast -similarly to <ref type=""bibr"" target=""#b35"">Parshakova et al. (2019a;</ref><ref type=""bibr"">b)</ref> in a different context -we clearly decouple the relatively sim IS (Self Normalized Importance Sampling) <ref type=""bibr"" target=""#b23"">(Kim &amp; Bengio, 2016;</ref><ref type=""bibr"" target=""#b34"">Owen, 2013;</ref><ref type=""bibr"" target=""#b35"">Parshakova et al., 2019a</ref>) SNIS consists in computing:</p><formula xml:id=""formula_4"">μ(λ) = N i=1 w i (λ) φ(x i ) 1"">(Andor et al., 2016;</ref><ref type=""bibr"" target=""#b4"">Belanger &amp; McCallum, 2016)</ref>. Some current applications to text generation include <ref type=""bibr"" target=""#b35"">Parshakova et al. (2019a)</ref> and <ref type=""bibr"" target=""#b14"">Deng et al. (2020)</ref>, who augment a standard aut",1
""" target=""#foot_1"">2</ref> To this end, we introduce KL-adaptive DPG (Distributional Policy Gradient), a variant of an algorithm recently proposed in <ref type=""bibr"" target=""#b36"">(Parshakova et al., 2019b)</ref>. We train the policy π θ to approximate p in an adaptive way, by speeding up the next e here, namely sampling from a sequential EBM that includes an autoregressive component a(x), is the DPG (""Distributional Policy Gradient"") algorithm <ref type=""bibr"" target=""#b36"">(Parshakova et al., 2019b)</ref>. The objective of DPG is to obtain an autoregressive policy π θ that approximates p, w",1
"in pretrained language models. <ref type=""bibr"" target=""#b51"">(Stanovsky et al., 2019;</ref><ref type=""bibr"" target=""#b40"">Prates et al., 2020;</ref><ref type=""bibr"" target=""#b47"">Sheng et al., 2019a;</ref><ref type=""bibr"" target=""#b7"">Brown et al., 2020b)</ref>. However, applying distributional co",0
"mographics are severely underrepresented. One of those demographics is women, whose biographies make up only 18.58% of English Wikipedia's biographies<ref type=""bibr"" target=""#b15"">(Graells-Garrido et al., 2015)</ref>. It is expected that such bias is transferred if not amplified by Language Models.",0
"fields. Some early NLP-related EBM research is concerned with neural-based sequence labelling problems (e.g. tagging) exploiting the global sequence <ref type=""bibr"" target=""#b1"">(Andor et al., 2016;</ref><ref type=""bibr"" target=""#b4"">Belanger &amp; McCallum, 2016)</ref>. Some current applications",0
"al. (2020)</ref>, who augment a standard autoregressive LM with an additional global factor in order to get a lower perplexity on the training data. <ref type=""bibr"" target=""#b53"">Tu et al. (2020)</ref> propose an energy-based method to perform inference networks from pretrained Non-Autoregressive",0
"rameters across all approaches by setting the temperature T = 1.0, using top-k sampling with k = 10, and removing the repetition penalty used in CTRL <ref type=""bibr"" target=""#b22"">(Keskar et al., 2019)</ref>. However, we notice that CTRL does not work well with higher T values (apparent in the samp",0
"zing KL divergence from a: </p><p>Equation ( <ref type=""formula"" target=""#formula_0"">1</ref>) is a generalization of the Maximum Entropy Principle of <ref type=""bibr"" target=""#b21"">Jaynes (1957)</ref>, which corresponds to the limit case where a is the uniform u distribution over X, noting that mini",0
"ization of the Maximum Entropy Principle and leads to a unique solution P (x). P (x) is an unnormalized distribution, aka an Energy-Based Model (EBM) <ref type=""bibr"" target=""#b16"">(Hinton, 2002;</ref><ref type=""bibr"" target=""#b26"">LeCun et al., 2006;</ref><ref type=""bibr"" target=""#b3"">Bakhtin et al ly in-adapted when focusing on language models where sample diversity is a requirement.</p><p>Energy Based Models for Text Energy-Based Models (EBMs) <ref type=""bibr"" target=""#b16"">(Hinton, 2002;</ref><ref type=""bibr"" target=""#b26"">LeCun et al., 2006;</ref><ref type=""bibr"" target=""#b42"">Ranzato et a",0
"ieved promising results in many NLP areas, such as machine translation <ref type=""bibr"" target=""#b10"">(Gu et al. 2018)</ref>, task-oriented dialogues <ref type=""bibr"" target=""#b24"">(Qian and Yu 2019;</ref><ref type=""bibr"" target=""#b20"">Mi et al. 2019), and</ref><ref type=""bibr"">text classification (",1
"h-resource diseases to others of data scarcity. Besides, existing knowledge-grounded approaches <ref type=""bibr"" target=""#b17"">(Liu et al. 2018;</ref><ref type=""bibr"" target=""#b15"">Lian et al. 2019</ref>) may fail to perform such transfer well, as they only learn one unified model for all diseases a ref type=""bibr"" target=""#b36"">Zhang et al. 2020;</ref><ref type=""bibr"" target=""#b21"">Moon et al. 2019)</ref> or retrieved from unstructured documents <ref type=""bibr"" target=""#b15"">(Lian et al. 2019;</ref><ref type=""bibr"" target=""#b37"">Zhao et al. 2019;</ref><ref type=""bibr"" target=""#b13"">Kim, Ahn, compare our base dialogue model MGR with two knowledge-grounded dialogue systems, NKD <ref type=""bibr"" target=""#b17"">(Liu et al. 2018)</ref> and POKS <ref type=""bibr"" target=""#b15"">(Lian et al. 2019)</ref>. NKD uses a neural knowledge diffusion module to introduce relative entities into dialogue gen tly on target diseases. We test the above three base models and denote them as PT-NKD <ref type=""bibr"" target=""#b17"">(Liu et al. 2018)</ref>, PT-POKS <ref type=""bibr"" target=""#b15"">(Lian et al. 2019</ref>) and PT-MGR in Sec. 4. This is a zero-shot learning scenario. ? Fine-tuning. We pre-train f ? o",0
"rpora have achieved significant success. However, fine-tuning such large language models in the medical domain requires sufficient task-specific data <ref type=""bibr"" target=""#b1"">(Bansal, Jha, and McCallum 2019;</ref><ref type=""bibr"" target=""#b5"">Dou, Yu, and Anastasopoulos 2019</ref>) so as to lea",0
"directly in absence of medical knowledge. Recently, large-scale pre-training language models <ref type=""bibr"" target=""#b3"">(Devlin et al. 2019;</ref><ref type=""bibr"" target=""#b25"">Radford et al. 2019;</ref><ref type=""bibr"" target=""#b29"">Song et al. 2019</ref>) over unsupervised corpora have achieve",0
"o MDS, which requires grounding on the external medical knowledge and reasoning for disease-symptom correlations. In this work, we employ the Reptile <ref type=""bibr"" target=""#b22"">(Nichol, Achiam, and Schulman 2018)</ref>, one firstorder model-agnostic meta learning approach, because of its efficie iate the computational cost for the second-order gradient, i.e. Hessian matrix, in Eqn. ( <ref type=""formula"" target=""#formula_12"">11</ref>), Reptile <ref type=""bibr"" target=""#b22"">(Nichol, Achiam, and Schulman 2018)</ref> approximates the second derivatives of the validation loss as</p><formula xml POKS and FT-MGR. ? Meta Learning. We first meta-train three base dialogue models over source diseases with the effective metalearning method, Reptile <ref type=""bibr"" target=""#b22"">(Nichol, Achiam, and Schulman 2018)</ref>, and then adapt the derived meta-learners to each target disease via fine-tun",0
"uickly adapting to new downstream tasks with only a few fine-tuning updates. The proposed learning to pre-train can be deemed a form of meta-learning <ref type=""bibr"" target=""#b12"">(Finn, Abbeel, and Levine 2017)</ref>, also known as learning to learn. For the second challenge, we propose a self-sup ent to conventional pre-training approaches. Furthermore, our strategy is a form of meta-learning, in particular, model agnostic meta-learning (MAML) <ref type=""bibr"" target=""#b12"">(Finn, Abbeel, and Levine 2017)</ref>. Meta-learning aims to learn prior knowledge from a set of training tasks that ca across tasks. Finally, some optimization-based methods directly adjust the optimization algorithm to enable quick adaptation with just a few examples <ref type=""bibr"" target=""#b12"">(Finn, Abbeel, and Levine 2017;</ref><ref type=""bibr"" target=""#b43"">Yao et al. 2019;</ref><ref type=""bibr"" target=""#b21",1
"y goal of pre-training GNNs <ref type=""bibr"" target=""#b28"">(Navarin, Tran, and Sperduti 2018;</ref><ref type=""bibr"" target=""#b18"">Hu et al. 2019</ref><ref type=""bibr"" target=""#b17"">Hu et al. , 2020) )</ref> is to learn transferable prior knowledge from mostly unlabeled data, which can be generalized an, and Sperduti 2018;</ref><ref type=""bibr"" target=""#b18"">Hu et al. 2019)</ref>, or still require supervised information for graph-level pretraining <ref type=""bibr"" target=""#b17"">(Hu et al. 2020</ref>). While at the node level, predicting links between node pairs is naturally self-supervised, grap al GNN Pre-training. The goal of pre-training GNNs is to learn a generic initialization for model parameters using readily available graph structures <ref type=""bibr"" target=""#b17"">(Hu et al. 2020</ref><ref type=""bibr"" target=""#b18"">(Hu et al. , 2019))</ref>. Conventional pre-training strategies lar .0""><head n=""5.1"">Experimental Settings</head><p>Datasets. We conduct experiments on data from two domains: biological function prediction in biology <ref type=""bibr"" target=""#b17"">(Hu et al. 2020</ref>) and research field prediction in bibliography. The biology graphs come from a public repository< b18"">(Hu et al. 2019)</ref> pre-trains graph encoders with three unsupervised tasks to capture different aspects of a graph. More recently, Hu et al. <ref type=""bibr"" target=""#b17"">(Hu et al. 2020)</ref> propose different strategies to pre-train graph neural networks at both node and graph levels, a (e.g., artificial intelligence, data mining). Each subgraph is centered at a paper and contains the associated information of For biology data, as in <ref type=""bibr"" target=""#b17"">(Hu et al. 2020)</ref>, we use 306,925 unlabeled protein ego-networks for pre-training. In fine-tuning, we predict 40 f biological functions with 88,000 labeled subgraphs that correspond to 40 binary classification tasks. We split the downstream data with species split <ref type=""bibr"" target=""#b17"">(Hu et al. 2020)</ref>, and evaluate the test performance with average ROC-AUC <ref type=""bibr"" target=""#b3"">(Bradley 1 . 2019</ref>) to maximize local mutual information across the graph's patch representations; (3) Context Prediction strategy (denoted by ContextPred) <ref type=""bibr"" target=""#b17"">(Hu et al. 2020)</ref> to explore graph structures and (4) Attribute Masking strategy (denoted by AttrMasking) <ref typ xtPred) <ref type=""bibr"" target=""#b17"">(Hu et al. 2020)</ref> to explore graph structures and (4) Attribute Masking strategy (denoted by AttrMasking) <ref type=""bibr"" target=""#b17"">(Hu et al. 2020)</ref> to learn the regularities of the node and edge attributes distributed over graphs. Further detai n information irrelevant to the downstream tasks, which harms the generalization of the pre-trained GNNs. This finding confirms previous observations <ref type=""bibr"" target=""#b17"">(Hu et al. 2020;</ref><ref type=""bibr"" target=""#b31"">Rosenstein et al. 2005</ref>) that negative transfer results in li",1
"lity to recursively aggregate information from neighborhoods on the graph, naturally capturing both graph structures as well as node or edge features <ref type=""bibr"" target=""#b47"">(Zhang, Cui, and Zhu 2020;</ref><ref type=""bibr"" target=""#b42"">Wu et al. 2020;</ref><ref type=""bibr"" target=""#b10"">Dwiv Ns, we refer readers to the literature <ref type=""bibr"" target=""#b42"">(Wu et al. 2020;</ref><ref type=""bibr"" target=""#b2"">Battaglia et al. 2018;</ref><ref type=""bibr"" target=""#b47"">Zhang, Cui, and Zhu 2020;</ref><ref type=""bibr"" target=""#b48"">Zhou et al. 2018)</ref>. To enable more effective learnin",0
"al. 2013</ref>) and image encoders <ref type=""bibr"" target=""#b13"">(Girshick et al. 2014;</ref><ref type=""bibr"" target=""#b8"">Donahue et al. 2014;</ref><ref type=""bibr"" target=""#b16"">He et al. 2019)</ref>, recent advances in pre-training GNNs have provided insights into reducing the labeling burden an",0
"[h l v ]</formula><p>is the node representation matrix. READOUT is typically implemented as a simple pooling operation like sum, max or mean-pooling <ref type=""bibr"" target=""#b1"">(Atwood and Towsley 2016;</ref><ref type=""bibr"" target=""#b9"">Duvenaud et al. 2015)</ref> or more complex approaches <ref",0
"significant attention due to the prevalence of graph-structured data <ref type=""bibr"" target=""#b4"">(Bronstein et al. 2017)</ref>. Originally proposed <ref type=""bibr"" target=""#b26"">(Marco, Gabriele, and Franco 2005;</ref><ref type=""bibr"" target=""#b33"">Scarselli et al. 2008</ref>) as a framework of u ction in bibliography. The biology graphs come from a public repository<ref type=""foot"" target=""#foot_0"">1</ref> , covering 394,925 protein subgraphs <ref type=""bibr"" target=""#b26"">(Marinka et al. 2019)</ref>. We further present a new collection of bibliographic graphs called PreDBLP, purposely comp",0
"""#b12"">(Finn, Abbeel, and Levine 2017;</ref><ref type=""bibr"" target=""#b43"">Yao et al. 2019;</ref><ref type=""bibr"" target=""#b21"">Lee et al. 2019;</ref><ref type=""bibr"" target=""#b24"">Lu, Fang, and Shi 2020)</ref>.</p><p>3 Learning to Pre-train:</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>",0
"mmendation systems <ref type=""bibr"" target=""#b11"">(Fan et al. 2019;</ref><ref type=""bibr"" target=""#b44"">Ying et al. 2018a</ref>) and graph generation <ref type=""bibr"" target=""#b23"">(Li et al. 2018;</ref><ref type=""bibr"" target=""#b46"">You et al. 2018</ref>). However, training GNNs usually requires ab",0
"d making use of abundant unlabeled data. The primary goal of pre-training GNNs <ref type=""bibr"" target=""#b28"">(Navarin, Tran, and Sperduti 2018;</ref><ref type=""bibr"" target=""#b18"">Hu et al. 2019</ref><ref type=""bibr"" target=""#b17"">Hu et al. , 2020) )</ref> is to learn transferable prior knowledge f ata? Existing methods either only take into account the node-level pretraining <ref type=""bibr"" target=""#b28"">(Navarin, Tran, and Sperduti 2018;</ref><ref type=""bibr"" target=""#b18"">Hu et al. 2019)</ref>, or still require supervised information for graph-level pretraining <ref type=""bibr"" target=""#b1 Navarin et al. <ref type=""bibr"" target=""#b28"">(Navarin, Tran, and Sperduti 2018)</ref> utilize the graph kernel for pre-training, while another work <ref type=""bibr"" target=""#b18"">(Hu et al. 2019)</ref> pre-trains graph encoders with three unsupervised tasks to capture different aspects of a graph. s to learn a generic initialization for model parameters using readily available graph structures <ref type=""bibr"" target=""#b17"">(Hu et al. 2020</ref><ref type=""bibr"" target=""#b18"">(Hu et al. , 2019))</ref>. Conventional pre-training strategies largely follow a twostep paradigm. (1) Pre-training a G",0
"ton, Ying, and Leskovec 2017;</ref><ref type=""bibr"" target=""#b39"">Velickovic et al. 2018;</ref><ref type=""bibr"" target=""#b45"">Ying et al. 2018b;</ref><ref type=""bibr"" target=""#b15"">Hasanzadeh et al. 2019;</ref><ref type=""bibr"" target=""#b30"">Qu, Bengio, and Tang 2019;</ref><ref type=""bibr"" target=""#b",0
"g from deep teachers. Different alternatives have been proposed to this end, which compare networks' internal layers in addition to final predictions <ref type=""bibr"" target=""#b15"">(Jiao et al. 2019;</ref><ref type=""bibr"" target=""#b21"">Sun et al. 2020</ref><ref type=""bibr"">Sun et al. , 2019))</ref>, .</p><p>A common heuristic to devise A is to divide teacher layers into m buckets with approximately the same sizes and pick only one layer from each <ref type=""bibr"" target=""#b15"">(Jiao et al. 2019;</ref><ref type=""bibr"">Sun et al. 2019)</ref>. Therefore, for the j-th layer of the student model, A( t of the l-th teacher layer (h i,j S $ h i,l T ). PKD is not the only model that utilizes internal layers' information. Other models such as TinyBERT <ref type=""bibr"" target=""#b15"">(Jiao et al. 2019)</ref> and MobileBERT <ref type=""bibr"" target=""#b21"">(Sun et al. 2020</ref>) also found it crucial fo",1
"proposed to this end, which compare networks' internal layers in addition to final predictions <ref type=""bibr"" target=""#b15"">(Jiao et al. 2019;</ref><ref type=""bibr"" target=""#b21"">Sun et al. 2020</ref><ref type=""bibr"">Sun et al. , 2019))</ref>, but they also suffer from other types of problems. The odel that utilizes internal layers' information. Other models such as TinyBERT <ref type=""bibr"" target=""#b15"">(Jiao et al. 2019)</ref> and MobileBERT <ref type=""bibr"" target=""#b21"">(Sun et al. 2020</ref>) also found it crucial for training competitive student models. However, as Equation <ref type=""",1
"anh et al. 2019)</ref>. Apart from this, we also consider it as a complementary and generic add-on to enrich the training process of any neural model <ref type=""bibr"" target=""#b11"">(Furlanello et al. 2018)</ref>.</p><p>In KD, a student network (S) is glued to a powerful teacher (T ) during training.",0
"rst and training is progressively shifted to upper layers. They claim that the way internal layers are trained during KD can play a significant role. <ref type=""bibr"" target=""#b16"">Liu et al. (2019)</ref> investigated KD from another perspective. Instead of focusing on the compression aspect, they k",0
"leDesc> 	</teiHeader> 	<text xml:lang=""en""> 		<body> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>Introduction</head><p>Knowledge distillation (KD) <ref type=""bibr"" target=""#b5"">(Buciluǎ, Caruana, and Niculescu-Mizil 2006;</ref><ref type=""bibr"" target=""#b13"">Hinton, Vinyals, and Dean 2015)</ref> i future directions.</p></div> <div xmlns=""http://www.tei-c.org/ns/1.0""><head>Related Work</head><p>KD was originally proposed for tasks other than NLP <ref type=""bibr"" target=""#b5"">(Buciluǎ, Caruana, and Niculescu-Mizil 2006;</ref><ref type=""bibr"" target=""#b13"">Hinton, Vinyals, and Dean 2015)</ref>.",0
". Recently, with the emergence of large NLP and language understanding (NLU) models such as <ref type=""bibr"">ELMO (Peters et al. 2018)</ref> and BERT <ref type=""bibr"" target=""#b8"">(Devlin et al. 2019</ref>) KD has gained extra attention. Deep models can be trained in a better fashion and compressed inal BERT<ref type=""foot"" target=""#foot_1"">2</ref> (also known as BERT Base ) as our teacher. We are faithful to the experimental setting proposed by <ref type=""bibr"" target=""#b8"">Devlin et al. (2019)</ref> for this model. Therefore, our in-house version of it also has 12 layers with 12 attention he .</p><p>After initializing students, they are fine-tuned during training. The concept of fine-tuning and its details are comprehensively discussed in <ref type=""bibr"" target=""#b8"">Devlin et al. (2019)</ref>, so we skip that part and refer the reader to the original paper. We have the same pipeline f .tei-c.org/ns/1.0"" place=""foot"" n=""1"" xml:id=""foot_0"">By the output, we mean the output of the layer for the CLS token. For more details about CLS see<ref type=""bibr"" target=""#b8"">Devlin et al. (2019)</ref>.</note> 			<note xmlns=""http://www.tei-c.org/ns/1.0"" place=""foot"" n=""2"" xml:id=""foot_1"">https",0
"6;</ref><ref type=""bibr"" target=""#b13"">Hinton, Vinyals, and Dean 2015)</ref> is a commonly-used technique to reduce the size of large neural networks <ref type=""bibr"" target=""#b18"">(Sanh et al. 2019)</ref>. Apart from this, we also consider it as a complementary and generic add-on to enrich the trai rformance in NLU tasks. Table <ref type=""table"" target=""#tab_0"">1</ref>  S RKD is equivalent to a configuration known as DistilBERT in the literature <ref type=""bibr"" target=""#b18"">(Sanh et al. 2019)</ref>. To have precise results and a better comparison we trained/fine-tuned all models in the same",0
"this issue by introducing a novel decoding objective to automatically generate prompts given the few-shot training data using the generative T5 model <ref type=""bibr"" target=""#b30"">(Raffel et al., 2020)</ref>. This allows us to cheaply obtain effective prompts that match or outperform our manually c enerate a diverse set of templates {T } automatically from a fixed set of label words M(Y). To address this challenging problem, we propose to use T5 <ref type=""bibr"" target=""#b30"">(Raffel et al., 2020)</ref>, a large pre-trained text-to-text Transformer. T5 is pre-trained on a mixture of unsupervis hat we use in our experiments are provided in Table <ref type=""table"">B</ref>.1.</p><p>For automatic template search with T5, we take the T5-3B model <ref type=""bibr"" target=""#b30"">(Raffel et al., 2020)</ref>, which is the largest publicly available one that can fit on a single GPU. For automaticall",1
"b9"">(Dagan et al., 2005;</ref><ref type=""bibr"" target=""#b3"">Bar-Haim et al., 2006;</ref><ref type=""bibr"" target=""#b15"">Giampiccolo et al., 2007;</ref><ref type=""bibr"" target=""#b5"">Bentivogli et al., 2009)</ref>, MRPC <ref type=""bibr"" target=""#b13"">(Dolan and Brockett, 2005)</ref>, QQP<ref type=""foot",0
"(Rajpurkar et al., 2016)</ref>, RTE <ref type=""bibr"" target=""#b9"">(Dagan et al., 2005;</ref><ref type=""bibr"" target=""#b3"">Bar-Haim et al., 2006;</ref><ref type=""bibr"" target=""#b15"">Giampiccolo et al., 2007;</ref><ref type=""bibr"" target=""#b5"">Bentivogli et al., 2009)</ref>, MRPC <ref type=""bibr"" targ",0
"nd Lee, 2005)</ref>, CR <ref type=""bibr"" target=""#b18"">(Hu and Liu, 2004)</ref>, MPQA <ref type=""bibr"" target=""#b43"">(Wiebe et al., 2005)</ref>, Subj <ref type=""bibr"" target=""#b24"">(Pang and Lee, 2004</ref>)-we simply randomly sample 2,000 examples as the testing set and leave them out from training",0
"</encodingDesc> 		<profileDesc> 			<abstract> <div xmlns=""http://www.tei-c.org/ns/1.0""><p>We generalize deep self-attention distillation in MINILM <ref type=""bibr"" target=""#b40"">(Wang et al., 2020)</ref> by only using self-attention relation distillation for taskagnostic compression of pretrained neck structures to keep their layer number and hidden size the same, layer-wisely transferring hidden states and self-attention distributions. MINILM <ref type=""bibr"" target=""#b40"">(Wang et al., 2020)</ref> proposes deep self-attention distillation, which uses self-attention distributions and value n heads of student model has to be the same as its teacher.</p><p>In this work, we generalize and simplify deep self-attention distillation of MINILM <ref type=""bibr"" target=""#b40"">(Wang et al., 2020)</ref> by using self-attention relation distillation. We introduce multi-head self-attention relatio "" target=""#b34"">(Sun et al., 2019b)</ref> assumes the student has the same number of layers as its teacher to perform layer-wise distillation. MINILM <ref type=""bibr"" target=""#b40"">(Wang et al., 2020)</ref> transfers selfattention knowledge of teacher's last layer to the student last Transformer lay "" target=""#b36"">Tsai et al., 2019;</ref><ref type=""bibr"" target=""#b14"">Jiao et al., 2019;</ref><ref type=""bibr"" target=""#b34"">Sun et al., 2019b;</ref><ref type=""bibr"" target=""#b40"">Wang et al., 2020)</ref>. The student models are distilled from large pretrained Transformers using large-scale text co target=""#b30"">(Sanh et al., 2019;</ref><ref type=""bibr"" target=""#b14"">Jiao et al., 2019;</ref><ref type=""bibr"" target=""#b33"">Sun et al., 2019a;</ref><ref type=""bibr"" target=""#b40"">Wang et al., 2020)</ref>  MobileBERT compresses a specially designed teacher model (in the BERT LARGE size) with invert target=""#b30"">Sanh et al. (2019)</ref>. The rest results of Distil-BERT, TinyBERT 2 , BERT SMALL , Truncated BERT BASE and 6×768 MINILM are taken from<ref type=""bibr"" target=""#b40"">Wang et al. (2020)</ref>. BERT SMALL</figDesc><table><row><cell>Model</cell><cell>Teacher</cell><cell cols=""9"">#Param S",1
"r"" target=""#b7"">Devlin et al., 2018;</ref><ref type=""bibr"" target=""#b9"">Dong et al., 2019;</ref><ref type=""bibr"" target=""#b9"">Yang et al., 2019;</ref><ref type=""bibr"" target=""#b15"">Joshi et al., 2019;</ref><ref type=""bibr"" target=""#b21"">Liu et al., 2019;</ref><ref type=""bibr"" target=""#b1"">Bao et al. type=""bibr"">XL-Net (Yang et al., 2019)</ref> introduces permutation language modeling objective to predict masked tokens auto-regressively. SpanBERT <ref type=""bibr"" target=""#b15"">(Joshi et al., 2019)</ref> improves BERT by incorporating span information. RoBERTa <ref type=""bibr"" target=""#b21"">(Liu",0
"r"" target=""#b9"">Yang et al., 2019;</ref><ref type=""bibr"" target=""#b15"">Joshi et al., 2019;</ref><ref type=""bibr"" target=""#b21"">Liu et al., 2019;</ref><ref type=""bibr"" target=""#b1"">Bao et al., 2020;</ref><ref type=""bibr"" target=""#b24"">Radford et al., 2019;</ref><ref type=""bibr"" target=""#b25"">Raffel e ART <ref type=""bibr"" target=""#b19"">(Lewis et al., 2019)</ref> employ a standard encoder-decoder structure and pretrain the decoder auto-regressively. <ref type=""bibr"" target=""#b1"">Bao et al. (2020)</ref> propose a pseud-masked language model by jointly pretrained on MLM and partially auto-regressive",0
"br"" target=""#b15"">Joshi et al., 2019;</ref><ref type=""bibr"" target=""#b21"">Liu et al., 2019;</ref><ref type=""bibr"" target=""#b1"">Bao et al., 2020;</ref><ref type=""bibr"" target=""#b24"">Radford et al., 2019;</ref><ref type=""bibr"" target=""#b25"">Raffel et al., 2019;</ref><ref type=""bibr"" target=""#b19"">Lewi",0
